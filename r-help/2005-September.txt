From jeaneid at chass.utoronto.ca  Thu Sep  1 00:46:09 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 31 Aug 2005 18:46:09 -0400
Subject: [R] loop
In-Reply-To: <1309.128.240.6.80.1125503094.squirrel@128.240.6.80>
Message-ID: <Pine.SGI.4.40.0508311844510.1317623-100000@origin.chass.utoronto.ca>

just one more thing. Why are you defining the function over and over
again...


On Wed, 31 Aug 2005, Hathaikan Chootrakool wrote:

> I was wondering why this loop doesn't work!
>
> for (i in 1:k)
> fnTr[i] <-  function (p) 0.5* sum ( n*log(2*pi) - log(sd(i)^2)
>                          +(logitp(i)-p)^2/sd(i)^2 )
> outTr[i]<- nlm (fnTr[i],p=c(10),hessian=TRUE)
> minimumTr[i] <- outTr[i]$minimum
> valueTr[i] <- outTr[i]$estimate
> list (minimumTr[i],valueTr[i])
>
> Has anyone can help me?
>
> Thank you very much
> Hathaikan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jeaneid at chass.utoronto.ca  Thu Sep  1 00:54:44 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 31 Aug 2005 18:54:44 -0400
Subject: [R] R environment
Message-ID: <Pine.SGI.4.40.0508311847100.1277557-100000@origin.chass.utoronto.ca>

This is probably a weird question but I need to know if there is a way...

I run an R batch job without saving the variables at each step to the
disk.   Is there a way to invoke another session of R and link it to the
same environment for read only.

The problem is that I am running optim with every step getting the
parameters into the global env using <<- However, I forgot to issue a
save(list=ls(),...) right after so I can load and see how the parameters
are changing. It's been couple of days and it is still running so I am
hoping that I can invoke another session of R and link it to the
environment of the batch session. Does this sound totally ridiculous ?

it is a debian machine with R 2.1.1


Thanks

Jean



From nkn at turing.une.edu.au  Thu Sep  1 01:14:08 2005
From: nkn at turing.une.edu.au (Nam-Ky Nguyen)
Date: Thu, 1 Sep 2005 09:14:08 +1000 (EST)
Subject: [R] R binaries, platform independent and Design of Experiments
In-Reply-To: <43156806.4020608@hhbio.wasser.tu-dresden.de>
References: <20735.129.180.11.34.1125460179.squirrel@129.180.11.34>
	<43156806.4020608@hhbio.wasser.tu-dresden.de>
Message-ID: <50879.59.167.30.131.1125530048.squirrel@59.167.30.131>

Dear Rexpert,

I would like to thank those who spend time  answering my email on the
burning of a CD with all R binary files for Windows and Linux. I have
tried a couple of suggestions but have not been successful. I will pass
these suggestions to our system administrator and I am sure that he can
sort them out.

I hope that the future version of R will be written in Java so that it is
platform independent. This means there will be a single binary file for
each new version of R. At the moment there are files for Windows, Mac OS,
Fedora 1, 2, 3, 4, SUSE 9.1, 9.2, 9.3, etc.

I also hope that the future of version of R include more DOE (Design of
Experiments) modules. R will be more useful if it is not only a language
and environment for statistical computing and graphics, but also for
design of experiments. I am toying with the idea of adding to R some
modules of my Gendex DOE toolkit (http://designcomputing.net/gendex/). I
learn from an Rexpert that the first step for this exercise is to convert
my java code to C++.  As I do not know C++ (and life is short) please let
me know whether there is an alternative way without this conversion and
whether you can actively help me in this exercise.

Regards,
-- 
Nam-Ky Nguyen, Senior Lecturer
School of Mathematics, Statistics and Computer Science
University of New England, Armidale NSW 2351 Australia
nkn at turing.une.edu.au              Tel: +612 6773 2763
http://turing.une.edu.au/~nkn      Fax: +612 6773 3312

Please convert Word files into PDF files before sending them to me.
See http://www.gnu.org/philosophy/no-word-attachments.html



From mkalisiak at gmail.com  Thu Sep  1 01:20:22 2005
From: mkalisiak at gmail.com (Maciej Kalisiak)
Date: Wed, 31 Aug 2005 19:20:22 -0400
Subject: [R] label *on the side* in conditional lattice plots?
In-Reply-To: <4E9A692D8755DF478B56A2892388EE1F06B6B4@usctmx1118.merck.com>
References: <4E9A692D8755DF478B56A2892388EE1F06B6B4@usctmx1118.merck.com>
Message-ID: <78e6ba3105083116207ba03979@mail.gmail.com>

On 8/31/05, Wiener, Matthew <matthew_wiener at merck.com> wrote:
> I think you might be able to use the "horizontal" argument to lattice to
> rotate all your plots and squish them in the other dimension.  (Though I
> don't know whether you consider that a good outcome ...)

I take it you mean to switch the layout so that the conditioned
bwplots are side by side, narrow but tall, with horizontal=TRUE, and
then incorporate into the paper by first rotating 90 degrees.  But is
there a way to rotate the axis labels then?  Having the z-label
written sideways is probably OK, but having the x- and y-axis labels
also sideways is just too much.

-- 
Maciej Kalisiak
<mkalisiak at gmail.com>
http://www.dgp.toronto.edu/~mac



From c_naber at yahoo.com.br  Thu Sep  1 01:25:19 2005
From: c_naber at yahoo.com.br (Caio Lucidius Naberezny Azevedo)
Date: Wed, 31 Aug 2005 23:25:19 +0000 (GMT)
Subject: [R] Block-diagonal matrix
Message-ID: <20050831232519.84816.qmail@web34002.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050831/eae2e65f/attachment.pl

From mkalisiak at gmail.com  Thu Sep  1 01:42:48 2005
From: mkalisiak at gmail.com (Maciej Kalisiak)
Date: Wed, 31 Aug 2005 19:42:48 -0400
Subject: [R] label *on the side* in conditional lattice plots?
In-Reply-To: <78e6ba3105083116207ba03979@mail.gmail.com>
References: <4E9A692D8755DF478B56A2892388EE1F06B6B4@usctmx1118.merck.com>
	<78e6ba3105083116207ba03979@mail.gmail.com>
Message-ID: <78e6ba31050831164231336ccc@mail.gmail.com>

On 8/31/05, Maciej Kalisiak <mkalisiak at gmail.com> wrote:
> I take it you mean to switch the layout so that the conditioned
> bwplots are side by side, narrow but tall, with horizontal=TRUE, and

Oops, I obviously meant horizontal=FALSE...

-- 
Maciej Kalisiak
<mkalisiak at gmail.com>
http://www.dgp.toronto.edu/~mac



From murdoch at stats.uwo.ca  Thu Sep  1 03:27:08 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 31 Aug 2005 21:27:08 -0400
Subject: [R] R environment
In-Reply-To: <Pine.SGI.4.40.0508311847100.1277557-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0508311847100.1277557-100000@origin.chass.utoronto.ca>
Message-ID: <431658EC.9040806@stats.uwo.ca>

Jean Eid wrote:
> This is probably a weird question but I need to know if there is a way...
> 
> I run an R batch job without saving the variables at each step to the
> disk.   Is there a way to invoke another session of R and link it to the
> same environment for read only.
> 
> The problem is that I am running optim with every step getting the
> parameters into the global env using <<- However, I forgot to issue a
> save(list=ls(),...) right after so I can load and see how the parameters
> are changing. It's been couple of days and it is still running so I am
> hoping that I can invoke another session of R and link it to the
> environment of the batch session. Does this sound totally ridiculous ?
> 
> it is a debian machine with R 2.1.1

If you happened to have compiled R with debug information, you might be 
able to use gdb or another debugger to examine variables in the running 
process, but you probably didn't, and it's probably easier to kill the 
job, fix it, and start it again, than it would be to learn how to see 
the active variables using gdb.

Duncan Murdoch



From mtorabi at math.carleton.ca  Thu Sep  1 03:51:13 2005
From: mtorabi at math.carleton.ca (Mahmoud Torabi)
Date: Wed, 31 Aug 2005 21:51:13 -0400 (EDT)
Subject: [R] Question
Message-ID: <Pine.GSO.4.05.10508312134210.21903-100000@gauss>

Dear Sir/Madam

I would be pleased if anybody can help me. I'm using linear mixed model
(lme) function.I'm doing some simulation in my research and need to be
assigned variance components values during of my program. Specifically,
when I use lme function, I can get some information by use summary() and I
can assign some valuse like variance of fixed parameters and variance of
random error
term by using for example  varFix and sigma.But I don't know how I can
assign for variance of random effect.
I know in SPLUS we have command var.ran, how about R ?

Thanks alot.
M.Torabi



From jiso at ucsd.edu  Thu Sep  1 04:03:24 2005
From: jiso at ucsd.edu (Jia-Shing So)
Date: Wed, 31 Aug 2005 19:03:24 -0700
Subject: [R] Linux Standalone Server Suggestions for R
Message-ID: <585D1920-C563-4483-9CEC-6F863A6437BB@ucsd.edu>

Hi All,

My group is  looking for any suggestions on what to purchase to  
achieve the most powerful number crunching system that $50k can buy.   
The main application that will be used is R so input on what hardware  
benefits R most will be appreciated.  The requirements are that it be  
a single standalone server (i.e. not a cluster solution), and it that  
must be able to run unix/linux.  If anyone has any experience/ 
suggestions regarding the following questions that would also be  
greatly appreciated.

AMD vs Intel chips, especially 64-bit versions of the two?
Using Itanium/Opterons and if so how much of a performance boost did  
you achieve vs other 64-bit chip sets?
Also, does anyone know if there is an upper thresh hold on much  
memory R can use?

Thanks in advance for any help and suggestions,

Jia-Shing So
Programmer Analyst
Biostatistics and Bioinformatics Lab
University of California, San Diego



From lisas at salford-systems.com  Thu Sep  1 04:16:19 2005
From: lisas at salford-systems.com (Lisa Solomon)
Date: Wed, 31 Aug 2005 19:16:19 -0700
Subject: [R] Data Mining Conference Program Announced
Message-ID: <43166473.8010709@salford-systems.com>

DATA MINING 2006 CONFERENCE sponsored by Salford Systems
San Diego, California: March 29 - March 31, 2006

The Conference will offer Real-World Case Study Presentations including 
Cutting Edge Topics:  Crime Prevention * Anti-Terrorism * Gambling * 
Sports * Video Games * Food & Water Science * Diet Research * 
Standardized Testing

There will be special courses available for attendees who are new to Data Mining covering 

        CART(R) Decision Trees
        MARS(R) Modern regression analysis
        TreeNet(tm) Jerome Friedman's Multiple Additive Trees
        Random Forests(tm) Leo Breiman's Tree Ensembles

See how Data Mining is Used for Business, Biomedial, and Environmental applications. Find out why previous attendees rate our conferences the most informative in the industry.

If you would like more information, please click here:
http://www.salforddatamining.com/program-sd.htm

To be placed on the conference mailing list, please click here:
http://www.salforddatamining.com/2006InfoRequest.php

Best regards,
Lisa Solomon
Ph: (619)543-8880



From whit at twinfieldscapital.com  Thu Sep  1 04:27:15 2005
From: whit at twinfieldscapital.com (Whit Armstrong)
Date: Wed, 31 Aug 2005 22:27:15 -0400
Subject: [R] Linux Standalone Server Suggestions for R
Message-ID: <726FC6DD09DE1046AF81B499D70C3BCE2C84FC@twinfields02.CORP.TWINFIELDSCAPITAL.COM>

you can test out a live PPC 64 bit system here:
http://www.openpowerproject.org/us/signup.php

I have successfully built R in my home dir and compiled a few packages for it.

Here are the machines available:

			Universit??t Augsburg  	Peking University  	
Server 		OpenPower 720 		OpenPower 720 	
Equipment 		4-Way POWER5 		4-Way POWER5 	
Memory 		8GB		 		16GB 	
Installation 	Debian	 		SUSE Enterprise 9 	 


I think the 16GB solution is approaching or beyond your budget.



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jia-Shing So
Sent: Wednesday, August 31, 2005 10:03 PM
To: r-help at stat.math.ethz.ch
Cc: Phuoc Hong
Subject: [R] Linux Standalone Server Suggestions for R

Hi All,

My group is  looking for any suggestions on what to purchase to  
achieve the most powerful number crunching system that $50k can buy.   
The main application that will be used is R so input on what hardware benefits R most will be appreciated.  The requirements are that it be a single standalone server (i.e. not a cluster solution), and it that must be able to run unix/linux.  If anyone has any experience/ suggestions regarding the following questions that would also be greatly appreciated.

AMD vs Intel chips, especially 64-bit versions of the two?
Using Itanium/Opterons and if so how much of a performance boost did you achieve vs other 64-bit chip sets?
Also, does anyone know if there is an upper thresh hold on much memory R can use?

Thanks in advance for any help and suggestions,

Jia-Shing So
Programmer Analyst
Biostatistics and Bioinformatics Lab
University of California, San Diego

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Thu Sep  1 04:32:13 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 31 Aug 2005 19:32:13 -0700
Subject: [R] basic anova and t-test question
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3FE@CRBSMXSUSR04>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3FE@CRBSMXSUSR04>
Message-ID: <4316682D.2070400@pdf.com>

	  Does the following answer your question:

 > set.seed(1)
 > z0 <- rnorm(100)
 > p.z <- 2*pnorm(-abs(z0))
 > sum(p.z<0.05)
[1] 5
 > pchisq(sum(z0^2), 100, lower=FALSE)
[1] 0.917285

	  Some of the 100 (in this case) normal random deviates seem 
statistically significant, even though the ensemble is not.

	  spencer graves

Arne.Muller at sanofi-aventis.com wrote:

> Hello,
> 
> I'm posting this to receive some comments/hints about a rather statistical than R-technical question ... .
> 
> In an anova of a lme factor SSPos11 shows up non-significant, 
but in the t-test of the summay 2 of the 4 levels (one for
constrast) are significant. See below for some truncated output.
> 
> I realize that the two test are different (F-test/t-test), 
but I'm looking for for a "meaning". Maye you have a schenario
that explains how these differences can be created and how you'd
go ahead and analyse it further.
> 
> When I use SSPos11 as te only fixed effect, it does it is not 
significant in either anova nor t-test, and a boxplot of the
factor shows that the levels are all quite similar (similar
variance and mean). Might the effect I observe be linked to
an unbalance design in the multifactorial model?
> 
> 	thanks a lot for your help,
> 	+kind regards,
> 
> 	Arne
> 
> 
>>anova(fit)
> 
>              numDF denDF  F-value p-value
> (Intercept)      1   540 323.4442  <.0001
> SSPos1           3   540  15.1206  <.0001
> ...
> SSPos11          3   540   1.1902  0.3128
> ...
> 
> 
>>summary(fit)
> 
> Linear mixed-effects model fit by REML
>  Data: d.orig 
>        AIC      BIC    logLik
>   1007.066 1153.168 -469.5329
> 
> Random effects:
>  Formula: ~1 | Method
>         (Intercept)  Residual
> StdDev:   0.4000478 0.4943817
> 
> Fixed effects: log(value + 7.5) ~ SSPos1 + SSPos2 + SSPos6 + SSPos7 + SSPos10 + SSPos11 + SSPos13 + SSPos14 + SSPos18 + SSPos19 +      
>                   Value  Std.Error  DF   t-value p-value
> (Intercept)   2.8621811 0.23125065 540 12.376964  0.0000
> SSPos1C      -0.1647937 0.06293993 540 -2.618269  0.0091
> SSPos1G      -0.3448095 0.05922479 540 -5.822047  0.0000
> SSPos1T       0.1083988 0.06087095 540  1.780797  0.0755
> ...
> SSPos11C     -0.1540292 0.06171635 540 -2.495761  0.0129
> SSPos11G     -0.1428980 0.05993122 540 -2.384368  0.0175
> SSPos11T     -0.0039434 0.06133920 540 -0.064289  0.9488
> ...
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Peter.Watkins at foodscience.afisc.csiro.au  Thu Sep  1 06:58:14 2005
From: Peter.Watkins at foodscience.afisc.csiro.au (Peter.Watkins@foodscience.afisc.csiro.au)
Date: Thu, 1 Sep 2005 14:58:14 +1000
Subject: [R] Dataset size in R
Message-ID: <CB3D0020D0BA674C99B5B7E751D1A99903AC89@exvicn1-mel.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050901/9fedd9e5/attachment.pl

From hb at maths.lth.se  Thu Sep  1 08:45:17 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 01 Sep 2005 08:45:17 +0200
Subject: [R] R environment
In-Reply-To: <431658EC.9040806@stats.uwo.ca>
References: <Pine.SGI.4.40.0508311847100.1277557-100000@origin.chass.utoronto.ca>
	<431658EC.9040806@stats.uwo.ca>
Message-ID: <4316A37D.8080402@maths.lth.se>

Unfortunately Duncan's suggestion to restart is probably the only way to 
go here.  I've done similar thing myself too.  What I've learned was 
that it is clever to include a so called hot-patch mechanism in your 
code, which will load R source code found in a certain directory, say, 
"hot/", once in a while (easy if you have do interations) and at the end 
of the batch code. Here is the idea (typed out of my head):

patchCode <- function(path="hot", pattern="[.]R$", removeAfter=FALSE, ...) {
   files <- list.files(pattern=pattern, path=path, full.names=TRUE);
   for (file in files) {
     tryCatch({
       # You don't want you patch code to kill you batch job,
       # if you for instance have a typo.
       cat("Hot-path file: ", file, "\n");
       source(file);
       # Remove the file afterwards? Especially useful if you only
       # want to redefine functions, and patch once.  This way you
       # can also see what files has successfully been source():ed.
       if (removeAfter)
         file.remove(file);
     }, error = function(ex) {
       cat("Ignored error when sourcing:\n");
       print(ex);
     })
   }
} # patchCode()

Then in your batch code something like this:

while (!converged) {
   # Patch the code every iteration.
   patchCode(path="hot/", removeAfter=TRUE)
   # The rest of your code here
}

# Put the code you want to call at the end, in a directory
# of its own.
patchCode(path="hot/onFinally/", removeAfter=TRUE)
# End of your batch code

Cheers

Henrik


Duncan Murdoch wrote:
> Jean Eid wrote:
> 
>>This is probably a weird question but I need to know if there is a way...
>>
>>I run an R batch job without saving the variables at each step to the
>>disk.   Is there a way to invoke another session of R and link it to the
>>same environment for read only.
>>
>>The problem is that I am running optim with every step getting the
>>parameters into the global env using <<- However, I forgot to issue a
>>save(list=ls(),...) right after so I can load and see how the parameters
>>are changing. It's been couple of days and it is still running so I am
>>hoping that I can invoke another session of R and link it to the
>>environment of the batch session. Does this sound totally ridiculous ?
>>
>>it is a debian machine with R 2.1.1
> 
> 
> If you happened to have compiled R with debug information, you might be 
> able to use gdb or another debugger to examine variables in the running 
> process, but you probably didn't, and it's probably easier to kill the 
> job, fix it, and start it again, than it would be to learn how to see 
> the active variables using gdb.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From r.hankin at noc.soton.ac.uk  Thu Sep  1 08:57:32 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 1 Sep 2005 07:57:32 +0100
Subject: [R] Block-diagonal matrix
In-Reply-To: <20050831232519.84816.qmail@web34002.mail.mud.yahoo.com>
References: <20050831232519.84816.qmail@web34002.mail.mud.yahoo.com>
Message-ID: <F97BD721-D63F-4838-832A-8FA369900BCF@soc.soton.ac.uk>

Hi

adiag() from the magic package does what you want:

 > library(magic)
Loading required package: abind
 > a <- matrix(1,2,2)
 > b <- matrix(6,2,3)
 > adiag(a,b)
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    1    0    0    0
[2,]    1    1    0    0    0
[3,]    0    0    6    6    6
[4,]    0    0    6    6    6
 >


HTH

rksh



On 1 Sep 2005, at 00:25, Caio Lucidius Naberezny Azevedo wrote:

> Dear R-users,
>
> Does anybody know how to construct a block-diagonal matrix (with  
> the blocks being different matrixs, concerning the dimension and  
> the values), without use loops ?
>
>
> Thanks all,
>
> Caio
>
>
>
> ---------------------------------
>
>     [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From Vidar.Hjellvik at imr.no  Thu Sep  1 10:01:16 2005
From: Vidar.Hjellvik at imr.no (Hjellvik Vidar)
Date: Thu, 1 Sep 2005 10:01:16 +0200
Subject: [R] source(file) => file becomes readonly
Message-ID: <0D54094F69391A438650C6151B0A33E46FD334@post2.imr.no>

Hello,
when I work in R, I write code in a text file that I run with the "source(filename)" command. In R2.1.1 the file is read-only while the source command is executed. This was not the case in R2.0.1. Is this a bug-fix or is it possible not to have the file read-only when executed?
Best regards
Vidar



From kar at itga.com.au  Thu Sep  1 10:25:14 2005
From: kar at itga.com.au (Kylie-Anne Richards)
Date: Thu, 1 Sep 2005 18:25:14 +1000
Subject: [R] Robust Regression - LTS
Message-ID: <20050901082535.F148844FC0@melmail.itga.com.au>

Hi,

I am using robust regression, i.e. model.robust<-ltsreg(MXD~ORR,data=DATA).
My question:- is there any way to determine the Robust Multiple R-Squared
(as returned in the summary output in splus)? I found an equivalent model in
the rrcov package which included R-square, residuals etc in it's list of
components, but when I used this package the only results returned were
equivalent to the LTS reg in the MASS package, which obviously indicates
that the summary method does not work for this class of models. 

If required:

##The output for the LTS reg (MASS) using print and summary

Call:
lqs.formula(formula = MXD ~ ORR, data = DATA, method = "lts")

Coefficients:
(Intercept)        ORR
  7.578e+08    2.533e+01

Scale estimates 1.333e+09 1.303e+09

              Length Class      Mode
crit             1   -none-     numeric
sing             1   -none-     character
coefficients     2   -none-     numeric
bestone          2   -none-     numeric
fitted.values 4899   -none-     numeric
residuals     4899   -none-     numeric
scale            2   -none-     numeric
terms            3   terms      call
call             4   -none-     call
xlevels          0   -none-     list
model            2   data.frame list



## The output for the LTS reg (rrcov) using print and summary
 
Coefficients:
Intercept      ORR
1.178e+09  2.387e+01

Scale estimate 1.722e+09

                 Length Class  Mode
best             2451   -none- numeric
raw.coefficients    2   -none- numeric
alpha               1   -none- numeric
quan                1   -none- numeric
raw.scale           1   -none- numeric
raw.resid        4899   -none- numeric
coefficients        2   -none- numeric
scale               1   -none- numeric
resid            4899   -none- numeric
lts.wt           4899   -none- numeric
crit                1   -none- numeric
rsquared            1   -none- numeric
residuals        4899   -none- numeric
intercept           1   -none- logical
method              1   -none- character
RD               4899   -none- numeric
X                9798   -none- numeric
Y                4899   -none- numeric
fitted.values    4899   -none- numeric

## The output for the LTS reg (SPLUS) using print and summary ****{What I am
wanting to achieve in R}****

> model.robust<-ltsreg(MXD~ORR,data=DATA,na.action=na.exclude)
> print(model.robust)
Method:
Least Trimmed Squares Robust Regression. 

Call:
ltsreg(formula = MXD ~ ORR, data = DATA, na.action = na.exclude)

Coefficients:
     Intercept         ORR 
 1.465502e+009 2.325200e+001

Scale estimate of residuals:  1639000000 

Total number of observations:  4899 

Number of observations that determine the LTS estimate:  4409 
> summary(model.robust)
Method:
[1] "Least Trimmed Squares Robust Regression."

Call:
ltsreg(formula = MXD ~ ORR, data = DATA, na.action = na.exclude)

Coefficients:
     Intercept         ORR 
 1.465502e+009 2.325200e+001

Scale estimate of residuals: 1639000000 

Robust Multiple R-Squared: 0.4733 

Total number of observations:  4899 

Number of observations that determine the LTS estimate:  4409 

Residuals:
          Min.       1st Qu.        Median       3rd Qu.          Max. 
 -228135629879   -1032103953    -231375637    1234533512   55539148696

Weights:
   0    1 
 588 4311




Thanks very much for any help you can offer. 

Kylie-Anne Richards



From otoomet at ut.ee  Thu Sep  1 10:32:04 2005
From: otoomet at ut.ee (Ott Toomet)
Date: Thu, 1 Sep 2005 11:32:04 +0300
Subject: [R] "best" c++ matrix library?
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD5034CA204@us-arlington-0668.mail.saic.com>
	(JAROSLAW.W.TUSZYNSKI@saic.com)
References: <CA0BCF3BED56294AB91E3AD74B849FD5034CA204@us-arlington-0668.mail.saic.com>
Message-ID: <200509010832.j818W4db012355@hugo.obs.ee>

Thanks for everyone who replied to my question.

I tried newmat myself, seems to be working well.  What I am interested in
is something like

* Fast element-wise operations.  You know, it may be slow in R.

* (Some) control over memory allocation.  I would like to specify when
  the matrix should not be copied.

* Compatibility with R.  At least the way how the matrix data is
  represented in memory should be the same as R has.  Otherwise, the
  data interchange may become slow and memory-consuming.

* And, of course, the basic matrix operations like multiplication,
  inverting, eigenvalues etc....

I am using linux/gcc if that matters.

Best,
Ott
 | From: "Tuszynski, Jaroslaw W." <JAROSLAW.W.TUSZYNSKI at saic.com>
 | Date: Wed, 31 Aug 2005 12:59:04 -0400
 | 
 | What kind of matrix operations do you need? 
 | 
 | Jarek
 | 
 | -----Original Message-----
 | From: r-help-bounces at stat.math.ethz.ch
 | [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ott Toomet
 | Sent: Wednesday, August 31, 2005 12:54 PM
 | To: r-help at stat.math.ethz.ch
 | Subject: [R] "best" c++ matrix library?
 | 
 | Hi folks,
 | 
 | I am planning to write some more time-consuming matrix manipulations in c++.
 | What is the experience with the existing c++ matrix libraries?  Do you have
 | some recommendations?  Are some libraries more compatible with R than the
 | others?
 | 
 | All suggestions welcome!
 | 
 | Best,
 | Ott



From Vanessa3 at fastermail.com  Thu Sep  1 11:48:05 2005
From: Vanessa3 at fastermail.com (Clement)
Date: Thu, 01 Sep 2005 13:48:05 +0400
Subject: [R] News from your Bank
Message-ID: <55OX9G61P00W71PC9T2O91C44@autecologic%.uymail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050901/c0e4cbbb/attachment.pl

From francoisromain at free.fr  Thu Sep  1 11:19:59 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 01 Sep 2005 11:19:59 +0200
Subject: [R] "best" c++ matrix library?
In-Reply-To: <200509010832.j818W4db012355@hugo.obs.ee>
References: <CA0BCF3BED56294AB91E3AD74B849FD5034CA204@us-arlington-0668.mail.saic.com>
	<200509010832.j818W4db012355@hugo.obs.ee>
Message-ID: <4316C7BF.2000809@free.fr>

Le 01.09.2005 10:32, Ott Toomet a ??crit :

>Thanks for everyone who replied to my question.
>
>I tried newmat myself, seems to be working well.  What I am interested in
>is something like
>
>* Fast element-wise operations.  You know, it may be slow in R.
>
>* (Some) control over memory allocation.  I would like to specify when
>  the matrix should not be copied.
>
>* Compatibility with R.  At least the way how the matrix data is
>  represented in memory should be the same as R has.  Otherwise, the
>  data interchange may become slow and memory-consuming.
>
>* And, of course, the basic matrix operations like multiplication,
>  inverting, eigenvalues etc....
>
>I am using linux/gcc if that matters.
>
>Best,
>Ott
> | From: "Tuszynski, Jaroslaw W." <JAROSLAW.W.TUSZYNSKI at saic.com>
> | Date: Wed, 31 Aug 2005 12:59:04 -0400
> | 
> | What kind of matrix operations do you need? 
> | 
> | Jarek
> | 
> | -----Original Message-----
> | From: r-help-bounces at stat.math.ethz.ch
> | [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ott Toomet
> | Sent: Wednesday, August 31, 2005 12:54 PM
> | To: r-help at stat.math.ethz.ch
> | Subject: [R] "best" c++ matrix library?
> | 
> | Hi folks,
> | 
> | I am planning to write some more time-consuming matrix manipulations in c++.
> | What is the experience with the existing c++ matrix libraries?  Do you have
> | some recommendations?  Are some libraries more compatible with R than the
> | others?
> | 
> | All suggestions welcome!
> | 
> | Best,
> | Ott
>  
>
Hello,

CPPLAPACK : <http://cpplapack.sourceforge.net/> is a c++ wrapper for 
lapack routines.
It is not so complicated to use once you have blas and lapack installed.
I think it is much faster than newmat.

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From admin at biostatistic.de  Thu Sep  1 12:32:45 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Thu, 01 Sep 2005 12:32:45 +0200
Subject: [R] standard deviation in barplots
Message-ID: <4316D8CD.1000900@biostatistic.de>

Is there any function to plot the standard deviation with the barplots


    _ _
      |     Deviation
----|---
|     -    |
|          |
|          |   Barplot
|          |
|          |
|          |
--------

with regards
Knut Krueger



From sdavis2 at mail.nih.gov  Thu Sep  1 12:39:59 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 01 Sep 2005 06:39:59 -0400
Subject: [R] standard deviation in barplots
In-Reply-To: <4316D8CD.1000900@biostatistic.de>
Message-ID: <BF3C52BF.D839%sdavis2@mail.nih.gov>

On 9/1/05 6:32 AM, "Knut Krueger" <admin at biostatistic.de> wrote:

> Is there any function to plot the standard deviation with the barplots
> 
> 
>   _ _
>     |     Deviation
> ----|---
> |     -    |
> |          |
> |          |   Barplot
> |          |
> |          |
> |          |
> --------
> 

I think barplot2 in the gregmisc (gplots) bundle will do that.

Sean



From maechler at stat.math.ethz.ch  Thu Sep  1 12:43:22 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 1 Sep 2005 12:43:22 +0200
Subject: [R] Digest reading is tedious
In-Reply-To: <42FB9390.7030005@stanford.edu>
References: <03091f4b0b5415eeb2d9be3e5e070ceb@stanford.edu>
	<17145.5735.426400.446812@stat.math.ethz.ch>
	<42FB9390.7030005@stanford.edu>
Message-ID: <17174.56138.693038.290299@stat.math.ethz.ch>

Hi Trevor,

please excuse my late reply; your e-mail was sent when I started
travelling (to Seattle and the DSC and Bioconductor workshops there).

Also, I hope you don't mind if I follow this up on R-help, since
there, the thread started and this does related to it.

>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
>>>>>     on Thu, 11 Aug 2005 11:06:08 -0700 writes:

    Trevor> Dear Martin I may still be doing something wrong.
    Trevor> Here is what I see now in Mozilla Firefox (attached
    Trevor> jpeg file of screen dump)

isn't that "Thunderbird" ?
"Firefox" the web browser, "Thunderbird" the mail client?

    Trevor> This is still not the convenient layout I was
    Trevor> envisaging.  Ideally one would have the usual list
    Trevor> of contributions, with authors, and they would be
    Trevor> clickable

I've now looked at your screen shot and agree this is really not
satisfactory.
{In my good old Emacs VM mail reader things are different: all
 is nice with a virtual mail folder of individual messages}

Hmm, I've checked :
The MIME type used for the whole digest is
         'Content-Type: multipart/digest'
and then 'Content-Type: message/rfc822'
for each individual message.

Since there *are* mail clients that nicely support these MIME
types, I wonder why thunderbird does not. 
I'd say this is clearly a deficiency in thunderbird;
or is it something you could optionally configure to it?

What are other readers' experiences with mailman mailing lists
in digest mode -- using "MIME" type delivery?

Regards,
Martin



From jeaneid at chass.utoronto.ca  Thu Sep  1 13:10:36 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 1 Sep 2005 07:10:36 -0400
Subject: [R] R environment
In-Reply-To: <4316A37D.8080402@maths.lth.se>
Message-ID: <Pine.SGI.4.40.0509010710030.43867-100000@origin.chass.utoronto.ca>

Thank you all for the answers. I will look into both methods for doing so.

Jean

On Thu, 1 Sep 2005, Henrik Bengtsson wrote:

> Unfortunately Duncan's suggestion to restart is probably the only way to
> go here.  I've done similar thing myself too.  What I've learned was
> that it is clever to include a so called hot-patch mechanism in your
> code, which will load R source code found in a certain directory, say,
> "hot/", once in a while (easy if you have do interations) and at the end
> of the batch code. Here is the idea (typed out of my head):
>
> patchCode <- function(path="hot", pattern="[.]R$", removeAfter=FALSE, ...) {
>    files <- list.files(pattern=pattern, path=path, full.names=TRUE);
>    for (file in files) {
>      tryCatch({
>        # You don't want you patch code to kill you batch job,
>        # if you for instance have a typo.
>        cat("Hot-path file: ", file, "\n");
>        source(file);
>        # Remove the file afterwards? Especially useful if you only
>        # want to redefine functions, and patch once.  This way you
>        # can also see what files has successfully been source():ed.
>        if (removeAfter)
>          file.remove(file);
>      }, error = function(ex) {
>        cat("Ignored error when sourcing:\n");
>        print(ex);
>      })
>    }
> } # patchCode()
>
> Then in your batch code something like this:
>
> while (!converged) {
>    # Patch the code every iteration.
>    patchCode(path="hot/", removeAfter=TRUE)
>    # The rest of your code here
> }
>
> # Put the code you want to call at the end, in a directory
> # of its own.
> patchCode(path="hot/onFinally/", removeAfter=TRUE)
> # End of your batch code
>
> Cheers
>
> Henrik
>
>
> Duncan Murdoch wrote:
> > Jean Eid wrote:
> >
> >>This is probably a weird question but I need to know if there is a way...
> >>
> >>I run an R batch job without saving the variables at each step to the
> >>disk.   Is there a way to invoke another session of R and link it to the
> >>same environment for read only.
> >>
> >>The problem is that I am running optim with every step getting the
> >>parameters into the global env using <<- However, I forgot to issue a
> >>save(list=ls(),...) right after so I can load and see how the parameters
> >>are changing. It's been couple of days and it is still running so I am
> >>hoping that I can invoke another session of R and link it to the
> >>environment of the batch session. Does this sound totally ridiculous ?
> >>
> >>it is a debian machine with R 2.1.1
> >
> >
> > If you happened to have compiled R with debug information, you might be
> > able to use gdb or another debugger to examine variables in the running
> > process, but you probably didn't, and it's probably easier to kill the
> > job, fix it, and start it again, than it would be to learn how to see
> > the active variables using gdb.
> >
> > Duncan Murdoch
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
>
>



From maechler at stat.math.ethz.ch  Thu Sep  1 13:30:12 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 1 Sep 2005 13:30:12 +0200
Subject: [R] "best" c++ matrix library?
In-Reply-To: <4316C7BF.2000809@free.fr>
References: <CA0BCF3BED56294AB91E3AD74B849FD5034CA204@us-arlington-0668.mail.saic.com>
	<200509010832.j818W4db012355@hugo.obs.ee>
	<4316C7BF.2000809@free.fr>
Message-ID: <17174.58948.861664.996767@stat.math.ethz.ch>

>>>>> "Romain" == Romain Francois <francoisromain at free.fr>
>>>>>     on Thu, 01 Sep 2005 11:19:59 +0200 writes:

    Romain> Le 01.09.2005 10:32, Ott Toomet a ??crit :
    >> Thanks for everyone who replied to my question.
    >> 
    >> I tried newmat myself, seems to be working well.  What I
    >> am interested in is something like
    >> 
    >> * Fast element-wise operations.  You know, it may be slow
    >> in R.
    >> 
    >> * (Some) control over memory allocation.  I would like to
    >> specify when the matrix should not be copied.
    >> 
    >> * Compatibility with R.  At least the way how the matrix
    >> data is represented in memory should be the same as R
    >> has.  Otherwise, the data interchange may become slow and
    >> memory-consuming.
    >> 
    >> * And, of course, the basic matrix operations like
    >> multiplication, inverting, eigenvalues etc....
    >> 
    >> I am using linux/gcc if that matters.
    >> 
    >> Best, Ott | From: "Tuszynski, Jaroslaw W."
    >> <JAROSLAW.W.TUSZYNSKI at saic.com> | Date: Wed, 31 Aug 2005
    >> 12:59:04 -0400
    >> | 
    >> | What kind of matrix operations do you need?
    >> | 
    >> | Jarek
    >> | 
    >> | -----Original Message----- | From:
    >> r-help-bounces at stat.math.ethz.ch |
    >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
    >> Ott Toomet | Sent: Wednesday, August 31, 2005 12:54 PM |
    >> To: r-help at stat.math.ethz.ch | Subject: [R] "best" c++
    >> matrix library?
    >> | 
    >> | Hi folks,
    >> | 
    >> | I am planning to write some more time-consuming matrix
    >> manipulations in c++.  | What is the experience with the
    >> existing c++ matrix libraries?  Do you have | some
    >> recommendations?  Are some libraries more compatible with
    >> R than the | others?
    >> | 
    >> | All suggestions welcome!
    >> | 
    >> | Best, | Ott
    >> 
    >> 
    Romain> Hello,

    Romain> CPPLAPACK : <http://cpplapack.sourceforge.net/> is a
    Romain> c++ wrapper for lapack routines.  It is not so
    Romain> complicated to use once you have blas and lapack
    Romain> installed.  I think it is much faster than newmat.

LAPACK is ``state of the art'' of numerical algebra.
Don't use anything else if you don't have to.

It's definitely *much* more ``compatible with R''
since we base almost all our matrix computations on LAPACK.
GNU octave and commercial Matlab are also entirely based on
LAPACK for the matrix computations.

I have no idea about the "CPP" (= c++) part in CPPLAPACK though.

The CRAN package 'Matrix' has several objectives, notably
*sparse* matrix computations.  But it also uses LAPACK for dense
matric computations, AFAIR also for a few things not available
(via LAPACK) in "core R".

Martin Maechler, ETH Zurich



From famille.tessier at wanadoo.fr  Thu Sep  1 13:35:50 2005
From: famille.tessier at wanadoo.fr (Laurent TESSIER)
Date: Thu,  1 Sep 2005 13:35:50 +0200 (CEST)
Subject: [R] ROracle under windows
Message-ID: <32981269.1125574550642.JavaMail.www@wwinf1305>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050901/d9e4ac36/attachment.pl

From zlatko.petrin at emg.umu.se  Thu Sep  1 13:51:42 2005
From: zlatko.petrin at emg.umu.se (Zlatko Petrin)
Date: Thu, 1 Sep 2005 13:51:42 +0200
Subject: [R] convex-polygon in scatter plot
Message-ID: <200509011151.j81Bpgxi002454@mail.umu.se>

Hello,

I am trying to draw a convex-polygon (envelope) connecting the outer points
(of a particular class) in a scatter plot

[which I want to do to illustrate the range of a particular type of samples
in an ordination plot].

Is there any function to plot this kind of polygon?

Kind regards
Zlatko Petrin



From detlef.steuer at hsu-hamburg.de  Thu Sep  1 13:58:55 2005
From: detlef.steuer at hsu-hamburg.de (Dr. Detlef Steuer)
Date: Thu, 1 Sep 2005 13:58:55 +0200
Subject: [R] convex-polygon in scatter plot
In-Reply-To: <200509011151.j81Bpgxi002454@mail.umu.se>
References: <200509011151.j81Bpgxi002454@mail.umu.se>
Message-ID: <20050901135855.4bca3c74.detlef.steuer@hsu-hamburg.de>

Hi,


ever thought about using the built-in help of R?

try

> example(chull)

cu

detlef

On Thu, 1 Sep 2005 13:51:42 +0200
"Zlatko Petrin" <zlatko.petrin at emg.umu.se> wrote:

> Hello,
> 
> I am trying to draw a convex-polygon (envelope) connecting the outer points
> (of a particular class) in a scatter plot
> 
> [which I want to do to illustrate the range of a particular type of samples
> in an ordination plot].
> 
> Is there any function to plot this kind of polygon?
> 
> Kind regards
> Zlatko Petrin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Christian.Prinoth at epsilonsgr.it  Thu Sep  1 14:00:33 2005
From: Christian.Prinoth at epsilonsgr.it (Christian Prinoth)
Date: Thu, 1 Sep 2005 14:00:33 +0200
Subject: [R] Newbie help on dim
Message-ID: <8D64D4652EB17048B874B0503309CFCA958FA3@epsilon2003.epsilonsgr.it>

Hi, if I do

Z<-rnorm(50)
Followed by
Dim(Z)
I get NULL. Is this correct? Shouldn't I get 50 instead?

TIA
Chris

DISCLAIMER:\ L'utilizzo non autorizzato del presente messagg...{{dropped}}



From dimitris.rizopoulos at med.kuleuven.be  Thu Sep  1 14:11:47 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 1 Sep 2005 14:11:47 +0200
Subject: [R] Newbie help on dim
References: <8D64D4652EB17048B874B0503309CFCA958FA3@epsilon2003.epsilonsgr.it>
Message-ID: <004701c5aeee$53bd94f0$0540210a@www.domain>

yes it is correct! look at ?dim() for more info. In this case you need 
length(), i.e.,

Z <- rnorm(50)
length(Z)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Christian Prinoth" <Christian.Prinoth at epsilonsgr.it>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, September 01, 2005 2:00 PM
Subject: [R] Newbie help on dim


> Hi, if I do
>
> Z<-rnorm(50)
> Followed by
> Dim(Z)
> I get NULL. Is this correct? Shouldn't I get 50 instead?
>
> TIA
> Chris
>
> DISCLAIMER:\ L'utilizzo non autorizzato del presente 
> messagg...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From abunn at whrc.org  Thu Sep  1 14:11:51 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 1 Sep 2005 08:11:51 -0400
Subject: [R] Newbie help on dim
In-Reply-To: <8D64D4652EB17048B874B0503309CFCA958FA3@epsilon2003.epsilonsgr.it>
Message-ID: <NEBBIPHDAMMOKDKPOFFIIELIDJAA.abunn@whrc.org>

Check out the "dim vs length for vectors" thread:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/50720.html

This thread goes through the bug-or-feature discussion which is always
entertaining from a socio-R perspective.

Also, note "Dim" with a capital D doesn't exist.

HTH, Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Christian Prinoth
> Sent: Thursday, September 01, 2005 8:01 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Newbie help on dim
>
>
> Hi, if I do
>
> Z<-rnorm(50)
> Followed by
> Dim(Z)
> I get NULL. Is this correct? Shouldn't I get 50 instead?
>
> TIA
> Chris
>
> DISCLAIMER:\ L'utilizzo non autorizzato del presente messagg...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Thu Sep  1 14:13:51 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 14:13:51 +0200
Subject: [R] ROracle under windows
In-Reply-To: <32981269.1125574550642.JavaMail.www@wwinf1305>
References: <32981269.1125574550642.JavaMail.www@wwinf1305>
Message-ID: <4316F07F.5040506@statistik.uni-dortmund.de>

Laurent TESSIER wrote:

> Hello,
> Iâ€™m currently working with Oracle under windows and Iâ€™d like to use the ROracle package which exists only for linux/unix. Is there any possibility to port this package under windows?


It is ported, but perhaps you need some additional files:

See
http://stat.bell-labs.com/RS-DBI/download/index.html
for the binary package provided by David James and read the comments.

Uwe Ligges


> 
> L. Tessier
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Sep  1 14:15:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 14:15:09 +0200
Subject: [R] Newbie help on dim
In-Reply-To: <8D64D4652EB17048B874B0503309CFCA958FA3@epsilon2003.epsilonsgr.it>
References: <8D64D4652EB17048B874B0503309CFCA958FA3@epsilon2003.epsilonsgr.it>
Message-ID: <4316F0CD.8040904@statistik.uni-dortmund.de>

Christian Prinoth wrote:

> Hi, if I do
> 
> Z<-rnorm(50)
> Followed by
> Dim(Z)
> I get NULL. Is this correct? Shouldn't I get 50 instead?


No, because Z has no dim attribute. ?dim tells you:

"For an array (and hence in particular, for a matrix) dim retrieves the 
dim attribute of the object. It is NULL or a vector of mode integer."

> TIA
> Chris
> 
> DISCLAIMER:\ L'utilizzo non autorizzato del presente messagg...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Yes, PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


Uwe Ligges



From ligges at statistik.uni-dortmund.de  Thu Sep  1 14:21:23 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 14:21:23 +0200
Subject: [R] source(file) => file becomes readonly
In-Reply-To: <0D54094F69391A438650C6151B0A33E46FD334@post2.imr.no>
References: <0D54094F69391A438650C6151B0A33E46FD334@post2.imr.no>
Message-ID: <4316F243.6010102@statistik.uni-dortmund.de>

Hjellvik Vidar wrote:

> Hello,
> when I work in R, I write code in a text file that I run with the "source(filename)" command. In R2.1.1 the file is read-only while the source command is executed. This was not the case in R2.0.1. Is this a bug-fix or is it possible not to have the file read-only when executed?

So you want to modify a file *while* it is executed?
Sounds dangerous to me, you may want to explain further...

BTW: Which OS are we talking about?

Uwe Ligges


> Best regards
> Vidar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Thu Sep  1 14:25:14 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 01 Sep 2005 08:25:14 -0400
Subject: [R] Newbie help on dim
In-Reply-To: <4316F0CD.8040904@statistik.uni-dortmund.de>
References: <8D64D4652EB17048B874B0503309CFCA958FA3@epsilon2003.epsilonsgr.it>
	<4316F0CD.8040904@statistik.uni-dortmund.de>
Message-ID: <4316F32A.4040001@optonline.net>

Should the Value: section of the help page read:

For an array (and hence in particular, for a matrix) dim retrieves the
dim attribute of the object. It is NULL for a vector of mode integer.

NOTE: for not or in the second sentence.

Uwe Ligges wrote:
> Christian Prinoth wrote:
> 
> 
>>Hi, if I do
>>
>>Z<-rnorm(50)
>>Followed by
>>Dim(Z)
>>I get NULL. Is this correct? Shouldn't I get 50 instead?
> 
> 
> 
> No, because Z has no dim attribute. ?dim tells you:
> 
> "For an array (and hence in particular, for a matrix) dim retrieves the 
> dim attribute of the object. It is NULL or a vector of mode integer."
> 
> 
>>TIA
>>Chris
>>
>>DISCLAIMER:\ L'utilizzo non autorizzato del presente messagg...{{dropped}}
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> Yes, PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ligges at statistik.uni-dortmund.de  Thu Sep  1 14:26:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 14:26:59 +0200
Subject: [R] Dataset size in R
In-Reply-To: <CB3D0020D0BA674C99B5B7E751D1A99903AC89@exvicn1-mel.nexus.csiro.au>
References: <CB3D0020D0BA674C99B5B7E751D1A99903AC89@exvicn1-mel.nexus.csiro.au>
Message-ID: <4316F393.3060201@statistik.uni-dortmund.de>

Peter.Watkins at foodscience.afisc.csiro.au wrote:

> Dear listers,
> 
>  
> 
> I would like to know whether a maximum size is set for data sets in R.


Well, in theory, there are some limits re. size of some integers, but 
you are far away, as far as I understand.

In practice, you will somewhere hit the limit of available memory in 
your machine.


> I'm planning on analysing data sets which have 330000 points for a
> sample. The response is from GCxGC output.


I don't understand this point:
How many observations and how many variables?


> Also, with data analysis, is there a time-cost saving by performing the
> analysis in Linux as against doing it in Windows XP?

Depends on the problem. For really huge data, you might want to use a 
"64bit machine", 64bit adressing for a single R process is (yet) only 
supported on Linux. If memory does not matter, Linux is sometimes a bit 
faster, but it depends also on other facts like using CPU optimized 
linear algebra systems such as ATLAS or Goto's BLAS.

Uwe Ligges



>  
> 
> Many thanks, Peter
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From t.h.amundsen at usit.uio.no  Thu Sep  1 14:26:31 2005
From: t.h.amundsen at usit.uio.no (Trond Hasle Amundsen)
Date: Thu, 01 Sep 2005 14:26:31 +0200
Subject: [R] Using non-gcc compilers with install.packages()
Message-ID: <15tirxl0ya0.fsf@tux.uio.no>


Hi,

I have built R 2.1.1 on Solaris and OSF1, and used the operating
system's native compiler set. This worked fine. However, when I try
using install.packages() it insists on using g77 etc., which doesn't
exist on those systems.

How can I specify which compilers to use? What about other
compile-time options like LDFLAGS etc.?

NB! Please include me in the CC list, since I'm not a mailinglist
member.

PS. I really like perl's approach to module building. When building
perl modules, it defaults to the same compilers as perl was initially
built with. This seems like a sane approach also for R IMHO.

Cheers,

-- 
Trond Hasle Amundsen <t.h.amundsen at usit.uio.no>
Center for Information Technology Services, University of Oslo



From ligges at statistik.uni-dortmund.de  Thu Sep  1 14:31:32 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 14:31:32 +0200
Subject: [R] Newbie help on dim
In-Reply-To: <4316F32A.4040001@optonline.net>
References: <8D64D4652EB17048B874B0503309CFCA958FA3@epsilon2003.epsilonsgr.it>
	<4316F0CD.8040904@statistik.uni-dortmund.de>
	<4316F32A.4040001@optonline.net>
Message-ID: <4316F4A4.805@statistik.uni-dortmund.de>

Chuck Cleland wrote:

> Should the Value: section of the help page read:
> 
> For an array (and hence in particular, for a matrix) dim retrieves the
> dim attribute of the object. It is NULL for a vector of mode integer.
> 
> NOTE: for not or in the second sentence.

NO!!! The help page is correct!

The *value* is either NULL *or* a vector of mode integer.
It is always NULL for any vector without dim attribute (also for 
character vectors!):

  x <- "a"
  dim(x) # NULL
  dim(x) <- 1
  dim(x) # 1

Uwe Ligges




> Uwe Ligges wrote:
> 
>> Christian Prinoth wrote:
>>
>>
>>> Hi, if I do
>>>
>>> Z<-rnorm(50)
>>> Followed by
>>> Dim(Z)
>>> I get NULL. Is this correct? Shouldn't I get 50 instead?
>>
>>
>>
>>
>> No, because Z has no dim attribute. ?dim tells you:
>>
>> "For an array (and hence in particular, for a matrix) dim retrieves 
>> the dim attribute of the object. It is NULL or a vector of mode integer."
>>
>>
>>> TIA
>>> Chris
>>>
>>> DISCLAIMER:\ L'utilizzo non autorizzato del presente 
>>> messagg...{{dropped}}
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>> Yes, PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>> Uwe Ligges
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>



From HDoran at air.org  Thu Sep  1 14:34:39 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 1 Sep 2005 08:34:39 -0400
Subject: [R] VarCorr function for assigning random effects: was Question
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A0A3A38@dc1ex2.air.org>

If you are indeed using lme and not lmer then the needed function is
VarCorr(). However, 2 recommendations. First, this is a busy list and
better emails subject headers get better attention. Second, I would
recommend using lmer as it is much faster. However, VarCorr seems to be
incompatible with lmer and I do not know of another function to work
with lmer.

Hence, a better email subject header will attract the attention of
others *much* smarter than me!

I hope this helps,
Harold

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mahmoud Torabi
Sent: Wednesday, August 31, 2005 9:51 PM
To: R-help at stat.math.ethz.ch
Cc: mtorabi at math.carleton.ca
Subject: [R] Question

Dear Sir/Madam

I would be pleased if anybody can help me. I'm using linear mixed model
(lme) function.I'm doing some simulation in my research and need to be
assigned variance components values during of my program. Specifically,
when I use lme function, I can get some information by use summary() and
I can assign some valuse like variance of fixed parameters and variance
of random error term by using for example  varFix and sigma.But I don't
know how I can assign for variance of random effect.
I know in SPLUS we have command var.ran, how about R ?

Thanks alot.
M.Torabi

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Sep  1 14:37:41 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 14:37:41 +0200
Subject: [R] R binaries, platform independent and Design of Experiments
In-Reply-To: <50879.59.167.30.131.1125530048.squirrel@59.167.30.131>
References: <20735.129.180.11.34.1125460179.squirrel@129.180.11.34>	<43156806.4020608@hhbio.wasser.tu-dresden.de>
	<50879.59.167.30.131.1125530048.squirrel@59.167.30.131>
Message-ID: <4316F615.1050000@statistik.uni-dortmund.de>

Nam-Ky Nguyen wrote:

> Dear Rexpert,
> 
> I would like to thank those who spend time  answering my email on the
> burning of a CD with all R binary files for Windows and Linux. I have
> tried a couple of suggestions but have not been successful. I will pass
> these suggestions to our system administrator and I am sure that he can
> sort them out.
> 
> I hope that the future version of R will be written in Java so that it is
> platform independent. This means there will be a single binary file for
> each new version of R. At the moment there are files for Windows, Mac OS,
> Fedora 1, 2, 3, 4, SUSE 9.1, 9.2, 9.3, etc.

Honestly ......
a) The sources are available and really easy to compile on all those 
operating systems.
b) You do NOT want to do numerical computations on software available in 
Java byte code.

> I also hope that the future of version of R include more DOE (Design of
> Experiments) modules. R will be more useful if it is not only a language

There are some packages availabe with tools for DOE, please check CRAN.
Further contributions from users who like to do DOE (like you!) are 
welocme, of course!

Uwe Ligges



> and environment for statistical computing and graphics, but also for
> design of experiments. I am toying with the idea of adding to R some
> modules of my Gendex DOE toolkit (http://designcomputing.net/gendex/). I
> learn from an Rexpert that the first step for this exercise is to convert
> my java code to C++.  As I do not know C++ (and life is short) please let
> me know whether there is an alternative way without this conversion and
> whether you can actively help me in this exercise.
> 
> Regards,



From Achim.Zeileis at wu-wien.ac.at  Thu Sep  1 14:33:59 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 1 Sep 2005 14:33:59 +0200
Subject: [R] Newbie help on dim
In-Reply-To: <4316F32A.4040001@optonline.net>
References: <8D64D4652EB17048B874B0503309CFCA958FA3@epsilon2003.epsilonsgr.it>
	<4316F0CD.8040904@statistik.uni-dortmund.de>
	<4316F32A.4040001@optonline.net>
Message-ID: <20050901143359.1c006219.Achim.Zeileis@wu-wien.ac.at>

On Thu, 01 Sep 2005 08:25:14 -0400 Chuck Cleland wrote:

> Should the Value: section of the help page read:
> 
> For an array (and hence in particular, for a matrix) dim retrieves the
> dim attribute of the object. It is NULL for a vector of mode integer.

This statement is true, but less general than
  It is NULL or a vector of mode integer.
hence the latter is surely preferred.
Z

> NOTE: for not or in the second sentence.
> 
> Uwe Ligges wrote:
> > Christian Prinoth wrote:
> > 
> > 
> >>Hi, if I do
> >>
> >>Z<-rnorm(50)
> >>Followed by
> >>Dim(Z)
> >>I get NULL. Is this correct? Shouldn't I get 50 instead?
> > 

> > 
> > 
> > No, because Z has no dim attribute. ?dim tells you:
> > 
> > "For an array (and hence in particular, for a matrix) dim retrieves
> > the dim attribute of the object. It is NULL or a vector of mode
> > integer."
> > 
> > 
> >>TIA
> >>Chris
> >>
> >>DISCLAIMER:\ L'utilizzo non autorizzato del presente
> >messagg...{{dropped}}>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> > 
> > 
> > Yes, PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> > Uwe Ligges
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Thu Sep  1 14:44:00 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 1 Sep 2005 14:44:00 +0200 (CEST)
Subject: [R] Using non-gcc compilers with install.packages()
In-Reply-To: <15tirxl0ya0.fsf@tux.uio.no>
Message-ID: <Pine.LNX.4.44.0509011439490.16501-100000@reclus.nhh.no>

On Thu, 1 Sep 2005, Trond Hasle Amundsen wrote:

> 
> Hi,
> 
> I have built R 2.1.1 on Solaris and OSF1, and used the operating
> system's native compiler set. This worked fine. However, when I try
> using install.packages() it insists on using g77 etc., which doesn't
> exist on those systems.
> 
> How can I specify which compilers to use? What about other
> compile-time options like LDFLAGS etc.?
> 
> NB! Please include me in the CC list, since I'm not a mailinglist
> member.
> 
> PS. I really like perl's approach to module building. When building
> perl modules, it defaults to the same compilers as perl was initially
> built with. This seems like a sane approach also for R IMHO.

Indeed, R is pretty sane. Fine tuning is conveniently carried out by
editing the config.site file in the root source directory; LDFLAGS are on
line 138 of the released 2.1.1 source. Which particular packages have you
found showing this behaviour?

> 
> Cheers,
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ligges at statistik.uni-dortmund.de  Thu Sep  1 14:44:53 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 14:44:53 +0200
Subject: [R] Newbie help on dim
In-Reply-To: <20050901143359.1c006219.Achim.Zeileis@wu-wien.ac.at>
References: <8D64D4652EB17048B874B0503309CFCA958FA3@epsilon2003.epsilonsgr.it>	<4316F0CD.8040904@statistik.uni-dortmund.de>	<4316F32A.4040001@optonline.net>
	<20050901143359.1c006219.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <4316F7C5.2030005@statistik.uni-dortmund.de>

Achim Zeileis wrote:

> On Thu, 01 Sep 2005 08:25:14 -0400 Chuck Cleland wrote:
> 
> 
>>Should the Value: section of the help page read:
>>
>>For an array (and hence in particular, for a matrix) dim retrieves the
>>dim attribute of the object. It is NULL for a vector of mode integer.
> 
> 
> This statement is true, but less general than
>   It is NULL or a vector of mode integer.
> hence the latter is surely preferred.

Z, in fact, it is not true:

  x <- integer(1)
  dim(x) <- 1
  typeof(x) # "integer"
  dim(x)    # 1

Best,
Uwe




> Z
> 
> 
>>NOTE: for not or in the second sentence.
>>
>>Uwe Ligges wrote:
>>
>>>Christian Prinoth wrote:
>>>
>>>
>>>
>>>>Hi, if I do
>>>>
>>>>Z<-rnorm(50)
>>>>Followed by
>>>>Dim(Z)
>>>>I get NULL. Is this correct? Shouldn't I get 50 instead?
>>>
> 
>>>
>>>No, because Z has no dim attribute. ?dim tells you:
>>>
>>>"For an array (and hence in particular, for a matrix) dim retrieves
>>>the dim attribute of the object. It is NULL or a vector of mode
>>>integer."
>>>
>>>
>>>
>>>>TIA
>>>>Chris
>>>>
>>>>DISCLAIMER:\ L'utilizzo non autorizzato del presente
>>>
>>>messagg...{{dropped}}>
>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>>Yes, PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>>Uwe Ligges
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>-- 
>>Chuck Cleland, Ph.D.
>>NDRI, Inc.
>>71 West 23rd Street, 8th floor
>>New York, NY 10010
>>tel: (212) 845-4495 (Tu, Th)
>>tel: (732) 452-1424 (M, W, F)
>>fax: (917) 438-0894
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>



From Vidar.Hjellvik at imr.no  Thu Sep  1 14:52:41 2005
From: Vidar.Hjellvik at imr.no (Hjellvik Vidar)
Date: Thu, 1 Sep 2005 14:52:41 +0200
Subject: [R] source(file) => file becomes readonly
Message-ID: <0D54094F69391A438650C6151B0A33E46FD335@post2.imr.no>

The OS is Windows XP. 
I use to work on the file while it's executed and save changes continually. It doesn't seem to have any effect on the current execution. I just find it annoying not to be able to do it. It's not crucial, but if there is some easy way around it I would like to know....

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
Sent: 1. september 2005 14:21
To: Hjellvik Vidar
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] source(file) => file becomes readonly


Hjellvik Vidar wrote:

> Hello,
> when I work in R, I write code in a text file that I run with the "source(filename)" command. In R2.1.1 the file is read-only while the source command is executed. This was not the case in R2.0.1. Is this a bug-fix or is it possible not to have the file read-only when executed?

So you want to modify a file *while* it is executed?
Sounds dangerous to me, you may want to explain further...

BTW: Which OS are we talking about?

Uwe Ligges


> Best regards
> Vidar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From t.h.amundsen at usit.uio.no  Thu Sep  1 14:58:25 2005
From: t.h.amundsen at usit.uio.no (Trond Hasle Amundsen)
Date: Thu, 01 Sep 2005 14:58:25 +0200
Subject: [R] Using non-gcc compilers with install.packages()
In-Reply-To: <Pine.LNX.4.44.0509011439490.16501-100000@reclus.nhh.no> (Roger
	Bivand's message of "Thu, 1 Sep 2005 14:44:00 +0200 (CEST)")
References: <Pine.LNX.4.44.0509011439490.16501-100000@reclus.nhh.no>
Message-ID: <15tacix0wsu.fsf@tux.uio.no>

Roger Bivand <Roger.Bivand at nhh.no> writes:

> On Thu, 1 Sep 2005, Trond Hasle Amundsen wrote:
>
>> 
>> Hi,
>> 
>> I have built R 2.1.1 on Solaris and OSF1, and used the operating
>> system's native compiler set. This worked fine. However, when I try
>> using install.packages() it insists on using g77 etc., which doesn't
>> exist on those systems.
>> 
>> How can I specify which compilers to use? What about other
>> compile-time options like LDFLAGS etc.?
>> 
>> NB! Please include me in the CC list, since I'm not a mailinglist
>> member.
>> 
>> PS. I really like perl's approach to module building. When building
>> perl modules, it defaults to the same compilers as perl was initially
>> built with. This seems like a sane approach also for R IMHO.
>
> Indeed, R is pretty sane. Fine tuning is conveniently carried out by
> editing the config.site file in the root source directory; LDFLAGS are on
> line 138 of the released 2.1.1 source. Which particular packages have you
> found showing this behaviour?

Hmm..

 - The following use correct c/c++ compilers, but fail to recognize
   this as as Solaris system and uses -fPIC instead of -KPIC for
   shared objects. They also use other gcc-specific options like
   '-O2':

      RandomFields
      geoR
      geoRglm
      rgl
      

 - The following packages try g77 without first checking if it
   exists. I did not specify the F77 variable when compiling R, since
   it found f77 by itself. If all I need is to re-compile with the F77
   variable set to 'f77', I won't mind.

      akima
      sm
      spatstat
      splancs

There are problably more.. these two problem seem to be consistent
across modules, but I can't be sure.

Cheers,

-- 
Trond Hasle Amundsen <t.h.amundsen at usit.uio.no>
Center for Information Technology Services, University of Oslo



From admin at biostatistic.de  Thu Sep  1 15:00:38 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Thu, 01 Sep 2005 15:00:38 +0200
Subject: [R] standard deviation in barplots
In-Reply-To: <BF3C52BF.D839%sdavis2@mail.nih.gov>
References: <BF3C52BF.D839%sdavis2@mail.nih.gov>
Message-ID: <4316FB76.8030709@biostatistic.de>



Sean Davis schrieb:

>I think barplot2 in the gregmisc (gplots) bundle will do that.
>
>Sean
>
>  
>
Ok thank??s but I've got an error and do not found the solution:


ci.l
 [1] 304.09677 202.49907   0.00000  63.14547   0.00000   0.00000   0.00000
 [8]   0.00000   0.00000   0.00000
ci.h
 [1] 633.50323 426.10093  45.12493 344.85453 196.19980 198.17632 208.96365
 [8]  76.49691   0.00000   0.00000
 xrow
 [1] 468.8 314.3  20.1 204.0  96.0  96.0 115.0  36.0   0.0   0.0
barplot2(xrow,plot.ci=TRUE,ci.l=ci.l,ci.h=ci.h)

Knut



From Achim.Zeileis at wu-wien.ac.at  Thu Sep  1 14:57:58 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 1 Sep 2005 14:57:58 +0200
Subject: [R] Newbie help on dim
In-Reply-To: <4316F7C5.2030005@statistik.uni-dortmund.de>
References: <8D64D4652EB17048B874B0503309CFCA958FA3@epsilon2003.epsilonsgr.it>
	<4316F0CD.8040904@statistik.uni-dortmund.de>
	<4316F32A.4040001@optonline.net>
	<20050901143359.1c006219.Achim.Zeileis@wu-wien.ac.at>
	<4316F7C5.2030005@statistik.uni-dortmund.de>
Message-ID: <20050901145758.0fd65a7e.Achim.Zeileis@wu-wien.ac.at>

On Thu, 01 Sep 2005 14:44:53 +0200 Uwe Ligges wrote:

> Achim Zeileis wrote:
> 
> > On Thu, 01 Sep 2005 08:25:14 -0400 Chuck Cleland wrote:
> > 
> > 
> >>Should the Value: section of the help page read:
> >>
> >>For an array (and hence in particular, for a matrix) dim retrieves
> >the>dim attribute of the object. It is NULL for a vector of mode
> >integer.
> > 
> > 
> > This statement is true, but less general than
> >   It is NULL or a vector of mode integer.
> > hence the latter is surely preferred.
> 
> Z, in fact, it is not true:
> 
>   x <- integer(1)
>   dim(x) <- 1
>   typeof(x) # "integer"
>   dim(x)    # 1

But then:
  is.vector(x) # FALSE
but let's not start nitpicking here.
Z

> Best,
> Uwe
> 
> 
> 
> 
> > Z
> > 
> > 
> >>NOTE: for not or in the second sentence.
> >>
> >>Uwe Ligges wrote:
> >>
> >>>Christian Prinoth wrote:
> >>>
> >>>
> >>>
> >>>>Hi, if I do
> >>>>
> >>>>Z<-rnorm(50)
> >>>>Followed by
> >>>>Dim(Z)
> >>>>I get NULL. Is this correct? Shouldn't I get 50 instead?
> >>>
> > 
> >>>
> >>>No, because Z has no dim attribute. ?dim tells you:
> >>>
> >>>"For an array (and hence in particular, for a matrix) dim retrieves
> >>>the dim attribute of the object. It is NULL or a vector of mode
> >>>integer."
> >>>
> >>>
> >>>
> >>>>TIA
> >>>>Chris
> >>>>
> >>>>DISCLAIMER:\ L'utilizzo non autorizzato del presente
> >>>
> >>>messagg...{{dropped}}>
> >>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide!
> >>>
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>>
> >>>Yes, PLEASE do read the posting guide! 
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>>
> >>>Uwe Ligges
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>>
> >>
> >>-- 
> >>Chuck Cleland, Ph.D.
> >>NDRI, Inc.
> >>71 West 23rd Street, 8th floor
> >>New York, NY 10010
> >>tel: (212) 845-4495 (Tu, Th)
> >>tel: (732) 452-1424 (M, W, F)
> >>fax: (917) 438-0894
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From cobleigh at gmail.com  Thu Sep  1 15:05:29 2005
From: cobleigh at gmail.com (Jamieson Cobleigh)
Date: Thu, 1 Sep 2005 09:05:29 -0400
Subject: [R] Spacing and margins in plot
Message-ID: <7f50836c050901060555b9289d@mail.gmail.com>

If I use the following command to plot points:

plot(c(1,2,2,3,3,3), type="p", pch=20, ylab="Y Label", xlab="X Label", xaxt="n")

there is a large amount of space between the label "X Label" and the
actual x-axis.  If I change the xaxt="n" to xaxt="s", the label "X
Label" don't move at all.  Is there a way to get the label "X Label"
closer to the x-axis when xaxt="n"?



The plot I am generating is going to be included in a paper I am
writing.  I can cause the plot to be saved in a PDF file by doing the
following:

pdf("foo.pdf", width=5.5, height=4.25, onefile=FALSE)

plot(c(1,2,2,3,3,3), type="p", pch=20, ylab="Y Label", xlab="X Label", xaxt="n")

dev.off();

In the resulting file, there is a lot of whitespace around the graph,
particularly between the top line of the plot area and the top of the
page.  Since I am including these plots in a paper, I want them to be
as large as possible and not take up any extra space.  Is there a way
to get R to draw a plot that goes all the way to the margins of the
print area?

Jamie



From c_naber at yahoo.com.br  Thu Sep  1 15:09:00 2005
From: c_naber at yahoo.com.br (Caio Lucidius Naberezny Azevedo)
Date: Thu, 1 Sep 2005 13:09:00 +0000 (GMT)
Subject: [R] Multivariate Skew Normal distribution
Message-ID: <20050901130901.61123.qmail@web34011.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050901/7b8d1243/attachment.pl

From admin at biostatistic.de  Thu Sep  1 15:12:56 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Thu, 01 Sep 2005 15:12:56 +0200
Subject: [R] standard deviation in barplots
In-Reply-To: <BF3C52BF.D839%sdavis2@mail.nih.gov>
References: <BF3C52BF.D839%sdavis2@mail.nih.gov>
Message-ID: <4316FE58.5020100@biostatistic.de>

I forgot the error message ...

Error in barplot2.default(xrow, plot.ci = TRUE, ci.l = ci.l, ci.h = ci.h) :
        confidence interval values are missing



From Roger.Bivand at nhh.no  Thu Sep  1 15:19:44 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 1 Sep 2005 15:19:44 +0200 (CEST)
Subject: [R] Using non-gcc compilers with install.packages()
In-Reply-To: <15tacix0wsu.fsf@tux.uio.no>
Message-ID: <Pine.LNX.4.44.0509011508290.16501-100000@reclus.nhh.no>

On Thu, 1 Sep 2005, Trond Hasle Amundsen wrote:

> Roger Bivand <Roger.Bivand at nhh.no> writes:
> 
> > On Thu, 1 Sep 2005, Trond Hasle Amundsen wrote:
> >
> >> 
> >> Hi,
> >> 
> >> I have built R 2.1.1 on Solaris and OSF1, and used the operating
> >> system's native compiler set. This worked fine. However, when I try
> >> using install.packages() it insists on using g77 etc., which doesn't
> >> exist on those systems.
> >> 
> >> How can I specify which compilers to use? What about other
> >> compile-time options like LDFLAGS etc.?
> >> 
> >> NB! Please include me in the CC list, since I'm not a mailinglist
> >> member.
> >> 
> >> PS. I really like perl's approach to module building. When building
> >> perl modules, it defaults to the same compilers as perl was initially
> >> built with. This seems like a sane approach also for R IMHO.
> >
> > Indeed, R is pretty sane. Fine tuning is conveniently carried out by
> > editing the config.site file in the root source directory; LDFLAGS are on
> > line 138 of the released 2.1.1 source. Which particular packages have you
> > found showing this behaviour?
> 
> Hmm..
> 
>  - The following use correct c/c++ compilers, but fail to recognize
>    this as as Solaris system and uses -fPIC instead of -KPIC for
>    shared objects. They also use other gcc-specific options like
>    '-O2':
> 
>       RandomFields
>       geoR
>       geoRglm
>       rgl
>       
> 
>  - The following packages try g77 without first checking if it
>    exists. I did not specify the F77 variable when compiling R, since
>    it found f77 by itself. If all I need is to re-compile with the F77
>    variable set to 'f77', I won't mind.
> 
>       akima
>       sm
>       spatstat
>       splancs
> 
> There are problably more.. these two problem seem to be consistent
> across modules, but I can't be sure.

OK. When installing packages, the settings in the R_HOME/etc/Makeconf file 
are used, unless over-ridden by a src/Makevars file in the source package. 
Since I maintain splancs, I know that it does not have a src/Makevars 
file. That means that you are getting the R_HOME/etc/Makeconf settings 
generated at compile time. Can you confirm that your R_HOME/etc/Makeconf 
file has the compile time settings?

> 
> Cheers,
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From MSchwartz at mn.rr.com  Thu Sep  1 15:24:41 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 01 Sep 2005 08:24:41 -0500
Subject: [R] standard deviation in barplots
In-Reply-To: <4316FB76.8030709@biostatistic.de>
References: <BF3C52BF.D839%sdavis2@mail.nih.gov>
	<4316FB76.8030709@biostatistic.de>
Message-ID: <1125581081.3239.4.camel@localhost.localdomain>

On Thu, 2005-09-01 at 15:00 +0200, Knut Krueger wrote:
> 
> Sean Davis schrieb:
> 
> >I think barplot2 in the gregmisc (gplots) bundle will do that.
> >
> >Sean
> >
> >  
> >
> Ok thankÂ´s but I've got an error and do not found the solution:
> 
> 
> ci.l
>  [1] 304.09677 202.49907   0.00000  63.14547   0.00000   0.00000   0.00000
>  [8]   0.00000   0.00000   0.00000
> ci.h
>  [1] 633.50323 426.10093  45.12493 344.85453 196.19980 198.17632 208.96365
>  [8]  76.49691   0.00000   0.00000
>  xrow
>  [1] 468.8 314.3  20.1 204.0  96.0  96.0 115.0  36.0   0.0   0.0
> barplot2(xrow,plot.ci=TRUE,ci.l=ci.l,ci.h=ci.h)
> 
> Knut


Presumably, you are getting:

> barplot2(xrow, plot.ci = TRUE, ci.l = ci.l, ci.h = ci.h)
Error in barplot2.default(xrow, plot.ci = TRUE, ci.l = ci.l, ci.h =
ci.h) : 
	confidence interval values are missing



There is an error in your function call. The argument 'ci.h' is
incorrect, as it should be 'ci.u'. Thus, use:


ci.l <- c(304.09677, 202.49907, 0.00000, 63.14547,
          0.00000, 0.00000, 0.00000, 0.00000,
          0.00000, 0.00000)

ci.u <- c(633.50323, 426.10093, 45.12493, 344.85453,
          196.19980, 198.17632, 208.96365, 76.49691,
          0.00000, 0.00000)

xrow <- c(468.8, 314.3, 20.1, 204.0, 96.0, 96.0,
          115.0, 36.0, 0.0, 0.0)

barplot2(xrow, plot.ci = TRUE, ci.l = ci.l, ci.u = ci.u)


HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Thu Sep  1 15:29:58 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 15:29:58 +0200
Subject: [R] Using non-gcc compilers with install.packages()
In-Reply-To: <15tacix0wsu.fsf@tux.uio.no>
References: <Pine.LNX.4.44.0509011439490.16501-100000@reclus.nhh.no>
	<15tacix0wsu.fsf@tux.uio.no>
Message-ID: <43170256.2070007@statistik.uni-dortmund.de>

Trond Hasle Amundsen wrote:

> Roger Bivand <Roger.Bivand at nhh.no> writes:
> 
> 
>>On Thu, 1 Sep 2005, Trond Hasle Amundsen wrote:
>>
>>
>>>Hi,
>>>
>>>I have built R 2.1.1 on Solaris and OSF1, and used the operating
>>>system's native compiler set. This worked fine. However, when I try
>>>using install.packages() it insists on using g77 etc., which doesn't
>>>exist on those systems.
>>>
>>>How can I specify which compilers to use? What about other
>>>compile-time options like LDFLAGS etc.?
>>>
>>>NB! Please include me in the CC list, since I'm not a mailinglist
>>>member.
>>>
>>>PS. I really like perl's approach to module building. When building
>>>perl modules, it defaults to the same compilers as perl was initially
>>>built with. This seems like a sane approach also for R IMHO.
>>
>>Indeed, R is pretty sane. Fine tuning is conveniently carried out by
>>editing the config.site file in the root source directory; LDFLAGS are on
>>line 138 of the released 2.1.1 source. Which particular packages have you
>>found showing this behaviour?
> 
> 
> Hmm..
> 
>  - The following use correct c/c++ compilers, but fail to recognize
>    this as as Solaris system and uses -fPIC instead of -KPIC for
>    shared objects. They also use other gcc-specific options like
>    '-O2':
> 
>       RandomFields
>       geoR
>       geoRglm
>       rgl
>       
> 
>  - The following packages try g77 without first checking if it
>    exists. I did not specify the F77 variable when compiling R, since
>    it found f77 by itself. If all I need is to re-compile with the F77
>    variable set to 'f77', I won't mind.
> 
>       akima
>       sm
>       spatstat
>       splancs
> 
> There are problably more.. these two problem seem to be consistent
> across modules, but I can't be sure.


So these are package related problems.
Can you please let the corresponding package authors know and ask them 
to fix their packages.

Uwe Ligges


> Cheers,
>



From herodote at oreka.com  Thu Sep  1 15:33:51 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Thu,  1 Sep 2005 14:33:51 +0100
Subject: [R] =?iso-8859-1?q?default_height_width_of_graphs?=
Message-ID: <IM54CF$BB19FF2B691E83F41BFC5B2F06F093CA@oreka.com>

hy all,

When i plot under R it generates a 440x440px image, is it possible to modify and increase this ?


thks all
guillaume.



From ligges at statistik.uni-dortmund.de  Thu Sep  1 15:34:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 15:34:50 +0200
Subject: [R] standard deviation in barplots
In-Reply-To: <4316FB76.8030709@biostatistic.de>
References: <BF3C52BF.D839%sdavis2@mail.nih.gov>
	<4316FB76.8030709@biostatistic.de>
Message-ID: <4317037A.5090103@statistik.uni-dortmund.de>

Knut Krueger wrote:

> 
> Sean Davis schrieb:
> 
> 
>>I think barplot2 in the gregmisc (gplots) bundle will do that.
>>
>>Sean
>>
>> 
>>
> 
> Ok thank??s but I've got an error and do not found the solution:
> 
> 
> ci.l
>  [1] 304.09677 202.49907   0.00000  63.14547   0.00000   0.00000   0.00000
>  [8]   0.00000   0.00000   0.00000
> ci.h
>  [1] 633.50323 426.10093  45.12493 344.85453 196.19980 198.17632 208.96365
>  [8]  76.49691   0.00000   0.00000
>  xrow
>  [1] 468.8 314.3  20.1 204.0  96.0  96.0 115.0  36.0   0.0   0.0
> barplot2(xrow,plot.ci=TRUE,ci.l=ci.l,ci.h=ci.h)


It is called ci.u, not ci.h!

Uwe Ligges


> Knut
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From admin at biostatistic.de  Thu Sep  1 15:51:48 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Thu, 01 Sep 2005 15:51:48 +0200
Subject: [R] standard deviation in barplots
In-Reply-To: <1125581081.3239.4.camel@localhost.localdomain>
References: <BF3C52BF.D839%sdavis2@mail.nih.gov>	<4316FB76.8030709@biostatistic.de>
	<1125581081.3239.4.camel@localhost.localdomain>
Message-ID: <43170774.7000703@biostatistic.de>




>
>There is an error in your function call. The argument 'ci.h' is
>incorrect, as it should be 'ci.u'. Thus, use:
>
>  
>
... I think I will need glasses ....


Tank's a lot Knut



From ligges at statistik.uni-dortmund.de  Thu Sep  1 15:56:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 15:56:42 +0200
Subject: [R] default height width of graphs
In-Reply-To: <IM54CF$BB19FF2B691E83F41BFC5B2F06F093CA@oreka.com>
References: <IM54CF$BB19FF2B691E83F41BFC5B2F06F093CA@oreka.com>
Message-ID: <4317089A.7090504@statistik.uni-dortmund.de>

herodote at oreka.com wrote:

> hy all,
> 
> When i plot under R it generates a 440x440px image, is it possible to modify and increase this ?


What device are we talking about?
See the corresponding help file, maybe starting at ?Devices

Uwe Ligges


> 
> thks all
> guillaume.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Sep  1 16:09:32 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 16:09:32 +0200
Subject: [R] Spacing and margins in plot
In-Reply-To: <7f50836c050901060555b9289d@mail.gmail.com>
References: <7f50836c050901060555b9289d@mail.gmail.com>
Message-ID: <43170B9C.1040202@statistik.uni-dortmund.de>

Jamieson Cobleigh wrote:

> If I use the following command to plot points:
> 
> plot(c(1,2,2,3,3,3), type="p", pch=20, ylab="Y Label", xlab="X Label", xaxt="n")
> 
> there is a large amount of space between the label "X Label" and the
> actual x-axis.  If I change the xaxt="n" to xaxt="s", the label "X
> Label" don't move at all.  Is there a way to get the label "X Label"
> closer to the x-axis when xaxt="n"?
> 
> 
> 
> The plot I am generating is going to be included in a paper I am
> writing.  I can cause the plot to be saved in a PDF file by doing the
> following:
> 
> pdf("foo.pdf", width=5.5, height=4.25, onefile=FALSE)
> 
> plot(c(1,2,2,3,3,3), type="p", pch=20, ylab="Y Label", xlab="X Label", xaxt="n")
> 
> dev.off();
> 
> In the resulting file, there is a lot of whitespace around the graph,
> particularly between the top line of the plot area and the top of the
> page.  Since I am including these plots in a paper, I want them to be
> as large as possible and not take up any extra space.  Is there a way
> to get R to draw a plot that goes all the way to the margins of the
> print area?
> 
> Jamie

For the size of margins see ?par and its argument "mar", for the 
position of the x-axis label, add it in a separate call to title() and 
specify the "line" where to add the text as in:

   par(mar = c(1, 4, 0, 0) + 0.1)
   plot(c(1,2,2,3,3,3), type="p", pch=20, ylab="Y Label",
       xlab="", xaxt="n")
   title(xlab="X Label", line=0)


Uwe Ligges

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From t.h.amundsen at usit.uio.no  Thu Sep  1 16:15:01 2005
From: t.h.amundsen at usit.uio.no (Trond Hasle Amundsen)
Date: Thu, 01 Sep 2005 16:15:01 +0200
Subject: [R] Using non-gcc compilers with install.packages()
In-Reply-To: <Pine.LNX.4.44.0509011508290.16501-100000@reclus.nhh.no> (Roger
	Bivand's message of "Thu, 1 Sep 2005 15:19:44 +0200 (CEST)")
References: <Pine.LNX.4.44.0509011508290.16501-100000@reclus.nhh.no>
Message-ID: <15t1x4827tm.fsf@tux.uio.no>

Roger Bivand <Roger.Bivand at nhh.no> writes:

> On Thu, 1 Sep 2005, Trond Hasle Amundsen wrote:
>
>> Roger Bivand <Roger.Bivand at nhh.no> writes:
>> 
>> > On Thu, 1 Sep 2005, Trond Hasle Amundsen wrote:
>> >
>> >> 
>> >> Hi,
>> >> 
>> >> I have built R 2.1.1 on Solaris and OSF1, and used the operating
>> >> system's native compiler set. This worked fine. However, when I try
>> >> using install.packages() it insists on using g77 etc., which doesn't
>> >> exist on those systems.
>> >> 
>> >> How can I specify which compilers to use? What about other
>> >> compile-time options like LDFLAGS etc.?
>> >> 
>> >> NB! Please include me in the CC list, since I'm not a mailinglist
>> >> member.
>> >> 
>> >> PS. I really like perl's approach to module building. When building
>> >> perl modules, it defaults to the same compilers as perl was initially
>> >> built with. This seems like a sane approach also for R IMHO.
>> >
>> > Indeed, R is pretty sane. Fine tuning is conveniently carried out by
>> > editing the config.site file in the root source directory; LDFLAGS are on
>> > line 138 of the released 2.1.1 source. Which particular packages have you
>> > found showing this behaviour?
>> 
>> Hmm..
>> 
>>  - The following use correct c/c++ compilers, but fail to recognize
>>    this as as Solaris system and uses -fPIC instead of -KPIC for
>>    shared objects. They also use other gcc-specific options like
>>    '-O2':
>> 
>>       RandomFields
>>       geoR
>>       geoRglm
>>       rgl
>>       
>> 
>>  - The following packages try g77 without first checking if it
>>    exists. I did not specify the F77 variable when compiling R, since
>>    it found f77 by itself. If all I need is to re-compile with the F77
>>    variable set to 'f77', I won't mind.
>> 
>>       akima
>>       sm
>>       spatstat
>>       splancs
>> 
>> There are problably more.. these two problem seem to be consistent
>> across modules, but I can't be sure.
>
> OK. When installing packages, the settings in the R_HOME/etc/Makeconf file 
> are used, unless over-ridden by a src/Makevars file in the source package. 
> Since I maintain splancs, I know that it does not have a src/Makevars 
> file. That means that you are getting the R_HOME/etc/Makeconf settings 
> generated at compile time. Can you confirm that your R_HOME/etc/Makeconf 
> file has the compile time settings?

Aha. This explains everything. At our site, we use a software
distribution and build system that classifies files a certain way. For
text files, the Linux version is used, for no other reason than that
the Linux compile server is the master of the cluster. This system
does not recognize files that look different on different
architectures. If it's a text file, it's considered architecture
independent and only one version (in our case, the one built on Linux)
is kept.

I'll rebuild R on all architectures, and flag the contents of
R_HOME/etc as architecture dependent so that the correct version is
kept. That way, module building shouldn't give me more problems.

Thanks for your quick and valuable input. You deserve a cold beer :)

Cheers,

-- 
Trond Hasle Amundsen <t.h.amundsen at usit.uio.no>
Gruppe for basis systemdrift (BSD), SAPP, USIT



From newsreader at zutt.org  Thu Sep  1 16:05:34 2005
From: newsreader at zutt.org (jonne)
Date: Thu, 01 Sep 2005 16:05:34 +0200
Subject: [R] 3d cube with labels along axes
Message-ID: <pan.2005.09.01.14.05.34.548177@zutt.org>

Hi R-users,

I would like to draw a cube with a grid on it and labels along all
three axes. I have trouble printing the labels correctly. My
best attempt is described below.
   Can somebody explain me how I can change the 0,20,40,80,100
along the x axis into character vectors like
"no", "light", "intermediate", "severe" ?

x <- seq(0, 100, length=10)
#x <- c("no", "light", "intermediate", "severe")
y <- x
f <- function(x,y) { numeric(length=100) + 5 }
z <- outer(x, y, f)

P <- persp(x, y, z, theta=30, phi=30, zlim=c(-10,10), ticktype="detailed")

text3d(0, 0, -10, "Hello world", P)

Thanks in advance,
Jonne.



From efg at stowers-institute.org  Thu Sep  1 16:17:16 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 1 Sep 2005 09:17:16 -0500
Subject: [R] Spacing and margins in plot
References: <7f50836c050901060555b9289d@mail.gmail.com>
Message-ID: <df72hd$ora$1@sea.gmane.org>

This technote explains the margin area (mar) and how to modify it to control
white space around a graphic:
http://research.stowers-institute.org/efg/R/Graphics/Basics/mar-oma/index.htm

When you have multiple figures on a graphic, you may also want to learn to
control the outer margin area (oma), which is also explained.

AFAIK, the only way to get the axis label "closer" to the axis is to
suppress the actual axis labels and use the mtext command to display
alternative text where you want it.  For example, look at the blue text in
Figure 2B (at the above link)  that is between the axis label and the axis.
This blue text is at line=2, when the axis labels are at line=3.

efg
Bioinformatics
Stowers Institute

"Jamieson Cobleigh" <cobleigh at gmail.com> wrote in message
news:7f50836c050901060555b9289d at mail.gmail.com...
> If I use the following command to plot points:
>
> plot(c(1,2,2,3,3,3), type="p", pch=20, ylab="Y Label", xlab="X Label",
xaxt="n")
>
> there is a large amount of space between the label "X Label" and the
> actual x-axis.  If I change the xaxt="n" to xaxt="s", the label "X
> Label" don't move at all.  Is there a way to get the label "X Label"
> closer to the x-axis when xaxt="n"?
>
> The plot I am generating is going to be included in a paper I am
> writing.  I can cause the plot to be saved in a PDF file by doing the
> following:
>
> pdf("foo.pdf", width=5.5, height=4.25, onefile=FALSE)
>
> plot(c(1,2,2,3,3,3), type="p", pch=20, ylab="Y Label", xlab="X Label",
xaxt="n")
>
> dev.off();
>
> In the resulting file, there is a lot of whitespace around the graph,
> particularly between the top line of the plot area and the top of the
> page.  Since I am including these plots in a paper, I want them to be
> as large as possible and not take up any extra space.  Is there a way
> to get R to draw a plot that goes all the way to the margins of the
> print area?



From vincent at 7d4.com  Thu Sep  1 16:27:20 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Thu, 01 Sep 2005 16:27:20 +0200
Subject: [R] Statistics with R
In-Reply-To: <df01a8eb050828110452381dd3@mail.gmail.com>
References: <df01a8eb050828110452381dd3@mail.gmail.com>
Message-ID: <43170FC8.7000808@7d4.com>

Hi Vincent,
I regularly have a look at your web site.
Very big thanks and bravo for this very useful work.
(Just a little remark, why not let the french html
version fully available on line, (findable by google, etc)
This would/could be useful to promote R for french peoples ?)
Many thanks.
Vincent



From cobleigh at gmail.com  Thu Sep  1 16:25:35 2005
From: cobleigh at gmail.com (Jamieson Cobleigh)
Date: Thu, 1 Sep 2005 10:25:35 -0400
Subject: [R] Spacing and margins in plot
In-Reply-To: <4317005B.1020805@optonline.net>
References: <7f50836c050901060555b9289d@mail.gmail.com>
	<4317005B.1020805@optonline.net>
Message-ID: <7f50836c050901072513efc07@mail.gmail.com>

That worked and gave me enough information so to make it look exactly
the way I want.

Thanks!

Jamie

On 9/1/05, Chuck Cleland <ccleland at optonline.net> wrote:
> How about this:
> 
> par(mar=c(2,4,1,1))
> 
> plot(c(1,2,2,3,3,3), type="p", pch=20, ylab="Y Label", xlab="", xaxt="n")
> 
> mtext(side=1, line=0.5, "X Label")
> 
> hope it helps,
> 
> Chuck
> 
> Jamieson Cobleigh wrote:
> > If I use the following command to plot points:
> >
> > plot(c(1,2,2,3,3,3), type="p", pch=20, ylab="Y Label", xlab="X Label", xaxt="n")
> >
> > there is a large amount of space between the label "X Label" and the
> > actual x-axis.  If I change the xaxt="n" to xaxt="s", the label "X
> > Label" don't move at all.  Is there a way to get the label "X Label"
> > closer to the x-axis when xaxt="n"?
> >
> >
> >
> > The plot I am generating is going to be included in a paper I am
> > writing.  I can cause the plot to be saved in a PDF file by doing the
> > following:
> >
> > pdf("foo.pdf", width=5.5, height=4.25, onefile=FALSE)
> >
> > plot(c(1,2,2,3,3,3), type="p", pch=20, ylab="Y Label", xlab="X Label", xaxt="n")
> >
> > dev.off();
> >
> > In the resulting file, there is a lot of whitespace around the graph,
> > particularly between the top line of the plot area and the top of the
> > page.  Since I am including these plots in a paper, I want them to be
> > as large as possible and not take up any extra space.  Is there a way
> > to get R to draw a plot that goes all the way to the margins of the
> > print area?
> >
> > Jamie
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> 
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
> 
>



From cobleigh at gmail.com  Thu Sep  1 16:29:11 2005
From: cobleigh at gmail.com (Jamieson Cobleigh)
Date: Thu, 1 Sep 2005 10:29:11 -0400
Subject: [R] Spacing and margins in lattice...
Message-ID: <7f50836c050901072912cd925f@mail.gmail.com>

Similar to my last question, I want to tighten up the spacing and
margins in a plot I am doing with lattice.

Here are the commands I'm using:

data <- data.frame(x=c(1:3, 1:3), y=c(1:3, 1:3*2),
cat=c("foo","foo","foo","bar", "bar","bar"))

xyplot(panel=panel.superpose, y~x, data=data, groups=cat, type="b",
scales=list(tck=c(2,0), axs="r", cex=c(1,0)))

I know that par(mar=c(2,2,1,1)) would do what I want with plot.  Is
there something similar for xyplot/lattice that can reduce the size of
the margins of the plot?

Thanks!

Jamie



From c.wallace at qmul.ac.uk  Thu Sep  1 16:34:07 2005
From: c.wallace at qmul.ac.uk (Chris Wallace)
Date: Thu, 01 Sep 2005 15:34:07 +0100
Subject: [R] Spacing and margins in plot
In-Reply-To: <df72hd$ora$1@sea.gmane.org> (Earl F. Glynn's message of "Thu, 1
	Sep 2005 09:17:16 -0500")
References: <7f50836c050901060555b9289d@mail.gmail.com>
	<df72hd$ora$1@sea.gmane.org>
Message-ID: <m3wtm0502o.fsf@qmul.ac.uk>

"Earl F. Glynn" <efg at stowers-institute.org> writes:

> AFAIK, the only way to get the axis label "closer" to the axis is to
> suppress the actual axis labels and use the mtext command to display
> alternative text where you want it.  For example, look at the blue text in
> Figure 2B (at the above link)  that is between the axis label and the axis.
> This blue text is at line=2, when the axis labels are at line=3.

how about
plot(..., xlab="")
title(xlab="label text", line=2)

?



From bill.shipley at usherbrooke.ca  Thu Sep  1 16:47:08 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Thu, 1 Sep 2005 10:47:08 -0400
Subject: [R] making self-starting  function for nls
Message-ID: <002601c5af04$07086d90$a01ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050901/d895f29d/attachment.pl

From CMiller at PICR.man.ac.uk  Thu Sep  1 16:54:05 2005
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Thu, 1 Sep 2005 15:54:05 +0100
Subject: [R] Matrices with a single column
Message-ID: <BAA35444B19AD940997ED02A6996AAE001AF903E@sanmail.picr.man.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050901/2fbfac21/attachment.pl

From RRoa at fisheries.gov.fk  Thu Sep  1 14:53:57 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Thu, 1 Sep 2005 10:53:57 -0200
Subject: [R] Multivariate Skew Normal distribution
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC41B@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Caio Lucidius
> Naberezny Azevedo
> Sent: 01 September 2005 12:09
> To: Help mailing list - R
> Subject: [R] Multivariate Skew Normal distribution
> 
> 
> Hi all,
>  
> Could anyone tell me if there is any package (or function) 
> that generates values from a multivariate skew normal distribution?
>  
> Thanks all,
>  
> Caio

Try
RSiteSearch("multivariate normal skew") 
I tried and 39 messages turned up.
R.



From reid_huntsinger at merck.com  Thu Sep  1 17:05:27 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 1 Sep 2005 11:05:27 -0400
Subject: [R] Matrices with a single column
Message-ID: <355C35514FEAC9458F75947F5270974D076CA8@usctmx1103.merck.com>

There's an optional "drop" argument to the indexing operations, which is by
default TRUE. You just want 

> x[4:6,,drop=FALSE]
     [,1]
[1,]    4
[2,]    5
[3,]    6

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Crispin Miller
Sent: Thursday, September 01, 2005 10:54 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Matrices with a single column


Hi,
I've got a quick question about what happens when indexing into matrices
with a single column. I was wondering if anyone can help ...
 
For example:
> x <- matrix(1:10)

> y <- cbind(x,x)

> x[4:6,]

[1] 4 5 6

> y[4:6,]

   [,1] [,2]

[1,] 4 4

[2,] 5 5

[3,] 6 6

> class(x[4:6,])

[1] "integer"

> class(y[4:6,])

[1] "matrix"

It seems that R is returning the results of indexing into a matrix with
one column as a vector rather than a matrix?

Does anyone have a good way of preventing this from happening?

cheers,

crispin
 
--------------------------------------------------------

 
This email is confidential and intended solely for the use o...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Thu Sep  1 16:41:56 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 01 Sep 2005 15:41:56 +0100 (BST)
Subject: [R] Multivariate Skew Normal distribution
In-Reply-To: <20050901130901.61123.qmail@web34011.mail.mud.yahoo.com>
Message-ID: <XFMail.050901150246.Ted.Harding@nessie.mcc.ac.uk>

On 01-Sep-05 Caio Lucidius Naberezny Azevedo wrote:
> Hi all,
>  
> Could anyone tell me if there is any package (or function) that
> generates values from a multivariate skew normal distribution?
>  
> Thanks all,
>  
> Caio

Hello, Caio

Please tell us what you mean by "skew normal distribution".

Since normal (i.e. gaussian) distributions are not skew, you
presumably mean something different from what you said, so
unless we understand this more clearly  it is unlikely that
anyone can make a suggestion which would meet your needs.

With best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 01-Sep-05                                       Time: 15:02:37
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Thu Sep  1 17:07:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 17:07:12 +0200
Subject: [R] Matrices with a single column
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE001AF903E@sanmail.picr.man.ac.uk>
References: <BAA35444B19AD940997ED02A6996AAE001AF903E@sanmail.picr.man.ac.uk>
Message-ID: <43171920.4000008@statistik.uni-dortmund.de>

Crispin Miller wrote:

> Hi,
> I've got a quick question about what happens when indexing into matrices
> with a single column. I was wondering if anyone can help ...
>  
> For example:
> 
>>x <- matrix(1:10)
> 
> 
>>y <- cbind(x,x)
> 
> 
>>x[4:6,]
> 
> 
> [1] 4 5 6
> 
> 
>>y[4:6,]
> 
> 
>    [,1] [,2]
> 
> [1,] 4 4
> 
> [2,] 5 5
> 
> [3,] 6 6
> 
> 
>>class(x[4:6,])
> 
> 
> [1] "integer"
> 
> 
>>class(y[4:6,])
> 
> 
> [1] "matrix"
> 
> It seems that R is returning the results of indexing into a matrix with
> one column as a vector rather than a matrix?
> 
> Does anyone have a good way of preventing this from happening?

Yes, the argument "drop":

x[4:6,,drop=FALSE]

Uwe Ligges

> cheers,
> 
> crispin
>  
> --------------------------------------------------------
> 
>  
> This email is confidential and intended solely for the use o...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Thu Sep  1 17:14:16 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 01 Sep 2005 10:14:16 -0500
Subject: [R] Spacing and margins in lattice...
In-Reply-To: <7f50836c050901072912cd925f@mail.gmail.com>
References: <7f50836c050901072912cd925f@mail.gmail.com>
Message-ID: <43171AC8.4060308@pdf.com>



Jamieson Cobleigh wrote:
> Similar to my last question, I want to tighten up the spacing and
> margins in a plot I am doing with lattice.
> 
> Here are the commands I'm using:
> 
> data <- data.frame(x=c(1:3, 1:3), y=c(1:3, 1:3*2),
> cat=c("foo","foo","foo","bar", "bar","bar"))
> 
> xyplot(panel=panel.superpose, y~x, data=data, groups=cat, type="b",
> scales=list(tck=c(2,0), axs="r", cex=c(1,0)))
> 
> I know that par(mar=c(2,2,1,1)) would do what I want with plot.  Is
> there something similar for xyplot/lattice that can reduce the size of
> the margins of the plot?
> 
> Thanks!
> 
> Jamie
> 

Hi, Jamie,

Will the following work for you?

library(lattice)
data <- data.frame(x = c(1:3, 1:3), y = c(1:3, 1:3*2),
                    cat = c("foo","foo","foo","bar", "bar","bar"))

trellis.par.set(theme = col.whitebg())

lw <- list(left.padding = list(x = 0, units = "inches"))
lw$right.padding <- list(x = -0.1, units = "inches")
lh <- list(bottom.padding = list(x = 0, units = "inches"))
lh$top.padding <- list(x = -0.2, units = "inches")

lattice.options(layout.widths = lw, layout.heights = lh)
xyplot(y ~ x, data, groups = cat, type="b")

HTH,

--sundar



From CMiller at PICR.man.ac.uk  Thu Sep  1 17:14:51 2005
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Thu, 1 Sep 2005 16:14:51 +0100
Subject: [R] Matrices with a single column
Message-ID: <BAA35444B19AD940997ED02A6996AAE001AF903F@sanmail.picr.man.ac.uk>

Thanks everyone!

Crispin
 
--------------------------------------------------------

 
This email is confidential and intended solely for the use o...{{dropped}}



From efg at stowers-institute.org  Thu Sep  1 17:14:09 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 1 Sep 2005 10:14:09 -0500
Subject: [R] Spacing and margins in plot
References: <7f50836c050901060555b9289d@mail.gmail.com><df72hd$ora$1@sea.gmane.org>
	<m3wtm0502o.fsf@qmul.ac.uk>
Message-ID: <df75s2$526$1@sea.gmane.org>

"Chris Wallace" <c.wallace at qmul.ac.uk> wrote in message
news:m3wtm0502o.fsf at qmul.ac.uk...

> how about
> plot(..., xlab="")
> title(xlab="label text", line=2)

Yes, Chris, I like your idea, especially when I can "fix" both X and Y axes
at the same time:

  plot(0, xlab="",ylab="")
  title(xlab="X axis", ylab="Y axis", line=2)

I'd prefer a way to set the axis title line at the same time I change the
mar parameters, but it's not a big deal.

Thanks.
efg



From dmbates at gmail.com  Thu Sep  1 17:27:47 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 1 Sep 2005 10:27:47 -0500
Subject: [R] making self-starting function for nls
In-Reply-To: <002601c5af04$07086d90$a01ad284@BIO041>
References: <002601c5af04$07086d90$a01ad284@BIO041>
Message-ID: <40e66e0b05090108272136cb07@mail.gmail.com>

On 9/1/05, Bill Shipley <bill.shipley at usherbrooke.ca> wrote:
> Hello.  Following pages 342-347 of Pinheiro & Bates, I am trying to
> write a self-starting nonlinear function (a non-rectagular hyperbola) to
> be used in nonlinear least squares regression (and eventually for a
> mixed model).  When I use the getInitial function for my self-starting
> function I get the following error message:

> > getInitial(photo~NRhyperbola(Irr,theta,Am,alpha,Rd),dat)
> 
> Error in tapply(y, x, mean, na.rm = TRUE) :
> 
>         arguments must have same length
> 
> Since I do not explicitly call tapply in my function that makes
> NRhyperbola a self-starting function (called NRhyperbolaInit, see
> below), I assume that the error is coming from within the mCall function
> but I can't figure out where or how.

My guess is that it is occuring in the call to sortedXyData but I
won't be able to tell for sure without test data.

One of the things that sortedXyData does is to average the y values
for replicated x values.  It seems that in the call the lengths of the
x and y arguments are different.

> 
> 
> 
> Would someone who has successfully done this be willing to look at my
> code and see where the problem arises?
> 
> 
> 
> > NRhyperbolaInit
> 
> function(mCall,LHS,data)
> 
> {
> 
> xy<-sortedXyData(mCall[["x"]],LHS,data)
> 
> if(nrow(xy)<3){
> 
>  stop("Too few unique irradiance values")
> 
> }
> 
> theta<-0.75
> 
> Rd<-min(xy[,"y"])
> 
> Am<-max(xy[,"y"]) + abs(Rd)
> 
> if(sum(xy[,"x"]<50)>3)alpha<-coef(lm(y~x,data=xy,subset=x<50))[2]
> 
> if(sum(xy[,"x"]<50)<=3)alpha<-0.07
> 
> value<-c(theta,Am,alpha,Rd)
> 
> names(value)<-mCall[c("theta","Am","alpha","Rd")]
> 
> value
> 
> }
> 
> 
> 
> Bill Shipley
> 
> Bill.Shipley at USherbrooke.ca
> 
>  <http://callisto.si.usherb.ca:8080/bshipley/>
> http://callisto.si.usherb.ca:8080/bshipley/
> 
> 
> 
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmbates at gmail.com  Thu Sep  1 17:52:19 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 1 Sep 2005 10:52:19 -0500
Subject: [R] VarCorr function for assigning random effects: was Question
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F740A0A3A38@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A3A38@dc1ex2.air.org>
Message-ID: <40e66e0b05090108525e41bff7@mail.gmail.com>

On 9/1/05, Doran, Harold <HDoran at air.org> wrote:
> If you are indeed using lme and not lmer then the needed function is
> VarCorr(). However, 2 recommendations. First, this is a busy list and
> better emails subject headers get better attention. Second, I would
> recommend using lmer as it is much faster. However, VarCorr seems to be
> incompatible with lmer and I do not know of another function to work
> with lmer.

I hope that VarCorr is compatible with lmer.  It is intended to be

> library(lme4)
Loading required package: Matrix
Loading required package: lattice
> example(VarCorr)

VarCrr> (fm2 <- lmer(Reaction ~ Days + (1 | Subject) + (0 + 
    Days | Subject), sleepstudy))
Linear mixed-effects model fit by REML
Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject) 
   Data: sleepstudy 
      AIC      BIC    logLik MLdeviance REMLdeviance
 1753.669 1769.634 -871.8346   1752.047     1743.669
Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept) 627.571  25.0514 
 Subject  Days         35.858   5.9881 
 Residual             653.584  25.5653 
# of obs: 180, groups: Subject, 18; Subject, 18

Fixed effects:
            Estimate Std. Error  DF t value  Pr(>|t|)    
(Intercept) 251.4051     6.8854 178 36.5128 < 2.2e-16 ***
Days         10.4673     1.5596 178  6.7117 2.480e-10 ***
---
Signif. codes:  0 $-1????***???? 0.001 ????**???? 0.01 ????*???? 0.05 ????.???? 0.1 ???? ???? 1 

VarCrr> VarCorr(fm2)
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept) 627.571  25.0514 
 Subject  Days         35.858   5.9881 
 Residual             653.584  25.5653 

What may have occurred is that you had the nlme package loaded after
the lme4 (actually the important package is Matrix which gets loaded
by lme4) package was loaded.  The VarCorr generic in nlme would mask
the VarCorr generic in the Matrix package.

> 
> Hence, a better email subject header will attract the attention of
> others *much* smarter than me!
> 
> I hope this helps,
> Harold
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mahmoud Torabi
> Sent: Wednesday, August 31, 2005 9:51 PM
> To: R-help at stat.math.ethz.ch
> Cc: mtorabi at math.carleton.ca
> Subject: [R] Question
> 
> Dear Sir/Madam
> 
> I would be pleased if anybody can help me. I'm using linear mixed model
> (lme) function.I'm doing some simulation in my research and need to be
> assigned variance components values during of my program. Specifically,
> when I use lme function, I can get some information by use summary() and
> I can assign some valuse like variance of fixed parameters and variance
> of random error term by using for example  varFix and sigma.But I don't
> know how I can assign for variance of random effect.
> I know in SPLUS we have command var.ran, how about R ?


The development version of the Matrix package, available at

https://svn.r-project.org/R-packages/trunk/Matrix

and soon to be Matrix_0.98-6 has a "simulate" method for lmer objects
that may be of interest to you.



From m_osm at gmx.net  Thu Sep  1 17:53:04 2005
From: m_osm at gmx.net (Mahdi Osman)
Date: Thu, 1 Sep 2005 17:53:04 +0200 (MEST)
Subject: [R] =?iso-8859-1?q?post_hoc_analysis_after_anova?=
Message-ID: <5765.1125589984@www27.gmx.net>

Dear list,

This is a simple question but I spent an hour on it without success. So I
sorry to bother you.

 I fit a linear model "lm" and then did "anova". 


My idea is to run multiple pairwise comparision for  several factor
variables (f1, f2, f3, f4 each with its own levels, say "0", "1", "3" etc
)at the same time and get statistics for each factor's levels. The
statistics I want is just the basic one such as mean standard error.

I tried the "pairwise.t.test", but I could not do what I wnated to.

I was wondering if anyone has idea about how I can accopmlish this task?


Thanks a lot for your tips and help


Greetings



Mahdi

-- 
-----------------------------------
Mahdi Osman (PhD)
E-mail: m_osm at gmx.net



From herodote at oreka.com  Thu Sep  1 17:58:40 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Thu,  1 Sep 2005 16:58:40 +0100
Subject: [R] =?iso-8859-1?q?axis_of_plot?=
Message-ID: <IM5B1S$B6F8A6EE5DEB2242A2DEF00B545B7682@oreka.com>

hy,

I need to have the 0 on the bottom left corner of the graph being joined , not with this little hole between x axis and y axis...

I've saw this question with the answer one time but i'm unable to find it again..


thks.
guillaume



From HDoran at air.org  Thu Sep  1 18:00:24 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 1 Sep 2005 12:00:24 -0400
Subject: [R] VarCorr function for assigning random effects: was Question
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A0A3AE6@dc1ex2.air.org>

You are correct, VarCorr IS compatible with lmer. It must be what you state below. I should have saved the message but VarCorr() complained that I was not dealing with an lme() object. I had multiple packages (nlme, Matrix, mlmRev, among others) in the search path. 

 
On 9/1/05, Doran, Harold <HDoran at air.org> wrote:
> If you are indeed using lme and not lmer then the needed function is 
> VarCorr(). However, 2 recommendations. First, this is a busy list and 
> better emails subject headers get better attention. Second, I would 
> recommend using lmer as it is much faster. However, VarCorr seems to 
> be incompatible with lmer and I do not know of another function to 
> work with lmer.

I hope that VarCorr is compatible with lmer.  It is intended to be



> library(lme4)
Loading required package: Matrix
Loading required package: lattice
> example(VarCorr)

VarCrr> (fm2 <- lmer(Reaction ~ Days + (1 | Subject) + (0 +
    Days | Subject), sleepstudy))
Linear mixed-effects model fit by REML
Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject) 
   Data: sleepstudy 
      AIC      BIC    logLik MLdeviance REMLdeviance
 1753.669 1769.634 -871.8346   1752.047     1743.669
Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept) 627.571  25.0514 
 Subject  Days         35.858   5.9881 
 Residual             653.584  25.5653 
# of obs: 180, groups: Subject, 18; Subject, 18

Fixed effects:
            Estimate Std. Error  DF t value  Pr(>|t|)    
(Intercept) 251.4051     6.8854 178 36.5128 < 2.2e-16 ***
Days         10.4673     1.5596 178  6.7117 2.480e-10 ***
---
Signif. codes:  0 $-1????***???? 0.001 ????**???? 0.01 ????*???? 0.05 ????.???? 0.1 ???? ???? 1 

VarCrr> VarCorr(fm2)
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept) 627.571  25.0514 
 Subject  Days         35.858   5.9881 
 Residual             653.584  25.5653 

What may have occurred is that you had the nlme package loaded after the lme4 (actually the important package is Matrix which gets loaded by lme4) package was loaded.  The VarCorr generic in nlme would mask the VarCorr generic in the Matrix package.

> 
> Hence, a better email subject header will attract the attention of 
> others *much* smarter than me!
> 
> I hope this helps,
> Harold
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mahmoud Torabi
> Sent: Wednesday, August 31, 2005 9:51 PM
> To: R-help at stat.math.ethz.ch
> Cc: mtorabi at math.carleton.ca
> Subject: [R] Question
> 
> Dear Sir/Madam
> 
> I would be pleased if anybody can help me. I'm using linear mixed 
> model
> (lme) function.I'm doing some simulation in my research and need to be 
> assigned variance components values during of my program. 
> Specifically, when I use lme function, I can get some information by 
> use summary() and I can assign some valuse like variance of fixed 
> parameters and variance of random error term by using for example  
> varFix and sigma.But I don't know how I can assign for variance of random effect.
> I know in SPLUS we have command var.ran, how about R ?


The development version of the Matrix package, available at

https://svn.r-project.org/R-packages/trunk/Matrix

and soon to be Matrix_0.98-6 has a "simulate" method for lmer objects that may be of interest to you.



From mschwartz at mn.rr.com  Thu Sep  1 18:08:45 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 01 Sep 2005 11:08:45 -0500
Subject: [R] axis of plot
In-Reply-To: <IM5B1S$B6F8A6EE5DEB2242A2DEF00B545B7682@oreka.com>
References: <IM5B1S$B6F8A6EE5DEB2242A2DEF00B545B7682@oreka.com>
Message-ID: <1125590925.4194.5.camel@localhost.localdomain>

On Thu, 2005-09-01 at 16:58 +0100, herodote at oreka.com wrote:
> hy,
> 
> I need to have the 0 on the bottom left corner of the graph being
> joined , not with this little hole between x axis and y axis...
> 
> I've saw this question with the answer one time but i'm unable to find
> it again..
> 
> 
> thks.
> guillaume

See 'xaxs' and 'yaxs' in ?par:

plot(1:5, xaxs = "i", yaxs = "i", xlim = c(0, 5), ylim = c(0, 5))

By default, both are set to "r", which adds +/- 4% to the range of each
axis.

HTH,

Marc Schwartz



From pauljohn at ku.edu  Thu Sep  1 18:51:16 2005
From: pauljohn at ku.edu (Paul Johnson)
Date: Thu, 01 Sep 2005 11:51:16 -0500
Subject: [R] controlling where *.Rout gets printed. Possible?
Message-ID: <43173184.6010901@ku.edu>

OK, my journey to make lab machines automagically install & update all 
desirable R packages is nearing an end!  The only question I have now is 
this: How can I control where the system prints the *.Rout file that is 
created automatically when the R batch program runs.  In "man R" I don't 
find any information about it.  When the cron job runs "R_installAll.sh" 
(see below), I'd like to re-direct it to someplace that ordinary users 
can read.

Here's the shell script I will schedule with cron

------------R_installAll.sh ----------
#!/bin/bash
R CMD BATCH /usr/local/bin/R_installAll.R
--------------------------------------

And here's the R program

-------------R_installAll.R-------------

# Paul Johnson <pauljohn _AT_ ku.edu> 2005-08-31
# This should update and then install all packages, except for
# ones I exclude because they don't work or we don't want them.


options(repos = "http://lib.stat.cmu.edu/R/CRAN/")

update.packages(ask=F)
theNew <- new.packages()
failPackages <- 
c("BRugs","GDD","gtkDevice","gap","gnomeGUI","mimR","ncdf","pathmix","rcdd","rgdal","rpvm",
"Rmpi","RQuantLib","RMySQL", 
"RNetCDF","RODBC","ROracle","RScaLAPACK","rsprng","RWinEdt","taskPR")

shouldFail <- theNew %in% failPackages

install.packages( theNew[!shouldFail],dependencies=T)


# VGAM is not in CRAN yet, but Zelig wants it.
# install.packages("VGAM", CRAN="http://www.stat.auckland.ac.nz/~yee");


update.packages(CRAN="http://www.stat.auckland.ac.nz/~yee")
--------------------------------------




-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From jhallman at frb.gov  Thu Sep  1 19:11:47 2005
From: jhallman at frb.gov (jhallman@frb.gov)
Date: Thu, 01 Sep 2005 13:11:47 -0400
Subject: [R] access to on.exit expressions in parent frames?
Message-ID: <20050901171147.35CAE53586@mail.rsma.frb.gov>

In R, sys.on.exit() retrieves the expression stored for use by 'on.exit' in
the function currently being evaluated.  In S, you get the list of expressions
for the current frame and its parents.  Is there any way I can do the same in
R?  If not, can this be added?  Maybe 'sys.on.exit' could take a frame number
or environment as an argument.

Jeff



From ligges at statistik.uni-dortmund.de  Thu Sep  1 19:12:34 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 19:12:34 +0200
Subject: [R] controlling where *.Rout gets printed. Possible?
In-Reply-To: <43173184.6010901@ku.edu>
References: <43173184.6010901@ku.edu>
Message-ID: <43173682.5030105@statistik.uni-dortmund.de>

Paul Johnson wrote:

> OK, my journey to make lab machines automagically install & update all 
> desirable R packages is nearing an end!  The only question I have now is 
> this: How can I control where the system prints the *.Rout file that is 
> created automatically when the R batch program runs.  In "man R" I don't 
> find any information about it.  When the cron job runs "R_installAll.sh" 
> (see below), I'd like to re-direct it to someplace that ordinary users 
> can read.

But you find help in
   R CMD BATCH --help
which tells you:
   "Usage: R CMD BATCH [options] infile [outfile]"

Hence simply call R by something like
   R CMD BATCH /usr/local/bin/R_installAll.R /path/to/R_installAll.Result


Uwe Ligges


> Here's the shell script I will schedule with cron
> 
> ------------R_installAll.sh ----------
> #!/bin/bash
> R CMD BATCH /usr/local/bin/R_installAll.R
> --------------------------------------
> 
> And here's the R program
> 
> -------------R_installAll.R-------------
> 
> # Paul Johnson <pauljohn _AT_ ku.edu> 2005-08-31
> # This should update and then install all packages, except for
> # ones I exclude because they don't work or we don't want them.
> 
> 
> options(repos = "http://lib.stat.cmu.edu/R/CRAN/")
> 
> update.packages(ask=F)
> theNew <- new.packages()
> failPackages <- 
> c("BRugs","GDD","gtkDevice","gap","gnomeGUI","mimR","ncdf","pathmix","rcdd","rgdal","rpvm",
> "Rmpi","RQuantLib","RMySQL", 
> "RNetCDF","RODBC","ROracle","RScaLAPACK","rsprng","RWinEdt","taskPR")
> 
> shouldFail <- theNew %in% failPackages
> 
> install.packages( theNew[!shouldFail],dependencies=T)
> 
> 
> # VGAM is not in CRAN yet, but Zelig wants it.
> # install.packages("VGAM", CRAN="http://www.stat.auckland.ac.nz/~yee");
> 
> 
> update.packages(CRAN="http://www.stat.auckland.ac.nz/~yee")
> --------------------------------------
> 
> 
> 
>



From cobleigh at gmail.com  Thu Sep  1 19:35:13 2005
From: cobleigh at gmail.com (Jamieson Cobleigh)
Date: Thu, 1 Sep 2005 13:35:13 -0400
Subject: [R] Spacing and margins in lattice...
In-Reply-To: <43171AC8.4060308@pdf.com>
References: <7f50836c050901072912cd925f@mail.gmail.com>
	<43171AC8.4060308@pdf.com>
Message-ID: <7f50836c0509011035421df9b@mail.gmail.com>

That did the trick.

Thanks!

Jamie

On 9/1/05, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> 
> 
> Jamieson Cobleigh wrote:
> > Similar to my last question, I want to tighten up the spacing and
> > margins in a plot I am doing with lattice.
> >
> > Here are the commands I'm using:
> >
> > data <- data.frame(x=c(1:3, 1:3), y=c(1:3, 1:3*2),
> > cat=c("foo","foo","foo","bar", "bar","bar"))
> >
> > xyplot(panel=panel.superpose, y~x, data=data, groups=cat, type="b",
> > scales=list(tck=c(2,0), axs="r", cex=c(1,0)))
> >
> > I know that par(mar=c(2,2,1,1)) would do what I want with plot.  Is
> > there something similar for xyplot/lattice that can reduce the size of
> > the margins of the plot?
> >
> > Thanks!
> >
> > Jamie
> >
> 
> Hi, Jamie,
> 
> Will the following work for you?
> 
> library(lattice)
> data <- data.frame(x = c(1:3, 1:3), y = c(1:3, 1:3*2),
>                     cat = c("foo","foo","foo","bar", "bar","bar"))
> 
> trellis.par.set(theme = col.whitebg())
> 
> lw <- list(left.padding = list(x = 0, units = "inches"))
> lw$right.padding <- list(x = -0.1, units = "inches")
> lh <- list(bottom.padding = list(x = 0, units = "inches"))
> lh$top.padding <- list(x = -0.2, units = "inches")
> 
> lattice.options(layout.widths = lw, layout.heights = lh)
> xyplot(y ~ x, data, groups = cat, type="b")
> 
> HTH,
> 
> --sundar
>



From drakegis at dacafe.com  Thu Sep  1 11:44:26 2005
From: drakegis at dacafe.com (DrakeGis)
Date: Thu, 1 Sep 2005 10:44:26 +0100 (PDT)
Subject: [R] SpatStat Kest -  Error Message help
Message-ID: <2506.141.211.76.26.1125596666.squirrel@cafemail.edacafe.com>

Hi I'm working with the function Kest in the package SpatStat (under LINUX
with R 2.1.0). In order to evaluate the statistical significance of my
point pattern I'm doing 999 Montecarlo replications. The script that use
the Kest function runs OK for most of the different point patterns that I
have but for a particular point pattern, which have only 17 points, it
runs until the 34th iteration and then I receive this message:

Error in "[<-"(`*tmp*`, index, value = NULL) :
	incompatible types (1000) in subassignment type fix
Execution halted

  Do you have any idea about what could be the cause of this ? Thanks in
advance

        D.






-----------------------------------------
Stay ahead of the information curve.
Receive EDA news and jobs on your desktop daily.
Subscribe today to the EDA CafeNews newsletter.
[ http://www10.edacafe.com/nl/newsletter_subscribe.php ]
It's informative and essential.



From JZajd at constellagroup.com  Thu Sep  1 19:46:58 2005
From: JZajd at constellagroup.com (Zajd, John)
Date: Thu, 1 Sep 2005 13:46:58 -0400 
Subject: [R]  Strange build message: request help w/resolving
Message-ID: <1BF5A584BBD24645A0524FA419524BCB0C9C6630@banyan.constellagroup.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050901/0e2a3312/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Sep  1 20:03:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Sep 2005 20:03:45 +0200
Subject: [R] Strange build message: request help w/resolving
In-Reply-To: <1BF5A584BBD24645A0524FA419524BCB0C9C6630@banyan.constellagroup.com>
References: <1BF5A584BBD24645A0524FA419524BCB0C9C6630@banyan.constellagroup.com>
Message-ID: <43174281.4010902@statistik.uni-dortmund.de>

Zajd, John wrote:

> Greetings,
>  
> I am attempting to build a R package, however the build fails and the
> following message is ouput:
>  
> C:\ConstellaGroup\EPA\RAGG\package>R CMD BUILD --binary --force RAGG
> * checking for file 'RAGG/DESCRIPTION' ... OK
> * preparing 'RAGG':
> * checking DESCRIPTION meta-information ... ERROR
> Error in if (regexpr(dep_regexp, dep) == -1) { :
>         missing value where TRUE/FALSE needed
> Execution halted
> 
> 
> The DESCRIPTION file is as follows:
> 
> Package: RAGG
> 
> Title: Aggregation Tool for CMAQ gridded model data
> 
> Version: 1.5
> 
> Date: 2004-08-23
> 
> Author: Nathan Shackles <nshackles at constellagroup.com>, ported from SAS code
> written by Renee Jaramillo <rjaramillo at constellagroup.com>
> 
> Description: The RAGG package performs aggregation of gridded CMAQ model
> data. The pacakge reads event data from Models3 IO-API files and outputs the
> aggregated annual data in Models3 IO-API file(s). 
> 
> Aggegation came about as a means to provide CMAQ-based estimates of total
> annual deposition and average annual concentrations. CMAQ is an episodic
> model requiring detailed meteorological and emissions inputs. Because of the
> extensive inputs and the time required to run CMAQ, it would be
> cost-prohibitive to simulate an entire year to estimate annual quantities.
> Aggregation formulas group events into clusters with similar 850-mb windflow
> patters to create annual estimates.
> 
> Maintainer: Nathan Shackles <nshackles at constellagroup.com>
> 
> Depends: Models3
> 
> License: GPL version 2 or newer
> 
> Any help that can be provided is appreciated.

One obvious point:
Continuation lines should begin with a white space (such as a tab). See 
the Writing R Extensions manual for more details.

Uwe Ligges


> Thank you,
> 
> John Zajd
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From deepayan.sarkar at gmail.com  Thu Sep  1 20:43:48 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 1 Sep 2005 13:43:48 -0500
Subject: [R] label *on the side* in conditional lattice plots?
In-Reply-To: <78e6ba3105083113215cb130e1@mail.gmail.com>
References: <78e6ba3105083113215cb130e1@mail.gmail.com>
Message-ID: <eb555e6605090111432ce5a2d8@mail.gmail.com>

On 8/31/05, Maciej Kalisiak <mkalisiak at gmail.com> wrote:
> I'm doing bwplot(x ~ y | z, ...) with lattice, but would like the
> z-labels to appear to the *side* of each bwplot, rather than on top...
> is this possible?  

Not currently (it's on my TODO list).

Deepayan



From br44114 at gmail.com  Thu Sep  1 20:54:10 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 1 Sep 2005 14:54:10 -0400
Subject: [R] Linux Standalone Server Suggestions for R
Message-ID: <8d5a3635050901115470119d24@mail.gmail.com>

Most powerful in what way? Quite a lot depends on the jobs you're going to run.
	- To run CPU-bound jobs, more CPUs is better. (Even though R doesn't
do threading, you can manually split some CPU-bound jobs in several
parts and run them simultaneously.) Apart from multiple CPUs and
hyperthreading, check the new dual-core CPUs.
	- To run very large jobs, more memory is better. You can easily spend
most of your money on memory. Get the fastest one.
	- You should get 64-bit CPUs, otherwise you won't be able to run very
large jobs (search the list for details).

I would suggest that you buy a configuration that can handle more CPUs
and memory than you think you need now (say, at least 4 max CPUs and
16 GB max memory), then keep on adding more memory and CPUs as your
needs change.
hth,
b.


> -----Original Message-----
> From: Jia-Shing So [mailto:jiso at ucsd.edu] 
> Sent: Wednesday, August 31, 2005 10:03 PM
> To: r-help at stat.math.ethz.ch
> Cc: Phuoc Hong
> Subject: [R] Linux Standalone Server Suggestions for R
> 
> 
> Hi All,
> 
> My group is  looking for any suggestions on what to purchase to  
> achieve the most powerful number crunching system that $50k 
> can buy.   
> The main application that will be used is R so input on what 
> hardware  
> benefits R most will be appreciated.  The requirements are 
> that it be  
> a single standalone server (i.e. not a cluster solution), and 
> it that  
> must be able to run unix/linux.  If anyone has any experience/ 
> suggestions regarding the following questions that would also be  
> greatly appreciated.
> 
> AMD vs Intel chips, especially 64-bit versions of the two?
> Using Itanium/Opterons and if so how much of a performance boost did  
> you achieve vs other 64-bit chip sets?
> Also, does anyone know if there is an upper thresh hold on much  
> memory R can use?
> 
> Thanks in advance for any help and suggestions,
> 
> Jia-Shing So
> Programmer Analyst
> Biostatistics and Bioinformatics Lab
> University of California, San Diego
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From admin at biostatistic.de  Thu Sep  1 21:51:27 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Thu, 01 Sep 2005 21:51:27 +0200
Subject: [R] png scaling problem
Message-ID: <43175BBF.4040907@biostatistic.de>

scaling<-4
xywidth<-480
resolution<-150
png(filename = "c:/r/anschluss/plots/4.png", width = xywidth*scaling, 
height = xywidth*scaling,pointsize = 12, bg = "white", res = 
resolution*scaling)
......

barplot(xrow,col = barcolors,cex.axis=scaling, ylab="mean time till attachment in sec",cex.lab=1.2*scaling) 

I tried to scale the barplot but there is one strange result:
scaling=1
http://biostatistic.de/temp/1.png    --- the ylab is ok

scaling=2
http://biostatistic.de/temp/2.png    --- the ylab is not ok

scaling=4
http://biostatistic.de/temp/4.png    --- the ylab is terrible

is there any better solution to scale the resolution and the width/height?


with regards
Knut



From mschwartz at mn.rr.com  Thu Sep  1 22:53:15 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 01 Sep 2005 15:53:15 -0500
Subject: [R] png scaling problem
In-Reply-To: <43175BBF.4040907@biostatistic.de>
References: <43175BBF.4040907@biostatistic.de>
Message-ID: <1125607996.1898.50.camel@localhost.localdomain>

On Thu, 2005-09-01 at 21:51 +0200, Knut Krueger wrote:
> scaling<-4
> xywidth<-480
> resolution<-150
> png(filename = "c:/r/anschluss/plots/4.png", width = xywidth*scaling, 
> height = xywidth*scaling,pointsize = 12, bg = "white", res = 
> resolution*scaling)
> ......
> 
> barplot(xrow,col = barcolors,cex.axis=scaling, ylab="mean time till attachment in sec",cex.lab=1.2*scaling) 
> 
> I tried to scale the barplot but there is one strange result:
> scaling=1
> http://biostatistic.de/temp/1.png    --- the ylab is ok
> 
> scaling=2
> http://biostatistic.de/temp/2.png    --- the ylab is not ok
> 
> scaling=4
> http://biostatistic.de/temp/4.png    --- the ylab is terrible
> 
> is there any better solution to scale the resolution and the width/height?
> 
> 
> with regards
> Knut

Probably a better first question is, why are you using a bitmapped
graphics format if you need the image to be re-scaled? In general,
bitmapped graphics do not resize well, though if you have a specific
need and know a target image size, you can adjust various parameters to
look decent. Are you going to view these images in a web browser, where
you are concerned with display size and resolution?

>From your e-mail headers it appears you are on Windows. If you need a
re-sizable graphic, use a vector based format such as wmf/emf,
especially if you need the graphics embedded in a Windows application
such as Word or Powerpoint. This is the default format under Windows
when copying and pasting a graphic between applications. You can then,
fairly freely, resize the image in the target application as you may
require.

If you are going to print the graphic directly or include it in a
document for printing (as opposed to just viewing it), then use PDF or
Postscript. The latter in EPS format, can be imported into many Windows
applications like Word, including the generation of a preview image.
However, they don't look good for direct use in presentations (unless
you print to a PS file and then convert to PDF for viewing).

See ?Devices for more information.

With a better idea of how you plan to use the graphic(s), we can offer
more specific recommendations on how to proceed.

Marc Schwartz



From ivo_welch-rstat8303 at mailblocks.com  Thu Sep  1 22:57:05 2005
From: ivo_welch-rstat8303 at mailblocks.com (ivo_welch-rstat8303@mailblocks.com)
Date: Thu, 01 Sep 2005 13:57:05 -0700
Subject: [R] transparency?
Message-ID: <200509012057.j81Kv70l009355@hypatia.math.ethz.ch>


dear R wizards:

I am getting to play more and more with fun fonts (irony warning).  I 
now know that I can safely use my TeXtext encoding with the postscript 
device, but not with the pdf device.  Unfortunately, I believe that the 
postscript device does not support translucent colors---or is there a 
version parameter (like the version="1.4" that I need with the pdf 
device) that would permit the use of translucent colors?

Right now, it seems that my choice either TeX encoding or translucence? 
  [there turns out to be another reason why I prefer the pdf to the 
postscript device.  ps2pdf13 seems to lose the bounding box, at least 
sometimes.  it seems to like to create one page and one page only.]

And let me add---thank you very mcuh paul murrell and brian ripley for 
helping me figure out how to get the lucida fonts to work under R.  
highly appreciated.

Regards,

/iaw

---
ivo welch


Suggestion to the R graphics folks:

* Why is R's default device pdf v1.1 ?  this was Acrobat version 2.0, 
isn't it?  we are now at version 7.  wouldn't it be useful to make the 
default 1.4 at this point?  acrobat version 5.0 would seem a reasonable 
default assumption these days.

* There is a bug (R segfaults) when afm files are not found.

* When something does not work, such as encoding that is not supported, 
it would be nice to print a warning.

       --- I should also email this to the ghostscript 7.07 folks.  If 
there is no Fontmap, they silently replace fonts.
             It is wonderful that they do replace, but they should print 
a warning " Font xyz not found, thus replacing
            it with Font abs."

The lack of good errors made it doubly hard to figure out the workings, 
and without the help of the aforementioned two angels, I would not have 
had a chance to figure it out.



From Mike.Prager at noaa.gov  Thu Sep  1 23:01:55 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Thu, 01 Sep 2005 17:01:55 -0400
Subject: [R] Request: in savePlot, type="eps"
Message-ID: <43176C43.5030309@noaa.gov>

Platform: Windows
R version: 2.1.1

This is a request that in the savePlot function, type="eps" be included 
in the next release. This would be an alias for type="ps" but with a 
different file extension.

Rationale:  The current version of R outputs excellent EPS files.  It is 
strictly correct to call these Postscript files, as EPS is a subset of 
PS.  However, on Windows, files are often distinguished by the file 
extension, and users may have different applications associated with the 
different extensions.  It would be nice to be able to save EPS files 
with the EPS extension.

I realize that this can be accomplished with

file.rename(paste(fn,".ps",sep=""), paste(fn,".eps",sep=""))

but saving directly would be more convenient.

Thanks for considering it.

...Mike Prager



From BPikouni at CNTUS.JNJ.COM  Thu Sep  1 23:18:50 2005
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Thu, 1 Sep 2005 17:18:50 -0400 
Subject: [R] Linux Standalone Server Suggestions for R
Message-ID: <A89517C7FD248040BB71CA3C04C1ACBB01990363@CNTUSMAEXS4.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050901/b5f234a3/attachment.pl

From JZajd at constellagroup.com  Thu Sep  1 23:45:38 2005
From: JZajd at constellagroup.com (Zajd, John)
Date: Thu, 1 Sep 2005 17:45:38 -0400 
Subject: [R]  Need help understanding/resolving build error message
Message-ID: <1BF5A584BBD24645A0524FA419524BCB0CA05ACE@banyan.constellagroup.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050901/d52f527e/attachment.pl

From d.scott at auckland.ac.nz  Thu Sep  1 23:54:38 2005
From: d.scott at auckland.ac.nz (David Scott)
Date: Fri, 2 Sep 2005 09:54:38 +1200 (NZST)
Subject: [R] Multivariate Skew Normal distribution
In-Reply-To: <XFMail.050901150246.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050901150246.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0509020943300.30010@stat12.stat.auckland.ac.nz>

On Thu, 1 Sep 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 01-Sep-05 Caio Lucidius Naberezny Azevedo wrote:
>> Hi all,
>>
>> Could anyone tell me if there is any package (or function) that
>> generates values from a multivariate skew normal distribution?
>>
>> Thanks all,
>>
>> Caio
>
> Hello, Caio
>
> Please tell us what you mean by "skew normal distribution".
>
> Since normal (i.e. gaussian) distributions are not skew, you
> presumably mean something different from what you said, so
> unless we understand this more clearly  it is unlikely that
> anyone can make a suggestion which would meet your needs.
>
Actually Ted it is perfectly clear. There are modifications of the normal 
(more than one approach) which incorporate skewness.

There is a very nice package by Azzellini called sn which deals with this 
and with the skew t.

A different version of the skew t is given in the package skewt. This is 
univariate only.

There also appear to be some skew normal and skew t tools in RMetrics (in 
FSeries and fPortfolio).

What is not clear is why Caio keeps asking the same question when I 
emailed him the name of the package yesterday.

David Scott
_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From gunter.berton at gene.com  Fri Sep  2 01:08:58 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 1 Sep 2005 16:08:58 -0700
Subject: [R] More block diagonal matrix construction code
Message-ID: <200509012309.j81N8wtd010095@volta.gene.com>

Folks:

In answer to a query, Andy Liaw recently submitted some code to construct a
block diagonal matrix. For what seemed a fairly straightforward task, the
code seemed a little "overweight" to me (that's an American stock analyst's
term, btw), so I came up with a slightly cleaner version (with help from
Andy):

bdiag<-function(...){
	mlist<-list(...)
	## handle case in which list of matrices is given
	if(length(mlist)==1)mlist<-unlist(mlist,rec=FALSE)
	csdim<-rbind(c(0,0),apply(sapply(mlist,dim),1,cumsum ))
	ret<-array(0,dim=csdim[length(mlist)+1,])
	add1<-matrix(rep(1:0,2),nc=2)
	for(i in seq(along=mlist)){
	    indx<-apply(csdim[i:(i+1),]+add1,2,function(x)x[1]:x[2])
	      ## non-square matrix
		if(is.null(dim(indx)))ret[indx[[1]],indx[[2]]]<-mlist[[i]]
		    ## square matrix
		else ret[indx[,1],indx[,2]]<-mlist[[i]]
	    }
	ret
	}

I doubt that there's any noticeable practical performance difference, of
course.

The strategy is entirely basic: just get the right indices for replacement
of the arguments into a matrix of 0's of the right dimensions. About the
only thing to notice is that the apply() construction returns either a list
or matrix depending on whether a matrix is square or not (a subtlety that
tripped me up in my first version of this code).

I would be pleased if this stimulated others to come up with cleverer/more
elegant approaches that they would share, as it's the sort of thing that
I'll learn from and find useful.

Cheers to all,

Bert Gunter



From kziendazheng at hotmail.com  Fri Sep  2 02:24:50 2005
From: kziendazheng at hotmail.com (kziendazheng@hotmail.com)
Date: Fri, 2 Sep 2005 02:24:50 +0200
Subject: [R]
	=?gb2312?B?OdTCMTXI1cew16Ky4bPJzqpTT0hPyczO8c3408O7pyzSu8LJw+K30cn9vLbOqtX9yr274dSxLLKi
Message-ID: <20050902082449.BE4CD1594013@lziendazheng>

y8011Koh?=
Date: Fri, 02 Sep 2005 08:24:49 +0800
MIME-Version: 1.0
Content-Type: multipart/alternative;
    boundary="----=_NextPart_001_0018_01C39816.2C871AA0"
X-Priority: 3
X-MSMail-Priority: Normal
X-Mailer: Microsoft Outlook Express 6.00.2800.1106
X-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1106

This is a multi-part message in MIME format.

------=_NextPart_001_0018_01C39816.2C871AA0
Content-Type: text/plain;
    charset="gb2312"
Content-Transfer-Encoding: base64

usPP+8+iITnUwjE1yNXHsNeisuGzyc6qU09IT8nMzvHN+NPDu6cs0rvCycPit9HJ/by2zqrV/cq9
u+HUsSyyosvNNdSqISC6z7eo0NTIz9akOrn6vNLQxc+isvrStbK/yM/WpNDtv8nX1rrFILumSUNQ
sbgwNTAzODIwMbrFo6zD4rfRvNPI63K1xNeisuHN+Na3o7podHRwOi8vd3d3LjExMXN0LmNvbS9z
b2hvL2luZGV4LmFzcD9pZD1nemdz

------=_NextPart_001_0018_01C39816.2C871AA0
Content-Type: text/html;
    charset="gb2312"
Content-Transfer-Encoding: base64

PCFET0NUWVBFIEhUTUwgUFVCTElDICItLy9XM0MvL0RURCBIVE1MIDQuMCBUcmFuc2l0aW9uYWwv
L0VOIj4NCjxIVE1MPjxIRUFEPg0KPE1FVEEgaHR0cC1lcXVpdj1Db250ZW50LVR5cGUgY29udGVu
dD0idGV4dC9odG1sOyBjaGFyc2V0PWdiMjMxMiI+DQo8TUVUQSBjb250ZW50PSJNU0hUTUwgNi4w
MC4yODAwLjE0NzYiIG5hbWU9R0VORVJBVE9SPg0KPFNUWUxFPjwvU1RZTEU+DQo8L0hFQUQ+DQo8
Qk9EWSBiZ0NvbG9yPSNmZmZmZmY+PEZPTlQgc2l6ZT0yPrrDz/vPoiE51MIxNcjVx7DXorLhs8nO
qlNPSE/JzM7xzfjTw7unLNK7wsnD4rfRyf28ts6q1f3Kvbvh1LEssqLLzTXUqiEgus+3qNDUyM/W
pDq5+rzS0MXPorL60rWyv8jP1qTQ7b/J19a6xSC7pklDULG4MDUwMzgyMDG6xaOsw+K30bzTyOty
tcTXorLhzfjWt6O6aHR0cDovL3d3dy4xMTFzdC5jb20vc29oby9pbmRleC5hc3A/aWQ9Z3pnczwv
Rk9OVD4NCjxESVY+PEZPTlQgc2l6ZT0yPjwvRk9OVD4mbmJzcDs8L0RJVj48L0JPRFk+PC9IVE1M
Pg==

------=_NextPart_001_0018_01C39816.2C871AA0--



From spencer.graves at pdf.com  Fri Sep  2 04:17:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 01 Sep 2005 19:17:01 -0700
Subject: [R] Pseudo-Voigt fit
In-Reply-To: <OFC04C1191.ECEE8B3E-ON85257069.0067EF28-85257069.0069294E@notes.cc.sunysb.edu>
References: <OFC04C1191.ECEE8B3E-ON85257069.0067EF28-85257069.0069294E@notes.cc.sunysb.edu>
Message-ID: <4317B61D.1080909@pdf.com>

	  I haven't seen a reply to this question, so I will attempt a few 
remarks in spite of some confusion about what you are asking.

	  1.  The function to use for parameter estimation depends on ths 
structure of the data.  My all-around preference for many purposes is 
for "optim", but I've used "nls", "fitdistr" (in the MASS package) and 
others in different circumstances.

	  2.  If you are doing nonlinear estimation with, e.g., optim, I 
suggest you request "hessian=TRUE".  The eigenvalues of the hessian will 
tell you if it is ill conditioned.  If it is, you might consider 
reparameterizing the model.

	  3.  I try to avoid using reserved words like "c".  R can often 
determine what you want from the context, but there are exceptions.  I 
try to avoid that problem by testing a name at a command prompt before I 
use it.  If it returns, "object not found", I'm fine;  if not, I try 
something different.

	  4.  Following the posting guide! 
"http://www.R-project.org/posting-guide.html" can on average increase 
the likelihood that you will receive helpful suggestions quickly.  (I've 
learned that people rarely respond to my incoherent screams;  when they 
do, it's rarely helpful.  I've reluctantly learned that there is often 
no substutute for reading the *#@%* manual.)

	  I'd be shocked if this answered your question, but I hope it is 
helpful nonetheless.
	
	  spencer graves

ppancoska at notes.cc.sunysb.edu wrote:

> Hi, I am sorry for this question, but I am trying to speed up an
> application....
> I will need to fit many x-y data sets (input from text files) to
> 4-parameter Pseudo-Voigt peak function.
> Until now I used SigmaPlot macro to do it (enclosed just in case...)
> 
> peaksign(q) = if(total(q)>q[1], 1, -1)
> xatymin(q,r) = xatymax(q,max(r)-r)
> [Parameters]
> a = if(peaksign(y)>0, max(y), min(y)) ''Auto {{previous: 60.8286}}
> b = fwhm(x,abs(y))/2 ''Auto {{previous: 0.656637}}
> c = .5 ''Auto {{previous: 6.82973e-010}}
> x0 = if(peaksign(y)>0, xatymax(x,y), xatymin(x,y)) ''Auto {{previous:
> 3.19308}}
> 
> 
> [Equation]
> f = a*(c*(1/(1+((x-x0)/b)^2))+(1-c)*exp(-0.5*((x-x0)/b)^2))
> 
> fit f to y
> 
>  (manageable for ~100), but it looks like the next project would need to
> process ~1000 member sets.
> 
> I am not as familiar with R to find the right info (although I can use R in
> general).
> 
> I am also nearly sure that there should be a solution to this task "out
> there" ready to be modified...
> 
> Could you be so kind and direct me please to the right package or web-site
> with examples?
> 
> Thank you very much
> 
> 
> 
> Dr. Petr Pancoska
> Department of Pathology
> SUNY Stony Brook, NY 11794
> phone:          (631)-444-3030
> 
> ******************************************************************************
> 
> This e- mail message, including any attachments,
> is for the sole use of the intended recipient(s) and may
> contain confidential and privileged information.
> Any unauthorized review, use, disclosure or distribution is prohibited.
> If you are not the intended recipient, please contact the sender
> by e-mail and destroy all copies of the original.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Fri Sep  2 04:29:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 01 Sep 2005 19:29:52 -0700
Subject: [R] Defining an ex-gaussian PDF
In-Reply-To: <C04F70B0-91E8-464B-82BA-1C18A3690D6B@virginia.edu>
References: <mailman.10.1125136801.20354.r-help@stat.math.ethz.ch>
	<C04F70B0-91E8-464B-82BA-1C18A3690D6B@virginia.edu>
Message-ID: <4317B920.5070802@pdf.com>

	  1.  I just got 10 hits from 'RSiteSearch("convolution of exponential 
and normal")', at least two of which look like they might interest you.

	  2.  I similarly got 27 hits from 'RSiteSearch("convolution 
distribution")', several of which mention the "distr" package, which you 
might find useful.

	  3.  I doubt if this solves your problem, but I hope you find it 
useful.  If you have other questions for this list, I encourage you to 
PLEASE do read the posting guide! 
"http://www.R-project.org/posting-guide.html".  It can increase the 
chances that you will get a useful reply quickly.

	  spencer graves

Michael Kubovy wrote:

> How does one define PDFs as yet undefined in R, such as the ex- 
> gaussian, the sum of two RVs, one exponential, one Gaussian? The PDF  
> would then be the convolution of an exponential PDF, dexp(), and a  
> normal, dnorm().
> 
> Kindly cc me in your reply to r-help.
> 
> Thanks,
> 
> 
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From 0034058 at fudan.edu.cn  Fri Sep  2 04:43:41 2005
From: 0034058 at fudan.edu.cn (0034058@fudan.edu.cn)
Date: Fri, 02 Sep 2005 10:43:41 +0800
Subject: [R] partial association model
Message-ID: <186d7de18754eb.18754eb186d7de@fudan.edu.cn>

my last post was filtered,so I post it again with another title.

If I do not make a mistake,the partial association model is an 
extension of log-linear model.I read a papers which gives an example 
of it.(Sloane and Morgan,1996,An Introduction to Categorical Data 
Analysis,Annual Review of Sociology.22:351-375) Can R fit such partial 
association model?

ps:Another somewhat off-topic question.What's the motivations to use 
log-linear model?Or why use log-linear model?I learn the log-linear 
model butI still do not master the the advantage of the model.thank 
you!



From admin at biostatistic.de  Fri Sep  2 07:10:38 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Fri, 02 Sep 2005 07:10:38 +0200
Subject: [R] png scaling problem
In-Reply-To: <1125607996.1898.50.camel@localhost.localdomain>
References: <43175BBF.4040907@biostatistic.de>
	<1125607996.1898.50.camel@localhost.localdomain>
Message-ID: <4317DECE.3030003@biostatistic.de>




>Probably a better first question is, why are you using a bitmapped
>graphics format if you need the image to be re-scaled? 
>
I need a 1000 dpi tif file in a size of appr. 10 to 10 cm for applied 
animal behaviour science:
http://authors.elsevier.com/GuideForAuthors.html?PubID=503301&dc=GFA

images to one of the following formats (Note the resolution requirements 
for line drawings, halftones, and
line/halftone combinations given below.):
EPS: Vector drawings. Embed the font or save the text as "graphics".
TIFF: Colour or greyscale photographs (halftones): always use a minimum 
of 300 dpi.
TIFF: Bitmapped line drawings: use a minimum of 1000 dpi.
TIFF: Combinations bitmapped line/half-tone (colour or greyscale): a 
minimum of 500 dpi is required.
DOC, XLS or PPT: If your electronic artwork is created in any of these 
Microsoft Office applications please
supply "as is".

I tired the Postscript file but the file is double heigh as width i do 
not know why.
The problem was already discussed in the tread: [R] High resolution plots

I have to send the images possibly yesterday and I am looking fo a 
suitable solution since two months now.
I tired gsview with converting to all possible Tiff formats but the 
images appear not in color and in a strange black and white way
Some readers are able to read it  (Windows Image view) other not and I 
do not know which reader the journal will use :-(
And the ylab is too small ...

http://biostatistic.de/temp/1.tif 
http://biostatistic.de/temp/2.tif 
http://biostatistic.de/temp/3.tif 
http://biostatistic.de/temp/4.tif 





>In general,
>bitmapped graphics do not resize well, though if you have a specific
>need and know a target image size, you can adjust various parameters to
>look decent. Are you going to view these images in a web browser, where
>you are concerned with display size and resolution?
>
>>From your e-mail headers it appears you are on Windows. If you need a
>re-sizable graphic, use a vector based format such as wmf/emf,
>especially if you need the graphics embedded in a Windows application
>such as Word or Powerpoint. This is the default format under Windows
>when copying and pasting a graphic between applications. You can then,
>fairly freely, resize the image in the target application as you may
>require.
>
>If you are going to print the graphic directly or include it in a
>document for printing (as opposed to just viewing it), then use PDF or
>Postscript.
>

Ok there is a second description for the file format :-(
http://authors.elsevier.com/ArtworkInstructions.html?dc=AI2
there are pdf formats welcome but with defined conditions:

Maybe anybody could give me a hint to get the files in the recommendet 
format.
I will ask them immediately which whether the pdf is allowed or not, 
becaus they have two different instruction sites :-(

Regards Knut



From ggrothendieck at gmail.com  Fri Sep  2 08:03:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Sep 2005 02:03:41 -0400
Subject: [R] png scaling problem
In-Reply-To: <4317DECE.3030003@biostatistic.de>
References: <43175BBF.4040907@biostatistic.de>
	<1125607996.1898.50.camel@localhost.localdomain>
	<4317DECE.3030003@biostatistic.de>
Message-ID: <971536df050901230363bc2a8a@mail.gmail.com>

If you have not already tried it try creating a fig file:

xfig("myfile.fig")
plot(1:10)
dev.off()

and then using the fig2dev utility (find it via google) to convert it to a tiff:

fig2dev -L tiff myfile.fig > myfile.tiff


On 9/2/05, Knut Krueger <admin at biostatistic.de> wrote:
> 
> 
> 
> >Probably a better first question is, why are you using a bitmapped
> >graphics format if you need the image to be re-scaled?
> >
> I need a 1000 dpi tif file in a size of appr. 10 to 10 cm for applied
> animal behaviour science:
> http://authors.elsevier.com/GuideForAuthors.html?PubID=503301&dc=GFA
> 
> images to one of the following formats (Note the resolution requirements
> for line drawings, halftones, and
> line/halftone combinations given below.):
> EPS: Vector drawings. Embed the font or save the text as "graphics".
> TIFF: Colour or greyscale photographs (halftones): always use a minimum
> of 300 dpi.
> TIFF: Bitmapped line drawings: use a minimum of 1000 dpi.
> TIFF: Combinations bitmapped line/half-tone (colour or greyscale): a
> minimum of 500 dpi is required.
> DOC, XLS or PPT: If your electronic artwork is created in any of these
> Microsoft Office applications please
> supply "as is".
> 
> I tired the Postscript file but the file is double heigh as width i do
> not know why.
> The problem was already discussed in the tread: [R] High resolution plots
> 
> I have to send the images possibly yesterday and I am looking fo a
> suitable solution since two months now.
> I tired gsview with converting to all possible Tiff formats but the
> images appear not in color and in a strange black and white way
> Some readers are able to read it  (Windows Image view) other not and I
> do not know which reader the journal will use :-(
> And the ylab is too small ...
> 
> http://biostatistic.de/temp/1.tif
> http://biostatistic.de/temp/2.tif
> http://biostatistic.de/temp/3.tif
> http://biostatistic.de/temp/4.tif
> 
> 
> 
> 
> 
> >In general,
> >bitmapped graphics do not resize well, though if you have a specific
> >need and know a target image size, you can adjust various parameters to
> >look decent. Are you going to view these images in a web browser, where
> >you are concerned with display size and resolution?
> >
> >>From your e-mail headers it appears you are on Windows. If you need a
> >re-sizable graphic, use a vector based format such as wmf/emf,
> >especially if you need the graphics embedded in a Windows application
> >such as Word or Powerpoint. This is the default format under Windows
> >when copying and pasting a graphic between applications. You can then,
> >fairly freely, resize the image in the target application as you may
> >require.
> >
> >If you are going to print the graphic directly or include it in a
> >document for printing (as opposed to just viewing it), then use PDF or
> >Postscript.
> >
> 
> Ok there is a second description for the file format :-(
> http://authors.elsevier.com/ArtworkInstructions.html?dc=AI2
> there are pdf formats welcome but with defined conditions:
> 
> Maybe anybody could give me a hint to get the files in the recommendet
> format.
> I will ask them immediately which whether the pdf is allowed or not,
> becaus they have two different instruction sites :-(
> 
> Regards Knut
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dieter.menne at menne-biomed.de  Fri Sep  2 08:12:01 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 2 Sep 2005 06:12:01 +0000 (UTC)
Subject: [R] post hoc analysis after anova
References: <5765.1125589984@www27.gmx.net>
Message-ID: <loom.20050902T081056-941@post.gmane.org>

Mahdi Osman <m_osm <at> gmx.net> writes:

> 
>  I fit a linear model "lm" and then did "anova". 
> 
> My idea is to run multiple pairwise comparision for  several factor

Package multcomp helps you here, and also protects you against overdoing it.

Dieter



From wuming.gong at gmail.com  Fri Sep  2 08:49:47 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Fri, 2 Sep 2005 14:49:47 +0800
Subject: [R] Calculating Goodman-Kurskal's gamma using delta method
Message-ID: <b428d06d050901234963905521@mail.gmail.com>

Dear list, 

I have a problem on calculating the standard error of
Goodman-Kurskal's gamma using delta method. I exactly follow the
method and forumla described in Problem 3.27 of Alan Agresti's
Categorical Data Analysis (2nd edition). The data I used is also from
the job satisfaction vs. income example from that book.

job <- matrix(c(1, 3, 10, 6, 2, 3, 10, 7, 1, 6, 14, 12, 0, 1, 9, 11),
nrow = 4, ncol = 4, byrow = TRUE, dimnames = list(c("< 15,000",
"15,000 - 25,000", "25,000 - 40,000", "> 40,000"), c("VD", "LD", "MS",
"VS")))

The following code is for calculating gamma value, which is consistent
with the result presented in section 2.4.5 of that book.

C <- 0
D <- 0
for (i in 1:nrow(job)){
	for (j in 1:ncol(job)){
		pi_c <- 0
		pi_d <- 0
		for (h in 1:nrow(job)){
			for (k in 1:ncol(job)){
				if ((h > i & k > j) | (h < i & k < j)){
					pi_c <- pi_c + job[h, k]/sum(job)
				}

				if ((h > i & k < j) | (h < i & k > j)){
					pi_d <- pi_d + job[h, k]/sum(job)
				}
			}
		}

		C <- C + job[i, j] * pi_c
		D <- D + job[i, j] * pi_d
	}
}
gamma <- (C - D) / (C + D) # gamma = 0.221 for this example.

The following code is for calculating stardard error of gamma.
sigma.squared <- 0
for (i in 1:nrow(job)){
	for (j in 1:ncol(job)){
		pi_c <- 0
		pi_d <- 0
		for (h in 1:nrow(job)){
			for (k in 1:ncol(job)){
				if ((h > i & k > j) | (h < i & k < j)){
					pi_c <- pi_c + job[h, k]/sum(job)
				}

				if ((h > i & k < j) | (h < i & k > j)){
					pi_d <- pi_d + job[h, k]/sum(job)
				}
			}
		}
		phi <- 4 * (pi_c * D - pi_d * C) / (C + D)^2

		sigma.squared <- sigma.squared + phi^2
	}	
}

se <- (sigma.squared/sum(job))^.5 # 0.00748, which is different from
the SE 0.117 given in section 3.4.3 of that book.

I am not able to figure out what is the problem with my code... Could
anyone point out what the problem is?

Thanks.

Wuming



From nkn at turing.une.edu.au  Fri Sep  2 09:05:53 2005
From: nkn at turing.une.edu.au (Nam-Ky Nguyen)
Date: Fri, 2 Sep 2005 17:05:53 +1000 (EST)
Subject: [R] R binaries, platform independent and Design of Experiments
In-Reply-To: <4316F615.1050000@statistik.uni-dortmund.de>
References: <20735.129.180.11.34.1125460179.squirrel@129.180.11.34>	<43156806.4020608@hhbio.wasser.tu-dresden.de>
	<50879.59.167.30.131.1125530048.squirrel@59.167.30.131>
	<4316F615.1050000@statistik.uni-dortmund.de>
Message-ID: <2482.129.180.11.34.1125644753.squirrel@129.180.11.34>

> a) The sources are available and really easy to compile on all those
> operating systems.
Honestly, I only know how to compile my Java programs. Anyway, we have
been able to work out how to download all files
http://cran.au.r-project.org/. The entire CRAN is 5.4GB. This requires two
DVDs!.

> b) You do NOT want to do numerical computations on software available in
> Java byte code.
You do not want to do heavy numerical computations with R either. Most
statistical calculation using R requires a fraction of a second and I
cannot see a real difference between say 0.05 second and 0.07 second. NKN.



From r.hankin at noc.soton.ac.uk  Fri Sep  2 09:41:38 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 2 Sep 2005 08:41:38 +0100
Subject: [R] More block diagonal matrix construction code
In-Reply-To: <200509012309.j81N8wtd010095@volta.gene.com>
References: <200509012309.j81N8wtd010095@volta.gene.com>
Message-ID: <0C63EBB7-A91E-454E-9ADD-372B7A588B09@soc.soton.ac.uk>

hi Bert, List

well now seems a good time to adduce adiag() of package magic.
Function adiag binds together arrays of arbitrary dimension
corner-to-corner.  Sensible interpretation is made for
arguments at the "edge"  of acceptability (eg one array
being a scalar).


The "meat" of the code is as follows:

adiag <- function(a,b,pad=0){
s <- array(pad, dim.a + dim.b)
     s <- do.call("[<-", c(list(s), lapply(dim.a, seq.new), list(a)))
     ind <- lapply(seq(dim.b), function(i) seq.new(dim.b[[i]]) +
         dim.a[[i]])
     out <- do.call("[<-", c(list(s), ind, list(b)))
     return(out)
}

where

  seq.new <- function(i) {   if (i == 0) { return(NULL)} else { return 
(1:i)  }  }

[NB: untested].

so it creates an array "s" of the right size filled with "pad", and then
fills one corner with "a", then fills the other corner with "b".

Note the absence of any for() loops.

Hope this is useful

rksh




On 2 Sep 2005, at 00:08, Berton Gunter wrote:

> Folks:
>
> In answer to a query, Andy Liaw recently submitted some code to  
> construct a
> block diagonal matrix. For what seemed a fairly straightforward  
> task, the
> code seemed a little "overweight" to me (that's an American stock  
> analyst's
> term, btw), so I came up with a slightly cleaner version (with help  
> from
> Andy):
>
> bdiag<-function(...){
>     mlist<-list(...)
>     ## handle case in which list of matrices is given
>     if(length(mlist)==1)mlist<-unlist(mlist,rec=FALSE)
>     csdim<-rbind(c(0,0),apply(sapply(mlist,dim),1,cumsum ))
>     ret<-array(0,dim=csdim[length(mlist)+1,])
>     add1<-matrix(rep(1:0,2),nc=2)
>     for(i in seq(along=mlist)){
>         indx<-apply(csdim[i:(i+1),]+add1,2,function(x)x[1]:x[2])
>           ## non-square matrix
>         if(is.null(dim(indx)))ret[indx[[1]],indx[[2]]]<-mlist[[i]]
>             ## square matrix
>         else ret[indx[,1],indx[,2]]<-mlist[[i]]
>         }
>     ret
>     }
>
> I doubt that there's any noticeable practical performance  
> difference, of
> course.
>
> The strategy is entirely basic: just get the right indices for  
> replacement
> of the arguments into a matrix of 0's of the right dimensions.  
> About the
> only thing to notice is that the apply() construction returns  
> either a list
> or matrix depending on whether a matrix is square or not (a  
> subtlety that
> tripped me up in my first version of this code).
>
> I would be pleased if this stimulated others to come up with  
> cleverer/more
> elegant approaches that they would share, as it's the sort of thing  
> that
> I'll learn from and find useful.
>
> Cheers to all,
>
> Bert Gunter
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ligges at statistik.uni-dortmund.de  Fri Sep  2 09:50:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 02 Sep 2005 09:50:06 +0200
Subject: [R] R binaries, platform independent and Design of Experiments
In-Reply-To: <2482.129.180.11.34.1125644753.squirrel@129.180.11.34>
References: <20735.129.180.11.34.1125460179.squirrel@129.180.11.34>	<43156806.4020608@hhbio.wasser.tu-dresden.de>
	<50879.59.167.30.131.1125530048.squirrel@59.167.30.131>
	<4316F615.1050000@statistik.uni-dortmund.de>
	<2482.129.180.11.34.1125644753.squirrel@129.180.11.34>
Message-ID: <4318042E.4010803@statistik.uni-dortmund.de>

Nam-Ky Nguyen wrote:

>>a) The sources are available and really easy to compile on all those
>>operating systems.
> 
> Honestly, I only know how to compile my Java programs. Anyway, we have

Oh dear, I only know how to compile my C programs, and I never read the 
docs when something has to be compiled with Java. So we have a problem 
now, since you have not read the docs on compiling R.

> been able to work out how to download all files
> http://cran.au.r-project.org/. The entire CRAN is 5.4GB. This requires two
> DVDs!.


The sources of base R are in one file with 12 Mb and it's not that hard 
to say

  ./configure
  make
  make install

is it?


Looks like you have to look for some other software than R.

Uwe Ligges



> 
>>b) You do NOT want to do numerical computations on software available in
>>Java byte code.
> 
> You do not want to do heavy numerical computations with R either. Most
> statistical calculation using R requires a fraction of a second and I
> cannot see a real difference between say 0.05 second and 0.07 second. NKN.



From abdelhafid.berrachi at gazdefrance.com  Fri Sep  2 09:58:07 2005
From: abdelhafid.berrachi at gazdefrance.com (Abdelhafid BERRICHI)
Date: Fri, 2 Sep 2005 09:58:07 +0200
Subject: [R] question sur R
Message-ID: <OF0284F780.781574A4-ONC1257070.002B5B26-C1257070.002BC617@notes.edfgdf.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050902/e1017758/attachment.pl

From p.campbell at econ.bbk.ac.uk  Fri Sep  2 09:54:38 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Fri, 02 Sep 2005 08:54:38 +0100
Subject: [R] Linux Standalone Server Suggestions for R
Message-ID: <s3181363.092@markets.econ.bbk.ac.uk>

I think I remember reading somewhere that using Sun Studio compiler
generates binaries that run faster than those built using GCC. 
Presumably this performance gain is increased if the Sun Fortran 95
compiler is used.

Whether the substantial cost of Sun Studio is money better spent than
that on extra RAM or bigger processors is not something I can answer.

HTH

Phineas

>>> "Pikounis, Bill [CNTUS]" <BPikouni at cntus.jnj.com> 09/01/05 10:18 PM
>>>
Jia-Shing,
I missed your original message, but would like to reiterate Bogdan's
comments and suggestions.

In a former life, a colleague of mine led the way for us to construct a
"small farm" of Opteron servers that all had 2 AMD64 CPU's, SUSE
Enterprise
Server OS, and the ability to have up to 16GB RAM. We experimented with
clustering them but that was not successful, and for practical purposes,
not
necessary. Penguin computing (http://www.penguincomputing.com) provided
us
very reliable products, solutions, and service, and I am sure there are
other vendors just as capable. As Bogdan mentioned, search the r-help
archives for various discussions on this over the past few years.

With $50K US, you likely will come up more computing power than you can
dream of (for now at least :-). That can get you multiple 16GB 2CPU
machines, I believe.

Good luck!

Hope that helps,
Bill

-------------------------------
Bill Pikounis, PhD
Nonclinical Statistics
Centocor, Inc.

> -----Original Message-----
> From: bogdan romocea [mailto:br44114 at gmail.com]
> Sent: Thursday, September 01, 2005 2:54 PM
> To: jiso at ucsd.edu
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Linux Standalone Server Suggestions for R
> 
> 
> Most powerful in what way? Quite a lot depends on the jobs 
> you're going to run.
> 	- To run CPU-bound jobs, more CPUs is better. (Even 
> though R doesn't
> do threading, you can manually split some CPU-bound jobs in several
> parts and run them simultaneously.) Apart from multiple CPUs and
> hyperthreading, check the new dual-core CPUs.
> 	- To run very large jobs, more memory is better. You 
> can easily spend
> most of your money on memory. Get the fastest one.
> 	- You should get 64-bit CPUs, otherwise you won't be 
> able to run very
> large jobs (search the list for details).
> 
> I would suggest that you buy a configuration that can handle more CPUs
> and memory than you think you need now (say, at least 4 max CPUs and
> 16 GB max memory), then keep on adding more memory and CPUs as your
> needs change.
> hth,
> b.
> 
> 
> > -----Original Message-----
> > From: Jia-Shing So [mailto:jiso at ucsd.edu] 
> > Sent: Wednesday, August 31, 2005 10:03 PM
> > To: r-help at stat.math.ethz.ch
> > Cc: Phuoc Hong
> > Subject: [R] Linux Standalone Server Suggestions for R
> > 
> > 
> > Hi All,
> > 
> > My group is  looking for any suggestions on what to purchase to  
> > achieve the most powerful number crunching system that $50k 
> > can buy.   
> > The main application that will be used is R so input on what 
> > hardware  
> > benefits R most will be appreciated.  The requirements are 
> > that it be  
> > a single standalone server (i.e. not a cluster solution), and 
> > it that  
> > must be able to run unix/linux.  If anyone has any experience/ 
> > suggestions regarding the following questions that would also be  
> > greatly appreciated.
> > 
> > AMD vs Intel chips, especially 64-bit versions of the two?
> > Using Itanium/Opterons and if so how much of a performance 
> boost did  
> > you achieve vs other 64-bit chip sets?
> > Also, does anyone know if there is an upper thresh hold on much  
> > memory R can use?
> > 
> > Thanks in advance for any help and suggestions,
> > 
> > Jia-Shing So
> > Programmer Analyst
> > Biostatistics and Bioinformatics Lab
> > University of California, San Diego
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sean.oriordain at gmail.com  Fri Sep  2 11:31:33 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Fri, 2 Sep 2005 10:31:33 +0100
Subject: [R] R binaries, platform independent and Design of Experiments
In-Reply-To: <2482.129.180.11.34.1125644753.squirrel@129.180.11.34>
References: <20735.129.180.11.34.1125460179.squirrel@129.180.11.34>
	<43156806.4020608@hhbio.wasser.tu-dresden.de>
	<50879.59.167.30.131.1125530048.squirrel@59.167.30.131>
	<4316F615.1050000@statistik.uni-dortmund.de>
	<2482.129.180.11.34.1125644753.squirrel@129.180.11.34>
Message-ID: <8ed68eed050902023114a4eed4@mail.gmail.com>

On 02/09/05, Nam-Ky Nguyen <nkn at turing.une.edu.au> wrote:
> > b) You do NOT want to do numerical computations on software available in
> > Java byte code.
> You do not want to do heavy numerical computations with R either. Most
> statistical calculation using R requires a fraction of a second and I
> cannot see a real difference between say 0.05 second and 0.07 second. NKN.

It is my understanding that the problem with Java is that it wasn't
written with serious numerical computation in mind - as far as I know
only in the latest version have Sun started to be address this issue. 
The byte code for the java virtual machine has a flawed numerical
model which is not fully compliant with the IEEE754 standard - this
has nothing to do with speed of computation.  Furthermore the integer
model is very restrictive when you want to work on random numbers
using bit-twiddling.

cheers!
Sean



From sekemp at glam.ac.uk  Fri Sep  2 12:07:40 2005
From: sekemp at glam.ac.uk (Samuel E. Kemp)
Date: Fri, 2 Sep 2005 11:07:40 +0100
Subject: [R] y-axis intercept
Message-ID: <c85998ce07fe863c186f37ef18036f1b@glam.ac.uk>

Hi,

Is there any way to enforce the plot so that it draws the y-axis 
intercepting the x-axis at zero.

Thanks in advance,

Sam.



From Achim.Zeileis at wu-wien.ac.at  Fri Sep  2 12:15:03 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 2 Sep 2005 12:15:03 +0200
Subject: [R] y-axis intercept
In-Reply-To: <c85998ce07fe863c186f37ef18036f1b@glam.ac.uk>
References: <c85998ce07fe863c186f37ef18036f1b@glam.ac.uk>
Message-ID: <20050902121503.01bd6f1f.Achim.Zeileis@wu-wien.ac.at>

On Fri, 2 Sep 2005 11:07:40 +0100 Samuel E. Kemp wrote:

> Hi,
> 
> Is there any way to enforce the plot so that it draws the y-axis 
> intercepting the x-axis at zero.

I'm not sure what exactly you want, maybe setting ylim or yaxs or both?
For example:

plot(1:10, xlim = c(0, 10.5), ylim = c(0, 10.5), xaxs = "i", yaxs = "i")

see also ?par for more information.
Z

> Thanks in advance,
> 
> Sam.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From palvarez7777 at yahoo.es  Fri Sep  2 12:59:09 2005
From: palvarez7777 at yahoo.es (Alvarez Pedro)
Date: Fri, 2 Sep 2005 12:59:09 +0200 (CEST)
Subject: [R] Reference manual is not available in the help menu of the rgui
Message-ID: <20050902105909.96285.qmail@web25204.mail.ukl.yahoo.com>

Dear R list,

I have installed R 2.1.1 for Windows. In the help menu
of the Rgui I can load all manuals except the
reference manual. I downloaded the reference manual
from the cran-site separately and saved it into the
same folder as the other manuals but still it is not
available in the menu. How can I solve this problem?

Thank your for the help



From p.dalgaard at biostat.ku.dk  Fri Sep  2 13:26:18 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Sep 2005 13:26:18 +0200
Subject: [R] Reference manual is not available in the help menu of the
	rgui
In-Reply-To: <20050902105909.96285.qmail@web25204.mail.ukl.yahoo.com>
References: <20050902105909.96285.qmail@web25204.mail.ukl.yahoo.com>
Message-ID: <x21x47d82t.fsf@turmalin.kubism.ku.dk>

Alvarez Pedro <palvarez7777 at yahoo.es> writes:

> Dear R list,
> 
> I have installed R 2.1.1 for Windows. In the help menu
> of the Rgui I can load all manuals except the
> reference manual. I downloaded the reference manual
> from the cran-site separately and saved it into the
> same folder as the other manuals but still it is not
> available in the menu. How can I solve this problem?
> 
> Thank your for the help

Would you really want to read that cover to cover? Everything in it is
available via the on-line help system.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Sep  2 13:37:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Sep 2005 13:37:39 +0200
Subject: [R] Gamma for ordinal trends
In-Reply-To: <b428d06d05083104554f03f8d6@mail.gmail.com>
References: <b428d06d05083102465de4ca09@mail.gmail.com>
	<20050831103123.GA23048@psych> <x2oe7egwnw.fsf@turmalin.kubism.ku.dk>
	<b428d06d05083104554f03f8d6@mail.gmail.com>
Message-ID: <x2wtlzbszg.fsf@turmalin.kubism.ku.dk>

Wuming Gong <wuming.gong at gmail.com> writes:

> Is it possible to use delta method to evaluate the standard error of
> Goodman-Kruskal gamma and then Wald test to evaluate the significance
> of association?
> 
> Wuming

Probably better to actually read the paper(s) by Goodman&Kruskal
(JASA(1972), vol.67, 415--421, &c). AFAICS, that basically *is* the
delta method, but you'd likely want a score test, not a Wald test
(i.e. calculate the variance under the null).
 
> 
> On 31 Aug 2005 13:42:27 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > Jonathan Baron <baron at psych.upenn.edu> writes:
> > 
> > > On 08/31/05 17:46, Wuming Gong wrote:
> > > > Dear list,
> > > >
> > > > Are there any functions for calculating gamma (and its standard
> > > > error), which measures the association of ordinal factors in I x J
> > > > contingency table. I did a RSiteSearch but did not find any clues...
> > >
> > > You have to look for Goodman-Kruskal gamma.  It is a bit
> > > obscure.  It is rcorr.cens in the Hmisc package.
> > >
> > > The significance test is the same as for Kendall's tau, according
> > > to some books.
> > 
> > Well, it would be if we handled ties in Kendall's tau correctly...
> > 
> > > I don't know about standard error.
> > 
> > --
> >    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> >
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Fri Sep  2 13:58:52 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 02 Sep 2005 07:58:52 -0400
Subject: [R] Reference manual is not available in the help menu of the
 rgui
In-Reply-To: <20050902105909.96285.qmail@web25204.mail.ukl.yahoo.com>
References: <20050902105909.96285.qmail@web25204.mail.ukl.yahoo.com>
Message-ID: <43183E7C.3000509@stats.uwo.ca>

Alvarez Pedro wrote:
> Dear R list,
> 
> I have installed R 2.1.1 for Windows. In the help menu
> of the Rgui I can load all manuals except the
> reference manual. I downloaded the reference manual
> from the cran-site separately and saved it into the
> same folder as the other manuals but still it is not
> available in the menu. How can I solve this problem?

Re-install, and this time check the box to install that manual.  But as 
Peter says, it's not really very useful, it's just a collection of man 
pages from the base packages.

Duncan Murdoch



From sean.oriordain at gmail.com  Fri Sep  2 14:32:19 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Fri, 2 Sep 2005 13:32:19 +0100
Subject: [R] Reference manual is not available in the help menu of the
	rgui
In-Reply-To: <43183E7C.3000509@stats.uwo.ca>
References: <20050902105909.96285.qmail@web25204.mail.ukl.yahoo.com>
	<43183E7C.3000509@stats.uwo.ca>
Message-ID: <8ed68eed050902053242816093@mail.gmail.com>

Actually, I've started reading the reference manual... :-)

I printed it out 2-to-a-page and I'm working my way through it, in
order to learn about the full capabilities of the base system... I
know I'm not going to remember everything, but when I bump into a
particular problem, I'll know what type of solutions to use and what
sort of keywords to search for...  frequently the problem with help is
knowing that vital keyword when I in my ignorant non-statistician way
want to use another vocabularly... :-)

cheers!
Sean


On 02/09/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> Alvarez Pedro wrote:
> > Dear R list,
> >
> > I have installed R 2.1.1 for Windows. In the help menu
> > of the Rgui I can load all manuals except the
> > reference manual. I downloaded the reference manual
> > from the cran-site separately and saved it into the
> > same folder as the other manuals but still it is not
> > available in the menu. How can I solve this problem?
> 
> Re-install, and this time check the box to install that manual.  But as
> Peter says, it's not really very useful, it's just a collection of man
> pages from the base packages.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rolf at math.unb.ca  Fri Sep  2 14:34:11 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Fri, 2 Sep 2005 09:34:11 -0300 (ADT)
Subject: [R] SpatStat Kest -  Error Message help
In-Reply-To: <2506.141.211.76.26.1125596666.squirrel@cafemail.edacafe.com>
Message-ID: <Pine.GSO.4.33.0509020916520.16730-100000@erdos.math.unb.ca>


(1) Questions about contributed packages (such as spatstat) should in the
first instance be directed to the authors/maintainers of the package (in
this case Adrian Baddeley and myself) rather than to the list.

(2) It's very hard to say what's causing your problem without digging into
the specifics.  If you could email the problematic data set (it's only 17
points, after all!) to me and/or Adrian, we could probably figure out
what's causing the hiccup.

(3) It sound's like you are using roll-your-own code to create
(effectively) a critical envelope for your estimated K function. Why
aren't you using the built-in function envelope()?

(4) I believe that 999 replicates is usually overkill for Monte Carlo
inference; 99 reps is probably adequate.  Other uncertainties that are
always present in any real data set dwarf the lack of precision resulting
from using ``only'' 99 reps.  (Ninety-nine reps is the default in
envelope(); but you can set the number of reps to anything you want,
including 999 if you insist.)

(5) What does ``statistical significance of a point pattern'' mean?

			cheers,

				Rolf Turner
				rolf at math.unb.ca

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
Please avoid sending me Word or PowerPoint attachments.
See http://www.fsf.org/philosophy/no-word-attachments.html
===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===

On Thu, 1 Sep 2005, DrakeGis wrote:

> Hi I'm working with the function Kest in the package SpatStat (under LINUX
> with R 2.1.0). In order to evaluate the statistical significance of my
> point pattern I'm doing 999 Montecarlo replications. The script that use
> the Kest function runs OK for most of the different point patterns that I
> have but for a particular point pattern, which have only 17 points, it
> runs until the 34th iteration and then I receive this message:
>
> Error in "[<-"(`*tmp*`, index, value = NULL) :
> 	incompatible types (1000) in subassignment type fix
> Execution halted
>
> Do you have any idea about what could be the cause of this ? Thanks in
> advance.



From palvarez7777 at yahoo.es  Fri Sep  2 14:59:30 2005
From: palvarez7777 at yahoo.es (Alvarez Pedro)
Date: Fri, 2 Sep 2005 14:59:30 +0200 (CEST)
Subject: [R] Reference manual is not available in the help menu of the
	rgui
In-Reply-To: <43183E7C.3000509@stats.uwo.ca>
Message-ID: <20050902125930.18660.qmail@web25203.mail.ukl.yahoo.com>

 > Re-install, and this time check the box to install
> that manual. But as 

Dear Mr. Murdoch, thank you for the answer, apparently
there is no other solution than a re-installation. 

> Peter says, it's not really very useful, it's just a
> collection of man pages from the base packages.
 
... I want to have the reference manual only because
my eyes like it more to read pdfs then text in the
console (and with the search function I am as quickly
as in the other case).



From kjetil at acelerate.com  Thu Sep  1 23:39:24 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 01 Sep 2005 17:39:24 -0400
Subject: [R] Multivariate Skew Normal distribution
In-Reply-To: <XFMail.050901150246.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050901150246.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <4317750C.3080107@acelerate.com>

(Ted Harding) wrote:

>On 01-Sep-05 Caio Lucidius Naberezny Azevedo wrote:
>  
>
>>Hi all,
>> 
>>Could anyone tell me if there is any package (or function) that
>>generates values from a multivariate skew normal distribution?
>> 
>>Thanks all,
>> 
>>Caio
>>    
>>
>
>Hello, Caio
>
>Please tell us what you mean by "skew normal distribution".
>
>Since normal (i.e. gaussian) distributions are not skew, you
>  
>
Well, but then somebody (Azzalini?) coined the term skew-normal, which 
you can
read about at    http://azzalini.stat.unipd.it//SN
or simply do
library(help=sn) # after installing from CRAN.
This family is obtained by skewing a normal family, hence the name.
You can also skew a t -family or whatever other symmetric family you like.

I found this usefull for modelling.

Kjetil

>presumably mean something different from what you said, so
>unless we understand this more clearly  it is unlikely that
>anyone can make a suggestion which would meet your needs.
>
>With best wishes,
>Ted.
>
>
>--------------------------------------------------------------------
>E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>Fax-to-email: +44 (0)870 094 0861
>Date: 01-Sep-05                                       Time: 15:02:37
>------------------------------ XFMail ------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
Internal Virus Database is out-of-date.
Checked by AVG Anti-Virus.



From f.harrell at vanderbilt.edu  Fri Sep  2 15:03:02 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 02 Sep 2005 09:03:02 -0400
Subject: [R] Calculating Goodman-Kurskal's gamma using delta method
In-Reply-To: <b428d06d050901234963905521@mail.gmail.com>
References: <b428d06d050901234963905521@mail.gmail.com>
Message-ID: <43184D86.8010507@vanderbilt.edu>

Wuming Gong wrote:
> Dear list, 
> 
> I have a problem on calculating the standard error of
> Goodman-Kurskal's gamma using delta method. I exactly follow the
> method and forumla described in Problem 3.27 of Alan Agresti's
> Categorical Data Analysis (2nd edition). The data I used is also from
> the job satisfaction vs. income example from that book.
> 
> job <- matrix(c(1, 3, 10, 6, 2, 3, 10, 7, 1, 6, 14, 12, 0, 1, 9, 11),
> nrow = 4, ncol = 4, byrow = TRUE, dimnames = list(c("< 15,000",
> "15,000 - 25,000", "25,000 - 40,000", "> 40,000"), c("VD", "LD", "MS",
> "VS")))
> 
> The following code is for calculating gamma value, which is consistent
> with the result presented in section 2.4.5 of that book.
> 
> C <- 0
> D <- 0
> for (i in 1:nrow(job)){
> 	for (j in 1:ncol(job)){
> 		pi_c <- 0
> 		pi_d <- 0
> 		for (h in 1:nrow(job)){
> 			for (k in 1:ncol(job)){
> 				if ((h > i & k > j) | (h < i & k < j)){
> 					pi_c <- pi_c + job[h, k]/sum(job)
> 				}
> 
> 				if ((h > i & k < j) | (h < i & k > j)){
> 					pi_d <- pi_d + job[h, k]/sum(job)
> 				}
> 			}
> 		}
> 
> 		C <- C + job[i, j] * pi_c
> 		D <- D + job[i, j] * pi_d
> 	}
> }
> gamma <- (C - D) / (C + D) # gamma = 0.221 for this example.
> 
> The following code is for calculating stardard error of gamma.
> sigma.squared <- 0
> for (i in 1:nrow(job)){
> 	for (j in 1:ncol(job)){
> 		pi_c <- 0
> 		pi_d <- 0
> 		for (h in 1:nrow(job)){
> 			for (k in 1:ncol(job)){
> 				if ((h > i & k > j) | (h < i & k < j)){
> 					pi_c <- pi_c + job[h, k]/sum(job)
> 				}
> 
> 				if ((h > i & k < j) | (h < i & k > j)){
> 					pi_d <- pi_d + job[h, k]/sum(job)
> 				}
> 			}
> 		}
> 		phi <- 4 * (pi_c * D - pi_d * C) / (C + D)^2
> 
> 		sigma.squared <- sigma.squared + phi^2
> 	}	
> }
> 
> se <- (sigma.squared/sum(job))^.5 # 0.00748, which is different from
> the SE 0.117 given in section 3.4.3 of that book.
> 
> I am not able to figure out what is the problem with my code... Could
> anyone point out what the problem is?
> 
> Thanks.
> 
> Wuming

Save your time (and much execution time) by using the Hmisc package's 
rcorr.cens function with the argument outx=TRUE.  rcorr.cens using a 
standard U-statistic variance estimator.


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From murdoch at stats.uwo.ca  Fri Sep  2 15:01:27 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 02 Sep 2005 09:01:27 -0400
Subject: [R] Reference manual is not available in the help menu of the
 rgui
In-Reply-To: <20050902125930.18660.qmail@web25203.mail.ukl.yahoo.com>
References: <20050902125930.18660.qmail@web25203.mail.ukl.yahoo.com>
Message-ID: <43184D27.5040209@stats.uwo.ca>

On 9/2/2005 8:59 AM, Alvarez Pedro wrote:
>  > Re-install, and this time check the box to install
>> that manual. But as 
> 
> Dear Mr. Murdoch, thank you for the answer, apparently
> there is no other solution than a re-installation. 

... or downloading it from CRAN, as you did.

Duncan Murdoch

> 
>> Peter says, it's not really very useful, it's just a
>> collection of man pages from the base packages.
>  
> ... I want to have the reference manual only because
> my eyes like it more to read pdfs then text in the
> console (and with the search function I am as quickly
> as in the other case).
> 
> 
> 
> 	
> 	
> 		
> ______________________________________________ 
> Renovamos el Correo Yahoo! 
> Nuevos servicios, m??s seguridad 
> http://correo.yahoo.es



From admin at biostatistic.de  Fri Sep  2 15:06:45 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Fri, 02 Sep 2005 15:06:45 +0200
Subject: [R] png scaling problem
In-Reply-To: <971536df050901230363bc2a8a@mail.gmail.com>
References: <43175BBF.4040907@biostatistic.de>	<1125607996.1898.50.camel@localhost.localdomain>	<4317DECE.3030003@biostatistic.de>
	<971536df050901230363bc2a8a@mail.gmail.com>
Message-ID: <43184E65.3050004@biostatistic.de>

   

Gabor Grothendieck schrieb:

>If you have not already tried it try creating a fig file:
>
>xfig("myfile.fig")
>plot(1:10)
>dev.off()
>
>and then using the fig2dev utility (find it via google) to convert it to a tiff:
>
>fig2dev -L tiff myfile.fig > myfile.tiff
>
>  
>
Error::  fig2def: broken pipe <ghostscript aborted?>
command was gs -q -dSAFER -sDEVICE=tiff24nc -r80 -g3830x506 
-sOutputFile=44.tif



From MSchwartz at mn.rr.com  Fri Sep  2 15:07:03 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 02 Sep 2005 08:07:03 -0500
Subject: [R] png scaling problem
In-Reply-To: <971536df050901230363bc2a8a@mail.gmail.com>
References: <43175BBF.4040907@biostatistic.de>
	<1125607996.1898.50.camel@localhost.localdomain>
	<4317DECE.3030003@biostatistic.de>
	<971536df050901230363bc2a8a@mail.gmail.com>
Message-ID: <1125666423.4305.15.camel@localhost.localdomain>

Knut,

Gabor has provided a possible approach here.

Your comments on using postscript make me wonder how your code looked.
The following, for example, will create a 4 inch by 4 inch square plot
to an encapsulated postscript file (EPS). It will also specify/include
required resources for the Helvetica font, which is one of the
requirements on the page you reference. Since Helvetica is one of the
standard Adobe PS fonts, I don't believe that it is necessary to
actually "embed" the font here, which would be the case if you used a
non-standard font. If you open the EPS file in a text editor, you will
see many lines referring to 'Resources' and fonts.

 postscript("RPlot.eps", onefile = FALSE, horizontal = FALSE, 
             paper = "special", height = 4, width = 4, 
             family = "Helvetica", font = "Helvetica")

 barplot(1:10)

 dev.off()


They keys above are the 'onefile', 'horizontal' and 'paper' arguments,
which must be set as above to generate an EPS file with the specified
size and bounding box. The page referenced mentions creating the image
as close as possible to the actual size required, so be sure to set the
'height' and 'width' arguments per that specification.

Using postscript here will also better enable the 50% reduction that is
mentioned, given the vector based format here.

The key here also is to be sure that the plot looks good in the target
format, not on the screen, which includes text size, readability and
positioning, etc.

HTH,

Marc Schwartz


On Fri, 2005-09-02 at 02:03 -0400, Gabor Grothendieck wrote:
> If you have not already tried it try creating a fig file:
> 
> xfig("myfile.fig")
> plot(1:10)
> dev.off()
> 
> and then using the fig2dev utility (find it via google) to convert it to a tiff:
> 
> fig2dev -L tiff myfile.fig > myfile.tiff
> 
> 
> On 9/2/05, Knut Krueger <admin at biostatistic.de> wrote:
> > 
> > 
> > 
> > >Probably a better first question is, why are you using a bitmapped
> > >graphics format if you need the image to be re-scaled?
> > >
> > I need a 1000 dpi tif file in a size of appr. 10 to 10 cm for applied
> > animal behaviour science:
> > http://authors.elsevier.com/GuideForAuthors.html?PubID=503301&dc=GFA
> > 
> > images to one of the following formats (Note the resolution requirements
> > for line drawings, halftones, and
> > line/halftone combinations given below.):
> > EPS: Vector drawings. Embed the font or save the text as "graphics".
> > TIFF: Colour or greyscale photographs (halftones): always use a minimum
> > of 300 dpi.
> > TIFF: Bitmapped line drawings: use a minimum of 1000 dpi.
> > TIFF: Combinations bitmapped line/half-tone (colour or greyscale): a
> > minimum of 500 dpi is required.
> > DOC, XLS or PPT: If your electronic artwork is created in any of these
> > Microsoft Office applications please
> > supply "as is".
> > 
> > I tired the Postscript file but the file is double heigh as width i do
> > not know why.
> > The problem was already discussed in the tread: [R] High resolution plots
> > 
> > I have to send the images possibly yesterday and I am looking fo a
> > suitable solution since two months now.
> > I tired gsview with converting to all possible Tiff formats but the
> > images appear not in color and in a strange black and white way
> > Some readers are able to read it  (Windows Image view) other not and I
> > do not know which reader the journal will use :-(
> > And the ylab is too small ...
> > 
> > http://biostatistic.de/temp/1.tif
> > http://biostatistic.de/temp/2.tif
> > http://biostatistic.de/temp/3.tif
> > http://biostatistic.de/temp/4.tif
> > 
> > 
> > 
> > 
> > 
> > >In general,
> > >bitmapped graphics do not resize well, though if you have a specific
> > >need and know a target image size, you can adjust various parameters to
> > >look decent. Are you going to view these images in a web browser, where
> > >you are concerned with display size and resolution?
> > >
> > >>From your e-mail headers it appears you are on Windows. If you need a
> > >re-sizable graphic, use a vector based format such as wmf/emf,
> > >especially if you need the graphics embedded in a Windows application
> > >such as Word or Powerpoint. This is the default format under Windows
> > >when copying and pasting a graphic between applications. You can then,
> > >fairly freely, resize the image in the target application as you may
> > >require.
> > >
> > >If you are going to print the graphic directly or include it in a
> > >document for printing (as opposed to just viewing it), then use PDF or
> > >Postscript.
> > >
> > 
> > Ok there is a second description for the file format :-(
> > http://authors.elsevier.com/ArtworkInstructions.html?dc=AI2
> > there are pdf formats welcome but with defined conditions:
> > 
> > Maybe anybody could give me a hint to get the files in the recommendet
> > format.
> > I will ask them immediately which whether the pdf is allowed or not,
> > becaus they have two different instruction sites :-(
> > 
> > Regards Knut
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From admin at biostatistic.de  Fri Sep  2 15:08:19 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Fri, 02 Sep 2005 15:08:19 +0200
Subject: [R] png scaling problem
In-Reply-To: <1125607996.1898.50.camel@localhost.localdomain>
References: <43175BBF.4040907@biostatistic.de>
	<1125607996.1898.50.camel@localhost.localdomain>
Message-ID: <43184EC3.2070807@biostatistic.de>

but back to the last problem,
what could be wrong that the ylab is not displayed as expected?

with regards
Knut



From admin at biostatistic.de  Fri Sep  2 15:09:18 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Fri, 02 Sep 2005 15:09:18 +0200
Subject: [R] png scaling problem
In-Reply-To: <4317DECE.3030003@biostatistic.de>
References: <43175BBF.4040907@biostatistic.de>	<1125607996.1898.50.camel@localhost.localdomain>
	<4317DECE.3030003@biostatistic.de>
Message-ID: <43184EFE.9050907@biostatistic.de>



Knut Krueger schrieb:

>Ok there is a second description for the file format :-(
>http://authors.elsevier.com/ArtworkInstructions.html?dc=AI2
>there are pdf formats welcome but with defined conditions:
>  
>

>Maybe anybody could give me a hint to get the files in the recommendet 
>format.
>I will ask them immediately which whether the pdf is allowed or not, 
>becaus they have two different instruction sites :-(
>  
>
This one is genarally for elsvier journals, but if there is a special
description in the journal page, authors must use this ...
means:

EPS: Vector drawings. Embed the font or save the text as "graphics".
TIFF: Colour or greyscale photographs (halftones): always use a minimum
of 300 dpi.
TIFF: Bitmapped line drawings: use a minimum of 1000 dpi.
TIFF: Combinations bitmapped line/half-tone (colour or greyscale): a
minimum of 500 dpi is required.



with regards
Knut Krueger




-- 
with regards
Knut Krueger
http://www.biostatistic.de



From sdavis2 at mail.nih.gov  Fri Sep  2 15:17:52 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 02 Sep 2005 09:17:52 -0400
Subject: [R] Finding all overlaps between two sets of 1-Dimensional regions
Message-ID: <BF3DC940.DB86%sdavis2@mail.nih.gov>

I have a simply defined regions ([start,end] where start<end).  I have two
large sets of them and want to find all regions in the first that overlap
any regions in the second.  The closest I could find by searching is
overlap.owin in I can do this by looping, but there is likely a better way
to do this.  Any suggestions?

Thanks,
Sean



From MSchwartz at mn.rr.com  Fri Sep  2 15:18:36 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 02 Sep 2005 08:18:36 -0500
Subject: [R] The Perils of PowerPoint
Message-ID: <1125667116.4305.24.camel@localhost.localdomain>

Hi all,

Below is a URL for an editorial published today in our local newspaper,
the Minneapolis StarTribune. It was originally published in the
Washington Post a couple of days ago:

http://www.washingtonpost.com/wp-dyn/content/article/2005/08/29/AR2005082901444.html

but that site requires registration. The 'Strib" site seems to be open
for the moment:

http://www.startribune.com/stories/1519/5591930.html


I thought folks might find it interesting.

Best regards,

Marc Schwartz



From ggrothendieck at gmail.com  Fri Sep  2 15:38:27 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Sep 2005 09:38:27 -0400
Subject: [R] png scaling problem
In-Reply-To: <43184E65.3050004@biostatistic.de>
References: <43175BBF.4040907@biostatistic.de>
	<1125607996.1898.50.camel@localhost.localdomain>
	<4317DECE.3030003@biostatistic.de>
	<971536df050901230363bc2a8a@mail.gmail.com>
	<43184E65.3050004@biostatistic.de>
Message-ID: <971536df050902063860be7a70@mail.gmail.com>

I can't reproduce this problem.  It works fine for me.  
Some possibilities are:

1. check which version of fig2dev you are using.  If you
are on Windows I am using the fig2dev that comes in
winfgi142.zip by Andreas Schmidt found at:

   http://user.cs.tu-berlin.de/~huluvu/WinFIG.htm

Here is the version info I get:

C:\bin>fig2dev -h | findstr Windows
Windows version 2 (02/08/2004) by Andreas Schmidt

2. Also, it seems from your output that fig2dev uses ghostscript.  
I am using ghostscript 8.13.  Check what version you are
using.


On 9/2/05, Knut Krueger <admin at biostatistic.de> wrote:
> 
> 
> Gabor Grothendieck schrieb:
> 
> >If you have not already tried it try creating a fig file:
> >
> >xfig("myfile.fig")
> >plot(1:10)
> >dev.off()
> >
> >and then using the fig2dev utility (find it via google) to convert it to a tiff:
> >
> >fig2dev -L tiff myfile.fig > myfile.tiff
> >
> >
> >
> Error::  fig2def: broken pipe <ghostscript aborted?>
> command was gs -q -dSAFER -sDEVICE=tiff24nc -r80 -g3830x506
> -sOutputFile=44.tif
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ppancoska at notes.cc.sunysb.edu  Fri Sep  2 15:38:35 2005
From: ppancoska at notes.cc.sunysb.edu (ppancoska@notes.cc.sunysb.edu)
Date: Fri, 2 Sep 2005 09:38:35 -0400
Subject: [R] Pseudo-Voigt fit
In-Reply-To: <4317B61D.1080909@pdf.com>
Message-ID: <OFE73821DA.2D284327-ON85257070.0049A9E4-85257070.004AF2EB@notes.cc.sunysb.edu>


Dear colleagues,
thank you very much for help.
I have got the most efficient message (?nls) from Bert Gunter and I took
off from there and now the routine is up and running with results validated
and doing exactly what SigmaPlot did.
It required intense "...reading the *#@%* manual...." as Spencer suggests
below, but it was worth of the effort! I am actually amazed how easily - in
many cases - one can find the right segment in the documentation even after
only partial reading of all those pages. But sometimes even the real
ingenuity of designers in naming all those functions cannot switch on that
intuition "radar" to navigate where one would like (or has to) be. Mea
maxima culpa....

Thanks again.


Petr P.

Dr. Petr Pancoska
Department of Pathology
SUNY Stony Brook, NY 11794
phone:          (631)-444-3030

******************************************************************************

This e- mail message, including any attachments,
is for the sole use of the intended recipient(s) and may
contain confidential and privileged information.
Any unauthorized review, use, disclosure or distribution is prohibited.
If you are not the intended recipient, please contact the sender
by e-mail and destroy all copies of the original.
******************************************************************************


                                                                           
             Spencer Graves                                                
             <spencer.graves at p                                             
             df.com>                                                    To 
                                       ppancoska at notes.cc.sunysb.edu       
             09/01/2005 10:17                                           cc 
             PM                        r-help at stat.math.ethz.ch            
                                                                   Subject 
                                       Re: [R] Pseudo-Voigt fit            
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




               I haven't seen a reply to this question, so I will attempt a
few
remarks in spite of some confusion about what you are asking.

               1.  The function to use for parameter estimation depends on
ths
structure of the data.  My all-around preference for many purposes is
for "optim", but I've used "nls", "fitdistr" (in the MASS package) and
others in different circumstances.

               2.  If you are doing nonlinear estimation with, e.g., optim,
I
suggest you request "hessian=TRUE".  The eigenvalues of the hessian will
tell you if it is ill conditioned.  If it is, you might consider
reparameterizing the model.

               3.  I try to avoid using reserved words like "c".  R can
often
determine what you want from the context, but there are exceptions.  I
try to avoid that problem by testing a name at a command prompt before I
use it.  If it returns, "object not found", I'm fine;  if not, I try
something different.

               4.  Following the posting guide!
"http://www.R-project.org/posting-guide.html" can on average increase
the likelihood that you will receive helpful suggestions quickly.  (I've
learned that people rarely respond to my incoherent screams;  when they
do, it's rarely helpful.  I've reluctantly learned that there is often
no substutute for reading the *#@%* manual.)

               I'd be shocked if this answered your question, but I hope it
is
helpful nonetheless.

               spencer graves

ppancoska at notes.cc.sunysb.edu wrote:

> Hi, I am sorry for this question, but I am trying to speed up an
> application....
> I will need to fit many x-y data sets (input from text files) to
> 4-parameter Pseudo-Voigt peak function.
> Until now I used SigmaPlot macro to do it (enclosed just in case...)
>
> peaksign(q) = if(total(q)>q[1], 1, -1)
> xatymin(q,r) = xatymax(q,max(r)-r)
> [Parameters]
> a = if(peaksign(y)>0, max(y), min(y)) ''Auto {{previous: 60.8286}}
> b = fwhm(x,abs(y))/2 ''Auto {{previous: 0.656637}}
> c = .5 ''Auto {{previous: 6.82973e-010}}
> x0 = if(peaksign(y)>0, xatymax(x,y), xatymin(x,y)) ''Auto {{previous:
> 3.19308}}
>
>
> [Equation]
> f = a*(c*(1/(1+((x-x0)/b)^2))+(1-c)*exp(-0.5*((x-x0)/b)^2))
>
> fit f to y
>
>  (manageable for ~100), but it looks like the next project would need to
> process ~1000 member sets.
>
> I am not as familiar with R to find the right info (although I can use R
in
> general).
>
> I am also nearly sure that there should be a solution to this task "out
> there" ready to be modified...
>
> Could you be so kind and direct me please to the right package or
web-site
> with examples?
>
> Thank you very much
>
>
>
> Dr. Petr Pancoska
> Department of Pathology
> SUNY Stony Brook, NY 11794
> phone:          (631)-444-3030
>
>
******************************************************************************

>
> This e- mail message, including any attachments,
> is for the sole use of the intended recipient(s) and may
> contain confidential and privileged information.
> Any unauthorized review, use, disclosure or distribution is prohibited.
> If you are not the intended recipient, please contact the sender
> by e-mail and destroy all copies of the original.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

--
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From fcsaszar at gmail.com  Fri Sep  2 09:16:04 2005
From: fcsaszar at gmail.com (Felipe Csaszar)
Date: Fri, 2 Sep 2005 03:16:04 -0400
Subject: [R] 3d cube with labels along axes
References: <pan.2005.09.01.14.05.34.548177@zutt.org>
Message-ID: <df8u83$nvf$1@sea.gmane.org>

This may work:
- use option axes=F in persp
- use locator() to get the coordinates where you want to put your new labels
- use text(x,y,...) to write your new labels

It is a little bit tedious, but it should work. Regards,

Felipe



"jonne" <newsreader at zutt.org> wrote in message 
news:pan.2005.09.01.14.05.34.548177 at zutt.org...
> Hi R-users,
>
> I would like to draw a cube with a grid on it and labels along all
> three axes. I have trouble printing the labels correctly. My
> best attempt is described below.
>   Can somebody explain me how I can change the 0,20,40,80,100
> along the x axis into character vectors like
> "no", "light", "intermediate", "severe" ?
>
> x <- seq(0, 100, length=10)
> #x <- c("no", "light", "intermediate", "severe")
> y <- x
> f <- function(x,y) { numeric(length=100) + 5 }
> z <- outer(x, y, f)
>
> P <- persp(x, y, z, theta=30, phi=30, zlim=c(-10,10), ticktype="detailed")
>
> text3d(0, 0, -10, "Hello world", P)
>
> Thanks in advance,
> Jonne.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From MSchwartz at mn.rr.com  Fri Sep  2 15:49:02 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 02 Sep 2005 08:49:02 -0500
Subject: [R] png scaling problem
In-Reply-To: <43184EC3.2070807@biostatistic.de>
References: <43175BBF.4040907@biostatistic.de>
	<1125607996.1898.50.camel@localhost.localdomain>
	<43184EC3.2070807@biostatistic.de>
Message-ID: <1125668943.4305.43.camel@localhost.localdomain>

On Fri, 2005-09-02 at 15:08 +0200, Knut Krueger wrote:
> but back to the last problem,
> what could be wrong that the ylab is not displayed as expected?
> 
> with regards
> Knut

The TIF files seem to be OK. However, the PNG files, as a result of your
attempt to scale the plot, do not have enough margin space for side = 2
(left). You would need to adjust the settings for par("mar") and perhaps
adjust the overall plot size in the png() call. This is one of the
reasons why trying to scale a bitmapped image is problematic.

If you want to have finer control over the text annotation, use
something like the following:

  # Create just the 'bare' barplot and save the bar
  # midpoints in 'mp'
  mp <- barplot(1:10, xaxt = "n", yaxt = "n", ann = FALSE)

  # Now create the x axis labels, using 'mp' for placement
  # 'cex' controls text size.
  # See ?axis for more details
  axis(1, at = mp, labels = LETTERS[1:10], cex = 1.25)

  # Do the same for the y axis
  axis(2, at = seq(0, 10, 2), cex = 1)

  # Do the x axis label, using mtext()
  # See ?mtext
  mtext(1, line = 3, text = "X Axis Label", cex = 1.5)

  # Same for y axis
  mtext(2, line = 2.5, text = "y Axis Label", cex = 1.5)

  # Now the title
  mtext(3, line = 2, text = "Main Barplot Title", cex = 4)


Again, with the above, be sure to check the output in the target format,
not on the screen. They will not always be the same.

HTH,

Marc Schwartz



From dimitris.rizopoulos at med.kuleuven.be  Fri Sep  2 16:05:13 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 2 Sep 2005 16:05:13 +0200
Subject: [R] Finding all overlaps between two sets of 1-Dimensional
	regions
References: <BF3DC940.DB86%sdavis2@mail.nih.gov>
Message-ID: <00bf01c5afc7$5644a860$0540210a@www.domain>

maybe something like this could work in your case:

set1 <- t(apply(matrix(rnorm(20), 10, 2), 1, sort))
set2 <- t(apply(matrix(rnorm(10), 5, 2), 1, sort))
########################
set1. <- apply(set1, 1, rev)
apply(set2, 1, function(x) apply(set1. <= x, 2, any))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Sean Davis" <sdavis2 at mail.nih.gov>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Friday, September 02, 2005 3:17 PM
Subject: [R] Finding all overlaps between two sets of 1-Dimensional 
regions


>I have a simply defined regions ([start,end] where start<end).  I 
>have two
> large sets of them and want to find all regions in the first that 
> overlap
> any regions in the second.  The closest I could find by searching is
> overlap.owin in I can do this by looping, but there is likely a 
> better way
> to do this.  Any suggestions?
>
> Thanks,
> Sean
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From bhx2 at mevik.net  Fri Sep  2 16:09:00 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 02 Sep 2005 16:09:00 +0200
Subject: [R] Reference manual is not available in the help menu of the
 rgui
In-Reply-To: <8ed68eed050902053242816093@mail.gmail.com> (Sean O'Riordain's
	message of "Fri, 2 Sep 2005 13:32:19 +0100")
References: <20050902105909.96285.qmail@web25204.mail.ukl.yahoo.com>
	<43183E7C.3000509@stats.uwo.ca>
	<8ed68eed050902053242816093@mail.gmail.com>
Message-ID: <m0d5nr36kj.fsf@bar.nemo-project.org>

Sean O'Riordain writes:

> Actually, I've started reading the reference manual... :-)
>
> I printed it out 2-to-a-page and I'm working my way through it,

Ah!  This reminds me of the `good old days', reading the Emacs manual,
Emacs lisp manual, Gnu C library manual, ....  The payoff came in the
section giving the meaning of the C library error codes: EGREGIOUS
means `You did *what*?'.  :-)

-- 
Bj??rn-Helge Mevik



From admin at biostatistic.de  Fri Sep  2 16:15:51 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Fri, 02 Sep 2005 16:15:51 +0200
Subject: [R] png scaling problem
In-Reply-To: <1125668943.4305.43.camel@localhost.localdomain>
References: <43175BBF.4040907@biostatistic.de>	<1125607996.1898.50.camel@localhost.localdomain>	<43184EC3.2070807@biostatistic.de>
	<1125668943.4305.43.camel@localhost.localdomain>
Message-ID: <43185E97.9040603@biostatistic.de>

thx I will try it ...

think I will be newbie in R for the next 10 jears ...

And I don't know why wh choosed the only journal which don't want pdf 
files :-(



From p.dalgaard at biostat.ku.dk  Fri Sep  2 16:37:51 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Sep 2005 16:37:51 +0200
Subject: [R] Reference manual is not available in the help menu of the
	rgui
In-Reply-To: <m0d5nr36kj.fsf@bar.nemo-project.org>
References: <20050902105909.96285.qmail@web25204.mail.ukl.yahoo.com>
	<43183E7C.3000509@stats.uwo.ca>
	<8ed68eed050902053242816093@mail.gmail.com>
	<m0d5nr36kj.fsf@bar.nemo-project.org>
Message-ID: <x2slwnbkn4.fsf@turmalin.kubism.ku.dk>

bhx2 at mevik.net (Bj??rn-Helge Mevik) writes:

> Sean O'Riordain writes:
> 
> > Actually, I've started reading the reference manual... :-)
> >
> > I printed it out 2-to-a-page and I'm working my way through it,
> 
> Ah!  This reminds me of the `good old days', reading the Emacs manual,
> Emacs lisp manual, Gnu C library manual, ....  The payoff came in the
> section giving the meaning of the C library error codes: EGREGIOUS
> means `You did *what*?'.  :-)

I actually have a huge list of those. Among the better ones:


ECHERNOBYL Broken pipe
EFLAT      String out of range
EGAD       Surely you jest
EH         Canadian user error
EIEIO      Bug, bug here, bug bug there
ENIXON     Tape problem
ENODICE    Error in rand
EZ         Had been faster on paper


(Report from Usenix 1986, EUUG Newsletter)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From admin at biostatistic.de  Fri Sep  2 17:05:40 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Fri, 02 Sep 2005 17:05:40 +0200
Subject: [R] png scaling problem - solved :-)
In-Reply-To: <971536df050902063860be7a70@mail.gmail.com>
References: <43175BBF.4040907@biostatistic.de>	<1125607996.1898.50.camel@localhost.localdomain>	<4317DECE.3030003@biostatistic.de>	<971536df050901230363bc2a8a@mail.gmail.com>	<43184E65.3050004@biostatistic.de>
	<971536df050902063860be7a70@mail.gmail.com>
Message-ID: <43186A44.4020906@biostatistic.de>



Gabor Grothendieck schrieb:

>I can't reproduce this problem.  It works fine for me.  
>Some possibilities are:
>
>1. check which version of fig2dev you are using.  If you
>are on Windows I am using the fig2dev that comes in
>winfgi142.zip by Andreas Schmidt found at:
>
>   http://user.cs.tu-berlin.de/~huluvu/WinFIG.htm
>
>Here is the version info I get:
>
>C:\bin>fig2dev -h | findstr Windows
>Windows version 2 (02/08/2004) by Andreas Schmidt
>

I have

Windows version 2.1 (11/08/2004) by Andreas Schmidt

>
>2. Also, it seems from your output that fig2dev uses ghostscript.  
>I am using ghostscript 8.13.  Check what version you are
>using.
>
>  
>
8.51

but the most easy solution was the link to winfig -
Now I need  three systems to convert the files ?!?
ghostscript
fig2dev
and winfig

but it works fine :-)

I will write down this solution in our forum ( If the paper is 
submitted) and will post you the link.
If anybody else will need the same you could post only the link ;-)


thx Knut



From 0034058 at fudan.edu.cn  Fri Sep  2 04:20:57 2005
From: 0034058 at fudan.edu.cn (0034058@fudan.edu.cn)
Date: Fri, 02 Sep 2005 10:20:57 +0800
Subject: [R] how to fit the partial association model with R?
Message-ID: <186b1aa1867073.1867073186b1aa@fudan.edu.cn>

If I do not make a mistake,the partial association model is an 
extension of log-linear model.I read a papers which gives an example 
of it.(Sloane and Morgan,1996,An Introduction to Categorical Data 
Analysis,Annual Review of Sociology.22:351-375) Can R fit such partial 
association model?

ps:Another somewhat off-topic question.What's the motivations to use 
log-linear model?Or why use log-linear model?I learn the log-linear 
model butI still do not master the the advantage of the model.thank 
you!



From ramasamy at stats.ox.ac.uk  Fri Sep  2 17:31:42 2005
From: ramasamy at stats.ox.ac.uk (Adaikalavan Ramasamy)
Date: Fri, 02 Sep 2005 16:31:42 +0100
Subject: [R] C-index : typical values
Message-ID: <1125675103.6005.134.camel@ipc143004.lif.icnet.uk>

I am doing some coxPH model fitting and would like to have some idea
about how good the fits are. Someone suggested to use Frank Harrell's
C-index measure.

As I understand it, a C-index > 0.5 indicates a useful model. I am
probably making an error here because I am getting values less than 0.5
on real datasets. Can someone tell me where I am going wrong please ? 

Here is an example using the German Breast Study Group data available in
the mfp package. The predictors in the model were selected by stepAIC().


 library(Design); library(Hmisc); library(mfp); data(GBSG)
 fit <- cph( Surv( rfst, cens ) ~ htreat + tumsize + tumgrad + 
                                  posnodal + prm, data=GBSG, x=T, y=T )

 val <- validate.cph( fit, dxy=T, B=200 )
 round(val, 3)
         index.orig training   test optimism index.corrected   n
   Dxy       -0.377   -0.383 -0.370   -0.013          -0.364 200 
   R2         0.140    0.148  0.132    0.016           0.124 200
   Slope      1.000    1.000  0.925    0.075           0.925 200
   D          0.028    0.030  0.027    0.004           0.025 200
   U         -0.001   -0.001  0.002   -0.002           0.002 200
   Q          0.029    0.031  0.025    0.006           0.023 200

1) Am I correct in assuming C-index = 0.5 * ( Dxy + 1 ) ?

2) If so, I am getting 0.5*(-0.3634+1) = 0.318 for the C-index. Does
this make sense ?

3) Should I be using some other measurement instead of C-index.

Thank you very much in advance.

Regards, Adai



From antonio.fabio at gmail.com  Fri Sep  2 17:57:26 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Fri, 2 Sep 2005 17:57:26 +0200
Subject: [R] source package linking problem under linux
Message-ID: <b0808fdc050902085732c60e02@mail.gmail.com>

I'm having some problems in installing some source packages under linux.
As an example, MCMCpack. An error is raised when linking:

> install.packages("MCMCpack")
[...]
* Installing *source* package 'MCMCpack' ...
checking for C++ compiler default output file name... a.out
checking whether the C++ compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes
checking for gcc... gcc
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for egrep... grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking ieeefp.h usability... no
checking ieeefp.h presence... no
checking for ieeefp.h... no
checking for trunc... no
configure: creating ./config.status
config.status: creating src/Makevars
** libs
g++ -I/usr/lib/R/include    -DSCYTHE_COMPILE_DIRECT -DSCYTHE_NO_RANGE 
    -c distributions.cc -o distributions.o
[...etc. etc. All compilations are ok]

g++   -o MCMCpack.so distributions.o ide.o la.o lecuyer.o MCMCdistn.o
MCMCdynamicEI.o MCMCfactanal.o MCMCfcds.o MCMChierEI.o MCMCirt1d.o
MCMClogit.o MCMCmetrop1R.o MCMCmixfactanal.o MCMCmnlMH.o
MCMCmnlslice.o MCMCoprobit.o MCMCordfactanal.o MCMCpanel.o
MCMCpoisson.o MCMCprobit.o MCMCprobitres.o MCMCregress.o MCMCrng.o
MCMCtobit.o mersenne.o optimize.o rng.o smath.o stat.o  
-L/usr/lib/R/lib -lR
/usr/lib/gcc-lib/i486-linux/3.3.5/../../../crt1.o(.text+0x18): In
function `_start':
../sysdeps/i386/elf/start.S:98: undefined reference to `main'
collect2: ld returned 1 exit status
make: *** [MCMCpack.so] Error 1
ERROR: compilation failed for package 'MCMCpack'

I don't know why it searches a reference to 'main'...

Antonio, Fabio Di Narzo.


> version
         _
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    1.1
year     2005
month    06
day      20
language R



From thpe at hhbio.wasser.tu-dresden.de  Fri Sep  2 18:03:16 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Fri, 02 Sep 2005 18:03:16 +0200
Subject: [R] png scaling problem - solved :-)
In-Reply-To: <43186A44.4020906@biostatistic.de>
References: <43175BBF.4040907@biostatistic.de>	<1125607996.1898.50.camel@localhost.localdomain>	<4317DECE.3030003@biostatistic.de>	<971536df050901230363bc2a8a@mail.gmail.com>	<43184E65.3050004@biostatistic.de>	<971536df050902063860be7a70@mail.gmail.com>
	<43186A44.4020906@biostatistic.de>
Message-ID: <431877C4.8080002@hhbio.wasser.tu-dresden.de>

Yet another Windows solution without winfig:

1. Create a postscript image in R
2. Open this image in Ghostscript
3. Select a reasonable resolution using "Display Settings" in ghostscript
4. Copy the image via clipboard into your favorite image viewer (e.g. 
IrfanView)
5. Save the image in the required format.



Thomas P.

PS: But the supermost ;-) flexible tool to perform such tasks is, of 
course, ImageMagick.



From p.dalgaard at biostat.ku.dk  Fri Sep  2 18:15:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Sep 2005 18:15:22 +0200
Subject: [R] source package linking problem under linux
In-Reply-To: <b0808fdc050902085732c60e02@mail.gmail.com>
References: <b0808fdc050902085732c60e02@mail.gmail.com>
Message-ID: <x2y86fwin9.fsf@turmalin.kubism.ku.dk>

"Antonio, Fabio Di Narzo" <antonio.fabio at gmail.com> writes:

> I'm having some problems in installing some source packages under linux.
> As an example, MCMCpack. An error is raised when linking:
> 
> > install.packages("MCMCpack")
> [...]
> * Installing *source* package 'MCMCpack' ...
> checking for C++ compiler default output file name... a.out
> checking whether the C++ compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C++ compiler... yes
> checking whether g++ accepts -g... yes
> checking for gcc... gcc
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for egrep... grep -E
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking ieeefp.h usability... no
> checking ieeefp.h presence... no
> checking for ieeefp.h... no
> checking for trunc... no
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> g++ -I/usr/lib/R/include    -DSCYTHE_COMPILE_DIRECT -DSCYTHE_NO_RANGE 
>     -c distributions.cc -o distributions.o
> [...etc. etc. All compilations are ok]
> 
> g++   -o MCMCpack.so distributions.o ide.o la.o lecuyer.o MCMCdistn.o
> MCMCdynamicEI.o MCMCfactanal.o MCMCfcds.o MCMChierEI.o MCMCirt1d.o
> MCMClogit.o MCMCmetrop1R.o MCMCmixfactanal.o MCMCmnlMH.o
> MCMCmnlslice.o MCMCoprobit.o MCMCordfactanal.o MCMCpanel.o
> MCMCpoisson.o MCMCprobit.o MCMCprobitres.o MCMCregress.o MCMCrng.o
> MCMCtobit.o mersenne.o optimize.o rng.o smath.o stat.o  
> -L/usr/lib/R/lib -lR
> /usr/lib/gcc-lib/i486-linux/3.3.5/../../../crt1.o(.text+0x18): In
> function `_start':
> ../sysdeps/i386/elf/start.S:98: undefined reference to `main'
> collect2: ld returned 1 exit status
> make: *** [MCMCpack.so] Error 1
> ERROR: compilation failed for package 'MCMCpack'
> 
> I don't know why it searches a reference to 'main'...
> 

Presumably because it thinks that MCMCpack.so is supposed to be a
standalone binary. (Compilers don't read filname suffixes...) 

There would seem to be something missing in that command line,
"-shared" if my memory serves me. Now *why* that happens is a bit hard
to figure out. Your version info below is not quite sufficient; which
linux distribution is it? Did you compile R itself from sources?


 Antonio, Fabio Di Narzo.
> 
> 
> > version
>          _
> platform i386-pc-linux-gnu
> arch     i386
> os       linux-gnu
> system   i386, linux-gnu
> status
> major    2
> minor    1.1
> year     2005
> month    06
> day      20
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From rbaer at atsu.edu  Fri Sep  2 18:29:53 2005
From: rbaer at atsu.edu (Robert Baer)
Date: Fri, 2 Sep 2005 11:29:53 -0500
Subject: [R] The Perils of PowerPoint
References: <1125667116.4305.24.camel@localhost.localdomain>
Message-ID: <00bc01c5afdb$8c15e170$6d0d010a@BigBaer>

The R relevance here might be that all the statistics in the world wrongly
applied to data will only bury its information content...    R and
Powerpoint (and Matlab and Perl and...) are all terrific tools for turning
data into knowledge, but tools DO NOT relieve us of the necessity of
thinking about and analyzing the meaning of the data with our intellect as
well.  It is wrong to blame ANY tool for our own shortcomings!

My two cents,
Rob

----- Original Message ----- 
From: "Marc Schwartz" <MSchwartz at mn.rr.com>
To: "R-Help" <r-help at stat.math.ethz.ch>
Sent: Friday, September 02, 2005 8:18 AM
Subject: [R] The Perils of PowerPoint


> Hi all,
>
> Below is a URL for an editorial published today in our local newspaper,
> the Minneapolis StarTribune. It was originally published in the
> Washington Post a couple of days ago:
>
>
http://www.washingtonpost.com/wp-dyn/content/article/2005/08/29/AR2005082901444.html
>
> but that site requires registration. The 'Strib" site seems to be open
> for the moment:
>
> http://www.startribune.com/stories/1519/5591930.html
>
>
> I thought folks might find it interesting.
>
> Best regards,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From joostvanevert at gmail.com  Fri Sep  2 18:36:08 2005
From: joostvanevert at gmail.com (Joost van Evert)
Date: Fri, 02 Sep 2005 18:36:08 +0200
Subject: [R] glm p-values on features
In-Reply-To: <mailman.11.1125655201.27534.r-help@stat.math.ethz.ch>
References: <mailman.11.1125655201.27534.r-help@stat.math.ethz.ch>
Message-ID: <1125678969.13108.1.camel@inpc93.et.tudelft.nl>

Dear list,

does anyone know how to get p-values on the coefficients returned by
glm?

thanks+greets,
Joost



From antonio.fabio at gmail.com  Fri Sep  2 19:07:36 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Fri, 2 Sep 2005 19:07:36 +0200
Subject: [R] source package linking problem under linux
In-Reply-To: <x2y86fwin9.fsf@turmalin.kubism.ku.dk>
References: <b0808fdc050902085732c60e02@mail.gmail.com>
	<x2y86fwin9.fsf@turmalin.kubism.ku.dk>
Message-ID: <b0808fdc05090210076316fa70@mail.gmail.com>

02 Sep 2005 18:15:22 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk>:
> "Antonio, Fabio Di Narzo" <antonio.fabio at gmail.com> writes:
> 
> > I'm having some problems in installing some source packages under linux.
> > As an example, MCMCpack. An error is raised when linking:
> >
> > > install.packages("MCMCpack")
> > [...]
> > * Installing *source* package 'MCMCpack' ...
> > checking for C++ compiler default output file name... a.out
> > checking whether the C++ compiler works... yes
> > checking whether we are cross compiling... no
> > checking for suffix of executables...
> > checking for suffix of object files... o
> > checking whether we are using the GNU C++ compiler... yes
> > checking whether g++ accepts -g... yes
> > checking for gcc... gcc
> > checking whether we are using the GNU C compiler... yes
> > checking whether gcc accepts -g... yes
> > checking for gcc option to accept ANSI C... none needed
> > checking how to run the C preprocessor... gcc -E
> > checking for egrep... grep -E
> > checking for ANSI C header files... yes
> > checking for sys/types.h... yes
> > checking for sys/stat.h... yes
> > checking for stdlib.h... yes
> > checking for string.h... yes
> > checking for memory.h... yes
> > checking for strings.h... yes
> > checking for inttypes.h... yes
> > checking for stdint.h... yes
> > checking for unistd.h... yes
> > checking ieeefp.h usability... no
> > checking ieeefp.h presence... no
> > checking for ieeefp.h... no
> > checking for trunc... no
> > configure: creating ./config.status
> > config.status: creating src/Makevars
> > ** libs
> > g++ -I/usr/lib/R/include    -DSCYTHE_COMPILE_DIRECT -DSCYTHE_NO_RANGE
> >     -c distributions.cc -o distributions.o
> > [...etc. etc. All compilations are ok]
> >
> > g++   -o MCMCpack.so distributions.o ide.o la.o lecuyer.o MCMCdistn.o
> > MCMCdynamicEI.o MCMCfactanal.o MCMCfcds.o MCMChierEI.o MCMCirt1d.o
> > MCMClogit.o MCMCmetrop1R.o MCMCmixfactanal.o MCMCmnlMH.o
> > MCMCmnlslice.o MCMCoprobit.o MCMCordfactanal.o MCMCpanel.o
> > MCMCpoisson.o MCMCprobit.o MCMCprobitres.o MCMCregress.o MCMCrng.o
> > MCMCtobit.o mersenne.o optimize.o rng.o smath.o stat.o
> > -L/usr/lib/R/lib -lR
> > /usr/lib/gcc-lib/i486-linux/3.3.5/../../../crt1.o(.text+0x18): In
> > function `_start':
> > ../sysdeps/i386/elf/start.S:98: undefined reference to `main'
> > collect2: ld returned 1 exit status
> > make: *** [MCMCpack.so] Error 1
> > ERROR: compilation failed for package 'MCMCpack'
> >
> > I don't know why it searches a reference to 'main'...
> >
> 
> Presumably because it thinks that MCMCpack.so is supposed to be a
> standalone binary. (Compilers don't read filname suffixes...)
> 
> There would seem to be something missing in that command line,
> "-shared" if my memory serves me. 
Yes, it is...

> Now *why* that happens is a bit hard
> to figure out. Your version info below is not quite sufficient; which
> linux distribution is it? Did you compile R itself from sources?

I'm using ubuntu 5.04 (debian based), and installed precompiled binary
version of R from an italian cran mirror ('woody' subdirectory).

Another package with the *same* problem: bayesm. Maybe the problem is
that ther's c++ code? What should I do?

> 
> 
>  Antonio, Fabio Di Narzo.
> >
> >
> > > version
> >          _
> > platform i386-pc-linux-gnu
> > arch     i386
> > os       linux-gnu
> > system   i386, linux-gnu
> > status
> > major    2
> > minor    1.1
> > year     2005
> > month    06
> > day      20
> > language R
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> --
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>



From info at aghmed.fsnet.co.uk  Fri Sep  2 17:56:29 2005
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Fri, 02 Sep 2005 16:56:29 +0100
Subject: [R] Digest reading is tedious
In-Reply-To: <17174.56138.693038.290299@stat.math.ethz.ch>
References: <03091f4b0b5415eeb2d9be3e5e070ceb@stanford.edu>
	<17145.5735.426400.446812@stat.math.ethz.ch>
	<42FB9390.7030005@stanford.edu>
	<17174.56138.693038.290299@stat.math.ethz.ch>
Message-ID: <6.2.1.2.0.20050902165220.02913960@pop.freeserve.net>

At 11:43 01/09/05, Martin Maechler wrote:

[snip section about Trevor Hastie's experience]


>What are other readers' experiences with mailman mailing lists
>in digest mode -- using "MIME" type delivery?

I use Eudora 6.2.1.2 (which is not the very latest version) running under 
Windows 98 or Windows XP and the digests are fine once you have found the 
option 'Receive MIME digest as mailbox attachment' and turned it on. I can 
then see the whole set of messages, sort them by subject and reply to them 
individually without having to change the subject of the message.

Hope that helps.

>Regards,
>Martin

Michael Dewey
http://www.aghmed.fsnet.co.uk



From davidr at rhotrading.com  Fri Sep  2 19:27:45 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Fri, 2 Sep 2005 12:27:45 -0500
Subject: [R] The Perils of PowerPoint
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A6E0CC4@rhosvr02.rhotrading.com>


> -----Original Message-----
> From: ... Robert Baer
> Sent: Friday, September 02, 2005 11:30 AM
....
> ....  It is wrong to blame ANY tool for our own shortcomings!

Surely a fortune!

David L. Reiner



From edd at debian.org  Fri Sep  2 19:33:46 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 2 Sep 2005 17:33:46 +0000 (UTC)
Subject: [R] source package linking problem under linux
References: <b0808fdc050902085732c60e02@mail.gmail.com>
	<x2y86fwin9.fsf@turmalin.kubism.ku.dk>
	<b0808fdc05090210076316fa70@mail.gmail.com>
Message-ID: <loom.20050902T193248-545@post.gmane.org>

Antonio, Fabio Di Narzo <antonio.fabio <at> gmail.com> writes:
> I'm using ubuntu 5.04 (debian based), and installed precompiled binary
> version of R from an italian cran mirror ('woody' subdirectory).
> 
> Another package with the *same* problem: bayesm. Maybe the problem is
> that ther's c++ code? What should I do?

Install the pre-compiled MCMCpack which Debian calls r-cran-mcmcpack?

Dirk



From cts at debian.org  Fri Sep  2 20:20:09 2005
From: cts at debian.org (Christian T. Steigies)
Date: Fri, 2 Sep 2005 20:20:09 +0200
Subject: [R] source package linking problem under linux
In-Reply-To: <loom.20050902T193248-545@post.gmane.org>
References: <b0808fdc050902085732c60e02@mail.gmail.com>
	<x2y86fwin9.fsf@turmalin.kubism.ku.dk>
	<b0808fdc05090210076316fa70@mail.gmail.com>
	<loom.20050902T193248-545@post.gmane.org>
Message-ID: <20050902182008.GA1009@gleep.debian.net>

On Fri, Sep 02, 2005 at 05:33:46PM +0000, Dirk Eddelbuettel wrote:
> Antonio, Fabio Di Narzo <antonio.fabio <at> gmail.com> writes:
> > I'm using ubuntu 5.04 (debian based), and installed precompiled binary
> > version of R from an italian cran mirror ('woody' subdirectory).
> > 
> > Another package with the *same* problem: bayesm. Maybe the problem is
> > that ther's c++ code? What should I do?
> 
> Install the pre-compiled MCMCpack which Debian calls r-cran-mcmcpack?

Dirk knows this much better than me, but if there is a problem with the
package, it might be my fault, in case you installed a backport. I just
tried to build the package in the woody chroot where I built the backports:

configure: WARNING: Only g++ version 3.0 or greater can be used with MCMCpack.

Maybe that is the problem? In woody the gcc version is 2.95, so the
precompiled binaries you are using were built with this gcc version. Ubuntu
5.04 might use a newer gcc, perhaps you can not mix and match those? If this
is the case, the sarge backports might work better for you. On my sarge box
install.packages("MCMCpack") works just fine when I install.packages("coda")
first. I don't have an ubuntu 5.04 box around, I wonder if I could install
it in a chroot as well. For you it might work if you rebuild R from source,
sources for the backports should be on the cran mirror as well.

Christian



From walton.green at yale.edu  Fri Sep  2 21:02:27 2005
From: walton.green at yale.edu (Walton A. Green)
Date: Fri, 2 Sep 2005 15:02:27 -0400 (EDT)
Subject: [R] setting par() for individual leaves/twigs/labels in
	plot.dendrogram
Message-ID: <Pine.LNX.4.44.0509021433170.18729-100000@argos.its.yale.edu>


R-helpers,

I seem to remember a discussion in r-help a while ago about plotting the 
individual leaves of a dendrogram in different colors, but I can't 
find the discussion in the archives or figure out how to do it. For my 
purposes, it doesn't really matter whether its the terminal points 
(leaves) or the terminal edges (twigs) or, for that matter, the labels 
that can be colored, but I would like to do the equivalent in 
plot.dendrogram of the bivariate:

plot(1:10, col = c(1,1,1,1,1,2,2,2,2,3))

but with the colors applied to the leaves (or twigs or leaf labels). The 
edgePar, nodePar, label, and col arguments don't seem to do it. Am I 
missing something in the documentation, or is there a function in a 
package that I don't know about? Or a workaround? Or is this just not 
implemented?

Walton Green



From Achim.Zeileis at wu-wien.ac.at  Fri Sep  2 22:30:50 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 2 Sep 2005 22:30:50 +0200
Subject: [R] The Perils of PowerPoint
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A6E0CC4@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A6E0CC4@rhosvr02.rhotrading.com>
Message-ID: <20050902223050.195144d9.Achim.Zeileis@wu-wien.ac.at>

On Fri, 2 Sep 2005 12:27:45 -0500 davidr at rhotrading.com wrote:

> 
> > -----Original Message-----
> > From: ... Robert Baer
> > Sent: Friday, September 02, 2005 11:30 AM
> ....
> > ....  It is wrong to blame ANY tool for our own shortcomings!
> 
> Surely a fortune!

thx, added to the devel-version of fortunes.

But allow me one remark: Although the above is certainly true, there are
computational tools that help us better to realize or avoid our own
shortcomings whereas others will make it harder to arrive at the right
conclusions.
I agree that PowerPoint cannot be blamed for the crash of the space
shuttle, but I also see the point that the way presentations are
generated in PowerPoint (or graphics in Excel) can easily tempt people
into producing presentations/graphics that conceal what is important.
This is certainly not an excuse, but I think some criticism (even
if phrased a bit provocatively) should be allowed.

just my EUR 0.02.
Z

> David L. Reiner
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From David.Brahm at geodecapital.com  Fri Sep  2 22:51:47 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Fri, 2 Sep 2005 16:51:47 -0400
Subject: [R] Superassignment (<<-) and indexing
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BC4F@MSGBOSCLF2WIN.DMN1.FMR.COM>

In a clean environment under R-2.1.0 on Linux:
> x <- 1:5
> x[3] <<- 9
Error: Object "x" not found

Isn't that odd?  (Note x <<- 9 works just fine.)

Why am I doing this?  Because I'm stepping through code that
normally lives inside a function, where "<<-" is appropriate.

-- David Brahm (brahm at alum.mit.edu)



From sean.oriordain at gmail.com  Fri Sep  2 23:10:18 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Fri, 2 Sep 2005 21:10:18 +0000
Subject: [R] The Perils of PowerPoint
In-Reply-To: <20050902223050.195144d9.Achim.Zeileis@wu-wien.ac.at>
References: <12AE52872B5C5348BE5CF47C707FF53A6E0CC4@rhosvr02.rhotrading.com>
	<20050902223050.195144d9.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <8ed68eed05090214102635e70d@mail.gmail.com>

I can't lay my hands n it at the moment - its around here somewhere,
but in "Numerical Methods That Work" by Forman Acton, the author
points out that "the result of computation should be insight, not
numbers"....

ps. an excellent book if you haven't seen it.
https://enterprise.maa.org/ecomtpro/Timssnet/products/TNT_products.cfm

cheers,
Sean


On 02/09/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> On Fri, 2 Sep 2005 12:27:45 -0500 davidr at rhotrading.com wrote:
> 
> >
> > > -----Original Message-----
> > > From: ... Robert Baer
> > > Sent: Friday, September 02, 2005 11:30 AM
> > ....
> > > ....  It is wrong to blame ANY tool for our own shortcomings!
> >
> > Surely a fortune!
> 
> thx, added to the devel-version of fortunes.
> 
> But allow me one remark: Although the above is certainly true, there are
> computational tools that help us better to realize or avoid our own
> shortcomings whereas others will make it harder to arrive at the right
> conclusions.
> I agree that PowerPoint cannot be blamed for the crash of the space
> shuttle, but I also see the point that the way presentations are
> generated in PowerPoint (or graphics in Excel) can easily tempt people
> into producing presentations/graphics that conceal what is important.
> This is certainly not an excuse, but I think some criticism (even
> if phrased a bit provocatively) should be allowed.
> 
> just my EUR 0.02.
> Z
> 
> > David L. Reiner
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Fri Sep  2 23:43:41 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 02 Sep 2005 14:43:41 -0700
Subject: [R] Superassignment (<<-) and indexing
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D801A2BC4F@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BC4F@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <4318C78D.1060401@pdf.com>

	  Permit a mild protest on the word "appropriate" in this context.  The 
global assignment operator "<<-" provides, for my tastes, excessive 
opportunities for problems.  If I define "x" someplace else and then 
call your function, it may change my "x" in ways that generate 
considerable wailing and gnashing of teeth.  Unless I assign the 
function output to "x", then the action of your function will change my 
"x" in ways I did not anticipate, possibly generating many problems for 
me later -- with extreme difficulties in finding the source of the 
problem.  Moreover, if your library expects to later find in "x" what 
your function stored there, there could be other problems, because I 
might redefine "x" before you use it.  The library might work fine when 
you use it but not for someone else -- and tracing the problem can be 
difficult.

	  I understand that "<<-" may allow your function f1 to call f2 and 
have f2 change "x" in f1.  However, if your f2 gets called some other 
way or if the name of "x" is misspelled or changed in either f1 or f2, 
we could be back to the situation I just described.

	  spencer graves

Brahm, David wrote:

> In a clean environment under R-2.1.0 on Linux:
> 
>>x <- 1:5
>>x[3] <<- 9
> 
> Error: Object "x" not found
> 
> Isn't that odd?  (Note x <<- 9 works just fine.)
> 
> Why am I doing this?  Because I'm stepping through code that
> normally lives inside a function, where "<<-" is appropriate.
> 
> -- David Brahm (brahm at alum.mit.edu)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From tlumley at u.washington.edu  Sat Sep  3 00:16:17 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 2 Sep 2005 15:16:17 -0700 (PDT)
Subject: [R] Superassignment (<<-) and indexing
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D801A2BC4F@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BC4F@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <Pine.A41.4.63a.0509021510150.217476@homer08.u.washington.edu>

On Fri, 2 Sep 2005, Brahm, David wrote:

> In a clean environment under R-2.1.0 on Linux:
>> x <- 1:5
>> x[3] <<- 9
> Error: Object "x" not found
>
> Isn't that odd?  (Note x <<- 9 works just fine.)
>

Well, yes and no.

It is the result of a bug fix a version or two ago that dealt with the 
case where there was a local variable with the same name as a 
variable being modified by a complex superassignment.

As the R language definition now explains
   x[3] <<- 9
is short for
   `*tmp*` <- get(x, envir=parent.env, inherits=TRUE)
   `*tmp*`[3] <- 9
   x <<- `*tmp*`

and so it doesn't work if x doesn't exist in the parent environment.

x <<- 9
is ok, since it doesn't have to look up x before assigning it, but it is 
still a wart that x<<-9 creates a local variable x when executed in the 
global environment but not when executed anywhere else.

 	-thomas



From tlumley at u.washington.edu  Sat Sep  3 00:23:49 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 2 Sep 2005 15:23:49 -0700 (PDT)
Subject: [R] Superassignment (<<-) and indexing
In-Reply-To: <4318C78D.1060401@pdf.com>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BC4F@MSGBOSCLF2WIN.DMN1.FMR.COM>
	<4318C78D.1060401@pdf.com>
Message-ID: <Pine.A41.4.63a.0509021518360.217476@homer08.u.washington.edu>

On Fri, 2 Sep 2005, Spencer Graves wrote:

> 	  Permit a mild protest on the word "appropriate" in this context.  The
> global assignment operator "<<-" provides, for my tastes, excessive
> opportunities for problems.  If I define "x" someplace else and then
> call your function, it may change my "x" in ways that generate
> considerable wailing and gnashing of teeth.

No, no, no.

The sensible and appropriate uses of <<- involve modifying a variable that 
already exists in the lexical parent environment.  In these cases it can't 
escape and ravage the calling environment.

Certainly using <<- to assign to the calling environment is bogus.  In 
addition to your complaints, it doesn't even work (except from the global 
environment), since <<- searches the lexical stack rather than the call 
stack.

In R, <<- can be used safely to maintain state inside a function or shared 
between a set of functions (as in demo(scoping), or demo(tkdensity)). In 
S-PLUS it is admittedly harder to come up with good uses.

 	-thomas



From spencer.graves at pdf.com  Sat Sep  3 00:28:49 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 02 Sep 2005 15:28:49 -0700
Subject: [R] Superassignment (<<-) and indexing
In-Reply-To: <Pine.A41.4.63a.0509021518360.217476@homer08.u.washington.edu>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BC4F@MSGBOSCLF2WIN.DMN1.FMR.COM>
	<4318C78D.1060401@pdf.com>
	<Pine.A41.4.63a.0509021518360.217476@homer08.u.washington.edu>
Message-ID: <4318D221.4060701@pdf.com>

	  Wow!  That's great.  Thanks.  spencer

Thomas Lumley wrote:

> On Fri, 2 Sep 2005, Spencer Graves wrote:
> 
>>       Permit a mild protest on the word "appropriate" in this 
>> context.  The
>> global assignment operator "<<-" provides, for my tastes, excessive
>> opportunities for problems.  If I define "x" someplace else and then
>> call your function, it may change my "x" in ways that generate
>> considerable wailing and gnashing of teeth.
> 
> 
> No, no, no.
> 
> The sensible and appropriate uses of <<- involve modifying a variable 
> that already exists in the lexical parent environment.  In these cases 
> it can't escape and ravage the calling environment.
> 
> Certainly using <<- to assign to the calling environment is bogus.  In 
> addition to your complaints, it doesn't even work (except from the 
> global environment), since <<- searches the lexical stack rather than 
> the call stack.
> 
> In R, <<- can be used safely to maintain state inside a function or 
> shared between a set of functions (as in demo(scoping), or 
> demo(tkdensity)). In S-PLUS it is admittedly harder to come up with good 
> uses.
> 
>     -thomas

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Soren.Hojsgaard at agrsci.dk  Sat Sep  3 00:57:36 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Sat, 3 Sep 2005 00:57:36 +0200
Subject: [R] tcltk - automatically moving cursor to last line of tktext box
	- how?
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9AD4@DJFPOST01.djf.agrsci.dk>

Hi; I have a program which writes lines to a tktext box (of height, say, 10) with
      tkinsert(txto, "end", paste(so,"\n"))
I would like my program to be such that it automatically scrolls down through the text box when it is full so that I always see the last 10 lines written. Can anyone help on this?
Best regards
S??ren



From p.dalgaard at biostat.ku.dk  Sat Sep  3 01:39:19 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Sep 2005 01:39:19 +0200
Subject: [R] tcltk - automatically moving cursor to last line of tktext
	box - how?
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9AD4@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9AD4@DJFPOST01.djf.agrsci.dk>
Message-ID: <x2k6hzox94.fsf@turmalin.kubism.ku.dk>

S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> writes:

> Hi; I have a program which writes lines to a tktext box (of height, say, 10) with
>       tkinsert(txto, "end", paste(so,"\n"))
> I would like my program to be such that it automatically scrolls down through the text box when it is full so that I always see the last 10 lines written. Can anyone help on this?
> Best regards
> S??ren

How about 

tksee(txto, "end")

or maybe

tkyview(txto, "end - 10 lines")

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From f.harrell at vanderbilt.edu  Sat Sep  3 01:55:46 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 02 Sep 2005 19:55:46 -0400
Subject: [R] C-index : typical values
In-Reply-To: <1125675103.6005.134.camel@ipc143004.lif.icnet.uk>
References: <1125675103.6005.134.camel@ipc143004.lif.icnet.uk>
Message-ID: <4318E682.70703@vanderbilt.edu>

Adaikalavan Ramasamy wrote:
> I am doing some coxPH model fitting and would like to have some idea
> about how good the fits are. Someone suggested to use Frank Harrell's
> C-index measure.
> 
> As I understand it, a C-index > 0.5 indicates a useful model. I am

No, that just means predictions are better than random.

> probably making an error here because I am getting values less than 0.5
> on real datasets. Can someone tell me where I am going wrong please ? 
> 
> Here is an example using the German Breast Study Group data available in
> the mfp package. The predictors in the model were selected by stepAIC().
> 
> 
>  library(Design); library(Hmisc); library(mfp); data(GBSG)
>  fit <- cph( Surv( rfst, cens ) ~ htreat + tumsize + tumgrad + 
>                                   posnodal + prm, data=GBSG, x=T, y=T )
> 
>  val <- validate.cph( fit, dxy=T, B=200 )
>  round(val, 3)
>          index.orig training   test optimism index.corrected   n
>    Dxy       -0.377   -0.383 -0.370   -0.013          -0.364 200 
>    R2         0.140    0.148  0.132    0.016           0.124 200
>    Slope      1.000    1.000  0.925    0.075           0.925 200
>    D          0.028    0.030  0.027    0.004           0.025 200
>    U         -0.001   -0.001  0.002   -0.002           0.002 200
>    Q          0.029    0.031  0.025    0.006           0.023 200
> 
> 1) Am I correct in assuming C-index = 0.5 * ( Dxy + 1 ) ?

Yes

> 
> 2) If so, I am getting 0.5*(-0.3634+1) = 0.318 for the C-index. Does
> this make sense ?

For the Cox model, the default calculation correlates the linear 
predictor with survival time.  A large linear predictor (large log 
hazard) means shorter survival time.  To phrase it in the more usually 
way, negate Dxy before computing C.

Frank

> 
> 3) Should I be using some other measurement instead of C-index.
> 
> Thank you very much in advance.
> 
> Regards, Adai
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Ted.Harding at nessie.mcc.ac.uk  Sat Sep  3 02:06:16 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 03 Sep 2005 01:06:16 +0100 (BST)
Subject: [R] The Perils of PowerPoint
In-Reply-To: <8ed68eed05090214102635e70d@mail.gmail.com>
Message-ID: <XFMail.050903010616.Ted.Harding@nessie.mcc.ac.uk>

On 02-Sep-05 Sean O'Riordain wrote:
> I can't lay my hands n it at the moment - its around here somewhere,
> but in "Numerical Methods That Work" by Forman Acton, the author
> points out that "the result of computation should be insight, not
> numbers"....
> 
> ps. an excellent book if you haven't seen it.
> https://enterprise.maa.org/ecomtpro/Timssnet/products/TNT_products.cfm
> 
> cheers,
> Sean

No doubt you're correct -- but I associate it with Richard Hamming
(title page of "Numerical Methods for Scientists and Engineers" as
I recall -- yes, for me too "it's around here somewhere" -- another
really excellent book) where he words it:

  "The purpose of computing is insight, not numbers."

to which he adds:

  "The purpose of computing numbers is not yet in sight."

By the way, the Washington Post/Minneapolis Star Tribune article is
somewhat reminiscent of a short (15 min) broadcast on BBC Radio 4
back on October 18 2004 15:45-16:00 called

  "Microsoft Powerpoint and the Decline of Civilisation"

which explores similar themes and also frequently quotes Tufte.
Unfortunately it lapsed for ever from "Listen Again" after the
statutory week, so I can't point you to a replay. (However, I
have carefully preserved the cassette recording I made).

We are not, of course, going Off Topic here. If, in R, you can not
indefinitely extend a tangent, then it's time to extend R.

(Oh dear, I feel a fortune coming on ... )

Best wishes to all,
Ted.

> On 02/09/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
>> On Fri, 2 Sep 2005 12:27:45 -0500 davidr at rhotrading.com wrote:
>> 
>> >
>> > > -----Original Message-----
>> > > From: ... Robert Baer
>> > > Sent: Friday, September 02, 2005 11:30 AM
>> > ....
>> > > ....  It is wrong to blame ANY tool for our own shortcomings!
>> >
>> > Surely a fortune!
>> 
>> thx, added to the devel-version of fortunes.
>> 
>> But allow me one remark: Although the above is certainly true, there
>> are
>> computational tools that help us better to realize or avoid our own
>> shortcomings whereas others will make it harder to arrive at the right
>> conclusions.
>> I agree that PowerPoint cannot be blamed for the crash of the space
>> shuttle, but I also see the point that the way presentations are
>> generated in PowerPoint (or graphics in Excel) can easily tempt people
>> into producing presentations/graphics that conceal what is important.
>> This is certainly not an excuse, but I think some criticism (even
>> if phrased a bit provocatively) should be allowed.
>> 
>> just my EUR 0.02.
>> Z
>> 
>> > David L. Reiner
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> > http://www.R-project.org/posting-guide.html
>> >
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 03-Sep-05                                       Time: 01:00:24
------------------------------ XFMail ------------------------------



From MSchwartz at mn.rr.com  Sat Sep  3 02:46:16 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 02 Sep 2005 19:46:16 -0500
Subject: [R] The Perils of PowerPoint
In-Reply-To: <XFMail.050903010616.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050903010616.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1125708376.29577.2.camel@localhost.localdomain>

LOL Ted!  That's a great quote for fortune()!

On Sat, 2005-09-03 at 01:06 +0100, Ted Harding wrote:
> On 02-Sep-05 Sean O'Riordain wrote:
> > I can't lay my hands n it at the moment - its around here somewhere,
> > but in "Numerical Methods That Work" by Forman Acton, the author
> > points out that "the result of computation should be insight, not
> > numbers"....
> > 
> > ps. an excellent book if you haven't seen it.
> > https://enterprise.maa.org/ecomtpro/Timssnet/products/TNT_products.cfm
> > 
> > cheers,
> > Sean
> 
> No doubt you're correct -- but I associate it with Richard Hamming
> (title page of "Numerical Methods for Scientists and Engineers" as
> I recall -- yes, for me too "it's around here somewhere" -- another
> really excellent book) where he words it:
> 
>   "The purpose of computing is insight, not numbers."
> 
> to which he adds:
> 
>   "The purpose of computing numbers is not yet in sight."
> 
> By the way, the Washington Post/Minneapolis Star Tribune article is
> somewhat reminiscent of a short (15 min) broadcast on BBC Radio 4
> back on October 18 2004 15:45-16:00 called
> 
>   "Microsoft Powerpoint and the Decline of Civilisation"
> 
> which explores similar themes and also frequently quotes Tufte.
> Unfortunately it lapsed for ever from "Listen Again" after the
> statutory week, so I can't point you to a replay. (However, I
> have carefully preserved the cassette recording I made).
> 
> We are not, of course, going Off Topic here. If, in R, you can not
> indefinitely extend a tangent, then it's time to extend R.
> 
> (Oh dear, I feel a fortune coming on ... )
> 
> Best wishes to all,
> Ted.
> 
> > On 02/09/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> >> On Fri, 2 Sep 2005 12:27:45 -0500 davidr at rhotrading.com wrote:
> >> 
> >> >
> >> > > -----Original Message-----
> >> > > From: ... Robert Baer
> >> > > Sent: Friday, September 02, 2005 11:30 AM
> >> > ....
> >> > > ....  It is wrong to blame ANY tool for our own shortcomings!
> >> >
> >> > Surely a fortune!
> >> 
> >> thx, added to the devel-version of fortunes.
> >> 
> >> But allow me one remark: Although the above is certainly true, there
> >> are
> >> computational tools that help us better to realize or avoid our own
> >> shortcomings whereas others will make it harder to arrive at the right
> >> conclusions.
> >> I agree that PowerPoint cannot be blamed for the crash of the space
> >> shuttle, but I also see the point that the way presentations are
> >> generated in PowerPoint (or graphics in Excel) can easily tempt people
> >> into producing presentations/graphics that conceal what is important.
> >> This is certainly not an excuse, but I think some criticism (even
> >> if phrased a bit provocatively) should be allowed.
> >> 
> >> just my EUR 0.02.
> >> Z
> >> 
> >> > David L. Reiner
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide!
> >> > http://www.R-project.org/posting-guide.html
> >> >
> >> 
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 03-Sep-05                                       Time: 01:00:24
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tchur at optushome.com.au  Sat Sep  3 04:07:52 2005
From: tchur at optushome.com.au (Tim Churches)
Date: Sat, 03 Sep 2005 12:07:52 +1000
Subject: [R] The Perils of PowerPoint
In-Reply-To: <XFMail.050903010616.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050903010616.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <43190578.1080601@optushome.com.au>

(Ted Harding) wrote:

>By the way, the Washington Post/Minneapolis Star Tribune article is
>somewhat reminiscent of a short (15 min) broadcast on BBC Radio 4
>back on October 18 2004 15:45-16:00 called
>
>  "Microsoft Powerpoint and the Decline of Civilisation"
>
>which explores similar themes and also frequently quotes Tufte.
>Unfortunately it lapsed for ever from "Listen Again" after the
>statutory week, so I can't point you to a replay. (However, I
>have carefully preserved the cassette recording I made).
>  
>
Try http://sooper.org/misc/powerpoint.mp3 (copyright law notwithstanding...)

Tim C



From jamaj at terra.com.br  Sat Sep  3 04:52:45 2005
From: jamaj at terra.com.br (Jose Augusto Jr - jamaj - terra)
Date: Fri, 2 Sep 2005 23:52:45 -0300
Subject: [R] Problems plotting time-series with multiple lines
Message-ID: <001b01c5b032$933e8a90$6300a8c0@3k54gri437t>

Dear Sirs,

I want to plot a time series with lines, one for each variable.
I have a dataset with dates, and the values.
How can i plot?
I could plot one variable using index plot, bu i want to put the labels on X 
axis. But i had two problems:
1) The plot function, when i try to plot(x,y), incorectly sort the date (on 
X axis). My dataset has the date in string format "%d/%m/%Y).
If i try to converto to date using
as.Date(Dataset.dates,"%d/%m/%Y)
it interprets the %Y incorrectly.

2) I have 187 rows. So i have to plot only some of the dates, not all.

Please, help me.

Thanks in advance.

Best Regards,

Jos?? Augusto M. de Andrade Jr.

"Computer games don't affect kids; I mean if Pac-Man affected us as kids,
we'd all be running around darkened rooms, munching magic pills and 
listening to
repetitive electronic music." (Kristian Wilson, Nintendo, Inc. 1989)



From ggrothendieck at gmail.com  Sat Sep  3 05:53:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Sep 2005 23:53:40 -0400
Subject: [R] Problems plotting time-series with multiple lines
In-Reply-To: <001b01c5b032$933e8a90$6300a8c0@3k54gri437t>
References: <001b01c5b032$933e8a90$6300a8c0@3k54gri437t>
Message-ID: <971536df050902205328a574d8@mail.gmail.com>

On 9/2/05, Jose Augusto Jr - jamaj - terra <jamaj at terra.com.br> wrote:
> Dear Sirs,
> 
> I want to plot a time series with lines, one for each variable.
> I have a dataset with dates, and the values.
> How can i plot?
> I could plot one variable using index plot, bu i want to put the labels on X
> axis. But i had two problems:
> 1) The plot function, when i try to plot(x,y), incorectly sort the date (on
> X axis). My dataset has the date in string format "%d/%m/%Y).
> If i try to converto to date using
> as.Date(Dataset.dates,"%d/%m/%Y)
> it interprets the %Y incorrectly.

Please provide a reproducible example.

> 
> 2) I have 187 rows. So i have to plot only some of the dates, not all.

Check out plot.Date, lines.Date and axis.Date.
Also plot.zoo in the zoo package may be of use.



From fcsaszar at gmail.com  Sat Sep  3 11:32:07 2005
From: fcsaszar at gmail.com (Felipe Csaszar)
Date: Sat, 3 Sep 2005 05:32:07 -0400
Subject: [R] Problems plotting time-series with multiple lines
References: <001b01c5b032$933e8a90$6300a8c0@3k54gri437t>
Message-ID: <dfbqit$uem$1@sea.gmane.org>

He you checked this example?:
z <- ts(matrix(rt(200 * 8, df = 3), 200, 8), start = c(1961,1), frequency = 
12)
z <- window(z[, 1:3], end = c(1969, 12))
plot(z, plot.type = "single", lty = 1:3, col = 4:2)

It is from the help page of plot.ts

Felipe


"Jose Augusto Jr - jamaj - terra" <jamaj at terra.com.br> wrote in message 
news:001b01c5b032$933e8a90$6300a8c0 at 3k54gri437t...
Dear Sirs,

I want to plot a time series with lines, one for each variable.
I have a dataset with dates, and the values.
How can i plot?
I could plot one variable using index plot, bu i want to put the labels on X
axis. But i had two problems:
1) The plot function, when i try to plot(x,y), incorectly sort the date (on
X axis). My dataset has the date in string format "%d/%m/%Y).
If i try to converto to date using
as.Date(Dataset.dates,"%d/%m/%Y)
it interprets the %Y incorrectly.

2) I have 187 rows. So i have to plot only some of the dates, not all.

Please, help me.

Thanks in advance.

Best Regards,

Jos?? Augusto M. de Andrade Jr.

"Computer games don't affect kids; I mean if Pac-Man affected us as kids,
we'd all be running around darkened rooms, munching magic pills and
listening to
repetitive electronic music." (Kristian Wilson, Nintendo, Inc. 1989)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From dlalountas at yahoo.com  Sat Sep  3 12:19:49 2005
From: dlalountas at yahoo.com (denis lalountas)
Date: Sat, 3 Sep 2005 03:19:49 -0700 (PDT)
Subject: [R] DESIGN package psm function
Message-ID: <20050903101949.67442.qmail@web54614.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050903/b952027d/attachment.pl

From ramasamy at stats.ox.ac.uk  Sat Sep  3 14:41:28 2005
From: ramasamy at stats.ox.ac.uk (Adaikalavan Ramasamy)
Date: Sat, 03 Sep 2005 13:41:28 +0100
Subject: [R] C-index : typical values
In-Reply-To: <4318E682.70703@vanderbilt.edu>
References: <1125675103.6005.134.camel@ipc143004.lif.icnet.uk>
	<4318E682.70703@vanderbilt.edu>
Message-ID: <1125751288.5999.10.camel@ipc143004.lif.icnet.uk>

Thank you ! So to be absolutely sure, the C-index in my case is
  0.5 * ( 0.3634 + 1 ) = 0.6817  right ?

If the above calculation is correct then why do I get the following :

  rcorr.cens( predict(fit), Surv( GBSG$rfst, GBSG$cens ) )[ "C Index" ]
    C Index
  0.3115156

( I am aware that is a re-substitution error rate and optimistic, but
this is what led me to believe that my C-index was < 0.5 ).


Can I suggest that it is probably worth adding a sentence about the
relationship between C-index and Dxy in validate.cph or elsewhere if
this is not a widely known issue.

Thank you again.

Regards, Adai



On Fri, 2005-09-02 at 19:55 -0400, Frank E Harrell Jr wrote:
> Adaikalavan Ramasamy wrote:
> > I am doing some coxPH model fitting and would like to have some idea
> > about how good the fits are. Someone suggested to use Frank Harrell's
> > C-index measure.
> > 
> > As I understand it, a C-index > 0.5 indicates a useful model. I am
> 
> No, that just means predictions are better than random.
> 
> > probably making an error here because I am getting values less than 0.5
> > on real datasets. Can someone tell me where I am going wrong please ? 
> > 
> > Here is an example using the German Breast Study Group data available in
> > the mfp package. The predictors in the model were selected by stepAIC().
> > 
> > 
> >  library(Design); library(Hmisc); library(mfp); data(GBSG)
> >  fit <- cph( Surv( rfst, cens ) ~ htreat + tumsize + tumgrad + 
> >                                   posnodal + prm, data=GBSG, x=T, y=T )
> > 
> >  val <- validate.cph( fit, dxy=T, B=200 )
> >  round(val, 3)
> >          index.orig training   test optimism index.corrected   n
> >    Dxy       -0.377   -0.383 -0.370   -0.013          -0.364 200 
> >    R2         0.140    0.148  0.132    0.016           0.124 200
> >    Slope      1.000    1.000  0.925    0.075           0.925 200
> >    D          0.028    0.030  0.027    0.004           0.025 200
> >    U         -0.001   -0.001  0.002   -0.002           0.002 200
> >    Q          0.029    0.031  0.025    0.006           0.023 200
> > 
> > 1) Am I correct in assuming C-index = 0.5 * ( Dxy + 1 ) ?
> 
> Yes
> 
> > 
> > 2) If so, I am getting 0.5*(-0.3634+1) = 0.318 for the C-index. Does
> > this make sense ?
> 
> For the Cox model, the default calculation correlates the linear 
> predictor with survival time.  A large linear predictor (large log 
> hazard) means shorter survival time.  To phrase it in the more usually 
> way, negate Dxy before computing C.
> 
> Frank
> 
> > 
> > 3) Should I be using some other measurement instead of C-index.
> > 
> > Thank you very much in advance.
> > 
> > Regards, Adai
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> 
>



From jsorkin at grecc.umaryland.edu  Sat Sep  3 16:54:33 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sat, 03 Sep 2005 10:54:33 -0400
Subject: [R] Inconsistence in specifying action for missing data
Message-ID: <s3198104.003@grecc.umaryland.edu>

A question for R (and perhaps S and SPlus) historians.

Does anyone know the reason for the inconsistency in the way that the
action that should be taken when data are missing is specified? There
are several variants, na.action, na.omit, "T", TRUE,  etc. I know that a
foolish consistency is the hobgoblin of a small mind, but consistency
can make things easier.

My question is not meant as a complaint. I very much admire the R
development team. I simply am curious.

John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From murdoch at stats.uwo.ca  Sat Sep  3 17:40:18 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 03 Sep 2005 11:40:18 -0400
Subject: [R] Inconsistence in specifying action for missing data
In-Reply-To: <s3198104.003@grecc.umaryland.edu>
References: <s3198104.003@grecc.umaryland.edu>
Message-ID: <4319C3E2.1020303@stats.uwo.ca>

John Sorkin wrote:
> A question for R (and perhaps S and SPlus) historians.
> 
> Does anyone know the reason for the inconsistency in the way that the
> action that should be taken when data are missing is specified? There
> are several variants, na.action, na.omit, "T", TRUE,  etc. I know that a
> foolish consistency is the hobgoblin of a small mind, but consistency
> can make things easier.
> 
> My question is not meant as a complaint. I very much admire the R
> development team. I simply am curious.

R and S have been developed by lots of people, over a long time.  I 
think that's it.

Duncan Murdoch



From f.harrell at vanderbilt.edu  Sat Sep  3 20:10:12 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 03 Sep 2005 14:10:12 -0400
Subject: [R] C-index : typical values
In-Reply-To: <1125751288.5999.10.camel@ipc143004.lif.icnet.uk>
References: <1125675103.6005.134.camel@ipc143004.lif.icnet.uk>	
	<4318E682.70703@vanderbilt.edu>
	<1125751288.5999.10.camel@ipc143004.lif.icnet.uk>
Message-ID: <4319E704.1080109@vanderbilt.edu>

Adaikalavan Ramasamy wrote:
> Thank you ! So to be absolutely sure, the C-index in my case is
>   0.5 * ( 0.3634 + 1 ) = 0.6817  right ?

correct

> 
> If the above calculation is correct then why do I get the following :
> 
>   rcorr.cens( predict(fit), Surv( GBSG$rfst, GBSG$cens ) )[ "C Index" ]
>     C Index
>   0.3115156
> 
> ( I am aware that is a re-substitution error rate and optimistic, but
> this is what led me to believe that my C-index was < 0.5 ).

You're right about the optimism but that's not the cause in this case.

> 
> 
> Can I suggest that it is probably worth adding a sentence about the
> relationship between C-index and Dxy in validate.cph or elsewhere if
> this is not a widely known issue.

Will do  -Frank

> 
> Thank you again.
> 
> Regards, Adai
> 
> 
> 
> On Fri, 2005-09-02 at 19:55 -0400, Frank E Harrell Jr wrote:
> 
>>Adaikalavan Ramasamy wrote:
>>
>>>I am doing some coxPH model fitting and would like to have some idea
>>>about how good the fits are. Someone suggested to use Frank Harrell's
>>>C-index measure.
>>>
>>>As I understand it, a C-index > 0.5 indicates a useful model. I am
>>
>>No, that just means predictions are better than random.
>>
>>
>>>probably making an error here because I am getting values less than 0.5
>>>on real datasets. Can someone tell me where I am going wrong please ? 
>>>
>>>Here is an example using the German Breast Study Group data available in
>>>the mfp package. The predictors in the model were selected by stepAIC().
>>>
>>>
>>> library(Design); library(Hmisc); library(mfp); data(GBSG)
>>> fit <- cph( Surv( rfst, cens ) ~ htreat + tumsize + tumgrad + 
>>>                                  posnodal + prm, data=GBSG, x=T, y=T )
>>>
>>> val <- validate.cph( fit, dxy=T, B=200 )
>>> round(val, 3)
>>>         index.orig training   test optimism index.corrected   n
>>>   Dxy       -0.377   -0.383 -0.370   -0.013          -0.364 200 
>>>   R2         0.140    0.148  0.132    0.016           0.124 200
>>>   Slope      1.000    1.000  0.925    0.075           0.925 200
>>>   D          0.028    0.030  0.027    0.004           0.025 200
>>>   U         -0.001   -0.001  0.002   -0.002           0.002 200
>>>   Q          0.029    0.031  0.025    0.006           0.023 200
>>>
>>>1) Am I correct in assuming C-index = 0.5 * ( Dxy + 1 ) ?
>>
>>Yes
>>
>>
>>>2) If so, I am getting 0.5*(-0.3634+1) = 0.318 for the C-index. Does
>>>this make sense ?
>>
>>For the Cox model, the default calculation correlates the linear 
>>predictor with survival time.  A large linear predictor (large log 
>>hazard) means shorter survival time.  To phrase it in the more usually 
>>way, negate Dxy before computing C.
>>
>>Frank
>>
>>
>>>3) Should I be using some other measurement instead of C-index.
>>>
>>>Thank you very much in advance.
>>>
>>>Regards, Adai
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
>>
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From dusa.adrian at gmail.com  Sat Sep  3 19:17:54 2005
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Sat, 3 Sep 2005 20:17:54 +0300
Subject: [R] producing SVG files
Message-ID: <200509032017.54974.adi@roda.ro>


I am trying to use the RSvgDevice package to produce some SVG graphs which I 
want to edit with Inkscape 0.42.
Under Linux (Kubuntu 5.04) I use the following:

library(RSvgDevice)
plot(1:10, 1:10)
devSVG(file = "/home/adi/Rplots.svg", width = 10, height = 8,
bg = "white", fg = "black", onefile=TRUE, xmlHeader=TRUE)

but when I tried to load the file into Inkscape it complained about finding an 
empty file.

Then I tried the example by:

devSVG()
plot(1:11,(-5:5)^2, type='b', main="Simple Example Plot")
dev.off()

and then again

devSVG(file = "/home/adi/Rplots.svg", width = 10, height = 8,
bg = "white", fg = "black", onefile=TRUE, xmlHeader=TRUE)

with the same result.
Could you please point me to the right direction, please?
Thank you in advance,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest
Romania
Tel./Fax: +40 21 3126618 \
              +40 21 3120210 / int.101



From dusa.adrian at gmail.com  Sat Sep  3 20:25:02 2005
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Sat, 3 Sep 2005 18:25:02 +0000 (UTC)
Subject: [R] producing SVG files
References: <200509032017.54974.adi@roda.ro>
Message-ID: <loom.20050903T202306-304@post.gmane.org>

Adrian DUSA <dusa.adrian <at> gmail.com> writes:

> I am trying to use the RSvgDevice package to produce some SVG graphs which I 
> want to edit with Inkscape 0.42.
> [...snip...]

Argh, a minute after posting a find out the solution here:
http://www.stat.auckland.ac.nz/~paul/Talks/gridSVG/slide8.html

It works brilliant.
Adrian



From maechler at stat.math.ethz.ch  Sat Sep  3 23:50:47 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 3 Sep 2005 23:50:47 +0200
Subject: [R] Inconsistence in specifying action for missing data
In-Reply-To: <4319C3E2.1020303@stats.uwo.ca>
References: <s3198104.003@grecc.umaryland.edu> <4319C3E2.1020303@stats.uwo.ca>
Message-ID: <17178.6839.595524.809235@stat.math.ethz.ch>

>>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca>
>>>>>     on Sat, 03 Sep 2005 11:40:18 -0400 writes:

    Duncan> John Sorkin wrote:
    >> A question for R (and perhaps S and SPlus) historians.
    >> 
    >> Does anyone know the reason for the inconsistency in the
    >> way that the action that should be taken when data are
    >> missing is specified? There are several variants,
    >> na.action, na.omit, "T", TRUE, etc. I know that a foolish
    >> consistency is the hobgoblin of a small mind, but
    >> consistency can make things easier.
    >> 
    >> My question is not meant as a complaint. I very much
    >> admire the R development team. I simply am curious.

    Duncan> R and S have been developed by lots of people, over
    Duncan> a long time.  I think that's it.

yes, but there's a bit more to it.

First, the question was "wrong" (don't you just hate such an answer?):
A more interesting  question would have asked why there was 
  'na.rm = {TRUE, FALSE}' 
on one hand and
  'na.action =  {na.omit, na.replace, .....}'
on the other hand,
since only these two appear as function *arguments* 
{at least in `decent' S and R functions}.

There, the answer has at least two parts:
- First, for some functionalities,  na.rm = TRUE/FALSE is the
  only thing that makes sense, so why should you have to use
  something more complicated?

- IIRC, 'na.rm' has been much earlier (S version 2),
  than 'na.action' (S version 3; with  na.replace much later IIRC);
  na.action was really becoming relevant only when thinking
  about model fitting and non-trivial missing value treatment.

Martin Maechler, ETH Zurich



From jrhodes at stat.psych.uiuc.edu  Sun Sep  4 00:59:51 2005
From: jrhodes at stat.psych.uiuc.edu (Justin Rhodes)
Date: Sat, 03 Sep 2005 17:59:51 -0500
Subject: [R] R-square n p-value
Message-ID: <6.2.3.4.2.20050903173945.01cda5d8@cyrus.psych.uiuc.edu>

Dear R-help,

Can someone please help me discover what function or code will give 
me a p-value from the input: 1) R-square statistic from a simple 
linear regression, and 2) sample size, n

This would be greatly appreciated.  I need this because I am using a 
database that gives me R-square and sample size for multiple 
comparisons, and I wish to determine the false discovery rate using 
q-value.  Thanks again,




Justin S. Rhodes
Assistant Professor of Psychology
Affiliate, Institute for Genomic Biology and Neuroscience Program
University of Illinois at Urbana-Champaign
405 N Mathews Ave, Urbana, Il, 61801
Tel. 217-265-0021 Fax 217-244-5876
Website: http://s.psych.uiuc.edu/people/showprofile.php?id=545



From edd at debian.org  Sun Sep  4 01:19:23 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 3 Sep 2005 18:19:23 -0500
Subject: [R] R-square n p-value
In-Reply-To: <6.2.3.4.2.20050903173945.01cda5d8@cyrus.psych.uiuc.edu>
References: <6.2.3.4.2.20050903173945.01cda5d8@cyrus.psych.uiuc.edu>
Message-ID: <17178.12155.815817.768335@basebud.nulle.part>


On 3 September 2005 at 17:59, Justin Rhodes wrote:
| Dear R-help,
| 
| Can someone please help me discover what function or code will give 
| me a p-value from the input: 1) R-square statistic from a simple 
| linear regression, and 2) sample size, n
| 
| This would be greatly appreciated.  I need this because I am using a 
| database that gives me R-square and sample size for multiple 
| comparisons, and I wish to determine the false discovery rate using 
| q-value.  Thanks again,

Do
	> example(lm)			# just to get an lm object
	> str(summary(lm.D9))		# to examine summary of an object

and you'll see that the object returned from summary has the two common R^2
measures, as well as things like residuals from which can compute n quite
easily -- which you could obviously also from your regressors and regressand.

	> length(summary(lm.D9)$residuals)

Hth, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From nkn at turing.une.edu.au  Sun Sep  4 01:49:21 2005
From: nkn at turing.une.edu.au (Nam-Ky Nguyen)
Date: Sun, 4 Sep 2005 09:49:21 +1000 (EST)
Subject: [R] R binaries, platform independent and Design of Experiments
In-Reply-To: <8ed68eed050902023114a4eed4@mail.gmail.com>
References: <20735.129.180.11.34.1125460179.squirrel@129.180.11.34> 
	<43156806.4020608@hhbio.wasser.tu-dresden.de> 
	<50879.59.167.30.131.1125530048.squirrel@59.167.30.131> 
	<4316F615.1050000@statistik.uni-dortmund.de> 
	<2482.129.180.11.34.1125644753.squirrel@129.180.11.34>
	<8ed68eed050902023114a4eed4@mail.gmail.com>
Message-ID: <50698.59.167.30.131.1125791361.squirrel@59.167.30.131>

> On 02/09/05, Nam-Ky Nguyen <nkn at turing.une.edu.au> wrote:
>> > b) You do NOT want to do numerical computations on software available
>> in
>> > Java byte code.
>> You do not want to do heavy numerical computations with R either. Most
>> statistical calculation using R requires a fraction of a second and I
>> cannot see a real difference between say 0.05 second and 0.07 second.
>> NKN.
>
> It is my understanding that the problem with Java is that it wasn't
> written with serious numerical computation in mind - as far as I know
> only in the latest version have Sun started to be address this issue.
> The byte code for the java virtual machine has a flawed numerical
> model which is not fully compliant with the IEEE754 standard - this
> has nothing to do with speed of computation.  Furthermore the integer
> model is very restrictive when you want to work on random numbers
> using bit-twiddling.

I am not sure that this is an appropriate platform to compare C++ (or C#)
and Java. Besides, there are already several articles on this topic on the
web. I like the BYOL idea. Like BYOB (Bring Your Own Beer), BYO Language
will allow you to use the language of your choice for software development
(Please see http://www.mono-project.com/Languages/).

I take this oppotunity to thank Berwin, Brian, Peter and Uwe for their
helpful emails.



From spencer.graves at pdf.com  Sun Sep  4 02:49:53 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 03 Sep 2005 17:49:53 -0700
Subject: [R] lme and ordering of terms
In-Reply-To: <43132654.6070807@uni-jena.de>
References: <43132654.6070807@uni-jena.de>
Message-ID: <431A44B1.2010409@pdf.com>

	  Since I have not seen a reply to this question, I will offer 
something:  The problem with testing interactions before main effects is 
that it's not clear what interactions even mean without the main 
effects.  This is true in virtually any context, not just lme.

	  Consider the following simulated data with two factors at two levels:

 > set.seed(1)
 > DF <- data.frame(x1=rep(LETTERS[1:2], 3),
+      x2=rep(letters[3:4], each=3), y=rnorm(6))

	  If you read the code for "lm", you will find that R translates the 
explanatory variables this into numbers using "model.matrix" something 
like the following:

 > options(contrasts=c("contr.treatment", "contr.poly"))# R default
 > model.matrix(~x1*x2, DF)
   (Intercept) x1B x2d x1B:x2d
1           1   0   0       0
2           1   1   0       0
3           1   0   0       0
4           1   1   1       1
5           1   0   1       0
6           1   1   1       1
attr(,"assign")
[1] 0 1 2 3
attr(,"contrasts")
attr(,"contrasts")$x1
[1] "contr.treatment"

attr(,"contrasts")$x2
[1] "contr.treatment"

	  S-Plus uses contr.helmert rather than contr.treatment by default, so 
it's "model.matrix" looks different":

 > options(contrasts=c("contr.helmert", "contr.poly"))# S-Plus default
 > model.matrix(~x1*x2, DF, contrasts=contr.helmert)
   (Intercept) x11 x21 x11:x21
1           1  -1  -1       1
2           1   1  -1      -1
3           1  -1  -1       1
4           1   1   1       1
5           1  -1   1      -1
6           1   1   1       1
attr(,"assign")
[1] 0 1 2 3
attr(,"contrasts")
attr(,"contrasts")$x1
[1] "contr.helmert"

attr(,"contrasts")$x2
[1] "contr.helmert"

	  CONCLUSION:  The definition of "interaction" depends on the choice of 
contrast coding.

	  Fortunately, however, if we consider main effects before 
interactions, we get the same analysis of variance regardless of 
contrast coding:

 > options(contrasts=c("contr.treatment", "contr.poly"))# R default
 > fit.t <- lm(y~x1*x2, DF)
 > anova(fit.t)
Analysis of Variance Table

Response: y
           Df  Sum Sq Mean Sq F value Pr(>F)
x1         1 0.72873 0.72873  0.4958 0.5543
x2         1 0.53283 0.53283  0.3625 0.6083
x1:x2      1 0.24469 0.24469  0.1665 0.7228
Residuals  2 2.93980 1.46990

 > options(contrasts=c("contr.helmert", "contr.poly"))# S-Plus default
 > fit.h <- lm(y~x1*x2, DF)
 > anova(fit.h)
Analysis of Variance Table

Response: y
           Df  Sum Sq Mean Sq F value Pr(>F)
x1         1 0.72873 0.72873  0.4958 0.5543
x2         1 0.53283 0.53283  0.3625 0.6083
x1:x2      1 0.24469 0.24469  0.1665 0.7228
Residuals  2 2.93980 1.46990
 >
	  In brief, the analysis of variance is unchanged, because regression 
is equivalent to projecting "y" considered as a 6 dimensional vector 
onto different subspaces.  We first project y onto the subspace spanned 
by the column of all 1's and x1.  Then we add x2 to that subspace and 
project y onto that larger dimensional subspace.  Then we add the x1:x2 
interaction.  Changing the contrast coding changes the choice of 
coordinate axes but does NOT change the subspaces involved UNLESS you 
extract the terms in a different order.  For more detail, consult any 
good book on regression.

	  spencer graves

Christoph Scherber wrote:

> Dear R users,
> 
> When fitting a lme() object (from the nlme library), is it possible to 
> test interactions *before* main effects? As I understand, R 
> conventionally re-orders all terms such that highest-order interactions 
> come last - but I??d like to know if it??s possible (and sensible) to 
> change this ordering of terms.
> 
> I??ve tried the terms() command (from aov) but I don??t know if something 
> similar exists for lme() objects.
> 
> Thanks a lot for your help!
> 
> Best wishes
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From kerbo2004 at yahoo.com  Sun Sep  4 02:58:25 2005
From: kerbo2004 at yahoo.com (A Das)
Date: Sat, 3 Sep 2005 17:58:25 -0700 (PDT)
Subject: [R] survey weights
Message-ID: <20050904005825.72670.qmail@web50007.mail.yahoo.com>

Hi all, I've been trying to get a large (12mb) Stata
survey database into R. I managed that, but when I
attach survey weights, something goes wrong. The error
message is: object dchina not found. Here's the
script:

library(car)
library(foreign)
library(survey)

China <- read.dta("C:/final07c2.dta")
attach(China)

data(China)
dchina<-svydesign(id=~psu,strata=~strata,weights=~weight0x,data=China,nest=TRUE)
summary(dchina)

Any thoughts?
       -Bobby



From spencer.graves at pdf.com  Sun Sep  4 03:05:03 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 03 Sep 2005 18:05:03 -0700
Subject: [R] loglinear model selection
In-Reply-To: <6.1.2.0.2.20050830071545.0375feb0@centroin.com.br>
References: <6.1.2.0.2.20050830071545.0375feb0@centroin.com.br>
Message-ID: <431A483F.9040406@pdf.com>

	  Does the following help you:

library(MASS)

set.seed(1)
x1 <- rep(1:2, 2)
x2 <- rep(1:2, each=2)
DF <- data.frame(x1=x1, x2=x2, y=rbinom(4, 1000,
           prob=(x1+x2-1.5)/3)/1000)

fit0 <- glm(y~1, family=binomial, data=DF,
            weights=rep(1000, 4))
stepAIC(fit0, y~x1+x2)

	  spencer graves

Bernardo Rangel Tura wrote:

> Hi R-masters!
> 
> I have a problem and need your help.
> I have 9 discrete variables with 2 levels each.
> In exploratory analisys I generate one matrix with chi-square for tables 
> with 2 ariables each with this script
> 
> setwd("F:/")
> dados<-read.csv("log.csv")[,2:10]
> dados.x<-matrix(NA,ncol=9,nrow=9)
> for(i in 1:8){
> for(j in (i+1):9){
> tab<-table(dados[,i],dados[,j])
> dados.x[i,j]<-round(as.numeric(chisq.test(tab)$statistic),3)
> dados.x[j,i]<-round(as.numeric(chisq.test(tab)$statistic),3)
>   }
> }
> dados.x
> 
> The result is:
> 
>         [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]  [,8]   [,9]
>   [1,]    NA  3.589  6.351  3.957  4.269  0.851  2.955 1.188  1.975
>   [2,] 3.589     NA  9.664 24.596 12.510 26.284  8.580 3.608  6.574
>   [3,] 6.351  9.664     NA 25.054 10.300 12.189 19.811 0.192 11.744
>   [4,] 3.957 24.596 25.054     NA 50.032 35.587 22.401 1.950 26.631
>   [5,] 4.269 12.510 10.300 50.032     NA 80.876 54.954 0.127 61.573
>   [6,] 0.851 26.284 12.189 35.587 80.876     NA 57.346 0.741 49.738
>   [7,] 2.955  8.580 19.811 22.401 54.954 57.346     NA 1.520 80.615
>   [8,] 1.188  3.608  0.192  1.950  0.127  0.741  1.520    NA  0.311
>   [9,] 1.975  6.574 11.744 26.631 61.573 49.738 80.615 0.311     NA
> 
> Now I need fit a loglinear model with this variables, but I need know have 
> a command with generate ALL models with the set this 8 vairables (ALL minus 
> [,8]) incluindind the interactions.
> 
> Can Anyone Help me?
> 
> 
> 
> Thanks in advance
> 
> Bernardo Rangel Tura, MD, MSc
> National Institute of Cardiology Laranjeiras
> Rio de Janeiro Brazil 
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Sep  4 03:13:23 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 03 Sep 2005 18:13:23 -0700
Subject: [R] under sample problem
In-Reply-To: <7d.6fd8d2d9.30462916@aol.com>
References: <7d.6fd8d2d9.30462916@aol.com>
Message-ID: <431A4A33.3070704@pdf.com>

	  Does the following help:

 > set.seed(1)
 > pop <- 1:100
 > s1 <- sample(pop, 5)
 > s1. <- pop[!(pop %in% s1)]
 > s2 <- sample(s1., 5)
 > s1
[1] 27 37 57 89 20
 > s2
[1] 91 94 66 62  6
 >
	  spencer graves

emmadigonnet at aol.com wrote:

>  
> Hello, 
> I  have a problem to treat my data. I seek the orders being able to treat 
> under  sampling: I have X samples divided into 10. How to take, in a random way, 
> under  sample from the 1st sample, and in addition, one under sample of the 
> 2nd sample,  and so on to X to calculate the average taxonomic richness of the  
> selection. 
> Then  I would like to know how to renew the experiment by taking this time 2, 
> 3... X  under samples at the same time, always in a random way.  
> Thank you in advance for your answer. 
> Manue.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From cberry at tajo.ucsd.edu  Sun Sep  4 03:21:01 2005
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Sat, 3 Sep 2005 18:21:01 -0700
Subject: [R] Finding all overlaps between two sets of 1-Dimensional
 regions
In-Reply-To: <BF3DC940.DB86%sdavis2@mail.nih.gov>
References: <BF3DC940.DB86%sdavis2@mail.nih.gov>
Message-ID: <Pine.LNX.4.63.0509031809500.2964@tajo.ucsd.edu>

On Fri, 2 Sep 2005, Sean Davis wrote:

> I have a simply defined regions ([start,end] where start<end).  I have two
> large sets of them and want to find all regions in the first that overlap
> any regions in the second.  The closest I could find by searching is
> overlap.owin in I can do this by looping, but there is likely a better way
> to do this.  Any suggestions?


findInterval can be helpful and fast for larger problems

Say s1,e1 (>0) are the starts and ends of your first set and s2,e2 (>0) 
are for the second and are in order according to s2 .

I think this is what you need to get going:

s2.0 <- c(0, s2) # if any s1 < any s2, this is needed
e2.0 <- c(0, e2)

s1.in.int <- s1 <= e.2.0 [ findInterval( s1, s2.0 ) ]
e1.in.same.int <- e1 <= e.2.0 [ findInterval( s1, s2.0 ) ]

e1.in.any.int <- e1 <= e.2.0 [ findInterval( e1, s2.0 ) ]

Caveat: Possible typos above

Chuck

>
> Thanks,
> Sean
>
>
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717



From p.dalgaard at biostat.ku.dk  Sun Sep  4 09:59:54 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Sep 2005 09:59:54 +0200
Subject: [R] Inconsistence in specifying action for missing data
In-Reply-To: <17178.6839.595524.809235@stat.math.ethz.ch>
References: <s3198104.003@grecc.umaryland.edu> <4319C3E2.1020303@stats.uwo.ca>
	<17178.6839.595524.809235@stat.math.ethz.ch>
Message-ID: <x2u0h11cw5.fsf@turmalin.kubism.ku.dk>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

> >>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca>
> >>>>>     on Sat, 03 Sep 2005 11:40:18 -0400 writes:
> 
>     Duncan> John Sorkin wrote:
>     >> A question for R (and perhaps S and SPlus) historians.
>     >> 
>     >> Does anyone know the reason for the inconsistency in the
>     >> way that the action that should be taken when data are
>     >> missing is specified? There are several variants,
>     >> na.action, na.omit, "T", TRUE, etc. I know that a foolish
>     >> consistency is the hobgoblin of a small mind, but
>     >> consistency can make things easier.
>     >> 
>     >> My question is not meant as a complaint. I very much
>     >> admire the R development team. I simply am curious.
> 
>     Duncan> R and S have been developed by lots of people, over
>     Duncan> a long time.  I think that's it.
> 
> yes, but there's a bit more to it.
> 
> First, the question was "wrong" (don't you just hate such an answer?):
> A more interesting  question would have asked why there was 
>   'na.rm = {TRUE, FALSE}' 
> on one hand and
>   'na.action =  {na.omit, na.replace, .....}'
> on the other hand,
> since only these two appear as function *arguments* 
> {at least in `decent' S and R functions}.

So cor() is "indecent" (with its use= argument)? ;-)


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Sun Sep  4 10:43:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Sep 2005 10:43:22 +0200
Subject: [R] R-square n p-value
In-Reply-To: <17178.12155.815817.768335@basebud.nulle.part>
References: <6.2.3.4.2.20050903173945.01cda5d8@cyrus.psych.uiuc.edu>
	<17178.12155.815817.768335@basebud.nulle.part>
Message-ID: <x2psrp1avp.fsf@turmalin.kubism.ku.dk>

Dirk Eddelbuettel <edd at debian.org> writes:

> On 3 September 2005 at 17:59, Justin Rhodes wrote:
> | Dear R-help,
> | 
> | Can someone please help me discover what function or code will give 
> | me a p-value from the input: 1) R-square statistic from a simple 
> | linear regression, and 2) sample size, n
> | 
> | This would be greatly appreciated.  I need this because I am using a 
> | database that gives me R-square and sample size for multiple 
> | comparisons, and I wish to determine the false discovery rate using 
> | q-value.  Thanks again,
> 
> Do
> 	> example(lm)			# just to get an lm object
> 	> str(summary(lm.D9))		# to examine summary of an object
> 
> and you'll see that the object returned from summary has the two common R^2
> measures, as well as things like residuals from which can compute n quite
> easily -- which you could obviously also from your regressors and regressand.
> 
> 	> length(summary(lm.D9)$residuals)

I think the problem was somewhat different: The *input* is coming from
some sort of (closed-source or otherwise impenetrable) database which
only gives out n and R^2, right?

Now R^2 = SSDmodel/(SSDmodel+SSDres) and F =
DFres/DFmodel*SSDmodel/SSDres, i.e. 

  1/R^2 = 1 + 1/F*DFmodel/DFres

or 

  F = 1/(1/R^2 - 1)*DFres/DFmodel = R^2/(1-R^2)*DFres/DFmodel

which can be looked up "in the F-table" using 

  pf(F, 1, N-2, lower.tail=FALSE)
 
(provided we have a 1 DF model)

Actually, R^2 itself has a beta distribution and you could use pbeta
directly, but then you'd need to figure out (or recall) what the
relation between the DF and the shape parameters of the beta
distribution are. By my reckoning, this should do it:

  pbeta(Rsq, 1/2, (N-2)/2, lower.tail=FALSE) 

"Proof":

....
Residual standard error: 1.143 on 8 degrees of freedom
Multiple R-Squared: 0.0004207,  Adjusted R-squared: -0.1245
F-statistic: 0.003367 on 1 and 8 DF,  p-value: 0.9552

> pbeta(0.0004207, 1/2, 8/2, lower=F)
[1] 0.9551511


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From r-stats at arcriswell.com  Sun Sep  4 10:48:26 2005
From: r-stats at arcriswell.com (Andrew R. Criswell)
Date: Sun, 04 Sep 2005 10:48:26 +0200
Subject: [R] specification for glmmPQL
Message-ID: <431AB4DA.5040606@arcriswell.com>

Hello All,

I have a question regarding how glmmPQL should be specified. Which of 
these two is correct?

summary(fm.3 <- glmmPQL(cbind(response, 100 - response) ~ expt,
                        data = data.1, random = ~ 1 | subject,
                        family = binomial))

summary(fm.4 <- glmmPQL(response ~ expt, data = data.2,
                        random = ~ 1 | subject, family = binomial))

One might think it makes no difference, but it does.

I have an experiment in which 8 individuals were subjected to two types 
of treatment, 100 times per day for 4 consecutive days. The response 
given is binary--yes or no--for each treatment.

I constructed two types of data sets. On Rfile-01.Rdata (attached here) 
are data frames, data.1 and data.2. The information is identical but the 
data are arranged differently between these two data frames. Data frame, 
data.1, groups frequencies by subject, day and treatment. Data frame, 
data.2, is ungrouped.

Consistency of these data frames is substantiated by computing these 
tables:

ftable(xtabs(response ~ expt + subject + day,
             data = data.1))
ftable(xtabs(as.numeric(response) - 1 ~ expt + subject + day,
             data = data.2))

If I ignore the repeated measurement aspect of the data, I get, using 
glm, identical results (but for deviance and df).

summary(fm.1 <- glm(cbind(response, 100 - response) ~ expt,
                    data = data.1, family = binomial))

summary(fm.2 <- glm(response ~ expt, data = data.2,
                    family = binomial))

However, if I estimate these two equations as a mixed model using 
glmPQL, I get completely different results between the two 
specifications, fm.3 and fm.4. Which one is right? The example which 
accompanies help(glmmPQL) suggests fm.4.

summary(fm.3 <- glmmPQL(cbind(response, 100 - response) ~ expt,
                        data = data.1, random = ~ 1 | subject,
                        family = binomial))

summary(fm.4 <- glmmPQL(response ~ expt, data = data.2,
                        random = ~ 1 | subject, family = binomial))

Thank you,
Andrew



From Bancoposta at poste.it  Sun Sep  4 11:39:27 2005
From: Bancoposta at poste.it (Bancoposta@poste.it)
Date: Sun, 04 Sep 2005 11:39:27 +0200
Subject: [R] Misure di sicurezza di cliente di BancoPosta ID0665
Message-ID: <E1EBqyF-0008O4-3b@server.spiderspider.nl>



From anette at geoplus.dk  Sun Sep  4 12:45:06 2005
From: anette at geoplus.dk (=?iso-8859-1?Q?Anette_N=F8rgaard?=)
Date: Sun, 4 Sep 2005 12:45:06 +0200
Subject: [R] time series graphs
Message-ID: <000001c5b13d$b6d486b0$0302a8c0@anette>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050904/500eefd0/attachment.pl

From anette at geoplus.dk  Sun Sep  4 13:04:08 2005
From: anette at geoplus.dk (=?iso-8859-1?Q?Anette_N=F8rgaard?=)
Date: Sun, 4 Sep 2005 13:04:08 +0200
Subject: [R] time series graphs
Message-ID: <000501c5b140$5faa0b50$0302a8c0@anette>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050904/9e6c7a21/attachment.pl

From patrick.giraudoux at univ-fcomte.fr  Sun Sep  4 14:52:01 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sun, 04 Sep 2005 14:52:01 +0200
Subject: [R] unexpected error message with variog.mc.env() - geoR
Message-ID: <431AEDF1.2080205@univ-fcomte.fr>

Dear R-listers,

I have got an error with variog.mc.env() package:geoR that I cannot sort 
the origin out.  The origianal data file can be sent to people interested.

bin0<-variog(don1bgeo,estimator.type="modulus",  direction=0)
bin90<-variog(don1bgeo,estimator.type="modulus",  direction=pi/2)
env.variog0<-variog.mc.env(don1bgeo,obj.variog=bin0)
env.variog90<-variog.mc.env(don1bgeo,obj.variog=bin90)

everything goes smoothly with bin90, but using bin0 gives this error
after permutations:

 > Error in variog.mc.env(don1bgeo, obj.variog = bin0) :
         (subscript) logical subscript too long

Any idea about what happens?

Regards,

Patrick

-- 

Department of Environmental Biology
EA3184 usc INRA
University of Franche-Comte
25030 Besancon Cedex
(France)

tel. +33 381 665 745
fax +33 381 665 797
http://lbe.univ-fcomte.fr



From ggrothendieck at gmail.com  Sun Sep  4 15:52:56 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 4 Sep 2005 09:52:56 -0400
Subject: [R] time series graphs
In-Reply-To: <000501c5b140$5faa0b50$0302a8c0@anette>
References: <000501c5b140$5faa0b50$0302a8c0@anette>
Message-ID: <971536df0509040652157cb8a0@mail.gmail.com>

On 9/4/05, Anette N??rgaard <anette at geoplus.dk> wrote:
> About time series graphs, I need help to move on:
> 
> A time series of data directly from a data logger comes in the dat
> format created below:
> 
> year<-c(rep(2005,10))
> doy<-c(rep(173,5),rep(174,5))
> time<-c(15,30,45,100,115,15,30,45,100,115)
> dat1<-c(0.022128,0.035036,0.051632,0.071916,0.081136,0.07837,0.083902,0.
> 126314,0.080214,0.117094)
> dat2<-
> c(0.533074667,0.887982667,1.284938,1.845450333,2.145839333,2.145126667,2
> .392422,3.60253,2.330776333,3.5277)
> dat<-cbind(year,doy,time,dat1,dat2)
> dat
> 
> time 15 corresponding to 00:15 (past 2400)
> 
> I'd like to make a graph illustrating all observation from
> the two time series dat1 and dat2
> in one graph with different possibilities of labeling the x-axis.
> 
> 1st wish:
> with the date and time drawn on the x-axis every hour (meaning every
> fourth observation)
> e.g. June 24 01:00, Jun 24 02:00 etc.
> 
> 2nd example:
> with the date label once per 24 hour period
> e.g. June 24
> 
> I've tried many things and have become very confused on the "ts"
> possibilities. Everything I have is
> timeseries, so it is very important to get a grip on this.
> 
> Please give me a hint.
> 

If you are going to be doing a lot of time series
manipulations that have dates and times then read
R News 4/1 Help Desk for an article about dates/times
and read the zoo vignette for information on zoo.

library(zoo)
vignette("zoo")


You don't seem to have enough room to label the hours
but you could label the days and just put ticks for
the hours.

Create a zoo series using chron dates and times and just
plot it.  That may be sufficient.

library(chron)
library(zoo)

# convert date/times to chron
tt <- chron(paste(1,1,dat[,1],sep="/"),dat[,3]/24)+dat[,2]-1

# create a zoo series using your data and the chron dates/times
z <- zoo(dat[,4:5],tt)

plot(z, plot.type = "single")

# Alternative.
# If you want more control, plot it without the X axis
# and use axis twice to draw the X axis yourself.  Use the
# tcl= argument to vary the tick sizes.

plot(z, plot.type = "single", xaxt = "n")

# X axis labelling days
dd <- seq(min(floor(tt)), max(floor(tt)))
axis(1, dd, format(as.Date(dd), "%b %d"), tcl = -0.6)

# X axis with just ticks for hours
hh <- seq(min(tt), max(tt), by = 1/24)
axis(1, hh, FALSE, tcl = -0.4)



From r-stats at arcriswell.com  Sun Sep  4 17:33:30 2005
From: r-stats at arcriswell.com (Andrew R. Criswell)
Date: Sun, 04 Sep 2005 17:33:30 +0200
Subject: [R] specification for glmmPQL
In-Reply-To: <40e66e0b05090408265bdcfbae@mail.gmail.com>
References: <431AB4DA.5040606@arcriswell.com>
	<40e66e0b05090408265bdcfbae@mail.gmail.com>
Message-ID: <431B13CA.7000908@arcriswell.com>

Hello Dr. Bates and group,

I understand, the attached data file did not accompany my original 
message. I have listed below the code used to create that file.

data.1 <- data.frame(subject  = factor(rep(c("one", "two", "three", "four",
                                             "five", "six", "seven", 
"eight"),
                                           each = 4),
                                       levels = c("one", "two", "three",
                                                  "four", "five", "six",
                                                  "seven", "eight")),
                     day      = factor(rep(c("one", "two", "three", "four"),
                                           times = 8),
                                       levels = c("one", "two", "three",
                                                  "four")),
                     expt     = rep(c("control", "treatment"), each = 16),
                     response = c(58, 63, 57, 54, 63, 59, 61, 53, 52, 62,
                                  46, 55, 59, 63, 58, 59, 62, 59, 64, 53,
                                  63, 75, 62, 64, 53, 58, 62, 53, 64, 72,
                                  65, 74))

mtrx.1 <- matrix(apply(data.1[, -4], 2, function(x)
                 rep(x, 100 - data.1$response)), ncol = 3, byrow = F)
mtrx.2 <- matrix(apply(data.1[, -4], 2, function(x)
                 rep(x, data.1$response)), ncol = 3, byrow = F)
                
data.2 <- data.frame(subject  = factor(c(mtrx.1[,1], mtrx.2[,1]),
                                       levels = c("one", "two", "three",
                                                  "four", "five", "six",
                                                  "seven", "eight")),
                     day      = factor(c(mtrx.1[,2], mtrx.2[,2]),
                                       levels = c("one", "two", "three",
                                                  "four")),
                     expt     = factor(c(mtrx.1[,3], mtrx.2[,3]),
                                       levels = c("control", "treatment")),
                     response = factor(c(rep("yes", nrow(mtrx.1)),
                                         rep("no", nrow(mtrx.2))),
                                       levels = c("yes", "no")))

#-------------------------------------------------------------------------------#


Douglas Bates wrote:

>On 9/4/05, Andrew R. Criswell <r-stats at arcriswell.com> wrote:
>
>>Hello All,
>>
>>I have a question regarding how glmmPQL should be specified. Which of
>>these two is correct?
>>
>>summary(fm.3 <- glmmPQL(cbind(response, 100 - response) ~ expt,
>>                        data = data.1, random = ~ 1 | subject,
>>                        family = binomial))
>>
>>summary(fm.4 <- glmmPQL(response ~ expt, data = data.2,
>>                        random = ~ 1 | subject, family = binomial))
>>
>>One might think it makes no difference, but it does.
>>
>>I have an experiment in which 8 individuals were subjected to two types
>>of treatment, 100 times per day for 4 consecutive days. The response
>>given is binary--yes or no--for each treatment.
>>
>>I constructed two types of data sets. On Rfile-01.Rdata (attached here)
>>are data frames, data.1 and data.2. The information is identical but the
>>data are arranged differently between these two data frames. Data frame,
>>data.1, groups frequencies by subject, day and treatment. Data frame,
>>data.2, is ungrouped.
>>
>
>I don't think your attached .Rdata file made it through the various
>filters on the lists or on my receiving email.  Could you send me a
>copy in a separate email message?
>
>
>>Consistency of these data frames is substantiated by computing these
>>tables:
>>
>>ftable(xtabs(response ~ expt + subject + day,
>>             data = data.1))
>>ftable(xtabs(as.numeric(response) - 1 ~ expt + subject + day,
>>             data = data.2))
>>
>>If I ignore the repeated measurement aspect of the data, I get, using
>>glm, identical results (but for deviance and df).
>>
>>summary(fm.1 <- glm(cbind(response, 100 - response) ~ expt,
>>                    data = data.1, family = binomial))
>>
>>summary(fm.2 <- glm(response ~ expt, data = data.2,
>>                    family = binomial))
>>
>>However, if I estimate these two equations as a mixed model using
>>glmPQL, I get completely different results between the two
>>specifications, fm.3 and fm.4. Which one is right? The example which
>>accompanies help(glmmPQL) suggests fm.4.
>>
>>summary(fm.3 <- glmmPQL(cbind(response, 100 - response) ~ expt,
>>                        data = data.1, random = ~ 1 | subject,
>>                        family = binomial))
>>
>>summary(fm.4 <- glmmPQL(response ~ expt, data = data.2,
>>                        random = ~ 1 | subject, family = binomial))
>>
>>Thank you,
>>Andrew
>>
>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>
>
>
>



From tlumley at u.washington.edu  Sun Sep  4 18:42:37 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 4 Sep 2005 09:42:37 -0700 (PDT)
Subject: [R] Inconsistence in specifying action for missing data
In-Reply-To: <s3198104.003@grecc.umaryland.edu>
References: <s3198104.003@grecc.umaryland.edu>
Message-ID: <Pine.A41.4.63a.0509040935580.174230@homer12.u.washington.edu>

On Sat, 3 Sep 2005, John Sorkin wrote:

> A question for R (and perhaps S and SPlus) historians.
>
> Does anyone know the reason for the inconsistency in the way that the
> action that should be taken when data are missing is specified? There
> are several variants, na.action, na.omit, "T", TRUE,  etc. I know that a
> foolish consistency is the hobgoblin of a small mind, but consistency
> can make things easier.
>

There's actually a little more consistency than first appears.  There are 
two most common ways to refer to missingness,  na.rm and na.action.  Usually 
na.rm has default TRUE (using T is a bug) and removes NAs from one vector 
at a time.

na.action usually has default na.omit() and works on whole data frames, eg 
na.omit and na.exclude do casewise deletion if any variable is NA.

These aren't completely uniform, and that is simply historical. I think 
there was once an attempt to make na.fail() the default na.action, but 
there was too much resistance to change.

 	-thomas



From tlumley at u.washington.edu  Sun Sep  4 18:52:31 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 4 Sep 2005 09:52:31 -0700 (PDT)
Subject: [R] survey weights
In-Reply-To: <20050904005825.72670.qmail@web50007.mail.yahoo.com>
References: <20050904005825.72670.qmail@web50007.mail.yahoo.com>
Message-ID: <Pine.A41.4.63a.0509040946550.174230@homer12.u.washington.edu>

On Sat, 3 Sep 2005, A Das wrote:

> Hi all, I've been trying to get a large (12mb) Stata
> survey database into R. I managed that, but when I
> attach survey weights, something goes wrong. The error
> message is: object dchina not found. Here's the
> script:

If that is the *first* message then something extremly strange is 
happening

> library(car)
> library(foreign)
> library(survey)
>
> China <- read.dta("C:/final07c2.dta")
> attach(China)

This attach() isn't necessary or helpful

> data(China)
You should get a warning here

Warning message:
data set 'China' not found in: data(China)

since China isn't one of the built-in data sets. If you don't get this 
message it suggests that you do have a built-in dataset called China, 
which will have overwritten your file.

> dchina<-svydesign(id=~psu,strata=~strata,weights=~weight0x,
                          data=China,nest=TRUE)

If this line doesn't produce an error message then a variable called 
"dchina" must have been produced, in which case you shouldn't get an error 
message saying it wasn't found in the next line.

> summary(dchina)
>


Are you sure there wasn't an earlier error message from the call to 
svydesign()?

 	-thomas



From bernd.weiss at uni-koeln.de  Sun Sep  4 18:58:30 2005
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Sun, 04 Sep 2005 18:58:30 +0200
Subject: [R] Question regarding lmer with binary response
Message-ID: <431B43D6.26777.2CEBD8F@localhost>

Dear all, dear Prof. Bates,

my dependent variable (school absenteeism, truancy[1]) is a binary 
response for which I am trying to compute an unconditional mixed 
effects model. I've got observations (monday, wednesday and friday) 
nested in individuals (ID2), which were nested in classes (KID2) and 
schools (SID), i.e. a 4-level mixed effects model. 

In short, I was trying without success. I got no sensible results 
using lmer as well as using glmmPQL. I played around with the control 
parameters and the methods (PQl, Laplace) in lmer without any effect. 
 
I would really appreciate if someone could have a look into my data 
and tell me what's going wrong here. 

My R script and data can be found at: 

http://www.metaanalyse.de/tmp/rhelp.R
http://www.metaanalyse.de/tmp/rhelp.txt

TIA,

Bernd



From h.wickham at gmail.com  Sun Sep  4 19:05:23 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 4 Sep 2005 12:05:23 -0500
Subject: [R] Displaying RProf output
Message-ID: <f8e6ff05050904100532c6a6ad@mail.gmail.com>

Hi,

I've been experimenting with a new way of displaying the output from
RProf, to make it easier to optimise your functions.  I've included an
example below.  I'd love to get your feedback on how easy you think
this graphic is to read, and on ways that it could be improved.

install.packages("butler")
library(butler)

# profile the glm example
profile_glm <- stopwatch(function() example(glm))

# Plot the profile
# y-axis gives percent of time spent in that function
# x-axis gives position in call stack
plot(profile_glm)

# The first few levels aren't of interest, so we can skip them:
# (see ?plot.call.tree for all options)
plot(profile_glm, startlevel=4)

# We might also want to see what's going further down
plot(profile_glm, startlevel=4, depth=10)

# By default only functions that take longer than 2% of the 
# total time are shown, setting mintime changes that
plot(profile_glm, startlevel=4, depth=10, mintime=1)

One interesting thing you can see from this example is that almost 30%
of the total time is just spent printing the output - why is it so
slow?  Well, it looks like print.summary.glm calculates a lot of the
summary statistics.

Thanks,

Hadley



From dmbates at gmail.com  Sun Sep  4 19:19:09 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 4 Sep 2005 12:19:09 -0500
Subject: [R] specification for glmmPQL
In-Reply-To: <431B13CA.7000908@arcriswell.com>
References: <431AB4DA.5040606@arcriswell.com>
	<40e66e0b05090408265bdcfbae@mail.gmail.com>
	<431B13CA.7000908@arcriswell.com>
Message-ID: <40e66e0b05090410192b8bfb06@mail.gmail.com>

On 9/4/05, Andrew R. Criswell <r-stats at arcriswell.com> wrote:
> Hello Dr. Bates and group,
> 
> I understand, the attached data file did not accompany my original
> message. I have listed below the code used to create that file.
> 
> data.1 <- data.frame(subject  = factor(rep(c("one", "two", "three", "four",
>                                              "five", "six", "seven",
> "eight"),
>                                            each = 4),
>                                        levels = c("one", "two", "three",
>                                                   "four", "five", "six",
>                                                   "seven", "eight")),
>                      day      = factor(rep(c("one", "two", "three", "four"),
>                                            times = 8),
>                                        levels = c("one", "two", "three",
>                                                   "four")),
>                      expt     = rep(c("control", "treatment"), each = 16),
>                      response = c(58, 63, 57, 54, 63, 59, 61, 53, 52, 62,
>                                   46, 55, 59, 63, 58, 59, 62, 59, 64, 53,
>                                   63, 75, 62, 64, 53, 58, 62, 53, 64, 72,
>                                   65, 74))
> 
> mtrx.1 <- matrix(apply(data.1[, -4], 2, function(x)
>                  rep(x, 100 - data.1$response)), ncol = 3, byrow = F)
> mtrx.2 <- matrix(apply(data.1[, -4], 2, function(x)
>                  rep(x, data.1$response)), ncol = 3, byrow = F)
> 
> data.2 <- data.frame(subject  = factor(c(mtrx.1[,1], mtrx.2[,1]),
>                                        levels = c("one", "two", "three",
>                                                   "four", "five", "six",
>                                                   "seven", "eight")),
>                      day      = factor(c(mtrx.1[,2], mtrx.2[,2]),
>                                        levels = c("one", "two", "three",
>                                                   "four")),
>                      expt     = factor(c(mtrx.1[,3], mtrx.2[,3]),
>                                        levels = c("control", "treatment")),
>                      response = factor(c(rep("yes", nrow(mtrx.1)),
>                                          rep("no", nrow(mtrx.2))),
>                                        levels = c("yes", "no")))
> 
> #-------------------------------------------------------------------------------#

Thanks for sending the data.

In your first message you said that you got completely different
results from glmmPQL when fitting the two models.  When I fit these
models with glmmPQL I got quite similar parameter estimates.  The
reported log-likelihood or AIC or BIC values are quite different but
these values apply to a different model (the list weighted linear
mixed model used in the PQL algorithm) and should not be used for a
glmm model in any case.

The fm4 results from lmer in the lme4 package (actually lmer is now in
the Matrix package but that is only temporary) confirm those from
glmmPQL.  The model fm3 when fit by lmer provides different standard
errors but that is because the weights are not being appropriately
adjusted in lmer.  We will fix that.

In general I think it is safest to use the long form of the data as in
your data.2.

Here are the results from lmer applied to the long form.  The results
from the Adaptive Gauss-Hermite Quadrature (AGQ) method are preferred
to those from the PQL method because AGQ is a more accurate
approximation to the log-likelihood of the GLMM model.  In this case
the differences are minor.

The log-likelihood reported here is an approximation to the
log-likelihood of the GLMM model.

> (fm.4 <- lmer(response ~ expt + (1|subject), data.2, binomial))
Generalized linear mixed model fit using PQL 
Formula: response ~ expt + (1 | subject) 
   Data: data.2 
 Family: binomial(logit link)
      AIC     BIC    logLik deviance
 4298.026 4322.31 -2145.013 4290.026
Random effects:
     Groups        Name    Variance    Std.Dev. 
    subject (Intercept)    0.015835     0.12584 
# of obs: 3200, groups: subject, 8

Estimated scale (compare to 1)  0.9990621 

Fixed effects:
              Estimate Std. Error z value  Pr(>|z|)
(Intercept)    0.30764    0.08075  3.8098 0.0001391
expttreatment  0.21319    0.11473  1.8582 0.0631454
> (fm.4a <- lmer(response ~ expt + (1|subject), data.2, binomial, method = "AGQ"))
Generalized linear mixed model fit using AGQ 
Formula: response ~ expt + (1 | subject) 
   Data: data.2 
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 4298.023 4322.306 -2145.011 4290.023
Random effects:
     Groups        Name    Variance    Std.Dev. 
    subject (Intercept)    0.015855     0.12592 
# of obs: 3200, groups: subject, 8

Estimated scale (compare to 1)  1.007675 

Fixed effects:
              Estimate Std. Error z value  Pr(>|z|)
(Intercept)    0.30811    0.08075  3.8156 0.0001358
expttreatment  0.21352    0.11473  1.8611 0.0627322


> 
> Douglas Bates wrote:
> 
> >On 9/4/05, Andrew R. Criswell <r-stats at arcriswell.com> wrote:
> >
> >>Hello All,
> >>
> >>I have a question regarding how glmmPQL should be specified. Which of
> >>these two is correct?
> >>
> >>summary(fm.3 <- glmmPQL(cbind(response, 100 - response) ~ expt,
> >>                        data = data.1, random = ~ 1 | subject,
> >>                        family = binomial))
> >>
> >>summary(fm.4 <- glmmPQL(response ~ expt, data = data.2,
> >>                        random = ~ 1 | subject, family = binomial))
> >>
> >>One might think it makes no difference, but it does.
> >>
> >>I have an experiment in which 8 individuals were subjected to two types
> >>of treatment, 100 times per day for 4 consecutive days. The response
> >>given is binary--yes or no--for each treatment.
> >>
> >>I constructed two types of data sets. On Rfile-01.Rdata (attached here)
> >>are data frames, data.1 and data.2. The information is identical but the
> >>data are arranged differently between these two data frames. Data frame,
> >>data.1, groups frequencies by subject, day and treatment. Data frame,
> >>data.2, is ungrouped.
> >>
> >
> >I don't think your attached .Rdata file made it through the various
> >filters on the lists or on my receiving email.  Could you send me a
> >copy in a separate email message?
> >
> >
> >>Consistency of these data frames is substantiated by computing these
> >>tables:
> >>
> >>ftable(xtabs(response ~ expt + subject + day,
> >>             data = data.1))
> >>ftable(xtabs(as.numeric(response) - 1 ~ expt + subject + day,
> >>             data = data.2))
> >>
> >>If I ignore the repeated measurement aspect of the data, I get, using
> >>glm, identical results (but for deviance and df).
> >>
> >>summary(fm.1 <- glm(cbind(response, 100 - response) ~ expt,
> >>                    data = data.1, family = binomial))
> >>
> >>summary(fm.2 <- glm(response ~ expt, data = data.2,
> >>                    family = binomial))
> >>
> >>However, if I estimate these two equations as a mixed model using
> >>glmPQL, I get completely different results between the two
> >>specifications, fm.3 and fm.4. Which one is right? The example which
> >>accompanies help(glmmPQL) suggests fm.4.
> >>
> >>summary(fm.3 <- glmmPQL(cbind(response, 100 - response) ~ expt,
> >>                        data = data.1, random = ~ 1 | subject,
> >>                        family = binomial))
> >>
> >>summary(fm.4 <- glmmPQL(response ~ expt, data = data.2,
> >>                        random = ~ 1 | subject, family = binomial))
> >>
> >>Thank you,
> >>Andrew
> >>
> >>
> >>
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
> >>
> >
> >
> >
> 
>



From kerbo2004 at yahoo.com  Sun Sep  4 19:29:10 2005
From: kerbo2004 at yahoo.com (A Das)
Date: Sun, 4 Sep 2005 10:29:10 -0700 (PDT)
Subject: [R] survey weights
In-Reply-To: <Pine.A41.4.63a.0509040946550.174230@homer12.u.washington.edu>
Message-ID: <20050904172910.49829.qmail@web50011.mail.yahoo.com>

Thanks, Thomas.
    Yes, that's exactly what happened: the warnings
came first after "data(China)", and then after
"dchina<-svydesign..." So the design object isn't
being produced? The dataset is very large, and the
weights were already set in Stata before importing.
Would either of those cause problems?
                           -Bobby
                                                      
                             

--- Thomas Lumley <tlumley at u.washington.edu> wrote:

> On Sat, 3 Sep 2005, A Das wrote:
> 
> > Hi all, I've been trying to get a large (12mb)
> Stata
> > survey database into R. I managed that, but when I
> > attach survey weights, something goes wrong. The
> error
> > message is: object dchina not found. Here's the
> > script:
> 
> If that is the *first* message then something
> extremly strange is 
> happening
> 
> > library(car)
> > library(foreign)
> > library(survey)
> >
> > China <- read.dta("C:/final07c2.dta")
> > attach(China)
> 
> This attach() isn't necessary or helpful
> 
> > data(China)
> You should get a warning here
> 
> Warning message:
> data set 'China' not found in: data(China)
> 
> since China isn't one of the built-in data sets. If
> you don't get this 
> message it suggests that you do have a built-in
> dataset called China, 
> which will have overwritten your file.
> 
> >
>
dchina<-svydesign(id=~psu,strata=~strata,weights=~weight0x,
>                           data=China,nest=TRUE)
> 
> If this line doesn't produce an error message then a
> variable called 
> "dchina" must have been produced, in which case you
> shouldn't get an error 
> message saying it wasn't found in the next line.
> 
> > summary(dchina)
> >
> 
> 
> Are you sure there wasn't an earlier error message
> from the call to 
> svydesign()?
> 
>  	-thomas
>



From tlumley at u.washington.edu  Sun Sep  4 19:36:09 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 4 Sep 2005 10:36:09 -0700 (PDT)
Subject: [R] survey weights
In-Reply-To: <20050904172910.49829.qmail@web50011.mail.yahoo.com>
References: <20050904172910.49829.qmail@web50011.mail.yahoo.com>
Message-ID: <Pine.A41.4.63a.0509041035160.215666@homer09.u.washington.edu>

On Sun, 4 Sep 2005, A Das wrote:

> Thanks, Thomas.
>    Yes, that's exactly what happened: the warnings
> came first after "data(China)", and then after
> "dchina<-svydesign..." So the design object isn't
> being produced? The dataset is very large, and the
> weights were already set in Stata before importing.
> Would either of those cause problems?

Probably not.  What was the error message from svydesign()?  That is what 
will say what went wrong.

 	-thomas



From kerbo2004 at yahoo.com  Sun Sep  4 19:44:05 2005
From: kerbo2004 at yahoo.com (A Das)
Date: Sun, 4 Sep 2005 10:44:05 -0700 (PDT)
Subject: [R] survey weights
In-Reply-To: <Pine.A41.4.63a.0509041035160.215666@homer09.u.washington.edu>
Message-ID: <20050904174406.85674.qmail@web50008.mail.yahoo.com>

Just: "missing values in object". That would imply the
object was created. But then I write "dchina", and it
says "object dchina not found". 
                          -Bobby

--- Thomas Lumley <tlumley at u.washington.edu> wrote:

> On Sun, 4 Sep 2005, A Das wrote:
> 
> > Thanks, Thomas.
> >    Yes, that's exactly what happened: the warnings
> > came first after "data(China)", and then after
> > "dchina<-svydesign..." So the design object isn't
> > being produced? The dataset is very large, and the
> > weights were already set in Stata before
> importing.
> > Would either of those cause problems?
> 
> Probably not.  What was the error message from
> svydesign()?  That is what 
> will say what went wrong.
> 
>  	-thomas
>



From tlumley at u.washington.edu  Sun Sep  4 20:03:59 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 4 Sep 2005 11:03:59 -0700 (PDT)
Subject: [R] survey weights
In-Reply-To: <20050904174406.85674.qmail@web50008.mail.yahoo.com>
References: <20050904174406.85674.qmail@web50008.mail.yahoo.com>
Message-ID: <Pine.A41.4.63a.0509041045020.215666@homer09.u.washington.edu>

On Sun, 4 Sep 2005, A Das wrote:

> Just: "missing values in object". That would imply the
> object was created. But then I write "dchina", and it
> says "object dchina not found".

No, it would not imply the object was created.  If it was an error message 
(rather than a warning) the object would not have been created.

I presume the full message was
  Error in na.fail.default(object) : missing values in object

If so, it sounds as though you have missing values in the id, weights, or 
strata variable.
    summary(China[,c("psu","stata","weight0x"])
will verify this.

Stata will just have dropped these observations (use -svydes- to verify 
this).  If you want to drop the observations in R you need to do this 
explicitly. Having missing data may be unavoidable, but if you have 
observations in a sample it seems that you should know how they were 
sampled.
To drop these observations you could use

obsChina <- subset(China, !is.na(psu) & !is.na(strata) & !is.na(weight0x))

and then use obsChina rather than China in the svydesign() function.

 	-thomas



>                          -Bobby
>
> --- Thomas Lumley <tlumley at u.washington.edu> wrote:
>
>> On Sun, 4 Sep 2005, A Das wrote:
>>
>>> Thanks, Thomas.
>>>    Yes, that's exactly what happened: the warnings
>>> came first after "data(China)", and then after
>>> "dchina<-svydesign..." So the design object isn't
>>> being produced? The dataset is very large, and the
>>> weights were already set in Stata before
>> importing.
>>> Would either of those cause problems?
>>
>> Probably not.  What was the error message from
>> svydesign()?  That is what
>> will say what went wrong.
>>
>>  	-thomas
>>
>
>
>
>
> ____________________________________________________
> Start your day with Yahoo! - make it your home page
> http://www.yahoo.com/r/hs
>
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From dmbates at gmail.com  Sun Sep  4 20:34:53 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 4 Sep 2005 13:34:53 -0500
Subject: [R] Question regarding lmer with binary response
In-Reply-To: <431B43D6.26777.2CEBD8F@localhost>
References: <431B43D6.26777.2CEBD8F@localhost>
Message-ID: <40e66e0b050904113430a65a66@mail.gmail.com>

On 9/4/05, Bernd Weiss <bernd.weiss at uni-koeln.de> wrote:
> Dear all, dear Prof. Bates,
> 
> my dependent variable (school absenteeism, truancy[1]) is a binary
> response for which I am trying to compute an unconditional mixed
> effects model. I've got observations (monday, wednesday and friday)
> nested in individuals (ID2), which were nested in classes (KID2) and
> schools (SID), i.e. a 4-level mixed effects model.
> 
> In short, I was trying without success. I got no sensible results
> using lmer as well as using glmmPQL. I played around with the control
> parameters and the methods (PQl, Laplace) in lmer without any effect.
> 
> I would really appreciate if someone could have a look into my data
> and tell me what's going wrong here.
> 
> My R script and data can be found at:
> 
> http://www.metaanalyse.de/tmp/rhelp.R
> http://www.metaanalyse.de/tmp/rhelp.txt
> 
> TIA,
> 
> Bernd

Thanks for making the data and your script available.  That helps a
lot when investigating cases like these.

As you say, you have 3 binary responses per student and that is just
not enough information to fit a model like a generalized linear mixed
model.  Most of the students had 3 positive responses and 0 negative. 
In fact, out of the 6708 students, only 444 missed any days at all. 
Only 186 out of the 302 classes had any missing data.  It is just not
possible to fit a four level mixed effects model to such sparse data.

Consider only the pattern within students.  I did some very messy
manipulations to look at the unique patterns of absent:present
observations with the results shown below.  (Challenge to the reader:
Can you come up with relatively clean method of calculating  the
number of students with each of the patterns of absent:present shown
below?)

  A:P Freq  Pct
  0:1  413    0
  0:2  161    0
  0:3 5690    0
  1:2  258   33
  1:1   10   50
  2:1   65   67
  1:0   19  100
  2:0   10  100
  3:0   82  100

The important point to understand is that students who are present at
all observations or who are absent at all observations contribute very
little information to such a model.  The model fitting ends up giving
them a very large positive or negative random effect and they
contribute no other information.  The most information comes from the
students who are present some of the time and absent some of the time
and those are 333 students out of 6708.
 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kerbo2004 at yahoo.com  Sun Sep  4 20:45:46 2005
From: kerbo2004 at yahoo.com (A Das)
Date: Sun, 4 Sep 2005 11:45:46 -0700 (PDT)
Subject: [R] survey weights
In-Reply-To: <Pine.A41.4.63a.0509041045020.215666@homer09.u.washington.edu>
Message-ID: <20050904184546.59389.qmail@web50009.mail.yahoo.com>

That worked. Many thanks, Thomas. 
                              -Bobby

--- Thomas Lumley <tlumley at u.washington.edu> wrote:

> On Sun, 4 Sep 2005, A Das wrote:
> 
> > Just: "missing values in object". That would imply
> the
> > object was created. But then I write "dchina", and
> it
> > says "object dchina not found".
> 
> No, it would not imply the object was created.  If
> it was an error message 
> (rather than a warning) the object would not have
> been created.
> 
> I presume the full message was
>   Error in na.fail.default(object) : missing values
> in object
> 
> If so, it sounds as though you have missing values
> in the id, weights, or 
> strata variable.
>     summary(China[,c("psu","stata","weight0x"])
> will verify this.
> 
> Stata will just have dropped these observations (use
> -svydes- to verify 
> this).  If you want to drop the observations in R
> you need to do this 
> explicitly. Having missing data may be unavoidable,
> but if you have 
> observations in a sample it seems that you should
> know how they were 
> sampled.
> To drop these observations you could use
> 
> obsChina <- subset(China, !is.na(psu) &
> !is.na(strata) & !is.na(weight0x))
> 
> and then use obsChina rather than China in the
> svydesign() function.
> 
>  	-thomas
> 
> 
> 
> >                          -Bobby
> >
> > --- Thomas Lumley <tlumley at u.washington.edu>
> wrote:
> >
> >> On Sun, 4 Sep 2005, A Das wrote:
> >>
> >>> Thanks, Thomas.
> >>>    Yes, that's exactly what happened: the
> warnings
> >>> came first after "data(China)", and then after
> >>> "dchina<-svydesign..." So the design object
> isn't
> >>> being produced? The dataset is very large, and
> the
> >>> weights were already set in Stata before
> >> importing.
> >>> Would either of those cause problems?
> >>
> >> Probably not.  What was the error message from
> >> svydesign()?  That is what
> >> will say what went wrong.
> >>
> >>  	-thomas
> >>
> >
> >
> >
> >
> >
> ____________________________________________________
> > Start your day with Yahoo! - make it your home
> page
> > http://www.yahoo.com/r/hs
> >
> >
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington,
> Seattle
>



From r.shengzhe at gmail.com  Sun Sep  4 23:30:03 2005
From: r.shengzhe at gmail.com (Shengzhe Wu)
Date: Sun, 4 Sep 2005 23:30:03 +0200
Subject: [R] Help: PLSR
Message-ID: <ea57975b0509041430173473b6@mail.gmail.com>

Hello,

I have a data set with 15 variables (first one is the response) and
1200 observations. Now I use pls package to do the plsr as below.

trainSet = as.data.frame(scale(trainSet, center = T, scale = T))
trainSet.plsr = mvr(formula, ncomp = 14, data = trainSet, method = "kernelpls",
                            model = TRUE, x = TRUE, y = TRUE)

from the model, I wish to know the values of Xvar (the amount of
X-variance explained by each number of components) and Xtotvar (total
variance in X).

Because the trainSet has been scaled before training, I think Xtotvar
should be equal to 14, but unexpectedly Xtotvar = 16562, and the
values of Xvar are also very big and sum of Xvar = 16562. Why does
this type of result occur? for the reason of kernel algorithm?

Thank you,
Shengzhe



From jrhodes at stat.psych.uiuc.edu  Mon Sep  5 00:39:18 2005
From: jrhodes at stat.psych.uiuc.edu (Justin Rhodes)
Date: Sun, 04 Sep 2005 17:39:18 -0500
Subject: [R] R-square n p-value
In-Reply-To: <x2psrp1avp.fsf@turmalin.kubism.ku.dk>
References: <6.2.3.4.2.20050903173945.01cda5d8@cyrus.psych.uiuc.edu>
	<17178.12155.815817.768335@basebud.nulle.part>
	<x2psrp1avp.fsf@turmalin.kubism.ku.dk>
Message-ID: <6.2.3.4.2.20050904171953.01ccafb8@cyrus.psych.uiuc.edu>

Dear Peter,

This is exactly what I needed.  The "input" is 
coming from the Mouse Phenome Project database 
(http://aretha.jax.org/pub-cgi/phenome/mpdcgi?rtn=docs/home) 
which only gives pearson's correlations r, and 
n.  Thank you very much.  I have used this R-help 
resource twice now recently and it is incredibly 
helpful and fast.  Thanks again.  It is much appreciated.

cc Dirk

Best,

Justin




At 03:43 AM 9/4/2005, Peter Dalgaard wrote:
>Dirk Eddelbuettel <edd at debian.org> writes:
>
> > On 3 September 2005 at 17:59, Justin Rhodes wrote:
> > | Dear R-help,
> > |
> > | Can someone please help me discover what function or code will give
> > | me a p-value from the input: 1) R-square statistic from a simple
> > | linear regression, and 2) sample size, n
> > |
> > | This would be greatly appreciated.  I need this because I am using a
> > | database that gives me R-square and sample size for multiple
> > | comparisons, and I wish to determine the false discovery rate using
> > | q-value.  Thanks again,
> >
> > Do
> >       > example(lm)                   # just to get an lm object
> >       > str(summary(lm.D9))           # to examine summary of an object
> >
> > and you'll see that the object returned from summary has the two common R^2
> > measures, as well as things like residuals from which can compute n quite
> > easily -- which you could obviously also from 
> your regressors and regressand.
> >
> >       > length(summary(lm.D9)$residuals)
>
>I think the problem was somewhat different: The *input* is coming from
>some sort of (closed-source or otherwise impenetrable) database which
>only gives out n and R^2, right?
>
>Now R^2 = SSDmodel/(SSDmodel+SSDres) and F =
>DFres/DFmodel*SSDmodel/SSDres, i.e.
>
>   1/R^2 = 1 + 1/F*DFmodel/DFres
>
>or
>
>   F = 1/(1/R^2 - 1)*DFres/DFmodel = R^2/(1-R^2)*DFres/DFmodel
>
>which can be looked up "in the F-table" using
>
>   pf(F, 1, N-2, lower.tail=FALSE)
>
>(provided we have a 1 DF model)
>
>Actually, R^2 itself has a beta distribution and you could use pbeta
>directly, but then you'd need to figure out (or recall) what the
>relation between the DF and the shape parameters of the beta
>distribution are. By my reckoning, this should do it:
>
>   pbeta(Rsq, 1/2, (N-2)/2, lower.tail=FALSE)
>
>"Proof":
>
>....
>Residual standard error: 1.143 on 8 degrees of freedom
>Multiple R-Squared: 0.0004207,  Adjusted R-squared: -0.1245
>F-statistic: 0.003367 on 1 and 8 DF,  p-value: 0.9552
>
> > pbeta(0.0004207, 1/2, 8/2, lower=F)
>[1] 0.9551511
>
>
>--
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

Justin S. Rhodes
Assistant Professor of Psychology
Affiliate, Institute for Genomic Biology and Neuroscience Program
University of Illinois at Urbana-Champaign
405 N Mathews Ave, Urbana, Il, 61801
Tel. 217-265-0021 Fax 217-244-5876
Website: http://s.psych.uiuc.edu/people/showprofile.php?id=545



From spencer.graves at pdf.com  Mon Sep  5 01:31:42 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Sep 2005 16:31:42 -0700
Subject: [R] Doubt about nested aov output
In-Reply-To: <200508301101.32212.chrysopa@gmail.com>
References: <200508301101.32212.chrysopa@gmail.com>
Message-ID: <431B83DE.5060605@pdf.com>

	  Others may know the answer to your question, but I don't.  However, 
since I have not seen a reply, I will offer a few comments:

	  1.  What version of R are you using?  I just tried superficially 
similar things with the examples in ?aov in R 2.1.1 patched and 
consistently got F and p values.

	  2.  My preference for this kind of thing is to use lme in 
library(nlme) or lmer in library(lme4).  Also, I highly recommend 
Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).

	  3.  If still want to use aov and are getting this problem in R 2.1.1, 
could you please provide this list with a small, self contained example 
that displays the symptoms that concern you?  And PLEASE do read the 
posting guide! "http://www.R-project.org/posting-guide.html".  It might 
increase the speed and utility of replies.

	  spencer graves

Ronaldo Reis-Jr. wrote:

> Hi,
> 
> I have two doubts about the nested aov output.
> 
> 1) I have this:
> 
>>anova.ratos <- aov(Glicogenio~Tratamento+Error(Tratamento/Rato/Figado))
>>summary(anova.ratos)
> 
> 
> Error: Tratamento
>            Df  Sum Sq Mean Sq
> Tratamento  2 1557.56  778.78
> 
> Error: Tratamento:Rato
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  3 797.67  265.89               
> 
> Error: Tratamento:Rato:Figado
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 12  594.0    49.5               
> 
> Error: Within
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 18 381.00   21.17               
> 
> R dont make the F and P automatically, it is possible?
> 
> I Like an output like this:
> 
> Error: Tratamento
>            Df  Sum Sq Mean Sq F value Pr(>F)
> Tratamento  2 1557.56  778.78   2.929 0.197
> 
> Error: Tratamento:Rato
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  3 797.67  265.89   5.372 0.0141      
> 
> Error: Tratamento:Rato:Figado
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 12  594.0    49.5   2.339 0.0503        
> 
> Error: Within
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 18 381.00   21.17         
> 
> Why it not make automatic calculations? It is possible?
> 
> 
> 2) I can say that Error: Tratamento:Rato means an interaction between 
> Tratamento and Rato? Normally the : represents an interaction, but in this 
> output I think that it dont mean the interaction. 
> 
> Any explanation are welcome.
> 
> Thanks
> Ronaldo

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Mon Sep  5 01:48:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Sep 2005 16:48:08 -0700
Subject: [R] Distributional characteristics
In-Reply-To: <BF3B33BF.5FAE%nassar@noos.fr>
References: <BF3B33BF.5FAE%nassar@noos.fr>
Message-ID: <431B87B8.4090003@pdf.com>

	  I haven't seen a reply to your post, and I would like to help you. 
Unfortunately, I don't see a question in your email.  Please tell us why 
"changing the variable (x-1,ln(x)) didn't get satisfying results", 
preferably using a toy example that someone else can copy from your 
email in to R and test a couple if ideas in a matter of seconds.  I 
believe doing that will increase the chances that you will receive a 
quick and useful reply.  (And PLEASE do read the posting guide! 
"http://www.R-project.org/posting-guide.html".  Many people have found 
it quite helpful.)

	  spencer graves

Naji wrote:

> Hi all
> 
> I've a continuous variable and I want to test (graphically, plotting
> observed and theoretical distr, qqplot) whether it follows some formal
> distribution. (some thing close to Ricci document : Fitting distributions
> with R, Feb05).
>  
> The distribution I want to fit is a truncated Gamma at 1 (the minimal value
> is 1), P(x)=Pgamma(rate,x)/(1-Pgamma(rate,x<1))
> 
> NB : changing the variable (x-1,ln(x)) didn't get satisfying results
> 
> Best
> Naji
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Mon Sep  5 01:53:13 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Sep 2005 16:53:13 -0700
Subject: [R] R package for ICA
In-Reply-To: <09D04633DAED5F489FFEB6A0EAF22C16022BC8B2@pikachu.health.pitt.edu>
References: <09D04633DAED5F489FFEB6A0EAF22C16022BC8B2@pikachu.health.pitt.edu>
Message-ID: <431B88E9.5080404@pdf.com>

	  I just got 48 hits from RSiteSearch("independent component 
analysis"), the first of which was 
"http://finzi.psych.upenn.edu/R/library/mlica/html/mlica.html".

	  hope this helps.
	  spencer graves
p.s.  I believe people who follow the posting guide typically get more 
useful information quicker.  "http://www.R-project.org/posting-guide.html"

Li, Ran wrote:

> Hi,
> 
>  
> 
> Which R packages are good to be used for independent component analysis?  
> 
>  
> 
> Thanks for any suggestions.
> 
>  
> 
> Ran 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Mon Sep  5 02:01:14 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Sep 2005 17:01:14 -0700
Subject: [R] Question
In-Reply-To: <Pine.GSO.4.05.10508312134210.21903-100000@gauss>
References: <Pine.GSO.4.05.10508312134210.21903-100000@gauss>
Message-ID: <431B8ACA.9030502@pdf.com>

	  1.  I could find no references to "var.ran" with 
RSiteSearch("var.ran") and when requesting "var.ran" from S-Plus 6.2.

	  2.  Have you considered "simulate.lme"?

	  3.  Are you familiar with Pinheiro and Bates (2000) Mixed-Effects 
Models in S and S-Plus (Springer)?  I highly recommend this book.

	  spencer graves

Mahmoud Torabi wrote:

> Dear Sir/Madam
> 
> I would be pleased if anybody can help me. I'm using linear mixed model
> (lme) function.I'm doing some simulation in my research and need to be
> assigned variance components values during of my program. Specifically,
> when I use lme function, I can get some information by use summary() and I
> can assign some valuse like variance of fixed parameters and variance of
> random error
> term by using for example  varFix and sigma.But I don't know how I can
> assign for variance of random effect.
> I know in SPLUS we have command var.ran, how about R ?
> 
> Thanks alot.
> M.Torabi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From David.Duffy at qimr.edu.au  Mon Sep  5 03:29:22 2005
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Mon, 5 Sep 2005 11:29:22 +1000 (EST)
Subject: [R] how to fit the partial association model with R?
In-Reply-To: <mailman.13.1125741601.29571.r-help@stat.math.ethz.ch>
References: <mailman.13.1125741601.29571.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0509051124280.26632@orpheus.qimr.edu.au>

On Fri, 2 Sep 2005 0034058 at fudan.edu.cn wrote:

>
> If I do not make a mistake,the partial association model is an
> extension of log-linear model.I read a papers which gives an example
> of it.(Sloane and Morgan,1996,An Introduction to Categorical Data
> Analysis,Annual Review of Sociology.22:351-375) Can R fit such partial
> association model?
>
> ps:Another somewhat off-topic question.What's the motivations to use
> log-linear model?Or why use log-linear model?I learn the log-linear
> model butI still do not master the the advantage of the model.thank
> you!

You might like to read the manual at http://math.cl.uh.edu/thompsonla/
under  "An S Manual to Accompany Agresti's Categorical Data Analysis 2nd ed.
(2002)" along with Agresti's book itself.

| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From adrian at maths.uwa.edu.au  Mon Sep  5 04:10:31 2005
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Mon, 5 Sep 2005 10:10:31 +0800
Subject: [R] SpatStat Kest -  Error Message help
Message-ID: <17179.43287.748245.156800@maths.uwa.edu.au>

On Thu, 1 Sep 2005, DrakeGis wrote:

> Hi I'm working with the function Kest in the package SpatStat (under LINUX
> with R 2.1.0). In order to evaluate the statistical significance of my
> point pattern I'm doing 999 Montecarlo replications. The script that use
> the Kest function runs OK for most of the different point patterns that I
> have but for a particular point pattern, which have only 17 points, it
> runs until the 34th iteration and then I receive this message:
>
> Error in "[<-"(`*tmp*`, index, value = NULL) :
> 	incompatible types (1000) in subassignment type fix
> Execution halted
>
> Do you have any idea about what could be the cause of this ? Thanks in
> advance.

  This is not an error message from 'spatstat' itself.

  The message has been generated by the function "[<-" 
  which is called when you assign values to a subset of a dataset 
  (in a command like x[z] <- v). The message appears to say that the
  replacement value v is not of the same type as the original vector x. 
  
  You say that you are running a "script that uses the Kest function".
  The error is probably inside that script. If you send the script to us
  we can probably spot the problem for you.

  As Rolf mentioned in his email, spatstat provides a command "envelope"
  to compute simulation envelopes. This might be sufficient for your needs.

regards
Adrian Baddeley



From bhx2 at mevik.net  Mon Sep  5 10:31:27 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Mon, 05 Sep 2005 10:31:27 +0200
Subject: [R] Help: PLSR
In-Reply-To: <ea57975b0509041430173473b6@mail.gmail.com> (Shengzhe Wu's
	message of "Sun, 4 Sep 2005 23:30:03 +0200")
References: <ea57975b0509041430173473b6@mail.gmail.com>
Message-ID: <m03bojc3vk.fsf@bar.nemo-project.org>

Shengzhe Wu writes:

> I have a data set with 15 variables (first one is the response) and
> 1200 observations. Now I use pls package to do the plsr as below.

[...]

> Because the trainSet has been scaled before training, I think Xtotvar
> should be equal to 14, but unexpectedly Xtotvar = 16562,

Because the Xtotvar is the "total X variation", measured by sum(X^2)
(where X has been centered).  With 14 variables, scaled to sd == 1,
and 1200 observations, you should get Xtotvar == 14*(1200-1) ==
16786.  (Maybe you have 1184 observations: 14*1183 == 16562.)

-- 
Bj??rn-Helge Mevik



From buser at stat.math.ethz.ch  Mon Sep  5 11:38:51 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 5 Sep 2005 11:38:51 +0200
Subject: [R] Doubt about nested aov output
In-Reply-To: <431B83DE.5060605@pdf.com>
References: <200508301101.32212.chrysopa@gmail.com> <431B83DE.5060605@pdf.com>
Message-ID: <17180.4651.324909.367229@stat.math.ethz.ch>

Hi

I think the problem is related to specifying "Tratamento" both
as a fixed factor and in the error term. I attached a script
with a reproducible example that shows a similar output.
I do not know the details of the original data and the questions
of interest, but maybe a model including "Tratamento" is more
what you wanted to implement.

Regards,

Christoph

## R-Script

library(nlme)

## Generating the data
set.seed(1)
ziel <- rep(c(-6,8,20), each = 40) + rep(rnorm(15, 0, 20), each = 4) +
  rep(rnorm(60, 0, 10), each = 2) + rnorm(120, 0, 3)
dat <- data.frame(y = ziel,
                  fix = factor(rep(1:3, each = 40)),
                  R1 = factor(rep(1:15, each = 8)),
                  R2 = factor(rep(1:60, each = 2)))

## Model including "fix" as fixed and random effect.
res2 <- aov(y ~ fix + Error(fix/R1/R2), data = dat)
summary(res2)

reslme2 <- lme(y ~ fix , data = dat, random = ~ 1|fix/R1/R2)
summary(reslme2)
anova(reslme2)

## Model including "fix" as fixed factor.
res1 <- aov(y ~ fix + Error(R1/R2), data = dat)
summary(res1)

reslme <- lme(y ~ fix , data = dat, random = ~ 1|R1/R2)
summary(reslme)
anova(reslme)

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



Spencer Graves writes:
 > 	  Others may know the answer to your question, but I don't.  However, 
 > since I have not seen a reply, I will offer a few comments:
 > 
 > 	  1.  What version of R are you using?  I just tried superficially 
 > similar things with the examples in ?aov in R 2.1.1 patched and 
 > consistently got F and p values.
 > 
 > 	  2.  My preference for this kind of thing is to use lme in 
 > library(nlme) or lmer in library(lme4).  Also, I highly recommend 
 > Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).
 > 
 > 	  3.  If still want to use aov and are getting this problem in R 2.1.1, 
 > could you please provide this list with a small, self contained example 
 > that displays the symptoms that concern you?  And PLEASE do read the 
 > posting guide! "http://www.R-project.org/posting-guide.html".  It might 
 > increase the speed and utility of replies.
 > 
 > 	  spencer graves
 > 
 > Ronaldo Reis-Jr. wrote:
 > 
 > > Hi,
 > > 
 > > I have two doubts about the nested aov output.
 > > 
 > > 1) I have this:
 > > 
 > >>anova.ratos <- aov(Glicogenio~Tratamento+Error(Tratamento/Rato/Figado))
 > >>summary(anova.ratos)
 > > 
 > > 
 > > Error: Tratamento
 > >            Df  Sum Sq Mean Sq
 > > Tratamento  2 1557.56  778.78
 > > 
 > > Error: Tratamento:Rato
 > >           Df Sum Sq Mean Sq F value Pr(>F)
 > > Residuals  3 797.67  265.89               
 > > 
 > > Error: Tratamento:Rato:Figado
 > >           Df Sum Sq Mean Sq F value Pr(>F)
 > > Residuals 12  594.0    49.5               
 > > 
 > > Error: Within
 > >           Df Sum Sq Mean Sq F value Pr(>F)
 > > Residuals 18 381.00   21.17               
 > > 
 > > R dont make the F and P automatically, it is possible?
 > > 
 > > I Like an output like this:
 > > 
 > > Error: Tratamento
 > >            Df  Sum Sq Mean Sq F value Pr(>F)
 > > Tratamento  2 1557.56  778.78   2.929 0.197
 > > 
 > > Error: Tratamento:Rato
 > >           Df Sum Sq Mean Sq F value Pr(>F)
 > > Residuals  3 797.67  265.89   5.372 0.0141      
 > > 
 > > Error: Tratamento:Rato:Figado
 > >           Df Sum Sq Mean Sq F value Pr(>F)
 > > Residuals 12  594.0    49.5   2.339 0.0503        
 > > 
 > > Error: Within
 > >           Df Sum Sq Mean Sq F value Pr(>F)
 > > Residuals 18 381.00   21.17         
 > > 
 > > Why it not make automatic calculations? It is possible?
 > > 
 > > 
 > > 2) I can say that Error: Tratamento:Rato means an interaction between 
 > > Tratamento and Rato? Normally the : represents an interaction, but in this 
 > > output I think that it dont mean the interaction. 
 > > 
 > > Any explanation are welcome.
 > > 
 > > Thanks
 > > Ronaldo
 > 
 > -- 
 > Spencer Graves, PhD
 > Senior Development Engineer
 > PDF Solutions, Inc.
 > 333 West San Carlos Street Suite 700
 > San Jose, CA 95110, USA
 > 
 > spencer.graves at pdf.com
 > www.pdf.com <http://www.pdf.com>
 > Tel:  408-938-4420
 > Fax: 408-280-7915
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
 > 
 > 
 > !DSPAM:431b8510220677348368323!



From Allan at STATS.uct.ac.za  Mon Sep  5 11:59:02 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Mon, 05 Sep 2005 11:59:02 +0200
Subject: [R] numerical intergation
Message-ID: <431C16E6.E0CC6C43@STATS.uct.ac.za>



how does one numerically intergate the following:

A=function(x,y)
{
	xy
}

over the range: 2<x<0	4<y<10

say.


ie how would one set up the integrate function?

i forgot!

From r.hankin at noc.soton.ac.uk  Mon Sep  5 12:13:28 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 5 Sep 2005 11:13:28 +0100
Subject: [R] numerical intergation
In-Reply-To: <431C16E6.E0CC6C43@STATS.uct.ac.za>
References: <431C16E6.E0CC6C43@STATS.uct.ac.za>
Message-ID: <9A9195F1-5FC5-4012-8A35-BA1D0307646D@soc.soton.ac.uk>

Hi Allan

you need adapt() from the adapt package.

 > library(adapt)
 > ?adapt
 > adapt(2,lower=c(-2,4),upper=c(0,10),functn=function(z){prod(z)})
       value      relerr      minpts      lenwrk       ifail
         -84 7.38275e-08         165          73           0
 >


NB: untested


HTH

rksh



On 5 Sep 2005, at 10:59, Clark Allan wrote:

>
>
> how does one numerically intergate the following:
>
> A=function(x,y)
> {
>     xy
> }
>
> over the range: 2<x<0    4<y<10
>
> say.
>
>
> ie how would one set up the integrate function?
>
> i forgot!______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From Allan at STATS.uct.ac.za  Mon Sep  5 13:04:08 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Mon, 05 Sep 2005 13:04:08 +0200
Subject: [R] numerical intergation
References: <431C16E6.E0CC6C43@STATS.uct.ac.za>
	<431C1B0D.6DC68729@STATS.uct.ac.za>
Message-ID: <431C2628.C2711D54@STATS.uct.ac.za>

found a solution: download the rmutil package from :
http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html

for those that are interested.

note that only works for two dimensions!

/
allan

(ranges was incorrect previously: should be:-2<x<0   4<y<10)


Clark Allan wrote:
> 
> i solved the problem:
> 
> adapt(2, lo=c(2,4), up=c(0,10))
> 
> is there any package that allows one to have infinite limits?? when
> integrating over more than one variable?
> 
> Clark Allan wrote:
> >
> > how does one numerically intergate the following:
> >
> > A=function(x,y)
> > {
> >         xy
> > }
> >
> > over the range: 2<x<0   4<y<10
> >
> > say.
> >
> > ie how would one set up the integrate function?
> >
> > i forgot!
> >
> >   ------------------------------------------------------------------------
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

From ramasamy at cancer.org.uk  Mon Sep  5 14:05:09 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 05 Sep 2005 13:05:09 +0100
Subject: [R] glm p-values on features
In-Reply-To: <1125678969.13108.1.camel@inpc93.et.tudelft.nl>
References: <mailman.11.1125655201.27534.r-help@stat.math.ethz.ch>
	<1125678969.13108.1.camel@inpc93.et.tudelft.nl>
Message-ID: <1125921909.6476.22.camel@localhost.localdomain>

mydf <- data.frame(x1=rnorm(100), x2=rnorm(100), x3=rnorm(100))
mydf$y <- 0.5 * mydf$x1 + 3 * mydf$x3 + rnorm(100)
 
fit <- glm( y ~ . , data=mydf )
coefficients( summary(fit) )
                Estimate Std. Error    t value     Pr(>|t|)
  (Intercept) -0.2385855  0.2541498 -0.9387593 3.502103e-01
x1           0.6956811  0.1330900  5.2271462 1.003295e-06
x2           0.1313823  0.1417679  0.9267420 3.563846e-01
x3           2.7986410  0.4531338  6.1761919 1.576173e-08


On Fri, 2005-09-02 at 18:36 +0200, Joost van Evert wrote:
> Dear list,
> 
> does anyone know how to get p-values on the coefficients returned by
> glm?
> 
> thanks+greets,
> Joost
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From r.shengzhe at gmail.com  Mon Sep  5 15:00:20 2005
From: r.shengzhe at gmail.com (Shengzhe Wu)
Date: Mon, 5 Sep 2005 15:00:20 +0200
Subject: [R] Help: Plsr
Message-ID: <ea57975b05090506005efcdd11@mail.gmail.com>

Dear Bj??rn-Helge,

Sorry, I wrote the wrong number of observation. It should be 1184.

I saw on the book that variance is defined by sd^2. If variation is a
different concept from variance and defined by sd^2*(n-1) ? Since I
formerly took variance and variation as the same.

Thank you,
Shengzhe




Shengzhe Wu writes:

> I have a data set with 15 variables (first one is the response) and
> 1200 observations. Now I use pls package to do the plsr as below.

[...]

> Because the trainSet has been scaled before training, I think Xtotvar
> should be equal to 14, but unexpectedly Xtotvar = 16562,

Because the Xtotvar is the "total X variation", measured by sum(X^2)
(where X has been centered).  With 14 variables, scaled to sd == 1,
and 1200 observations, you should get Xtotvar == 14*(1200-1) ==
16786.  (Maybe you have 1184 observations: 14*1183 == 16562.)

--
Bj?rn-Helge Mevik



From azzalini at stat.unipd.it  Mon Sep  5 14:56:10 2005
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Mon, 5 Sep 2005 14:56:10 +0200
Subject: [R] Multivariate Skew Normal distribution
In-Reply-To: <20050901130901.61123.qmail@web34011.mail.mud.yahoo.com>
References: <20050901130901.61123.qmail@web34011.mail.mud.yahoo.com>
Message-ID: <20050905145610.5f343c63.azzalini@stat.unipd.it>

On Thu, 1 Sep 2005 13:09:00 +0000 (GMT), Caio Lucidius Naberezny
Azevedo wrote:

CLNA> 
CLNA>  Could anyone tell me if there is any package (or function) that
CLNA>  generates values from a multivariate skew normal distribution?

try the following

library(sn)
location <- c(20,4) # e.g. for a two-dimensional variable
dispers  <- matrix(c(3,-2,-2,2), 2,2)
skew <- c(10,-6)
rmsn(n=10, xi=location, Omega=dispers, alpha=skew) # for skew-normal data
rmst(n=10, xi=location, Omega=dispers, alpha=skew, df=5) # for skew-t data

see also help(rsn) and help(rst) for the univariate case

for more information, see also (as already pointed out in the list):
  http://azzalini.stat.unipd.it/SN

best wishes,
Adelchi Azzalini

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit?? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/



From t.muhlhofer at lse.ac.uk  Mon Sep  5 15:03:33 2005
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Mon, 05 Sep 2005 14:03:33 +0100
Subject: [R] Dummy variables model
Message-ID: <431C4225.2070602@lse.ac.uk>

Hi, all!

Anyone know an easy way to specify the following model.

Panel dataset, with stock through time, by firm.

I want to run a model of y on a bunch of explanatory variables, and one 
dummy for each firm, which is 1 for observations that come from firm i, 
and 0 everywhere else. I have over 200 firms (and a factor variable that 
  contains a firm identifier).

Any easy way of going about this, without having to define all these 
dummies? I checked lme() with random = ~ 1|firm, but the problem is that 
these are random effects, i.e. that there are firm-by-firm disturbance 
terms and overall disturbance terms, whereas I want just overall 
disturbance terms. This is generally called a "fixed effects" model, 
although it seems like the term "fixed effects" is being used somewhat 
differently in the context of the nlme package.

Toby

-- 
**************************************************************************
When Thomas Edison invented the light bulb he tried over 2000
experiments before he got it to work. A young reporter asked
him how it felt to have failed so many times. He said
"I never failed once. I invented the light bulb.
It just happened to be a 2000-step process."



From thpe at hhbio.wasser.tu-dresden.de  Mon Sep  5 15:01:13 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 05 Sep 2005 15:01:13 +0200
Subject: [R] model comparison and Wald-tests (e.g. in lmer)
Message-ID: <431C4199.6070206@hhbio.wasser.tu-dresden.de>

Dear expeRts,

there is obviously a general trend to use model comparisons, LRT and AIC
instead of Wald-test-based significance, at least in the R community.
I personally like this approach. And, when using LME's, it seems to be 
the preferred way (concluded from postings of Brian Ripley and Douglas 
Bates' article in R-News 5(2005)1), esp. because of problems with the 
d.f. approximation.

But, on the other hand I found that not all colleagues are happy with the
resulting AIC/LRT tables and the comparison of multiple models.

As a compromise, and after a suggestion in Crawley's "Statistical
computing" one may consider to supply "traditional" ANOVA tables as an
additional explanation for the reader (e.g. field biologists).

An example:

one has fitted 5 models m1..m5 and after:

 >anova(m1,m2,m3,m4,m5) # giving AIC and LRT-tests

he selects m3 as the most parsimonious model and calls anova with the 
best model (Wald-test):

 >anova(m3)             # the additional explanatory table

My questions:

* Do people outside the S-PLUS/R world still understand us?

* Is it wise to add such an explanatory table (in particular when the 
results are the same) to make the results more transparent to the reader?

* Are such additional ANOVA tables *really helpful* or are they (in 
combination with a model comparison) just another source of confusion?


Thank you!

Thomas P.



From gb at stat.umu.se  Mon Sep  5 15:59:41 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Mon, 5 Sep 2005 15:59:41 +0200
Subject: [R] TeX distribution on Windows
Message-ID: <20050905135941.GB18905@stat.umu.se>

I'm looking for a Windows distribution of TeX that works with  R, after a 
few years' absence from Windows. On Duncan Murdoch's Rtools page fptex is 
still recommended, but it turns out that fptex is "defunct" as of May 2005,
see 

http://www.metz.supelec.fr/~popineau/xemtex-7.html

So, what is suggested? TUG (tug.org) recommends something called proTeXt,
which is said to be "based on MiKTeX", for Windows users. Since MikTeX 
could be used with  R, that sounds like a good alternative.

Any comments to that?

-- 
G??ran Brostr??m                    tel: +46 90 786 5223
Department of Statistics          fax: +46 90 786 6614
Ume?? University                   http://www.stat.umu.se/~goran.brostrom/
SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From dkat at stats.uct.ac.za  Mon Sep  5 14:06:24 2005
From: dkat at stats.uct.ac.za (Dominique Katshunga)
Date: Mon, 05 Sep 2005 14:06:24 +0200
Subject: [R] help
Message-ID: <431C34C0.EADF5711@stats.uct.ac.za>

Dear helpeRs,
I seem to be a little bit confused on the result I am getting from the
few codes below:
> u=v=seq(0,1,length=30)
> u
 [1] 0.00000000 0.03448276 0.06896552 0.10344828 0.13793103 0.17241379
 [7] 0.20689655 0.24137931 0.27586207 0.31034483 0.34482759 0.37931034
[13] 0.41379310 0.44827586 0.48275862 0.51724138 0.55172414 0.58620690
[19] 0.62068966 0.65517241 0.68965517 0.72413793 0.75862069 0.79310345
[25] 0.82758621 0.86206897 0.89655172 0.93103448 0.96551724 1.00000000
> v
 [1] 0.00000000 0.03448276 0.06896552 0.10344828 0.13793103 0.17241379
 [7] 0.20689655 0.24137931 0.27586207 0.31034483 0.34482759 0.37931034
[13] 0.41379310 0.44827586 0.48275862 0.51724138 0.55172414 0.58620690
[19] 0.62068966 0.65517241 0.68965517 0.72413793 0.75862069 0.79310345
[25] 0.82758621 0.86206897 0.89655172 0.93103448 0.96551724 1.00000000
> f=function(x,y){cp=max(x+y-1,0)}
> z=outer(u,v,f)
z is a 30x30 matrix which is fine, but why all its entries are equal to
1? for example, the maximum between u[22]+v[22]-1 and 0 is not 1?? I
don't really know where I went wrong! 
thanks,
Dominique



From antonio.fabio at gmail.com  Mon Sep  5 16:04:23 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Mon, 5 Sep 2005 16:04:23 +0200
Subject: [R] TeX distribution on Windows
In-Reply-To: <20050905135941.GB18905@stat.umu.se>
References: <20050905135941.GB18905@stat.umu.se>
Message-ID: <b0808fdc05090507047f2a6994@mail.gmail.com>

With my windows installation, MikTeX works fine, without any problem
in compiling packages documentation.

Antonio, Fabio Di Narzo.

2005/9/5, G??ran Brostr??m <gb at stat.umu.se>:
> I'm looking for a Windows distribution of TeX that works with  R, after a
> few years' absence from Windows. On Duncan Murdoch's Rtools page fptex is
> still recommended, but it turns out that fptex is "defunct" as of May 2005,
> see
> 
> http://www.metz.supelec.fr/~popineau/xemtex-7.html
> 
> So, what is suggested? TUG (tug.org) recommends something called proTeXt,
> which is said to be "based on MiKTeX", for Windows users. Since MikTeX
> could be used with  R, that sounds like a good alternative.
> 
> Any comments to that?
> 
> --
> G??ran Brostr??m                    tel: +46 90 786 5223
> Department of Statistics          fax: +46 90 786 6614
> Ume?? University                   http://www.stat.umu.se/~goran.brostrom/
> SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Kevin.Wang at maths.anu.edu.au  Mon Sep  5 16:07:21 2005
From: Kevin.Wang at maths.anu.edu.au (Ko-Kang Kevin Wang)
Date: Tue, 06 Sep 2005 00:07:21 +1000
Subject: [R] TeX distribution on Windows
In-Reply-To: <20050905135941.GB18905@stat.umu.se>
References: <20050905135941.GB18905@stat.umu.se>
Message-ID: <431C5119.4070102@maths.anu.edu.au>

Hi,

G??ran Brostr??m wrote:

> So, what is suggested? TUG (tug.org) recommends something called proTeXt,
> which is said to be "based on MiKTeX", for Windows users. Since MikTeX 
> could be used with  R, that sounds like a good alternative.
> 
> Any comments to that?

I've been using MikTeX on Windows for years and have never had any 
problems.  Its "Update Wizard" also has a nice and intuitive user 
interface.  I've never had any problems using it with R.

Cheers,

Kev


-- 
Ko-Kang Kevin Wang
PhD Student
Centre for Bioinformation Science
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 2601
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7488
Ph (M): +61-40-451-8301



From dimitris.rizopoulos at med.kuleuven.be  Mon Sep  5 14:03:15 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 5 Sep 2005 14:03:15 +0200
Subject: [R] [R-pkgs] New package for grouped data models
Message-ID: <009e01c5b211$cc248220$0540210a@www.domain>

Dear R-users,

We'd like to announce the release of our new package "grouped" 
(available from CRAN), for fitting models for grouped or coarse data, 
under the Coarsened At Random assumption. This is useful in cases 
where the true response variable is known only up to an interval in 
which it lies. Features of the package include: power calculations for 
two-group comparisons, computation of Bayesian residuals using 
Multiple Imputation, capability of fitting models for index-kind data 
(e.g., quality of life indexes). Any kind of feedback (questions, 
suggestions, bug-reports, etc.) is more than welcome.


Best,
Dimitris Rizopoulos
Roula Tsonaka


----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From thpe at hhbio.wasser.tu-dresden.de  Mon Sep  5 16:16:15 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 05 Sep 2005 16:16:15 +0200
Subject: [R] TeX distribution on Windows
In-Reply-To: <20050905135941.GB18905@stat.umu.se>
References: <20050905135941.GB18905@stat.umu.se>
Message-ID: <431C532F.2080102@hhbio.wasser.tu-dresden.de>

  gb at stat.umu.se wrote:
> I'm looking for a Windows distribution of TeX that works with R,
> after a few years' absence from Windows. On Duncan Murdoch's Rtools
> page fptex is still recommended, but it turns out that fptex is
> "defunct" as of May  2005,
> see
> 
> http://www.metz.supelec.fr/~popineau/xemtex-7.html
> 
> So, what is suggested? TUG (tug.org) recommends something called
> proTeXt, which is said to be "based on MiKTeX", for Windows users.
> Since MikTeX could be used with R, that sounds like a good
> alternative.
> 
> Any comments to that?

MikTeX works very well for us:

- when writing reports using R figures,
- with SWeave and
- in the R package building process.

You can get the web installer from:

http://sourceforge.net/projects/miktex

Miktex can be used with WinEDT, Emacs, Texniccenter and others as editor.

HTH

Thomas Petzoldt



From liu2074 at yahoo.com  Mon Sep  5 16:42:39 2005
From: liu2074 at yahoo.com (liu abc)
Date: Mon, 5 Sep 2005 07:42:39 -0700 (PDT)
Subject: [R] convergence for proportional odds model
Message-ID: <20050905144239.39764.qmail@web51712.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050905/1895d2b6/attachment.pl

From Kevin.Wang at maths.anu.edu.au  Mon Sep  5 16:45:26 2005
From: Kevin.Wang at maths.anu.edu.au (Ko-Kang Kevin Wang)
Date: Tue, 06 Sep 2005 00:45:26 +1000
Subject: [R] TeX distribution on Windows
In-Reply-To: <431C532F.2080102@hhbio.wasser.tu-dresden.de>
References: <20050905135941.GB18905@stat.umu.se>
	<431C532F.2080102@hhbio.wasser.tu-dresden.de>
Message-ID: <431C5A06.8000506@maths.anu.edu.au>

Thomas Petzoldt wrote:


> 
> Miktex can be used with WinEDT, Emacs, Texniccenter and others as editor.

Slightly off topic, if you want to get MikTeX working with Emacs and 
ESS, the  Claus Dethlefsen has a wonderful web site (in fact, best 
website on this topic IMHO) http://www.math.auc.dk/~dethlef/Tips/

Cheers,

Kev

-- 
Ko-Kang Kevin Wang
PhD Student
Centre for Bioinformation Science
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 2601
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7488
Ph (M): +61-40-451-8301



From buser at stat.math.ethz.ch  Mon Sep  5 16:47:16 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 5 Sep 2005 16:47:16 +0200
Subject: [R] Dummy variables model
In-Reply-To: <431C4225.2070602@lse.ac.uk>
References: <431C4225.2070602@lse.ac.uk>
Message-ID: <17180.23156.728927.540188@stat.math.ethz.ch>

Hi

If you'd like to fit a fixed effect model without random
effects, you can use lm() or aov() (see ?lm and ?aov). If your
variable is a factor (?factor) then you can specify your model
in lm() without coding all dummy variables.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Tobias Muhlhofer writes:
 > Hi, all!
 > 
 > Anyone know an easy way to specify the following model.
 > 
 > Panel dataset, with stock through time, by firm.
 > 
 > I want to run a model of y on a bunch of explanatory variables, and one 
 > dummy for each firm, which is 1 for observations that come from firm i, 
 > and 0 everywhere else. I have over 200 firms (and a factor variable that 
 >   contains a firm identifier).
 > 
 > Any easy way of going about this, without having to define all these 
 > dummies? I checked lme() with random = ~ 1|firm, but the problem is that 
 > these are random effects, i.e. that there are firm-by-firm disturbance 
 > terms and overall disturbance terms, whereas I want just overall 
 > disturbance terms. This is generally called a "fixed effects" model, 
 > although it seems like the term "fixed effects" is being used somewhat 
 > differently in the context of the nlme package.
 > 
 > Toby
 > 
 > -- 
 > **************************************************************************
 > When Thomas Edison invented the light bulb he tried over 2000
 > experiments before he got it to work. A young reporter asked
 > him how it felt to have failed so many times. He said
 > "I never failed once. I invented the light bulb.
 > It just happened to be a 2000-step process."
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
 > 
 > 
 > !DSPAM:431c4675196241771238468!



From jeaneid at chass.utoronto.ca  Mon Sep  5 16:48:43 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Mon, 5 Sep 2005 10:48:43 -0400
Subject: [R] Dummy variables model
In-Reply-To: <431C4225.2070602@lse.ac.uk>
Message-ID: <Pine.SGI.4.40.0509051046160.468230-100000@origin.chass.utoronto.ca>

You can turn the identity vector of the firms into a factor and do lm ....

Jean

On Mon, 5 Sep 2005, Tobias Muhlhofer wrote:

> Hi, all!
>
> Anyone know an easy way to specify the following model.
>
> Panel dataset, with stock through time, by firm.
>
> I want to run a model of y on a bunch of explanatory variables, and one
> dummy for each firm, which is 1 for observations that come from firm i,
> and 0 everywhere else. I have over 200 firms (and a factor variable that
>   contains a firm identifier).
>
> Any easy way of going about this, without having to define all these
> dummies? I checked lme() with random = ~ 1|firm, but the problem is that
> these are random effects, i.e. that there are firm-by-firm disturbance
> terms and overall disturbance terms, whereas I want just overall
> disturbance terms. This is generally called a "fixed effects" model,
> although it seems like the term "fixed effects" is being used somewhat
> differently in the context of the nlme package.
>
> Toby
>
> --
> **************************************************************************
> When Thomas Edison invented the light bulb he tried over 2000
> experiments before he got it to work. A young reporter asked
> him how it felt to have failed so many times. He said
> "I never failed once. I invented the light bulb.
> It just happened to be a 2000-step process."
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From t.muhlhofer at lse.ac.uk  Mon Sep  5 16:53:55 2005
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Mon, 05 Sep 2005 15:53:55 +0100
Subject: [R] Dummy variables model
In-Reply-To: <Pine.SGI.4.40.0509051046160.468230-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0509051046160.468230-100000@origin.chass.utoronto.ca>
Message-ID: <431C5C03.9030704@lse.ac.uk>

So are you guys saying to me that if I have variable firm which is the 
factor of all firm identifiers, I could just go

lm(y ~ x + firm)

and that will implicitly include a dummy for each level of factor firm, 
thus making this a fixed effects (aka LSDV) model?

T


Jean Eid wrote:
> You can turn the identity vector of the firms into a factor and do lm ....
> 
> Jean
> 
> On Mon, 5 Sep 2005, Tobias Muhlhofer wrote:
> 
> 
>>Hi, all!
>>
>>Anyone know an easy way to specify the following model.
>>
>>Panel dataset, with stock through time, by firm.
>>
>>I want to run a model of y on a bunch of explanatory variables, and one
>>dummy for each firm, which is 1 for observations that come from firm i,
>>and 0 everywhere else. I have over 200 firms (and a factor variable that
>>  contains a firm identifier).
>>
>>Any easy way of going about this, without having to define all these
>>dummies? I checked lme() with random = ~ 1|firm, but the problem is that
>>these are random effects, i.e. that there are firm-by-firm disturbance
>>terms and overall disturbance terms, whereas I want just overall
>>disturbance terms. This is generally called a "fixed effects" model,
>>although it seems like the term "fixed effects" is being used somewhat
>>differently in the context of the nlme package.
>>
>>Toby
>>
>>--
>>**************************************************************************
>>When Thomas Edison invented the light bulb he tried over 2000
>>experiments before he got it to work. A young reporter asked
>>him how it felt to have failed so many times. He said
>>"I never failed once. I invented the light bulb.
>>It just happened to be a 2000-step process."
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> 

-- 
**************************************************************************
When Thomas Edison invented the light bulb he tried over 2000
experiments before he got it to work. A young reporter asked
him how it felt to have failed so many times. He said
"I never failed once. I invented the light bulb.
It just happened to be a 2000-step process."



From dmbates at gmail.com  Mon Sep  5 16:55:24 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 5 Sep 2005 09:55:24 -0500
Subject: [R] model comparison and Wald-tests (e.g. in lmer)
In-Reply-To: <431C4199.6070206@hhbio.wasser.tu-dresden.de>
References: <431C4199.6070206@hhbio.wasser.tu-dresden.de>
Message-ID: <40e66e0b050905075520749792@mail.gmail.com>

On 9/5/05, Thomas Petzoldt <thpe at hhbio.wasser.tu-dresden.de> wrote:
> Dear expeRts,
> 
> there is obviously a general trend to use model comparisons, LRT and AIC
> instead of Wald-test-based significance, at least in the R community.
> I personally like this approach. And, when using LME's, it seems to be
> the preferred way (concluded from postings of Brian Ripley and Douglas
> Bates' article in R-News 5(2005)1), esp. because of problems with the
> d.f. approximation.
> 
> But, on the other hand I found that not all colleagues are happy with the
> resulting AIC/LRT tables and the comparison of multiple models.
> 
> As a compromise, and after a suggestion in Crawley's "Statistical
> computing" one may consider to supply "traditional" ANOVA tables as an
> additional explanation for the reader (e.g. field biologists).
> 
> An example:
> 
> one has fitted 5 models m1..m5 and after:
> 
>  >anova(m1,m2,m3,m4,m5) # giving AIC and LRT-tests
> 
> he selects m3 as the most parsimonious model and calls anova with the
> best model (Wald-test):
> 
>  >anova(m3)             # the additional explanatory table

Whether or not this is a good idea will depend on what the differences
in the models are.  Two mixed-effects models for the same data set can
differ in their random effects specification or in the fixed-effects
specification or both.  The anova() function applied to a single lmer
model produces a sequential anova table for the fixed-effects terms.

If the models differ in the random effects specification - say the
full model has random effects for slope and intercept but the
restricted model has a random effect for the intercept only - then a
Wald test is not appropriate (and it is not provided).  In those cases
I would use a likelihood ratio test and, if necessary, approximate the
p-value by simulating from the null hypothesis rather than assuming a
chi-squared distribution of the test statistic.

Recent versions of the mlmRev package have a vignette with extensive
analysis of the Exam data, including MCMC samples from the posterior
distribution of the parameters.  The marginal posterior distribution
of the variance components are quite clearly skewed (not surprisingly,
they look like scaled chi-squared distributions).  Testing whether
such a parameter could be zero by creating a z-statistic from the
estimate and its standard error is inappropriate.

Changing both the fixed-effects and the random-effects specification
is tricky.  I would try to do such changes in stages, settling on the
fixed-effects terms first then checking the random-effects
specification.
> 
> My questions:
> 
> * Do people outside the S-PLUS/R world still understand us?
> 
> * Is it wise to add such an explanatory table (in particular when the
> results are the same) to make the results more transparent to the reader?
> 
> * Are such additional ANOVA tables *really helpful* or are they (in
> combination with a model comparison) just another source of confusion?
> 
> 
> Thank you!
> 
> Thomas P.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Mon Sep  5 18:05:00 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 05 Sep 2005 17:05:00 +0100
Subject: [R] problems with outer ( was Re:  help )
In-Reply-To: <431C34C0.EADF5711@stats.uct.ac.za>
References: <431C34C0.EADF5711@stats.uct.ac.za>
Message-ID: <1125936300.6476.112.camel@localhost.localdomain>

[[ Please use a meaningful subject line. Thank you. ]]

Dominique,

I am unable to reproduce your example in R-2.1.1. Can you reproduce this
in a fresh session of R and state which version are you using ?

 u <- v <- seq(0, 1, length=30)
 f <- function(x, y){ cp=max(x+y-1,0) }
 z <- outer(u, v, f)
Error in outer(u, v, f) : dim<- : dims [product 900] do not match the
length ofobject [1]



As shown in the R FAQ 7.17 (http://tinyurl.com/amofj), you will need to
write a wrapper function. This works for me.

 wrapper <- function(x, y, my.fun, ...) {
      sapply(seq(along = x), FUN = function(i) f(x[i], y[i], ...))
  }
 z <- outer(u, v, wrapper)

dim(z)
[1] 30 30

range(z)
[1] 0 1


Regards, Adai

 


On Mon, 2005-09-05 at 14:06 +0200, Dominique Katshunga wrote:
> Dear helpeRs,
> I seem to be a little bit confused on the result I am getting from the
> few codes below:
> > u=v=seq(0,1,length=30)
> > u
>  [1] 0.00000000 0.03448276 0.06896552 0.10344828 0.13793103 0.17241379
>  [7] 0.20689655 0.24137931 0.27586207 0.31034483 0.34482759 0.37931034
> [13] 0.41379310 0.44827586 0.48275862 0.51724138 0.55172414 0.58620690
> [19] 0.62068966 0.65517241 0.68965517 0.72413793 0.75862069 0.79310345
> [25] 0.82758621 0.86206897 0.89655172 0.93103448 0.96551724 1.00000000
> > v
>  [1] 0.00000000 0.03448276 0.06896552 0.10344828 0.13793103 0.17241379
>  [7] 0.20689655 0.24137931 0.27586207 0.31034483 0.34482759 0.37931034
> [13] 0.41379310 0.44827586 0.48275862 0.51724138 0.55172414 0.58620690
> [19] 0.62068966 0.65517241 0.68965517 0.72413793 0.75862069 0.79310345
> [25] 0.82758621 0.86206897 0.89655172 0.93103448 0.96551724 1.00000000
> > f=function(x,y){cp=max(x+y-1,0)}
> > z=outer(u,v,f)
> z is a 30x30 matrix which is fine, but why all its entries are equal to
> 1? for example, the maximum between u[22]+v[22]-1 and 0 is not 1?? I
> don't really know where I went wrong! 
> thanks,
> Dominique
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From chaix at u707.jussieu.fr  Mon Sep  5 17:12:55 2005
From: chaix at u707.jussieu.fr (Basile Chaix)
Date: Mon, 5 Sep 2005 17:12:55 +0200
Subject: [R] Cox model with a frailty term
Message-ID: <20050905151203.873259BB55@mailer.b3e.jussieu.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050905/2c61a7a3/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Mon Sep  5 17:11:28 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Sep 2005 16:11:28 +0100 (BST)
Subject: [R] help
In-Reply-To: <431C34C0.EADF5711@stats.uct.ac.za>
Message-ID: <XFMail.050905161128.Ted.Harding@nessie.mcc.ac.uk>

On 05-Sep-05 Dominique Katshunga wrote:
> Dear helpeRs,
> I seem to be a little bit confused on the result I am getting from the
> few codes below:
>> u=v=seq(0,1,length=30)
>> u
>  [1] 0.00000000 0.03448276 0.06896552 0.10344828 0.13793103 0.17241379
>  [7] 0.20689655 0.24137931 0.27586207 0.31034483 0.34482759 0.37931034
> [13] 0.41379310 0.44827586 0.48275862 0.51724138 0.55172414 0.58620690
> [19] 0.62068966 0.65517241 0.68965517 0.72413793 0.75862069 0.79310345
> [25] 0.82758621 0.86206897 0.89655172 0.93103448 0.96551724 1.00000000
>> v
>  [1] 0.00000000 0.03448276 0.06896552 0.10344828 0.13793103 0.17241379
>  [7] 0.20689655 0.24137931 0.27586207 0.31034483 0.34482759 0.37931034
> [13] 0.41379310 0.44827586 0.48275862 0.51724138 0.55172414 0.58620690
> [19] 0.62068966 0.65517241 0.68965517 0.72413793 0.75862069 0.79310345
> [25] 0.82758621 0.86206897 0.89655172 0.93103448 0.96551724 1.00000000
>> f=function(x,y){cp=max(x+y-1,0)}
>> z=outer(u,v,f)
> z is a 30x30 matrix which is fine, but why all its entries are equal to
> 1? for example, the maximum between u[22]+v[22]-1 and 0 is not 1?? I
> don't really know where I went wrong! 
> thanks,
> Dominique

A trap easy to fall into!

First, note from "?outer" that "FUN" must be a function which
"operates elementwise on arrays". In other words, given two
array arguments (x,y), for each element of x and the corresponding
element of y, it returns a value.

However, 'max' is not such a function (despite intuitive expectations).

Specifically:

  max(u+v-1,0)
  [1] 1

In other words, 'max' is like 'sum': it evaluates a single result
for the whole array. However, if you look at "?max" and read far
enough, you will find 'pmax' -- "parallel maximum" -- which does
it element by element.

Look at

  pmax(u+v-1,0)

So try

  f<-function(x,y){pmax(x+y-1,0)}
  outer(u,v,f)

The reason this is a trap is that without looking at the code for
'outer' one might perhaps expect that 'outer' picks in turn each
possible pair of values (u[i],v[j]) and passes this pair as a
set of two numbers (x,y) to f(x,y). This is not so: it passes the
entire array of pairs all at once. When "FUN" is 'max' it returns the
maximum of this entire array for each position in the array!

Look at the code for 'outer' to see how this happens: just enter

  outer

The clue is in the line

  robj <- array(FUN(X, Y, ...), c(dX, dY))

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Sep-05                                       Time: 16:11:22
------------------------------ XFMail ------------------------------



From ggrothendieck at gmail.com  Mon Sep  5 17:19:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Sep 2005 11:19:55 -0400
Subject: [R] TeX distribution on Windows
In-Reply-To: <20050905135941.GB18905@stat.umu.se>
References: <20050905135941.GB18905@stat.umu.se>
Message-ID: <971536df05090508194dfc180d@mail.gmail.com>

On 9/5/05, G??ran Brostr??m <gb at stat.umu.se> wrote:
> I'm looking for a Windows distribution of TeX that works with  R, after a
> few years' absence from Windows. On Duncan Murdoch's Rtools page fptex is
> still recommended, but it turns out that fptex is "defunct" as of May 2005,
> see
> 
> http://www.metz.supelec.fr/~popineau/xemtex-7.html
> 
> So, what is suggested? TUG (tug.org) recommends something called proTeXt,
> which is said to be "based on MiKTeX", for Windows users. Since MikTeX
> could be used with  R, that sounds like a good alternative.
> 
> Any comments to that?
> 
> --
> G??ran Brostr??m                    tel: +46 90 786 5223
> Department of Statistics          fax: +46 90 786 6614
> Ume?? University                   http://www.stat.umu.se/~goran.brostrom/
> SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

As others have commented MiKTeX works with R.  Duncan Murdoch's
site mentions some ways to circumvent its one limitation.  This is
automated in the miktex-update.bat file in 

http://cran.r-project.org/contrib/extra/batchfiles/

for Windows XP systems.  It will locate the current version of R on
your system using
the registry and then copy the appropriate .fd and .sty files from your R 
distribution to the appropriate MiKTeX directory.   Assuming R and MiKTeX
are installed, just issue this command without arguments:

   miktex-refresh

at the Windows console.  Using that command, all you have to do is install
MiKTeX without any customizations and then run the above command to
copy the mentioned files from R to MiKTeX.

(If any of the TeX files in the R distribution change then you should rerun
the above command after each install of R.  If they have not changed
you can run it or not, it does not matter.)

Also in batchfiles, is the following Windows XP command:

   Rfind

which when issued without arguments will list the location on your system of a 
number of programs used with R including MiKTeX.   It does not actually
modify any environment variables or any other aspect of your system.



From gb at stat.umu.se  Mon Sep  5 17:22:35 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Mon, 5 Sep 2005 17:22:35 +0200
Subject: [R] TeX distribution on Windows
In-Reply-To: <431C5119.4070102@maths.anu.edu.au>
References: <20050905135941.GB18905@stat.umu.se>
	<431C5119.4070102@maths.anu.edu.au>
Message-ID: <20050905152235.GA475@stat.umu.se>

On Tue, Sep 06, 2005 at 12:07:21AM +1000, Ko-Kang Kevin Wang wrote:

> Slightly off topic, if you want to get MikTeX working with Emacs and 
> ESS, the  Claus Dethlefsen has a wonderful web site (in fact, best 
> website on this topic IMHO) http://www.math.auc.dk/~dethlef/Tips/

Thanks for that tips, it looks promising (btw, it should be 
"www.math.aau.dk").

The consensus (so far) seems to be to stick to MikTeX and skip proTeXt.
Thanks for all the input.

G??ran



From ramasamy at cancer.org.uk  Mon Sep  5 18:30:12 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 05 Sep 2005 17:30:12 +0100
Subject: [R] Dummy variables model
In-Reply-To: <431C5C03.9030704@lse.ac.uk>
References: <Pine.SGI.4.40.0509051046160.468230-100000@origin.chass.utoronto.ca>
	<431C5C03.9030704@lse.ac.uk>
Message-ID: <1125937812.6476.121.camel@localhost.localdomain>

You will need to ensure that firm is a factor and not numerical (i.e.
continuous). Here is an example 


 firm <- factor( sample(1:3, 20, replace=T) )
 x1   <- runif(20)
 y    <- rnorm(20)

 summary( fit <- lm( y ~ -1 + x1 + firm ) )
  ...
  Coefficients:
        Estimate Std. Error t value Pr(>|t|)
  x1    -0.04964    0.74861  -0.066    0.948
  firm1  0.10732    0.48269   0.222    0.827
  firm2  0.27548    0.48781   0.565    0.580
  firm3 -0.07651    0.53384  -0.143    0.888

NB : The "-1" in the formula forces each firm to have its own intercept.


Use model.matrix, you will see the dummy variables created within lm().

 model.matrix( fit )
           x1 firm1 firm2 firm3
 1  0.6641647     0     1     0
 2  0.5142712     1     0     0
 3  0.2197956     1     0     0
 4  0.3211675     0     1     0
 5  0.1892449     1     0     0
 6  0.7740754     0     0     1
 7  0.3486932     0     1     0
 8  0.2116816     0     0     1
 9  0.2426825     0     1     0
 10 0.2219768     1     0     0
 11 0.9328514     1     0     0
 12 0.7880405     0     0     1
 13 0.8673492     0     1     0
 14 0.1777998     0     1     0
 15 0.3178498     1     0     0
 16 0.3379726     0     0     1
 17 0.9193359     1     0     0
 18 0.6998152     0     1     0
 19 0.2825702     0     0     1
 20 0.6139586     1     0     0

Regards, Adai



On Mon, 2005-09-05 at 15:53 +0100, Tobias Muhlhofer wrote:
> So are you guys saying to me that if I have variable firm which is the 
> factor of all firm identifiers, I could just go
> 
> lm(y ~ x + firm)
> 
> and that will implicitly include a dummy for each level of factor firm, 
> thus making this a fixed effects (aka LSDV) model?
> 
> T
> 
> 
> Jean Eid wrote:
> > You can turn the identity vector of the firms into a factor and do lm ....
> > 
> > Jean
> > 
> > On Mon, 5 Sep 2005, Tobias Muhlhofer wrote:
> > 
> > 
> >>Hi, all!
> >>
> >>Anyone know an easy way to specify the following model.
> >>
> >>Panel dataset, with stock through time, by firm.
> >>
> >>I want to run a model of y on a bunch of explanatory variables, and one
> >>dummy for each firm, which is 1 for observations that come from firm i,
> >>and 0 everywhere else. I have over 200 firms (and a factor variable that
> >>  contains a firm identifier).
> >>
> >>Any easy way of going about this, without having to define all these
> >>dummies? I checked lme() with random = ~ 1|firm, but the problem is that
> >>these are random effects, i.e. that there are firm-by-firm disturbance
> >>terms and overall disturbance terms, whereas I want just overall
> >>disturbance terms. This is generally called a "fixed effects" model,
> >>although it seems like the term "fixed effects" is being used somewhat
> >>differently in the context of the nlme package.
> >>
> >>Toby
> >>
> >>--
> >>**************************************************************************
> >>When Thomas Edison invented the light bulb he tried over 2000
> >>experiments before he got it to work. A young reporter asked
> >>him how it felt to have failed so many times. He said
> >>"I never failed once. I invented the light bulb.
> >>It just happened to be a 2000-step process."
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> > 
> > 
> > 
>



From jeaneid at chass.utoronto.ca  Mon Sep  5 17:30:31 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Mon, 5 Sep 2005 11:30:31 -0400
Subject: [R] Dummy variables model
In-Reply-To: <431C5C03.9030704@lse.ac.uk>
Message-ID: <Pine.SGI.4.40.0509051122050.478705-100000@origin.chass.utoronto.ca>

here's an example

data(iris)
iris1<-iris
iris1$setosa<-0
iris1[iris1$Species%in%"setosa", "setosa"]<-1
iris1$versicolor<-0
iris1$virginica<-0
iris1[iris1$Species%in%"virginica", "virginica"]<-1
iris1[iris1$Species%in%"versicolor", "versicolor"]<-1
iris1<-iris1[, !colnames(iris1)%in%"Species"]
summary(lm(Sepal.Length~.-1, data=iris1))

Call:
lm(formula = Sepal.Length ~ . - 1, data = iris1)

Residuals:
      Min        1Q    Median        3Q       Max
-0.794236 -0.218743  0.008987  0.202546  0.731034

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
Sepal.Width   0.49589    0.08607   5.761 4.87e-08 ***
Petal.Length  0.82924    0.06853  12.101  < 2e-16 ***
Petal.Width  -0.31516    0.15120  -2.084  0.03889 *
setosa        2.17127    0.27979   7.760 1.43e-12 ***
versicolor    1.44770    0.28149   5.143 8.68e-07 ***
virginica     1.14777    0.35356   3.246  0.00145 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.3068 on 144 degrees of freedom
Multiple R-Squared: 0.9974,     Adjusted R-squared: 0.9973
F-statistic:  9224 on 6 and 144 DF,  p-value: < 2.2e-16



summary(lm(Sepal.Length~.-1, data=iris))

Call:
lm(formula = Sepal.Length ~ . - 1, data = iris)

Residuals:
      Min        1Q    Median        3Q       Max
-0.794236 -0.218743  0.008987  0.202546  0.731034

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)
Sepal.Width        0.49589    0.08607   5.761 4.87e-08 ***
Petal.Length       0.82924    0.06853  12.101  < 2e-16 ***
Petal.Width       -0.31516    0.15120  -2.084  0.03889 *
Speciessetosa      2.17127    0.27979   7.760 1.43e-12 ***
Speciesversicolor  1.44770    0.28149   5.143 8.68e-07 ***
Speciesvirginica   1.14777    0.35356   3.246  0.00145 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.3068 on 144 degrees of freedom
Multiple R-Squared: 0.9974,     Adjusted R-squared: 0.9973
F-statistic:  9224 on 6 and 144 DF,  p-value: < 2.2e-16






On Mon, 5 Sep 2005, Tobias Muhlhofer wrote:

> So are you guys saying to me that if I have variable firm which is the
> factor of all firm identifiers, I could just go
>
> lm(y ~ x + firm)
>
> and that will implicitly include a dummy for each level of factor firm,
> thus making this a fixed effects (aka LSDV) model?
>
> T
>
>
> Jean Eid wrote:
> > You can turn the identity vector of the firms into a factor and do lm ....
> >
> > Jean
> >
> > On Mon, 5 Sep 2005, Tobias Muhlhofer wrote:
> >
> >
> >>Hi, all!
> >>
> >>Anyone know an easy way to specify the following model.
> >>
> >>Panel dataset, with stock through time, by firm.
> >>
> >>I want to run a model of y on a bunch of explanatory variables, and one
> >>dummy for each firm, which is 1 for observations that come from firm i,
> >>and 0 everywhere else. I have over 200 firms (and a factor variable that
> >>  contains a firm identifier).
> >>
> >>Any easy way of going about this, without having to define all these
> >>dummies? I checked lme() with random = ~ 1|firm, but the problem is that
> >>these are random effects, i.e. that there are firm-by-firm disturbance
> >>terms and overall disturbance terms, whereas I want just overall
> >>disturbance terms. This is generally called a "fixed effects" model,
> >>although it seems like the term "fixed effects" is being used somewhat
> >>differently in the context of the nlme package.
> >>
> >>Toby
> >>
> >>--
> >>**************************************************************************
> >>When Thomas Edison invented the light bulb he tried over 2000
> >>experiments before he got it to work. A young reporter asked
> >>him how it felt to have failed so many times. He said
> >>"I never failed once. I invented the light bulb.
> >>It just happened to be a 2000-step process."
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >
> >
> >
>
> --
> **************************************************************************
> When Thomas Edison invented the light bulb he tried over 2000
> experiments before he got it to work. A young reporter asked
> him how it felt to have failed so many times. He said
> "I never failed once. I invented the light bulb.
> It just happened to be a 2000-step process."
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Mon Sep  5 17:32:58 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Sep 2005 11:32:58 -0400
Subject: [R] numerical intergation
In-Reply-To: <431C16E6.E0CC6C43@STATS.uct.ac.za>
References: <431C16E6.E0CC6C43@STATS.uct.ac.za>
Message-ID: <971536df050905083255628af7@mail.gmail.com>

On 9/5/05, Clark Allan <Allan at stats.uct.ac.za> wrote:
> 
> 
> how does one numerically intergate the following:
> 
> A=function(x,y)
> {
>        xy
> }
> 
> over the range: 2<x<0   4<y<10
> 
> say.
> 
> 
> ie how would one set up the integrate function?
> 
> i forgot!
> 

In this particular case its separable so you could just
integrate each factor and multiply the two results
but if you want to do it in terms of A then see:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/43836.html



From spencer.graves at pdf.com  Mon Sep  5 17:51:24 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 05 Sep 2005 08:51:24 -0700
Subject: [R] Doubt about nested aov output
In-Reply-To: <17180.4651.324909.367229@stat.math.ethz.ch>
References: <200508301101.32212.chrysopa@gmail.com>	<431B83DE.5060605@pdf.com>
	<17180.4651.324909.367229@stat.math.ethz.ch>
Message-ID: <431C697C.1080906@pdf.com>

	  That's great.  In your example, the aov fit is following by a warning:

 > res2 <- aov(y ~ fix + Error(fix/R1/R2), data = dat)
Warning message:
Error() model is singular in: aov(y ~ fix + Error(fix/R1/R2), data = dat)

	  Moreover, to test whether your change was statistically significant, 
library(nlme) supports "anova" with two nested models.  In this case, it 
produced the following:

 > anova(reslme, reslme2)
         Model df      AIC      BIC    logLik   Test      L.Ratio p-value
reslme      1  6 864.4973 881.0704 -426.2487
reslme2     2  7 866.4973 885.8325 -426.2487 1 vs 2 1.818989e-12       1
 >
	  I don't know if this relates to Ronaldo Reis' question, but it 
certainly seems plausible.

	  spencer graves	

Christoph Buser wrote:

> Hi
> 
> I think the problem is related to specifying "Tratamento" both
> as a fixed factor and in the error term. I attached a script
> with a reproducible example that shows a similar output.
> I do not know the details of the original data and the questions
> of interest, but maybe a model including "Tratamento" is more
> what you wanted to implement.
> 
> Regards,
> 
> Christoph
> 
> ## R-Script
> 
> library(nlme)
> 
> ## Generating the data
> set.seed(1)
> ziel <- rep(c(-6,8,20), each = 40) + rep(rnorm(15, 0, 20), each = 4) +
>   rep(rnorm(60, 0, 10), each = 2) + rnorm(120, 0, 3)
> dat <- data.frame(y = ziel,
>                   fix = factor(rep(1:3, each = 40)),
>                   R1 = factor(rep(1:15, each = 8)),
>                   R2 = factor(rep(1:60, each = 2)))
> 
> ## Model including "fix" as fixed and random effect.
> res2 <- aov(y ~ fix + Error(fix/R1/R2), data = dat)
> summary(res2)
> 
> reslme2 <- lme(y ~ fix , data = dat, random = ~ 1|fix/R1/R2)
> summary(reslme2)
> anova(reslme2)
> 
> ## Model including "fix" as fixed factor.
> res1 <- aov(y ~ fix + Error(R1/R2), data = dat)
> summary(res1)
> 
> reslme <- lme(y ~ fix , data = dat, random = ~ 1|R1/R2)
> summary(reslme)
> anova(reslme)
> 
> --------------------------------------------------------------
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C13
> ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> phone: x-41-44-632-4673		fax: 632-1228
> http://stat.ethz.ch/~buser/
> --------------------------------------------------------------
> 
> 
> 
> Spencer Graves writes:
>  > 	  Others may know the answer to your question, but I don't.  However, 
>  > since I have not seen a reply, I will offer a few comments:
>  > 
>  > 	  1.  What version of R are you using?  I just tried superficially 
>  > similar things with the examples in ?aov in R 2.1.1 patched and 
>  > consistently got F and p values.
>  > 
>  > 	  2.  My preference for this kind of thing is to use lme in 
>  > library(nlme) or lmer in library(lme4).  Also, I highly recommend 
>  > Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).
>  > 
>  > 	  3.  If still want to use aov and are getting this problem in R 2.1.1, 
>  > could you please provide this list with a small, self contained example 
>  > that displays the symptoms that concern you?  And PLEASE do read the 
>  > posting guide! "http://www.R-project.org/posting-guide.html".  It might 
>  > increase the speed and utility of replies.
>  > 
>  > 	  spencer graves
>  > 
>  > Ronaldo Reis-Jr. wrote:
>  > 
>  > > Hi,
>  > > 
>  > > I have two doubts about the nested aov output.
>  > > 
>  > > 1) I have this:
>  > > 
>  > >>anova.ratos <- aov(Glicogenio~Tratamento+Error(Tratamento/Rato/Figado))
>  > >>summary(anova.ratos)
>  > > 
>  > > 
>  > > Error: Tratamento
>  > >            Df  Sum Sq Mean Sq
>  > > Tratamento  2 1557.56  778.78
>  > > 
>  > > Error: Tratamento:Rato
>  > >           Df Sum Sq Mean Sq F value Pr(>F)
>  > > Residuals  3 797.67  265.89               
>  > > 
>  > > Error: Tratamento:Rato:Figado
>  > >           Df Sum Sq Mean Sq F value Pr(>F)
>  > > Residuals 12  594.0    49.5               
>  > > 
>  > > Error: Within
>  > >           Df Sum Sq Mean Sq F value Pr(>F)
>  > > Residuals 18 381.00   21.17               
>  > > 
>  > > R dont make the F and P automatically, it is possible?
>  > > 
>  > > I Like an output like this:
>  > > 
>  > > Error: Tratamento
>  > >            Df  Sum Sq Mean Sq F value Pr(>F)
>  > > Tratamento  2 1557.56  778.78   2.929 0.197
>  > > 
>  > > Error: Tratamento:Rato
>  > >           Df Sum Sq Mean Sq F value Pr(>F)
>  > > Residuals  3 797.67  265.89   5.372 0.0141      
>  > > 
>  > > Error: Tratamento:Rato:Figado
>  > >           Df Sum Sq Mean Sq F value Pr(>F)
>  > > Residuals 12  594.0    49.5   2.339 0.0503        
>  > > 
>  > > Error: Within
>  > >           Df Sum Sq Mean Sq F value Pr(>F)
>  > > Residuals 18 381.00   21.17         
>  > > 
>  > > Why it not make automatic calculations? It is possible?
>  > > 
>  > > 
>  > > 2) I can say that Error: Tratamento:Rato means an interaction between 
>  > > Tratamento and Rato? Normally the : represents an interaction, but in this 
>  > > output I think that it dont mean the interaction. 
>  > > 
>  > > Any explanation are welcome.
>  > > 
>  > > Thanks
>  > > Ronaldo
>  > 
>  > -- 
>  > Spencer Graves, PhD
>  > Senior Development Engineer
>  > PDF Solutions, Inc.
>  > 333 West San Carlos Street Suite 700
>  > San Jose, CA 95110, USA
>  > 
>  > spencer.graves at pdf.com
>  > www.pdf.com <http://www.pdf.com>
>  > Tel:  408-938-4420
>  > Fax: 408-280-7915
>  > 
>  > ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-help
>  > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  > 
>  > 
>  > !DSPAM:431b8510220677348368323!

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From cniharral_rhelp at yahoo.es  Mon Sep  5 18:15:20 2005
From: cniharral_rhelp at yahoo.es (C NL)
Date: Mon, 5 Sep 2005 18:15:20 +0200 (CEST)
Subject: [R] Fisher's method in discriminant analysis
Message-ID: <20050905161520.25704.qmail@web25405.mail.ukl.yahoo.com>

Hi,

  I'm using mda library to solve a discriminant
analysis. I get results, but the thing is that I want
to use Fisher's method to obtain the classification
functions and I'm lost in what I should do: libraries
to use, ... Can anybody give me a clue??

Thanks.

   Carlos Niharra L??pez



From tlumley at u.washington.edu  Mon Sep  5 18:25:00 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 5 Sep 2005 09:25:00 -0700 (PDT)
Subject: [R] help
In-Reply-To: <XFMail.050905161128.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050905161128.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.63a.0509050924300.366298@homer04.u.washington.edu>

On Mon, 5 Sep 2005, Ted.Harding at nessie.mcc.ac.uk wrote:
> A trap easy to fall into!
>

So easy, in fact, that it's a FAQ.

 	-thomas



From ramasamy at cancer.org.uk  Mon Sep  5 19:52:38 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 05 Sep 2005 18:52:38 +0100
Subject: [R] Fisher's method in discriminant analysis
In-Reply-To: <20050905161520.25704.qmail@web25405.mail.ukl.yahoo.com>
References: <20050905161520.25704.qmail@web25405.mail.ukl.yahoo.com>
Message-ID: <1125942758.6006.15.camel@dhcp-123.wolf.ox.ac.uk>

See this thread
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/34951.html

Sorry, my memory has not improved since then but there are others on
this list who know better this than myself.

Regards, Adai



On Mon, 2005-09-05 at 18:15 +0200, C NL wrote:
> Hi,
> 
>   I'm using mda library to solve a discriminant
> analysis. I get results, but the thing is that I want
> to use Fisher's method to obtain the classification
> functions and I'm lost in what I should do: libraries
> to use, ... Can anybody give me a clue??
> 
> Thanks.
> 
>    Carlos Niharra LÃ³pez
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From flom at ndri.org  Mon Sep  5 19:55:39 2005
From: flom at ndri.org (Peter Flom)
Date: Mon, 05 Sep 2005 13:55:39 -0400
Subject: [R] TeX distribution on Windows
Message-ID: <s31c4e70.032@MAIL.NDRI.ORG>

Goran wrote
<<<
The consensus (so far) seems to be to stick to MikTeX and skip proTeXt.
Thanks for all the input.
>>>

Well, nothing against MikTeX, but ProteXt also works fine with R.

Peter


Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From buddhahead at ranpura.com  Mon Sep  5 20:05:58 2005
From: buddhahead at ranpura.com (Ashish Ranpura)
Date: Mon, 5 Sep 2005 19:05:58 +0100
Subject: [R] simple line plots?
In-Reply-To: <mailman.9.1125914401.6646.r-help@stat.math.ethz.ch>
References: <mailman.9.1125914401.6646.r-help@stat.math.ethz.ch>
Message-ID: <8C0EED53-F193-4EF4-ADAF-D89F9B1E589D@ranpura.com>


I've spent quite a lot of the day trying to construct a fairly  
standard line plot in R, and I have the feeling there is a simple way  
that I haven't discovered.

I have a large vector of measurements (TIME), and each measurement  
falls into one of three categories (PHASE). For each PHASE value, I  
want a mean of the corresponding TIME measurements plotted as a point  
along with standard error bars. I'd like the three resulting point  
connected with line segments.

I'd like to have two data series like this plotted on the same graph  
-- one in red, one in blue.

Excel, as awful as it is, does this kind of graph quite easily.

After sifting through the scattered documentation, the best I could  
do was to store the mean values of the three points, plot those three  
numbers against the values 1,2, and 3, then use the arrows() function  
to draw error bars on each one. This is a LOT of manual effort, as  
you can imagine (in addition to the means I have to calculate the  
standard errors for each point, and I still don't know how to draw  
each of the three line segments I need).

Any suggestions?

Thanks,

--Ashish.


-----
Ashish Ranpura
Institute of Cognitive Neuroscience
University College London
17 Queen Square
LONDON WC1N 3AR



From Allan at STATS.uct.ac.za  Mon Sep  5 20:10:51 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Mon, 05 Sep 2005 20:10:51 +0200
Subject: [R] help
References: <431C34C0.EADF5711@stats.uct.ac.za>
Message-ID: <431C8A2B.7369D435@STATS.uct.ac.za>

howzit dom

i just saw your mail now!

i tried the following and got an error:

> u=v=seq(1,3)
> u
[1] 1 2 3
> v
[1] 1 2 3

> f=function(x,y){max(x+y-1,0)}
> z=outer(u,v,f)
Error in outer(u, v, f) : dim<- : dims [product 9] do not match the
length of object [1]

it seems s if r is not performing elementwise maximisation!!! 

all you need is:

f=function(x,y){pmax(x+y-1,0)}
z=outer(u,v,f)

ok lets try it!!! i used length =5 to save space

u=v=seq(0,1,length=5)

f=function(x,y){pmax(x+y-1,0)}
z=outer(u,v,f)
z
     [,1] [,2] [,3] [,4] [,5]
[1,]    0 0.00 0.00 0.00 0.00
[2,]    0 0.00 0.00 0.00 0.25
[3,]    0 0.00 0.00 0.25 0.50
[4,]    0 0.00 0.25 0.50 0.75
[5,]    0 0.25 0.50 0.75 1.00

think this works!

check ?pmax

see ya
allan

Dominique Katshunga wrote:
> 
> Dear helpeRs,
> I seem to be a little bit confused on the result I am getting from the
> few codes below:
> > u=v=seq(0,1,length=30)
> > u
>  [1] 0.00000000 0.03448276 0.06896552 0.10344828 0.13793103 0.17241379
>  [7] 0.20689655 0.24137931 0.27586207 0.31034483 0.34482759 0.37931034
> [13] 0.41379310 0.44827586 0.48275862 0.51724138 0.55172414 0.58620690
> [19] 0.62068966 0.65517241 0.68965517 0.72413793 0.75862069 0.79310345
> [25] 0.82758621 0.86206897 0.89655172 0.93103448 0.96551724 1.00000000
> > v
>  [1] 0.00000000 0.03448276 0.06896552 0.10344828 0.13793103 0.17241379
>  [7] 0.20689655 0.24137931 0.27586207 0.31034483 0.34482759 0.37931034
> [13] 0.41379310 0.44827586 0.48275862 0.51724138 0.55172414 0.58620690
> [19] 0.62068966 0.65517241 0.68965517 0.72413793 0.75862069 0.79310345
> [25] 0.82758621 0.86206897 0.89655172 0.93103448 0.96551724 1.00000000
> > f=function(x,y){cp=max(x+y-1,0)}
> > z=outer(u,v,f)
> z is a 30x30 matrix which is fine, but why all its entries are equal to
> 1? for example, the maximum between u[22]+v[22]-1 and 0 is not 1?? I
> don't really know where I went wrong!
> thanks,
> Dominique
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

From michael.holm at dicar.dk  Mon Sep  5 20:27:58 2005
From: michael.holm at dicar.dk (Michael Holm)
Date: Mon, 5 Sep 2005 20:27:58 +0200
Subject: [R] bivirate k-function
Message-ID: <20050905202876.SM00357@IBM3E731A7F216>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050905/41ee78f8/attachment.pl

From ggrothendieck at gmail.com  Mon Sep  5 21:05:58 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Sep 2005 15:05:58 -0400
Subject: [R] TeX distribution on Windows
In-Reply-To: <s31c4e70.032@MAIL.NDRI.ORG>
References: <s31c4e70.032@MAIL.NDRI.ORG>
Message-ID: <971536df05090512056d08a994@mail.gmail.com>

Goran wrote
> <<<
> The consensus (so far) seems to be to stick to MikTeX and skip proTeXt.
> Thanks for all the input.
> >>>

Just to clarify, I have never used proTeXt and my comment was based
on having used fptex and MiKTeX and the fact that the batchfiles distribution
provides specific additional R support for MiKTeX.



From t.muhlhofer at lse.ac.uk  Mon Sep  5 21:11:09 2005
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Mon, 05 Sep 2005 20:11:09 +0100
Subject: [R] Dummy variables model
In-Reply-To: <1125937812.6476.121.camel@localhost.localdomain>
References: <Pine.SGI.4.40.0509051046160.468230-100000@origin.chass.utoronto.ca>	
	<431C5C03.9030704@lse.ac.uk>
	<1125937812.6476.121.camel@localhost.localdomain>
Message-ID: <431C984D.7010005@lse.ac.uk>

Dang! That's awesome!!!!!

Being at the end of an empirical PhD in which all the econometrics was 
done in R, I was already a longtime R enthusiast, but you never stop 
learning more neat features!!!

YAY to everyone involved in R's development!!!!

Toby

Adaikalavan Ramasamy wrote:
> You will need to ensure that firm is a factor and not numerical (i.e.
> continuous). Here is an example 
> 
> 
>  firm <- factor( sample(1:3, 20, replace=T) )
>  x1   <- runif(20)
>  y    <- rnorm(20)
> 
>  summary( fit <- lm( y ~ -1 + x1 + firm ) )
>   ...
>   Coefficients:
>         Estimate Std. Error t value Pr(>|t|)
>   x1    -0.04964    0.74861  -0.066    0.948
>   firm1  0.10732    0.48269   0.222    0.827
>   firm2  0.27548    0.48781   0.565    0.580
>   firm3 -0.07651    0.53384  -0.143    0.888
> 
> NB : The "-1" in the formula forces each firm to have its own intercept.
> 
> 
> Use model.matrix, you will see the dummy variables created within lm().
> 
>  model.matrix( fit )
>            x1 firm1 firm2 firm3
>  1  0.6641647     0     1     0
>  2  0.5142712     1     0     0
>  3  0.2197956     1     0     0
>  4  0.3211675     0     1     0
>  5  0.1892449     1     0     0
>  6  0.7740754     0     0     1
>  7  0.3486932     0     1     0
>  8  0.2116816     0     0     1
>  9  0.2426825     0     1     0
>  10 0.2219768     1     0     0
>  11 0.9328514     1     0     0
>  12 0.7880405     0     0     1
>  13 0.8673492     0     1     0
>  14 0.1777998     0     1     0
>  15 0.3178498     1     0     0
>  16 0.3379726     0     0     1
>  17 0.9193359     1     0     0
>  18 0.6998152     0     1     0
>  19 0.2825702     0     0     1
>  20 0.6139586     1     0     0
> 
> Regards, Adai
> 
> 
> 
> On Mon, 2005-09-05 at 15:53 +0100, Tobias Muhlhofer wrote:
> 
>>So are you guys saying to me that if I have variable firm which is the 
>>factor of all firm identifiers, I could just go
>>
>>lm(y ~ x + firm)
>>
>>and that will implicitly include a dummy for each level of factor firm, 
>>thus making this a fixed effects (aka LSDV) model?
>>
>>T
>>
>>
>>Jean Eid wrote:
>>
>>>You can turn the identity vector of the firms into a factor and do lm ....
>>>
>>>Jean
>>>
>>>On Mon, 5 Sep 2005, Tobias Muhlhofer wrote:
>>>
>>>
>>>
>>>>Hi, all!
>>>>
>>>>Anyone know an easy way to specify the following model.
>>>>
>>>>Panel dataset, with stock through time, by firm.
>>>>
>>>>I want to run a model of y on a bunch of explanatory variables, and one
>>>>dummy for each firm, which is 1 for observations that come from firm i,
>>>>and 0 everywhere else. I have over 200 firms (and a factor variable that
>>>> contains a firm identifier).
>>>>
>>>>Any easy way of going about this, without having to define all these
>>>>dummies? I checked lme() with random = ~ 1|firm, but the problem is that
>>>>these are random effects, i.e. that there are firm-by-firm disturbance
>>>>terms and overall disturbance terms, whereas I want just overall
>>>>disturbance terms. This is generally called a "fixed effects" model,
>>>>although it seems like the term "fixed effects" is being used somewhat
>>>>differently in the context of the nlme package.
>>>>
>>>>Toby
>>>>
>>>>--
>>>>**************************************************************************
>>>>When Thomas Edison invented the light bulb he tried over 2000
>>>>experiments before he got it to work. A young reporter asked
>>>>him how it felt to have failed so many times. He said
>>>>"I never failed once. I invented the light bulb.
>>>>It just happened to be a 2000-step process."
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>>
> 
> 

-- 
**************************************************************************
When Thomas Edison invented the light bulb he tried over 2000
experiments before he got it to work. A young reporter asked
him how it felt to have failed so many times. He said
"I never failed once. I invented the light bulb.
It just happened to be a 2000-step process."



From ggrothendieck at gmail.com  Mon Sep  5 21:15:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Sep 2005 15:15:41 -0400
Subject: [R] TeX distribution on Windows
In-Reply-To: <971536df05090512056d08a994@mail.gmail.com>
References: <s31c4e70.032@MAIL.NDRI.ORG>
	<971536df05090512056d08a994@mail.gmail.com>
Message-ID: <971536df05090512153bb14b01@mail.gmail.com>

On 9/5/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Goran wrote
> > <<<
> > The consensus (so far) seems to be to stick to MikTeX and skip proTeXt.
> > Thanks for all the input.
> > >>>
> 
> Just to clarify, I have never used proTeXt and my comment was based
> on having used fptex and MiKTeX and the fact that the batchfiles distribution
> provides specific additional R support for MiKTeX.
> 

One other comment.  If anyone is using proTeXt or other TeX distribution
and wants to modify miktex-update.bat and Rfind.bat to support proTeXt or 
other TeX distribution I would be happy to provide a link to it from
the batchfiles documentation or even include it in batchfiles, if desired.



From shigesong at gmail.com  Mon Sep  5 21:16:07 2005
From: shigesong at gmail.com (Shige Song)
Date: Tue, 6 Sep 2005 03:16:07 +0800
Subject: [R] How to set starting values for lmer?
In-Reply-To: <40e66e0b050830103068bfbd86@mail.gmail.com>
References: <5abc11d805083008092fe632d4@mail.gmail.com>
	<40e66e0b050830103068bfbd86@mail.gmail.com>
Message-ID: <5abc11d805090512164099e634@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050906/a77c4951/attachment.pl

From gb at stat.umu.se  Mon Sep  5 21:42:34 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Mon, 5 Sep 2005 21:42:34 +0200
Subject: [R] TeX distribution on Windows
In-Reply-To: <s31c4e70.033@MAIL.NDRI.ORG>
References: <s31c4e70.033@MAIL.NDRI.ORG>
Message-ID: <20050905194234.GA16584@stat.umu.se>

On Mon, Sep 05, 2005 at 01:55:39PM -0400, Peter Flom wrote:
> Goran wrote
> <<<
> The consensus (so far) seems to be to stick to MikTeX and skip proTeXt.
> Thanks for all the input.
> >>>
> 
> Well, nothing against MikTeX, but ProteXt also works fine with R.

I have made some studying; it seems as if proTeXt is nothing but MikTeX 
together with WinEdt or TeXnicsCenter and ghostscript/ghostview, so the 
choice is really between WinEdt and (X)Emacs; you get MikTeX in both cases.
 
G??ran



From ggrothendieck at gmail.com  Mon Sep  5 21:56:25 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Sep 2005 15:56:25 -0400
Subject: [R] TeX distribution on Windows
In-Reply-To: <20050905194234.GA16584@stat.umu.se>
References: <s31c4e70.033@MAIL.NDRI.ORG> <20050905194234.GA16584@stat.umu.se>
Message-ID: <971536df050905125680f60b0@mail.gmail.com>

On 9/5/05, G??ran Brostr??m <gb at stat.umu.se> wrote:
> On Mon, Sep 05, 2005 at 01:55:39PM -0400, Peter Flom wrote:
> > Goran wrote
> > <<<
> > The consensus (so far) seems to be to stick to MikTeX and skip proTeXt.
> > Thanks for all the input.
> > >>>
> >
> > Well, nothing against MikTeX, but ProteXt also works fine with R.
> 
> I have made some studying; it seems as if proTeXt is nothing but MikTeX
> together with WinEdt or TeXnicsCenter and ghostscript/ghostview, so the
> choice is really between WinEdt and (X)Emacs; you get MikTeX in both cases.

In that case it probably uses the same registry entries in which case the batch
utilities I mentioned would likely work with it without any change.



From Roger.Bivand at nhh.no  Mon Sep  5 22:06:34 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 5 Sep 2005 22:06:34 +0200 (CEST)
Subject: [R] bivirate k-function
In-Reply-To: <20050905202876.SM00357@IBM3E731A7F216>
Message-ID: <Pine.LNX.4.44.0509052151420.32632-100000@reclus.nhh.no>

On Mon, 5 Sep 2005, Michael Holm wrote:

> Hi
> 
> Is there anyone who can help a reporter (me) with the following:
> 
>  
> 
> I want to use the bivirate k-function to determin if burgerbars are
> clustered around schools.
> 
> To that purpose I?ve been told to use the bivirate k-function, but my
> program Arcview doesn?t contain that.
> 
> Im stuck on a dead end ? how to I for a start get the data into R?
> 

The file format prefered by ArcView for point data is shapefile - see 
packages maptools or shapefiles on CRAN. Bivariate K-hat functions are 
available in splancs (k12hat), or in spatstat (Kcross and Kdot). A direct 
reference to the splancs functions is given by Bailey and Gatrell (1995) 
Interactive Spatial Data Analysis, Ch. 4. Spatstat is described clearly 
in:

http://www.jstatsoft.org/counter.php?id=113&url=v12/i06/v12i06.pdf&ct=1

You may need to import a bounding polygon too.

Please feel free to follow this up on the R-sig-geo list, access easiest
from the CRAN "Spatial" task view:

http://cran.r-project.org/src/contrib/Views/Spatial.html

(Note though, that schools may be close to other confounding locations
attracting burger-vores, so you may need a school just by itself to draw
any sensible conclusions)

>  
> 
> Michael Holm
> 
> DICAR ? Center for Anaytical Reporting ? HYPERLINK
> "http://www.dicar.org/"www.dicar.org
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From dmbates at gmail.com  Mon Sep  5 22:25:49 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 5 Sep 2005 15:25:49 -0500
Subject: [R] How to set starting values for lmer?
In-Reply-To: <5abc11d805090512164099e634@mail.gmail.com>
References: <5abc11d805083008092fe632d4@mail.gmail.com>
	<40e66e0b050830103068bfbd86@mail.gmail.com>
	<5abc11d805090512164099e634@mail.gmail.com>
Message-ID: <40e66e0b05090513251bc8d830@mail.gmail.com>

On 9/5/05, Shige Song <shigesong at gmail.com> wrote:
> Dear Professor Bates,
> 
> Thanks, that will probably do the job.

OK, I will add that capability.

> By the way, how to cite lme4 in my work?

For now I suggest citing either the package itself or the R News
article that I wrote about it.

@Article{Rnews:Bates:2005,
  author       = {Douglas Bates},
  title	       = {Fitting Linear Mixed Models in {R}},
  journal      = {R News},
  year	       = 2005,
  volume       = 5,
  number       = 1,
  pages	       = {27--30},
  month	       = {May},
  url	       = {http://CRAN.R-project.org/doc/Rnews/},
}

Eventually there will be a book that you can cite but I have to finish
writing it first :-)

> 
> Shige
> 
> On 8/31/05, Douglas Bates <dmbates at gmail.com> wrote:
> >
> > On 8/30/05, Shige Song <shigesong at gmail.com> wrote:
> > > Dear All,
> > >
> > > Can anyone give me some hints about how to set starting values for a
> > lmer
> > > model? For complicated models like this, good starting values can help
> > the
> > > numerical computation and make the model converge faster. Thanks!
> > >
> > > Shige
> >
> > I agree but I haven't gotten around to designing how that could be
> > done. It could be easy or difficult depending on how you want to
> > represent the starting values.
> >
> > If you look at the (only) lmer method function you will see that it
> > has a section
> >
> > if (lmm) { ## linear mixed model
> > .Call("lmer_initial", mer, PACKAGE="Matrix")
> > .Call("lmer_ECMEsteps", mer, cv$niterEM, cv$EMverbose,
> > PACKAGE = "Matrix")
> > LMEoptimize(mer) <- cv
> >
> > for linear mixed models. The object "mer" is a mixed-effects
> > representation and the list "cv" is the control values. The only
> > thing that the C function "lmer_initial" does is set the initial
> > values of the relative precision matrices for the random effects.
> > These are the inverses of the variance-covariance matrices relative to
> > the variance of the per-observation noise term. They are stored
> > (upper triangle only) in a slot called "Omega" of the mer class (which
> > is contained in the lmer class).
> >
> > There is no purpose in setting initial values for the fixed-effects
> > parameters or the variance of the per-observation noise term because
> > these are profiled out of the optimization. The optimization is only
> > over the values in the Omega slot.
> >
> > I can allow those values to be set from an argument and only call
> > "lmer_initialize" if that argument is missing. Will that be
> > sufficient for you?
> >
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Tue Sep  6 00:01:17 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 05 Sep 2005 15:01:17 -0700
Subject: [R] partial association model
In-Reply-To: <186d7de18754eb.18754eb186d7de@fudan.edu.cn>
References: <186d7de18754eb.18754eb186d7de@fudan.edu.cn>
Message-ID: <431CC02D.9050307@pdf.com>

	  I'm not familiar with the term "partial association model", and I 
don't have the Sloane and Morgan paper you cite.  Neither Google nor 
RSiteSearch("partial association model") produced anything that looked 
to me like what you were asking.

	  Part of the R culture is that one can fit anything with R;  the only 
question is how.  To determine that and to answer your other question on 
the advantages of log-linear models, I believe you could best approach 
those questions by consulting the references in the help file for "glm". 
  In particular, I recommend you start with Venables and Ripley (2002) 
Modern Applied Statistics with S, 4th ed. (Springer).  It is probably 
the best known book on this issue.  If  you are not already familiar 
with this book, I highly recommend it.  I don't have a copy at my 
fingertips now, but with luck, you may find answers to both your 
questions.  It may not discuss "partial association models" by that 
name, but if you know partial association models, you might be able to 
find something relevant in that book.

	  McCullagh P. and Nelder, J. A. (1989) Generalized Linear Models. 
(London: Chapman and Hall) is the second and substantially enlarged 
edition of the initial defining book on this topic.  Hastie, T. J. and 
Pregibon, D. (1992) "Generalized linear models", Chapter 6 of 
_Statistical Models in S_ eds J. M. Chambers and T. J. Hastie, 
(Wadsworth & Brooks/Cole) may provide useful information not available 
in Venables and Ripley.

	  I doubt if this answered your question, but I hope it at least will 
help you in some way.  Feel free to post another question.  However, I 
think you will help yourself if you read and follow the posting guide! 
"http://www.R-project.org/posting-guide.html".  I suspect that on 
average, posts prepared following this guide get more useful answers 
quicker.

	  spencer graves

0034058 at fudan.edu.cn wrote:

> my last post was filtered,so I post it again with another title.
> 
> If I do not make a mistake,the partial association model is an 
> extension of log-linear model.I read a papers which gives an example 
> of it.(Sloane and Morgan,1996,An Introduction to Categorical Data 
> Analysis,Annual Review of Sociology.22:351-375) Can R fit such partial 
> association model?
> 
> ps:Another somewhat off-topic question.What's the motivations to use 
> log-linear model?Or why use log-linear model?I learn the log-linear 
> model butI still do not master the the advantage of the model.thank 
> you!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From nikko at hailmail.net  Tue Sep  6 00:16:23 2005
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Mon, 05 Sep 2005 17:16:23 -0500
Subject: [R] tcltk, X11 protocol error:  Bug?
In-Reply-To: <mailman.7.1125828001.28816.r-devel@r-project.org>
References: <mailman.7.1125828001.28816.r-devel@r-project.org>
Message-ID: <1125958583.21550.242223315@webmail.messagingengine.com>

Hi,
I am having trouble debugging this one. The code is attached below, but
it seems to be a problem at the
C-tk interface. If I run this 1 time there are no problems if I run it
more than once I start to get warnings
that increase in multiples of 11 everytime I run it. Here is a sample
session


> source("clrramp2.r")
Loading required package: tcltk
Loading Tcl/Tk interface ... done
> clrRamp()
>
> tt<-clrRamp()
> tt
function (n)
{
    x <- ramp(seq(0, 1, length = n))
    rgb(x[, 1], x[, 2], x[, 3], max = 255)
}
<environment: 0x8b8674c>
> image(matrix(1:10),col=tt(10))
> tt<-clrRamp()
There were 22 warnings (use warnings() to see them)
> image(matrix(1:10),col=tt(10))
There were 11 warnings (use warnings() to see them)
> warnings()
Warning messages:
1: X11 protocol error: BadWindow (invalid Window parameter)
2: X11 protocol error: BadWindow (invalid Window parameter)
3: X11 protocol error: BadWindow (invalid Window parameter)
4: X11 protocol error: BadWindow (invalid Window parameter)
5: X11 protocol error: BadWindow (invalid Window parameter)
6: X11 protocol error: BadWindow (invalid Window parameter)
7: X11 protocol error: BadWindow (invalid Window parameter)
8: X11 protocol error: BadWindow (invalid Window parameter)
9: X11 protocol error: BadWindow (invalid Window parameter)
10: X11 protocol error: BadWindow (invalid Window parameter)
11: X11 protocol error: BadWindow (invalid Window parameter)

I am running R-2.1.1 on ubuntu linux 5.04, compiled from source (not the
deb package).
My version of tcl/tk is 8.4. The code is below. If anyone sees something
I am doing foolish
let me know, otherwise I will file a bug report.

Nicholas

##### File clrramp2.r ##############

require(tcltk)
clrRamp <- function(n.col, b.color=NULL,e.color=NULL){

  B.ChangeColor <- function()
    {
 
      b.color <<- tclvalue(tkcmd("tk_chooseColor",initialcolor=e.color,
                                 title="Choose a color"))
      if (nchar(b.color)>0){
        tkconfigure(canvas.b,bg=b.color)
        Rmp.Draw()
      }
    }

  E.ChangeColor <- function()
    {
 
      e.color <<- tclvalue(tkcmd("tk_chooseColor",initialcolor=e.color,
                                 title="Choose a color"))
      ##cat(e.color)
      if (nchar(e.color)>0){
        tkconfigure(canvas.e,bg=e.color)
        Rmp.Draw()
      }
    }

  Rmp.Draw <-function(){
    cr<<-colorRampPalette(c(b.color,e.color),space="Lab",interpolate="spline")
    rmpcol <- cr(n.col)
    #rmpcol<-rgb( rmpcol[,1],rmpcol[,2],rmpcol[,3])
    inc <- 300/n.col
    xl <- 0
    for(i in 1:n.col){
      ##item <- 
      tkitemconfigure(canvas.r,barlst[[i]],fill=rmpcol[i],outline=rmpcol[i])
      #xl <- xl+inc
    }
  }

  save.ramp <- function(){
    cr<<-colorRampPalette(c(b.color,e.color),space="Lab",interpolate="spline")
    tkdestroy(tt)
    ##invisible(cr)
  }

  tt <- tktoplevel()
  tkwm.title(tt,"Color Ramp Tool")
  frame <- tkframe(tt)
  bframe <- tkframe(frame,relief="groove",borderwidth=1)

  if(is.null(b.color)) b.color <- "blue"
  if(is.null(e.color)) e.color <- "yellow"
  if(missing(n.col)) n.col <- 100

  canvas.b <- tkcanvas(bframe,width="50",height="25",bg=b.color)
  canvas.e <- tkcanvas(bframe,width="50",height="25",bg=e.color)
  canvas.r <- tkcanvas(tt,width="300",height="50",bg="white")
  
  BColor.button <- tkbutton(bframe,text="Begin
  Color",command=B.ChangeColor)
  ##tkgrid(canvas.b,BColor.button)
  EColor.button <- tkbutton(bframe,text="End
  Color",command=E.ChangeColor)
  killbutton <- tkbutton(bframe,text="Save",command=save.ramp)
  tkgrid(canvas.b,BColor.button,canvas.e,EColor.button)
  tkgrid(bframe)
  tkgrid(frame)
  tkgrid(canvas.r)
  tkgrid(killbutton)

  cr<-colorRampPalette(c(b.color,e.color),space="Lab",interpolate="spline")
  ##rmpcol <- hex(mixcolor(alpha,bc,ec,where="LUV"))
  rmpcol <- cr(n.col)
  inc <- 300/n.col
  xl <- 0
  #barlst <- vector(length=n.col,mode="list")
  barlst <- tclArray()
  for(i in 1:n.col){
    item<-tkcreate(canvas.r,"rect",xl,0,xl+inc,50,
                   fill=rmpcol[i],outline=rmpcol[i])
    ##tkaddtag(canvas.r, "point", "withtag", item)
    barlst[[i]]<-item
    xl <- xl+inc
  }
  tkgrab.set(tt)
  tkwait.window(tt)

  ##tkdestroy(tt)
  invisible(cr)
}



From spencer.graves at pdf.com  Tue Sep  6 00:26:50 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 05 Sep 2005 15:26:50 -0700
Subject: [R] question sur R
In-Reply-To: <OF0284F780.781574A4-ONC1257070.002B5B26-C1257070.002BC617@notes.edfgdf.fr>
References: <OF0284F780.781574A4-ONC1257070.002B5B26-C1257070.002BC617@notes.edfgdf.fr>
Message-ID: <431CC62A.1000402@pdf.com>

	  Thanks for providing such a concise example.  I ran your example and 
got the same error message with R 2.1.1 patched and with the MASS 
package version 7.2-19.  Moreover, I got the same error with the 
following simplification:

test1 = polr(rating ~ X1)
summary(test1)

	  There are two possible next steps from this point:

	  (1) Copy the code for "polr" into a script file and work through it 
line by line until you understand where the error message arises and why.

	  (2) Report the problem to the package maintainer, who in this case is 
Prof. Brian Ripley <ripley at stats.ox.ac.uk>.  In reporting your problem, 
please preceed your script with something like "set.seed(1)".  Because 
you use pseudo-random numbers, someone else may get a different result 
unless you both start with the same seed.  Also, please report which 
version of R and the MASS package you used.  And you can certainly add 
that I get essentially the same result.

	  If it were my problem, I would try these two steps in this order. 
However, a polite note to Prof. Ripley asking about this without tracing 
the error yourself would also be acceptable.

	  I'm sorry I was not of more help, but I don't have time to trace this 
problem myself.

	  spencer graves

Abdelhafid BERRICHI wrote:

>>hello
>>
> 
> 
> 
>>I've tried to simulate a normal law, like that :
>>
>>X1 = c(rnorm(90,50,5583),rnorm(160,1198,13034597),rnorm(40,13,125))
>>
>>
>>then, I've regressed my ordinal polytomic variable "rating"
> 
> 
> 
>         rating=c(rep(2,90),rep(3,160), rep(4,40))
>         rating = as.factor(rating)
>         rating = as.ordered(rating)
>         (ratins is an ordered factor)
> 
> 
> 
>   on the continuous variable X1 like that
> 
> 
> 
> 
>>library(MASS)
>>test2 = polr(rating ~ X1, method = c("probit"))
>>summary(test2)
>>
>>
>>but R indicates me the following error whereas X does not have infinite 
>>or missing values?
>>
>>
>>Re-fitting to get Hessian
>>Error in svd(X) : infinite or missing values in x
>>
>>THANKS
>>good by
> 
> 
> abdelhafid berrichi
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From murdoch at stats.uwo.ca  Tue Sep  6 00:55:26 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 05 Sep 2005 18:55:26 -0400
Subject: [R] TeX distribution on Windows
In-Reply-To: <20050905135941.GB18905@stat.umu.se>
References: <20050905135941.GB18905@stat.umu.se>
Message-ID: <431CCCDD.6020200@stats.uwo.ca>

G??ran Brostr??m wrote:
> I'm looking for a Windows distribution of TeX that works with  R, after a 
> few years' absence from Windows. On Duncan Murdoch's Rtools page fptex is 
> still recommended, but it turns out that fptex is "defunct" as of May 2005,
> see 
> 
> http://www.metz.supelec.fr/~popineau/xemtex-7.html
> 
> So, what is suggested? TUG (tug.org) recommends something called proTeXt,
> which is said to be "based on MiKTeX", for Windows users. Since MikTeX 
> could be used with  R, that sounds like a good alternative.

I use MikTeX, with one or another of the workarounds listed on my page. 
   I've never tried proTeXt; I did a little googling, but I still don't 
see the point of it exactly.

fptex is still available in various repositories, and is likely to keep 
working for quite a long time:  R doesn't demand the latest and greatest 
innovations from TeX/eTeX.

Duncan Murdoch



From graumann at caltech.edu  Tue Sep  6 03:02:02 2005
From: graumann at caltech.edu (Johannes Graumann)
Date: Mon, 05 Sep 2005 18:02:02 -0700
Subject: [R] MASS: rlm, MM and errors in observations AND regressors
Message-ID: <dfipqb$qd6$1@sea.gmane.org>

Hello,

I need to perform a robust regression on data which contains errors in BOTH
observations and regressors. Right now I am using rlm from the MASS package
with 'method="MM"' and get visually very nice results. MASS is quite clear,
however, that the described methodologies are only applicable to
observation-error only data (p. 157, 4th Ed.). So here's the questions now:

a) is there methodology for robust and resistant regression on data with
errors in BOTH observations and regressors?

b) if "yes" at a): does somebody know of an implementation in R?

c) if "no" at b): would somebody reading this be open to implement this? If
yes: please get in contact with me off list.

Please excuse my unfamiliarity with terminology and subject - I'm very new
to statistics.

Joh

-- 
+----------------------------------------------------------------------+
| Johannes Graumann, Dipl. Biol.                                       |
|                                                                      |
|      Graduate Student                Tel.: ++1 (626) 395 6602        |
|      Deshaies Lab                    Fax.: ++1 (626) 395 5739        |
|      Department of Biology                                           |
|      CALTECH, M/C 156-29                                             |
|      1200 E. California Blvd.                                        |
|      Pasadena, CA 91125                                              |
|      USA                                                             |
+----------------------------------------------------------------------+



From shigesong at gmail.com  Tue Sep  6 04:45:42 2005
From: shigesong at gmail.com (Shige Song)
Date: Tue, 6 Sep 2005 10:45:42 +0800
Subject: [R] How to set starting values for lmer?
In-Reply-To: <40e66e0b05090513251bc8d830@mail.gmail.com>
References: <5abc11d805083008092fe632d4@mail.gmail.com>
	<40e66e0b050830103068bfbd86@mail.gmail.com>
	<5abc11d805090512164099e634@mail.gmail.com>
	<40e66e0b05090513251bc8d830@mail.gmail.com>
Message-ID: <5abc11d8050905194559a108e0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050906/813599a8/attachment.pl

From gb at stat.umu.se  Tue Sep  6 08:16:05 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Tue, 6 Sep 2005 08:16:05 +0200
Subject: [R] TeX distribution on Windows
In-Reply-To: <431CCCDD.6020200@stats.uwo.ca>
References: <20050905135941.GB18905@stat.umu.se>
	<431CCCDD.6020200@stats.uwo.ca>
Message-ID: <20050906061605.GC5203@stat.umu.se>

On Mon, Sep 05, 2005 at 06:55:26PM -0400, Duncan Murdoch wrote:
> G??ran Brostr??m wrote:
> >I'm looking for a Windows distribution of TeX that works with  R, after a 
> >few years' absence from Windows. On Duncan Murdoch's Rtools page fptex is 
> >still recommended, but it turns out that fptex is "defunct" as of May 2005,
> >see 
> >
> >http://www.metz.supelec.fr/~popineau/xemtex-7.html
> >
> >So, what is suggested? TUG (tug.org) recommends something called proTeXt,
> >which is said to be "based on MiKTeX", for Windows users. Since MikTeX 
> >could be used with  R, that sounds like a good alternative.
> 
> I use MikTeX, with one or another of the workarounds listed on my page. 
>   I've never tried proTeXt; I did a little googling, but I still don't 
> see the point of it exactly.

It's just MiKTeX with a few extras, like WinEdt, ghostscript, etc. As far 
as I understand, MikTeX itself is untouched.

> 
> fptex is still available in various repositories, and is likely to keep 
> working for quite a long time:  R doesn't demand the latest and greatest 
> innovations from TeX/eTeX.

Right. But maybe you should change the broken link to www.fptex.org.

G??ran



From Tom.Mulholland at dpi.wa.gov.au  Tue Sep  6 08:26:48 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 6 Sep 2005 14:26:48 +0800
Subject: [R] The Perils of PowerPoint
Message-ID: <4702645135092E4497088F71D9C8F51A128C13@afhex01.dpi.wa.gov.au>

For some reason (probably that our organisation has blocked the site) I could not see the original articles that prompted the post. I however immediately assumed that this was precipitated by Tufte and his comments about PowerPoint (I recall seeing a good example of PowerPoint on his site) http://www.edwardtufte.com/tufte/powerpoint

When this first came up I recall some dispute about the comments www.sociablemedia.com/articles_dispute.htm and that John Fox did something http://ils.unc.edu/~jfox/powerpoint/introduction.html that I enjoyed reading.

Other links that are lying on my computer are
"In defense of PowerPoint" http://www.jnd.org/dn.mss/in_defense_of_powerp.html
and "Does PowerPoint make you stupid?" at http://www.presentations.com/presentations/delivery/article_display.jsp?vnu_content_id=1000482464
 
Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Tim Churches
> Sent: Saturday, 3 September 2005 10:08 AM
> To: ted.harding at nessie.mcc.ac.uk
> Cc: Achim Zeileis; r-help at stat.math.ethz.ch
> Subject: Re: [R] The Perils of PowerPoint
> 
> 
> (Ted Harding) wrote:
> 
> >By the way, the Washington Post/Minneapolis Star Tribune article is
> >somewhat reminiscent of a short (15 min) broadcast on BBC Radio 4
> >back on October 18 2004 15:45-16:00 called
> >
> >  "Microsoft Powerpoint and the Decline of Civilisation"
> >
> >which explores similar themes and also frequently quotes Tufte.
> >Unfortunately it lapsed for ever from "Listen Again" after the
> >statutory week, so I can't point you to a replay. (However, I
> >have carefully preserved the cassette recording I made).
> >  
> >
> Try http://sooper.org/misc/powerpoint.mp3 (copyright law 
> notwithstanding...)
> 
> Tim C
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From thpe at hhbio.wasser.tu-dresden.de  Tue Sep  6 09:33:56 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Tue, 06 Sep 2005 09:33:56 +0200
Subject: [R] model selection vs. H0 based testing
In-Reply-To: <40e66e0b050905075520749792@mail.gmail.com>
References: <431C4199.6070206@hhbio.wasser.tu-dresden.de>
	<40e66e0b050905075520749792@mail.gmail.com>
Message-ID: <431D4664.20200@hhbio.wasser.tu-dresden.de>

Hello,

I wish to thank Douglas Bates very much for clarification and pointing 
me to the MCMC simulation method to get p values even for cases where 
Wald tests are inappropriate.

One question however remains when publishing statistical results: does 
it help readers if we combine both,

- AIC based model selection
*and*
- null hypothesis based tests statistics

or should we focus on model selection only and try to reduce the amount 
of tables provided?

Apologies if this is question is too much off-topic, so you may decide 
to answer off-list. I will give a short summary at the end.

Thomas P.

An article explaining the background:

Johnson, J. & Omland, K.S., Model Selection in Ecology and Evolution. 
Trends in Ecology and Evolution, 2004, 19 , 101-108



From maechler at stat.math.ethz.ch  Tue Sep  6 10:33:46 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Sep 2005 10:33:46 +0200
Subject: [R] SpatStat Kest -  Error Message help
In-Reply-To: <17179.43287.748245.156800@maths.uwa.edu.au>
References: <17179.43287.748245.156800@maths.uwa.edu.au>
Message-ID: <17181.21610.306995.558950@stat.math.ethz.ch>

>>>>> "AB" == Adrian Baddeley <adrian at maths.uwa.edu.au>
>>>>>     on Mon, 5 Sep 2005 10:10:31 +0800 writes:

    AB> On Thu, 1 Sep 2005, DrakeGis wrote:
    >> Hi I'm working with the function Kest in the package SpatStat (under LINUX
    >> with R 2.1.0). In order to evaluate the statistical significance of my
    >> point pattern I'm doing 999 Montecarlo replications. The script that use
    >> the Kest function runs OK for most of the different point patterns that I
    >> have but for a particular point pattern, which have only 17 points, it
    >> runs until the 34th iteration and then I receive this message:
    >> 
    >> Error in "[<-"(`*tmp*`, index, value = NULL) :
    >> incompatible types (1000) in subassignment type fix
    >> Execution halted
    >> 
    >> Do you have any idea about what could be the cause of this ? Thanks in
    >> advance.

    AB> This is not an error message from 'spatstat' itself.

    AB> The message has been generated by the function "[<-" 
    AB> which is called when you assign values to a subset of a dataset 
    AB> (in a command like x[z] <- v). The message appears to say that the
    AB> replacement value v is not of the same type as the original vector x. 
  
yes.
And please get into the habit of saying

    traceback()

after such an error.
This would have quickly revealed if the error came from R function
called from a function from 'spatstat' or not.

Also, maybe more people should learn about `basic debugging', 
by using something like

   options(error = recover)
or
   options(error = dump.frames) ## needs a later call to debugger()

before running the script that produces the error.
The end of the examples  ?options  show an example to use when
running an R script.
      
    AB> You say that you are running a "script that uses the Kest function".
    AB> The error is probably inside that script. If you send the script to us
    AB> we can probably spot the problem for you.

    AB> As Rolf mentioned in his email, spatstat provides a
    AB> command "envelope" to compute simulation envelopes. This
    AB> might be sufficient for your needs.

    AB> regards
    AB> Adrian Baddeley

Regards,
Martin Maechler



From Allan at STATS.uct.ac.za  Tue Sep  6 11:55:24 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Tue, 06 Sep 2005 11:55:24 +0200
Subject: [R] r: chinese installation of r
Message-ID: <431D678C.BB3B9562@STATS.uct.ac.za>

can any one help:

A friends query:
"My pc is using the chinese version windows xp, so when I installed R
Chinese was 
automatically selected as the default language.How can I change it? It
brings a lot of 
trouble since some of the output is in chinese too."

From 0034058 at fudan.edu.cn  Tue Sep  6 11:53:55 2005
From: 0034058 at fudan.edu.cn (0034058@fudan.edu.cn)
Date: Tue, 06 Sep 2005 17:53:55 +0800
Subject: [R] : r: chinese installation of r
Message-ID: <1c471cd1c4a5af.1c4a5af1c471cd@fudan.edu.cn>

In the "selection components" step of installation,uncheck 
the "message translations".Then it will be OK!

----- Ô­ÓÊ¼þ -----
´Ó: Clark Allan <Allan at stats.uct.ac.za>
ÈÕÆÚ: ÐÇÆÚ¶þ, ¾ÅÔÂ 6ÈÕ, 2005 ÏÂÎç5:55
Ö÷Ìâ: [R] r: chinese installation of r
> can any one help:
> 
> A friends query:
> "My pc is using the chinese version windows xp, so when I 
> installed R
> Chinese was 
> automatically selected as the default language.How can I change 
> it? It
> brings a lot of 
> trouble since some of the output is in chinese too."
-------------- next part --------------
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

From thpe at hhbio.wasser.tu-dresden.de  Tue Sep  6 12:11:31 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Tue, 06 Sep 2005 12:11:31 +0200
Subject: [R] r: chinese installation of r
In-Reply-To: <431D678C.BB3B9562@STATS.uct.ac.za>
References: <431D678C.BB3B9562@STATS.uct.ac.za>
Message-ID: <431D6B53.8040100@hhbio.wasser.tu-dresden.de>

Clark Allan schrieb:
> can any one help:
> 
> A friends query:
> "My pc is using the chinese version windows xp, so when I installed R
> Chinese was 
> automatically selected as the default language.How can I change it? It
> brings a lot of 
> trouble since some of the output is in chinese too."


The R admin manual

http://cran.r-project.org/doc/manuals/R-admin.html

says:

"The preferred language for messages is by default taken from the 
locale. This can be overridden first by the setting of the environment 
variable LANGUAGE and then by the environment variables LC_ALL, 
LC_MESSAGES and LANG. (The last three are normally used to set the 
locale and so should not be needed, but the first is only used to select 
the language for messages.) The code tries hard to map locale names to 
languages, even on Windows.

Note that you should not expect to be able to change the language once R 
is running. "

If your system runs on Windows, define a variable LANGUAGE in the 
systems settings (environment) and set it to EN.

Hope it helps

Thomas Petzoldt



From Allan at STATS.uct.ac.za  Tue Sep  6 12:53:46 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Tue, 06 Sep 2005 12:53:46 +0200
Subject: [R] R: optim
Message-ID: <431D753A.5C2D2AF3@STATS.uct.ac.za>

hi all

i dont understand the error message that is produced by the optim
function. can anybody help???

ie: 
[[1]]$message
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"

can anyone help?



###########################################################################

SK.FIT(XDATA=a,XDATAname="a",PHI1=1,v=5,vlo=2,vhi=300,phi2lo=.01)
[[1]]
[[1]]$par
[1]  -0.01377906   0.83859445   0.34675230 300.00000000

[[1]]$value
[1] 90.59185

[[1]]$counts
function gradient 
      53       53 

[[1]]$convergence
[1] 0

[[1]]$message
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"

#################################################################################



i ghave included the function used in the optim call:

SKEWMLE=function(l,DATA=XDATA,...)
	{
		#alpha = l[1]
		#beta = l[2]
		#phi2 = l[3]
		#v= l[4]
		phi1=PHI1

		DATA<-as.matrix(DATA)
	
		fnew<-function(x,y,l,...)
		{
			#when we do not estimate phi1
			t1=(1+((y-l[1]-l[2]*x)^2)/(l[4]*l[3]^2))^(-0.5*(1+l[4]))
			t2=(1+(x^2)/l[4])^(-0.5*(1+l[4]))
			t3=2*((gamma(0.5*(1+l[4]))/(gamma(0.5*l[4])*sqrt(l[4]*pi)))^2)/l[3]

			t1*t2*t3
		}

		a<-double(length(DATA))
		y=DATA
		a=apply(y,1,function(q)
log(integrate(fnew,lower=0,upper=Inf,y=q,l=l)$value))
		-sum(a)
	}

From chrysopa at gmail.com  Tue Sep  6 14:00:27 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Tue, 6 Sep 2005 09:00:27 -0300
Subject: [R] Doubt about nested aov output
In-Reply-To: <431B83DE.5060605@pdf.com>
References: <200508301101.32212.chrysopa@gmail.com> <431B83DE.5060605@pdf.com>
Message-ID: <200509060900.27991.chrysopa@gmail.com>

Hi Spencer,

Em Dom 04 Set 2005 20:31, Spencer Graves escreveu:
> 	  Others may know the answer to your question, but I don't.  However,
> since I have not seen a reply, I will offer a few comments:
>
> 	  1.  What version of R are you using?  I just tried superficially
> similar things with the examples in ?aov in R 2.1.1 patched and
> consistently got F and p values.

I'm using the R version 2.1.1 on Linux Debian
Version 2.1.1  (2005-06-20), ISBN 3-900051-07-0

> 	  2.  My preference for this kind of thing is to use lme in
> library(nlme) or lmer in library(lme4).  Also, I highly recommend
> Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).

Yes, this is my preference too, but I need aov for classes.

> 	  3.  If still want to use aov and are getting this problem in R 2.1.1,
> could you please provide this list with a small, self contained example
> that displays the symptoms that concern you?  And PLEASE do read the
> posting guide! "http://www.R-project.org/posting-guide.html".  It might
> increase the speed and utility of replies.
>
> 	  spencer graves

I send the complete example. This is a example from the Crwaley's book 
(Statistical Computing: An introdution to data analysis using S-Plus.

This is a classical experiment to show pseudoreplication, from Sokal and Rohlf 
(1995).

In this experiments, It have 3 treatmens applied to 6 rats, for each rat it  
make 3 liver preparation and for each liver it make 2 readings of glycogen. 
This generated 6 pseudoreplication per rat. I'm interested on the effect os 
treatment on the glycogen readings.

Look the R analyses:

--------------------
> Glycogen <- 
c(131,130,131,125,136,142,150,148,140,143,160,150,157,145,154,142,147,153,151,155,147,147,162,152,134,125,138,138,135,136,138,140,139,138,134,127)
> Glycogen
 [1] 131 130 131 125 136 142 150 148 140 143 160 150 157 145 154 142 147 153 
151
[20] 155 147 147 162 152 134 125 138 138 135 136 138 140 139 138 134 127
> Treatment <- factor(rep(c(1,2,3),c(12,12,12)))
> Treatment
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3
Levels: 1 2 3
> Rat <- factor(rep(rep(c(1,2),c(6,6)),3))
> Rat
 [1] 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 2 2 2 2 2 2
Levels: 1 2
> Liver <- factor(rep(rep(c(1,2,3),c(2,2,2)),6))
> Liver
 [1] 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3
Levels: 1 2 3
> 
> ### Model made identical to the book
> 
> model <- aov(Glycogen~Treatment/Rat/Liver+Error(Treatment/Rat/Liver))
> 
> summary(model)

Error: Treatment
          Df  Sum Sq Mean Sq
Treatment  2 1557.56  778.78

Error: Treatment:Rat
              Df Sum Sq Mean Sq
Treatment:Rat  3 797.67  265.89

Error: Treatment:Rat:Liver
                    Df Sum Sq Mean Sq
Treatment:Rat:Liver 12  594.0    49.5

Error: Within
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals 18 381.00   21.17               
> 
> ### Model made by myself, I'm interested only in Treatment effects
> 
> model <- aov(Glycogen~Treatment+Error(Treatment/Rat/Liver))
> 
> summary(model)

Error: Treatment
          Df  Sum Sq Mean Sq
Treatment  2 1557.56  778.78

Error: Treatment:Rat
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals  3 797.67  265.89               

Error: Treatment:Rat:Liver
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals 12  594.0    49.5               

Error: Within
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals 18 381.00   21.17               
--------------------

What it dont calculate the F and P for treatment?

Thanks
Ronaldo

-- 
Tristezas n??o pagam d??vidas. Nem bravatas, por
falar nisso.

--Mill??r Fernandes
Retirado de http://www.uol.com.br/millor
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From murdoch at stats.uwo.ca  Tue Sep  6 14:07:23 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 06 Sep 2005 08:07:23 -0400
Subject: [R] The Perils of PowerPoint
In-Reply-To: <4702645135092E4497088F71D9C8F51A128C13@afhex01.dpi.wa.gov.au>
References: <4702645135092E4497088F71D9C8F51A128C13@afhex01.dpi.wa.gov.au>
Message-ID: <431D867B.2000308@stats.uwo.ca>

Mulholland, Tom wrote:
> For some reason (probably that our organisation has blocked the site) I could not see the original articles that prompted the post. I however immediately assumed that this was precipitated by Tufte and his comments about PowerPoint (I recall seeing a good example of PowerPoint on his site) http://www.edwardtufte.com/tufte/powerpoint
> 
> When this first came up I recall some dispute about the comments www.sociablemedia.com/articles_dispute.htm and that John Fox did something 
 >http://ils.unc.edu/~jfox/powerpoint/introduction.html that I enjoyed 
reading.

I think that's by a different Fox named Jackson, not John.  It's an 
interesting reading, though.

Duncan Murdoch
> 
> Other links that are lying on my computer are
> "In defense of PowerPoint" http://www.jnd.org/dn.mss/in_defense_of_powerp.html
> and "Does PowerPoint make you stupid?" at http://www.presentations.com/presentations/delivery/article_display.jsp?vnu_content_id=1000482464
>  
> Tom
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Tim Churches
>>Sent: Saturday, 3 September 2005 10:08 AM
>>To: ted.harding at nessie.mcc.ac.uk
>>Cc: Achim Zeileis; r-help at stat.math.ethz.ch
>>Subject: Re: [R] The Perils of PowerPoint
>>
>>
>>(Ted Harding) wrote:
>>
>>
>>>By the way, the Washington Post/Minneapolis Star Tribune article is
>>>somewhat reminiscent of a short (15 min) broadcast on BBC Radio 4
>>>back on October 18 2004 15:45-16:00 called
>>>
>>> "Microsoft Powerpoint and the Decline of Civilisation"
>>>
>>>which explores similar themes and also frequently quotes Tufte.
>>>Unfortunately it lapsed for ever from "Listen Again" after the
>>>statutory week, so I can't point you to a replay. (However, I
>>>have carefully preserved the cassette recording I made).
>>> 
>>>
>>
>>Try http://sooper.org/misc/powerpoint.mp3 (copyright law 
>>notwithstanding...)
>>
>>Tim C
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Tue Sep  6 14:35:44 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 6 Sep 2005 07:35:44 -0500
Subject: [R] R: optim
In-Reply-To: <431D753A.5C2D2AF3@STATS.uct.ac.za>
References: <431D753A.5C2D2AF3@STATS.uct.ac.za>
Message-ID: <40e66e0b050906053591b35dd@mail.gmail.com>

On 9/6/05, Clark Allan <Allan at stats.uct.ac.za> wrote:
> hi all
> 
> i dont understand the error message that is produced by the optim
> function. can anybody help???
> 
> ie:
> [[1]]$message
> [1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
> 
> can anyone help?

That code indicates that the optimizer has declared convergence
because the relative reduction in the objective function in successive
iterates is below a tolerance.  As documented in ?optim, a convergence
code of 0 indicates success

...
convergence: An integer code. '0' indicates successful convergence.
          Error codes are
...

This may be counter-intuitive but it does make sense to shell
programmers.  The idea is that there is only one way you can succeed
but there are many different ways of failing so you use the nonzero
codes to indicate the types of failure and the zero code, which we
usually read as FALSE in a logical context, to indicate success.

> 
> 
> 
> ###########################################################################
> 
> SK.FIT(XDATA=a,XDATAname="a",PHI1=1,v=5,vlo=2,vhi=300,phi2lo=.01)
> [[1]]
> [[1]]$par
> [1]  -0.01377906   0.83859445   0.34675230 300.00000000
> 
> [[1]]$value
> [1] 90.59185
> 
> [[1]]$counts
> function gradient
>       53       53
> 
> [[1]]$convergence
> [1] 0
> 
> [[1]]$message
> [1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
> 
> #################################################################################
> 
> 
> 
> i ghave included the function used in the optim call:
> 
> SKEWMLE=function(l,DATA=XDATA,...)
>         {
>                 #alpha = l[1]
>                 #beta = l[2]
>                 #phi2 = l[3]
>                 #v= l[4]
>                 phi1=PHI1
> 
>                 DATA<-as.matrix(DATA)
> 
>                 fnew<-function(x,y,l,...)
>                 {
>                         #when we do not estimate phi1
>                         t1=(1+((y-l[1]-l[2]*x)^2)/(l[4]*l[3]^2))^(-0.5*(1+l[4]))
>                         t2=(1+(x^2)/l[4])^(-0.5*(1+l[4]))
>                         t3=2*((gamma(0.5*(1+l[4]))/(gamma(0.5*l[4])*sqrt(l[4]*pi)))^2)/l[3]
> 
>                         t1*t2*t3
>                 }
> 
>                 a<-double(length(DATA))
>                 y=DATA
>                 a=apply(y,1,function(q)
> log(integrate(fnew,lower=0,upper=Inf,y=q,l=l)$value))
>                 -sum(a)
>         }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From RRoa at fisheries.gov.fk  Tue Sep  6 13:20:24 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Tue, 6 Sep 2005 09:20:24 -0200
Subject: [R] model selection vs. H0 based testing
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC43D@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Thomas Petzoldt
> Sent: 06 September 2005 06:34
> Cc: petzoldt at rcs.urz.tu-dresden.de; R-Help
> Subject: Re: [R] model selection vs. H0 based testing
> 
> 
> Hello,
> 
> I wish to thank Douglas Bates very much for clarification and 
> pointing me to the MCMC simulation method to get p values even for cases where 
> Wald tests are inappropriate.
> 
> One question however remains when publishing statistical 
> results: does it help readers if we combine both,
> 
> - AIC based model selection
> *and*
> - null hypothesis based tests statistics
> 
> or should we focus on model selection only and try to reduce 
> the amount of tables provided?

IMHO the AIC is sufficient and the null hypothesis test is
not well suited to the problem. As stated by Akaike (1974,
A new look at the statistical model identification, IEEE
Transactions on Automatic Control 19:716-723):"As was noticed by
Lehman [this is the classic book on the Neyman-Pearson theory of
hypothesis testing], hypothesis testing procedures are traditionally
applied to the situation where actually multiple decision procedures
are required. If the statistical identification procedure is considered
as a decision procedure the very basic problem is the appropriate choice
of the loss function. In the Neyman-Pearson theory of statistical
hypothesis testing only the probabilities of rejecting and accepting
the correct and incorrect hypothesis, respectively, are considered
to define the loss caused by the decision. In practical situations
the assumed null hypotheses are only approximations and they
are almost always different from the reality. Thus the choice of the
loss function in the test theory makes its practical application
logically contradictory. The recongnition of this point that the
hypothesis testing procedure is not adequately formulated as a 
procedure of approximation is very important for the development
of practically useful identification procedures."
Note that Akaike speaks of 'model identification' whereas now this
subject are is usually referred to as 'model selection'.
Ruben



From tolga.uzuner at csfb.com  Tue Sep  6 15:33:07 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Tue, 6 Sep 2005 14:33:07 +0100
Subject: [R] help.search problem
Message-ID: <BDF571786CAD224F966FEB86BEDED52F1433E448@elon12p32001.csfp.co.uk>

Dear Fellow R Users,

I have recently come across a weird problem with help.search:

> help.search("tps")
Error in rbind(...) : number of columns of matrices must match (see arg 8)
> 

This happens no matter what I search for...

Any thoughts ?
Thanks,
Tolga

Please follow the attached hyperlink to an important disclaimer
<http://www.csfb.com/legal_terms/disclaimer_europe.shtml>



==============================================================================
Please access the attached hyperlink for an important electronic communications disclaimer: 

http://www.csfb.com/legal_terms/disclaimer_external_email.shtml



From riedwyl at giub.unibe.ch  Tue Sep  6 15:39:20 2005
From: riedwyl at giub.unibe.ch (Nadja Riedwyl)
Date: Tue, 6 Sep 2005 15:39:20 +0200
Subject: [R] fitting distributions with R
Message-ID: <200509061539.20358.riedwyl@giub.unibe.ch>

Dear all
I've got the dataset
data:2743;4678;21427;6194;10286;1505;12811;2161;6853;2625;14542;694;11491;
?? ?? ?? ?? ?? 14924;28640;17097;2136;5308;3477;91301;11488;3860;64114;14334
I know from other testing that it should be possible to fit the data with the 
exponentialdistribution. I tried to get parameterestimates for the 
exponentialdistribution with R, but as the values 
of the parameter are very close to 0 i get into troubles. Do you know, what i 
could do in order to get estimates?How do you choose the starting values? in 
my opinion it should be around 1/mean(data).

 
#Parameterestimation ??with mle() with the log-likelihood funktion of the ??
#exponentialdistribution
library(stats4) 
ll<-function(beta) 
{n<-24 
x<-data2
-n*log(beta)+beta*sum(x)} 
est<-mle(minuslog=ll, start=list(beta=0.1))
summary(est) 

#instead of a result, i get:


Error in optim(start, f, method = method, hessian = TRUE, ...) :
?? ?? ?? ?? non-finite finite-difference value [1]
In addition: There were 50 or more warnings (use warnings() to see the first 
50)
#with fitdistr() for the exponentialdistribution
library(MASS)
fitdistr(data2,densfun=dexp,start=list(rate=0.1),lower=6e-06,method="BFGS")

#instead of a result, i get

Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
?? ?? ?? ?? non-finite finite-difference value [1]
In addition: Warning messages:
1: bounds can only be used with method L-BFGS-B in: optim(start, mylogfn, x = 
x, hessian = TRUE, ...)
2: NaNs produced in: dexp(x, 1/rate, log)


i'll be very happy for any help i can get to solve this problem
thank you!



From reid_huntsinger at merck.com  Tue Sep  6 16:00:08 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 6 Sep 2005 10:00:08 -0400
Subject: [R] fitting distributions with R
Message-ID: <355C35514FEAC9458F75947F5270974D076CAA@usctmx1103.merck.com>

The MLE of beta is the reciprocal of the sample mean, so you don't need an
optimizer here. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nadja Riedwyl
Sent: Tuesday, September 06, 2005 9:39 AM
To: r-help at stat.math.ethz.ch
Subject: [R] fitting distributions with R


Dear all
I've got the dataset
data:2743;4678;21427;6194;10286;1505;12811;2161;6853;2625;14542;694;11491;
?? ?? ?? ?? ?? 14924;28640;17097;2136;5308;3477;91301;11488;3860;64114;14334
I know from other testing that it should be possible to fit the data with
the 
exponentialdistribution. I tried to get parameterestimates for the 
exponentialdistribution with R, but as the values 
of the parameter are very close to 0 i get into troubles. Do you know, what
i 
could do in order to get estimates?How do you choose the starting values? in

my opinion it should be around 1/mean(data).

 
#Parameterestimation ??with mle() with the log-likelihood funktion of the ??
#exponentialdistribution
library(stats4) 
ll<-function(beta) 
{n<-24 
x<-data2
-n*log(beta)+beta*sum(x)} 
est<-mle(minuslog=ll, start=list(beta=0.1))
summary(est) 

#instead of a result, i get:


Error in optim(start, f, method = method, hessian = TRUE, ...) :
?? ?? ?? ?? non-finite finite-difference value [1]
In addition: There were 50 or more warnings (use warnings() to see the first

50)
#with fitdistr() for the exponentialdistribution
library(MASS)
fitdistr(data2,densfun=dexp,start=list(rate=0.1),lower=6e-06,method="BFGS")

#instead of a result, i get

Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
?? ?? ?? ?? non-finite finite-difference value [1]
In addition: Warning messages:
1: bounds can only be used with method L-BFGS-B in: optim(start, mylogfn, x
= 
x, hessian = TRUE, ...)
2: NaNs produced in: dexp(x, 1/rate, log)


i'll be very happy for any help i can get to solve this problem
thank you!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Tue Sep  6 15:53:10 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 06 Sep 2005 14:53:10 +0100 (BST)
Subject: [R] The Perils of PowerPoint
In-Reply-To: <4702645135092E4497088F71D9C8F51A128C13@afhex01.dpi.wa.gov.au>
Message-ID: <XFMail.050906145310.Ted.Harding@nessie.mcc.ac.uk>

On 06-Sep-05 Mulholland, Tom wrote:
> For some reason (probably that our organisation has blocked the site) I
> could not see the original articles that prompted the post. I however
> immediately assumed that this was precipitated by Tufte and his
> comments about PowerPoint (I recall seeing a good example of PowerPoint
> on his site) http://www.edwardtufte.com/tufte/powerpoint
> 
> When this first came up I recall some dispute about the comments
> www.sociablemedia.com/articles_dispute.htm and that John Fox did
> something http://ils.unc.edu/~jfox/powerpoint/introduction.html that I
> enjoyed reading.
> 
> Other links that are lying on my computer are
> "In defense of PowerPoint"
> http://www.jnd.org/dn.mss/in_defense_of_powerp.html
> and "Does PowerPoint make you stupid?" at
http://www.presentations.com/presentations/delivery/
article_display.jsp?vnu_content_id=1000482464
>  
> Tom

Thanks, Tom, for these pointers to interesting discussions!
One must of course agree with the general comments to the effect
that the quality and merits of a presentation are the result of
choices made by the person who designed it, and not primarily due
to the software itself. It is also true that software such as
PowerPoint provides ready-made mechanisms for linking-in a great
variety of content, thereby making it -- in principle -- easier
for the designer to choose judiciously what would be best for the
result they wish to achieve and -- in principle -- to design an
outstanding presentation.

It is nevertheless still true that in practice the result is often
dreadful, for reasons which largely reside in the software (but
which take effect by virtue of user deficiency).

I tend to put this down to the provision of so-called "Wizards"
-- in reality electronic snake-oil merchants -- the protoype of
which is the dancing paper-clip masquerading as an "Office
Assistant". There are other "resources" which can have similar
effects -- "spell-checkers", "grammar-checkers", auto-formatters
which brush you aside and re-arrange your intentions and which
can be difficult to evade: indeed, one can form the impression
that it has been deliberately made difficult for users to ignore
these things and make their own choices.

In case you may wonder how I hope to bring this On-Topic, it is
as follows. The result of such things is that users' thought
and practice become software-led and software-driven. The software
is both carrot and stick. The user is the donkey.

In contrast, as software and in its implementation as a compendium
of resources and documentation, R expects users to know what they
are doing and to understand the rationale of the methods. R also
requires users to have the capability to locate necessary inforamtion
in the documentation. Indeed, one might even describe R documentation
as notoriously unintrusive!

So using R should educate users in thoughtful and judicious use of
statistical software. The same cannot be said so wholeheartedly of
S-Plus. While the latter is basically routine-equivalent to R, and
the help and menu systems properly used can also encourage judicious
use, there is nevertheless a superficial aspect which can seduce users
into a "check-box" mentality; and the printed manuals strike me as
both unclear and unduly prescriptive.

In other words, while S-Plus may tend to attract users who do not
know what to do and who expect the softare to tell them what to do
(and subsequently will not know what they have done), R will not.

This spartan environment is lean and healthy, so successful R users
will become lean and healthy! Not donkeys, but mountain-goats.
R-help is there for those who need it, and very few responses to
queries have been at all superficial. Often it is clear that
respondents themselves have had to think before being able to come
up with an answer, and very often the response urges the questioner
to think! Indeed, evidence of thought on the part of the questioner
is something of a pre-requisite for getting a response.

The underlying thought behind all this is that there is something
of an under-current of disquiet in the statistical community about
"software-driven analysis", an increasingly prevalent abuse of our
subject. Occasionally it comes to the surface. Crass abuses such
as are encouraged by PowerPoint snake-oil and the like are obvious;
but once we perceive them we can be sensitised to similar but more
subtle dangers in other software. Conscious remedial effort would
be a good thing, and R seems to be an excellent vehicle for it.

Thanks for reading so far!

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Sep-05                                       Time: 14:29:26
------------------------------ XFMail ------------------------------



From hb at maths.lth.se  Tue Sep  6 16:29:15 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 06 Sep 2005 16:29:15 +0200
Subject: [R] help.search problem
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433E448@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433E448@elon12p32001.csfp.co.uk>
Message-ID: <431DA7BB.4060705@maths.lth.se>

What version of R and what operating system?  What packages do you have 
loaded?

Try utils::help.search("tps"), does that work? Have you tried it in a 
fresh R session, i.e. start with R --vanilla.

If you can't get it to work after this, report the above information 
plus what you get from traceback() after you get the error.

Cheers

Henrik

Uzuner, Tolga wrote:
> Dear Fellow R Users,
> 
> I have recently come across a weird problem with help.search:
> 
> 
>>help.search("tps")
> 
> Error in rbind(...) : number of columns of matrices must match (see arg 8)
> 
> 
> This happens no matter what I search for...
> 
> Any thoughts ?
> Thanks,
> Tolga
> 
> Please follow the attached hyperlink to an important disclaimer
> <http://www.csfb.com/legal_terms/disclaimer_europe.shtml>
> 
> 
> 
> ==============================================================================
> Please access the attached hyperlink for an important electronic communications disclaimer: 
> 
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From dmbates at gmail.com  Tue Sep  6 16:47:46 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 6 Sep 2005 09:47:46 -0500
Subject: [R] Doubt about nested aov output
In-Reply-To: <200509060900.27991.chrysopa@gmail.com>
References: <200508301101.32212.chrysopa@gmail.com> <431B83DE.5060605@pdf.com>
	<200509060900.27991.chrysopa@gmail.com>
Message-ID: <40e66e0b050906074719835158@mail.gmail.com>

On 9/6/05, Ronaldo Reis-Jr. <chrysopa at gmail.com> wrote:
> Hi Spencer,
> 
> Em Dom 04 Set 2005 20:31, Spencer Graves escreveu:
> >         Others may know the answer to your question, but I don't.  However,
> > since I have not seen a reply, I will offer a few comments:
> >
> >         1.  What version of R are you using?  I just tried superficially
> > similar things with the examples in ?aov in R 2.1.1 patched and
> > consistently got F and p values.
> 
> I'm using the R version 2.1.1 on Linux Debian
> Version 2.1.1  (2005-06-20), ISBN 3-900051-07-0
> 
> >         2.  My preference for this kind of thing is to use lme in
> > library(nlme) or lmer in library(lme4).  Also, I highly recommend
> > Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).
> 
> Yes, this is my preference too, but I need aov for classes.
> 
> >         3.  If still want to use aov and are getting this problem in R 2.1.1,
> > could you please provide this list with a small, self contained example
> > that displays the symptoms that concern you?  And PLEASE do read the
> > posting guide! "http://www.R-project.org/posting-guide.html".  It might
> > increase the speed and utility of replies.
> >
> >         spencer graves
> 
> I send the complete example. This is a example from the Crwaley's book
> (Statistical Computing: An introdution to data analysis using S-Plus.
> 
> This is a classical experiment to show pseudoreplication, from Sokal and Rohlf
> (1995).
> 
> In this experiments, It have 3 treatmens applied to 6 rats, for each rat it
> make 3 liver preparation and for each liver it make 2 readings of glycogen.
> This generated 6 pseudoreplication per rat. I'm interested on the effect os
> treatment on the glycogen readings.
> 
> Look the R analyses:
> 
> --------------------
> > Glycogen <-
> c(131,130,131,125,136,142,150,148,140,143,160,150,157,145,154,142,147,153,151,155,147,147,162,152,134,125,138,138,135,136,138,140,139,138,134,127)
> > Glycogen
>  [1] 131 130 131 125 136 142 150 148 140 143 160 150 157 145 154 142 147 153
> 151
> [20] 155 147 147 162 152 134 125 138 138 135 136 138 140 139 138 134 127
> > Treatment <- factor(rep(c(1,2,3),c(12,12,12)))
> > Treatment
>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3
> Levels: 1 2 3
> > Rat <- factor(rep(rep(c(1,2),c(6,6)),3))
> > Rat
>  [1] 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 2 2 2 2 2 2
> Levels: 1 2
> > Liver <- factor(rep(rep(c(1,2,3),c(2,2,2)),6))
> > Liver
>  [1] 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3
> Levels: 1 2 3
> >
> > ### Model made identical to the book
> >
> > model <- aov(Glycogen~Treatment/Rat/Liver+Error(Treatment/Rat/Liver))
> >
> > summary(model)
> 
> Error: Treatment
>           Df  Sum Sq Mean Sq
> Treatment  2 1557.56  778.78
> 
> Error: Treatment:Rat
>               Df Sum Sq Mean Sq
> Treatment:Rat  3 797.67  265.89
> 
> Error: Treatment:Rat:Liver
>                     Df Sum Sq Mean Sq
> Treatment:Rat:Liver 12  594.0    49.5
> 
> Error: Within
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 18 381.00   21.17
> >
> > ### Model made by myself, I'm interested only in Treatment effects
> >
> > model <- aov(Glycogen~Treatment+Error(Treatment/Rat/Liver))
> >
> > summary(model)
> 
> Error: Treatment
>           Df  Sum Sq Mean Sq
> Treatment  2 1557.56  778.78
> 
> Error: Treatment:Rat
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  3 797.67  265.89
> 
> Error: Treatment:Rat:Liver
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 12  594.0    49.5
> 
> Error: Within
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 18 381.00   21.17
> --------------------
> 
> What it dont calculate the F and P for treatment?

Would it be easier to do it this way?

> library(lme4)
Loading required package: Matrix
Loading required package: lattice
> (fm1 <- lmer(Glycogen ~ Treatment + (1|Treatment:Rat) + (1|Treatment:Rat:Liver)))
Linear mixed-effects model fit by REML
Formula: Glycogen ~ Treatment + (1 | Treatment:Rat) + (1 | Treatment:Rat:Liver) 
      AIC      BIC    logLik MLdeviance REMLdeviance
 231.6213 241.1224 -109.8106    234.297     219.6213
Random effects:
 Groups              Name        Variance Std.Dev.
 Treatment:Rat:Liver (Intercept) 14.167   3.7639  
 Treatment:Rat       (Intercept) 36.065   6.0054  
 Residual                        21.167   4.6007  
# of obs: 36, groups: Treatment:Rat:Liver, 18; Treatment:Rat, 6

Fixed effects:
            Estimate Std. Error DF t value Pr(>|t|)    
(Intercept) 140.5000     4.7072 33 29.8481   <2e-16
Treatment2   10.5000     6.6569 33  1.5773   0.1243    
Treatment3   -5.3333     6.6569 33 -0.8012   0.4288    
> anova(fm1)
Analysis of Variance Table
          Df  Sum Sq Mean Sq   Denom F value  Pr(>F)  
Treatment  2 123.993  61.996  33.000   2.929 0.06746

The degrees of freedom for the denominator are an upper bound (in this
case a rather gross upper bound) so the p-value is a lower bound.  It
is on my "To Do" list to improve tthis but I have a rather long "To
Do" list.



From jsorkin at grecc.umaryland.edu  Tue Sep  6 17:19:51 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 06 Sep 2005 11:19:51 -0400
Subject: [R] The Perils of PowerPoint
Message-ID: <s31d7b6e.051@grecc.umaryland.edu>

Please, do not blame PowerPoint for a poorly prepared or delivered talk.
Blame the person who developed the presentation and the person who
delivered the talk. PowerPoint is a tool. It can use used well or it can
be used poorly. If I may quote a once popular newspaper cartoon
character, Pogo, "We Have Met The Enemy and He Is Us".
John 

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu

>>> "Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au> 09/06 2:26 AM >>>
For some reason (probably that our organisation has blocked the site) I
could not see the original articles that prompted the post. I however
immediately assumed that this was precipitated by Tufte and his comments
about PowerPoint (I recall seeing a good example of PowerPoint on his
site) http://www.edwardtufte.com/tufte/powerpoint 

When this first came up I recall some dispute about the comments
www.sociablemedia.com/articles_dispute.htm and that John Fox did
something http://ils.unc.edu/~jfox/powerpoint/introduction.html that I
enjoyed reading.

Other links that are lying on my computer are
"In defense of PowerPoint"
http://www.jnd.org/dn.mss/in_defense_of_powerp.html 
and "Does PowerPoint make you stupid?" at
http://www.presentations.com/presentations/delivery/article_display.jsp?vnu_content_id=1000482464

 
Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Tim Churches
> Sent: Saturday, 3 September 2005 10:08 AM
> To: ted.harding at nessie.mcc.ac.uk 
> Cc: Achim Zeileis; r-help at stat.math.ethz.ch 
> Subject: Re: [R] The Perils of PowerPoint
> 
> 
> (Ted Harding) wrote:
> 
> >By the way, the Washington Post/Minneapolis Star Tribune article is
> >somewhat reminiscent of a short (15 min) broadcast on BBC Radio 4
> >back on October 18 2004 15:45-16:00 called
> >
> >  "Microsoft Powerpoint and the Decline of Civilisation"
> >
> >which explores similar themes and also frequently quotes Tufte.
> >Unfortunately it lapsed for ever from "Listen Again" after the
> >statutory week, so I can't point you to a replay. (However, I
> >have carefully preserved the cassette recording I made).
> >  
> >
> Try http://sooper.org/misc/powerpoint.mp3 (copyright law 
> notwithstanding...)
> 
> Tim C
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tolga.uzuner at csfb.com  Tue Sep  6 17:35:53 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Tue, 6 Sep 2005 16:35:53 +0100
Subject: [R] help.search problem
Message-ID: <BDF571786CAD224F966FEB86BEDED52F1433E451@elon12p32001.csfp.co.uk>

Hi there,

I am using 2.0.1 . However, I was not having this problem with this version of R when I first installed it and started using it.

Thanks for your suggestion, I tried it, but that doesn't work either:

> help.search("tps")
Error in rbind(...) : number of columns of matrices must match (see arg 8)
> utils::help.search("tps")
Error in rbind(...) : number of columns of matrices must match (see arg 8)
> 

Traceback results below:

    "Convert Sweave Syntax", "Sweave Driver Utilities", "Find Objects by (Partial) Name", 
       "Browse Objects in Environment", "Load URL into a WWW Browser", 
       "Send a Bug Report", "Send output to a character string or file", 
       "Writing Package CITATION Files", "Citing R and R Packages in Publications", 
       "Close a Socket", "Compare Two Package Version Numbers", 
       "Data Sets", "Spreadsheet Interface for Entering Data", "Post-Mortem Debugging", 
       "Demonstrations of R Functionality", "Download File from the Internet", 
       "Invoke a Text Editor", "Edit Data Frames and Matrices", 
       "Run an Examples Section from the Online Help", "Edit One or More Files", 
       "Fix an Object", "Retrieve an R Object, Including from a Namespace", 
       "Utility functions for Developing Namespaces", "Get An S3 Method", 
       "Return the First or Last Part of an Object", "Documentation", 
       "Search the Help System", "Hypertext Documentation", "Search Indices for Help Files", 
       "Find Installed Packages", "List Objects and their Structure", 
       "Create a Socket Connection", "Menu Interaction Function", 
       "List Methods for S3 Generic Functions or Classes", "Report the Space Allocated for an Object", 
       "Create a skeleton for a new package", "Package Description", 
       "Package Management Tools", "Invoke a Pager on an R Object", 
       "Person Names and Contact Information", "Produce Prototype of an R Documentation File", 
       "Generate a Shell for Documentation of Data Sets", "Read fixed-format data", 
       "Read Fixed Width Format Files", "Read from or Write to a Socket", 
       "Browsing after an Error", "Remove Installed Packages", "Load or Save or Display the Commands History", 
       "Collect Information About the Current R Session", "Compactly Display the Structure of an Arbitrary R Object", 
       "Summarise Output of R Profiler", "Converting R Objects to BibTeX or LaTeX", 
       "Download Packages from CRAN", "Display a text URL", "Defunct Functions in Package utils", 
       "Deprecated Functions in Package utils", "View or List Vignettes", 
       "Batch Execution of R", "DLL Version Information", "Install Add-on Packages from Sources", 
       "Remove Add-on Packages", "R for Windows Configuration", 
       "Build a DLL for Dynamic Loading", "Choose a List of Files Interactively", 
       "Read/Write Text to/from the Windows Clipboard", "Get a Windows Handle", 
       "Update HTML documentation files", "Report on Memory Allocation", 
       "Select Items from a List", "Set or get the Window Title", 
       "Dialog Boxes under Windows", "User Menus under Windows", 
       "Auxiliary Functions for the Windows Port", "build", "Rprof", 
       "Rtangle", "RweaveLatex", "Sweave", "SweaveSyntConv", "SweaveUtils", 
       "apropos", "browseEnv", "browseURL", "bug.report", "capture.output", 
       "citEntry", "citation", "close.socket", "compareVersion", 
       "data", "data.entry", "debugger", "demo", "download.file", 
       "edit", "edit.data.frame", "example", "file.edit", "fix", 
       "getAnywhere", "assignInNamespace", "getS3method", "head", 
       "help", "help.search", "help.start", "index.search", "installed.packages", 
       "ls.str", "make.socket", "menu", "methods", "object.size", 
       "package.skeleton", "packageDescription", "packageStatus", 
       "page", "person", "prompt", "promptData", "read.fortran", 
       "read.fwf", "read.socket", "recover", "remove.packages", 
       "loadhistory", "sessionInfo", "str", "summaryRprof", "toLatex", 
       "update.packages", "url.show", "utils-defunct", "utils-deprecated", 
       "vignette", "BATCH", "DLL.version", "INSTALL", "REMOVE", 
       "Rconsole", "SHLIB", "choose.files", "readClipboard", "getWindowsHandle", 
       "link.html.help", "memory.size", "select.list", "setWindowTitle", 
       "winDialog", "winMenuAdd", "flush.console"))
2: do.call("rbind", dbMat[, 1])
1: utils::help.search("tps")
> 





Please follow the attached hyperlink to an important disclaimer
http://www.csfb.com/legal_terms/disclaimer_europe.shtml



-----Original Message-----
From: Henrik Bengtsson [mailto:hb at maths.lth.se]
Sent: 06 September 2005 15:29
To: Uzuner, Tolga
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] help.search problem


What version of R and what operating system?  What packages do you have 
loaded?

Try utils::help.search("tps"), does that work? Have you tried it in a 
fresh R session, i.e. start with R --vanilla.

If you can't get it to work after this, report the above information 
plus what you get from traceback() after you get the error.

Cheers

Henrik

Uzuner, Tolga wrote:
> Dear Fellow R Users,
> 
> I have recently come across a weird problem with help.search:
> 
> 
>>help.search("tps")
> 
> Error in rbind(...) : number of columns of matrices must match (see arg 8)
> 
> 
> This happens no matter what I search for...
> 
> Any thoughts ?
> Thanks,
> Tolga
> 
> Please follow the attached hyperlink to an important disclaimer
> <http://www.csfb.com/legal_terms/disclaimer_europe.shtml>
> 
> 
> 
> ==============================================================================
> Please access the attached hyperlink for an important electronic communications disclaimer: 
> 
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


==============================================================================
Please access the attached hyperlink for an important electronic communications disclaimer: 

http://www.csfb.com/legal_terms/disclaimer_external_email.shtml



From Ted.Harding at nessie.mcc.ac.uk  Tue Sep  6 17:46:16 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 06 Sep 2005 16:46:16 +0100 (BST)
Subject: [R] fitting distributions with R
In-Reply-To: <355C35514FEAC9458F75947F5270974D076CAA@usctmx1103.merck.com>
Message-ID: <XFMail.050906164616.Ted.Harding@nessie.mcc.ac.uk>

On 06-Sep-05 Huntsinger, Reid wrote:
> The MLE of beta is the reciprocal of the sample mean, so you
> don't need an optimizer here.
> 
> Reid Huntsinger

While that is true (and Naja clearly knew this), nevertheless
one expects that using an optimiser should also work. Nadja's
observations need an explanation.

If things don't behave as expected, it is worth while embedding
"debug prints" so as to monitor what is going on internally (as
fas as one can). In this case, if one modifies Nadja's "ll"
function to

ll<-function(beta){
  n<-24
  x<-data2
  temp<-(-n*log(beta)+beta*sum(x))
  print(temp)
  temp
}

and re-runs 'mle', one sees that while there are some numerical
values in the output, there are many NaNs. Also, given the
warning message and the advice to look at "warnings()", one
learns that "NaNs produced in: log(x)" repeatedly. This very
strongly suggests that attempts have been made to take logs
of negative numbers which in trun suggests that the method
of computing the next approximation readily takes the value
of beta outside the valid range of beta > 0.

Now is the time to look at "?mle", which says that the default
method is "BGFS" for which "see optim". Under "?optim" we learn
that "BGFS" is a "quasi-Newton method". Such methods work by
calculating a local tangent to the derivative function and
extrapolating this until it meets the beta-axis, and this can
easily take the estimate outside admissible ranges (try using
Newton-Raphson to solve sqrt(x) = 0).

However, a related method available for 'optim' is "L-BFGS-B"
"which allows _box constraints_, that is each variable can be
given a lower and/or upper bound. The initial value must satisfy
the constraints." This can be set in a parameter for 'mle'.

So now you can try something like

  est<-mle(minuslog=ll, start=list(beta=0.1),
           method="L-BFGS-B", lower=10*(.Machine$double.eps))

and now the trace-prints show a series of numbers, with no NaNs,
so clearly we are getting somewhere (and have identified and
dealt with at least one aspect of the problem). However, observing
the prints, one sees that after an initial trend to convergence
there is a tendency to oscillate between values in the neighbouhood
of beta=360 and values in the neighbourhood of beta=800, finally
failing when two successive values "360.6573" are printed, which
in turn suggests that an attempt is made to comuted a gradient
from identical points. So clearly there is something not right
about how the method works for this particular problem (which,
as a statistical estimation problem, could hardly be simpler!).

Now, "?optim" has, at the end, a Note to the effect that the
default method (admittedly Nelder-Mead, which is not relevant
to the above) may not work well and suggests using 'optimize'
instead. So let's try 'optimize' anyway.

Now, with

  optimize(ll,lower=10*(.Machine$double.eps),upper=1e10)

we get a clean set of debug-prints, and convergence to

  beta = 5.881105e-05

with "minimum" 'll' equal to 254.6480.

Now compare with the known MLE which is

  beta = 1/mean(data2) = 6.766491e-05

giving

  ll(1/mean(data2)) = 254.4226, 

So clearly, now, using 'optimise' instead of 'optim' which
is what 'mle' uses, we are now "in the right parish". However,
there is apparently no parameter to 'mle' which would enable
us to force it to use 'optimize' rather than 'optim'!

This interesting saga, provoked by Nadja's query, now raises
an important general question: Given the simplicity of the
problem, why is the use of 'mle' so unexpectedly problematic?

While in the case of an exponential distribution (which has a
well-known analytical solution) one would not want to use 'mle'
to find the MLE (except as a test of 'mle'. perhaps), one can
easily think of other distributions, in form and behaviour very
similar to the negative exponential but without analytical solution,
for which use of 'mle' or some other optimisation routine would
be required. Such distributions could well give rise to similar
problems -- or worse: in Nadja's example,it was clear that it was
not working; in other cases, it might appear to give a result,
but the result might be very wrong and this would not be obvious.

Hmmm.

Ted.

> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nadja Riedwyl
> Sent: Tuesday, September 06, 2005 9:39 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] fitting distributions with R
> 
> 
> Dear all
> I've got the dataset
> data:2743;4678;21427;6194;10286;1505;12811;2161;6853;2625;14542;694;1149
> 1;
> _ _ _ _ _ 14924;28640;17097;2136;5308;3477;91301;11488;3860;64114;14334
> I know from other testing that it should be possible to fit the data
> with
> the 
> exponentialdistribution. I tried to get parameterestimates for the 
> exponentialdistribution with R, but as the values 
> of the parameter are very close to 0 i get into troubles. Do you know,
> what
> i 
> could do in order to get estimates?How do you choose the starting
> values? in
> 
> my opinion it should be around 1/mean(data).
> 
>  
>#Parameterestimation _with mle() with the log-likelihood funktion of the
>#_
>#exponentialdistribution
> library(stats4) 
> ll<-function(beta) 
> {n<-24 
> x<-data2
> -n*log(beta)+beta*sum(x)} 
> est<-mle(minuslog=ll, start=list(beta=0.1))
> summary(est) 
> 
>#instead of a result, i get:
> 
> 
> Error in optim(start, f, method = method, hessian = TRUE, ...) :
> _ _ _ _ non-finite finite-difference value [1]
> In addition: There were 50 or more warnings (use warnings() to see the
> first
> 
> 50)
>#with fitdistr() for the exponentialdistribution
> library(MASS)
> fitdistr(data2,densfun=dexp,start=list(rate=0.1),lower=6e-06,method="BFG
> S")
> 
>#instead of a result, i get
> 
> Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
> _ _ _ _ non-finite finite-difference value [1]
> In addition: Warning messages:
> 1: bounds can only be used with method L-BFGS-B in: optim(start,
> mylogfn, x
> = 
> x, hessian = TRUE, ...)
> 2: NaNs produced in: dexp(x, 1/rate, log)
> 
> 
> i'll be very happy for any help i can get to solve this problem
> thank you!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Sep-05                                       Time: 16:46:12
------------------------------ XFMail ------------------------------



From efg at stowers-institute.org  Tue Sep  6 18:01:42 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 6 Sep 2005 11:01:42 -0500
Subject: [R] simple line plots?
References: <mailman.9.1125914401.6646.r-help@stat.math.ethz.ch>
	<8C0EED53-F193-4EF4-ADAF-D89F9B1E589D@ranpura.com>
Message-ID: <dfkeh7$5eu$1@sea.gmane.org>

"Ashish Ranpura" <buddhahead at ranpura.com> wrote in message
news:8C0EED53-F193-4EF4-ADAF-D89F9B1E589D at ranpura.com...

> I still don't know how to draw
> each of the three line segments I need).

See ?segments

Could you post a small "toy" problem so we can see exactly what segements
you're wanting to draw?

efg



From maechler at stat.math.ethz.ch  Tue Sep  6 18:11:27 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Sep 2005 18:11:27 +0200
Subject: [R] help.search problem
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433E451@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433E451@elon12p32001.csfp.co.uk>
Message-ID: <17181.49071.491047.479746@stat.math.ethz.ch>

>>>>> "ToUz" == Uzuner, Tolga <tolga.uzuner at csfb.com>
>>>>>     on Tue, 6 Sep 2005 16:35:53 +0100 writes:

    ToUz> Hi there,
    ToUz> I am using 2.0.1 . However, I was not having this problem with this version of R when I first installed it and started using it.

yes.  It only happens because of an ``incorrectly installed package'' 
installed somewhere in
your
	.libPaths()

and you may have ``wrong-installed'' it only recently.

If you would upgrade to R 2.1.1, the problem would go away,
insofar as  help.start() would report about the package(s) with
invalid installation.
	
Otherwise (in R 2.0.1), it's somewhat tedious to find IIRC:
You may set
    options(error = recover)
immediately before 
	    help.start()

and then inspect the pretty large matrix with the invalid entry
leading to the error.
The matrix has one row per package, and so you can find the
invalid package.

Once you know that, remove the package, and try again.

[As hinted at, you should rather upgrade R]

Martin Maechler


    ToUz> Thanks for your suggestion, I tried it, but that doesn't work either:

    >> help.search("tps")
    ToUz> Error in rbind(...) : number of columns of matrices must match (see arg 8)
    >> utils::help.search("tps")
    ToUz> Error in rbind(...) : number of columns of matrices must match (see arg 8)
    >> 

    ToUz> Traceback results below:

    ToUz> "Convert Sweave Syntax", "Sweave Driver Utilities", "Find Objects by (Partial) Name", 

    .................................
    .................................

    ToUz> "winDialog", "winMenuAdd", "flush.console"))
    ToUz> 2: do.call("rbind", dbMat[, 1])
    ToUz> 1: utils::help.search("tps")
    >> 





    ToUz> -----Original Message-----
    ToUz> From: Henrik Bengtsson [mailto:hb at maths.lth.se]
    ToUz> Sent: 06 September 2005 15:29
    ToUz> To: Uzuner, Tolga
    ToUz> Cc: 'r-help at stat.math.ethz.ch'
    ToUz> Subject: Re: [R] help.search problem


    ToUz> What version of R and what operating system?  What packages do you have 
    ToUz> loaded?

    ToUz> Try utils::help.search("tps"), does that work? Have you tried it in a 
    ToUz> fresh R session, i.e. start with R --vanilla.

    ToUz> If you can't get it to work after this, report the above information 
    ToUz> plus what you get from traceback() after you get the error.

    ToUz> Cheers

    ToUz> Henrik

    ToUz> Uzuner, Tolga wrote:
    >> Dear Fellow R Users,
    >> 
    >> I have recently come across a weird problem with help.search:
    >> 
    >> 
    >>> help.search("tps")
    >> 
    >> Error in rbind(...) : number of columns of matrices must match (see arg 8)
    >> 
    >> 
    >> This happens no matter what I search for...
    >> 
    >> Any thoughts ?
    >> Thanks,
    >> Tolga



From reid_huntsinger at merck.com  Tue Sep  6 18:43:11 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 6 Sep 2005 12:43:11 -0400
Subject: [R] fitting distributions with R
Message-ID: <355C35514FEAC9458F75947F5270974D076CAB@usctmx1103.merck.com>

In "optim" you need to set "ndeps" (the "delta x" parameter controlling the
finite-difference approximation) to a sufficiently small value (or supply
the gradient yourself to avoid finite differences, which are messy on a
restricted parameter space.) Since you expect a minimum at about 6.7e-5 the
default ndeps=1e-3 is definitely too large.

> optim(par=0.1,fn=ll,method="BFGS",control=list(ndeps=1e-6))
$par
[1] 6.76644e-05

$value
[1] 254.4226

$counts
function gradient 
     136       18 

$convergence
[1] 0

$message
NULL

There were 50 or more warnings (use warnings() to see the first 50)
> 

The warnings are "NaNs produced in: log(x)" which can be avoided by making
sure the function doesn't try to take the log of something <= 0, for example
change the last line to 

ifelse(beta > 0, -n*log(beta)+beta*sum(x), Inf)

and then optim is happy.

>From mle() you can pass "control" to optim via "..."

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Ted.Harding at nessie.mcc.ac.uk
Sent: Tuesday, September 06, 2005 11:46 AM
To: r-help at stat.math.ethz.ch
Cc: Nadja Riedwyl
Subject: Re: [R] fitting distributions with R


On 06-Sep-05 Huntsinger, Reid wrote:
> The MLE of beta is the reciprocal of the sample mean, so you
> don't need an optimizer here.
> 
> Reid Huntsinger

While that is true (and Naja clearly knew this), nevertheless
one expects that using an optimiser should also work. Nadja's
observations need an explanation.

If things don't behave as expected, it is worth while embedding
"debug prints" so as to monitor what is going on internally (as
fas as one can). In this case, if one modifies Nadja's "ll"
function to

ll<-function(beta){
  n<-24
  x<-data2
  temp<-(-n*log(beta)+beta*sum(x))
  print(temp)
  temp
}

and re-runs 'mle', one sees that while there are some numerical
values in the output, there are many NaNs. Also, given the
warning message and the advice to look at "warnings()", one
learns that "NaNs produced in: log(x)" repeatedly. This very
strongly suggests that attempts have been made to take logs
of negative numbers which in trun suggests that the method
of computing the next approximation readily takes the value
of beta outside the valid range of beta > 0.

Now is the time to look at "?mle", which says that the default
method is "BGFS" for which "see optim". Under "?optim" we learn
that "BGFS" is a "quasi-Newton method". Such methods work by
calculating a local tangent to the derivative function and
extrapolating this until it meets the beta-axis, and this can
easily take the estimate outside admissible ranges (try using
Newton-Raphson to solve sqrt(x) = 0).

However, a related method available for 'optim' is "L-BFGS-B"
"which allows _box constraints_, that is each variable can be
given a lower and/or upper bound. The initial value must satisfy
the constraints." This can be set in a parameter for 'mle'.

So now you can try something like

  est<-mle(minuslog=ll, start=list(beta=0.1),
           method="L-BFGS-B", lower=10*(.Machine$double.eps))

and now the trace-prints show a series of numbers, with no NaNs,
so clearly we are getting somewhere (and have identified and
dealt with at least one aspect of the problem). However, observing
the prints, one sees that after an initial trend to convergence
there is a tendency to oscillate between values in the neighbouhood
of beta=360 and values in the neighbourhood of beta=800, finally
failing when two successive values "360.6573" are printed, which
in turn suggests that an attempt is made to comuted a gradient
from identical points. So clearly there is something not right
about how the method works for this particular problem (which,
as a statistical estimation problem, could hardly be simpler!).

Now, "?optim" has, at the end, a Note to the effect that the
default method (admittedly Nelder-Mead, which is not relevant
to the above) may not work well and suggests using 'optimize'
instead. So let's try 'optimize' anyway.

Now, with

  optimize(ll,lower=10*(.Machine$double.eps),upper=1e10)

we get a clean set of debug-prints, and convergence to

  beta = 5.881105e-05

with "minimum" 'll' equal to 254.6480.

Now compare with the known MLE which is

  beta = 1/mean(data2) = 6.766491e-05

giving

  ll(1/mean(data2)) = 254.4226, 

So clearly, now, using 'optimise' instead of 'optim' which
is what 'mle' uses, we are now "in the right parish". However,
there is apparently no parameter to 'mle' which would enable
us to force it to use 'optimize' rather than 'optim'!

This interesting saga, provoked by Nadja's query, now raises
an important general question: Given the simplicity of the
problem, why is the use of 'mle' so unexpectedly problematic?

While in the case of an exponential distribution (which has a
well-known analytical solution) one would not want to use 'mle'
to find the MLE (except as a test of 'mle'. perhaps), one can
easily think of other distributions, in form and behaviour very
similar to the negative exponential but without analytical solution,
for which use of 'mle' or some other optimisation routine would
be required. Such distributions could well give rise to similar
problems -- or worse: in Nadja's example,it was clear that it was
not working; in other cases, it might appear to give a result,
but the result might be very wrong and this would not be obvious.

Hmmm.

Ted.

> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nadja Riedwyl
> Sent: Tuesday, September 06, 2005 9:39 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] fitting distributions with R
> 
> 
> Dear all
> I've got the dataset
> data:2743;4678;21427;6194;10286;1505;12811;2161;6853;2625;14542;694;1149
> 1;
> _ _ _ _ _ 14924;28640;17097;2136;5308;3477;91301;11488;3860;64114;14334
> I know from other testing that it should be possible to fit the data
> with
> the 
> exponentialdistribution. I tried to get parameterestimates for the 
> exponentialdistribution with R, but as the values 
> of the parameter are very close to 0 i get into troubles. Do you know,
> what
> i 
> could do in order to get estimates?How do you choose the starting
> values? in
> 
> my opinion it should be around 1/mean(data).
> 
>  
>#Parameterestimation _with mle() with the log-likelihood funktion of the
>#_
>#exponentialdistribution
> library(stats4) 
> ll<-function(beta) 
> {n<-24 
> x<-data2
> -n*log(beta)+beta*sum(x)} 
> est<-mle(minuslog=ll, start=list(beta=0.1))
> summary(est) 
> 
>#instead of a result, i get:
> 
> 
> Error in optim(start, f, method = method, hessian = TRUE, ...) :
> _ _ _ _ non-finite finite-difference value [1]
> In addition: There were 50 or more warnings (use warnings() to see the
> first
> 
> 50)
>#with fitdistr() for the exponentialdistribution
> library(MASS)
> fitdistr(data2,densfun=dexp,start=list(rate=0.1),lower=6e-06,method="BFG
> S")
> 
>#instead of a result, i get
> 
> Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
> _ _ _ _ non-finite finite-difference value [1]
> In addition: Warning messages:
> 1: bounds can only be used with method L-BFGS-B in: optim(start,
> mylogfn, x
> = 
> x, hessian = TRUE, ...)
> 2: NaNs produced in: dexp(x, 1/rate, log)
> 
> 
> i'll be very happy for any help i can get to solve this problem
> thank you!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Sep-05                                       Time: 16:46:12
------------------------------ XFMail ------------------------------

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bstabler at ptvamerica.com  Tue Sep  6 18:48:15 2005
From: bstabler at ptvamerica.com (Ben Stabler)
Date: Tue, 6 Sep 2005 09:48:15 -0700
Subject: [R] Revised shapefiles package
Message-ID: <000001c5b302$c7d4dd90$d400a8c0@STABLERLAPTOP>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050906/4bfd7452/attachment.pl

From tlumley at u.washington.edu  Tue Sep  6 18:50:48 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 6 Sep 2005 09:50:48 -0700 (PDT)
Subject: [R] fitting distributions with R
In-Reply-To: <XFMail.050906164616.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050906164616.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.63a.0509060939180.310042@homer04.u.washington.edu>

On Tue, 6 Sep 2005, Ted.Harding at nessie.mcc.ac.uk wrote:

>
> However, a related method available for 'optim' is "L-BFGS-B"
> "which allows _box constraints_, that is each variable can be
> given a lower and/or upper bound. The initial value must satisfy
> the constraints." This can be set in a parameter for 'mle'.

These box constraints are really designed for situations where the 
boundary is a valid parameter value (so you are really doing constrained 
estimation) rather than situations where the boundary is an artifact of 
parameterisation.

> This interesting saga, provoked by Nadja's query, now raises
> an important general question: Given the simplicity of the
> problem, why is the use of 'mle' so unexpectedly problematic?
>

The problem is simple only in that it is one-dimensional, and optim() 
doesn't take advantage of this.  It is poorly scaled: since the starting 
value is 0.1, the maximum is at 0.00006, and there is a singularity at 0, 
it would be helpful to specify the parscale control option to optim.

The other problem is that we are using finite-difference approximations to 
the derivatives. These are bound to perform badly near the singularity at 
zero, especially in a badly scaled problem.  There is a bug in that 
L-BFGS-B doesn't respect the bounds in computing finite-differences, but 
this is not going to be easy to fix (there was recent discussion on 
r-devel about this).

If I remove the singularity by defining

> lll
function(beta) if(beta<0) 1e6 else ll(beta)

and specify parscale, I get
> est

Call:
mle(minuslogl = lll, start = list(beta = 0.01), control = list(parscale = 
1e-05))

Coefficients:
         beta
6.767725e-05

(Any parscale below 0.01 will give basically the same answer).


Incidentally, the trace output may look as if it is oscillating, but that 
is partly an artifact of the line search that BFGS uses.  The last few 
printed loglikelihoods are
[1] 254.4226
[1] 254.4226
[1] 543.2361
[1] 542.5717


Finally, as I noted earlier, this isn't really a constrained estimation 
problem, it is a problem of a function defined on an open interval with a 
singularity at one end.  In this case (in contrast to real constrained 
estimation problems) it might well be sensible to reparametrize.  mle() 
then works with no problems.

 	-thomas



From riedwyl at giub.unibe.ch  Tue Sep  6 19:22:27 2005
From: riedwyl at giub.unibe.ch (Nadja Riedwyl)
Date: Tue, 6 Sep 2005 19:22:27 +0200
Subject: [R] (no subject)
Message-ID: <200509061922.27370.riedwyl@giub.unibe.ch>

my problem actually arised with fitting the data to the weibulldistribution, 
where it is hard to see, if the proposed parameterestimates make sense.

data1:2743;4678;21427;6194;10286;1505;12811;2161;6853;2625;14542;694;11491;
?? ?? ?? ?? ?? 14924;28640;17097;2136;5308;3477;91301;11488;3860;64114;14334

how am I supposed to know what starting values i have to take?
i get different parameterestimates depending on the starting values i choose, 
this shouldn't be, no? how am i supposed to know, which the "right" estimates 
should be?


> library(MASS)
> fitdistr(data2,densfun=dweibull,start=list(scale=2 ,shape=1 ))
      scale          shape
  1.378874e+04   8.788857e-01
 (3.842224e+03) (1.312395e-01)

> fitdistr(data2,densfun=dweibull,start=list(scale=6 ,shape=2 ))
     scale        shape
  7.81875000   0.12500000
 (4.18668905) (0.01803669)

#if i use the lognormaldistribution instead, i would get the same estimates, 
#no matter, what starting values i choose.

#or if i tried it so fare with mle(), i got different values depending on the 
#starting values too, i use the trial and error method to find appropriate 
#starting values, but i am sure, there is a clear way how to do it, no?
#shouldn't i actually get more or less the same parameterestimates with both 
#methods?
 library(stats4)
> ll<-function(alfa,beta)
+ {n<-24
+ x<-data2
+ -n*log(alfa)-n*log(beta)+alfa*sum(x^beta)-(beta-1)*sum(log(x))}
> est<-mle(minuslog=ll, start=list(alfa=10, beta=1))
There were 50 or more warnings (use warnings() to see the first 50)
> summary(est)
Maximum likelihood estimation

Call:
mle(minuslogl = ll, start = list(alfa = 10, beta = 1))

Coefficients:
        Estimate   Std. Error
alfa 0.002530163 0.0006828505
beta 0.641873010 0.0333072184

-2 log L: 511.6957

> library(stats4)
> ll<-function(alfa,beta)
+ {n<-24
+ x<-data2
+ -n*log(alfa)-n*log(beta)+alfa*sum(x^beta)-(beta-1)*sum(log(x))}
> est<-mle(minuslog=ll, start=list(alfa=5, beta=17))
There were 50 or more warnings (use warnings() to see the first 50)
> summary(est)
Maximum likelihood estimation

Call:
mle(minuslogl = ll, start = list(alfa = 5, beta = 17))

Coefficients:
        Estimate  Std. Error
alfa 0.002143305 0.000378592
beta 0.660359789 0.026433665

-2 log L: 511.1296


thank you very much for all your comments, it really helps me to get further!
Nadja



From chrish at stats.ucl.ac.uk  Tue Sep  6 19:34:27 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Tue, 6 Sep 2005 18:34:27 +0100 (BST)
Subject: [R] (no subject)
In-Reply-To: <200509061922.27370.riedwyl@giub.unibe.ch>
References: <200509061922.27370.riedwyl@giub.unibe.ch>
Message-ID: <Pine.LNX.4.58.0509061827290.17548@egon.stats.ucl.ac.uk>

Dear Nadja,

if the loglikelihood function has various local maxima, the result
may depend on the starting values. This is not unusual. The best estimator
is the one with the maximum loglikelihood, i.e., the smallest value of
-2 log L in the mle output. (Unfortunately, it seems that the
loglikelihood value is not accessible using fitdistr - you would have to
implement the loglikelihood function on you own.)

You could use a lot of starting values, for example generated by some
random mechanism, and take the best estimator.
If you want a single good starting value, you could try to fit a Weibull
distribution "by eye"  and trial-and error to the histogram and use the
corresponding parameters.

Best,
Christian

PS: Please use informative subject lines.

On Tue, 6 Sep 2005, Nadja Riedwyl wrote:

> my problem actually arised with fitting the data to the weibulldistribution,
> where it is hard to see, if the proposed parameterestimates make sense.
>
> data1:2743;4678;21427;6194;10286;1505;12811;2161;6853;2625;14542;694;11491;
> ?? ?? ?? ?? ?? 14924;28640;17097;2136;5308;3477;91301;11488;3860;64114;14334
>
> how am I supposed to know what starting values i have to take?
> i get different parameterestimates depending on the starting values i choose,
> this shouldn't be, no? how am i supposed to know, which the "right" estimates
> should be?
>
>
> > library(MASS)
> > fitdistr(data2,densfun=dweibull,start=list(scale=2 ,shape=1 ))
>       scale          shape
>   1.378874e+04   8.788857e-01
>  (3.842224e+03) (1.312395e-01)
>
> > fitdistr(data2,densfun=dweibull,start=list(scale=6 ,shape=2 ))
>      scale        shape
>   7.81875000   0.12500000
>  (4.18668905) (0.01803669)
>
> #if i use the lognormaldistribution instead, i would get the same estimates,
> #no matter, what starting values i choose.
>
> #or if i tried it so fare with mle(), i got different values depending on the
> #starting values too, i use the trial and error method to find appropriate
> #starting values, but i am sure, there is a clear way how to do it, no?
> #shouldn't i actually get more or less the same parameterestimates with both
> #methods?
>  library(stats4)
> > ll<-function(alfa,beta)
> + {n<-24
> + x<-data2
> + -n*log(alfa)-n*log(beta)+alfa*sum(x^beta)-(beta-1)*sum(log(x))}
> > est<-mle(minuslog=ll, start=list(alfa=10, beta=1))
> There were 50 or more warnings (use warnings() to see the first 50)
> > summary(est)
> Maximum likelihood estimation
>
> Call:
> mle(minuslogl = ll, start = list(alfa = 10, beta = 1))
>
> Coefficients:
>         Estimate   Std. Error
> alfa 0.002530163 0.0006828505
> beta 0.641873010 0.0333072184
>
> -2 log L: 511.6957
>
> > library(stats4)
> > ll<-function(alfa,beta)
> + {n<-24
> + x<-data2
> + -n*log(alfa)-n*log(beta)+alfa*sum(x^beta)-(beta-1)*sum(log(x))}
> > est<-mle(minuslog=ll, start=list(alfa=5, beta=17))
> There were 50 or more warnings (use warnings() to see the first 50)
> > summary(est)
> Maximum likelihood estimation
>
> Call:
> mle(minuslogl = ll, start = list(alfa = 5, beta = 17))
>
> Coefficients:
>         Estimate  Std. Error
> alfa 0.002143305 0.000378592
> beta 0.660359789 0.026433665
>
> -2 log L: 511.1296
>
>
> thank you very much for all your comments, it really helps me to get further!
> Nadja
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From gunter.berton at gene.com  Tue Sep  6 19:41:21 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 6 Sep 2005 10:41:21 -0700
Subject: [R] (no subject)
In-Reply-To: <200509061922.27370.riedwyl@giub.unibe.ch>
Message-ID: <200509061741.j86HfLYc021878@meitner.gene.com>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nadja Riedwyl
> Sent: Tuesday, September 06, 2005 10:22 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] (no subject)
> 
> my problem actually arised with fitting the data to the 
> weibulldistribution, 
> where it is hard to see, if the proposed parameterestimates 
> make sense.
> 
> data1:2743;4678;21427;6194;10286;1505;12811;2161;6853;2625;145
> 42;694;11491;
> ?? ?? ?? ?? ?? 
> 14924;28640;17097;2136;5308;3477;91301;11488;3860;64114;14334
> 
> how am I supposed to know what starting values i have to take?
> i get different parameterestimates depending on the starting 
> values i choose, 
> this shouldn't be, no? how am i supposed to know, which the 
> "right" estimates 
> should be?
> 

This is a general issue with all (gradient-based) optimization methods when
the response to be optimized has many local optima and/or is poorly
conditioned. As Doug Bates and others have often remarked, finding good
starting values is an "art" that is often problem-specific. Ditto for "good"
parameterizations. There is no universal "magic" answer.

In many respects, this is the monster hiding in the closet of many of the
complex modeling methods being proposed in statistics and other disciplines:
when the response function to be optimized is a nonlinear function of "many"
parameters, convergence may be difficult to achieve. Presumably stochastic
optimization methods like simulated annealing and mcmc are less susceptible
to such problems, but they pay a large efficiency price to be so.

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA



From br44114 at gmail.com  Tue Sep  6 19:43:21 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 6 Sep 2005 13:43:21 -0400
Subject: [R] The Perils of PowerPoint
Message-ID: <8d5a3635050906104346501e11@mail.gmail.com>

I don't understand why there's so much discussion on PowerPoint. IMHO,
that can only obscure the real thing:
	- The Perils of Miscommunication
	- The Perils of Not Taking Responsibility (if PowerPoint is to blame
for X, then who's to blame for choosing and using PowerPoint in the
first place?)
	- The Perils of Being an Idiot
	- and so on.
(I'm in grave danger here, and also responsible for using R.)


> -----Original Message-----
> From: Mulholland, Tom [mailto:Tom.Mulholland at dpi.wa.gov.au] 
> Sent: Tuesday, September 06, 2005 2:27 AM
> Cc: Achim Zeileis; r-help at stat.math.ethz.ch
> Subject: Re: [R] The Perils of PowerPoint
> 
> 
> For some reason (probably that our organisation has blocked 
> the site) I could not see the original articles that prompted 
> the post. I however immediately assumed that this was 
> precipitated by Tufte and his comments about PowerPoint (I 
> recall seeing a good example of PowerPoint on his site) 
> http://www.edwardtufte.com/tufte/powerpoint
> 
> When this first came up I recall some dispute about the 
> comments www.sociablemedia.com/articles_dispute.htm and that 
> John Fox did something 
> http://ils.unc.edu/~jfox/powerpoint/introduction.html that I 
> enjoyed reading.
> 
> Other links that are lying on my computer are
> "In defense of PowerPoint" 
> http://www.jnd.org/dn.mss/in_defense_of_powerp.html
> and "Does PowerPoint make you stupid?" at 
> http://www.presentations.com/presentations/delivery/article_di
splay.jsp?vnu_content_id=1000482464
 
Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Tim Churches
> Sent: Saturday, 3 September 2005 10:08 AM
> To: ted.harding at nessie.mcc.ac.uk
> Cc: Achim Zeileis; r-help at stat.math.ethz.ch
> Subject: Re: [R] The Perils of PowerPoint
> 
> 
> (Ted Harding) wrote:
> 
> >By the way, the Washington Post/Minneapolis Star Tribune article is
> >somewhat reminiscent of a short (15 min) broadcast on BBC Radio 4
> >back on October 18 2004 15:45-16:00 called
> >
> >  "Microsoft Powerpoint and the Decline of Civilisation"
> >
> >which explores similar themes and also frequently quotes Tufte.
> >Unfortunately it lapsed for ever from "Listen Again" after the
> >statutory week, so I can't point you to a replay. (However, I
> >have carefully preserved the cassette recording I made).
> >  
> >
> Try http://sooper.org/misc/powerpoint.mp3 (copyright law 
> notwithstanding...)
> 
> Tim C
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dr.mike at ntlworld.com  Tue Sep  6 21:00:52 2005
From: dr.mike at ntlworld.com (Mike Waters)
Date: Tue, 6 Sep 2005 20:00:52 +0100
Subject: [R] The Perils of PowerPoint
In-Reply-To: <8d5a3635050906104346501e11@mail.gmail.com>
Message-ID: <20050906190101.HJHZ23288.aamta09-winn.ispmail.ntl.com@d600>

 And thus to that 'New Age' Management Role, that of the Professional
PowePoint Ranger. He (invariably he) who culls the fruits of the labours of
others to present in ever more slick PowerPoint compendia, whilst never
sullying their hands with 'real' work.

8??>

Mike

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> Sent: 06 September 2005 18:43
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] The Perils of PowerPoint
> 
> I don't understand why there's so much discussion on 
> PowerPoint. IMHO, that can only obscure the real thing:
> 	- The Perils of Miscommunication
> 	- The Perils of Not Taking Responsibility (if 
> PowerPoint is to blame for X, then who's to blame for 
> choosing and using PowerPoint in the first place?)
> 	- The Perils of Being an Idiot
> 	- and so on.
> (I'm in grave danger here, and also responsible for using R.)
> 
> 
> > -----Original Message-----
> > From: Mulholland, Tom [mailto:Tom.Mulholland at dpi.wa.gov.au]
> > Sent: Tuesday, September 06, 2005 2:27 AM
> > Cc: Achim Zeileis; r-help at stat.math.ethz.ch
> > Subject: Re: [R] The Perils of PowerPoint
> > 
> > 
> > For some reason (probably that our organisation has blocked 
> the site) 
> > I could not see the original articles that prompted the post. I 
> > however immediately assumed that this was precipitated by Tufte and 
> > his comments about PowerPoint (I recall seeing a good example of 
> > PowerPoint on his site) http://www.edwardtufte.com/tufte/powerpoint
> > 
> > When this first came up I recall some dispute about the comments 
> > www.sociablemedia.com/articles_dispute.htm and that John Fox did 
> > something 
> http://ils.unc.edu/~jfox/powerpoint/introduction.html that I 
> > enjoyed reading.
> > 
> > Other links that are lying on my computer are "In defense of 
> > PowerPoint"
> > http://www.jnd.org/dn.mss/in_defense_of_powerp.html
> > and "Does PowerPoint make you stupid?" at 
> > http://www.presentations.com/presentations/delivery/article_di
> splay.jsp?vnu_content_id=1000482464
>  
> Tom
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Tim Churches
> > Sent: Saturday, 3 September 2005 10:08 AM
> > To: ted.harding at nessie.mcc.ac.uk
> > Cc: Achim Zeileis; r-help at stat.math.ethz.ch
> > Subject: Re: [R] The Perils of PowerPoint
> > 
> > 
> > (Ted Harding) wrote:
> > 
> > >By the way, the Washington Post/Minneapolis Star Tribune 
> article is 
> > >somewhat reminiscent of a short (15 min) broadcast on BBC Radio 4 
> > >back on October 18 2004 15:45-16:00 called
> > >
> > >  "Microsoft Powerpoint and the Decline of Civilisation"
> > >
> > >which explores similar themes and also frequently quotes Tufte.
> > >Unfortunately it lapsed for ever from "Listen Again" after the 
> > >statutory week, so I can't point you to a replay. (However, I have 
> > >carefully preserved the cassette recording I made).
> > >  
> > >
> > Try http://sooper.org/misc/powerpoint.mp3 (copyright law
> > notwithstanding...)
> > 
> > Tim C
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bob.ohara at helsinki.fi  Tue Sep  6 21:19:05 2005
From: bob.ohara at helsinki.fi (Anon.)
Date: Tue, 06 Sep 2005 22:19:05 +0300
Subject: [R] The Perils of PowerPoint
In-Reply-To: <20050906190101.HJHZ23288.aamta09-winn.ispmail.ntl.com@d600>
References: <20050906190101.HJHZ23288.aamta09-winn.ispmail.ntl.com@d600>
Message-ID: <431DEBA9.4000000@helsinki.fi>

Mike Waters wrote:

> And thus to that 'New Age' Management Role, that of the Professional
>PowePoint Ranger. He (invariably he) who culls the fruits of the labours of
>others to present in ever more slick PowerPoint compendia, whilst never
>sullying their hands with 'real' work.
>
>  
>
In academia they're known as "professors".

Bob

-- 
Bob O'Hara
Department of Mathematics and Statistics
P.O. Box 68 (Gustaf H??llstr??min katu 2b)
FIN-00014 University of Helsinki
Finland

Telephone: +358-9-191 51479
Mobile: +358 50 599 0540
Fax:  +358-9-191 51400
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: www.jnr-eeb.org



From luisen.p at gmail.com  Tue Sep  6 21:33:55 2005
From: luisen.p at gmail.com (Luis Pineda)
Date: Tue, 6 Sep 2005 15:33:55 -0400
Subject: [R] Predicting responses using ace
Message-ID: <859087cf0509061233350d7dba@mail.gmail.com>

Hello everybody,

I'm a new user of R and I'm working right now with the ACE function
from the acepack library. I Have a question: Is there a way to predict
new responses using ACE? What I mean is doing something similar to the
following code that uses PPR (Projection Pursuit Regression):

library(MASS)
x <- runif(20, 0, 1)
xnew <- runif(2000, 0, 1)
y <- sin(x)
a <- ppr(x, y, 2)
ynew <- predict(ppr, xnew)

Any help would be much appretiated, Thanks in advance,
Luis Pineda



From richard_raubertas at merck.com  Wed Sep  7 00:13:06 2005
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Tue, 6 Sep 2005 18:13:06 -0400
Subject: [R] Spacing and margins in plot
Message-ID: <D9786C12E3E5534884EA7CBFB251576A388F0C@usctmx1114.merck.com>

You can do this with the 'mgp' argument to par()  (see ?par).
For example, I find par(mgp=c(2, 0.75, 0)) (which puts the
axis label on line 2 and the axis values on line 0.75) nicely
"tightens up" the space around a plot.

Rich Raubertas

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Earl F. Glynn
> Sent: Thursday, September 01, 2005 11:14 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Spacing and margins in plot
> 
> 
> "Chris Wallace" <c.wallace at qmul.ac.uk> wrote in message
> news:m3wtm0502o.fsf at qmul.ac.uk...
> 
> > how about
> > plot(..., xlab="")
> > title(xlab="label text", line=2)
> 
> Yes, Chris, I like your idea, especially when I can "fix" 
> both X and Y axes
> at the same time:
> 
>   plot(0, xlab="",ylab="")
>   title(xlab="X axis", ylab="Y axis", line=2)
> 
> I'd prefer a way to set the axis title line at the same time 
> I change the
> mar parameters, but it's not a big deal.
> 
> Thanks.
> efg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From jacob.etches at utoronto.ca  Wed Sep  7 00:51:03 2005
From: jacob.etches at utoronto.ca (Jacob Etches)
Date: Tue, 6 Sep 2005 18:51:03 -0400
Subject: [R] R Cocoa GUI assumes Japanese locale
Message-ID: <634495ac573bed698bd05d00b377ae77@utoronto.ca>

After installing the latest binary for OS X, the R Cocoa GUI provides 
output in the console in Japanese only.  I would prefer the output to 
be in English, but cannot figure out how to change the setting.  If I 
start R in a terminal window, output is in English.

Version is R Cocoa GUI 1.12 (1622), S.M.Iacus & S.Urbanek.

Thanks for any help.

Jacob Etches

Doctoral candidate, Epidemiology
Department of Public Health Sciences
University of Toronto Faculty of Medicine

Research Associate
Institute for Work & Health
800-481 University Ave.
Toronto, ON
M5G 2E9
416.927.2027x2290
www.iwh.on.ca



From r at roryt.gr  Wed Sep  7 01:06:07 2005
From: r at roryt.gr (I.Ioannou)
Date: Wed, 7 Sep 2005 02:06:07 +0300
Subject: [R] LV path analysis with PLS (was Re: PLSR: model notation and
	reliabilities)
In-Reply-To: <20050831093128.GA5790@argeas.cs-net.gr>
References: <20050827010413.GA27127@argeas.cs-net.gr>
	<20050828231855.GA3475@argeas.cs-net.gr>
	<m0wtm5xoju.fsf@bar.nemo-project.org>
	<20050831093128.GA5790@argeas.cs-net.gr>
Message-ID: <20050906230606.GA28000@argeas.cs-net.gr>

On Wed, Aug 31, 2005 at 12:31:29PM +0300, I.Ioannou wrote:
> On Mon, Aug 29, 2005 at 08:08:53AM +0200, Bj?rn-Helge Mevik wrote:
>  
> > It seems to me that what you are looking for, is some sort of
> > structured equation models (? la Lisrel).  The pls package implements
--snipped--
> and the explained variance seem to be ok, but I'm afraid that 
> this is not my case. I thought that plsr should be used to perform 
--snipped--

Well, I should had asked  : Is there a way to use plsr to perform 
(or another R package that implements) "latent variables path analysis 
with partial least-squares estimation", i.e. the algorithm that was 
implemented in the old DOS lvpls program ?
(http://kiptron.psyc.virginia.edu/Programs/lvplsmanual.pdf) 

TIA
Ioannis Ioannou



From Tom.Mulholland at dpi.wa.gov.au  Wed Sep  7 02:55:14 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 7 Sep 2005 08:55:14 +0800
Subject: [R] The Perils of PowerPoint
Message-ID: <4702645135092E4497088F71D9C8F51A128C15@afhex01.dpi.wa.gov.au>

I incorrectly relied upon my memory 

...
> and that 
> John Fox did something 
> http://ils.unc.edu/~jfox/powerpoint/introduction.html that I 
> enjoyed reading.

The work is that of Jackson Fox

Tom



From f.harrell at vanderbilt.edu  Wed Sep  7 03:43:32 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 06 Sep 2005 20:43:32 -0500
Subject: [R] Predicting responses using ace
In-Reply-To: <859087cf0509061233350d7dba@mail.gmail.com>
References: <859087cf0509061233350d7dba@mail.gmail.com>
Message-ID: <431E45C4.401@vanderbilt.edu>

Luis Pineda wrote:
> Hello everybody,
> 
> I'm a new user of R and I'm working right now with the ACE function
> from the acepack library. I Have a question: Is there a way to predict
> new responses using ACE? What I mean is doing something similar to the
> following code that uses PPR (Projection Pursuit Regression):
> 
> library(MASS)
> x <- runif(20, 0, 1)
> xnew <- runif(2000, 0, 1)
> y <- sin(x)
> a <- ppr(x, y, 2)
> ynew <- predict(ppr, xnew)
> 
> Any help would be much appretiated, Thanks in advance,
> Luis Pineda
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

Look at the areg.boot function in the Hmisc package, and its associated 
predict method.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From David.Duffy at qimr.edu.au  Wed Sep  7 04:39:29 2005
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 7 Sep 2005 12:39:29 +1000 (EST)
Subject: [R]  convergence for proportional odds model
In-Reply-To: <mailman.9.1126000801.10221.r-help@stat.math.ethz.ch>
References: <mailman.9.1126000801.10221.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0509071226060.20473@orpheus.qimr.edu.au>

liu abc <liu2074 at yahoo.com> wrote:
>
> I am using proportional odds model for ordinal responses in
> dose-response experiments. For some samll data, SAS can successfully
> provide estimators of the parameters, but the built-in function polr()
> in R fails. Would you like to tell me how to make some change so I
> can use polr() to obtain the estimators? Or anyone can give me a hint
> about the conditions for the existance of MLE in such a simple case?
> By the way, for the variable "resp" which must be ordered factor, how
> can I do it? Thanks a lot.
>
> Guohui

> The following is one example I used both in SAS and R.
>
> in R:
>
> library(MASS)
> dose.resp = matrix( c(1,1,1,1,2,2,2,3,3,3, 2,2,3,3,4,4,5,4,5,5), ncol=2)
> colnames(dose.resp)= c("resp", "dose")
> polr( factor(resp, ordered=T)~dose, data=dose.resp)
> #Error in optim(start, fmin, gmin, method = "BFGS", hessian = Hess, ...) :
> # initial value in 'vmmin' is not finite

It seems to be the starting values.  Using lrm() from the Design package gave

> dose.resp <- as.data.frame(dose.resp)
> dose.resp$resp <- factor(dose.resp$resp)
> library(Design)
> lrm(resp ~ dose, data=dose.resp)

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy
        10      6e-06      11.43          1      7e-04      0.909      0.818
     Gamma      Tau-a         R2      Brier
     0.931        0.6      0.768      0.014

     Coef    S.E.  Wald Z P
y>=2 -10.904 5.137 -2.12  0.0338
y>=3 -14.336 6.287 -2.28  0.0226
dose   3.160 1.399  2.26  0.0239

and giving polr starting values:


> print(m1 <- polr(resp ~ dose, data=dose.resp, start=c(-1, -4, 3)))
Call:
polr(formula = resp ~ dose, data = dose.resp, start = c(-1, -4,
    3))

Coefficients:
    dose
3.158911

Intercepts:
     1|2      2|3
10.90172 14.33296

Residual Deviance: 10.34367
AIC: 16.34367

Even then, summary(m1) gives the same problem (as it refits).  There is
separation in the data, of course, but I presume the ordinality gives
some extra information.

David Duffy.



From maj at waikato.ac.nz  Wed Sep  7 05:45:55 2005
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Wed, 07 Sep 2005 15:45:55 +1200
Subject: [R] Sorting Text Frames
Message-ID: <431E6273.3070501@waikato.ac.nz>

[Using 2.0.1 under Windows XP]
There are a few pages on the internet that list equivalents of
"thank you" in many languages. I downloaded one from a Google search
and I thought that it would be interesting and a good R exercise to
sort the file into the order of the expressions, rather than the languages.

I tidied up the web page and got it into the format that it was nearly
in: Language Name in columns 1-43, the expression in the remaining
columns.

Then I read it in:

 > thanks <- read.fwf("C:\\Files\\Reading\\thankyou.txt", c(43,37))
 > thanks[1:4,]
                                            V1            V2
1 Abenaki (Maine USA, Montreal Canada)            Wliwni ni
2 Abenaki (Maine USA, Montreal Canada)               Wliwni
3 Abenaki (Maine USA, Montreal Canada)               Oliwni
4 Ach?? (Baja Verapaz Guatemala)               Mantiox chawe

 > dim(thanks)
[1] 1254    2

Now I tried sorting the frame into the order of the second column:

tord <- order(thanks$V2)
sink("C:\\Files\\Reading\\thanks.txt")
thanks[tord[1:74],]
sink()

This gives more or less the expected output, the file thanks.txt beginning

                                                   V1 
    V2
145      Cahuila (United States)                                '\301cha-ma
862      Paipai (Mexico, USA)                                    'Ara'ya:ikm
863      Paipai (Mexico, USA)                                    'Ara'yai:km
864      Paipai (Mexico, USA)                                     'Ara'ye:km
311      Eyak (Alaska)                                            'Awa'ahdah

[you may get a bit of wrapping there!]

However I don't really want just 74 lines, I would like the whole file. But
if I get rid of the [1:74] or replace 74 with any larger number I get 
output
like this, with no second column:

                                                   V1
145      Cahuila (United States)
862      Paipai (Mexico, USA)
863      Paipai (Mexico, USA)
864      Paipai (Mexico, USA)
311      Eyak (Alaska)

Does anyone know what is going on?
Tusen tak in advance, in fact 1254 tak in advance!

Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk     Home +64 7 825 0441   Mobile 021 1395 862



From p.connolly at hortresearch.co.nz  Wed Sep  7 06:54:40 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 7 Sep 2005 16:54:40 +1200
Subject: [R] Lattice key seems to ignore the key list
Message-ID: <20050907045440.GA12344@hortresearch.co.nz>

I've never had this problem before and can't see what could be
different from other times I've used keys with lattice.



It appears that auto.key is being taken as TRUE when I specify a key
list.  The list I specify seems to be ignored.

Where can I place a browser to figure out what is going on?

Having made a list key.symbol from trellis.par.get, and specified a
scales list and a between list, and a formula object (form), I use
xyplot like this:

xyplot(form, data = xx, groups = Entry, layout = c(8,8, 1), 
                par.strip.text = list(cex = .65), between = between,
                scales = scales,
                panel = function(x, y, ...)
                  panel.superpose(x, y, ...),
                key = list(points = Rows(key.symbol, 1:4),
                  text = list(levels(xx$Entry),
                    space = "right", columns = 1))
                )

What is implied in there that would set auto.key to TRUE?  The space
and columns part of the list seems to be ignored and the autokey
values substituted.  

Ideas, please.

Thanks.

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From hypan at scbit.org  Wed Sep  7 07:16:18 2005
From: hypan at scbit.org (Salang Pan)
Date: Wed, 7 Sep 2005 13:16:18 +0800
Subject: [R] (no subject)
Message-ID: <200509070519.j875IvT2020123@hypatia.math.ethz.ch>

hi£¬
  
  Is it possible to draw a string text in a rectangle according the width of this rectangle?   that is, the fontsize of this string text can be adjusted according the width of the rectangle.
 How to set the cex parameter in text function? 

text (x, y = NULL, labels = seq(along = x), adj = NULL,
          pos = NULL, offset = 0.5, vfont = NULL,
          cex = 1, col = NULL, font = NULL, xpd = NULL, ...)

   thanks!

	

	

			 
=====================================================¡¡
 Salang
 hypan at scbit.org

 Tel: 021-64363311-123
 Shanghai Center for Bioinformatics Technology
 Floor 12th,100# QinZhou Road
 Shanghai,China,200235



From tchur at optushome.com.au  Wed Sep  7 07:52:27 2005
From: tchur at optushome.com.au (Tim Churches)
Date: Wed, 07 Sep 2005 15:52:27 +1000
Subject: [R] Leading in line-wrapped Lattice value and panel labels
Message-ID: <200509070552.j875qRjV000523@mail22.syd.optusnet.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050907/d6f1b486/attachment.pl

From Allan at STATS.uct.ac.za  Wed Sep  7 08:16:57 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Wed, 07 Sep 2005 08:16:57 +0200
Subject: [R] R: optim
References: <431D753A.5C2D2AF3@STATS.uct.ac.za>
	<40e66e0b050906053591b35dd@mail.gmail.com>
Message-ID: <431E85D9.45FD7DF7@STATS.uct.ac.za>

thanx for the reply. i understood that the function found a maximum. i
was just a bit worried about the message.  i assumed that it was an
ERROR message. 

i see now that it is some sort of stopping rule. does this make sense?
/
allan

Douglas Bates wrote:
> 
> On 9/6/05, Clark Allan <Allan at stats.uct.ac.za> wrote:
> > hi all
> >
> > i dont understand the error message that is produced by the optim
> > function. can anybody help???
> >
> > ie:
> > [[1]]$message
> > [1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
> >
> > can anyone help?
> 
> That code indicates that the optimizer has declared convergence
> because the relative reduction in the objective function in successive
> iterates is below a tolerance.  As documented in ?optim, a convergence
> code of 0 indicates success
> 
> ...
> convergence: An integer code. '0' indicates successful convergence.
>           Error codes are
> ...
> 
> This may be counter-intuitive but it does make sense to shell
> programmers.  The idea is that there is only one way you can succeed
> but there are many different ways of failing so you use the nonzero
> codes to indicate the types of failure and the zero code, which we
> usually read as FALSE in a logical context, to indicate success.
> 
> >
> >
> >
> > ###########################################################################
> >
> > SK.FIT(XDATA=a,XDATAname="a",PHI1=1,v=5,vlo=2,vhi=300,phi2lo=.01)
> > [[1]]
> > [[1]]$par
> > [1]  -0.01377906   0.83859445   0.34675230 300.00000000
> >
> > [[1]]$value
> > [1] 90.59185
> >
> > [[1]]$counts
> > function gradient
> >       53       53
> >
> > [[1]]$convergence
> > [1] 0
> >
> > [[1]]$message
> > [1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
> >
> > #################################################################################
> >
> >
> >
> > i ghave included the function used in the optim call:
> >
> > SKEWMLE=function(l,DATA=XDATA,...)
> >         {
> >                 #alpha = l[1]
> >                 #beta = l[2]
> >                 #phi2 = l[3]
> >                 #v= l[4]
> >                 phi1=PHI1
> >
> >                 DATA<-as.matrix(DATA)
> >
> >                 fnew<-function(x,y,l,...)
> >                 {
> >                         #when we do not estimate phi1
> >                         t1=(1+((y-l[1]-l[2]*x)^2)/(l[4]*l[3]^2))^(-0.5*(1+l[4]))
> >                         t2=(1+(x^2)/l[4])^(-0.5*(1+l[4]))
> >                         t3=2*((gamma(0.5*(1+l[4]))/(gamma(0.5*l[4])*sqrt(l[4]*pi)))^2)/l[3]
> >
> >                         t1*t2*t3
> >                 }
> >
> >                 a<-double(length(DATA))
> >                 y=DATA
> >                 a=apply(y,1,function(q)
> > log(integrate(fnew,lower=0,upper=Inf,y=q,l=l)$value))
> >                 -sum(a)
> >         }
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >

From ligges at statistik.uni-dortmund.de  Wed Sep  7 08:48:54 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 07 Sep 2005 08:48:54 +0200
Subject: [R] Sorting Text Frames
In-Reply-To: <431E6273.3070501@waikato.ac.nz>
References: <431E6273.3070501@waikato.ac.nz>
Message-ID: <431E8D56.7010805@statistik.uni-dortmund.de>

Murray Jorgensen wrote:
> [Using 2.0.1 under Windows XP]
> There are a few pages on the internet that list equivalents of
> "thank you" in many languages. I downloaded one from a Google search
> and I thought that it would be interesting and a good R exercise to
> sort the file into the order of the expressions, rather than the languages.
> 
> I tidied up the web page and got it into the format that it was nearly
> in: Language Name in columns 1-43, the expression in the remaining
> columns.
> 
> Then I read it in:
> 
>  > thanks <- read.fwf("C:\\Files\\Reading\\thankyou.txt", c(43,37))
>  > thanks[1:4,]
>                                             V1            V2
> 1 Abenaki (Maine USA, Montreal Canada)            Wliwni ni
> 2 Abenaki (Maine USA, Montreal Canada)               Wliwni
> 3 Abenaki (Maine USA, Montreal Canada)               Oliwni
> 4 Ach?? (Baja Verapaz Guatemala)               Mantiox chawe
> 
>  > dim(thanks)
> [1] 1254    2
> 
> Now I tried sorting the frame into the order of the second column:
> 
> tord <- order(thanks$V2)
> sink("C:\\Files\\Reading\\thanks.txt")
> thanks[tord[1:74],]
> sink()
> 
> This gives more or less the expected output, the file thanks.txt beginning
> 
>                                                    V1 
>     V2
> 145      Cahuila (United States)                                '\301cha-ma
> 862      Paipai (Mexico, USA)                                    'Ara'ya:ikm
> 863      Paipai (Mexico, USA)                                    'Ara'yai:km
> 864      Paipai (Mexico, USA)                                     'Ara'ye:km
> 311      Eyak (Alaska)                                            'Awa'ahdah
> 
> [you may get a bit of wrapping there!]
> 
> However I don't really want just 74 lines, I would like the whole file. But
> if I get rid of the [1:74] or replace 74 with any larger number I get 
> output
> like this, with no second column:
> 
>                                                    V1
> 145      Cahuila (United States)
> 862      Paipai (Mexico, USA)
> 863      Paipai (Mexico, USA)
> 864      Paipai (Mexico, USA)
> 311      Eyak (Alaska)

I guess there is just too much space or some special characters in your 
variables that cause problems when printing ...
Hence you have to "debug" your data yourself.

Uwe Ligges



> Does anyone know what is going on?
> Tusen tak in advance, in fact 1254 tak in advance!
> 
> Murray Jorgensen



From ligges at statistik.uni-dortmund.de  Wed Sep  7 09:35:26 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 07 Sep 2005 09:35:26 +0200
Subject: [R] (no subject)
In-Reply-To: <200509070519.j875IvT2020123@hypatia.math.ethz.ch>
References: <200509070519.j875IvT2020123@hypatia.math.ethz.ch>
Message-ID: <431E983E.2020203@statistik.uni-dortmund.de>

Salang Pan wrote:

> hi£¬
>   
>   Is it possible to draw a string text in a rectangle according the width of this rectangle?   that is, the fontsize of this string text can be adjusted according the width of the rectangle.
>  How to set the cex parameter in text function? 
> 
> text (x, y = NULL, labels = seq(along = x), adj = NULL,
>           pos = NULL, offset = 0.5, vfont = NULL,
>           cex = 1, col = NULL, font = NULL, xpd = NULL, ...)
> 
>    thanks!

The grid framework might be helpful.

For standard graphics, you can optimze the strwidth():

 plot(1:10)
 rect(1,1,9,9)
 cex <- optimize(function(x)
     abs(strwidth("Hello World", cex=x)-8),
   interval=c(1, 10))$minimum
 text(5,5, "Hello World", cex=cex)


Uwe Ligges



> 	
> 
> 	
> 
> 			 
> =====================================================¡¡
>  Salang
>  hypan at scbit.org
> 
>  Tel: 021-64363311-123
>  Shanghai Center for Bioinformatics Technology
>  Floor 12th,100# QinZhou Road
>  Shanghai,China,200235
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From cniharral_rhelp at yahoo.es  Wed Sep  7 09:35:49 2005
From: cniharral_rhelp at yahoo.es (C NL)
Date: Wed, 7 Sep 2005 09:35:49 +0200 (CEST)
Subject: [R] Fisher's method in discriminant analysis
In-Reply-To: <1125942758.6006.15.camel@dhcp-123.wolf.ox.ac.uk>
Message-ID: <20050907073549.64090.qmail@web25408.mail.ukl.yahoo.com>

Hi,

   I read your answer and the message you pointed me
at, and you talked about the page 347 of the book MASS
3 in your posting as a place where the Fisher's method
was mentioned. The thing is that I don't have that
book, so I would like to ask you if you can give me
that information. If you don't, do you know any other
resource where I can search for it?

   I'm desperately looking for information related to
how to obtain Fisher's classification functions, and I
don't know if it is too obvious or simply I'm not
finding what I need.

   Thanks a lot and sorry for my insistence.

Regards Carlos

 --- Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
escribi??:

> See this thread
>
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/34951.html
> 
> Sorry, my memory has not improved since then but
> there are others on
> this list who know better this than myself.
> 
> Regards, Adai
> 
> 
> 
> On Mon, 2005-09-05 at 18:15 +0200, C NL wrote:
> > Hi,
> > 
> >   I'm using mda library to solve a discriminant
> > analysis. I get results, but the thing is that I
> want
> > to use Fisher's method to obtain the
> classification
> > functions and I'm lost in what I should do:
> libraries
> > to use, ... Can anybody give me a clue??
> > 
> > Thanks.
> > 
> >    Carlos Niharra L??pez
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > 
> 
>



From maj at waikato.ac.nz  Wed Sep  7 10:32:39 2005
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Wed, 07 Sep 2005 20:32:39 +1200
Subject: [R] Sorting Text Frames
In-Reply-To: <431E8D56.7010805@statistik.uni-dortmund.de>
References: <431E6273.3070501@waikato.ac.nz>
	<431E8D56.7010805@statistik.uni-dortmund.de>
Message-ID: <431EA5A7.3030709@waikato.ac.nz>

Uwe Ligges wrote:
> I guess there is just too much space or some special characters in your 
> variables that cause problems when printing ...
> Hence you have to "debug" your data yourself.
> 
> Uwe Ligges

However the problem persists when I don't try to print the fram "thanks" 
to a file.

thanks[tord[1:74],]
gives me row numbers, V1 and V2 to the console, but

thanks[tord[1:75],]
gives only the row numbes and V1.

It is not a special problem with row tord[75]:

 > thanks[tord[75],]
                                               V1 
    V2
1192 Votic (Russia) [may God give you health]    Antagoo Jumal terv??t teilee

Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk     Home +64 7 825 0441   Mobile 021 1395 862



From jfontain at free.fr  Wed Sep  7 12:19:58 2005
From: jfontain at free.fr (jfontain@free.fr)
Date: Wed, 07 Sep 2005 12:19:58 +0200
Subject: [R] irregular time series prediction
Message-ID: <1126088398.431ebece6b42c@imp3-g19.free.fr>

Hello.

This is my first post, so allow me to introduce myself.

But first, I'd like to thank all the authors and contributors to the R software,
as I think that it is truly a great and very useful package.

I am the author of moodss, a GPL modular monitoring application
(http://moodss.sourceforge.net). Moodss collects, archives in a SQL database
and displays data from monitored devices, mostly computers, databases and
network equipment.

My idea is to use the stored data to perform predictions for capacity planning
purposes. For example, based on the trafic on a network line for the last 12
months, what is the expected evolution in the next 3 months.

Since there is no guarantee that the data samples are regularly spaced in time,
I was thinking of using the "its" package for a start.
But data samples, most of the time, are roughly regularly spaced. For example,
the monitored network device could return data every 10 seconds, but sometimes
at 11 seconds or 9 seconds after the last sample. So another idea would be to
normalize the data (by interpolation maybe) to make it a regular time series as
a first step.

All I need from you at this time is to point me in the right direction, maybe
suggest resources on the web about this subject applied to R, knowing that I
would prefer to use only R base packages if possible.

I apologize if this is a trivial question, but last time I studied statistics
was more than 20 years ago, so I need a little time to warm up...

Many thanks in advance,


--
Jean-Luc



From ligges at statistik.uni-dortmund.de  Wed Sep  7 12:48:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 07 Sep 2005 12:48:10 +0200
Subject: [R] Sorting Text Frames
In-Reply-To: <431EBEC9.2010401@waikato.ac.nz>
References: <431E6273.3070501@waikato.ac.nz>
	<431E8D56.7010805@statistik.uni-dortmund.de>
	<431EA5A7.3030709@waikato.ac.nz>
	<431EA7F4.4060403@statistik.uni-dortmund.de>
	<431EBEC9.2010401@waikato.ac.nz>
Message-ID: <431EC56A.7060909@statistik.uni-dortmund.de>

Murray Jorgensen wrote:

>  > thanks <- read.fwf("C:\\Files\\Reading\\thankyou.txt", c(43,37))

[CCing R-help again: I have looked at Murray Jorgensen's data in the 
meantime]

tord <- order(thanks$V2)
sink("C:\\thanks.txt")
thanks[tord,]
sink()

Works for me with R-2.1.1.

--> Please upgrade your version of R and try again.


Uwe Ligges



From wilks at dial.pipex.com  Wed Sep  7 13:03:44 2005
From: wilks at dial.pipex.com (John Wilkinson (pipex))
Date: Wed, 7 Sep 2005 12:03:44 +0100
Subject: [R] Doubt about nested aov output
Message-ID: <JCEIJNOHMNBPLMGFDHNDAEPGCAAA.wilks@dial.pipex.com>

Ronaldo ,

It looks as though you have specified you model incorrectly.

In the Rats example ,the Treatment is the only fixed effect,Rat and Liver
are random effects

In aov testing for sig of 'Means' of Random Effects is pointless and that is
why 'p' values are not given.Further more the interaction between a Random
Effect and a Fixed Effect is also a Random Effect. The 'aov' with error
structure terms output reflects this by only giving 'p' values to
Fixed Effects and their interactions.



Note That the Fixed Effects of Block, Variety and their interaction
Block:Variety are given "p" values while the Field'Random Effects have not.



>  model <- aov(Glycogen~Treatment+Error(Rat/Liver))
> summary(model)

Error: Rat
          Df Sum Sq Mean Sq F value Pr(>F) #Rat is random effect
Residuals  1 413.44  413.44

Error: Rat:Liver				      #Rat:Liver is Random effect
          Df  Sum Sq Mean Sq F value Pr(>F)
Residuals  4 164.444  41.111

Error: Within
          Df  Sum Sq Mean Sq F value    Pr(>F)
Treatment  2 1557.56  778.78  18.251 8.437e-06 *** #Fixed effect
Residuals 28 1194.78   42.67
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



I hope that this is of help.

John



From hypan at scbit.org  Wed Sep  7 12:56:12 2005
From: hypan at scbit.org (Salang Pan)
Date: Wed, 7 Sep 2005 18:56:12 +0800
Subject: [R] (no subject)
Message-ID: <200509071058.j87Awv5m026354@hypatia.math.ethz.ch>


hi,
   when I use bclust in R,   bclust(dat,centers=5,minsize=3,base.centers=4)
dat has 25 rows, there is an error as following:
    Error in knn1(object$allcenters, x, factor(1:nrow(object$allcenters))) : 
        train and class have different lengths
when I debug this function , I found the error is in the prune.bclust function. But I can not search the help doc of this function.
which package is this function in?

Meanwhile I think there is a  bug in blcust:   bclust(x, centers=2, iter.base=10, minsize=0,
            dist.method="euclidian",
            hclust.method="average", base.method="kmeans",
            base.centers=20, verbose=TRUE,
            final.kmeans=FALSE, docmdscale=FALSE,
            resample=TRUE, weights=NULL, maxcluster=base.centers, ...)

if the centers is larger than base.centers, maxcluster is default,ie is equal to base.centers, then it will occur  an error as following:
	Error in clusters.bclust(object, centers) : subscript out of bounds
because this line object$members <- cutree(object$hclust, 2:maxcluster) in hclust.bclust function denotes that object$members has (maxcluster-1) columns, it will occur subscript out of bounds.

I think the maxcluster may be setted as max(base.centers, centers), is it right?
Thanks you very much!
>
>	
>
>	
>
>			 
>=====================================================¡¡
> Salang
> hypan at scbit.org
>
> Tel: 021-64363311-123
> Shanghai Center for Bioinformatics Technology
> Floor 12th,100# QinZhou Road
> Shanghai,China,200235



From carsten.steinhoff at stud.uni-goettingen.de  Wed Sep  7 13:15:39 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Wed, 7 Sep 2005 13:15:39 +0200
Subject: [R] fitting distribution tails
Message-ID: <200509071116.j87BGZuw030048@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050907/23b1c8e6/attachment.pl

From I.Visser at uva.nl  Wed Sep  7 13:58:10 2005
From: I.Visser at uva.nl (Ingmar Visser)
Date: Wed, 07 Sep 2005 13:58:10 +0200
Subject: [R] fitting distribution tails
In-Reply-To: <200509071116.j87BGZuw030048@hypatia.math.ethz.ch>
Message-ID: <BF44A272.75EF%I.Visser@uva.nl>

Not an R-response, but see this reference:

Dolan CV, van der Maas HLJ, Molenaar PCM
A framework for ML estimation of parameters of (mixtures of) common reaction
time distributions given optional truncation or censoring??
 BEHAVIOR RESEARCH METHODS INSTRUMENTS & COMPUTERS 34 (3): 304-323 AUG 2002

on estimating distribution parameters on truncated data sets, there is an
accompanying program available (which may or may not easily port to R ...)

hth, ingmar


> From: "Carsten Steinhoff" <carsten.steinhoff at stud.uni-goettingen.de>
> Date: Wed, 7 Sep 2005 13:15:39 +0200
> To: <r-help at stat.math.ethz.ch>
> Subject: [R] fitting distribution tails
> 
> Hello,
> 
> I want to fit a distribution to a dataset. Important is not the "overall"
> fitting but the fitting in the tail (e.g. all observations > x or the n
> highest values). Standard ML-estimation sometimes doesn't work here very
> well. We see that especially when we have truncated datasets the algorithms
> won't converge. In the case of lognormal distribution: It seems that the
> farer the truncation point is away from the peak of the whole distribution
> the more unlikely is the convergence.
> 
> So I think to do the following. And my questions are: Before I try to do it
> with basic R-knowledge on my own ... maybe there is a similar solution
> already available in R. And:  maybe somebody can give me further reading on
> this topic or has other/better ideas how to cope this type of problem
> (except EVT-Approaches).
> 
> First I produce the quantiles for all points of my dataset. I fix that the
> fitting will be done for the n largest values. Then an optimization
> algorithm starts. The objective function could be a goodness-of-fit
> criterion, for a first try e.g: Minimize the sum over all squared deltas
> [empirical - theoretical distribution]. In *any* case should be found
> parameters that fulfill the condition. The criterion should be able to
> overweight observations the higher they are.
> 
> What do you think about ?
> 
> Regards, Carsten
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Allan at STATS.uct.ac.za  Wed Sep  7 14:06:46 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Wed, 07 Sep 2005 14:06:46 +0200
Subject: [R] R: optim
References: <431D753A.5C2D2AF3@STATS.uct.ac.za>
	<40e66e0b050906053591b35dd@mail.gmail.com>
	<431E85D9.45FD7DF7@STATS.uct.ac.za>
Message-ID: <431ED7D6.192463D7@STATS.uct.ac.za>

funny optim message:

$MLE
$MLE$par
[1] -0.09554688  1.13100488  0.06651340

$MLE$value
[1] 48.93381

$MLE$counts
function gradient 
     100      100 

$MLE$convergence
[1] 52

$MLE$message
[1] "ERROR: ABNORMAL_TERMINATION_IN_LNSRCH"


WHAT DOES THIS ERROR MESSAGE MEAN???

hope some one can help.
/
allan


Clark Allan wrote:
> 
> thanx for the reply. i understood that the function found a maximum. i
> was just a bit worried about the message.  i assumed that it was an
> ERROR message.
> 
> i see now that it is some sort of stopping rule. does this make sense?
> /
> allan
> 
> Douglas Bates wrote:
> >
> > On 9/6/05, Clark Allan <Allan at stats.uct.ac.za> wrote:
> > > hi all
> > >
> > > i dont understand the error message that is produced by the optim
> > > function. can anybody help???
> > >
> > > ie:
> > > [[1]]$message
> > > [1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
> > >
> > > can anyone help?
> >
> > That code indicates that the optimizer has declared convergence
> > because the relative reduction in the objective function in successive
> > iterates is below a tolerance.  As documented in ?optim, a convergence
> > code of 0 indicates success
> >
> > ...
> > convergence: An integer code. '0' indicates successful convergence.
> >           Error codes are
> > ...
> >
> > This may be counter-intuitive but it does make sense to shell
> > programmers.  The idea is that there is only one way you can succeed
> > but there are many different ways of failing so you use the nonzero
> > codes to indicate the types of failure and the zero code, which we
> > usually read as FALSE in a logical context, to indicate success.
> >
> > >
> > >
> > >
> > > ###########################################################################
> > >
> > > SK.FIT(XDATA=a,XDATAname="a",PHI1=1,v=5,vlo=2,vhi=300,phi2lo=.01)
> > > [[1]]
> > > [[1]]$par
> > > [1]  -0.01377906   0.83859445   0.34675230 300.00000000
> > >
> > > [[1]]$value
> > > [1] 90.59185
> > >
> > > [[1]]$counts
> > > function gradient
> > >       53       53
> > >
> > > [[1]]$convergence
> > > [1] 0
> > >
> > > [[1]]$message
> > > [1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
> > >
> > > #################################################################################
> > >
> > >
> > >
> > > i ghave included the function used in the optim call:
> > >
> > > SKEWMLE=function(l,DATA=XDATA,...)
> > >         {
> > >                 #alpha = l[1]
> > >                 #beta = l[2]
> > >                 #phi2 = l[3]
> > >                 #v= l[4]
> > >                 phi1=PHI1
> > >
> > >                 DATA<-as.matrix(DATA)
> > >
> > >                 fnew<-function(x,y,l,...)
> > >                 {
> > >                         #when we do not estimate phi1
> > >                         t1=(1+((y-l[1]-l[2]*x)^2)/(l[4]*l[3]^2))^(-0.5*(1+l[4]))
> > >                         t2=(1+(x^2)/l[4])^(-0.5*(1+l[4]))
> > >                         t3=2*((gamma(0.5*(1+l[4]))/(gamma(0.5*l[4])*sqrt(l[4]*pi)))^2)/l[3]
> > >
> > >                         t1*t2*t3
> > >                 }
> > >
> > >                 a<-double(length(DATA))
> > >                 y=DATA
> > >                 a=apply(y,1,function(q)
> > > log(integrate(fnew,lower=0,upper=Inf,y=q,l=l)$value))
> > >                 -sum(a)
> > >         }
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> > >
> 
>   ------------------------------------------------------------------------
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

From sundar.dorai-raj at pdf.com  Wed Sep  7 15:09:39 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 07 Sep 2005 08:09:39 -0500
Subject: [R] Lattice key seems to ignore the key list
In-Reply-To: <20050907045440.GA12344@hortresearch.co.nz>
References: <20050907045440.GA12344@hortresearch.co.nz>
Message-ID: <431EE693.60109@pdf.com>



Patrick Connolly wrote:
> I've never had this problem before and can't see what could be
> different from other times I've used keys with lattice.
> 
> 
> 
> It appears that auto.key is being taken as TRUE when I specify a key
> list.  The list I specify seems to be ignored.
> 
> Where can I place a browser to figure out what is going on?
> 
> Having made a list key.symbol from trellis.par.get, and specified a
> scales list and a between list, and a formula object (form), I use
> xyplot like this:
> 
> xyplot(form, data = xx, groups = Entry, layout = c(8,8, 1), 
>                 par.strip.text = list(cex = .65), between = between,
>                 scales = scales,
>                 panel = function(x, y, ...)
>                   panel.superpose(x, y, ...),
>                 key = list(points = Rows(key.symbol, 1:4),
>                   text = list(levels(xx$Entry),
>                     space = "right", columns = 1))
>                 )
> 
> What is implied in there that would set auto.key to TRUE?  The space
> and columns part of the list seems to be ignored and the autokey
> values substituted.  
> 
> Ideas, please.
> 
> Thanks.
> 

Hi, Patrick,

You have "space" and "column" in your "text" list. I.e.

text = list(levels(xx$Entry), space = "right", columns = 1))

This should be

text = list(levels(xx$Entry)), space = "right", columns = 1)

Note the placement of the parantheses.

--sundar



From stephane.mattei at epfl.ch  Wed Sep  7 15:28:45 2005
From: stephane.mattei at epfl.ch (=?iso-8859-1?b?U3TpcGhhbmU=?= Mattei)
Date: Wed,  7 Sep 2005 15:28:45 +0200
Subject: [R] Plot of multiple data sets
Message-ID: <1126099725.431eeb0d43dcb@imapwww.epfl.ch>

Hello !


There is something quite simple I want to do with R but I found nowhere in the help how to do it.
I just want to plot data which are in a matrix, every column being a data set and having the same
x-axis (just an index).

So for example if I have a 50 x 6 matrix I want 6 set of points on the same plot.

I tried
plot,new()
plot(MATRIX[,1])
plot(MATRIX[,2])
...

but it replaces the previous plot each time.


Thank you very much if you can help !



From murdoch at stats.uwo.ca  Wed Sep  7 15:37:18 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 07 Sep 2005 09:37:18 -0400
Subject: [R] Plot of multiple data sets
In-Reply-To: <1126099725.431eeb0d43dcb@imapwww.epfl.ch>
References: <1126099725.431eeb0d43dcb@imapwww.epfl.ch>
Message-ID: <431EED0E.1000303@stats.uwo.ca>

On 9/7/2005 9:28 AM, St??phane Mattei wrote:
> Hello !
> 
> 
> There is something quite simple I want to do with R but I found nowhere in the help how to do it.
> I just want to plot data which are in a matrix, every column being a data set and having the same
> x-axis (just an index).
> 
> So for example if I have a 50 x 6 matrix I want 6 set of points on the same plot.
> 
> I tried
> plot,new()
> plot(MATRIX[,1])
> plot(MATRIX[,2])
> ...
> 
> but it replaces the previous plot each time.

See ?matplot.  For example,

matplot(1:10, cbind(rnorm(10),rnorm(10)), type='l')



From chrisb at fcdarwin.org.ec  Wed Sep  7 14:40:51 2005
From: chrisb at fcdarwin.org.ec (Chris Buddenhagen)
Date: Wed, 7 Sep 2005 07:40:51 -0500
Subject: [R] references in the manual to endnote and spanish versions of
	everything
In-Reply-To: <8d5a3635050906104346501e11@mail.gmail.com>
Message-ID: <000001c5b3a9$6310af90$4c01a8c0@Chris>



Chris Buddenhagen, Botany Department, Charles Darwin Research Station, Santa
Cruz,Galapagos. Mail: Charles Darwin Foundation, Casilla 17-01-3891 Avenida
6 de Diciembre N36-109 y Pasaje California Quito, ECUADOR
 
Dear all

1) I have been really pleased with R as a means of doing and learning
statistics. I work in a Spanish speaking country- and I wanted to pass on
the benefits of R to my Spanish speaking colleagues. There are a couple of
introductions to R in Spanish but I could only find Portuguese installation
option, is there a Spanish version of the fullref manual or the program?

2) You can some references out of R and the help files in BibTex or LaTex or
something, does anyone have the refs entered into endnote? Can you extract
them into something readable by endnote?

Chris Buddenhagen, Botany Department, Charles Darwin Research Station, Santa
Cruz, Galapagos
Mailing address: Charles Darwin Foundation Casilla 17-01-3891 Avenida 6 de
Diciembre N36-109 y Pasaje California Quito, ECUADOR






______________________________________________________________________
EL CONTENIDO DE ESTE MENSAJE ES DE ABSOLUTA RESPONSABILIDAD DEL AUTOR.
FUNDACION CHARLES DARWIN
WWW.DARWINFOUNDATION.ORG



From francoisromain at free.fr  Wed Sep  7 15:45:20 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 07 Sep 2005 15:45:20 +0200
Subject: [R] Plot of multiple data sets
In-Reply-To: <1126099725.431eeb0d43dcb@imapwww.epfl.ch>
References: <1126099725.431eeb0d43dcb@imapwww.epfl.ch>
Message-ID: <431EEEF0.80204@free.fr>

Le 07.09.2005 15:28, St??phane Mattei a ??crit :

>Hello !
>
>
>There is something quite simple I want to do with R but I found nowhere in the help how to do it.
>I just want to plot data which are in a matrix, every column being a data set and having the same
>x-axis (just an index).
>
>So for example if I have a 50 x 6 matrix I want 6 set of points on the same plot.
>
>I tried
>plot,new()
>plot(MATRIX[,1])
>plot(MATRIX[,2])
>...
>
>but it replaces the previous plot each time.
>  
>
Hello Stephane,

there is a lot of solutions for your problem. It's up to you :

?matplot
?points
?lines
par(new=TRUE)
http://addictedtor.free.fr/graphiques/search.php?q=parallel

Good day.

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From h.andersson at nioo.knaw.nl  Wed Sep  7 15:39:14 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Wed, 07 Sep 2005 15:39:14 +0200
Subject: [R] Plot of multiple data sets
In-Reply-To: <1126099725.431eeb0d43dcb@imapwww.epfl.ch>
References: <1126099725.431eeb0d43dcb@imapwww.epfl.ch>
Message-ID: <dfmqlu$999$1@sea.gmane.org>

Have a look at ?matplot

St??phane Mattei wrote:
> Hello !
> 
> 
> There is something quite simple I want to do with R but I found nowhere in the help how to do it.
> I just want to plot data which are in a matrix, every column being a data set and having the same
> x-axis (just an index).
> 
> So for example if I have a 50 x 6 matrix I want 6 set of points on the same plot.
> 
> I tried
> plot,new()
> plot(MATRIX[,1])
> plot(MATRIX[,2])
> ...
> 
> but it replaces the previous plot each time.
> 
> 
> Thank you very much if you can help !
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From bdonner2 at yahoo.com  Wed Sep  7 15:48:06 2005
From: bdonner2 at yahoo.com (Bill Donner)
Date: Wed, 7 Sep 2005 06:48:06 -0700 (PDT)
Subject: [R] Hotelling Test
Message-ID: <20050907134806.93457.qmail@web35706.mail.mud.yahoo.com>

Hello R-users,

I've been looking for a function performing one and two sample Hotelling
test for testing equality of mean vectors. Has anyone implemented such a
function in R?


thanks a lot,

Bill

==============
Bill Donner
Statistician



From stephane.mattei at epfl.ch  Wed Sep  7 16:00:29 2005
From: stephane.mattei at epfl.ch (=?iso-8859-1?b?U3TpcGhhbmU=?= Mattei)
Date: Wed,  7 Sep 2005 16:00:29 +0200
Subject: [R] Plot of multiple data sets
Message-ID: <1126101629.431ef27db93a9@imapwww.epfl.ch>

Thank you all for your answers.

I eventually use the points command

plot(MATRIX[,1])
points(MATRIX[,2])
points(MATRIX[,3])
...

with matplot I had numbers instead of points with type="p" and par(new=TRUE) makes complications
with the axis.



From chaix at u707.jussieu.fr  Wed Sep  7 16:01:46 2005
From: chaix at u707.jussieu.fr (Basile Chaix)
Date: Wed, 7 Sep 2005 16:01:46 +0200
Subject: [R] Survival analysis with COXPH
Message-ID: <20050907140057.69D769BB55@mailer.b3e.jussieu.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050907/7e7a949c/attachment.pl

From ggrothendieck at gmail.com  Wed Sep  7 16:02:53 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Sep 2005 10:02:53 -0400
Subject: [R] Plot of multiple data sets
In-Reply-To: <1126099725.431eeb0d43dcb@imapwww.epfl.ch>
References: <1126099725.431eeb0d43dcb@imapwww.epfl.ch>
Message-ID: <971536df05090707024597b8bc@mail.gmail.com>

On 9/7/05, St??phane Mattei <stephane.mattei at epfl.ch> wrote:
> Hello !
> 
> 
> There is something quite simple I want to do with R but I found nowhere in the help how to do it.
> I just want to plot data which are in a matrix, every column being a data set and having the same
> x-axis (just an index).
> 
> So for example if I have a 50 x 6 matrix I want 6 set of points on the same plot.
> 
> I tried
> plot,new()
> plot(MATRIX[,1])
> plot(MATRIX[,2])
> ...
> 
> but it replaces the previous plot each time.

Others have already mentioned matplot, lines and points but just to add
to the list, if your problem is a time series then you could also use
plot.zoo:

library(zoo)
z <- zoo(MATRIX, x)
plot(z, plot.type ="single")  # one plot
plot(z) # separate plots



From tlumley at u.washington.edu  Wed Sep  7 16:06:08 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 7 Sep 2005 07:06:08 -0700 (PDT)
Subject: [R] references in the manual to endnote and spanish versions of
 everything
In-Reply-To: <000001c5b3a9$6310af90$4c01a8c0@Chris>
References: <000001c5b3a9$6310af90$4c01a8c0@Chris>
Message-ID: <Pine.A41.4.63a.0509070657290.164894@homer12.u.washington.edu>

On Wed, 7 Sep 2005, Chris Buddenhagen wrote:
>
> 1) I have been really pleased with R as a means of doing and learning
> statistics. I work in a Spanish speaking country- and I wanted to pass on
> the benefits of R to my Spanish speaking colleagues. There are a couple of
> introductions to R in Spanish but I could only find Portuguese installation
> option, is there a Spanish version of the fullref manual or the program?

Not at the moment.  The translations rely very heavily on volunteers who 
actually know the languages in question. Spanish-speaking users have 
contributed documentation, but we have no translation teams for any 
version of Spanish.

> 2) You can some references out of R and the help files in BibTex or LaTex or
> something, does anyone have the refs entered into endnote? Can you extract
> them into something readable by endnote?

There are bibtex to endnote converters out there on the web
One is at
  http://www.cs.usyd.edu.au/~tapted/bib2endnote.html


 	-thomas



From dimitris.rizopoulos at med.kuleuven.be  Wed Sep  7 16:08:16 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 7 Sep 2005 16:08:16 +0200
Subject: [R] Hotelling Test
References: <20050907134806.93457.qmail@web35706.mail.mud.yahoo.com>
Message-ID: <00eb01c5b3b5$979b0a60$0540210a@www.domain>

some time ago I've written a function for the Hotelling test, maybe 
you could give it a try:

hotel.test <- function(x, y = NULL, mu = 0) {
    if(!is.numeric(x) || !is.matrix(x))
        stop("'x' must be a numeric matrix")
    n <- nrow(x)
    p <- ncol(x)
    xbar <- colMeans(x, na.rm = TRUE)
    if(!is.numeric(mu) || ((lmu <- length(mu)) > 1 & lmu != p))
        stop("'mu' must be a numeric vector of length ", p)
    if(lmu == 1)
        mu <- rep(mu, p)
    xbar.mu <- xbar - mu
    V <- var(x, na.rm = TRUE)
    out <- if(is.null(y)){
        k <- n / (n - 1) * (n - p) / p
        stat <- k * crossprod(xbar.mu, solve(V, xbar.mu))[1, ]
        pvalue <- 1 - pf(stat, p, n - p)
        list(statistic = stat, parameter = c(p, n - p), p.value = 
pvalue, estimate = xbar,
                null.value = mu, alternative = "two-sided", method = 
"Hotelling one sample test",
                data.name = deparse(substitute(x)))
    } else{
        if(!is.numeric(y) || !is.matrix(y))
            stop("'y' must be a numeric matrix")
        if(ncol(y) != p)
            stop("incompatible arguments")
        ny <- nrow(y)
        k <- n + ny - p - 1
        k1 <- (n * ny) / (n + ny)
        k2 <- (n + ny - 2) * p
        dif.means <- xbar - colMeans(y, na.rm = TRUE)
        Vy <- var(y, na.rm = TRUE)
        V <- ((n - 1) * V + (ny - 1) * Vy) / (n + ny - 2)
        stat <- k * k1 * crossprod(dif.means, solve(V, 
dif.means))[1, ] / k2
        pvalue <- 1 - pf(stat, p, k)
        list(statistic = stat, parameter = c(p, k), p.value = pvalue, 
estimate = rbind(xbar, ybar = xbar - dif.means),
                null.value = NULL, alternative = "two-sided", method = 
"Hotelling two sample test",
                data.name = c(deparse(substitute(x)), 
deparse(substitute(y))))
    }
    class(out) <- "hotel"
    out
}

print.hotel <- function(x, digits = 3, ...){
    cat("\n\t", x$method, "\n\n")
    if(length(dnam <- x$data.name) == 1)
        cat("data:", dnam, "\n")
    else
        cat("data:", dnam[1],  "and", dnam[2], "\n")
    pval <- if(x$p.value < 1e-04) "< 1e-04" else paste("= ", 
round(x$p.value, 4), sep = "")
    cat("t = ", x$statistic, ", df1 = ", x$parameter[1], ", df2 = ", 
x$parameter[2],
            ", p-value ", pval, "\n", sep = "")
    if(!is.null(null <- x$null.value))
        cat("alternative hypothesis: true mean vector is not equal 
to",
                paste("(", paste(round(null, digits), collapse = ", 
"), ")'", sep = ""), "\n")
    else
        cat("alternative hypothesis: true difference in mean vectors 
is not equal to 0\n")
    cat("sample estimates:")
    if(!is.matrix(est <- x$estimate))
        cat("\nmean x-vector", paste("(", paste(round(est, digits), 
collapse = ", "), ")'", sep = ""), "\n")
    else{
        rownames(est) <- c("mean x-vector", "mean y-vector")
        if(is.null(colnames(est)))
            colnames(est) <- rep("", ncol(est))
        print(round(est, digits))
    }
    cat("\n")
    invisible(x)
}


# Examples

mat <- matrix(rnorm(100 * 3), 100, 3)
mat2 <- matrix(rnorm(100 * 3), 100, 3)

hotel.test(mat)
hotel.test(mat, mu = -1:1)

hotel.test(mat, y = mat2)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm




----- Original Message ----- 
From: "Bill Donner" <bdonner2 at yahoo.com>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, September 07, 2005 3:48 PM
Subject: [R] Hotelling Test


> Hello R-users,
>
> I've been looking for a function performing one and two sample 
> Hotelling
> test for testing equality of mean vectors. Has anyone implemented 
> such a
> function in R?
>
>
> thanks a lot,
>
> Bill
>
> ==============
> Bill Donner
> Statistician
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Moshe.Olshansky at brevanhoward.com  Wed Sep  7 16:08:30 2005
From: Moshe.Olshansky at brevanhoward.com (Olshansky,Moshe)
Date: Wed, 7 Sep 2005 15:08:30 +0100
Subject: [R] solving a system of nonlinear equations
Message-ID: <7B0ABB9DDAFDF04B8BE51CA9ECA2CB70B5CA@bhmail1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050907/962d74ed/attachment.pl

From tlumley at u.washington.edu  Wed Sep  7 16:11:46 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 7 Sep 2005 07:11:46 -0700 (PDT)
Subject: [R] Survival analysis with COXPH
In-Reply-To: <20050907140057.69D769BB55@mailer.b3e.jussieu.fr>
References: <20050907140057.69D769BB55@mailer.b3e.jussieu.fr>
Message-ID: <Pine.A41.4.63a.0509070706380.164894@homer12.u.washington.edu>

On Wed, 7 Sep 2005, Basile Chaix wrote:

> Dear all,
> I would have some questions on the coxph function for survival analysis,
> which I use with frailty terms.
>
> My model is:
> mdcox<-coxph(Surv(time,censor)~ gender + age + frailty(area, dist='gauss'),
> data)
> I have a very large proportion of censored observations.
>
> - If I understand correctly, the function mdcox$frail will return the random
> effect estimated for each group on the scale of the linear predictor?

Yes

> - Similarly, the variance of the frailties is the variance of the terms on
> the scale of the linear predictor?

Yes

> - A p-value is provided for this variance. Is that possible to obtain a 95%
> CI for the variance of the random effect instead of a p-value?

I don't think anyone knows how to do this.  Personally, I'm not really 
convinced of the usefulness of these frailty models and I don't know how 
well their properties are known.  I wouldn't use them except when I was 
actually interested in the variance components, and I haven't worked on 
any problems like that, so I haven't investigated the issue.

[I don't write the survival package, I just port it]


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ggrothendieck at gmail.com  Wed Sep  7 16:12:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Sep 2005 10:12:15 -0400
Subject: [R] irregular time series prediction
In-Reply-To: <1126088398.431ebece6b42c@imp3-g19.free.fr>
References: <1126088398.431ebece6b42c@imp3-g19.free.fr>
Message-ID: <971536df0509070712298f33fa@mail.gmail.com>

On 9/7/05, jfontain at free.fr <jfontain at free.fr> wrote:
> Hello.
> 
> This is my first post, so allow me to introduce myself.
> 
> But first, I'd like to thank all the authors and contributors to the R software,
> as I think that it is truly a great and very useful package.
> 
> I am the author of moodss, a GPL modular monitoring application
> (http://moodss.sourceforge.net). Moodss collects, archives in a SQL database
> and displays data from monitored devices, mostly computers, databases and
> network equipment.
> 
> My idea is to use the stored data to perform predictions for capacity planning
> purposes. For example, based on the trafic on a network line for the last 12
> months, what is the expected evolution in the next 3 months.
> 
> Since there is no guarantee that the data samples are regularly spaced in time,
> I was thinking of using the "its" package for a start.
> But data samples, most of the time, are roughly regularly spaced. For example,
> the monitored network device could return data every 10 seconds, but sometimes
> at 11 seconds or 9 seconds after the last sample. So another idea would be to
> normalize the data (by interpolation maybe) to make it a regular time series as
> a first step.
> 
> All I need from you at this time is to point me in the right direction, maybe
> suggest resources on the web about this subject applied to R, knowing that I
> would prefer to use only R base packages if possible.
> 
> I apologize if this is a trivial question, but last time I studied statistics
> was more than 20 years ago, so I need a little time to warm up...
> 

The zoo package has support for irregular series and weakly regular
series (classes 'zoo' and 'zooreg') where the latter are series that have 
an underlying regularity, e.g. every 10 minutes, but there may be some 
missing ones.   Also you might want to look at the article on dates
and times in R News 4/1.

library(zoo)
vignette("zoo")



From wilks at dial.pipex.com  Wed Sep  7 16:30:24 2005
From: wilks at dial.pipex.com (John Wilkinson (pipex))
Date: Wed, 7 Sep 2005 15:30:24 +0100
Subject: [R] FW: Re:  Doubt about nested aov output
Message-ID: <JCEIJNOHMNBPLMGFDHNDMEPGCAAA.wilks@dial.pipex.com>

Ronaldo,

Further to my previous posting  on your Glycogen nested aov model.

Having read Douglas Bates' response and  Reflected on his lmer analysis
output of your aov nested model example as given.The Glycogen treatment has
to be a Fixed Effect.If a 'treatment' isn't a Fixed Effect what is ? If
Douglas Bates' lmer model is modified to treat Glycogen Treatment as a
purely Fixed Effect,with Rat and the interaction Rat:Liver as random effects
then--

> model.lmer<-lmer(Glycogen~Treatment+(1|Rat)+(1|Rat:Liver))
> summary(model.lmer)
Linear mixed-effects model fit by REML
Formula: Glycogen ~ Treatment + (1 | Rat) + (1 | Rat:Liver)
     AIC      BIC    logLik MLdeviance REMLdeviance
 239.095 248.5961 -113.5475   238.5439      227.095
Random effects:
 Groups    Name        Variance   Std.Dev.
 Rat:Liver (Intercept) 2.1238e-08 0.00014573
 Rat       (Intercept) 2.0609e+01 4.53976242
 Residual              4.2476e+01 6.51733769
# of obs: 36, groups: Rat:Liver, 6; Rat, 2

Fixed effects:
            Estimate Std. Error DF t value  Pr(>|t|)
(Intercept) 140.5000     3.7208 33 37.7607 < 2.2e-16 ***
Treatment2   10.5000     2.6607 33  3.9463 0.0003917 ***
Treatment3   -5.3333     2.6607 33 -2.0045 0.0532798 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
           (Intr) Trtmn2
Treatment2 -0.358
Treatment3 -0.358  0.500

> anova(model.lmer)
Analysis of Variance Table
          Df  Sum Sq Mean Sq   Denom F value    Pr(>F)
Treatment  2 1557.56  778.78   33.00  18.335 4.419e-06 ***
 --------------------------------------------------------
which agrees with the aov model below.

>  model <- aov(Glycogen~Treatment+Error(Rat/Liver))
> summary(model)


John

-----Original Message-----
From: John Wilkinson (pipex) [mailto:wilks at dial.pipex.com]
Sent: 07 September 2005 12:04 PM
To: "Ronaldo Reis-Jr."
Cc: r-help
Subject: Re: [R] Doubt about nested aov output


Ronaldo ,

It looks as though you have specified your model incorrectly.

In the Rats example ,the Treatment is the only fixed effect,Rat and Liver
are random effects

In aov testing for sig of 'Means' of Random Effects is pointless and that is
why 'p' values are not given.Further more the interaction between a Random
Effect and a Fixed Effect is also a Random Effect. The 'aov' with error
structure terms output reflects this by only giving 'p' values to
Fixed Effects and their interactions

>  model <- aov(Glycogen~Treatment+Error(Rat/Liver))
> summary(model)

Error: Rat
          Df Sum Sq Mean Sq F value Pr(>F) #Rat is random effect
Residuals  1 413.44  413.44

Error: Rat:Liver				      #Rat:Liver is Random effect
          Df  Sum Sq Mean Sq F value Pr(>F)
Residuals  4 164.444  41.111

Error: Within
          Df  Sum Sq Mean Sq F value    Pr(>F)
Treatment  2 1557.56  778.78  18.251 8.437e-06 *** #Fixed effect
Residuals 28 1194.78   42.67
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



I hope that this is of help.

John



From stephane.mattei at epfl.ch  Wed Sep  7 16:24:58 2005
From: stephane.mattei at epfl.ch (=?iso-8859-1?b?U3TpcGhhbmU=?= Mattei)
Date: Wed,  7 Sep 2005 16:24:58 +0200
Subject: [R] Plot of multiple data sets
In-Reply-To: <971536df05090707024597b8bc@mail.gmail.com>
References: <1126099725.431eeb0d43dcb@imapwww.epfl.ch>
	<971536df05090707024597b8bc@mail.gmail.com>
Message-ID: <1126103098.431ef83abe5c8@imapwww.epfl.ch>

Selon Gabor Grothendieck <ggrothendieck at gmail.com>:

> On 9/7/05, St??phane Mattei <stephane.mattei at epfl.ch> wrote:
> > Hello !
> >
> >
> > There is something quite simple I want to do with R but I found nowhere in the help how to do
> it.
> > I just want to plot data which are in a matrix, every column being a data set and having the
> same
> > x-axis (just an index).
> >
> > So for example if I have a 50 x 6 matrix I want 6 set of points on the same plot.
> >
> > I tried
> > plot,new()
> > plot(MATRIX[,1])
> > plot(MATRIX[,2])
> > ...
> >
> > but it replaces the previous plot each time.
>
> Others have already mentioned matplot, lines and points but just to add
> to the list, if your problem is a time series then you could also use
> plot.zoo:
> library(zoo)
> z <- zoo(MATRIX, x)
> plot(z, plot.type ="single")  # one plot
> plot(z) # separate plots
>
Thank you very much Gabor!
Your solution is best because with points it doesn't adapt the axis. But it's funny it needs an
extra-package for such a simple thing ! I thought that could be done with just the plot command



From sundar.dorai-raj at pdf.com  Wed Sep  7 16:25:55 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 07 Sep 2005 09:25:55 -0500
Subject: [R] solving a system of nonlinear equations
In-Reply-To: <7B0ABB9DDAFDF04B8BE51CA9ECA2CB70B5CA@bhmail1>
References: <7B0ABB9DDAFDF04B8BE51CA9ECA2CB70B5CA@bhmail1>
Message-ID: <431EF873.8040506@pdf.com>



Olshansky,Moshe wrote:
> What is the "classic" R function for solving a (possibly over
> determined) system of non-linear equations?
> 
>  
> 
> Thank you!
> 
>  
> 
> Moshe Olshansky
> 
> e-mail:  moshe.olshansky at brevanhoward.com
> 
>  
> 

I'm not sure what your definition of '"classic"' is, but there are 
several options in R:

?optim
?nls
?nlm

Or better yet, try RSiteSearch("nonlinear system of equations"). This 
gave me 43 hits, many of which are directly related to your question.

--sundar



From uofiowa at gmail.com  Wed Sep  7 16:47:43 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 7 Sep 2005 10:47:43 -0400
Subject: [R] variables from command line
Message-ID: <3f87cc6d0509070747503f3f1b@mail.gmail.com>

How can I pass parameters to an R script from the command line. And
how can I read them from within the script?

This is how I want to invoke the script:
R CMD BATCH r.in r.out  <input values>

The script with read in the input values, process them and spit the
output to r.out.



From francoisromain at free.fr  Wed Sep  7 17:09:07 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 07 Sep 2005 17:09:07 +0200
Subject: [R] Plot of multiple data sets
In-Reply-To: <1126101629.431ef27db93a9@imapwww.epfl.ch>
References: <1126101629.431ef27db93a9@imapwww.epfl.ch>
Message-ID: <431F0293.1010203@free.fr>

Le 07.09.2005 16:00, St??phane Mattei a ??crit :

>Thank you all for your answers.
>
>I eventually use the points command
>
>plot(MATRIX[,1])
>points(MATRIX[,2])
>points(MATRIX[,3])
>...
>
>with matplot I had numbers instead of points with type="p" and par(new=TRUE) makes complications
>with the axis.
>  
>
So, add pch=21 or something else to your matplot call and you won't need 
an extra package for that.

R> matplot(iris[,1:4], type="p", pch=21)

Also, if you want to do it with plot and points, you must specify the y 
range like that :

R> plot(MATRIX[,1], ylim=range(MATRIX))
R> points(MATRIX[,2])
R> points(MATRIX[,3])

Proceeding that way, par(new=TRUE) won't make complications anymore.

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From petr.pikal at precheza.cz  Wed Sep  7 17:21:58 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 07 Sep 2005 17:21:58 +0200
Subject: [R] Plot of multiple data sets
In-Reply-To: <1126103098.431ef83abe5c8@imapwww.epfl.ch>
References: <971536df05090707024597b8bc@mail.gmail.com>
Message-ID: <431F21B6.13860.201422A@localhost>

Hi

what is wrong with matplot

sines <- outer(1:20, 1:4, function(x, y) sin(x / 20 * pi * y))
matplot(sines, pch = 1:4, type = "o", col = rainbow(ncol(sines)))

so you can use aditional parameters to exactly specify what type of 
point and/or line and in what colour you will plot.

Or with plot/points construction you can adapt xlim and ylim 
setting to suite the whole plotting range e.g..

ddd<-dim(sines)[1] # length of x axis

plot(1:ddd,rep(1,ddd), type="n", ylim=range(sines)) # y in propper 
range

for (i in 1:4) points(1:ddd,sines[,i], pch = i, type = "b", col = 
rainbow(ncol(sines))[i]) # 4 columns plotted

If you want do more complicated things you usually has to use 
more complicated constructions.

BTW

help.search("plot column") goes stright to matplot and from its 
help page you could learn how to use it.

HTH
Petr




On 7 Sep 2005 at 16:24, St??phane Mattei wrote:

> Selon Gabor Grothendieck <ggrothendieck at gmail.com>:
> 
> > On 9/7/05, St??phane Mattei <stephane.mattei at epfl.ch> wrote:
> > > Hello !
> > >
> > >
> > > There is something quite simple I want to do with R but I found
> > > nowhere in the help how to do
> > it.
> > > I just want to plot data which are in a matrix, every column being
> > > a data set and having the
> > same
> > > x-axis (just an index).
> > >
> > > So for example if I have a 50 x 6 matrix I want 6 set of points on
> > > the same plot.
> > >
> > > I tried
> > > plot,new()
> > > plot(MATRIX[,1])
> > > plot(MATRIX[,2])
> > > ...
> > >
> > > but it replaces the previous plot each time.
> >
> > Others have already mentioned matplot, lines and points but just to
> > add to the list, if your problem is a time series then you could
> > also use plot.zoo: library(zoo) z <- zoo(MATRIX, x) plot(z,
> > plot.type ="single")  # one plot plot(z) # separate plots
> >
> Thank you very much Gabor!
> Your solution is best because with points it doesn't adapt the axis.
> But it's funny it needs an extra-package for such a simple thing ! I
> thought that could be done with just the plot command
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From uofiowa at gmail.com  Wed Sep  7 17:22:35 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 7 Sep 2005 11:22:35 -0400
Subject: [R] using system()
Message-ID: <3f87cc6d050907082243015763@mail.gmail.com>

Using system() is theer a way to make the R interpreter not wait for
the command to finish?



From thpe at hhbio.wasser.tu-dresden.de  Wed Sep  7 17:30:54 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 07 Sep 2005 17:30:54 +0200
Subject: [R] using system()
In-Reply-To: <3f87cc6d050907082243015763@mail.gmail.com>
References: <3f87cc6d050907082243015763@mail.gmail.com>
Message-ID: <431F07AE.8010104@hhbio.wasser.tu-dresden.de>

Omar Lakkis schrieb:
> Using system() is theer a way to make the R interpreter not wait for
> the command to finish?

system("cmd", wait=FALSE)

see ?system in online help.

Thomas P.



From vincent.goulet at act.ulaval.ca  Wed Sep  7 17:43:41 2005
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Wed, 7 Sep 2005 11:43:41 -0400
Subject: [R] Avoid Sweave from stopping on errors
Message-ID: <200509071143.41666.vincent.goulet@act.ulaval.ca>

Hi all,

Is there an option in Sweave to avoid it from stopping on a code chunk with an 
error? (I purposefully want to include code with an error in class notes.)

I suspect the answer is "no" and that I will be pointed to options("error"). 
That'd be fine, but which error parameter will just "do nothing" in case of 
an error?

Thanks in advance!

-- 
  Vincent Goulet, Associate Professor
  ??cole d'actuariat
  Universit?? Laval, Qu??bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca



From ggrothendieck at gmail.com  Wed Sep  7 17:45:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Sep 2005 11:45:32 -0400
Subject: [R] using system()
In-Reply-To: <431F07AE.8010104@hhbio.wasser.tu-dresden.de>
References: <3f87cc6d050907082243015763@mail.gmail.com>
	<431F07AE.8010104@hhbio.wasser.tu-dresden.de>
Message-ID: <971536df05090708457d83b5c8@mail.gmail.com>

On 9/7/05, Thomas Petzoldt <thpe at hhbio.wasser.tu-dresden.de> wrote:
> Omar Lakkis schrieb:
> > Using system() is theer a way to make the R interpreter not wait for
> > the command to finish?
> 
> system("cmd", wait=FALSE)
> 
> see ?system in online help.
> 


Coincidentially I recently posted on r-devel a note pointing out that
system is not the same on all operating systems.
https://www.stat.math.ethz.ch/pipermail/r-devel/2005-September/034539.html

In particular, wait= is available on Windows but not on UNIX.
On UNIX you could & in the usual way.



From gerifalte28 at hotmail.com  Wed Sep  7 18:11:54 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 07 Sep 2005 16:11:54 +0000
Subject: [R] Hotelling Test
In-Reply-To: <20050907134806.93457.qmail@web35706.mail.mud.yahoo.com>
Message-ID: <BAY103-F3303BA177AC1536EAA9E4AA6A60@phx.gbl>

Check some of the threads at RSiteSearch("Hotelling")

Cheers

Francisco

>From: Bill Donner <bdonner2 at yahoo.com>
>To: R-help at stat.math.ethz.ch
>Subject: [R] Hotelling Test
>Date: Wed, 7 Sep 2005 06:48:06 -0700 (PDT)
>
>Hello R-users,
>
>I've been looking for a function performing one and two sample Hotelling
>test for testing equality of mean vectors. Has anyone implemented such a
>function in R?
>
>
>thanks a lot,
>
>Bill
>
>==============
>Bill Donner
>Statistician
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Sep  7 18:27:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Sep 2005 18:27:07 +0200
Subject: [R] Hotelling Test
In-Reply-To: <BAY103-F3303BA177AC1536EAA9E4AA6A60@phx.gbl>
References: <BAY103-F3303BA177AC1536EAA9E4AA6A60@phx.gbl>
Message-ID: <x2d5nksv1g.fsf@turmalin.kubism.ku.dk>

"Francisco J. Zagmutt" <gerifalte28 at hotmail.com> writes:

> Check some of the threads at RSiteSearch("Hotelling")

Or use anova(lm(X~g), test="Hotelling"), where X is the matrix of
responses and g is the grouping factor.
 
> Cheers
> 
> Francisco
> 
> >From: Bill Donner <bdonner2 at yahoo.com>
> >To: R-help at stat.math.ethz.ch
> >Subject: [R] Hotelling Test
> >Date: Wed, 7 Sep 2005 06:48:06 -0700 (PDT)
> >
> >Hello R-users,
> >
> >I've been looking for a function performing one and two sample Hotelling
> >test for testing equality of mean vectors. Has anyone implemented such a
> >function in R?
> >
> >
> >thanks a lot,
> >
> >Bill
> >
> >==============
> >Bill Donner
> >Statistician
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Wed Sep  7 18:35:17 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Sep 2005 18:35:17 +0200
Subject: [R] Hotelling Test
In-Reply-To: <x2d5nksv1g.fsf@turmalin.kubism.ku.dk>
References: <BAY103-F3303BA177AC1536EAA9E4AA6A60@phx.gbl>
	<x2d5nksv1g.fsf@turmalin.kubism.ku.dk>
Message-ID: <x28xy8sunu.fsf@turmalin.kubism.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> "Francisco J. Zagmutt" <gerifalte28 at hotmail.com> writes:
> 
> > Check some of the threads at RSiteSearch("Hotelling")
> 
> Or use anova(lm(X~g), test="Hotelling"), where X is the matrix of
> responses and g is the grouping factor.

Oops, sorry. That is in r-devel only. For R 2.1.1 you need an explicit
comparison of models:

anova(lm(X~g), lm(X~1), test="Hotelling")

For the one-sample test, you probably need to sweep() out the
H0 column mean vector and compare X~1 to X~-1 .
  
> > Cheers
> > 
> > Francisco
> > 
> > >From: Bill Donner <bdonner2 at yahoo.com>
> > >To: R-help at stat.math.ethz.ch
> > >Subject: [R] Hotelling Test
> > >Date: Wed, 7 Sep 2005 06:48:06 -0700 (PDT)
> > >
> > >Hello R-users,
> > >
> > >I've been looking for a function performing one and two sample Hotelling
> > >test for testing equality of mean vectors. Has anyone implemented such a
> > >function in R?
> > >
> > >
> > >thanks a lot,
> > >
> > >Bill
> > >
> > >==============
> > >Bill Donner
> > >Statistician
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide! 
> > >http://www.R-project.org/posting-guide.html
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From kingroi at hotmail.com  Wed Sep  7 18:55:39 2005
From: kingroi at hotmail.com (paul king)
Date: Wed, 07 Sep 2005 16:55:39 +0000
Subject: [R] Course***R/Splus Programming Techniques***New York,
	September 2005
Message-ID: <BAY107-F214918383E0C3F24BFC235B0A60@phx.gbl>

FYI

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" in New York City.
www.xlsolutions-corp.com/Rfund.htm


**** New York City ------------------------ September 22nd-23rd, 2005

Reserve your seat now at the early bird rates!
Payment due AFTER the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava

Interested in R/Splus Advanced course? email us.

Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com

_________________________________________________________________
Don’t just search. Find. Check out the new MSN Search!



From HDoran at air.org  Wed Sep  7 19:22:31 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 7 Sep 2005 13:22:31 -0400
Subject: [R] Avoid Sweave from stopping on errors
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A0A3F02@dc1ex2.air.org>

You could use <<eval=FALSE>>= and the code inside the chunk will not be evaluated. I suppose two other options could be to comment out the bad code inside the code chunk or to use verbatim to make it look like a code chunk in your output. 

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vincent Goulet
Sent: Wednesday, September 07, 2005 11:44 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Avoid Sweave from stopping on errors

Hi all,

Is there an option in Sweave to avoid it from stopping on a code chunk with an error? (I purposefully want to include code with an error in class notes.)

I suspect the answer is "no" and that I will be pointed to options("error"). 
That'd be fine, but which error parameter will just "do nothing" in case of an error?

Thanks in advance!

--
  Vincent Goulet, Associate Professor
  ??cole d'actuariat
  Universit?? Laval, Qu??bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Wed Sep  7 19:36:23 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 7 Sep 2005 12:36:23 -0500
Subject: [R] Avoid Sweave from stopping on errors
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F740A0A3F02@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A3F02@dc1ex2.air.org>
Message-ID: <40e66e0b05090710365939409e@mail.gmail.com>

On 9/7/05, Doran, Harold <HDoran at air.org> wrote:
> You could use <<eval=FALSE>>= and the code inside the chunk will not be evaluated. I suppose two other options could be to comment out the bad code inside the code chunk or to use verbatim to make it look like a code chunk in your output.
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vincent Goulet
> Sent: Wednesday, September 07, 2005 11:44 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Avoid Sweave from stopping on errors
> 
> Hi all,
> 
> Is there an option in Sweave to avoid it from stopping on a code chunk with an error? (I purposefully want to include code with an error in class notes.)
> 
> I suspect the answer is "no" and that I will be pointed to options("error").
> That'd be fine, but which error parameter will just "do nothing" in case of an error?
> 
> Thanks in advance!
> 

Another option is to wrap the call that will produce an error in
try().  You still get the error report, etc. but execution does not
stop.



From bill.shipley at usherbrooke.ca  Wed Sep  7 19:38:08 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Wed, 7 Sep 2005 13:38:08 -0400
Subject: [R] summary of problem with mCall function.
Message-ID: <003d01c5b3d2$e9318d50$a01ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050907/9344a3dd/attachment.pl

From dmbates at gmail.com  Wed Sep  7 19:54:04 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 7 Sep 2005 12:54:04 -0500
Subject: [R] FW: Re:  Doubt about nested aov output
In-Reply-To: <JCEIJNOHMNBPLMGFDHNDMEPGCAAA.wilks@dial.pipex.com>
References: <JCEIJNOHMNBPLMGFDHNDMEPGCAAA.wilks@dial.pipex.com>
Message-ID: <40e66e0b05090710542753ca4@mail.gmail.com>

On 9/7/05, John Wilkinson (pipex) <wilks at dial.pipex.com> wrote:
> Ronaldo,
> 
> Further to my previous posting  on your Glycogen nested aov model.
> 
> Having read Douglas Bates' response and  Reflected on his lmer analysis
> output of your aov nested model example as given.The Glycogen treatment has
> to be a Fixed Effect.If a 'treatment' isn't a Fixed Effect what is ? If
> Douglas Bates' lmer model is modified to treat Glycogen Treatment as a
> purely Fixed Effect,with Rat and the interaction Rat:Liver as random effects
> then--
> 
> > model.lmer<-lmer(Glycogen~Treatment+(1|Rat)+(1|Rat:Liver))
> > summary(model.lmer)
> Linear mixed-effects model fit by REML
> Formula: Glycogen ~ Treatment + (1 | Rat) + (1 | Rat:Liver)
>      AIC      BIC    logLik MLdeviance REMLdeviance
>  239.095 248.5961 -113.5475   238.5439      227.095
> Random effects:
>  Groups    Name        Variance   Std.Dev.
>  Rat:Liver (Intercept) 2.1238e-08 0.00014573
>  Rat       (Intercept) 2.0609e+01 4.53976242
>  Residual              4.2476e+01 6.51733769
> # of obs: 36, groups: Rat:Liver, 6; Rat, 2
> 
> Fixed effects:
>             Estimate Std. Error DF t value  Pr(>|t|)
> (Intercept) 140.5000     3.7208 33 37.7607 < 2.2e-16 ***
> Treatment2   10.5000     2.6607 33  3.9463 0.0003917 ***
> Treatment3   -5.3333     2.6607 33 -2.0045 0.0532798 .
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Correlation of Fixed Effects:
>            (Intr) Trtmn2
> Treatment2 -0.358
> Treatment3 -0.358  0.500
> 
> > anova(model.lmer)
> Analysis of Variance Table
>           Df  Sum Sq Mean Sq   Denom F value    Pr(>F)
> Treatment  2 1557.56  778.78   33.00  18.335 4.419e-06 ***
>  --------------------------------------------------------
> which agrees with the aov model below.
> 
> >  model <- aov(Glycogen~Treatment+Error(Rat/Liver))
> > summary(model)
> 
> 
> John
> 
> -----Original Message-----
> From: John Wilkinson (pipex) [mailto:wilks at dial.pipex.com]
> Sent: 07 September 2005 12:04 PM
> To: "Ronaldo Reis-Jr."
> Cc: r-help
> Subject: Re: [R] Doubt about nested aov output
> 
> 
> Ronaldo ,
> 
> It looks as though you have specified your model incorrectly.
> 
> In the Rats example ,the Treatment is the only fixed effect,Rat and Liver
> are random effects
> 
> In aov testing for sig of 'Means' of Random Effects is pointless and that is
> why 'p' values are not given.Further more the interaction between a Random
> Effect and a Fixed Effect is also a Random Effect. The 'aov' with error
> structure terms output reflects this by only giving 'p' values to
> Fixed Effects and their interactions
> 
> >  model <- aov(Glycogen~Treatment+Error(Rat/Liver))
> > summary(model)
> 
> Error: Rat
>           Df Sum Sq Mean Sq F value Pr(>F) #Rat is random effect
> Residuals  1 413.44  413.44
> 
> Error: Rat:Liver                                      #Rat:Liver is Random effect
>           Df  Sum Sq Mean Sq F value Pr(>F)
> Residuals  4 164.444  41.111
> 
> Error: Within
>           Df  Sum Sq Mean Sq F value    Pr(>F)
> Treatment  2 1557.56  778.78  18.251 8.437e-06 *** #Fixed effect
> Residuals 28 1194.78   42.67
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> 
> 
> I hope that this is of help.
> 
> John

The difference between models like
  lmer(Glycogen~Treatment+(1|Rat)+(1|Rat:Liver))
and
  lmer(Glycogen~Treatment+(1|Treatment:Rat)+(1|Treatment:Rat:Liver))

is more about the meaning of the levels of "Rat" than about the
meaning of "Treatment".  As I understood it there are three different
rats labelled 1.  There is a rat 1 on treatment 1 and a rat 1 on
treatment 2 and a rat 1 on treatment 3.  Thus the levels of Rat do not
designate the "experimental unit", it is the levels of Treatment:Rat
that do this.



From Roger.Bivand at nhh.no  Wed Sep  7 20:16:15 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 7 Sep 2005 20:16:15 +0200 (CEST)
Subject: [R] Avoid Sweave from stopping on errors
In-Reply-To: <40e66e0b05090710365939409e@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0509072010260.7066-100000@reclus.nhh.no>

On Wed, 7 Sep 2005, Douglas Bates wrote:

> On 9/7/05, Doran, Harold <HDoran at air.org> wrote:
> > You could use <<eval=FALSE>>= and the code inside the chunk will not
> > be evaluated. I suppose two other options could be to comment out the
> > bad code inside the code chunk or to use verbatim to make it look like
> > a code chunk in your output.
> > 
> > 
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vincent Goulet
> > Sent: Wednesday, September 07, 2005 11:44 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Avoid Sweave from stopping on errors
> > 
> > Hi all,
> > 
> > Is there an option in Sweave to avoid it from stopping on a code chunk
> > with an error? (I purposefully want to include code with an error in
> > class notes.)
> > 
> > I suspect the answer is "no" and that I will be pointed to
> > options("error"). That'd be fine, but which error parameter will just
> > "do nothing" in case of an error?
> > 
> > Thanks in advance!
> > 
> 
> Another option is to wrap the call that will produce an error in
> try().  You still get the error report, etc. but execution does not
> stop.

This works very nicely for the original purposes, as the try() can be 
hidden and the command in error echoed:

<<echo=TRUE,eval=TRUE>>=
y <- rnorm(10)
is.na(y) <- 1 
y
@
<<echo=TRUE,eval=FALSE>>=
lm(y ~ 1, na.action=na.fail)
@
<<echo=FALSE,eval=TRUE,results=verbatim>>=
try_out <- try(lm(y ~ 1, na.action=na.fail))
cat(try_out)
@

looks in output as it should, with the error message set after the command 
that provoked it.

Sweave is a remarkable tool.

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Roger.Bivand at nhh.no  Wed Sep  7 20:41:08 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 7 Sep 2005 20:41:08 +0200 (CEST)
Subject: [R] [R-pkgs] Revised versions of the maptools and sp packages
Message-ID: <Pine.LNX.4.44.0509072038490.7066-100000@reclus.nhh.no>

Revised versions of maptools, a package for reading geographical data from
shapefiles, and sp, a package with classes and methods for spatial data
handling, have been released on CRAN. They are maptools release 0.5-1 and
sp release 0.8-1.

The maptools package now depends on sp (>= 0.8), so that users of maptools
will need both to update the package itself, and to install the sp
package. All previous maptools classes and functions are still available,
but users are encouraged to try out the cleaner and more robust versions
in sp.

Further details of changes (particularly incompatible changes in sp) are
available on the R-sig-geo mailing list (see the "Spatial" task view on
CRAN), and at http://r-spatial.sourceforge.net

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From luisen.p at gmail.com  Wed Sep  7 21:44:39 2005
From: luisen.p at gmail.com (Luis Pineda)
Date: Wed, 7 Sep 2005 15:44:39 -0400
Subject: [R] Predicting responses using ace
In-Reply-To: <431EFEF5.5080704@vanderbilt.edu>
References: <859087cf0509061233350d7dba@mail.gmail.com>
	<431E45C4.401@vanderbilt.edu>
	<859087cf05090704347babc11b@mail.gmail.com>
	<431ED777.1080208@vanderbilt.edu>
	<859087cf05090705371776d8eb@mail.gmail.com>
	<431EFEF5.5080704@vanderbilt.edu>
Message-ID: <859087cf05090712446d40ad0b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050907/0e0b1d08/attachment.pl

From jhallman at frb.gov  Wed Sep  7 22:57:19 2005
From: jhallman at frb.gov (jhallman@frb.gov)
Date: Wed, 07 Sep 2005 16:57:19 -0400
Subject: [R] Using Tk table widget to display matrix
Message-ID: <20050907205719.CB90653583@mail.rsma.frb.gov>

Has anyone written a matrix editor or data.entry() replacement using the Tk
table widget?  I've been playing around with the examples at

http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/tktable.html

and making some progress, but I'd rather not spend much time on this if
someone else has already done it.

Jeff



From deepayan.sarkar at gmail.com  Wed Sep  7 23:12:08 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 7 Sep 2005 16:12:08 -0500
Subject: [R] Leading in line-wrapped Lattice value and panel labels
In-Reply-To: <200509070552.j875qRjV000523@mail22.syd.optusnet.com.au>
References: <200509070552.j875qRjV000523@mail22.syd.optusnet.com.au>
Message-ID: <eb555e66050907141235ac2529@mail.gmail.com>

On 9/7/05, Tim Churches <tchur at optushome.com.au> wrote:
> Version 2.1.1
> Platforms: all
> 
> What is the trellis parameter (or is there a trellis parameter) to set the leading (the gap between lines) when long axis values labels or panel header labels wrap over more than one line? By default, there is a huge gap between lines, and much looking and experimentation has not revealed to me a suitable parameter to adjust this.
> 

There is none. Whatever grid.text does happens.

Deepayan



From sebastien.durand at UMontreal.CA  Wed Sep  7 23:21:54 2005
From: sebastien.durand at UMontreal.CA (Sebastien Durand)
Date: Wed, 7 Sep 2005 17:21:54 -0400
Subject: [R] Language issue
Message-ID: <p06230905bf4506c5cf3e@[192.168.0.11]>

Dear all,

I am running
R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.1.1  (2005-06-20), ISBN 3-900051-07-0
Under Mac os X, a french version!

I am preparing a package and I got the following issue

I am trying to read dates that are written in 
english and have them recognized by R using 
as.Date function.

I realized strangely that when I type
>  month.abb
  [1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct"
[11] "Nov" "Dec"

I get the abbreviated english version of every month

>  x <- c("1-jan-1960", "2-feb-1960", 
>"31-mar-1960", "30-apr-1960","2-may-1960", 
>"31-jun-1960", "30-jul-1960","2-aug-1960", 
>"31-sep-1960", "30-oct-1960", "30-nov-1960", 
>"30-dec-1960");
>  strptime(x, "%d-%b-%Y")
  [1] "1960-01-01" NA           "1960-03-31" NA
  [5] NA           NA           "1960-07-30" NA
  [9] "1960-10-01" "1960-10-30" "1960-11-30" NA

It is only once I have found through trial an 
error the french abbreviation, that I got a match 
for every month.

>  x <- c("1-jan-1960", "2-f??v-1960", 
>"31-mar-1960", "30-avr-1960","2-mai-1960", 
>"31-jui-1960", "30-jul-1960","2-ao??-1960", 
>"31-sep-1960", "30-oct-1960", "30-nov-1960", 
>"30-d??c-1960");
>  strptime(x, "%d-%b-%Y")
  [1] "1960-01-01" "1960-02-02" "1960-03-31" "1960-04-30"
  [5] "1960-05-02" "1960-07-01" "1960-07-30" "1960-08-02"
  [9] "1960-10-01" "1960-10-30" "1960-11-30" "1960-12-30"

I got simply two questions:

First, why since R was install on a french system 
the month.abb command didn't give me the french 
abbreviations.

Secondly, since I am producing a package, I would 
like to know how can I tell R  to momentairly use 
the english abbreviations instead of the french 
ones...

Thanks a lot


-- 
  S??bastien Durand
Ma??trise en biologie
Universit?? de Montr??al
(514) 343-6864
Universit?? du Qu??bec ?? Montr??al
(514) 987-3000 (1572#)



From p.murrell at auckland.ac.nz  Wed Sep  7 23:19:19 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 08 Sep 2005 09:19:19 +1200
Subject: [R] Leading in line-wrapped Lattice value and panel labels
References: <200509070552.j875qRjV000523@mail22.syd.optusnet.com.au>
	<eb555e66050907141235ac2529@mail.gmail.com>
Message-ID: <431F5957.2070606@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
 > On 9/7/05, Tim Churches <tchur at optushome.com.au> wrote:
 >
 >> Version 2.1.1 Platforms: all
 >>
 >> What is the trellis parameter (or is there a trellis parameter) to
 >> set the leading (the gap between lines) when long axis values
 >> labels or panel header labels wrap over more than one line? By
 >> default, there is a huge gap between lines, and much looking and
 >> experimentation has not revealed to me a suitable parameter to
 >> adjust this.
 >>
 >
 >
 > There is none. Whatever grid.text does happens.


grid does have a "lineheight" graphical parameter.  For example,

library(grid)
grid.text("line one\nlinetwo",
           x=rep(1:3/4, each=3),
           y=rep(1:3/4, 3),
           gp=gpar(lineheight=1:9/2))

Could you add this in relevant places in trellis.par Deepayan?

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.dalgaard at biostat.ku.dk  Wed Sep  7 23:25:59 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Sep 2005 23:25:59 +0200
Subject: [R] Using Tk table widget to display matrix
In-Reply-To: <20050907205719.CB90653583@mail.rsma.frb.gov>
References: <20050907205719.CB90653583@mail.rsma.frb.gov>
Message-ID: <x2vf1cpo2g.fsf@turmalin.kubism.ku.dk>

jhallman at frb.gov writes:

> Has anyone written a matrix editor or data.entry() replacement using the Tk
> table widget?  I've been playing around with the examples at
> 
> http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/tktable.html
> 
> and making some progress, but I'd rather not spend much time on this if
> someone else has already done it.

I don't think so. It's been on my mind for a long time a I put in the
tclArray stuff to support it. As you've seen, James W. took it up and
went quite some distance with it, but he didn't go all the way.

When I've been thinking of these matters, I've been sidetracked by a
generic issue with Tcl extensions: How do you make sure that e.g.
tktable is available? Installation instructions tend to become
nontrivial and depend on details of the Tcl installation. One possible
idea that I have been toying with is to wrap the whole extension in an
R package, but we seem to have some issues on Windows where building
Tcl/Tk themselves using the Mingw32 toolchain doesn't quite work.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tchur at optushome.com.au  Wed Sep  7 23:40:42 2005
From: tchur at optushome.com.au (Tim Churches)
Date: Thu, 08 Sep 2005 07:40:42 +1000
Subject: [R] Leading in line-wrapped Lattice axis value and panel labels
In-Reply-To: <431F5957.2070606@stat.auckland.ac.nz>
References: <200509070552.j875qRjV000523@mail22.syd.optusnet.com.au>
	<eb555e66050907141235ac2529@mail.gmail.com>
	<431F5957.2070606@stat.auckland.ac.nz>
Message-ID: <431F5E5A.1070100@optushome.com.au>

Paul Murrell wrote:

> Hi
>
> Deepayan Sarkar wrote:
> > On 9/7/05, Tim Churches <tchur at optushome.com.au> wrote:
> >
> >> Version 2.1.1 Platforms: all
> >>
> >> What is the trellis parameter (or is there a trellis parameter) to
> >> set the leading (the gap between lines) when long axis values
> >> labels or panel header labels wrap over more than one line? By
> >> default, there is a huge gap between lines, and much looking and
> >> experimentation has not revealed to me a suitable parameter to
> >> adjust this.
> >>
> >
> > There is none. Whatever grid.text does happens.
>
> grid does have a "lineheight" graphical parameter.  For example,
>
> library(grid)
> grid.text("line one\nlinetwo",
>           x=rep(1:3/4, each=3),
>           y=rep(1:3/4, 3),
>           gp=gpar(lineheight=1:9/2))
>
> Could you add this in relevant places in trellis.par Deepayan?
>
Is there a work around we could use in the meantime, or should we 
attempt to hack trellis.par as per Paul's suggestion (gulp!)?  I suppose 
that is like asking "Should we attempt to climb Teichelmann?" - it 
depends... We have increased the depth of the panel headers, but this 
wastes plotting area and the tops of the tees and effs on the upper line 
and the bottoms of the gees and whys on the bottom line are still cut 
off, so large is the gap between the two lines.  And increasing the 
panel header depth it doesn't help with y-axis labels - typically the 
second line of one label will abut the first line of the next label, 
giving a results which  is rather like:

Value
               -
One
Value
              -
Two
Value
             -
Three
Value
            -
Four

where the actual value labels are "Value One", "Value Two" etc and the 
"-" are the tick marks. Less than ideal.

Suggestions for interim fixes (other than using abbreviated labels... 
we've thought of that) most welcome.

Tim C



From f.harrell at vanderbilt.edu  Wed Sep  7 23:58:53 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 07 Sep 2005 16:58:53 -0500
Subject: [R] Predicting responses using ace
In-Reply-To: <859087cf05090712446d40ad0b@mail.gmail.com>
References: <859087cf0509061233350d7dba@mail.gmail.com>	<431E45C4.401@vanderbilt.edu>	<859087cf05090704347babc11b@mail.gmail.com>	<431ED777.1080208@vanderbilt.edu>	<859087cf05090705371776d8eb@mail.gmail.com>	<431EFEF5.5080704@vanderbilt.edu>
	<859087cf05090712446d40ad0b@mail.gmail.com>
Message-ID: <431F629D.3050405@vanderbilt.edu>

Luis Pineda wrote:
> I sent this email before, but I got a r-help-bounce message and I don know 
> if it got to the m-list. Sorry if you had already seen it.
> 
> I'm using the areg.boot function to do an ace regression. So far I've been 
> able to do some simple running tests to fit a model with some input data, 
> predict the response with new data, and find the inverse transform of such 
> prediction (apparently). The commands I'm using are the following:
> 
> library(MASS)
> library(Hmisc) 
> xyt = read.table("tra100unifxy.dat") #2x100 table of training data (expl. 
> variables)
> zt = read.table("tra100unifzR.dat") #1x100 table of training data (resp. 
> variables)
> zs = read.table("sim10000z.dat") #2x10000 table of new data to predict 
> (expl. variables)
> xys = read.table("sim10000xy.dat") #1x10000 table of expected responses
> x = xyt[,1]
> y = xyt[,2]
> z = zt[,1]
> xynew = data.frame(x=xys[,1],y=xys[,2])
> ace.r = areg.boot(z ~ x + y, B = 100)
> f = Function(ace.r, ytype='inverse')
> za = f$z(predict(ace.r,xynew))
> 
> I have a couple question:
> 
> 1.) Is that the correct way of finding the inverse transform for the 
> responses?

Yes

> 2.) I'm evaluating the model's goodness of fit using the Fraction of 
> Variance Unexplained, which I'm calculating as:
> 
> rsa = za - zs
> FVUa = sum(rsa*rsa)/(10000*var(zs)) #10000 is the size of the test set

That is not corrected for overfitting.  You need to use the print method 
for the areg.boot object and note the Bootstrap validated R2

> 
> The thing is I'm not getting satisfactory results. Is there a way to improve 
> the results of the regression?. At the moment I'm not too confident with the 
> formula I'm using as a parameter for areg.boot, since the response variables 
> were generated as a substantially more complex function than z = x+y. I 
> don't get this formula thing yet and maybe I'm passing a totally unrelated 
> formula to the function.

z ~ x + y tells areg.boot to fit a model f(z) = g(x) + h(y) which is 
quite general, if x and y are additive.

Frank

> 
> On 9/7/05, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> 
>>Luis Pineda wrote:
>>
>>>Well, I had no idea, since I read this in the documentation:
>>>
>>>"|x| - for |transace| a numeric matrix. For |areg.boot| |x| may be a
>>>numeric matrix or a formula...||"
>>>
>>>Luis Pineda
>>
>>Sorry about that - you are right.
>>
>>Thomas - please debug the code to make areg.boot work with x = numeric
>>matrix, or correct the help file. Thanks -Frank
>>
>>--


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From p.murrell at auckland.ac.nz  Thu Sep  8 00:11:48 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 08 Sep 2005 10:11:48 +1200
Subject: [R] Leading in line-wrapped Lattice axis value and panel labels
References: <200509070552.j875qRjV000523@mail22.syd.optusnet.com.au>
	<eb555e66050907141235ac2529@mail.gmail.com>
	<431F5957.2070606@stat.auckland.ac.nz>
	<431F5E5A.1070100@optushome.com.au>
Message-ID: <431F65A4.3070505@stat.auckland.ac.nz>

Hi


Tim Churches wrote:
> Paul Murrell wrote:
> 
>> Hi
>>
>> Deepayan Sarkar wrote:
>> > On 9/7/05, Tim Churches <tchur at optushome.com.au> wrote:
>> >
>> >> Version 2.1.1 Platforms: all
>> >>
>> >> What is the trellis parameter (or is there a trellis parameter) to
>> >> set the leading (the gap between lines) when long axis values
>> >> labels or panel header labels wrap over more than one line? By
>> >> default, there is a huge gap between lines, and much looking and
>> >> experimentation has not revealed to me a suitable parameter to
>> >> adjust this.
>> >>
>> >
>> > There is none. Whatever grid.text does happens.
>>
>> grid does have a "lineheight" graphical parameter.  For example,
>>
>> library(grid)
>> grid.text("line one\nlinetwo",
>>           x=rep(1:3/4, each=3),
>>           y=rep(1:3/4, 3),
>>           gp=gpar(lineheight=1:9/2))
>>
>> Could you add this in relevant places in trellis.par Deepayan?
>>
> Is there a work around we could use in the meantime, or should we 
> attempt to hack trellis.par as per Paul's suggestion (gulp!)?  I suppose 
> that is like asking "Should we attempt to climb Teichelmann?" - it 
> depends... We have increased the depth of the panel headers, but this 
> wastes plotting area and the tops of the tees and effs on the upper line 
> and the bottoms of the gees and whys on the bottom line are still cut 
> off, so large is the gap between the two lines.  And increasing the 
> panel header depth it doesn't help with y-axis labels - typically the 
> second line of one label will abut the first line of the next label, 
> giving a results which  is rather like:
> 
> Value
>               -
> One
> Value
>              -
> Two
> Value
>             -
> Three
> Value
>            -
> Four
> 
> where the actual value labels are "Value One", "Value Two" etc and the 
> "-" are the tick marks. Less than ideal.
> 
> Suggestions for interim fixes (other than using abbreviated labels... 
> we've thought of that) most welcome.


I don't think lattice explicitly sets lineheight so you could try 
something like the following (push a [full-page] grid viewport that sets 
lineheight then draw lattice plot within that) ...

library(grid)
library(lattice)
states <- data.frame(state.x77,
                      state.name = dimnames(state.x77)[[1]],
                      state.region = factor(state.region))
levels(states$state.region) <-
     c("Northeast", "South", "North\n Central",  "West")
xyp <- xyplot(Murder  ~ Population | state.region, data = states,
        groups = as.character(state.name),
        panel = function(x, y, subscripts, groups)
        ltext(x = x, y = y, label = groups[subscripts], srt = -50,
              col = "blue",
              cex=.9, fontfamily = "HersheySans"),
        par.strip.text = list(cex = 1.3, font = 4, col = "brown",
                              lines = 2),
        xlab = "Estimated Population\nJuly 1, 1975",
        ylab = "Murder Rate \n(per 100,000 population)\n 1976",
        main = "Murder Rates in US states")
# default line height for comparison
print(xyp)
# control line height
grid.newpage()
pushViewport(viewport(gp=gpar(lineheight=0.8)))
print(xyp, newpage=FALSE)
popViewport()

... this does not work perfectly for me, but I'm not sure (yet) whether 
that is a problem in grid, a problem in lattice, or a problem with 
Hershey fonts (that are used in this example) so your mileage may vary.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From murdoch at stats.uwo.ca  Thu Sep  8 00:49:58 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 07 Sep 2005 18:49:58 -0400
Subject: [R] Language issue
In-Reply-To: <p06230905bf4506c5cf3e@[192.168.0.11]>
References: <p06230905bf4506c5cf3e@[192.168.0.11]>
Message-ID: <431F6E96.7050702@stats.uwo.ca>

Sebastien Durand wrote:
> Dear all,
> 
> I am running
> R : Copyright 2005, The R Foundation for Statistical Computing
> Version 2.1.1  (2005-06-20), ISBN 3-900051-07-0
> Under Mac os X, a french version!
> 
> I am preparing a package and I got the following issue
> 
> I am trying to read dates that are written in 
> english and have them recognized by R using 
> as.Date function.
> 
> I realized strangely that when I type
> 
>> month.abb
> 
>   [1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct"
> [11] "Nov" "Dec"
> 
> I get the abbreviated english version of every month
> 
> 
>> x <- c("1-jan-1960", "2-feb-1960", 
>>"31-mar-1960", "30-apr-1960","2-may-1960", 
>>"31-jun-1960", "30-jul-1960","2-aug-1960", 
>>"31-sep-1960", "30-oct-1960", "30-nov-1960", 
>>"30-dec-1960");
>> strptime(x, "%d-%b-%Y")
> 
>   [1] "1960-01-01" NA           "1960-03-31" NA
>   [5] NA           NA           "1960-07-30" NA
>   [9] "1960-10-01" "1960-10-30" "1960-11-30" NA
> 
> It is only once I have found through trial an 
> error the french abbreviation, that I got a match 
> for every month.
> 
> 
>> x <- c("1-jan-1960", "2-f??v-1960", 
>>"31-mar-1960", "30-avr-1960","2-mai-1960", 
>>"31-jui-1960", "30-jul-1960","2-ao??-1960", 
>>"31-sep-1960", "30-oct-1960", "30-nov-1960", 
>>"30-d??c-1960");
>> strptime(x, "%d-%b-%Y")
> 
>   [1] "1960-01-01" "1960-02-02" "1960-03-31" "1960-04-30"
>   [5] "1960-05-02" "1960-07-01" "1960-07-30" "1960-08-02"
>   [9] "1960-10-01" "1960-10-30" "1960-11-30" "1960-12-30"
> 
> I got simply two questions:
> 
> First, why since R was install on a french system 
> the month.abb command didn't give me the french 
> abbreviations.

It is documented to give English names.  It might be an idea to have a 
variable that gives months in the local language, but I don't know if 
the current translation system supports that.  It would need to have a 
different name than month.abb.
> 
> Secondly, since I am producing a package, I would 
> like to know how can I tell R  to momentairly use 
> the english abbreviations instead of the french 
> ones...

I don't know, but I'd try Sys.setlocale().

Duncan Murdoch



From tura at centroin.com.br  Thu Sep  8 01:17:36 2005
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Wed, 07 Sep 2005 20:17:36 -0300
Subject: [R] Fisher's method in discriminant analysis
Message-ID: <6.1.2.0.2.20050907201719.037e7850@centroin.com.br>

At 13:15 5/9/2005, you wrote:

>Hi,
>
>   I'm using mda library to solve a discriminant
>analysis. I get results, but the thing is that I want
>to use Fisher's method to obtain the classification
>functions and I'm lost in what I should do: libraries
>to use, ... Can anybody give me a clue??

Hi Carlos,

I think you need something this
require(mda)
data(iris)
attach(iris)
irisfit <- mda(Species ~ ., data = iris)
irisfit$fit$coef[,1:2]
r1<- 
-4.23481161+0.37972423*Sepal.Length+0.59682846*Sepal.Width+0.01575609*Petal.Length+0.11009570*Petal.Width
r2<-0.31169082-0.05826293*Sepal.Length-0.23855482*Sepal.Width+0.18922693*Petal.Length+0.03917652*Petal.Width
plot(r1,r2,pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])




Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil  


-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From tlumley at u.washington.edu  Thu Sep  8 01:55:14 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 7 Sep 2005 16:55:14 -0700 (PDT)
Subject: [R] R: optim
In-Reply-To: <431ED7D6.192463D7@STATS.uct.ac.za>
References: <431D753A.5C2D2AF3@STATS.uct.ac.za>
	<40e66e0b050906053591b35dd@mail.gmail.com>
	<431E85D9.45FD7DF7@STATS.uct.ac.za> <431ED7D6.192463D7@STATS.uct.ac.za>
Message-ID: <Pine.A41.4.63a.0509071647320.98910@homer03.u.washington.edu>

On Wed, 7 Sep 2005, Clark Allan wrote:

>
> $MLE$message
> [1] "ERROR: ABNORMAL_TERMINATION_IN_LNSRCH"
>
>
> WHAT DOES THIS ERROR MESSAGE MEAN???
>

Looking at the code in optim() a little, it looks as though this error 
comes when the optimiser tries to do a line search in the steepest descent 
direction and finds that the derivative along this line is positive, which 
is impossible.

I have seen this when the gradient is wrong, and I suppose it could also 
happen with numerical gradients when the surface is nearly flat or the 
problem is very badly scaled.

Eg, using the functions in example(optim) and making the gradient wrong:

>  optim(c(-1.2, 1), fr, function(theta) grr(theta)+1, method = 
"L-BFGS-B")
$par
[1] 0.25034245 0.05769649

$value
[1] 0.5644614

$counts
function gradient
       96       96

$convergence
[1] 52

$message
[1] "ERROR: ABNORMAL_TERMINATION_IN_LNSRCH"


 	-thomas



From lenk at insightful.com  Thu Sep  8 02:07:28 2005
From: lenk at insightful.com (Leonard Kannapell)
Date: Wed, 7 Sep 2005 17:07:28 -0700
Subject: [R] graphics support in R help files
Message-ID: <4A9EF66708CA0B4395CB5D88D13C5CA201A1EBFE@se2kexch01.insightful.com>

I looked through the "Writing R Extensions" pdf, and I don't see how graphics
can be input in help files. For example, if I had a .eps plot that I wanted
to include in a help file, what would the syntax be to include it in an R
help file?

If there is graphics support in help files, which format are supported (e.g.,
gif, png, jpg, eps, ps)? Thanks,

-Len Kannapell



From wcai11 at hotmail.com  Thu Sep  8 02:22:02 2005
From: wcai11 at hotmail.com (Weijie Cai)
Date: Wed, 07 Sep 2005 20:22:02 -0400
Subject: [R] control parameter for Nelder-Mead algorithm in optim()
Message-ID: <BAY103-F26666316CB8300AA12B25AD3990@phx.gbl>

Hi,

In manual for optim() function, there are three control parameters for 
Nelder-Mead algorithm: alpha (reflection),beta(contraction) and 
gamma(expansion), but in the original paper, there is another parameter 
delta which controls shrinkage, how can I set this shrink parameter? Or is 
beta actually the same meaning as delta?

TIA

WC



From djames at frontierassoc.com  Thu Sep  8 02:24:00 2005
From: djames at frontierassoc.com (David James)
Date: Wed, 7 Sep 2005 19:24:00 -0500
Subject: [R] Interpolating / smoothing missing time series data
Message-ID: <7B5B13D4-C743-4B15-9E98-2FD563905DDE@frontierassoc.com>

The purpose of this email is to ask for pre-built procedures or  
techniques for smoothing and interpolating missing time series data.

I've made some headway on my problem in my spare time.  I started  
with an irregular time series with lots of missing data.  It even had  
duplicated data.  Thanks to zoo, I've cleaned that up -- now I have a  
regular time series with lots of NA's.

I want to use a regression model (i.e. ARIMA) to ill in the gaps.  I  
am certainly open to other suggestions, especially if they are easy  
to implement.

My specific questions:
1.  Presumably, once I get ARIMA working, I still have the problem of  
predicting the past missing values -- I've only seen examples of  
predicting into the future.
2.  When predicting the past (backcasting), I also want to take  
reasonable steps to make the data look smooth.

I guess I'm looking for a really good example in a textbook or white  
paper (or just an R guru with some experience in this area) that can  
offer some guidance.

Venables and Ripley was a great start (Modern Applied Statistics with  
S).  I really had hoped that the "Seasonal ARIMA Models" section on  
page 405 would help.  It was helpful, but only to a point.  I have a  
hunch (based on me crashing arima numerous times -- maybe I'm just  
new to this and doing things that are unreasonable?) that using  
hourly data just does not mesh well with the seasonal arima code?

-David



From djames at frontierassoc.com  Thu Sep  8 02:24:42 2005
From: djames at frontierassoc.com (David James)
Date: Wed, 7 Sep 2005 19:24:42 -0500
Subject: [R] Crash with seasonal ARIMA
Message-ID: <8B01770B-8A57-419B-BE2B-A1D02CBA4129@frontierassoc.com>

The following command crashes my Mac OS X version of R:
(I'm running R on a PowerMac G5, with 1 GB of RAM and dual processors.)

 > arima.0 <- arima(w3.ts,order=c(1,0,0),seasonal=list(order=c 
(1,0,0),period=365))

-David



Here is some background:

w3.ts is hourly temperature data with about 20% NA's.  It contains  
about 3.5 years of data.

I am using period = 365, which makes sense me to me.  Is this wrong?



 > summary(w3.ts)
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
   14.00   54.00   68.00   66.09   78.00  108.00 2976.00

 > length(w3.ts)
[1] 29542

 > frequency(w3.ts)
[1] 24

 > deltat(w3.ts)
[1] 0.04166667

 > w3.ts[300:400]
   [1] 61 63 64 68 66 64 61 59 62 56 53 49 43 45 47 46 44 42 42 40 42  
48 52 55
[25] 57 59 60 60 60 59 52 50 49 45 47 41 38 36 37 35 36 37 39 39 38  
42 49 54
[49] 58 61 63 64 63 62 59 55 54 51 55 53 53 NA NA NA NA NA NA NA NA  
NA NA NA
[73] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA  
NA NA NA
[97] NA NA NA NA NA

 > w3.ts[720:770]
[1]       NA       NA       NA       NA       NA       NA        
NA       NA
[9]       NA       NA       NA       NA       NA       NA        
NA       NA
[17]       NA       NA       NA       NA       NA       NA        
NA       NA
[25]       NA 41.50000 46.50000 40.00000 42.50000 37.50000 41.00000  
36.50000
[33] 40.50000 38.50000 42.50000 41.00000 46.00000 43.50000 46.50000  
43.50000
[41] 47.50000 41.50000 47.50000 42.50000 47.33333 47.66667 51.00000  
41.50000
[49] 51.50000 40.00000 52.50000


-David



From Tom.Mulholland at dpi.wa.gov.au  Thu Sep  8 02:32:53 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 8 Sep 2005 08:32:53 +0800
Subject: [R] graphics support in R help files
Message-ID: <4702645135092E4497088F71D9C8F51A128C18@afhex01.dpi.wa.gov.au>

I cannot state this with the certainty that others might, but the Rd format is a text format. If you want to produce something else then you need to choose an alternative method. For instance, 1.4 of "Writing R Extensions" notes that "Documents in 'inst/doc' can be in arbitrary format, however we strongly recommend to provide them in PDF
format, such that users on all platforms can easily read them." 

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Leonard 
> Kannapell
> Sent: Thursday, 8 September 2005 8:07 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] graphics support in R help files
> 
> 
> I looked through the "Writing R Extensions" pdf, and I don't 
> see how graphics
> can be input in help files. For example, if I had a .eps plot 
> that I wanted
> to include in a help file, what would the syntax be to 
> include it in an R
> help file?
> 
> If there is graphics support in help files, which format are 
> supported (e.g.,
> gif, png, jpg, eps, ps)? Thanks,
> 
> -Len Kannapell
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From djames at frontierassoc.com  Thu Sep  8 02:56:04 2005
From: djames at frontierassoc.com (David James)
Date: Wed, 7 Sep 2005 19:56:04 -0500
Subject: [R] Crash with seasonal ARIMA
In-Reply-To: <8B01770B-8A57-419B-BE2B-A1D02CBA4129@frontierassoc.com>
References: <8B01770B-8A57-419B-BE2B-A1D02CBA4129@frontierassoc.com>
Message-ID: <A8F663AF-5F77-471F-8184-2F16B5B85BDF@frontierassoc.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050907/edf6d379/attachment.pl

From br44114 at gmail.com  Thu Sep  8 03:14:54 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 7 Sep 2005 21:14:54 -0400
Subject: [R] RMySQL installation problem on FC4 x86_64
Message-ID: <8d5a363505090718143b89dd79@mail.gmail.com>

Dear useRs,

I'm having a hard time installing RMySQL on a FC4 x86_64 box (R 2.1.0
and MySQL 4.1.11-2 installed through yum). After an initial
configuration error ("could not find the MySQL installation include
and/or library directories") I managed to install RMySQL with
   # export PKG_LIBS="-L</usr/lib64/mysql> -lmysqlclient"
   # R CMD INSTALL RMySQL_0.5-5.tar.gz

However, when I load the package I get this error:
> require(RMySQL)
Loading required package: RMySQL
Loading required package: DBI
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
'/usr/lib64/R/library/RMySQL/libs/RMySQL.so':
  /usr/lib64/R/library/RMySQL/libs/RMySQL.so: undefined symbol:
mysql_field_count
[1] FALSE

Can anyone offer a suggestion, or perhaps email me a precompiled binary?
Thank you,
b.

platform "x86_64-redhat-linux-gnu"
arch     "x86_64"
os       "linux-gnu"
system   "x86_64, linux-gnu"
status   ""
major    "2"
minor    "1.0"
year     "2005"
month    "04"
day      "18"
language "R"

# yum list installed mysql
Installed Packages
mysql.i386            4.1.11-2               installed
mysql.x86_64       4.1.11-2               installed



From macmanes at berkeley.edu  Thu Sep  8 03:29:44 2005
From: macmanes at berkeley.edu (Matthew MacManes)
Date: Wed, 07 Sep 2005 18:29:44 -0700
Subject: [R] Experimental data analysis (eda) function
Message-ID: <6.2.3.4.2.20050907182541.01d13928@calmail.berkeley.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050907/6485032c/attachment.pl

From bitwrit at ozemail.com.au  Thu Sep  8 13:43:06 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 08 Sep 2005 11:43:06 +0000
Subject: [R] (no subject)
In-Reply-To: <200509070519.j875IvT2020123@hypatia.math.ethz.ch>
References: <200509070519.j875IvT2020123@hypatia.math.ethz.ch>
Message-ID: <432023CA.1050109@ozemail.com.au>

Salang Pan wrote:
> hi£¬
>   
>   Is it possible to draw a string text in a rectangle according the width of this rectangle?   that is, the fontsize of this string text can be adjusted according the width of the rectangle.
>  How to set the cex parameter in text function? 
> 
> text (x, y = NULL, labels = seq(along = x), adj = NULL,
>           pos = NULL, offset = 0.5, vfont = NULL,
>           cex = 1, col = NULL, font = NULL, xpd = NULL, ...)
> 
Hi Salang,

Have a look at boxed.labels() and textbox() in the plotrix package.

Jim



From ggrothendieck at gmail.com  Thu Sep  8 04:19:17 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Sep 2005 22:19:17 -0400
Subject: [R] Interpolating / smoothing missing time series data
In-Reply-To: <7B5B13D4-C743-4B15-9E98-2FD563905DDE@frontierassoc.com>
References: <7B5B13D4-C743-4B15-9E98-2FD563905DDE@frontierassoc.com>
Message-ID: <971536df05090719199a2ce02@mail.gmail.com>

On 9/7/05, David James <djames at frontierassoc.com> wrote:
> The purpose of this email is to ask for pre-built procedures or
> techniques for smoothing and interpolating missing time series data.
> 
> I've made some headway on my problem in my spare time.  I started
> with an irregular time series with lots of missing data.  It even had
> duplicated data.  Thanks to zoo, I've cleaned that up -- now I have a
> regular time series with lots of NA's.
> 
> I want to use a regression model (i.e. ARIMA) to ill in the gaps.  I
> am certainly open to other suggestions, especially if they are easy
> to implement.
> 
> My specific questions:
> 1.  Presumably, once I get ARIMA working, I still have the problem of
> predicting the past missing values -- I've only seen examples of
> predicting into the future.
> 2.  When predicting the past (backcasting), I also want to take
> reasonable steps to make the data look smooth.
> 
> I guess I'm looking for a really good example in a textbook or white
> paper (or just an R guru with some experience in this area) that can
> offer some guidance.
> 
> Venables and Ripley was a great start (Modern Applied Statistics with
> S).  I really had hoped that the "Seasonal ARIMA Models" section on
> page 405 would help.  It was helpful, but only to a point.  I have a
> hunch (based on me crashing arima numerous times -- maybe I'm just
> new to this and doing things that are unreasonable?) that using
> hourly data just does not mesh well with the seasonal arima code?

Not sure if this answers your question but if you are looking for something
simple then na.approx in the zoo package will linearly interpolate for you.

> z <- zoo(c(1,2,NA,4,5))
> na.approx(z)
1 2 3 4 5 
1 2 3 4 5



From Peter.Watkins at foodscience.afisc.csiro.au  Thu Sep  8 04:21:28 2005
From: Peter.Watkins at foodscience.afisc.csiro.au (Peter.Watkins@foodscience.afisc.csiro.au)
Date: Thu, 8 Sep 2005 12:21:28 +1000
Subject: [R] Effect of data set size on calculation
Message-ID: <CB3D0020D0BA674C99B5B7E751D1A99903AC90@exvicn1-mel.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050908/191f127c/attachment.pl

From A.Robinson at ms.unimelb.edu.au  Thu Sep  8 05:40:17 2005
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 8 Sep 2005 13:40:17 +1000
Subject: [R] Creating very small plots (2.5 cm wide) in Sweave
Message-ID: <20050908034017.GV661@ms.unimelb.edu.au>

Hi everyone,

I was wondering if anyone has any code they could share for creating
thumbnail plots in Sweave.  For example, I'd like a plot like the
following:

y <- c(40, 46, 39, 44, 23, 36, 70, 39, 30, 73, 53, 74)
x <- c(6, 4, 3, 6, 1, 5, 6, 2, 1, 8, 4, 6)
opar <- par(mar=c(3,3,0,0))
plot(x, y, xlab="", ylab="")
abline(h=mean(y), col="red")
par(opar)

to come out about 2.5 cm wide.

Thanks for any assistance,

Andrew
-- 
Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344-4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au



From jsorkin at grecc.umaryland.edu  Thu Sep  8 06:05:43 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Thu, 08 Sep 2005 00:05:43 -0400
Subject: [R] Prediction with multiple zeros in the dependent variable
Message-ID: <s31f806d.080@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050908/d1d4b10f/attachment.pl

From sfalcon at fhcrc.org  Thu Sep  8 06:39:40 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 07 Sep 2005 21:39:40 -0700
Subject: [R] RMySQL installation problem on FC4 x86_64
In-Reply-To: <8d5a363505090718143b89dd79@mail.gmail.com> (bogdan romocea's
	message of "Wed, 7 Sep 2005 21:14:54 -0400")
References: <8d5a363505090718143b89dd79@mail.gmail.com>
Message-ID: <m264tcnpf7.fsf@macaroni.local>

On  7 Sep 2005, br44114 at gmail.com wrote:
> # yum list installed mysql
> Installed Packages
> mysql.i386            4.1.11-2               installed
> mysql.x86_64       4.1.11-2               installed

I would have thought that you need to have a mysql-dev.x86_64 rpm
package installed in order to get the headers to be able to compile,
etc.  Not a redhat or yum user myself, so don't know.  But the error
message about unable to find header files is often solved by installed
the appropriate devel package that contains those headers.

+ seth



From gerifalte28 at hotmail.com  Thu Sep  8 07:18:34 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 08 Sep 2005 05:18:34 +0000
Subject: [R] Interpolating / smoothing missing time series data
In-Reply-To: <971536df05090719199a2ce02@mail.gmail.com>
Message-ID: <BAY103-F10EBC58AD14F15DEB52901A6990@phx.gbl>

I don't have much experience in the subject but it seems that library(akima) 
should be useful for your problem. Try library(help="akima") to see a list 
of the functions available in the library.

I hope this helps

Francisco


>From: Gabor Grothendieck <ggrothendieck at gmail.com>
>Reply-To: ggrothendieck at gmail.com
>To: David James <djames at frontierassoc.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Interpolating / smoothing missing time series data
>Date: Wed, 7 Sep 2005 22:19:17 -0400
>
>On 9/7/05, David James <djames at frontierassoc.com> wrote:
> > The purpose of this email is to ask for pre-built procedures or
> > techniques for smoothing and interpolating missing time series data.
> >
> > I've made some headway on my problem in my spare time.  I started
> > with an irregular time series with lots of missing data.  It even had
> > duplicated data.  Thanks to zoo, I've cleaned that up -- now I have a
> > regular time series with lots of NA's.
> >
> > I want to use a regression model (i.e. ARIMA) to ill in the gaps.  I
> > am certainly open to other suggestions, especially if they are easy
> > to implement.
> >
> > My specific questions:
> > 1.  Presumably, once I get ARIMA working, I still have the problem of
> > predicting the past missing values -- I've only seen examples of
> > predicting into the future.
> > 2.  When predicting the past (backcasting), I also want to take
> > reasonable steps to make the data look smooth.
> >
> > I guess I'm looking for a really good example in a textbook or white
> > paper (or just an R guru with some experience in this area) that can
> > offer some guidance.
> >
> > Venables and Ripley was a great start (Modern Applied Statistics with
> > S).  I really had hoped that the "Seasonal ARIMA Models" section on
> > page 405 would help.  It was helpful, but only to a point.  I have a
> > hunch (based on me crashing arima numerous times -- maybe I'm just
> > new to this and doing things that are unreasonable?) that using
> > hourly data just does not mesh well with the seasonal arima code?
>
>Not sure if this answers your question but if you are looking for something
>simple then na.approx in the zoo package will linearly interpolate for you.
>
> > z <- zoo(c(1,2,NA,4,5))
> > na.approx(z)
>1 2 3 4 5
>1 2 3 4 5
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From gerifalte28 at hotmail.com  Thu Sep  8 07:44:59 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 08 Sep 2005 05:44:59 +0000
Subject: [R] Creating very small plots (2.5 cm wide) in Sweave
In-Reply-To: <20050908034017.GV661@ms.unimelb.edu.au>
Message-ID: <BAY103-F29643DF07E26943BED4F81A6990@phx.gbl>

Others may propose more elegant solutions but, in windows one quick an dirty 
option would be to change the argument 'pin' and 'fin' within par to get an 
image of exactly 1 inch (2.54 cm) i.e.

y <- c(40, 46, 39, 44, 23, 36, 70, 39, 30, 73, 53, 74)
x <- c(6, 4, 3, 6, 1, 5, 6, 2, 1, 8, 4, 6)
par(pin=c(1,1), fin=c(1,1))
plot(x, y, xlab="", ylab="")
abline(h=mean(y), col="red")

#Save the plot in bmp format
savePlot("myplot", "bmp")

and then manually crop the picture using your favorite picture package or 
even within a word processor.

I hope this helps

Francisco


>From: Andrew Robinson <A.Robinson at ms.unimelb.edu.au>
>To: R-Help Discussion <r-help at stat.math.ethz.ch>
>Subject: [R] Creating very small plots (2.5 cm wide) in Sweave
>Date: Thu, 8 Sep 2005 13:40:17 +1000
>
>Hi everyone,
>
>I was wondering if anyone has any code they could share for creating
>thumbnail plots in Sweave.  For example, I'd like a plot like the
>following:
>
>y <- c(40, 46, 39, 44, 23, 36, 70, 39, 30, 73, 53, 74)
>x <- c(6, 4, 3, 6, 1, 5, 6, 2, 1, 8, 4, 6)
>opar <- par(mar=c(3,3,0,0))
>plot(x, y, xlab="", ylab="")
>abline(h=mean(y), col="red")
>par(opar)
>
>to come out about 2.5 cm wide.
>
>Thanks for any assistance,
>
>Andrew
>--
>Andrew Robinson
>Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
>Department of Mathematics and Statistics            Fax: +61-3-8344-4599
>University of Melbourne, VIC 3010 Australia
>Email: a.robinson at ms.unimelb.edu.au    Website: 
>http://www.ms.unimelb.edu.au
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From thpe at hhbio.wasser.tu-dresden.de  Thu Sep  8 08:17:10 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 08 Sep 2005 08:17:10 +0200
Subject: [R] Interpolating / smoothing missing time series data
In-Reply-To: <BAY103-F10EBC58AD14F15DEB52901A6990@phx.gbl>
References: <BAY103-F10EBC58AD14F15DEB52901A6990@phx.gbl>
Message-ID: <431FD766.6070600@hhbio.wasser.tu-dresden.de>

Francisco J. Zagmutt wrote:
> I don't have much experience in the subject but it seems that library(akima) 
> should be useful for your problem. Try library(help="akima") to see a list 
> of the functions available in the library.
> 
> I hope this helps
> 
> Francisco

Yes, function aspline() of package akima is well suited for such things: 
no wiggles like in spline() and less variance reducing than approx(). 
But in any case: excessive interpolation will definitely lead to biased 
results, in particular artificial autocorrelations.

If ever possible, David should look for methods, capable of dealing with 
missing data directly.

Thomas P.



From A.Robinson at ms.unimelb.edu.au  Thu Sep  8 08:15:48 2005
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 8 Sep 2005 16:15:48 +1000
Subject: [R] Creating very small plots (2.5 cm wide) in Sweave
In-Reply-To: <BAY103-F29643DF07E26943BED4F81A6990@phx.gbl>
References: <20050908034017.GV661@ms.unimelb.edu.au>
	<BAY103-F29643DF07E26943BED4F81A6990@phx.gbl>
Message-ID: <20050908061548.GB38964@ms.unimelb.edu.au>

Dear Francisco,

thanks for your solution.  It turns out that it's best for me to use

\setkeys{Gin}{width=0.15\textwidth}

directly before I call the plot - that seems to work just fine.

Andrwe

On Thu, Sep 08, 2005 at 05:44:59AM +0000, Francisco J. Zagmutt wrote:
> Others may propose more elegant solutions but, in windows one quick an 
> dirty option would be to change the argument 'pin' and 'fin' within par to 
> get an image of exactly 1 inch (2.54 cm) i.e.
> 
> y <- c(40, 46, 39, 44, 23, 36, 70, 39, 30, 73, 53, 74)
> x <- c(6, 4, 3, 6, 1, 5, 6, 2, 1, 8, 4, 6)
> par(pin=c(1,1), fin=c(1,1))
> plot(x, y, xlab="", ylab="")
> abline(h=mean(y), col="red")
> 
> #Save the plot in bmp format
> savePlot("myplot", "bmp")
> 
> and then manually crop the picture using your favorite picture package or 
> even within a word processor.
> 
> I hope this helps
> 
> Francisco
> 
> 
> >From: Andrew Robinson <A.Robinson at ms.unimelb.edu.au>
> >To: R-Help Discussion <r-help at stat.math.ethz.ch>
> >Subject: [R] Creating very small plots (2.5 cm wide) in Sweave
> >Date: Thu, 8 Sep 2005 13:40:17 +1000
> >
> >Hi everyone,
> >
> >I was wondering if anyone has any code they could share for creating
> >thumbnail plots in Sweave.  For example, I'd like a plot like the
> >following:
> >
> >y <- c(40, 46, 39, 44, 23, 36, 70, 39, 30, 73, 53, 74)
> >x <- c(6, 4, 3, 6, 1, 5, 6, 2, 1, 8, 4, 6)
> >opar <- par(mar=c(3,3,0,0))
> >plot(x, y, xlab="", ylab="")
> >abline(h=mean(y), col="red")
> >par(opar)
> >
> >to come out about 2.5 cm wide.
> >
> >Thanks for any assistance,
> >
> >Andrew
> >--
> >Andrew Robinson
> >Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
> >Department of Mathematics and Statistics            Fax: +61-3-8344-4599
> >University of Melbourne, VIC 3010 Australia
> >Email: a.robinson at ms.unimelb.edu.au    Website: 
> >http://www.ms.unimelb.edu.au
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> 

-- 
Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344-4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au



From cpdiehl at gmail.com  Wed Sep  7 22:31:05 2005
From: cpdiehl at gmail.com (Chris Diehl)
Date: Wed, 7 Sep 2005 16:31:05 -0400
Subject: [R] Text Size in Legend
Message-ID: <62bdf668050907133115da46aa@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050907/11b07676/attachment.pl

From spyridoula.tsonaka at med.kuleuven.be  Thu Sep  8 10:29:46 2005
From: spyridoula.tsonaka at med.kuleuven.be (Spyridoula Tsonaka)
Date: Thu, 8 Sep 2005 10:29:46 +0200
Subject: [R] Text Size in Legend
References: <62bdf668050907133115da46aa@mail.gmail.com>
Message-ID: <007a01c5b44f$7895dab0$1540210a@www.domain>

Hi Chris,

To change the scale of the whole legend you can use the argument 'cex' in 
legend.

I hope this helps!

Regards,
Roula

=================
Spyridoula Tsonaka
Doctoral Student
Biostatistical Centre
Catholic University of Leuven
Kapucijnenvoer 35
B-3000 Leuven
Belgium
Tel: +32/16/336899
Fax: +32/16/337015


----- Original Message ----- 
From: "Chris Diehl" <cpdiehl at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, September 07, 2005 10:31 PM
Subject: [R] Text Size in Legend


> Hello,
>
> I need to reduce the size of the text in a legend since the legend is
> overlapping with the curves in my plot. I've not been able to identify any
> way to achieve this in the documentation. Anyone have any suggestions on 
> how
> to scale down the text or the overall legend?
>
> Thanks in advance for your help!
>
> Chris Diehl
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From pensterfuzzer at yahoo.de  Thu Sep  8 10:52:16 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Thu, 8 Sep 2005 10:52:16 +0200 (CEST)
Subject: [R] Using R map data to determine associated state for a coordinate?
Message-ID: <20050908085216.35296.qmail@web25806.mail.ukl.yahoo.com>

Hi!

I have no idea if this is maybe an easy task utilizing
R since I read there is 
geographical map data in some package:

I have a huge number of geographical points with their
coordinates in Germany. 
Now I want to determine for each point in which
"Bundesland" = state it is located.

Can anybody tell me if this is doable relatively easy
in R and if so give me 
some hints or links how to do it?

Thanks a million,
   Werner



From wuming.gong at gmail.com  Thu Sep  8 10:59:16 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Thu, 8 Sep 2005 16:59:16 +0800
Subject: [R] "Too long to display" problem
Message-ID: <b428d06d0509080159498b47bb@mail.gmail.com>

Dear list, 

I used read.xls in gdata package to read a worksheet in which certain
field contains very long character strings (nucleotides sequence,
nchar > 10,000). Then, the values in these fields are automatically
converted to "TOO LONG TO DISPLAY". How can I get those original
characters instead of "TOO LONG TO DISPLAY"?

Thanks,

Wuming



From shigesong at gmail.com  Thu Sep  8 11:08:59 2005
From: shigesong at gmail.com (Shige Song)
Date: Thu, 8 Sep 2005 17:08:59 +0800
Subject: [R] Survival model with cross-classified shared frailties
Message-ID: <5abc11d8050908020811812817@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050908/6ebca382/attachment.pl

From maechler at stat.math.ethz.ch  Thu Sep  8 11:15:48 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 8 Sep 2005 11:15:48 +0200
Subject: [R] variables from command line
In-Reply-To: <3f87cc6d0509070747503f3f1b@mail.gmail.com>
References: <3f87cc6d0509070747503f3f1b@mail.gmail.com>
Message-ID: <17184.324.565507.806445@stat.math.ethz.ch>

>>>>> "Omar" == Omar Lakkis <uofiowa at gmail.com>
>>>>>     on Wed, 7 Sep 2005 10:47:43 -0400 writes:

    Omar> How can I pass parameters to an R script from the
    Omar> command line. And how can I read them from within the
    Omar> script?

    Omar> This is how I want to invoke the script: R CMD BATCH
    Omar> r.in r.out <input values>

    Omar> The script with read in the input values, process them
    Omar> and spit the output to r.out.

I think     commandArgs()    should solve this.

Regard,
Martin Maechler



From knoblauch at lyon.inserm.fr  Thu Sep  8 15:23:28 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Thu, 8 Sep 2005 11:23:28 -0200 (CEST)
Subject: [R]  FW: Re: Doubt about nested aov output
Message-ID: <49747.194.57.165.22.1126171408.squirrel@webmail.lyon.inserm.fr>

Your response nicely clarifies a question that I've had for a long time,
but which I've dealt
with by giving each subject a unique label.  Unless I'm missing something,
both techniques should
work as the toy example below gives exactly the same output in all 3 cases
below (forgetting
about the convergence problem).  Would there be a reason to prefer
labeling the levels
one way or another or is it just a matter of convenience?

library(lmer)
y <- rnorm(15)
cond <- gl(3, 5, 15)
obs <- gl(15, 1)
subj <- gl(5, 1, 15)
dd <- data.frame(y = y, cond = cond, obs = obs, subj = subj)

l1 <- lmer(y~cond + (1|cond:obs), dd)
l2 <- lmer(y~cond + (1|cond:subj), dd)
l3 <- lmer(y~cond + (1|obs), dd)

Douglas Bates a ??crit:

The difference between models like
  lmer(Glycogen~Treatment+(1|Rat)+(1|Rat:Liver))
and
  lmer(Glycogen~Treatment+(1|Treatment:Rat)+(1|Treatment:Rat:Liver))

is more about the meaning of the levels of "Rat" than about the
meaning of "Treatment".  As I understood it there are three different
rats labelled 1.  There is a rat 1 on treatment 1 and a rat 1 on
treatment 2 and a rat 1 on treatment 3.  Thus the levels of Rat do not
designate the "experimental unit", it is the levels of Treatment:Rat
that do this.

-- 
Ken Knoblauch
Inserm U371
Cerveau et Vision
Dept. of Cognitive Neuroscience
18 avenue du Doyen L??pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.lyon.inserm.fr/371/



From wuming.gong at gmail.com  Thu Sep  8 11:33:52 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Thu, 8 Sep 2005 17:33:52 +0800
Subject: [R] "Too long to display" problem
In-Reply-To: <b428d06d0509080159498b47bb@mail.gmail.com>
References: <b428d06d0509080159498b47bb@mail.gmail.com>
Message-ID: <b428d06d05090802347f02c7c3@mail.gmail.com>

Dear list, 

Please ignore this thread - the "TOO LONG TO DISPLAY" is brought by
another tool when parsing data sets. Sorry for this ...

Wuming

On 9/8/05, Wuming Gong <wuming.gong at gmail.com> wrote:
> Dear list,
> 
> I used read.xls in gdata package to read a worksheet in which certain
> field contains very long character strings (nucleotides sequence,
> nchar > 10,000). Then, the values in these fields are automatically
> converted to "TOO LONG TO DISPLAY". How can I get those original
> characters instead of "TOO LONG TO DISPLAY"?
> 
> Thanks,
> 
> Wuming
>



From thpe at hhbio.wasser.tu-dresden.de  Thu Sep  8 12:32:41 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 08 Sep 2005 12:32:41 +0200
Subject: [R] Using R map data to determine associated state for a
	coordinate?
In-Reply-To: <20050908085216.35296.qmail@web25806.mail.ukl.yahoo.com>
References: <20050908085216.35296.qmail@web25806.mail.ukl.yahoo.com>
Message-ID: <43201349.2070402@hhbio.wasser.tu-dresden.de>

Werner Wernersen wrote:
> Hi!
> 
> I have no idea if this is maybe an easy task utilizing
> R since I read there is 
> geographical map data in some package:
> 
> I have a huge number of geographical points with their
> coordinates in Germany. 
> Now I want to determine for each point in which
> "Bundesland" = state it is located.
> 
> Can anybody tell me if this is doable relatively easy
> in R and if so give me 
> some hints or links how to do it?
> 
> Thanks a million,
>    Werner

Hello Werner,

two building blocks, but don't know if the precision meets your needs.

1. Do you have a good map of Germany *with* Federal States?

* If YES and if it's free:
==> I would be interested! Please post it's source.

* If NO:
==> Downloadable map data are available on:
     http://www.vdstech.com/map_data.htm

2. The following approach reads and converts a shapefile with functions 
from maptools and then follows the example of inside.owin() from the 
spatstat package.

Hope that helps

Thomas Petzoldt


##########################################################
library(maptools)
library(spatstat)

ger <- read.shape("germany.shp")
plot(ger)

pger <- Map2poly(ger)
sx<- pger[[13]]
lines(sx, type="l", col="red") # Saxony ;-)

## Create an owin (observation window) object
# direction of coordinates must be reversed, in some cases
# if error message: remove rev()'s
saxony <- owin(poly=list(x=rev(sx[,1]), y=rev(sx[,2])))

# random points in rectangle
x <- runif(1000, min= 6, max=15)
y <- runif(1000, min=46, max=56)

ok <- inside.owin(x, y, saxony)

points(x[ok], y[ok])
points(x[!ok], y[!ok], pch=".")
##########################################################



From sdavis2 at mail.nih.gov  Thu Sep  8 13:03:27 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 08 Sep 2005 07:03:27 -0400
Subject: [R] Interpolating / smoothing missing time series data
In-Reply-To: <971536df05090719199a2ce02@mail.gmail.com>
Message-ID: <BF4592BF.DDF0%sdavis2@mail.nih.gov>

On 9/7/05 10:19 PM, "Gabor Grothendieck" <ggrothendieck at gmail.com> wrote:

> On 9/7/05, David James <djames at frontierassoc.com> wrote:
>> The purpose of this email is to ask for pre-built procedures or
>> techniques for smoothing and interpolating missing time series data.
>> 
>> I've made some headway on my problem in my spare time.  I started
>> with an irregular time series with lots of missing data.  It even had
>> duplicated data.  Thanks to zoo, I've cleaned that up -- now I have a
>> regular time series with lots of NA's.
>> 
>> I want to use a regression model (i.e. ARIMA) to ill in the gaps.  I
>> am certainly open to other suggestions, especially if they are easy
>> to implement.
>> 
>> My specific questions:
>> 1.  Presumably, once I get ARIMA working, I still have the problem of
>> predicting the past missing values -- I've only seen examples of
>> predicting into the future.
>> 2.  When predicting the past (backcasting), I also want to take
>> reasonable steps to make the data look smooth.
>> 
>> I guess I'm looking for a really good example in a textbook or white
>> paper (or just an R guru with some experience in this area) that can
>> offer some guidance.
>> 
>> Venables and Ripley was a great start (Modern Applied Statistics with
>> S).  I really had hoped that the "Seasonal ARIMA Models" section on
>> page 405 would help.  It was helpful, but only to a point.  I have a
>> hunch (based on me crashing arima numerous times -- maybe I'm just
>> new to this and doing things that are unreasonable?) that using
>> hourly data just does not mesh well with the seasonal arima code?
> 
> Not sure if this answers your question but if you are looking for something
> simple then na.approx in the zoo package will linearly interpolate for you.
> 
>> z <- zoo(c(1,2,NA,4,5))
>> na.approx(z)
> 1 2 3 4 5 
> 1 2 3 4 5

Alternatively, if you are looking for "more smoothing", you could look at
using a moving average or median applied at points of interest with an
"appropriate" window size--see wapply in the gplots package (gregmisc
bundle).  There are a number of other functions that can accomplish the same
task.  A search for "moving window" or "moving average" in the archives may
produce some other ideas.

Sean



From Roger.Bivand at nhh.no  Thu Sep  8 13:08:19 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 8 Sep 2005 13:08:19 +0200 (CEST)
Subject: [R] Using R map data to determine associated state for a
 coordinate?
In-Reply-To: <43201349.2070402@hhbio.wasser.tu-dresden.de>
Message-ID: <Pine.LNX.4.44.0509081248580.7673-100000@reclus.nhh.no>

On Thu, 8 Sep 2005, Thomas Petzoldt wrote:

> Werner Wernersen wrote:
> > Hi!
> > 
> > I have no idea if this is maybe an easy task utilizing
> > R since I read there is 
> > geographical map data in some package:
> > 
> > I have a huge number of geographical points with their
> > coordinates in Germany. 
> > Now I want to determine for each point in which
> > "Bundesland" = state it is located.
> > 
> > Can anybody tell me if this is doable relatively easy
> > in R and if so give me 
> > some hints or links how to do it?
> > 
> > Thanks a million,
> >    Werner
> 
> Hello Werner,
> 
> two building blocks, but don't know if the precision meets your needs.
> 
> 1. Do you have a good map of Germany *with* Federal States?
> 
> * If YES and if it's free:
> ==> I would be interested! Please post it's source.
> 
> * If NO:
> ==> Downloadable map data are available on:
>      http://www.vdstech.com/map_data.htm
> 
> 2. The following approach reads and converts a shapefile with functions 
> from maptools and then follows the example of inside.owin() from the 
> spatstat package.

The example code will work very well, but since yesterday, when we
released a new version of maptools depending on the sp package, it can
look like:

> library(maptools)
Loading required package: foreign
Loading required package: sp
> nc <- readShapePoly(system.file("shapes/sids.shp", package="maptools")[1])
> plot(nc, lwd=2, border="grey")
> bbox(nc)
         min       max
r1 -84.32385 -75.45698
r2  33.88199  36.58965
> x <- runif(1000, min=-84.32385, max=-75.45698)
> y <- runif(1000, min=33.88199, max=36.58965)
> xypts <- SpatialPoints(cbind(x, y))
> plot(xypts, add=TRUE, pch=19, cex=0.2)
> where_am_i <- overlay(xypts, nc)
> plot(xypts[is.na(where_am_i),], add=TRUE, pch=19, cex=0.2, col="grey80")  
> summary(where_am_i)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
   1.00   29.00   51.00   53.11   77.00  100.00  450.00 

and in this case the points would be read into the SpatialPoints object 
directly. Should overlay have trouble with the "huge" number of points, 
you could take them in smaller batches, storing the intermediate results. 
As Thomas said, you need the boundaries of the "Bundesland" first, and the 
accuracy of your results will depend on the degree of detail of the 
boundary polygons.

Roger Bivand

> 
> Hope that helps
> 
> Thomas Petzoldt
> 
> 
> ##########################################################
> library(maptools)
> library(spatstat)
> 
> ger <- read.shape("germany.shp")
> plot(ger)
> 
> pger <- Map2poly(ger)
> sx<- pger[[13]]
> lines(sx, type="l", col="red") # Saxony ;-)
> 
> ## Create an owin (observation window) object
> # direction of coordinates must be reversed, in some cases
> # if error message: remove rev()'s
> saxony <- owin(poly=list(x=rev(sx[,1]), y=rev(sx[,2])))
> 
> # random points in rectangle
> x <- runif(1000, min= 6, max=15)
> y <- runif(1000, min=46, max=56)
> 
> ok <- inside.owin(x, y, saxony)
> 
> points(x[ok], y[ok])
> points(x[!ok], y[!ok], pch=".")
> ##########################################################
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Ted.Harding at nessie.mcc.ac.uk  Thu Sep  8 13:01:46 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 08 Sep 2005 12:01:46 +0100 (BST)
Subject: [R] Prediction with multiple zeros in the dependent variable
In-Reply-To: <s31f806d.080@grecc.umaryland.edu>
Message-ID: <XFMail.050908120146.Ted.Harding@nessie.mcc.ac.uk>

On 08-Sep-05 John Sorkin wrote:
> I have a batch of data in each line of data contains three values,
> calcium score, age, and sex. I would like to predict calcium scores
> as a function of age and sex, i.e. calcium=f(age,sex). Unfortunately
> the calcium scorers have a very "ugly distribution". There are
> multiple zeros, and multiple values between 300 and 600. There are
> no values between zero and 300. Needless to say, the calcium scores
> are not normally distributed, however, the values between 300 and 600
> have a distribution that is log normal. As you might imagine, the
> residuals from the regression are not normally distributed and thus
> violates the basic assumption of regression analyses. Does anyone
> have a suggestion for a method (or a transformation) that will allow
> me predict calcium from age and sex without violating the assumptions
> of the model?
> Thanks,
> John

>From your description (but only from your description) one might be
tempted to suggest (borrowing a term from Joe Shafer) a "semi-continuous"
model. This means that each observation either takes a discrete value,
or takes a value with a continuous distribution. In your case this
might be

Score = 0 with probability p which is a function of Age and Sex
Score = X with probability (1-p) where X has a log-normal distribution.

Whether using such a model, for data arising in the context you refer
to, is reasonable  depends on whether "Calcium Score = 0" is a reasonable
description of a biological state of things. Even if not a reasonable
biological state, it may be a reasonable description of the outcome
of a measurement process (e.g. too small to measure), in which case
there may be a consequential issue -- what is the likely distribution
of calcium values which give rise to Score = 0? (Though your data may
be uninformative about this). However, if your aim is simply predicting
calcium scores, then this may be irrelevant.

With such a model, you should be able to make progress by using
a log-linear model for the probability p (which may be adequately
addressed by simply using a logistic regression for the event
"Score = 0" or equivalently "score != 0", though you may need to
be careful about how you represent Age as a covariate; Sex, being
binary, should not present problems). This then allowes you to predict
the probability of zero score, and the complementary probability
of non-zero score.

Then you can consider the problem of estimating the relationship
between Score and (Age, Sex) conditional on Score != 0.

This, in turn, is no more (and no less!) complicated than estimating
the continuous distribution of non-zero scores from the subset of
the data which carries such scores.

If the distribution of non-zero scores were (as you suggest) a simple
log-normal distribution, then a regression of log(Score) on Age and
Sex might do well.

However, from your description, it may not be a simple log-normal.

The absence of scores between 0 and 300, and the containment of
score values betweem 300 and 600, suggests a 3-parameter log-normal
in which, as well as the mean and SD for the normal distribution of
log(X) there is also a lower limit S0, so that it is

  log(S - S0)

which has the N(mean,SD^2) distribution. The distribution might be
more complicated than this.

So, in summary, provided a "semi-continuous" model is acceptable,
you can proceed by estimating its two aspects separately: The
discrete part by a logistic (or other suitable binary) regression,
using 'glm' in R; the continuous part by a suitable regression
(using e.g. 'lm' in R) perhaps after suitable transformation
(though this may need care). In each case, it is only the relevant
part of the data (the proportions with "Score = 0" and "Score != 0"
on the one hand, the values of Score where "Score != 0" on the other
hand, in each case using the corresponding (Age, Sex) as covariates)
which will be needed.

Once you have these estimated models, they can be used straightforwardly
for prediction: Given Age and Sex, the Score will be zero with
estimated probability p(Age,Sex) or, with probability (1 - p(Age,Sex)),
will have a distribution implied by your regression.

So the structure of the predicted values will be the same as the
structure of the observed values. All very straightforward, provided
this is a reasonable way to go.

However, there is a complication in that the above might well not
be a reasonable model (as hinted at above). As an example, consider
the following (purely hypothetical assumptions).

1. The true distribution of Calcium Score is (say) simple log-normal
   such that log(Score) is normal with mean linearly dependent on Age
   and Sex, in all subjects.

2. In attempting to measure true Score (i.e. in obtaining observed
   Calcium Score data), there is a probability that "Score = 0"
   will be obtained, and this probability depends on the true Score
   (e.g. the smaller the true Score, the higher the probability of
   obtaining "Score = 0").

The resulting non-zero score data will then no longer have the log-normal
distribution assumed in (1), since the frequency of occurrence of
smaller values will be attenutated by a factor equal to the probability
that such a value will result in "Score != 0".

(I'm inclined to suspect, from your statement about "300-600", that
this might indeed be the case.)

If this is what is going on, then a different kind of approach is
needed. Each "Score = 0" would in fact correspond to an unobserved
non-zero value of Score, and the estimation of the distribution of
true Score would be straightforward if you knew what these values
were. Conditional on knowing the overall distribution, the distribution
of unobserved values conditional on "Score = 0" could be obtained,
and from this distribution could be derived the information you would
need to estimate the distribution of true Score which you need for
estimating the cinditional distribution ...

In other words, we are in effect in an "EM-Algorithm" situation.
This can certainly be solved in R (though I can't at this moment
provide any pointers to R-implementations of a solution for your
specific problem).

However, it would be quite feasible for poeple to construct
suggestions for solving your problem along these lines. But before
people get involved in the work needed, it would be very helpful
if you would respond to the comments above in terms of the real
situation you are dealing with, so that we know what sort of thing
we should be thinking about.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Sep-05                                       Time: 12:01:38
------------------------------ XFMail ------------------------------



From nhy303 at abdn.ac.uk  Thu Sep  8 13:33:19 2005
From: nhy303 at abdn.ac.uk (nhy303@abdn.ac.uk)
Date: Thu, 8 Sep 2005 12:33:19 +0100 (BST)
Subject: [R] Time series ARIMAX and multivariate models
Message-ID: <1390.139.133.94.35.1126179199.squirrel@www.abdn.ac.uk>

Dear List,

The purpose of this e-mail is to ask about R time series procedures - as a
biologist with only basic time series knowledge and about a year's
experience in R.

I have been using ARIMAX models with seasonal components on seasonal data.
 However I am now moving on to annual data (with only 34 time points) and
understand that ARIMA is not suitable for these shorter time periods -
does R have other, more robust, methods?

I have tried looking through the R help pages & documentation for packages
but am unsure what model type is suitable.

Secondly, I wish to start building multivariate time series models in R to
look at how fish condition (for several sizes of fish) is affected by
environmental factors and numbers of prey.  It would be great if someone
could suggest what R packages/documentation would be useful to research?

Thankyou,

Lillian.



From jfox at mcmaster.ca  Thu Sep  8 13:44:20 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 8 Sep 2005 07:44:20 -0400
Subject: [R] tcltk, X11 protocol error:  Bug?
In-Reply-To: <1125958583.21550.242223315@webmail.messagingengine.com>
Message-ID: <20050908114419.ILBU21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Nicholas,

This problem has been reported before (enter "X11 protocol error" on the R
site search at http://finzi.psych.upenn.edu/search.html to see the previous
threads), but as far as I know, there's no definitive explanation or
solution. As well, things appear to work fine, despite the warnings. The way
I handle the problem in the Rcmdr package is simply to intercept the
warnings.

I hope this helps,
 John 

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Nicholas Lewin-Koh
> Sent: Monday, September 05, 2005 5:16 PM
> To: r-help at r-project.org
> Subject: [R] tcltk, X11 protocol error: Bug?
> 
> Hi,
> I am having trouble debugging this one. The code is attached 
> below, but it seems to be a problem at the C-tk interface. If 
> I run this 1 time there are no problems if I run it more than 
> once I start to get warnings that increase in multiples of 11 
> everytime I run it. Here is a sample session
> 
> 
> > source("clrramp2.r")
> Loading required package: tcltk
> Loading Tcl/Tk interface ... done
> > clrRamp()
> >
> > tt<-clrRamp()
> > tt
> function (n)
> {
>     x <- ramp(seq(0, 1, length = n))
>     rgb(x[, 1], x[, 2], x[, 3], max = 255) }
> <environment: 0x8b8674c>
> > image(matrix(1:10),col=tt(10))
> > tt<-clrRamp()
> There were 22 warnings (use warnings() to see them)
> > image(matrix(1:10),col=tt(10))
> There were 11 warnings (use warnings() to see them)
> > warnings()
> Warning messages:
> 1: X11 protocol error: BadWindow (invalid Window parameter)
> 2: X11 protocol error: BadWindow (invalid Window parameter)
> 3: X11 protocol error: BadWindow (invalid Window parameter)
> 4: X11 protocol error: BadWindow (invalid Window parameter)
> 5: X11 protocol error: BadWindow (invalid Window parameter)
> 6: X11 protocol error: BadWindow (invalid Window parameter)
> 7: X11 protocol error: BadWindow (invalid Window parameter)
> 8: X11 protocol error: BadWindow (invalid Window parameter)
> 9: X11 protocol error: BadWindow (invalid Window parameter)
> 10: X11 protocol error: BadWindow (invalid Window parameter)
> 11: X11 protocol error: BadWindow (invalid Window parameter)
> 
> I am running R-2.1.1 on ubuntu linux 5.04, compiled from 
> source (not the deb package).
> My version of tcl/tk is 8.4. The code is below. If anyone 
> sees something I am doing foolish let me know, otherwise I 
> will file a bug report.
> 
> Nicholas
> 
> ##### File clrramp2.r ##############
> 
> require(tcltk)
> clrRamp <- function(n.col, b.color=NULL,e.color=NULL){
> 
>   B.ChangeColor <- function()
>     {
>  
>       b.color <<- 
> tclvalue(tkcmd("tk_chooseColor",initialcolor=e.color,
>                                  title="Choose a color"))
>       if (nchar(b.color)>0){
>         tkconfigure(canvas.b,bg=b.color)
>         Rmp.Draw()
>       }
>     }
> 
>   E.ChangeColor <- function()
>     {
>  
>       e.color <<- 
> tclvalue(tkcmd("tk_chooseColor",initialcolor=e.color,
>                                  title="Choose a color"))
>       ##cat(e.color)
>       if (nchar(e.color)>0){
>         tkconfigure(canvas.e,bg=e.color)
>         Rmp.Draw()
>       }
>     }
> 
>   Rmp.Draw <-function(){
>     
> cr<<-colorRampPalette(c(b.color,e.color),space="Lab",interpola
> te="spline")
>     rmpcol <- cr(n.col)
>     #rmpcol<-rgb( rmpcol[,1],rmpcol[,2],rmpcol[,3])
>     inc <- 300/n.col
>     xl <- 0
>     for(i in 1:n.col){
>       ##item <- 
>       
> tkitemconfigure(canvas.r,barlst[[i]],fill=rmpcol[i],outline=rmpcol[i])
>       #xl <- xl+inc
>     }
>   }
> 
>   save.ramp <- function(){
>     
> cr<<-colorRampPalette(c(b.color,e.color),space="Lab",interpola
> te="spline")
>     tkdestroy(tt)
>     ##invisible(cr)
>   }
> 
>   tt <- tktoplevel()
>   tkwm.title(tt,"Color Ramp Tool")
>   frame <- tkframe(tt)
>   bframe <- tkframe(frame,relief="groove",borderwidth=1)
> 
>   if(is.null(b.color)) b.color <- "blue"
>   if(is.null(e.color)) e.color <- "yellow"
>   if(missing(n.col)) n.col <- 100
> 
>   canvas.b <- tkcanvas(bframe,width="50",height="25",bg=b.color)
>   canvas.e <- tkcanvas(bframe,width="50",height="25",bg=e.color)
>   canvas.r <- tkcanvas(tt,width="300",height="50",bg="white")
>   
>   BColor.button <- tkbutton(bframe,text="Begin
>   Color",command=B.ChangeColor)
>   ##tkgrid(canvas.b,BColor.button)
>   EColor.button <- tkbutton(bframe,text="End
>   Color",command=E.ChangeColor)
>   killbutton <- tkbutton(bframe,text="Save",command=save.ramp)
>   tkgrid(canvas.b,BColor.button,canvas.e,EColor.button)
>   tkgrid(bframe)
>   tkgrid(frame)
>   tkgrid(canvas.r)
>   tkgrid(killbutton)
> 
>   
> cr<-colorRampPalette(c(b.color,e.color),space="Lab",interpolat
> e="spline")
>   ##rmpcol <- hex(mixcolor(alpha,bc,ec,where="LUV"))
>   rmpcol <- cr(n.col)
>   inc <- 300/n.col
>   xl <- 0
>   #barlst <- vector(length=n.col,mode="list")
>   barlst <- tclArray()
>   for(i in 1:n.col){
>     item<-tkcreate(canvas.r,"rect",xl,0,xl+inc,50,
>                    fill=rmpcol[i],outline=rmpcol[i])
>     ##tkaddtag(canvas.r, "point", "withtag", item)
>     barlst[[i]]<-item
>     xl <- xl+inc
>   }
>   tkgrab.set(tt)
>   tkwait.window(tt)
> 
>   ##tkdestroy(tt)
>   invisible(cr)
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From fisher at plessthan.com  Thu Sep  8 14:22:51 2005
From: fisher at plessthan.com (Dennis Fisher)
Date: Thu, 8 Sep 2005 05:22:51 -0700
Subject: [R] Converting a matrix to a dataframe: how to prevent conversion
	to factor
In-Reply-To: <mailman.13.1126173601.27361.r-help@stat.math.ethz.ch>
References: <mailman.13.1126173601.27361.r-help@stat.math.ethz.ch>
Message-ID: <03F2E9C8-4844-4916-8CB2-0335CC7744BD@plessthan.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050908/504eea9c/attachment.pl

From f.harrell at vanderbilt.edu  Thu Sep  8 14:24:51 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 08 Sep 2005 07:24:51 -0500
Subject: [R] Prediction with multiple zeros in the dependent variable
In-Reply-To: <s31f806d.080@grecc.umaryland.edu>
References: <s31f806d.080@grecc.umaryland.edu>
Message-ID: <43202D93.9070500@vanderbilt.edu>

John Sorkin wrote:
> I have a batch of data in each line of data contains three values,
> calcium score, age, and sex. I would like to predict calcium scores as a
> function of age and sex, i.e. calcium=f(age,sex). Unfortunately the
> calcium scorers have a very "ugly distribution". There are multiple
> zeros, and multiple values between 300 and 600. There are no values
> between zero and 300. Needless to say, the calcium scores are not
> normally distributed, however, the values between 300 and 600 have a
> distribution that is log normal. As you might imagine, the residuals
> from the regression are not normally distributed and thus violates the
> basic assumption of regression analyses. Does anyone have a suggestion
> for a method (or a transformation) that will allow me predict calcium
> from age and sex without violating the assumptions of the model?
> Thanks,
> John
>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC

John - first I would try a proportional odds model, with zero as its own 
category then treating all other values as continuous or collapsing them 
into 20-tiles.  If the PO assumption happens to hold (look at partial 
residual plots) you have a simple solution.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From jfontain at free.fr  Thu Sep  8 14:33:44 2005
From: jfontain at free.fr (jfontain@free.fr)
Date: Thu, 08 Sep 2005 14:33:44 +0200
Subject: [R] Time Series Analysis: book?
Message-ID: <1126182824.43202fa8c49be@imp1-g19.free.fr>

There has been a few questions on the subject lately.
Is there any book on the subject, if possible with a computer processing flavor,
that you would highly recommend?


Many thanks in advance,


--
Jean-Luc



From p.dalgaard at biostat.ku.dk  Thu Sep  8 14:51:54 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Sep 2005 14:51:54 +0200
Subject: [R] Converting a matrix to a dataframe: how to prevent
	conversion to factor
In-Reply-To: <03F2E9C8-4844-4916-8CB2-0335CC7744BD@plessthan.com>
References: <mailman.13.1126173601.27361.r-help@stat.math.ethz.ch>
	<03F2E9C8-4844-4916-8CB2-0335CC7744BD@plessthan.com>
Message-ID: <x2d5nj2045.fsf@turmalin.kubism.ku.dk>

Dennis Fisher <fisher at plessthan.com> writes:

> Colleages
> 
> I am running R 2.1.0 on a Mac (same problem occurs in Linux).  In  
> some situations, I have mixed text/numeric data that is stored as  
> characters in a matrix.  If I convert this matrix to a dataframe, the  
> numeric data becomes factors, not what I intend.
> 
>      TEXT    <- paste("Text", 1:4, sep="")
>      NUMBERS    <- 10 + 4:1
>      MATRIX    <- cbind(TEXT, NUMBERS)
>      FRAME    <- as.data.frame(MATRIX)
> 
>  > str(FRAME)
> `data.frame':    4 obs. of  2 variables:
> $ TEXT   : Factor w/ 4 levels "Text1","Text2",..: 1 2 3 4
> $ NUMBERS: Factor w/ 4 levels "11","12","13",..: 4 3 2 1
> 
> One work-around is to write the matrix (or the dataframe) to a file,  
> then read the file back using the as.is argument.
>      write.table(MATRIX, "JUNK", row.names=F)
>      NEWFRAME    <- read.table("JUNK", as.is=T, header=T)
> 
>  > str(NEWFRAME)
> `data.frame':    4 obs. of  2 variables:
> $ TEXT   : chr  "Text1" "Text2" "Text3" "Text4"
> $ NUMBERS: int  14 13 12 11
> 
> This restores the NUMBERS to their intended mode (integers, not  
> factors).  The text column is also not read as a factor (not a  
> problem for me).
> 
> It appears that the function AsIs [I(x)] would enable me to  
> accomplish this without the write/read steps.  However, it is not  
> obvious to me how to implement I(x).  Can anyone advise?

I don't think that is going to help.... 

There are really several issues here: Your numeric column was
converted to character by the cbind, using as.data.frame(I(MATRIX))
will not split it into individual columns, and things like
apply(MATRIX,2,f) may do the right thing to begin with, but then
there's coercion due to an implicit cbind at the end.

It's a bit awkward, but this may do it:

> FRAME <- as.data.frame(lapply(split(MATRIX,col(MATRIX)),type.convert))
> names(FRAME) <- colnames(MATRIX)
> str(FRAME)
`data.frame':   4 obs. of  2 variables:
 $ TEXT   : Factor w/ 4 levels "Text1","Text2",..: 1 2 3 4
 $ NUMBERS: int  14 13 12 11

whereas this isn't right:

> str(apply(MATRIX,2,type.convert))
 int [1:4, 1:2] 1 2 3 4 14 13 12 11
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:2] "TEXT" "NUMBERS"



-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dmbates at gmail.com  Thu Sep  8 14:56:43 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 8 Sep 2005 07:56:43 -0500
Subject: [R] FW: Re: Doubt about nested aov output
In-Reply-To: <49747.194.57.165.22.1126171408.squirrel@webmail.lyon.inserm.fr>
References: <49747.194.57.165.22.1126171408.squirrel@webmail.lyon.inserm.fr>
Message-ID: <40e66e0b05090805564dd77cba@mail.gmail.com>

On 9/8/05, Ken Knoblauch <knoblauch at lyon.inserm.fr> wrote:
> Your response nicely clarifies a question that I've had for a long time,
> but which I've dealt
> with by giving each subject a unique label.  Unless I'm missing something,
> both techniques should
> work as the toy example below gives exactly the same output in all 3 cases
> below (forgetting
> about the convergence problem).  Would there be a reason to prefer
> labeling the levels
> one way or another or is it just a matter of convenience?
> 
> library(lmer)
> y <- rnorm(15)
> cond <- gl(3, 5, 15)
> obs <- gl(15, 1)
> subj <- gl(5, 1, 15)
> dd <- data.frame(y = y, cond = cond, obs = obs, subj = subj)
> 
> l1 <- lmer(y~cond + (1|cond:obs), dd)
> l2 <- lmer(y~cond + (1|cond:subj), dd)
> l3 <- lmer(y~cond + (1|obs), dd)

I prefer to have a grouping factor constructed with unique levels for
each distinct unit.  The only reason I mention constructions like
Treatment:Rat in the original part of this thread is that data are
often provided in that form.

Reusing "subject" labels within another group is awkward and can be
error prone.  One of the data sets I examine in the MlmSoftRev
vignette of the mlmRev package is called Exam and has student
identifiers within schools.  The student identifiers are not unique
but the school:student combination should be.  It isn't.  These data
have been analyzed in scores of books and articles and apparently none
of the other authors bothered to check this.  There are some
interesting ramifications such as some of the schools that are
classified as mixed-sex schools are likely single-sex schools because
the only student of one of the sexes in that school is apparently
mislabelled.

BTW, in your example you have only one observation per level of 'obs'
so you can't use obs as a grouping factor as this variance component
would be completely confounded with the per-observation noise.

> 
> Douglas Bates a ??crit:
> 
> The difference between models like
>   lmer(Glycogen~Treatment+(1|Rat)+(1|Rat:Liver))
> and
>   lmer(Glycogen~Treatment+(1|Treatment:Rat)+(1|Treatment:Rat:Liver))
> 
> is more about the meaning of the levels of "Rat" than about the
> meaning of "Treatment".  As I understood it there are three different
> rats labelled 1.  There is a rat 1 on treatment 1 and a rat 1 on
> treatment 2 and a rat 1 on treatment 3.  Thus the levels of Rat do not
> designate the "experimental unit", it is the levels of Treatment:Rat
> that do this.
> 
> --
> Ken Knoblauch
> Inserm U371
> Cerveau et Vision
> Dept. of Cognitive Neuroscience
> 18 avenue du Doyen L??pine
> 69500 Bron
> France
> tel: +33 (0)4 72 91 34 77
> fax: +33 (0)4 72 91 34 61
> portable: +33 (0)6 84 10 64 10
> http://www.lyon.inserm.fr/371/
> 
>



From knoblauch at lyon.inserm.fr  Thu Sep  8 19:10:29 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Thu, 8 Sep 2005 15:10:29 -0200 (CEST)
Subject: [R] FW: Re: Doubt about nested aov output
In-Reply-To: <40e66e0b05090805564dd77cba@mail.gmail.com>
References: <49747.194.57.165.22.1126171408.squirrel@webmail.lyon.inserm.fr>
	<40e66e0b05090805564dd77cba@mail.gmail.com>
Message-ID: <50184.194.57.165.22.1126185029.squirrel@webmail.lyon.inserm.fr>

Thank you for your response.  The single response/observer most probably
explains
the complaints that lmer was giving for my example.  Maybe this small
modification
provides a better example and corrects a more serious error in my previous
post:

library(lme4)
y<-rnorm(30)
cond <- rep(gl(3,5,15), 2)
obs<-rep(gl(15,1), 2)
subj<-rep(gl(5,1,15), 2)
dd<-data.frame(y=y,cond=cond,obs=obs,subj=subj)

l1 <- lmer(y~cond + (1|cond:obs), data=dd)
l2 <- lmer(y~cond + (1|cond:subj), data=dd)
l3 <- lmer(y~cond + (1|obs), dd)

Understanding the notation is often about 99% of the job, and it is
very helpful to see multiple ways to accomplish the same thing.

> Douglas Bates a ??crit:
> I prefer to have a grouping factor constructed with unique levels for
> each distinct unit.  The only reason I mention constructions like
> Treatment:Rat in the original part of this thread is that data are
> often provided in that form.
>
> Reusing "subject" labels within another group is awkward and can be
> error prone.  One of the data sets I examine in the MlmSoftRev
> vignette of the mlmRev package is called Exam and has student
> identifiers within schools.  The student identifiers are not unique
> but the school:student combination should be.  It isn't.  These data
> have been analyzed in scores of books and articles and apparently none
> of the other authors bothered to check this.  There are some
> interesting ramifications such as some of the schools that are
> classified as mixed-sex schools are likely single-sex schools because
> the only student of one of the sexes in that school is apparently
> mislabelled.
>
> BTW, in your example you have only one observation per level of 'obs'
> so you can't use obs as a grouping factor as this variance component
> would be completely confounded with the per-observation noise.
>
>>
>> Douglas Bates a ??crit:
>>
>> The difference between models like
>>   lmer(Glycogen~Treatment+(1|Rat)+(1|Rat:Liver))
>> and
>>   lmer(Glycogen~Treatment+(1|Treatment:Rat)+(1|Treatment:Rat:Liver))
>>
>> is more about the meaning of the levels of "Rat" than about the
>> meaning of "Treatment".  As I understood it there are three different
>> rats labelled 1.  There is a rat 1 on treatment 1 and a rat 1 on
>> treatment 2 and a rat 1 on treatment 3.  Thus the levels of Rat do not
>> designate the "experimental unit", it is the levels of Treatment:Rat
>> that do this.
>>
>> --
>> Ken Knoblauch
>> Inserm U371
>> Cerveau et Vision
>> Dept. of Cognitive Neuroscience
>> 18 avenue du Doyen L??pine
>> 69500 Bron
>> France
>> tel: +33 (0)4 72 91 34 77
>> fax: +33 (0)4 72 91 34 61
>> portable: +33 (0)6 84 10 64 10
>> http://www.lyon.inserm.fr/371/
>>
>>
>


-- 
Ken Knoblauch
Inserm U371
Cerveau et Vision
Dept. of Cognitive Neuroscience
18 avenue du Doyen L??pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.lyon.inserm.fr/371/



From tlumley at u.washington.edu  Thu Sep  8 16:22:32 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 8 Sep 2005 07:22:32 -0700 (PDT)
Subject: [R] Prediction with multiple zeros in the dependent variable
In-Reply-To: <s31f806d.080@grecc.umaryland.edu>
References: <s31f806d.080@grecc.umaryland.edu>
Message-ID: <Pine.A41.4.63a.0509080716490.177972@homer04.u.washington.edu>

On Thu, 8 Sep 2005, John Sorkin wrote:
> I have a batch of data in each line of data contains three values,
> calcium score, age, and sex. I would like to predict calcium scores as a
> function of age and sex, i.e. calcium=f(age,sex). Unfortunately the
> calcium scorers have a very "ugly distribution". There are multiple
> zeros, and multiple values between 300 and 600. There are no values
> between zero and 300. Needless to say, the calcium scores are not
> normally distributed, however, the values between 300 and 600 have a
> distribution that is log normal.

[Coronary artery calcium by EBCT, I presume]

Our approach to modelling calcium scores is to do it in two parts.  First 
fit something like a logistic regression model where the outcome is zero 
vs non-zero calcium.  Then, for the non-zero use something like a linear 
regression model for log calcium.

You could presumably use such a model for prediction or imputation too, 
and you can work out means, medians etc from the two models.

One particular reason for using this two-part model is that we find 
different predictors of zero/non-zero and of amount. This makes biological 
sense -- a factor that makes arterial plaques calcify might well have no 
impact until you have arterial plaques.

Or you could use smooth quantile regression in the rq package.

 	-thomas



From tlumley at u.washington.edu  Thu Sep  8 16:27:32 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 8 Sep 2005 07:27:32 -0700 (PDT)
Subject: [R] Survival model with cross-classified shared frailties
In-Reply-To: <5abc11d8050908020811812817@mail.gmail.com>
References: <5abc11d8050908020811812817@mail.gmail.com>
Message-ID: <Pine.A41.4.63a.0509080724420.177972@homer04.u.washington.edu>

On Thu, 8 Sep 2005, Shige Song wrote:

> Dear All,
>
> The "coxph" function in the "survival" package allows multiple frailty
> terms.

Um, no, it doesn't.

> In all the examples I saw, however, the frailty terms are nested.
> What will happen if I have non-nested (that is, cross-classified) frailties
> in the model?

This wouldn't work even if it did allow multiple frailty terms.


You may want the coxme() function in the "kinship" package.


 	-thomas



From erich.neuwirth at univie.ac.at  Thu Sep  8 16:51:37 2005
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Thu, 08 Sep 2005 16:51:37 +0200
Subject: [R] array indices in synced vectors
Message-ID: <43204FF9.2010708@univie.ac.at>

Let us start with the following definitions

xxx<-rep(c(1,2),times=5)
yyy<-rep(c(1,2),each=5)
a<-c(11,12)
b<-matrix(1:4,2,2)

a[xxx] produces
[1] 11 12 11 12 11 12 11 12 11 12

b[xxx,yyy] produces
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]    1    1    1    1    1    3    3    3    3     3
 [2,]    2    2    2    2    2    4    4    4    4     4
 [3,]    1    1    1    1    1    3    3    3    3     3
 [4,]    2    2    2    2    2    4    4    4    4     4
 [5,]    1    1    1    1    1    3    3    3    3     3
 [6,]    2    2    2    2    2    4    4    4    4     4
 [7,]    1    1    1    1    1    3    3    3    3     3
 [8,]    2    2    2    2    2    4    4    4    4     4
 [9,]    1    1    1    1    1    3    3    3    3     3
[10,]    2    2    2    2    2    4    4    4    4     4

so it does an implicit "outer" for the indices in xxx and yyy.

sapply(1:length(xxx),function(x)b[xxx[x],yyy[x]])
does what I need and produces
 [1] 1 2 1 2 1 4 3 4 3 4

Is there a function taking xxx,yyy, and b as arguments
producing the same result?

Essentially, I am asking for a version of lapply and/or sapply
which works with functions of more than one argument and takes the
iteration arguments as vectors or lists of equal length.




-- 
Erich Neuwirth, Didactic Center for Computer Science
University of Vienna
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-39902 Fax: +43-1-4277-9399



From tlumley at u.washington.edu  Thu Sep  8 16:58:39 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 8 Sep 2005 07:58:39 -0700 (PDT)
Subject: [R] array indices in synced vectors
In-Reply-To: <43204FF9.2010708@univie.ac.at>
References: <43204FF9.2010708@univie.ac.at>
Message-ID: <Pine.A41.4.63a.0509080756430.177972@homer04.u.washington.edu>

On Thu, 8 Sep 2005, Erich Neuwirth wrote:
>
> sapply(1:length(xxx),function(x)b[xxx[x],yyy[x]])
> does what I need and produces
> [1] 1 2 1 2 1 4 3 4 3 4
>
> Is there a function taking xxx,yyy, and b as arguments
> producing the same result?

b[cbind(xxx,yyy)]

> Essentially, I am asking for a version of lapply and/or sapply
> which works with functions of more than one argument and takes the
> iteration arguments as vectors or lists of equal length.

More generally there is mapply(), but the matrix subscript solution is 
better in this example
> mapply(function(i,j) b[i,j], xxx,yyy)
  [1] 1 2 1 2 1 4 3 4 3 4

 	-thomas



From luisen.p at gmail.com  Thu Sep  8 17:09:25 2005
From: luisen.p at gmail.com (Luis Pineda)
Date: Thu, 8 Sep 2005 11:09:25 -0400
Subject: [R] Predicting responses using ace
In-Reply-To: <431F629D.3050405@vanderbilt.edu>
References: <859087cf0509061233350d7dba@mail.gmail.com>
	<431E45C4.401@vanderbilt.edu>
	<859087cf05090704347babc11b@mail.gmail.com>
	<431ED777.1080208@vanderbilt.edu>
	<859087cf05090705371776d8eb@mail.gmail.com>
	<431EFEF5.5080704@vanderbilt.edu>
	<859087cf05090712446d40ad0b@mail.gmail.com>
	<431F629D.3050405@vanderbilt.edu>
Message-ID: <859087cf05090808097f6a692f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050908/ab1310d6/attachment.pl

From luisen.p at gmail.com  Thu Sep  8 17:20:55 2005
From: luisen.p at gmail.com (Luis Pineda)
Date: Thu, 8 Sep 2005 11:20:55 -0400
Subject: [R] Predicting responses using ace
In-Reply-To: <859087cf05090808097f6a692f@mail.gmail.com>
References: <859087cf0509061233350d7dba@mail.gmail.com>
	<431E45C4.401@vanderbilt.edu>
	<859087cf05090704347babc11b@mail.gmail.com>
	<431ED777.1080208@vanderbilt.edu>
	<859087cf05090705371776d8eb@mail.gmail.com>
	<431EFEF5.5080704@vanderbilt.edu>
	<859087cf05090712446d40ad0b@mail.gmail.com>
	<431F629D.3050405@vanderbilt.edu>
	<859087cf05090808097f6a692f@mail.gmail.com>
Message-ID: <859087cf050908082062c553dd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050908/02db66f6/attachment.pl

From gunter.berton at gene.com  Thu Sep  8 17:21:28 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 8 Sep 2005 08:21:28 -0700
Subject: [R] Prediction with multiple zeros in the dependent variable
In-Reply-To: <s31f806d.080@grecc.umaryland.edu>
Message-ID: <200509081521.j88FLSVQ026556@faraday.gene.com>

John:

1. As George Box long ago emphasized and proved, normality is **NOT** that
important in regression, certainly not for estimation and not even for
inference in balanced designs. Independence of the observations is far more
important. 

2. That said, it sounds like what you have here is a mixture of some sort.
Before running off to do fancy modeling, I would work very hard to look for
some kind of "lurking variable" or experimental aberration -- what was going
on in the experiment or study that might have caused all the zeros? Was
there an instrument problem? -- a bad reagent? -- improper handling of the
samples? It might very well be that you need to throw away part of the data
because it's useless, rather than artificially attempt to model it.

3. And having said that, if a comprehensive model IS called for, one rather
cynical approach to take is just to add a grouping variable as a covariate
that has a value of 1 for all data in the zero group and 2 for all the
nonzero data. Your model is f(age,sex) = 0 for all data in group 1 and your
linear or nonlinear regression for group 2. Of course, this merely cloaks
the cynicism in respectable dress. It's hard for me to believe that it was
Mother Nature and not some kind of experimental problem that you see. 

A slightly less cynical approach might be to use some sort of changepoint
model (in both age and sex) of the form f(age, sex) = g(age,sex) for age>=k1
and sex <=k2 and h(age,sex) otherwise. Well, perhaps **not** less cynical --
the response data are so widely separated that you'll just be using a bunch
of extra (nonlinear, incidentally) parameters to essentially reproduce the
use of a covariate.

So I guess the point is that unless you already have a previously developed
nonlinear model that could explain the behavior you see (perhaps based on
some kind of mechanistic reasoning) it's not a good idea to try to develop
an artificial empirical model that comprehends all the data. The fact is (a
horrible phrase) that no modeling at all is needed for the most important
message the data have to convey: rather, focus on the cause of the message
instead of statistical artifice. Once you have determined that, you may be
able to do something sensible. Clear thinking trumps muddy modeling every
time.

(Hopefully, this is sufficiently inflammatory that others will vigorously
and wisely dispute me).

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Sorkin
> Sent: Wednesday, September 07, 2005 9:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Prediction with multiple zeros in the dependent variable
> 
> I have a batch of data in each line of data contains three values,
> calcium score, age, and sex. I would like to predict calcium 
> scores as a
> function of age and sex, i.e. calcium=f(age,sex). Unfortunately the
> calcium scorers have a very "ugly distribution". There are multiple
> zeros, and multiple values between 300 and 600. There are no values
> between zero and 300. Needless to say, the calcium scores are not
> normally distributed, however, the values between 300 and 600 have a
> distribution that is log normal. As you might imagine, the residuals
> from the regression are not normally distributed and thus violates the
> basic assumption of regression analyses. Does anyone have a suggestion
> for a method (or a transformation) that will allow me predict calcium
> from age and sex without violating the assumptions of the model?
> Thanks,
> John
>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>  
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>  
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ariel.chernomoretz at crchul.ulaval.ca  Thu Sep  8 17:40:09 2005
From: ariel.chernomoretz at crchul.ulaval.ca (Ariel Chernomoretz)
Date: Thu, 8 Sep 2005 11:40:09 -0400
Subject: [R] ROracle install problem
Message-ID: <200509081140.09814.ariel.chernomoretz@crchul.ulaval.ca>

Hi, 

I am trying to install the ROracle package in a Linux-64 machine.
I downloaded from Oracle's site their Instant Client bundle but it seems that
ROracle needs some stuff not included in that kit in order to compile (in 
particuar, the 'proc' executable).

I did not find any other linux client suite in Oracle's site, (our db runs on 
a Solaris server, so I can not use the included binaries).

Does anybody know how to solve this? Is there any workaround? 
 
Thanks,

Ariel./


-- 
Ariel Chernomoretz, Ph.D.
Centre de recherche du CHUL
2705 Blv Laurier, bloc T-367
Sainte-Foy, Qc
G1V 4G2
(418)-525-4444 ext 46339



From sonia.petrone at uni-bocconi.it  Thu Sep  8 17:47:43 2005
From: sonia.petrone at uni-bocconi.it (sonia)
Date: Thu,  8 Sep 2005 17:47:43 +0200
Subject: [R] package installation error (LF versus CR)
Message-ID: <1126194463.43205d1fa71e5@webmail.uni-bocconi.it>

Hello, 
I have the following problem in installing a package (in windows xp) 

>rcmd install -c  dlm

[ ..stuff deleted ]

  ... DLL made
  installing DLL
  installing R files
  installing inst files
  installing data files
  installing man source files
  installing indices
Errore in load(zfile, envir = envir) : l'input ?? stato danneggiato, LF 
sostituiti da CR
Esecuzione interrotta
make[2]: *** [indices] Error 1
make[1]: *** [all] Error 2
make: *** [pkg-dlm] Error 2
*** Installation of dlm failed ***

Does anybody please have suggestions? 
If relevant, the source package was downloaded from a unix server using cvs+ssh

Thanks a lot, 
best, 
Sonia 

-- 
Sonia Petrone
Istituto di Metodi Quantitativi
Universit?? Bocconi
Viale Isonzo 25
20135 Milano, Italia.



From famille.tessier at wanadoo.fr  Thu Sep  8 17:47:49 2005
From: famille.tessier at wanadoo.fr (Laurent TESSIER)
Date: Thu,  8 Sep 2005 17:47:49 +0200 (CEST)
Subject: [R] R API call from delphi
Message-ID: <18668981.1126194469884.JavaMail.www@wwinf1615>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050908/c31db1e7/attachment.pl

From liuwensui at gmail.com  Thu Sep  8 18:14:33 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 8 Sep 2005 12:14:33 -0400
Subject: [R] Time Series Analysis: book?
In-Reply-To: <1126182824.43202fa8c49be@imp1-g19.free.fr>
References: <1126182824.43202fa8c49be@imp1-g19.free.fr>
Message-ID: <1115a2b005090809146ca2509e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050908/63c0b6a8/attachment.pl

From bernarduse1 at yahoo.fr  Thu Sep  8 18:17:33 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Thu, 8 Sep 2005 18:17:33 +0200 (CEST)
Subject: [R] data manipulation
Message-ID: <20050908161733.30565.qmail@web25805.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050908/95e0e5c9/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Sep  8 18:38:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 08 Sep 2005 18:38:37 +0200
Subject: [R] package installation error (LF versus CR)
In-Reply-To: <1126194463.43205d1fa71e5@webmail.uni-bocconi.it>
References: <1126194463.43205d1fa71e5@webmail.uni-bocconi.it>
Message-ID: <4320690D.2050909@statistik.uni-dortmund.de>

sonia wrote:

> Hello, 
> I have the following problem in installing a package (in windows xp) 
> 
> 
>>rcmd install -c  dlm
> 
> 
> [ ..stuff deleted ]
> 
>   ... DLL made
>   installing DLL
>   installing R files
>   installing inst files
>   installing data files
>   installing man source files
>   installing indices
> Errore in load(zfile, envir = envir) : l'input ?? stato danneggiato, LF 
> sostituiti da CR
> Esecuzione interrotta
> make[2]: *** [indices] Error 1
> make[1]: *** [all] Error 2
> make: *** [pkg-dlm] Error 2
> *** Installation of dlm failed ***
> 
> Does anybody please have suggestions? 
> If relevant, the source package was downloaded from a unix server using cvs+ssh

In principle, it should not matter if it works on the unix machine.
Anyway, can you try to *build* the package on that unix machine and 
install from the tar.gz file on Windows. Maybe some line endings got 
mixed up by cvs...

For further report, please set
LANGUAGE=en
before a sample-run you want to include in a question to R-help, because 
not everybody understands italian.

Uwe Ligges


> Thanks a lot, 
> best, 
> Sonia 
>



From spencer.graves at pdf.com  Thu Sep  8 18:59:41 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 08 Sep 2005 09:59:41 -0700
Subject: [R] Interpolating / smoothing missing time series data
In-Reply-To: <BF4592BF.DDF0%sdavis2@mail.nih.gov>
References: <BF4592BF.DDF0%sdavis2@mail.nih.gov>
Message-ID: <43206DFD.7030706@pdf.com>

(see inline)

Sean Davis wrote:

> On 9/7/05 10:19 PM, "Gabor Grothendieck" <ggrothendieck at gmail.com> wrote:
> 
> 
>>On 9/7/05, David James <djames at frontierassoc.com> wrote:
>>
>>>The purpose of this email is to ask for pre-built procedures or
>>>techniques for smoothing and interpolating missing time series data.
>>>
>>>I've made some headway on my problem in my spare time.  I started
>>>with an irregular time series with lots of missing data.  It even had
>>>duplicated data.  Thanks to zoo, I've cleaned that up -- now I have a
>>>regular time series with lots of NA's.
>>>
>>>I want to use a regression model (i.e. ARIMA) to ill in the gaps.  I
>>>am certainly open to other suggestions, especially if they are easy
>>>to implement.
>>>
>>>My specific questions:
>>>1.  Presumably, once I get ARIMA working, I still have the problem of
>>>predicting the past missing values -- I've only seen examples of
>>>predicting into the future.
>>>2.  When predicting the past (backcasting), I also want to take
>>>reasonable steps to make the data look smooth.
>>>
>>>I guess I'm looking for a really good example in a textbook or white
>>>paper (or just an R guru with some experience in this area) that can
>>>offer some guidance.
>>>
>>>Venables and Ripley was a great start (Modern Applied Statistics with
>>>S).  I really had hoped that the "Seasonal ARIMA Models" section on
>>>page 405 would help.  It was helpful, but only to a point.  I have a
>>>hunch (based on me crashing arima numerous times -- maybe I'm just
>>>new to this and doing things that are unreasonable?) that using
>>>hourly data just does not mesh well with the seasonal arima code?
>>

	  Have you looked at Durbin, J. and Koopman, S. J. (2001) _Time Series 
Analysis by State Space Methods._  Oxford University Press, cited with 
"?arima"?  They explain that Kalman filtering is predicting the future, 
while Kalman smoothing is using all the data to fill the gaps, which 
seems to match your question.  I was able to reproduce Figure 2.1 in 
that book but got bogged down with Figure 2.2 before I dropped the 
project.  I can send you the script file I developed when working on 
that if it would help you.

	  I'm still interested in learning how to reproduce in R all the 
examples in that book, and I'd happily receive suggestions from others 
on how to do that.

	  spencer graves

>>Not sure if this answers your question but if you are looking for something
>>simple then na.approx in the zoo package will linearly interpolate for you.
>>
>>
>>>z <- zoo(c(1,2,NA,4,5))
>>>na.approx(z)
>>
>>1 2 3 4 5 
>>1 2 3 4 5
> 
> 
> Alternatively, if you are looking for "more smoothing", you could look at
> using a moving average or median applied at points of interest with an
> "appropriate" window size--see wapply in the gplots package (gregmisc
> bundle).  There are a number of other functions that can accomplish the same
> task.  A search for "moving window" or "moving average" in the archives may
> produce some other ideas.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From djames at frontierassoc.com  Thu Sep  8 19:04:58 2005
From: djames at frontierassoc.com (David James)
Date: Thu, 8 Sep 2005 12:04:58 -0500
Subject: [R] Tip: I() can designate constants in a regression
Message-ID: <5284E979-AA7E-4FAD-ACE7-0783C6090FDE@frontierassoc.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050908/9ec57ae6/attachment.pl

From tmlammail at yahoo.com  Thu Sep  8 19:12:30 2005
From: tmlammail at yahoo.com (Martin Lam)
Date: Thu, 8 Sep 2005 10:12:30 -0700 (PDT)
Subject: [R] data manipulation
In-Reply-To: <20050908161733.30565.qmail@web25805.mail.ukl.yahoo.com>
Message-ID: <20050908171230.18061.qmail@web40514.mail.yahoo.com>

Hi,

This may not be the best solution, but at least it's
easy to see what i'm doing, assume that your data set
is called "data":

# remove the 4th column
data1 = data[,-4]

# remove the 3rd column
data2 = data[,-3]

# use cbind to add an extra column with only X1 
#elements
data1 = cbind(data1, array("X1", nrow(data1), 1)

# use cbind to add an extra column with only X2
#elements
data2 = cbind(data2, array("X2", nrow(data2), 1)

# use rbind to add them together as rows
data3 = rbind(data1, data2)

# rename the names of the columns
colnames(data3) <- c("ID", "time", "X", "type")

# show output
data3

The only thing I couldn't figure out is how to sort
the data set per row, perhaps someone else could help
us out on this?

Martin

--- Marc Bernard <bernarduse1 at yahoo.fr> wrote:

> Dear All,
>  
> I would be grateful if you can help me. My problem
> is the following:
> I have a data set like:
>  
> ID  time      X1          X2
> 1    1          x111      x211
> 1    2          x112      x212
> 2    1          x121      x221
> 2    2          x122      x222
> 2    3          x123      x223
>  
> where X1 and X2 are 2 covariates and "time" is the
> time of observation and ID indicates the cluster.
>  
> I want to merge the above data by creating a new
> variable  "X" and "type" as follows:
>  
> ID   time    X            type
> 1     1      x111         X1
> 1     2      x112         X1
> 1     1      x211         X2
> 1     2      x212         X2
> 2     1      x121         X1
> 2     2      x122         X1
> 2     3      x123         X1
> 2     1      x221         X2
> 2     2      x222         X2
> 2     3      x223         X2
> 
>  
> Where "type" is a factor variable indicating if the
> observation is related to X1 or X2...
>  
> Many thanks in advance,
>  
> Bernard
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 



	
		
______________________________________________________
Click here to donate to the Hurricane Katrina relief effort.



From spluque at gmail.com  Thu Sep  8 19:12:48 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Thu, 08 Sep 2005 12:12:48 -0500
Subject: [R] data manipulation
References: <20050908161733.30565.qmail@web25805.mail.ukl.yahoo.com>
Message-ID: <87u0gvbi0f.fsf@gmail.com>

Marc Bernard <bernarduse1 at yahoo.fr> wrote:
> Dear All,

> I would be grateful if you can help me. My problem is the following:
> I have a data set like:

> ID  time      X1          X2
> 1    1          x111      x211
> 1    2          x112      x212
> 2    1          x121      x221
> 2    2          x122      x222
> 2    3          x123      x223

> where X1 and X2 are 2 covariates and "time" is the time of observation and ID
> 	indicates the cluster.

> I want to merge the above data by creating a new variable "X" and "type" as
> 	follows:

> ID   time    X            type
> 1     1      x111         X1
> 1     2      x112         X1
> 1     1      x211         X2
> 1     2      x212         X2
> 2     1      x121         X1
> 2     2      x122         X1
> 2     3      x123         X1
> 2     1      x221         X2
> 2     2      x222         X2
> 2     3      x223         X2


> Where "type" is a factor variable indicating if the observation is related to
> 	X1 or X2...


Say your original data is in dataframe df, then this might do what you
want:

R> newdf <- rbind(df[, 1:3], df[, c(1, 2, 4)])
R> names(newdf)[3] <- "X"
R> newdf$type <- substr(c(df[[3]], df[[4]]), 1, 2)

Cheers,

-- 
Sebastian P. Luque



From p.dalgaard at biostat.ku.dk  Thu Sep  8 19:23:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Sep 2005 19:23:00 +0200
Subject: [R] Tip: I() can designate constants in a regression
In-Reply-To: <5284E979-AA7E-4FAD-ACE7-0783C6090FDE@frontierassoc.com>
References: <5284E979-AA7E-4FAD-ACE7-0783C6090FDE@frontierassoc.com>
Message-ID: <x2d5njo4nf.fsf@turmalin.kubism.ku.dk>

David James <djames at frontierassoc.com> writes:

> Just thought I would share a tip I learned:
> The function I() is useful for specifying constants to formulas and  
> regressions.
> 
> It will prevent nls (for example) from trying to treat the variable  
> inside I() as something it needs to estimate.  An example is below.
> 
> -David
> 
> P.S.  This may be obvious to some, but it is not made clear to be by  
> the documentation or common books that I reviewed.  These books, of  
> course, do tend to mention others aspects of I(), which seems to be a  
> very diverse function.  For example:
> * ISwR by Dalgaard (p. 160, 177)
> * MASwS by Venables and Ripley (p.18)
> 
> However, the books I looked at do not mention the specific tip here:  
> Wrapping I() around a variable will make it a constant from the  
> perspective of a regression.
> 
> A humble suggestion to the many authors of the many great R and S  
> books out there: I would find it helpful if more R books had the word  
> "constants" in the index.  Perhaps there could be a brief section  
> that explained how to create constants in a regression.  These sorts  
> of problems, I would guess, occur more commonly with nls models than  
> lm models.

First check whether your claim is actually correct:

>      x = 1:10
>      y = x                                  # perfect fit
>      yeps = y + rnorm(length(y), sd = 0.01) # added noise
>      nls(yeps ~ a + b*x, start = list(a = 0.12345, b = 0.54321),
+           trace = TRUE)
74.2686 :  0.12345 0.54321
0.0006529895 :  -0.002666984  1.000334031
Nonlinear regression model
  model:  yeps ~ a + b * x
   data:  parent.frame()
           a            b
-0.002666984  1.000334031
 residual sum-of-squares:  0.0006529895
> a <- 0
>      nls(yeps ~ a + b*x, start = list(b = 0.54321),trace=TRUE)
80.31713 :  0.54321
0.0006682311 :  0.999953
Nonlinear regression model
  model:  yeps ~ a + b * x
   data:  parent.frame()
       b
0.999953
 residual sum-of-squares:  0.0006682311

I.e., turning a into a constant works quite happily without the I().


> Here is the example that motivated my tip:
> 
> > weather.df : a data frame, where each row is one hour
> > weather.df$temp : the temperature
> > weather.df$annual : time offset, adjusted so that its period is one  
> > year
> > weather.df$daily : time offset, adjusted so that its period is one day
> >
> > # I want a1,a2 to be constants from the point of view of nls
> > a1 <- 66
> > a2 <- -18
> > nls.example  <- nls( temp ~ I(a1) + I(a2)*sin( ts.annual ) + a3*sin 
> > ( ts.daily ), data=weather.df, start=c(a3=1) )
> > # leaving out the I() will cause nls to estimate values for a1 and a2

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From sijeon at ucdavis.edu  Thu Sep  8 19:23:02 2005
From: sijeon at ucdavis.edu (Sangick Jeon)
Date: Thu, 8 Sep 2005 10:23:02 -0700 (PDT)
Subject: [R] Multinomial Logit and p-values
Message-ID: <200509081723.j88HN2Wo000793@phaenicia.ucdavis.edu>


Hi,

I am trying to obtain p-values for coefficient estimates in a multinomial
logit model.  Although I am able to test for significance using other
methods (e.g., Wald statistics), I can't seem to get R to give me simple
p-values. I am sure there is a very simple solution to this, but the R
archives seem to have nothing on this issue. I would appreciate any help. 
Thanks in advance!

Best,
Sangick Jeon



From tlumley at u.washington.edu  Thu Sep  8 19:30:34 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 8 Sep 2005 10:30:34 -0700 (PDT)
Subject: [R] data manipulation
In-Reply-To: <20050908161733.30565.qmail@web25805.mail.ukl.yahoo.com>
References: <20050908161733.30565.qmail@web25805.mail.ukl.yahoo.com>
Message-ID: <Pine.A41.4.63a.0509081030240.148304@homer09.u.washington.edu>


This is what reshape() does.

 	-thomas

On Thu, 8 Sep 2005, Marc Bernard wrote:

> Dear All,
>
> I would be grateful if you can help me. My problem is the following:
> I have a data set like:
>
> ID  time      X1          X2
> 1    1          x111      x211
> 1    2          x112      x212
> 2    1          x121      x221
> 2    2          x122      x222
> 2    3          x123      x223
>
> where X1 and X2 are 2 covariates and "time" is the time of observation and ID indicates the cluster.
>
> I want to merge the above data by creating a new variable  "X" and "type" as follows:
>
> ID   time    X            type
> 1     1      x111         X1
> 1     2      x112         X1
> 1     1      x211         X2
> 1     2      x212         X2
> 2     1      x121         X1
> 2     2      x122         X1
> 2     3      x123         X1
> 2     1      x221         X2
> 2     2      x222         X2
> 2     3      x223         X2
>
>
> Where "type" is a factor variable indicating if the observation is related to X1 or X2...
>
> Many thanks in advance,
>
> Bernard
>
>
> ---------------------------------
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From uofiowa at gmail.com  Thu Sep  8 19:30:43 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Thu, 8 Sep 2005 13:30:43 -0400
Subject: [R] execute R expression from command line
Message-ID: <3f87cc6d050908103035c44c1e@mail.gmail.com>

Can I execute an R expression from the command line without having it
in an infile, something like perl's -e flag. So it would look like:

R {Rexpression;} > outfile



From jporzak at gmail.com  Thu Sep  8 19:37:52 2005
From: jporzak at gmail.com (Jim Porzak)
Date: Thu, 8 Sep 2005 10:37:52 -0700
Subject: [R] data manipulation
In-Reply-To: <Pine.A41.4.63a.0509081030240.148304@homer09.u.washington.edu>
References: <20050908161733.30565.qmail@web25805.mail.ukl.yahoo.com>
	<Pine.A41.4.63a.0509081030240.148304@homer09.u.washington.edu>
Message-ID: <2a9c000c05090810371ce446fc@mail.gmail.com>

Also see Hadley Wickham's reshape package for more bells & whistles.
-- 
HTH!
Jim Porzak
Loyalty Matrix Inc.



On 9/8/05, Thomas Lumley <tlumley at u.washington.edu> wrote:
> 
> This is what reshape() does.
> 
>         -thomas
> 
> On Thu, 8 Sep 2005, Marc Bernard wrote:
> 
> > Dear All,
> >
> > I would be grateful if you can help me. My problem is the following:
> > I have a data set like:
> >
> > ID  time      X1          X2
> > 1    1          x111      x211
> > 1    2          x112      x212
> > 2    1          x121      x221
> > 2    2          x122      x222
> > 2    3          x123      x223
> >
> > where X1 and X2 are 2 covariates and "time" is the time of observation and ID indicates the cluster.
> >
> > I want to merge the above data by creating a new variable  "X" and "type" as follows:
> >
> > ID   time    X            type
> > 1     1      x111         X1
> > 1     2      x112         X1
> > 1     1      x211         X2
> > 1     2      x212         X2
> > 2     1      x121         X1
> > 2     2      x122         X1
> > 2     3      x123         X1
> > 2     1      x221         X2
> > 2     2      x222         X2
> > 2     3      x223         X2
> >
> >
> > Where "type" is a factor variable indicating if the observation is related to X1 or X2...
> >
> > Many thanks in advance,
> >
> > Bernard
> >
> >
> > ---------------------------------
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> Thomas Lumley                   Assoc. Professor, Biostatistics
> tlumley at u.washington.edu        University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tmlammail at yahoo.com  Thu Sep  8 19:50:30 2005
From: tmlammail at yahoo.com (Martin Lam)
Date: Thu, 8 Sep 2005 10:50:30 -0700 (PDT)
Subject: [R]  Re-evaluating the tree in the random forest
Message-ID: <20050908175030.50447.qmail@web40528.mail.yahoo.com>

Dear mailinglist members,

I was wondering if there was a way to re-evaluate the
instances of a tree (in the forest) again after I have
manually changed a splitpoint (or split variable) of a
decision node. Here's an illustration:

library("randomForest")

forest.rf <- randomForest(formula = Species ~ ., data
= iris, do.trace = TRUE, ntree = 3, mtry = 2,
norm.votes = FALSE)

# I am going to change the splitpoint of the root node
of the first tree to 1
forest.rf$forest$xbestsplit[1,]
forest.rf$forest$xbestsplit[1,1] <- 1
forest.rf$forest$xbestsplit[1,]

Because I've changed the splitpoint, some instances in
the leafs are not supposed where they should be. Is
there a way to reappoint them to the correct leaf?


I was also wondering how I should interpret the output
of do.trace:

ntree      OOB      1      2      3
    1:   3.70%  0.00%  6.25%  5.88%
    2:   3.49%  0.00%  3.85%  7.14%
    3:   3.57%  0.00%  5.56%  5.26%

What's OOB and what does the percentages mean?

Thanks in advance,

Martin


	
		
______________________________________________________
Click here to donate to the Hurricane Katrina relief effort.



From duncan at wald.ucdavis.edu  Thu Sep  8 19:53:03 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Thu, 08 Sep 2005 10:53:03 -0700
Subject: [R] R API call from delphi
In-Reply-To: <18668981.1126194469884.JavaMail.www@wwinf1615>
References: <18668981.1126194469884.JavaMail.www@wwinf1615>
Message-ID: <43207A7F.2030305@wald.ucdavis.edu>


On approach is to create a native/foreign interface to R by
linking R as a library (libR.a and R.dll) file and
calling the C routines in the library to
    i) initialize the R interpreter
   ii) call an R function

We have done this with many languages and the procedure is well
understood at this point, but requires some C-level programming
and converting between the standard data types of both systems.

Another approach is to use R via DCOM.  There are two different
approaches to this. One has the R interpreter as the DCOM
server and the client (the Delphi application here) would send
R commands to that server. The other approach has regular
DCOM servers that are implemented via R functions.  (The
ability to send R commands is a simple case of this.)  It is up
to you to define the servers via a few extra lines of R code.

I would suggest you pursue the DCOM route unless you are
keen to do the necessary work to embed the R  library
in Delphi and deal with some technical details about
calling conventions of C routines.

  D

Laurent TESSIER wrote:
> Hello,
> Has anyone tried to call R API from Delphi under windows ? How was it done if it was ? Or has anyone any idea about how it could be done ?
> Thanks for your answers
> Laurent Tessier
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Thu Sep  8 20:00:33 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 08 Sep 2005 13:00:33 -0500
Subject: [R] Predicting responses using ace
In-Reply-To: <859087cf050908082062c553dd@mail.gmail.com>
References: <859087cf0509061233350d7dba@mail.gmail.com>	<431E45C4.401@vanderbilt.edu>	<859087cf05090704347babc11b@mail.gmail.com>	<431ED777.1080208@vanderbilt.edu>	<859087cf05090705371776d8eb@mail.gmail.com>	<431EFEF5.5080704@vanderbilt.edu>	<859087cf05090712446d40ad0b@mail.gmail.com>	<431F629D.3050405@vanderbilt.edu>	<859087cf05090808097f6a692f@mail.gmail.com>
	<859087cf050908082062c553dd@mail.gmail.com>
Message-ID: <43207C41.2050101@vanderbilt.edu>

Luis Pineda wrote:
> I gave a quick read to the documentation again and noticed I misinterpreted 
> it. It was print.summary.areg.boot the method I was referring to (although 
> the summary error should still work). Sorry for the inconvenience
> 
> Anyway, I used the print method on my |areg.boot| object and I got this:
> ------
> Apparent R2 on transformed Y scale: 0.798
> Bootstrap validated R2 : 0.681
> ...
> Residuals on transformed scale:
> Min 1Q Median 3Q Max
> -1.071312e+00 -2.876245e-01 -3.010081e-02 2.123566e-01 1.867036e+00
> Mean S.D.
> 1.290634e-17 4.462159e-01
> ------
> I suppose thats the R^2 evaluated using the training set, but how do I 
> evaluate the performance of the model on a uncontaminated test set?

Please read my last note.  Bootstrap validated R2 is corrected for 
overfitting and is an estimate of the likely future R2 on a totally 
independent dataset.  The bootstrap is more efficient than data 
splitting for this purpose.

Frank

> 
> 
> On 9/8/05, Luis Pineda <luisen.p at gmail.com> wrote:
> 
>>I'm trying to run the print method, but according to the documentation it 
>>needs as a parameter an object created by |summary.areg.boot| . 
>>
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From jeaneid at chass.utoronto.ca  Thu Sep  8 20:08:12 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 8 Sep 2005 14:08:12 -0400
Subject: [R] data manipulation
In-Reply-To: <87u0gvbi0f.fsf@gmail.com>
Message-ID: <Pine.SGI.4.40.0509081403010.776293-100000@origin.chass.utoronto.ca>

I am sure all this work but If you want exaclty the output to be the way
you mentioned do this

temp<-read.table("yourfile", as.is=T, header=T)
temp1<-temp[, 1:3]
temp2<-temp[, c(1,2,4)]
colnames(temp1)[3]<-"X"
colnames(temp2)[3]<-"X"
temp3<-merge(temp1, temp2, all=T)
temp3$type<-toupper(substr(temp3$X, 1,2))


after which you can generate factors and such..
note the as.is=T in read.table keeps the variables X1, X2, as characters.
This is done for substr...


P.S. I am sure you can use reshape instead of the second to the fifth
commands above

?reshape

Jean

On Thu, 8 Sep 2005, Sebastian Luque wrote:

> Marc Bernard <bernarduse1 at yahoo.fr> wrote:
> > Dear All,
>
> > I would be grateful if you can help me. My problem is the following:
> > I have a data set like:
>
> > ID  time      X1          X2
> > 1    1          x111      x211
> > 1    2          x112      x212
> > 2    1          x121      x221
> > 2    2          x122      x222
> > 2    3          x123      x223
>
> > where X1 and X2 are 2 covariates and "time" is the time of observation and ID
> > 	indicates the cluster.
>
> > I want to merge the above data by creating a new variable "X" and "type" as
> > 	follows:
>
> > ID   time    X            type
> > 1     1      x111         X1
> > 1     2      x112         X1
> > 1     1      x211         X2
> > 1     2      x212         X2
> > 2     1      x121         X1
> > 2     2      x122         X1
> > 2     3      x123         X1
> > 2     1      x221         X2
> > 2     2      x222         X2
> > 2     3      x223         X2
>
>
> > Where "type" is a factor variable indicating if the observation is related to
> > 	X1 or X2...
>
>
> Say your original data is in dataframe df, then this might do what you
> want:
>
> R> newdf <- rbind(df[, 1:3], df[, c(1, 2, 4)])
> R> names(newdf)[3] <- "X"
> R> newdf$type <- substr(c(df[[3]], df[[4]]), 1, 2)
>
> Cheers,
>
> --
> Sebastian P. Luque
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From adi_due at yahoo.ca  Thu Sep  8 20:15:58 2005
From: adi_due at yahoo.ca (adalbert duerrer)
Date: Thu, 8 Sep 2005 14:15:58 -0400 (EDT)
Subject: [R] writing data to sheet in excel workbook
Message-ID: <20050908181558.5990.qmail@web35903.mail.mud.yahoo.com>

Hi,

I believe to remember there is a package that lets you
write data from R to different sheets in a Excel
workbook. I've been looking around on CRAN but could
not find what I am looking for.

Any help would be greatly appreciated.
Cheers,
Adi



From deepayan.sarkar at gmail.com  Thu Sep  8 20:20:09 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 8 Sep 2005 13:20:09 -0500
Subject: [R] Leading in line-wrapped Lattice value and panel labels
In-Reply-To: <431F5957.2070606@stat.auckland.ac.nz>
References: <200509070552.j875qRjV000523@mail22.syd.optusnet.com.au>
	<eb555e66050907141235ac2529@mail.gmail.com>
	<431F5957.2070606@stat.auckland.ac.nz>
Message-ID: <eb555e6605090811201657e469@mail.gmail.com>

On 9/7/05, Paul Murrell <p.murrell at auckland.ac.nz> wrote:
> Hi
> 
> 
> Deepayan Sarkar wrote:
>  > On 9/7/05, Tim Churches <tchur at optushome.com.au> wrote:
>  >
>  >> Version 2.1.1 Platforms: all
>  >>
>  >> What is the trellis parameter (or is there a trellis parameter) to
>  >> set the leading (the gap between lines) when long axis values
>  >> labels or panel header labels wrap over more than one line? By
>  >> default, there is a huge gap between lines, and much looking and
>  >> experimentation has not revealed to me a suitable parameter to
>  >> adjust this.
>  >>
>  >
>  >
>  > There is none. Whatever grid.text does happens.
> 
> 
> grid does have a "lineheight" graphical parameter.  For example,
> 
> library(grid)
> grid.text("line one\nlinetwo",
>            x=rep(1:3/4, each=3),
>            y=rep(1:3/4, 3),
>            gp=gpar(lineheight=1:9/2))
> 
> Could you add this in relevant places in trellis.par Deepayan?

I will (don't know how soon). The description in ?gpar is not very
informative though:

       lineheight  The height of a line as a multiple of the size of text

(or maybe it's a standard term in typography that I'm not familiar with).

Deepayan



From sfalcon at fhcrc.org  Thu Sep  8 20:34:02 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 08 Sep 2005 11:34:02 -0700
Subject: [R] execute R expression from command line
In-Reply-To: <3f87cc6d050908103035c44c1e@mail.gmail.com> (Omar Lakkis's
	message of "Thu, 8 Sep 2005 13:30:43 -0400")
References: <3f87cc6d050908103035c44c1e@mail.gmail.com>
Message-ID: <m2d5njjtnp.fsf@macaroni.local>

On  8 Sep 2005, uofiowa at gmail.com wrote:

> Can I execute an R expression from the command line without having
> it in an infile, something like perl's -e flag. So it would look
> like:
>
> R {Rexpression;} > outfile

With a bash-like shell, you can do:

echo "library(foo); somefunc(5)" | R --slave 

HTH,

+ seth



From shigesong at gmail.com  Thu Sep  8 20:59:48 2005
From: shigesong at gmail.com (Shige Song)
Date: Fri, 9 Sep 2005 02:59:48 +0800
Subject: [R] Survival model with cross-classified shared frailties
In-Reply-To: <Pine.A41.4.63a.0509080724420.177972@homer04.u.washington.edu>
References: <5abc11d8050908020811812817@mail.gmail.com>
	<Pine.A41.4.63a.0509080724420.177972@homer04.u.washington.edu>
Message-ID: <5abc11d8050908115915baec07@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/e057ae28/attachment.pl

From dushoff at eno.princeton.edu  Thu Sep  8 21:01:23 2005
From: dushoff at eno.princeton.edu (Jonathan Dushoff)
Date: Thu, 8 Sep 2005 15:01:23 -0400 (EDT)
Subject: [R] Setting width in batch mode
Message-ID: <Pine.LNX.4.61.0509081431050.30337@tahawus.Princeton.EDU>

As instructed, I have spent a long time searching the web for an answer
to this question.

I am trying to use Sweave to produce lecture slides, and have the
problem that I can't control the formatting of my R source.  Setting
options(width), as recommended in this forum, works fine on the R
_output_, but seems to have unpredictable effects on the echoing of the
source code.

If I try setting options(width) directly in R, I note that it has _no_
effect on echoed source code, whereas Sweave does sometimes break source
code, but not predictably, and not to the same width as output code.

I would be happy with any method of manually or automatically
controlling the line width of Sweave source, using R, Sweave or LaTeX
options.  Making the font smaller does not count, though; I want to
break the lines.

Any help is appreciated.

An example of Sweave input and output is appended.

The last break is right, while the others are too late.

Jonathan Dushoff

----------------------------------------------------------------------
bug.rnw

<<>>=
options(width=55)
data(state)
data.frame(area=mean(state.area),   pop=mean(state.pop),   hop=mean(state.area))
c(medianarea=median(state.area),   medianpop=median(state.pop))
c(medianarea=median(median(state.area)),   medianpop=median(state.pop))
@

----------------------------------------------------------------------
bug.tex

\begin{Schunk}
\begin{Sinput}
> options(width = 55)
> data(state)
> data.frame(area = mean(state.area), pop = mean(state.pop), 
+     hop = mean(state.area))
\end{Sinput}
\begin{Soutput}
       area     pop      hop
1 72367.98 4246420 72367.98

\end{Soutput}
\begin{Sinput}
> c(medianarea = median(state.area), medianpop = median(state.pop))
\end{Sinput}
\begin{Soutput}
medianarea  medianpop
      56222    2838500

\end{Soutput}
\begin{Sinput}
> c(medianarea = median(median(state.area)), 
+     medianpop = median(state.pop))
\end{Sinput}
\begin{Soutput}
medianarea  medianpop
      56222    2838500

\end{Soutput}
\end{Schunk}



From ggrothendieck at gmail.com  Thu Sep  8 21:26:23 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Sep 2005 15:26:23 -0400
Subject: [R] writing data to sheet in excel workbook
In-Reply-To: <20050908181558.5990.qmail@web35903.mail.mud.yahoo.com>
References: <20050908181558.5990.qmail@web35903.mail.mud.yahoo.com>
Message-ID: <971536df0509081226768f2c56@mail.gmail.com>

On 9/8/05, adalbert duerrer <adi_due at yahoo.ca> wrote:
> Hi,
> 
> I believe to remember there is a package that lets you
> write data from R to different sheets in a Excel
> workbook. I've been looking around on CRAN but could
> not find what I am looking for.
> 

See

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/58249.html



From gerifalte28 at hotmail.com  Thu Sep  8 21:49:54 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 08 Sep 2005 19:49:54 +0000
Subject: [R] R API call from delphi
In-Reply-To: <18668981.1126194469884.JavaMail.www@wwinf1615>
Message-ID: <BAY103-F36279148DE68108C5ED651A6990@phx.gbl>

Follow this thread 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/50598.html

Cheers

Francisco

>From: Laurent TESSIER <famille.tessier at wanadoo.fr>
>Reply-To: famille.tessier at wanadoo.fr
>To: r-help at stat.math.ethz.ch
>Subject: [R] R API call from delphi
>Date: Thu,  8 Sep 2005 17:47:49 +0200 (CEST)
>
>Hello,
>Has anyone tried to call R API from Delphi under windows ? How was it done 
>if it was ? Or has anyone any idea about how it could be done ?
>Thanks for your answers
>Laurent Tessier
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From polyng at u.washington.edu  Thu Sep  8 22:34:22 2005
From: polyng at u.washington.edu (Gallagher Dionysus Polyn)
Date: Thu, 8 Sep 2005 13:34:22 -0700 (PDT)
Subject: [R] can't successfully use installed evir package
Message-ID: <Pine.LNX.4.43.0509081334220.32115@hymn05.u.washington.edu>

I'm next at installing packages. I seem to have successfully installed "evir", but I can't use it. I'm wondering if I need to specify the installation to match my working directory, or something else.

thx,

G



From nikko at hailmail.net  Thu Sep  8 22:38:55 2005
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Thu, 08 Sep 2005 15:38:55 -0500
Subject: [R] clustering: Multivariate t mixtures
Message-ID: <1126211935.3644.242488691@webmail.messagingengine.com>

Hi,
Before I write code to do it does anyone know of code for fitting
mixtures of multivariate-t distributions.
I can't use McLachan's EMMIX code because the license is "For non
commercial use only". 
I checked, mclust and flexmix but both only do Gaussian. 

Thanks
Nicholas



From tring at gvdnet.dk  Thu Sep  8 23:03:03 2005
From: tring at gvdnet.dk (Troels Ring)
Date: Thu, 08 Sep 2005 23:03:03 +0200
Subject: [R] generating a vector from clusters of logicals
Message-ID: <6.2.1.2.0.20050908224318.04da0a90@home.gvdnet.dk>

dear friends,
I have a vector of clusters of TRUE and FALSE like 
c(TRUE,TRUE,TRUE...,FALSE,FALSE,FALSE,....TRUE,TRUE...) and want to make 
that into a vector
of c(1,1,1,1...2,2,2,2,.....3,3,3,3) increasing the number assigned to each 
cluster as they change.
How would I do that ?

Best wishes

Troels Ring, Aalborg, Denmark



From Achim.Zeileis at wu-wien.ac.at  Thu Sep  8 23:07:13 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 8 Sep 2005 23:07:13 +0200
Subject: [R] clustering: Multivariate t mixtures
In-Reply-To: <1126211935.3644.242488691@webmail.messagingengine.com>
References: <1126211935.3644.242488691@webmail.messagingengine.com>
Message-ID: <20050908230713.7e42ca52.Achim.Zeileis@wu-wien.ac.at>

On Thu, 08 Sep 2005 15:38:55 -0500 Nicholas Lewin-Koh wrote:

> Hi,
> Before I write code to do it does anyone know of code for fitting
> mixtures of multivariate-t distributions.
> I can't use McLachan's EMMIX code because the license is "For non
> commercial use only". 
> I checked, mclust and flexmix but both only do Gaussian. 

The Gaussian case is available in a pre-packaged function FLXmclust(),
but the flexmix framework is not limited to that case. There is a paper
which appeared in the Journal of Statistical Software
(http://www.jstatsoft.org/) that explains how to write new M-steps for
flexmix. It is also contained in the package as
  vignette("flexmix-intro")

Best,
Z

> Thanks
> Nicholas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Achim.Zeileis at wu-wien.ac.at  Thu Sep  8 23:11:12 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 8 Sep 2005 23:11:12 +0200
Subject: [R] generating a vector from clusters of logicals
In-Reply-To: <6.2.1.2.0.20050908224318.04da0a90@home.gvdnet.dk>
References: <6.2.1.2.0.20050908224318.04da0a90@home.gvdnet.dk>
Message-ID: <20050908231112.19dec9b6.Achim.Zeileis@wu-wien.ac.at>

On Thu, 08 Sep 2005 23:03:03 +0200 Troels Ring wrote:

> dear friends,
> I have a vector of clusters of TRUE and FALSE like 
> c(TRUE,TRUE,TRUE...,FALSE,FALSE,FALSE,....TRUE,TRUE...) and want to
> make that into a vector
> of c(1,1,1,1...2,2,2,2,.....3,3,3,3) increasing the number assigned to
> each cluster as they change.
> How would I do that ?

Does this what you want:

R> set.seed(123)
R> x <- sample(c(TRUE, FALSE), 10, replace = TRUE)
R> x
 [1]  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE
R> c(1, cumsum(abs(diff(x))) + 1)
 [1] 1 2 3 4 4 5 6 6 6 7

?
Z

> Best wishes
> 
> Troels Ring, Aalborg, Denmark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From nikko at hailmail.net  Thu Sep  8 23:13:05 2005
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Thu, 08 Sep 2005 16:13:05 -0500
Subject: [R] clustering: Multivariate t mixtures
In-Reply-To: <20050908230713.7e42ca52.Achim.Zeileis@wu-wien.ac.at>
References: <1126211935.3644.242488691@webmail.messagingengine.com>
	<20050908230713.7e42ca52.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <1126213985.8115.242491601@webmail.messagingengine.com>

Hi,
Actually that was my plan was to implement a new flexmix class.
Thanks for the pointer to the jss paper, that will be helpful.

Nicholas 
On Thu, 8 Sep 2005 23:07:13 +0200, "Achim Zeileis"
<Achim.Zeileis at wu-wien.ac.at> said:
> On Thu, 08 Sep 2005 15:38:55 -0500 Nicholas Lewin-Koh wrote:
> 
> > Hi,
> > Before I write code to do it does anyone know of code for fitting
> > mixtures of multivariate-t distributions.
> > I can't use McLachan's EMMIX code because the license is "For non
> > commercial use only". 
> > I checked, mclust and flexmix but both only do Gaussian. 
> 
> The Gaussian case is available in a pre-packaged function FLXmclust(),
> but the flexmix framework is not limited to that case. There is a paper
> which appeared in the Journal of Statistical Software
> (http://www.jstatsoft.org/) that explains how to write new M-steps for
> flexmix. It is also contained in the package as
>   vignette("flexmix-intro")
> 
> Best,
> Z
> 
> > Thanks
> > Nicholas
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >



From jfontain at free.fr  Thu Sep  8 23:14:30 2005
From: jfontain at free.fr (Jean-Luc Fontaine)
Date: Thu, 08 Sep 2005 23:14:30 +0200
Subject: [R] Time Series Analysis: book?
In-Reply-To: <1115a2b005090809146ca2509e@mail.gmail.com>
References: <1126182824.43202fa8c49be@imp1-g19.free.fr>
	<1115a2b005090809146ca2509e@mail.gmail.com>
Message-ID: <4320A9B6.7020309@free.fr>

Wensui Liu wrote:

> TS is a huge topic. The book recomended by statisitcian might be
> different from the one recommended by econometrician. Finance guy
> might recommend another. Could you please be more specific?


My software (http://moodss.sourceforge.net) collects, archives in a
SQL database and displays data from monitored devices, mostly
computers, databases and network equipment.
My idea is to use the stored data to perform predictions for capacity
planning purposes. For example, based on the trafic on a network
line for the last 12 months, what is the expected evolution in the
next 3 months.
So the data is more of the engineering type, I guess. But since the
software is modular, somebody could also use it to monitor the stock
market.
Actually, anything can be monitored so the data could come from
any source although practically mostly from computing related devices
and activities.

So I would like a book covering at least those subjects if possible.

Thanks very much for your help.

-- 
Jean-Luc Fontaine  http://jfontain.free.fr/



From ray at mcs.vuw.ac.nz  Thu Sep  8 23:15:00 2005
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Fri, 9 Sep 2005 09:15:00 +1200 (NZST)
Subject: [R] generating a vector from clusters of logicals
Message-ID: <200509082115.j88LF01V027395@tahi.mcs.vuw.ac.nz>

> From: Troels Ring <tring at gvdnet.dk>
> 
> I have a vector of clusters of TRUE and FALSE like 
> c(TRUE,TRUE,TRUE...,FALSE,FALSE,FALSE,....TRUE,TRUE...) and want to make 
> that into a vector
> of c(1,1,1,1...2,2,2,2,.....3,3,3,3) increasing the number assigned to each 
> cluster as they change.
> How would I do that ?
> 
How about:
> TF <- c(TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE)
> rep(1:length(rlel <- rle(TF)$lengths), rlel)
[1] 1 1 1 2 2 2 3 3 4

HTH,
Ray Brownrigg



From greg.snow at ihc.com  Thu Sep  8 23:27:26 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Thu, 08 Sep 2005 15:27:26 -0600
Subject: [R] generating a vector from clusters of logicals
Message-ID: <s3205864.090@lp-msg1.co.ihc.com>

Try:

x <- c(TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE)
tmp <- rle(x)
tmp$values <- seq(along=tmp$lengths)
new.x <- inverse.rle(tmp)
new.x
 


Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> Troels Ring <tring at gvdnet.dk> 09/08/05 03:03PM >>>
dear friends,
I have a vector of clusters of TRUE and FALSE like 
c(TRUE,TRUE,TRUE...,FALSE,FALSE,FALSE,....TRUE,TRUE...) and want to
make 
that into a vector
of c(1,1,1,1...2,2,2,2,.....3,3,3,3) increasing the number assigned to
each 
cluster as they change.
How would I do that ?

Best wishes

Troels Ring, Aalborg, Denmark

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.murrell at auckland.ac.nz  Thu Sep  8 23:39:05 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 09 Sep 2005 09:39:05 +1200
Subject: [R] Data Expo 2006 (off-topic)
Message-ID: <4320AF79.80400@stat.auckland.ac.nz>

Hi

This is to let R folks know about the Data Expo that is being run by the 
ASA Sections on Statistical Graphics, Statistical Computing,
and Statistics and the Environment for JSM 2006.

This competition provides a data set of geographic and
atmospheric data from NASA and entrants are asked to provide
a graphical summary of the important features of the data set.
The emphasis is on graphical display, but the data set has
time series, spatial, and multivariate features that allow the
focus to be directed in a number of different ways.

Entries will be presented in a poster session at JSM 2006 and
the best entries will receive cash prizes totalling $1700
plus NASA merchandise.  Student and group entries are encouraged.

It would be good to see some R-based entries!

For more information, please see the Data Expo web site
http://www.amstat-online.org/sections/graphics/dataexpo/2006.php

Paul Murrell
(on behalf of the Data Expo organising team)
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From dhinds at sonic.net  Thu Sep  8 23:42:54 2005
From: dhinds at sonic.net (dhinds@sonic.net)
Date: Thu, 8 Sep 2005 21:42:54 +0000 (UTC)
Subject: [R] ROracle install problem
References: <200509081140.09814.ariel.chernomoretz@crchul.ulaval.ca>
Message-ID: <dfqb8u$b8n$1@sea.gmane.org>

Ariel Chernomoretz <ariel.chernomoretz at crchul.ulaval.ca> wrote:
> Hi, 

> I am trying to install the ROracle package in a Linux-64 machine.
> I downloaded from Oracle's site their Instant Client bundle but it seems that
> ROracle needs some stuff not included in that kit in order to compile (in 
> particuar, the 'proc' executable).

You can't use the Instant Client.  You need to get the full client CD
for your platform.

> I did not find any other linux client suite in Oracle's site

It is there.  Go to:

  http://www.oracle.com/technology/software/index.html

and click on "Oracle Database 10g" (or 9i, or whatever), then on your
platform, and look for the Client CD.

-- Dave



From rkhayat at eng.uwo.ca  Thu Sep  8 23:59:40 2005
From: rkhayat at eng.uwo.ca (Khayat, Roger)
Date: Thu, 8 Sep 2005 17:59:40 -0400 
Subject: [R]  Re: General Matrix Inner Product?
Message-ID: <C8FEFC3492303C41A9196CFF106FA13776469A@exch2003.sf.engineering>



Roger E. Khayat, Professor
Department of Mechanical and Materials Engineering
The University of Western Ontario
London, Ontario, Canada N6A 5B9

Email: rkhayat at eng.uwo.ca
Tel: (519) 661-2111 Ext 88253
Fax: (519) 661-3020

http://www.engga.uwo.ca/people/rkhayat/



From maj at waikato.ac.nz  Thu Sep  8 23:59:58 2005
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Fri, 09 Sep 2005 09:59:58 +1200
Subject: [R] Coarsening Factors
Message-ID: <4320B45E.4060204@waikato.ac.nz>

It is not uncommon to want to coarsen a factor by grouping levels 
together. I have found one way to do this in R:

 > sites
  [1] F A A D A A B F C F A D E E D C F A E D F C E D E F F D B C
Levels: A B C D E F
 > regions <- list(I = c("A","B","C"), II = "D", III = c("E","F"))
 > library(Epi)
 > region <- Relevel(sites,regions)
 > region
  [1] III I   I   II  I   I   I   III I   III I   II  III III II  I 
III I   III
[20] II  III I   III II  III III III II  I   I
Levels: I II III

However this seems like using a sledgehammer to crack a nut. Can someone 
suggest a simpler way to do this task?

Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk     Home +64 7 825 0441   Mobile 021 1395 862



From p.dalgaard at biostat.ku.dk  Fri Sep  9 00:26:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Sep 2005 00:26:00 +0200
Subject: [R] Coarsening Factors
In-Reply-To: <4320B45E.4060204@waikato.ac.nz>
References: <4320B45E.4060204@waikato.ac.nz>
Message-ID: <x2ek7zgps7.fsf@turmalin.kubism.ku.dk>

Murray Jorgensen <maj at waikato.ac.nz> writes:

> It is not uncommon to want to coarsen a factor by grouping levels 
> together. I have found one way to do this in R:
> 
>  > sites
>   [1] F A A D A A B F C F A D E E D C F A E D F C E D E F F D B C
> Levels: A B C D E F
>  > regions <- list(I = c("A","B","C"), II = "D", III = c("E","F"))
>  > library(Epi)
>  > region <- Relevel(sites,regions)
>  > region
>   [1] III I   I   II  I   I   I   III I   III I   II  III III II  I 
> III I   III
> [20] II  III I   III II  III III III II  I   I
> Levels: I II III
> 
> However this seems like using a sledgehammer to crack a nut. Can someone 
> suggest a simpler way to do this task?

Yes,

> regions <- list(I = c("A","B","C"), II = "D", III = c("E","F"))
> levels(sites) <- regions
> sites
 [1] III I   I   II  I   I   I   III I   III I   II  III III II  I III I   III 
[20] II  III I   III II  III III III II  I   I
Levels: I II III


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tuechler at gmx.at  Fri Sep  9 01:58:29 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Fri, 09 Sep 2005 01:58:29 +0200
Subject: [R] change in read.spss, package foreing?
Message-ID: <3.0.6.32.20050909015829.007bed60@pop.gmx.net>

Dear All,

it seems to me that the function read.spss of package foreign changed its
behaviour regarding factors. I noted that in version 0.8-8 variables with
value labels in SPSS were transformed in factors with the labels in
alphabetic order.
In version 0.8-10 they seem to be ordered preserving the order
corresponding to their numerical codes in SPSS.
However I could not find a description of this supposed change. Since the
different behaviour seems to depend on the installed version of the
foreign-package I don't know how to give a reproducible example.
It also affects spss.get of the Hmisc-package, which is not surprising.
I prefer the new behaviour and would like to know, if it will persist in
future versions.

Comments?

Heinz T??chler



From spencer.graves at pdf.com  Fri Sep  9 05:44:43 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 08 Sep 2005 20:44:43 -0700
Subject: [R] Time Series Analysis: book?
In-Reply-To: <4320A9B6.7020309@free.fr>
References: <1126182824.43202fa8c49be@imp1-g19.free.fr>	<1115a2b005090809146ca2509e@mail.gmail.com>
	<4320A9B6.7020309@free.fr>
Message-ID: <4321052B.3000405@pdf.com>

	  1.  Have you read the appropriate chapter in Venables and Ripley 
(2002) Modern Applied Statists with S (Springer)?  If no, I suggest you 
start there.

	  2.  Have you worked through the vignettes associated with the "zoo" 
package?  If no, you might find that quite useful.  [Are you aware that 
edit(vignette(...)) will provide a script file with the R code discussed 
in the vignette, which can be viewed in Adobe Acrobat while you are 
working throught the examples line by line, modifying them, etc.?  I've 
found this to be very useful.  If you use XEmacs, "edit(vignette(...))" 
may not work.  Instead, try Stangle(vignette(...)$file).  This will copy 
the R code to a file in the working directory, which you can then open.]

	  3.  Have you considered Durbin, J. and Koopman, S. J. (2001) _Time 
Series Analysis by State Space Methods._  Oxford University Press?  If 
no, you might want to spend some time with that.

	   I'm still looking for the right kind of introduction and overview to 
what is available in R for time series analysis, especially with a 
Bayesian approach to Kalman filtering and smoothing.  Unfortunately, I 
have yet to find the key I feel I need to get started, though I found 
the vignettes with zoo to be quite helpful.

	  spencer graves

Jean-Luc Fontaine wrote:

> Wensui Liu wrote:
> 
> 
>>TS is a huge topic. The book recomended by statisitcian might be
>>different from the one recommended by econometrician. Finance guy
>>might recommend another. Could you please be more specific?
> 
> 
> 
> My software (http://moodss.sourceforge.net) collects, archives in a
> SQL database and displays data from monitored devices, mostly
> computers, databases and network equipment.
> My idea is to use the stored data to perform predictions for capacity
> planning purposes. For example, based on the trafic on a network
> line for the last 12 months, what is the expected evolution in the
> next 3 months.
> So the data is more of the engineering type, I guess. But since the
> software is modular, somebody could also use it to monitor the stock
> market.
> Actually, anything can be monitored so the data could come from
> any source although practically mostly from computing related devices
> and activities.
> 
> So I would like a book covering at least those subjects if possible.
> 
> Thanks very much for your help.
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From kyegon at hotmail.com  Fri Sep  9 07:02:10 2005
From: kyegon at hotmail.com (ERICK YEGON)
Date: Fri, 09 Sep 2005 08:02:10 +0300
Subject: [R] SPSS Dataset
Message-ID: <BAY22-F228E51D8490BDAE2D183D8DE980@phx.gbl>

How would one read SPSS data sets directly into R



From jfontain at free.fr  Fri Sep  9 07:07:14 2005
From: jfontain at free.fr (jfontain@free.fr)
Date: Fri, 09 Sep 2005 07:07:14 +0200
Subject: [R] Time Series Analysis: book?
In-Reply-To: <4321052B.3000405@pdf.com>
References: <1126182824.43202fa8c49be@imp1-g19.free.fr>
	<1115a2b005090809146ca2509e@mail.gmail.com>
	<4320A9B6.7020309@free.fr> <4321052B.3000405@pdf.com>
Message-ID: <1126242434.4321188254039@imp1-g19.free.fr>

Quoting Spencer Graves <spencer.graves at pdf.com>:

> 	  1.  Have you read the appropriate chapter in Venables and Ripley
> (2002) Modern Applied Statists with S (Springer)?  If no, I suggest you
> start there.
>
> 	  2.  Have you worked through the vignettes associated with the "zoo"
> package?  If no, you might find that quite useful.  [Are you aware that
> edit(vignette(...)) will provide a script file with the R code discussed
> in the vignette, which can be viewed in Adobe Acrobat while you are
> working throught the examples line by line, modifying them, etc.?  I've
> found this to be very useful.  If you use XEmacs, "edit(vignette(...))"
> may not work.  Instead, try Stangle(vignette(...)$file).  This will copy
> the R code to a file in the working directory, which you can then open.]
>
> 	  3.  Have you considered Durbin, J. and Koopman, S. J. (2001) _Time
> Series Analysis by State Space Methods._  Oxford University Press?  If
> no, you might want to spend some time with that.
>
> 	   I'm still looking for the right kind of introduction and overview to
> what is available in R for time series analysis, especially with a
> Bayesian approach to Kalman filtering and smoothing.  Unfortunately, I
> have yet to find the key I feel I need to get started, though I found
> the vignettes with zoo to be quite helpful.

Thank you very much Spencer and all who responded.

I think I have enough to get started with all this valuable information.


--
Jean-Luc



From Scott.Williams at petermac.org  Fri Sep  9 07:35:10 2005
From: Scott.Williams at petermac.org (Williams Scott)
Date: Fri, 9 Sep 2005 15:35:10 +1000
Subject: [R] strata in crr (cmprsk library)
Message-ID: <46B75B4A4A45914ABB0901364EFF4A206B0E@PMC-EMAIL.petermac.org.au>

Hi all, I am aware that crr lacks the "friendly" command structure of
functions such as cph. All is clear to me about including covariates
until I want to include a stratification term in the competing risk
framework (no nice strat command). 

I am still a bit of a novice in R - I am looking for an example to help
me with this, but can't seem to find one. Any advice appreciated (no
matter how simple).

Thanks

Scott Williams MD
Peter MacCallum Cancer Centre
Melbourne, Australia



From wettenhall at wehi.EDU.AU  Fri Sep  9 07:55:20 2005
From: wettenhall at wehi.EDU.AU (James Wettenhall)
Date: Fri, 9 Sep 2005 15:55:20 +1000 (EST)
Subject: [R] Debugging R/Fortran in Windows
Message-ID: <3095.192.168.65.113.1126245320.squirrel@homebase.wehi.edu.au>

Hi,

I'm trying to debug an R interface to a Fortran subroutine from Windows. 
(Yes, I know I should try Unix/Linux as well, but a quick attempt
suggested that the (MinGW g77) Fortran compiler I have installed on my
Windows laptop works better on this Fortran code.)

I'm trying to follow the instructions in the "Writing R Extensions" Manual:

Start R under the debugger after setting a breakpoint for WinMain.
          gdb .../bin/Rgui.exe
          (gdb) break WinMain
          (gdb) run

But when I run gdb on Rgui.exe, I get the message:
"no debugging symbols found"
and then when I try "break WinMain", I get:
"No symbol table is loaded.  use the 'file' command."

I'm using R 2.1.1 on Windows 2000 and gdb 5.2.1 from MSys's MinGW.

I'm calling a Fortran function (several times) from R.  And I seem to have
the basic two-way data communication working - I appear to have
succesfully passed all required data types (integer, real, double
precision) to and from Fortran with sensible results both from within R
and from using WRITE(FILENUM,*) from within Fortran.  But unfortunately
there is still evidence of memory leakage.

Any suggestions would be greatly appreciated.

Regards,
James



From Matthias.Templ at statistik.gv.at  Fri Sep  9 07:56:51 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 9 Sep 2005 07:56:51 +0200
Subject: [R] SPSS Dataset
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BABDB@xchg1.statistik.local>

RSiteSearch("read spss data")
-->
library(foreign)
?read.spss

Best,
Matthias

> -----Urspr??ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von ERICK YEGON
> Gesendet: Freitag, 09. September 2005 07:02
> An: R-help at stat.math.ethz.ch
> Betreff: [R] SPSS Dataset
> 
> 
> How would one read SPSS data sets directly into R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From Friedrich.Leisch at tuwien.ac.at  Fri Sep  9 08:34:15 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Fri, 9 Sep 2005 08:34:15 +0200
Subject: [R] Setting width in batch mode
In-Reply-To: <Pine.LNX.4.61.0509081431050.30337@tahawus.Princeton.EDU>
References: <Pine.LNX.4.61.0509081431050.30337@tahawus.Princeton.EDU>
Message-ID: <17185.11495.864605.695631@galadriel.ci.tuwien.ac.at>

>>>>> On Thu, 8 Sep 2005 15:01:23 -0400 (EDT),
>>>>> Jonathan Dushoff (JD) wrote:

  > As instructed, I have spent a long time searching the web for an answer
  > to this question.

  > I am trying to use Sweave to produce lecture slides, and have the
  > problem that I can't control the formatting of my R source.  Setting
  > options(width), as recommended in this forum, works fine on the R
  > _output_, but seems to have unpredictable effects on the echoing of the
  > source code.

  > If I try setting options(width) directly in R, I note that it has _no_
  > effect on echoed source code, whereas Sweave does sometimes break source
  > code, but not predictably, and not to the same width as output code.

  > I would be happy with any method of manually or automatically
  > controlling the line width of Sweave source, using R, Sweave or LaTeX
  > options.  Making the font smaller does not count, though; I want to
  > break the lines.

  > Any help is appreciated.

  > An example of Sweave input and output is appended.

  > The last break is right, while the others are too late.

The deparser of R only *tries* to break lines at the given cutoff
(Sweave uses 0.75*getOption("width") for input lines), so you
soemtimes have to play a little bit, and yes, results are somewhat
unpredictable. After storing your code in file ex1.R I get:

R> expr=parse("ex1.R")
R> length(expr)
[1] 5

R> for(n in 1:length(expr)) print(deparse(expr[[n]], width.cutoff=.75*55))
[1] "options(width = 55)"
[1] "data(state)"
[1] "data.frame(area = mean(state.area), pop = mean(state.pop), "
[2] "    hop = mean(state.area))"                                
[1] "c(medianarea = median(state.area), medianpop = median(state.pop))"
[1] "c(medianarea = median(median(state.area)), "
[2] "    medianpop = median(state.pop))"         

R> for(n in 1:length(expr)) print(deparse(expr[[n]], width.cutoff=.75*45))
[1] "options(width = 55)"
[1] "data(state)"
[1] "data.frame(area = mean(state.area), "              
[2] "    pop = mean(state.pop), hop = mean(state.area))"
[1] "c(medianarea = median(state.area), "
[2] "    medianpop = median(state.pop))" 
[1] "c(medianarea = median(median(state.area)), "
[2] "    medianpop = median(state.pop))"         


I know that's not exactly the answer you were looking for, but that's
the way it is. I'll add a sentence or two to the FAQ.

Best,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From tuechler at gmx.at  Fri Sep  9 08:51:22 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Fri, 09 Sep 2005 08:51:22 +0200
Subject: [R] SPSS Dataset
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BABDB@xchg1.statistik.l ocal>
Message-ID: <3.0.6.32.20050909085122.007c5e80@pop.gmx.net>

At 07:56 09.09.2005 +0200, TEMPL Matthias wrote:
>RSiteSearch("read spss data")
>-->
>library(foreign)
>?read.spss
>
>Best,
>Matthias
>

or spss.get in Hmisc

Heinz
>> -----Urspr??ngliche Nachricht-----
>> Von: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von ERICK YEGON
>> Gesendet: Freitag, 09. September 2005 07:02
>> An: R-help at stat.math.ethz.ch
>> Betreff: [R] SPSS Dataset
>> 
>> 
>> How would one read SPSS data sets directly into R
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read 
>> the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From ssquid at gmail.com  Fri Sep  9 08:59:35 2005
From: ssquid at gmail.com (Y Y)
Date: Fri, 9 Sep 2005 01:59:35 -0500
Subject: [R] Win32 network drive install
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DC7@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DC7@usctmx1106.merck.com>
Message-ID: <7148b3d00509082359209dc1a9@mail.gmail.com>

 I want to install r on a windows network drive so that
users who have their own Win2000 machines can run something like

x:\bin\rterm.exe CMD BATCH x:\url\prog.r c:\out\prog.Rout

I do not want to make n users install their own versions of R;
I want to install once, and give users the ability to run the copy
of R that I maintain on a group shared drive.

Given that I've installed r21xx.exe locally on my personal drive,
if I re-run the installer and target a network drive, would this
 * much up my local install
 * make r available to all who can access the network drive ?

The closest I could find in the FAQ
http://cran.r-project.org/bin/windows/base/rw-FAQ.html#Can-I-run-R-from-a-CD-or-USB-drive_003f

'2.8 Can I run R from a CD or USB drive?

Yes, with care. A basic R installation is relocatable, so you can burn an 
image on the R installation on your hard disc or install directly onto a 
removable storage device such as a flash-memory USB drive. '

Looks like I could put an image on the server; I'm not quite sure what
the steps are to make an executable image or if the files I make an
image of are exactly laid out after installing r locally.



From H.RINNER at tirol.gv.at  Fri Sep  9 10:15:39 2005
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Fri, 9 Sep 2005 10:15:39 +0200
Subject: [R] adding text to the corner of a lattice plot
Message-ID: <FBB425B009EF1C408643F2462A9D914934C0F2@mxs1.tirol.local>

Dear R community,

I am using R 2.1.1 on Windows XP, package lattice Version 0.12-5, and
want to add text (sort of a dat-stamp actually) to the lower left corner
of a lattice plot, prefarably _after_ the plot has been created.

Here is a simple example what I do in base graphics:

# base graphics:
> plot(rnorm(100), rnorm(100))
> mtext(as.character(Sys.Date()), side = 1,line = -2, outer = T, adj =
0, font = 1, cex = 0.7)

How can I get the same using lattice?
# lattice:
> require(lattice)
> xyplot(rnorm(100) ~ rnorm(100))
> ???



From justin_bem at yahoo.fr  Fri Sep  9 10:18:13 2005
From: justin_bem at yahoo.fr (justin bem)
Date: Fri, 9 Sep 2005 10:18:13 +0200 (CEST)
Subject: [R] shapefiles manipulations ??
Message-ID: <20050909081813.96508.qmail@web25709.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/1a9a680e/attachment.pl

From thpe at hhbio.wasser.tu-dresden.de  Fri Sep  9 10:13:32 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Fri, 09 Sep 2005 10:13:32 +0200
Subject: [R] Win32 network drive install
In-Reply-To: <7148b3d00509082359209dc1a9@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DC7@usctmx1106.merck.com>
	<7148b3d00509082359209dc1a9@mail.gmail.com>
Message-ID: <4321442C.3040700@hhbio.wasser.tu-dresden.de>

ssquid at gmail.com wrote:

 >  I want to install r on a windows network drive so that
 > users who have their own Win2000 machines can run something like
 >
 > x:\bin\rterm.exe CMD BATCH x:\url\prog.r c:\out\prog.Rout
 >
 > I do not want to make n users install their own versions of R;
 > I want to install once, and give users the ability to run the copy
 > of R that I maintain on a group shared drive.
 >
 > Given that I've installed r21xx.exe locally on my personal drive,
 > if I re-run the installer and target a network drive, would this
 >  * much up my local install

No.

 >  * make r available to all who can access the network drive ?

Yes.
Don't forget to set the PATH if you want to run it in BATCH mode.

[...]

 > Looks like I could put an image on the server; I'm not quite sure what
 > the steps are to make an executable image or if the files I make an
 > image of are exactly laid out after installing r locally.

Exactly laid out. You can install R to the server or simply copy an 
existing R installation from one drive to another. R does not install 
things into the Windows system directories and the registry settings are 
purely optional.

More reading:

http://cran.r-project.org/bin/windows/base/rw-FAQ.html

and

http://cran.r-project.org/doc/manuals/R-admin.html

Conclusion: R is a network administrator's friend!!!

Thomas P.

  wrote:
 >  I want to install r on a windows network drive so that
 > users who have their own Win2000 machines can run something like
 >
 > x:\bin\rterm.exe CMD BATCH x:\url\prog.r c:\out\prog.Rout
 >
 > I do not want to make n users install their own versions of R;
 > I want to install once, and give users the ability to run the copy
 > of R that I maintain on a group shared drive.
 >
 > Given that I've installed r21xx.exe locally on my personal drive,
 > if I re-run the installer and target a network drive, would this
 >  * much up my local install

No.

 >  * make r available to all who can access the network drive ?

Yes.

[...]

 > Looks like I could put an image on the server; I'm not quite sure what
 > the steps are to make an executable image or if the files I make an
 > image of are exactly laid out after installing r locally.

Exactly laid out. You can install R to the server or simply copy an 
existing R installation from one drive to another. R does not install 
things into the Windows system directories and the registry settings are 
purely optional.

More reading:

http://cran.r-project.org/bin/windows/base/rw-FAQ.html

and

http://cran.r-project.org/doc/manuals/R-admin.html

Conclusion: R is a network administrator's friend!!!

Thomas P.



From thpe at hhbio.wasser.tu-dresden.de  Fri Sep  9 10:28:30 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Fri, 09 Sep 2005 10:28:30 +0200
Subject: [R] shapefiles manipulations ??
In-Reply-To: <20050909081813.96508.qmail@web25709.mail.ukl.yahoo.com>
References: <20050909081813.96508.qmail@web25709.mail.ukl.yahoo.com>
Message-ID: <432147AE.6080809@hhbio.wasser.tu-dresden.de>

justin bem wrote:

> cheers ,
> 
> I need help in shapefile manipulations. I have two shapefiles of my
> country Cameroon.
> The first contain 10 provinces and each province contains a certain
> number of administratives units. I dont have ESRI Arc view. I want
> to add a admistrative unit of the center province to the map with
> 10 province.
> 
> Is it possible to to take a peace of shapefile and combining it 
> to another ?

In principle yes using the "maptools" package.

Maybe, this thread from yesterday can give you an idea:

https://stat.ethz.ch/pipermail/r-help/2005-September/077221.html

Thomas P.



From ligges at statistik.uni-dortmund.de  Fri Sep  9 10:41:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 09 Sep 2005 10:41:45 +0200
Subject: [R] can't successfully use installed evir package
In-Reply-To: <Pine.LNX.4.43.0509081334220.32115@hymn05.u.washington.edu>
References: <Pine.LNX.4.43.0509081334220.32115@hymn05.u.washington.edu>
Message-ID: <43214AC9.8090002@statistik.uni-dortmund.de>

Gallagher Dionysus Polyn wrote:

> I'm next at installing packages. I seem to have successfully installed "evir", but I can't use it. I'm wondering if I need to specify the installation to match my working directory, or something else.

Load the package at first:
library(evir)


Uwe Ligges

> thx,
> 
> G
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Roger.Bivand at nhh.no  Fri Sep  9 10:44:48 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 9 Sep 2005 10:44:48 +0200 (CEST)
Subject: [R] shapefiles manipulations ??
In-Reply-To: <20050909081813.96508.qmail@web25709.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.44.0509091036520.11434-100000@reclus.nhh.no>

On Fri, 9 Sep 2005, justin bem wrote:

> cheers ,
>  I need help in shapefile manipulations. I have two shapefiles of my
> country Cameroon. The first contain 10 provinces and each province
> contains a certain number of administratives units. I dont have ESRI Arc
> view. I want to add a admistrative unit of the center province to the
> map with 10 province.
>  
> Is it possible to to take a peace of shapefile and combining it to another ?

In principle, yes. There are two R packages (maptools and shapefiles) that 
read and write shapefiles, and the newly released combination of maptools 
and the sp package should be able to handle this.

Since sp has objects defined down to the simple polygons, extracting a 
subset from one map and inserting it into another should not be too 
difficult.

The specific difficulty will be both to add the administrative unit, and 
to adjust the borders of the province that it belongs to. I suggest 
following this up on the R-sig-geo list; if you can make your shapefiles 
available (perhaps on a website), together with details of which 
administrative unit needs to be inserted, help should be forthcoming.

>  
> Thanks !
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ligges at statistik.uni-dortmund.de  Fri Sep  9 10:51:07 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 09 Sep 2005 10:51:07 +0200
Subject: [R] Debugging R/Fortran in Windows
In-Reply-To: <3095.192.168.65.113.1126245320.squirrel@homebase.wehi.edu.au>
References: <3095.192.168.65.113.1126245320.squirrel@homebase.wehi.edu.au>
Message-ID: <43214CFB.7090804@statistik.uni-dortmund.de>

James Wettenhall wrote:

> Hi,
> 
> I'm trying to debug an R interface to a Fortran subroutine from Windows. 
> (Yes, I know I should try Unix/Linux as well, but a quick attempt
> suggested that the (MinGW g77) Fortran compiler I have installed on my
> Windows laptop works better on this Fortran code.)
> 
> I'm trying to follow the instructions in the "Writing R Extensions" Manual:
> 
> Start R under the debugger after setting a breakpoint for WinMain.
>           gdb .../bin/Rgui.exe
>           (gdb) break WinMain
>           (gdb) run
> 
> But when I run gdb on Rgui.exe, I get the message:
> "no debugging symbols found"
> and then when I try "break WinMain", I get:
> "No symbol table is loaded.  use the 'file' command."
> 
> I'm using R 2.1.1 on Windows 2000 and gdb 5.2.1 from MSys's MinGW.
> 
> I'm calling a Fortran function (several times) from R.  And I seem to have
> the basic two-way data communication working - I appear to have
> succesfully passed all required data types (integer, real, double
> precision) to and from Fortran with sensible results both from within R
> and from using WRITE(FILENUM,*) from within Fortran.  But unfortunately
> there is still evidence of memory leakage.
> 
> Any suggestions would be greatly appreciated.

James, please read the posting guide which asks you to read the R for 
Windows FAQ (which includes an answer on the question "How do I debug 
code that I have compiled and dyn.load-ed?").

It also tells you to send a question like this (given it is not answered 
in the manuals or FAQs etc.) to R-devel rather than R-help.

[hence further responses on this thread seem to be more appropriate for 
R-devel]

Best,
Uwe


> Regards,
> James
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From a.trapletti at swissonline.ch  Thu Sep  8 20:45:08 2005
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Thu, 08 Sep 2005 20:45:08 +0200
Subject: [R] [R-pkgs] tseries
Message-ID: <432086B4.3080804@swissonline.ch>

There is a new version of tseries with an enhanced "get.hist.quote" 
available:

* "get.hist.quote" now optionally returns a "zoo", "ts", or "its" object 
(thanks to Achim Zeileis)
* New provider "oanda" is implemented which provides access to one of 
the largest foreign exchange databases
* Some minor improvements, see the "ChangeLog" for details

Best regards
Adrian Trapletti

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From dusa.adrian at gmail.com  Fri Sep  9 10:54:06 2005
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Fri, 9 Sep 2005 11:54:06 +0300
Subject: [R] measurement unit
Message-ID: <200509091154.06287.adi@roda.ro>


Dear R-list,

Could anybody tell me where to find information about changing the measurement 
unit from inch to centimeters? 
I read the help from X11, I read R-intro and I did some searhing in the R 
archives, but I couldn't find the answer.
For example, I would like to produce a plot of a certain width and height:

X11(width=10, height=5)

and I would like these to be centimeters, rather than inches.

Thank you,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest
Romania
Tel./Fax: +40 21 3126618 \
              +40 21 3120210 / int.101



From cdeclercq at nordnet.fr  Fri Sep  9 12:03:12 2005
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Fri, 9 Sep 2005 12:03:12 +0200
Subject: [R] measurement unit
In-Reply-To: <200509091154.06287.adi@roda.ro>
Message-ID: <MJELLLFFFCNHMHOOLCMBEELADIAA.cdeclercq@nordnet.fr>

Adrian,

> -----Message d'origine-----
> De : r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]De la part de Adrian DUSA
> Envoye : vendredi 9 septembre 2005 09:54
> 
> Dear R-list,
> 
> Could anybody tell me where to find information about 
> changing the measurement 
> unit from inch to centimeters? 
> I read the help from X11, I read R-intro and I did some 
> searhing in the R 
> archives, but I couldn't find the answer.
> For example, I would like to produce a plot of a certain 
> width and height:
> 
> X11(width=10, height=5)
> and I would like these to be centimeters, rather than inches.
> 
> Thank you,
> Adrian

1 inch = 2.54 cm

So you could try what I do for that

> X11(width=10/2.54, height=5/2.54)

Or

> cm2in<-function(x) x/2.54
> X11(width=cm2in(10), height=cm2in(5))

HTH

Christophe
--
Christophe Declercq, MD
Observatoire regional de la sante Nord-Pas-de-Calais
13, rue Faidherbe
F-59046 LILLE Cedex
Phone 33 3 20 15 49 24
Fax 33 3 20 55 92 30
E-mail c.declercq at orsnpdc.org
 

> 
> -- 
> Adrian Dusa
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest
> Romania
> Tel./Fax: +40 21 3126618 \
>               +40 21 3120210 / int.101
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Fri Sep  9 12:09:33 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 9 Sep 2005 12:09:33 +0200
Subject: [R] change in read.spss, package foreign?
In-Reply-To: <3.0.6.32.20050909015829.007bed60@pop.gmx.net>
References: <3.0.6.32.20050909015829.007bed60@pop.gmx.net>
Message-ID: <17185.24413.389850.992957@stat.math.ethz.ch>

>>>>> "Heinz" == Heinz Tuechler <tuechler at gmx.at>
>>>>>     on Fri, 09 Sep 2005 01:58:29 +0200 writes:

    Heinz> Dear All,
    Heinz> it seems to me that the function read.spss of package
    Heinz> foreign changed its behaviour regarding factors. I
    Heinz> noted that in version 0.8-8 variables with value
    Heinz> labels in SPSS were transformed in factors with the
    Heinz> labels in alphabetic order.

    Heinz> In version 0.8-10 they seem to be ordered preserving
    Heinz> the order corresponding to their numerical codes in
    Heinz> SPSS.  However I could not find a description of this
    Heinz> supposed change. Since the different behaviour seems
    Heinz> to depend on the installed version of the
    Heinz> foreign-package I don't know how to give a
    Heinz> reproducible example.  It also affects spss.get of
    Heinz> the Hmisc-package, which is not surprising.

    Heinz> I prefer the new behaviour and would like to know, if
    Heinz> it will persist in future versions.

Yes, it was on purpose.

Note that the development of "foreign" is also on
svn.R-project.org, and you can easily get at its 'ChangeLog' :

         https://svn.R-project.org/R-packages/trunk/foreign/ChangeLog

where you find the relevant entry at 2005-08-15 .

Regards,
Martin Maechler



From dusa.adrian at gmail.com  Fri Sep  9 11:35:18 2005
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Fri, 9 Sep 2005 12:35:18 +0300
Subject: [R] measurement unit
In-Reply-To: <MJELLLFFFCNHMHOOLCMBEELADIAA.cdeclercq@nordnet.fr>
References: <MJELLLFFFCNHMHOOLCMBEELADIAA.cdeclercq@nordnet.fr>
Message-ID: <200509091235.18885.adi@roda.ro>

On Friday 09 September 2005 13:03, Christophe Declercq wrote:
> Adrian,
> 1 inch = 2.54 cm
>
> So you could try what I do for that
>
> > X11(width=10/2.54, height=5/2.54)
>
> Or
>
> > cm2in<-function(x) x/2.54
> > X11(width=cm2in(10), height=cm2in(5))
>
> HTH
>
> Christophe

Thank you Cristophe,

This solves it. I also thought about transforming, but I was curious if 
there's an already built in argument.

Best,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest
Romania
Tel./Fax: +40 21 3126618 \
              +40 21 3120210 / int.101



From liuwensui at gmail.com  Fri Sep  9 13:08:11 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 9 Sep 2005 07:08:11 -0400
Subject: [R] writing data to sheet in excel workbook
In-Reply-To: <971536df0509081226768f2c56@mail.gmail.com>
References: <20050908181558.5990.qmail@web35903.mail.mud.yahoo.com>
	<971536df0509081226768f2c56@mail.gmail.com>
Message-ID: <1115a2b00509090408545b72f2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/fac2db2c/attachment.pl

From mikewhite.diu at tiscali.co.uk  Fri Sep  9 13:02:52 2005
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Fri, 9 Sep 2005 12:02:52 +0100
Subject: [R]  Plotting an ellipse in 3D
Message-ID: <000b01c5b52e$06c7d610$75fe2f50@FSSFQCV7BGDVED>

I have been using the ellipse function from the car package and the
covariance matrix to draw an ellipse around a group of points to show the
confidence limits.  However, the points are actually represented by 3
variables so rather than plot each pairwise combination of variables in 2D I
would like to plot the 'ellipse' in 3D using the djmrgl package.  Can anyone
offer advice on how I can plot the surface of  a 3D 'ellipse' using the
covariance matrix to define the shape, so that the points inside can also be
seen.

Thanks
Mike White



From phgrosjean at sciviews.org  Fri Sep  9 13:34:29 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 09 Sep 2005 13:34:29 +0200
Subject: [R] writing data to sheet in excel workbook
In-Reply-To: <1115a2b00509090408545b72f2@mail.gmail.com>
References: <20050908181558.5990.qmail@web35903.mail.mud.yahoo.com>	<971536df0509081226768f2c56@mail.gmail.com>
	<1115a2b00509090408545b72f2@mail.gmail.com>
Message-ID: <43217345.1050704@sciviews.org>

Hello,

You cvould also look at http://www.sciviews.org/SciViews-R and download 
tcltk2 package there. ?tk2dde gives you some examples on how to 
manipulate Excel from R easily.
Best,

Philippe Grosjean

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Wensui Liu wrote:
> RDCOMClient package works great. But it might take lots of coding if you 
> want to write a big report.
> 
> On 9/8/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 
>>On 9/8/05, adalbert duerrer <adi_due at yahoo.ca> wrote:
>>
>>>Hi,
>>>
>>>I believe to remember there is a package that lets you
>>>write data from R to different sheets in a Excel
>>>workbook. I've been looking around on CRAN but could
>>>not find what I am looking for.
>>>
>>
>>See
>>
>>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/58249.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
>



From andy_liaw at merck.com  Fri Sep  9 14:35:54 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 9 Sep 2005 08:35:54 -0400
Subject: [R] Re-evaluating the tree in the random forest
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED418@usctmx1106.merck.com>

> From: Martin Lam
> 
> Dear mailinglist members,
> 
> I was wondering if there was a way to re-evaluate the
> instances of a tree (in the forest) again after I have
> manually changed a splitpoint (or split variable) of a
> decision node. Here's an illustration:
> 
> library("randomForest")
> 
> forest.rf <- randomForest(formula = Species ~ ., data
> = iris, do.trace = TRUE, ntree = 3, mtry = 2,
> norm.votes = FALSE)
> 
> # I am going to change the splitpoint of the root node
> of the first tree to 1
> forest.rf$forest$xbestsplit[1,]
> forest.rf$forest$xbestsplit[1,1] <- 1
> forest.rf$forest$xbestsplit[1,]
> 
> Because I've changed the splitpoint, some instances in
> the leafs are not supposed where they should be. Is
> there a way to reappoint them to the correct leaf?

I'm not sure what you want to do exactly, but I suspect you can use
predict().
 
> I was also wondering how I should interpret the output
> of do.trace:
> 
> ntree      OOB      1      2      3
>     1:   3.70%  0.00%  6.25%  5.88%
>     2:   3.49%  0.00%  3.85%  7.14%
>     3:   3.57%  0.00%  5.56%  5.26%
> 
> What's OOB and what does the percentages mean?

OOB stands for `Out-of-bag'.  Read up on random forests (e.g., the article
in R News) to learn about it.  Those numbers are estimated error rates.  The
`OOB' column is across all data, while the others are for the classes.

Andy

 
> Thanks in advance,
> 
> Martin
> 
> 
> 	
> 		
> ______________________________________________________
> Click here to donate to the Hurricane Katrina relief effort.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From tuechler at gmx.at  Fri Sep  9 14:50:10 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Fri, 09 Sep 2005 14:50:10 +0200
Subject: [R] change in read.spss, package foreign?
In-Reply-To: <17185.24413.389850.992957@stat.math.ethz.ch>
References: <3.0.6.32.20050909015829.007bed60@pop.gmx.net>
	<3.0.6.32.20050909015829.007bed60@pop.gmx.net>
Message-ID: <3.0.6.32.20050909145010.007c2a70@pop.gmx.net>

At 12:09 09.09.2005 +0200, Martin Maechler wrote:
>>>>>> "Heinz" == Heinz Tuechler <tuechler at gmx.at>
>>>>>>     on Fri, 09 Sep 2005 01:58:29 +0200 writes:
>
>    Heinz> Dear All,
>    Heinz> it seems to me that the function read.spss of package
>    Heinz> foreign changed its behaviour regarding factors. I
>    Heinz> noted that in version 0.8-8 variables with value
>    Heinz> labels in SPSS were transformed in factors with the
>    Heinz> labels in alphabetic order.
>
>    Heinz> In version 0.8-10 they seem to be ordered preserving
>    Heinz> the order corresponding to their numerical codes in
>    Heinz> SPSS.  However I could not find a description of this
>    Heinz> supposed change. Since the different behaviour seems
>    Heinz> to depend on the installed version of the
>    Heinz> foreign-package I don't know how to give a
>    Heinz> reproducible example.  It also affects spss.get of
>    Heinz> the Hmisc-package, which is not surprising.
>
>    Heinz> I prefer the new behaviour and would like to know, if
>    Heinz> it will persist in future versions.
>
>Yes, it was on purpose.
>
>Note that the development of "foreign" is also on
>svn.R-project.org, and you can easily get at its 'ChangeLog' :
>
>         https://svn.R-project.org/R-packages/trunk/foreign/ChangeLog
>
>where you find the relevant entry at 2005-08-15 .
>
>Regards,
>Martin Maechler
>
Dear Martin, 
Thank you for your answer. As I said, I appreciate this change. The
documentation does not explain precisely, how variables with labels are
treated now. It only tells "If SPSS value labels are converted to factors
the underlying numerical codes will not in general be the same as the SPSS
numerical values, since the numerical codes in R are always 1,2,3,...".
Will now the created factor levels in any case be ordered according to the
order of the original numerical codes in SPSS?

In general I wonder, how I could get to know such critical changes before I
update a package instead of finding it out by chance. Is there a place,
where a responsible R-user should look, when updating the program?
Thanks again,
Heinz T??chler



From murdoch at stats.uwo.ca  Fri Sep  9 15:02:58 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Sep 2005 09:02:58 -0400
Subject: [R] Plotting an ellipse in 3D
In-Reply-To: <000b01c5b52e$06c7d610$75fe2f50@FSSFQCV7BGDVED>
References: <000b01c5b52e$06c7d610$75fe2f50@FSSFQCV7BGDVED>
Message-ID: <43218802.6090401@stats.uwo.ca>

Mike White wrote:
> I have been using the ellipse function from the car package and the
> covariance matrix to draw an ellipse around a group of points to show the
> confidence limits.  However, the points are actually represented by 3
> variables so rather than plot each pairwise combination of variables in 2D I
> would like to plot the 'ellipse' in 3D using the djmrgl package.  Can anyone
> offer advice on how I can plot the surface of  a 3D 'ellipse' using the
> covariance matrix to define the shape, so that the points inside can also be
> seen.

You should use rgl, rather than djmrgl.  It now has most of the same 
functions plus a lot more.

Then you can plot the ellipse as a wireframe or transparent object.  See 
the demo(regression) example for that kind of drawing; demo(shapes3d) 
for ellipses.  (The demo names are from memory, I don't have access to 
it right now.)

Duncan Murdoch



From tmlammail at yahoo.com  Fri Sep  9 15:04:07 2005
From: tmlammail at yahoo.com (Martin Lam)
Date: Fri, 9 Sep 2005 06:04:07 -0700 (PDT)
Subject: [R] Re-evaluating the tree in the random forest
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED418@usctmx1106.merck.com>
Message-ID: <20050909130407.85860.qmail@web40525.mail.yahoo.com>

Hi,

Let me give a simple example, assume a dataset
containing 5 instances  with 1 variable and the class
label:

[x1, y]:
[0.5, A]
[3.2, B]
[4.5, B]
[1.4, C]
[1.6, C]
[1.9, C]

Assume that the randomForest algorithm create this (2
levels deep) tree:

Root node: question: x1 < 2.2?

Left terminal node:
[0.5, A]
[1.4, C]
[1.6, C]
[1.9, C]
Leaf classification: C

Right terminal node:
[3.2, B]
[4.5, B]
Leaf classification: B

If I change the question at the root node to "x1 <
1?", the instances in the left leaf node are not
correctly passed down the tree anymore.  
My original question was if there was a way to
re-evaluate the instances again into:

Root node: question: x1 < 1?

Left terminal node:
[0.5, A]
Leaf classification: A

Right terminal node:
[3.2, B]
[4.5, B]
[1.4, C]
[1.6, C]
[1.9, C]
Leaf classification: C

Cheers,

Martin

--- "Liaw, Andy" <andy_liaw at merck.com> wrote:

> > From: Martin Lam
> > 
> > Dear mailinglist members,
> > 
> > I was wondering if there was a way to re-evaluate
> the
> > instances of a tree (in the forest) again after I
> have
> > manually changed a splitpoint (or split variable)
> of a
> > decision node. Here's an illustration:
> > 
> > library("randomForest")
> > 
> > forest.rf <- randomForest(formula = Species ~ .,
> data
> > = iris, do.trace = TRUE, ntree = 3, mtry = 2,
> > norm.votes = FALSE)
> > 
> > # I am going to change the splitpoint of the root
> node
> > of the first tree to 1
> > forest.rf$forest$xbestsplit[1,]
> > forest.rf$forest$xbestsplit[1,1] <- 1
> > forest.rf$forest$xbestsplit[1,]
> > 
> > Because I've changed the splitpoint, some
> instances in
> > the leafs are not supposed where they should be.
> Is
> > there a way to reappoint them to the correct leaf?
> 
> I'm not sure what you want to do exactly, but I
> suspect you can use
> predict().
>  
> > I was also wondering how I should interpret the
> output
> > of do.trace:
> > 
> > ntree      OOB      1      2      3
> >     1:   3.70%  0.00%  6.25%  5.88%
> >     2:   3.49%  0.00%  3.85%  7.14%
> >     3:   3.57%  0.00%  5.56%  5.26%
> > 
> > What's OOB and what does the percentages mean?
> 
> OOB stands for `Out-of-bag'.  Read up on random
> forests (e.g., the article
> in R News) to learn about it.  Those numbers are
> estimated error rates.  The
> `OOB' column is across all data, while the others
> are for the classes.
> 
> Andy
> 
>  
> > Thanks in advance,
> > 
> > Martin
> > 
> > 
> > 	
> > 		
> >
>
______________________________________________________
> > Click here to donate to the Hurricane Katrina
> relief effort.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> > 
> 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any
> attachments, contains information of Merck & Co.,
> Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may
> be known outside the United States as Merck Frosst,
> Merck Sharp & Dohme or MSD and in Japan, as Banyu)
> that may be confidential, proprietary copyrighted
> and/or legally privileged. It is intended solely for
> the use of the individual or entity named on this
> message.  If you are not the intended recipient, and
> have received this message in error, please notify
> us immediately by reply e-mail and then delete it
> from your system.
>
------------------------------------------------------------------------------
> 



	
		
______________________________________________________
Click here to donate to the Hurricane Katrina relief effort.



From matthew_wiener at merck.com  Fri Sep  9 15:31:00 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Fri, 9 Sep 2005 09:31:00 -0400
Subject: [R] adding text to the corner of a lattice plot
Message-ID: <4E9A692D8755DF478B56A2892388EE1F06B717@usctmx1118.merck.com>

I think this message from the help archives might address your question
(found using the query "lattice mtext" on R site search:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/51605.html.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of RINNER Heinrich
Sent: Friday, September 09, 2005 4:16 AM
To: r-help at stat.math.ethz.ch
Subject: [R] adding text to the corner of a lattice plot


Dear R community,

I am using R 2.1.1 on Windows XP, package lattice Version 0.12-5, and
want to add text (sort of a dat-stamp actually) to the lower left corner
of a lattice plot, prefarably _after_ the plot has been created.

Here is a simple example what I do in base graphics:

# base graphics:
> plot(rnorm(100), rnorm(100))
> mtext(as.character(Sys.Date()), side = 1,line = -2, outer = T, adj =
0, font = 1, cex = 0.7)

How can I get the same using lattice?
# lattice:
> require(lattice)
> xyplot(rnorm(100) ~ rnorm(100))
> ???

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Fri Sep  9 15:36:18 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Sep 2005 09:36:18 -0400
Subject: [R] writing data to sheet in excel workbook
In-Reply-To: <43217345.1050704@sciviews.org>
References: <20050908181558.5990.qmail@web35903.mail.mud.yahoo.com>
	<971536df0509081226768f2c56@mail.gmail.com>
	<1115a2b00509090408545b72f2@mail.gmail.com>
	<43217345.1050704@sciviews.org>
Message-ID: <971536df05090906367f8f92e2@mail.gmail.com>

Hi, 
I tried downloading tcltk2 and using 
    Packages | Install Packages from Local Zip(s)
under R 2.2.0 but got this:

> utils:::menuInstallLocal()
package 'tcltk2' successfully unpacked and MD5 sums checked
updating HTML package descriptions
> library(tcltk2)
Loading required package: tcltk
Loading Tcl/Tk interface ... done
Error in structure(.External("dotTclObjv", objv, PACKAGE = "tcltk"),
class = "tclObj") :
        [tcl] couldn't load library
"C:/PROGRA~1/R/R-22~1.0DE/library/tcltk2/tklibs/winico0.5/winico05.dll":
this library or a dependent library could not be found in library
path.
In addition: Warning message:
Tcl package 'tile' not found in: tclRequire("tile") 
Error: .onLoad failed in 'loadNamespace' for 'tcltk2'
Error: package/namespace load failed for 'tcltk2'
> R.version.string
[1] "R version 2.2.0, 2005-09-03" 

On 9/9/05, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
> Hello,
> 
> You cvould also look at http://www.sciviews.org/SciViews-R and download
> tcltk2 package there. ?tk2dde gives you some examples on how to
> manipulate Excel from R easily.
> Best,
> 
> Philippe Grosjean
> 
> ..............................................<??}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
> ( ( ( ( (
>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>  ) ) ) ) )          http://www.sciviews.org
> ( ( ( ( (
> ..............................................................
> 
> Wensui Liu wrote:
> > RDCOMClient package works great. But it might take lots of coding if you
> > want to write a big report.
> >
> > On 9/8/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> >>On 9/8/05, adalbert duerrer <adi_due at yahoo.ca> wrote:
> >>
> >>>Hi,
> >>>
> >>>I believe to remember there is a package that lets you
> >>>write data from R to different sheets in a Excel
> >>>workbook. I've been looking around on CRAN but could
> >>>not find what I am looking for.
> >>>
> >>
> >>See
> >>
> >>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/58249.html
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>
> >
> >
> >
> >
> 
>



From andy_liaw at merck.com  Fri Sep  9 15:39:49 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 9 Sep 2005 09:39:49 -0400
Subject: [R] Re-evaluating the tree in the random forest
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED41B@usctmx1106.merck.com>

Here's an example, using the iris data:

> ## Grow one tree, using all data, and try all variables at all splits,
> ## using large nodesize to get smaller tree.
> iris.rf <- randomForest(iris[-5], iris[[5]], ntree=1, nodesize=20, mtry=4,
+                         sampsize=150, replace=FALSE)
> getTree(iris.rf, 1)
   left daughter right daughter split var split point status prediction
1              2              3         3        2.45      1          0
2              0              0         0        0.00     -1          1
3              4              5         4        1.75      1          0
4              6              7         3        4.95      1          0
5              8              9         3        4.85      1          0
6             10             11         4        1.65      1          0
7              0              0         0        0.00     -1          3
8              0              0         0        0.00     -1          3
9              0              0         0        0.00     -1          3
10             0              0         0        0.00     -1          2
11             0              0         0        0.00     -1          3
> idx <- with(iris, Petal.Length > 2.45 & Petal.Length < 3.5)
> predict(iris.rf, iris[idx, -5])
[1] versicolor versicolor versicolor
Levels: setosa versicolor virginica
> iris.rf$forest$xbestsplit[1,1] <- 3.5
> predict(iris.rf, iris[newiris, -5])
[1] setosa setosa setosa
Levels: setosa versicolor virginica

Note how the predictions have changed.

HTH,
Andy

> -----Original Message-----
> From: Martin Lam [mailto:tmlammail at yahoo.com] 
> Sent: Friday, September 09, 2005 9:04 AM
> To: Liaw, Andy; r-help at stat.math.ethz.ch
> Subject: RE: [R] Re-evaluating the tree in the random forest
> 
> 
> Hi,
> 
> Let me give a simple example, assume a dataset
> containing 5 instances  with 1 variable and the class
> label:
> 
> [x1, y]:
> [0.5, A]
> [3.2, B]
> [4.5, B]
> [1.4, C]
> [1.6, C]
> [1.9, C]
> 
> Assume that the randomForest algorithm create this (2
> levels deep) tree:
> 
> Root node: question: x1 < 2.2?
> 
> Left terminal node:
> [0.5, A]
> [1.4, C]
> [1.6, C]
> [1.9, C]
> Leaf classification: C
> 
> Right terminal node:
> [3.2, B]
> [4.5, B]
> Leaf classification: B
> 
> If I change the question at the root node to "x1 <
> 1?", the instances in the left leaf node are not
> correctly passed down the tree anymore.  
> My original question was if there was a way to
> re-evaluate the instances again into:
> 
> Root node: question: x1 < 1?
> 
> Left terminal node:
> [0.5, A]
> Leaf classification: A
> 
> Right terminal node:
> [3.2, B]
> [4.5, B]
> [1.4, C]
> [1.6, C]
> [1.9, C]
> Leaf classification: C
> 
> Cheers,
> 
> Martin
> 
> --- "Liaw, Andy" <andy_liaw at merck.com> wrote:
> 
> > > From: Martin Lam
> > > 
> > > Dear mailinglist members,
> > > 
> > > I was wondering if there was a way to re-evaluate
> > the
> > > instances of a tree (in the forest) again after I
> > have
> > > manually changed a splitpoint (or split variable)
> > of a
> > > decision node. Here's an illustration:
> > > 
> > > library("randomForest")
> > > 
> > > forest.rf <- randomForest(formula = Species ~ .,
> > data
> > > = iris, do.trace = TRUE, ntree = 3, mtry = 2,
> > > norm.votes = FALSE)
> > > 
> > > # I am going to change the splitpoint of the root
> > node
> > > of the first tree to 1
> > > forest.rf$forest$xbestsplit[1,]
> > > forest.rf$forest$xbestsplit[1,1] <- 1
> > > forest.rf$forest$xbestsplit[1,]
> > > 
> > > Because I've changed the splitpoint, some
> > instances in
> > > the leafs are not supposed where they should be.
> > Is
> > > there a way to reappoint them to the correct leaf?
> > 
> > I'm not sure what you want to do exactly, but I
> > suspect you can use
> > predict().
> >  
> > > I was also wondering how I should interpret the
> > output
> > > of do.trace:
> > > 
> > > ntree      OOB      1      2      3
> > >     1:   3.70%  0.00%  6.25%  5.88%
> > >     2:   3.49%  0.00%  3.85%  7.14%
> > >     3:   3.57%  0.00%  5.56%  5.26%
> > > 
> > > What's OOB and what does the percentages mean?
> > 
> > OOB stands for `Out-of-bag'.  Read up on random
> > forests (e.g., the article
> > in R News) to learn about it.  Those numbers are
> > estimated error rates.  The
> > `OOB' column is across all data, while the others
> > are for the classes.
> > 
> > Andy
> > 
> >  
> > > Thanks in advance,
> > > 
> > > Martin
> > > 
> > > 
> > > 	
> > > 		
> > >
> >
> ______________________________________________________
> > > Click here to donate to the Hurricane Katrina
> > relief effort.
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > > 
> > > 
> > 
> > 
> > 
> >
> --------------------------------------------------------------
> ----------------
> > Notice:  This e-mail message, together with any
> > attachments, contains information of Merck & Co.,
> > Inc. (One Merck Drive, Whitehouse Station, New
> > Jersey, USA 08889), and/or its affiliates (which may
> > be known outside the United States as Merck Frosst,
> > Merck Sharp & Dohme or MSD and in Japan, as Banyu)
> > that may be confidential, proprietary copyrighted
> > and/or legally privileged. It is intended solely for
> > the use of the individual or entity named on this
> > message.  If you are not the intended recipient, and
> > have received this message in error, please notify
> > us immediately by reply e-mail and then delete it
> > from your system.
> >
> --------------------------------------------------------------
> ----------------
> > 
> 
> 
> 
> 	
> 		
> ______________________________________________________
> Click here to donate to the Hurricane Katrina relief effort.
> http://store.yahoo.com/redcross-donate3/
> 
> 
>



From jmoreira at fe.up.pt  Fri Sep  9 15:45:29 2005
From: jmoreira at fe.up.pt (=?iso-8859-1?Q?Jo=E3o_Mendes_Moreira?=)
Date: Fri, 9 Sep 2005 14:45:29 +0100
Subject: [R] Finding a decision tree's leaf node from a new value
Message-ID: <001601c5b544$bd9ffeb0$5e7aa8c0@FEUPsig.fe.up.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/54398679/attachment.pl

From riedwyl at giub.unibe.ch  Fri Sep  9 16:03:29 2005
From: riedwyl at giub.unibe.ch (riedwyl@giub.unibe.ch)
Date: Fri,  9 Sep 2005 16:03:29 +0200
Subject: [R] test for exponential,lognormal and gammadistribution
Message-ID: <1126274609.43219631dccdc@www.cx.unibe.ch>


hello!
i don't want to test my sample data for normality, but exponential- lognormal- 
or gammadistribution.
as i've learnt the anderson-darling-test in R is only for normality and i am 
not supposed to use the kolmogorov-smirnov test of R for parameter estimates 
from sample data, is that true?
can you help me, how to do this anyway!
thank you very much!
nadja



From HDoran at air.org  Fri Sep  9 16:03:53 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 9 Sep 2005 10:03:53 -0400
Subject: [R] Off-topic: Comparing standard errors from simulation and
	analytical model
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A0A4159@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/94c476ed/attachment.pl

From fcombes at gmail.com  Fri Sep  9 16:09:24 2005
From: fcombes at gmail.com (Florence Combes)
Date: Fri, 9 Sep 2005 16:09:24 +0200
Subject: [R] how to do something like " subset(mat, ("col1">4 & "col2">4)) "
Message-ID: <73dae306050909070918483ad8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/be555dcf/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Sep  9 16:23:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Sep 2005 16:23:22 +0200
Subject: [R] how to do something like " subset(mat,
	("col1">4 & "col2">4)) "
In-Reply-To: <73dae306050909070918483ad8@mail.gmail.com>
References: <73dae306050909070918483ad8@mail.gmail.com>
Message-ID: <x2d5nicobp.fsf@turmalin.kubism.ku.dk>

Florence Combes <fcombes at gmail.com> writes:

> Dear all, 
> 
> I have a problem with the "subset()" function. I spent all day yesterday 
> with a collegue to solve it and we did not find a satisfying solution (even 
> in the archived mails), so I ask for your help. 
> Let's say (for a simple example) a matrix mat: 
> 
> R> mat
> cola colb colc
> [1,] 1 4 7
> [2,] 2 5 8
> [3,] 3 6 9
> 
> My goal is to select the lines of the matrix on the basis of the values of 
> more than one column (let's say colb and colc). 
> For example I want to select all the lines of the matrix for which values in 
> colb and colc are more than 4. 
> 
> I tried several ways that did not work: 
> 
> R> mat2 <- subset(mat, ("colb">4 & "colc">4))
> R> mat2
> [1] 1 2 3 4 5 6 7 8 9
> 
> it is a vector, not a matrix. 
> 
> > mat2 <- subset(mat, mat[,2:3]>4)
> > mat2
> [1] 2 3 4 5 6 8 9
> 
> tha same: it is a vector; so I tried: 
> 
> > mat2 <- as.matrix(subset(mat, mat[,("colb">4 & "colc">4)]))
> > mat2
> [,1]
> [1,] 1
> [2,] 2
> [3,] 3
> [4,] 4
> [5,] 5
> [6,] 6
> [7,] 7
> [8,] 8
> [9,] 9
> 
> not good :(
> 
> Did someone have an idea of how to select the only the lines 2 and 3 of mat 
> by a selection on "colb" and "colc" >4 ? 


Well, subset has methods for vectors and data frames, so what happens
for matrices is basically that they get converted to vectors. I don't
know what gave you the idea of quoting the names, but 

"colb">4

is TRUE because numbers sort before letters!

Try something like

as.matrix(subset(as.data.frame(mat),colb>4 & colc>4))


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From abunn at whrc.org  Fri Sep  9 16:26:25 2005
From: abunn at whrc.org (Andy Bunn)
Date: Fri, 9 Sep 2005 10:26:25 -0400
Subject: [R] how to do something like " subset(mat,
	("col1">4 & "col2">4)) "
In-Reply-To: <73dae306050909070918483ad8@mail.gmail.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIGEDEDKAA.abunn@whrc.org>

Some thing like this?

mat <- matrix(1:9,3,3)
mat
mat[apply(mat[,2:3] > 4,1,all),]
# or less cryptically
foo <- mat[,2:3] > 4
bar <- apply(foo,1,all)
mat[bar,]

HTH, Andy

NB: No need to send this kind of message to r-devel


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Florence Combes
> Sent: Friday, September 09, 2005 10:09 AM
> To: r-help at stat.math.ethz.ch; r-devel-request at stat.math.ethz.ch
> Subject: [R] how to do something like " subset(mat, ("col1">4 &
> "col2">4)) "
> 
> 
> Dear all, 
> 
> I have a problem with the "subset()" function. I spent all day yesterday 
> with a collegue to solve it and we did not find a satisfying 
> solution (even 
> in the archived mails), so I ask for your help. 
> Let's say (for a simple example) a matrix mat: 
> 
> R> mat
> cola colb colc
> [1,] 1 4 7
> [2,] 2 5 8
> [3,] 3 6 9
> 
> My goal is to select the lines of the matrix on the basis of the 
> values of 
> more than one column (let's say colb and colc). 
> For example I want to select all the lines of the matrix for 
> which values in 
> colb and colc are more than 4. 
> 
> I tried several ways that did not work: 
> 
> R> mat2 <- subset(mat, ("colb">4 & "colc">4))
> R> mat2
> [1] 1 2 3 4 5 6 7 8 9
> 
> it is a vector, not a matrix. 
> 
> > mat2 <- subset(mat, mat[,2:3]>4)
> > mat2
> [1] 2 3 4 5 6 8 9
> 
> tha same: it is a vector; so I tried: 
> 
> > mat2 <- as.matrix(subset(mat, mat[,("colb">4 & "colc">4)]))
> > mat2
> [,1]
> [1,] 1
> [2,] 2
> [3,] 3
> [4,] 4
> [5,] 5
> [6,] 6
> [7,] 7
> [8,] 8
> [9,] 9
> 
> not good :(
> 
> Did someone have an idea of how to select the only the lines 2 
> and 3 of mat 
> by a selection on "colb" and "colc" >4 ? 
> 
> Thanks a lot, 
> 
> Florence. 
> 
> Version 2.0.1 (2004-11-15)
> (Linux Debian).
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From chrisb at fcdarwin.org.ec  Fri Sep  9 15:29:53 2005
From: chrisb at fcdarwin.org.ec (Chris Buddenhagen)
Date: Fri, 9 Sep 2005 08:29:53 -0500
Subject: [R] Plot of multiple data sets
Message-ID: <000501c5b542$8feead60$4c01a8c0@Chris>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/7b45f11c/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Fri Sep  9 16:31:12 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 9 Sep 2005 16:31:12 +0200
Subject: [R] how to do something like " subset(mat,
	("col1">4 & "col2">4)) "
References: <73dae306050909070918483ad8@mail.gmail.com>
Message-ID: <006101c5b54b$207154c0$0540210a@www.domain>

maybe something like this:

subset(mat, mat[, 2] > 4 & mat[, 3] > 4)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Florence Combes" <fcombes at gmail.com>
To: <r-help at stat.math.ethz.ch>; <r-devel-request at stat.math.ethz.ch>
Sent: Friday, September 09, 2005 4:09 PM
Subject: [R] how to do something like " subset(mat, ("col1">4 & 
"col2">4)) "


> Dear all,
>
> I have a problem with the "subset()" function. I spent all day 
> yesterday
> with a collegue to solve it and we did not find a satisfying 
> solution (even
> in the archived mails), so I ask for your help.
> Let's say (for a simple example) a matrix mat:
>
> R> mat
> cola colb colc
> [1,] 1 4 7
> [2,] 2 5 8
> [3,] 3 6 9
>
> My goal is to select the lines of the matrix on the basis of the 
> values of
> more than one column (let's say colb and colc).
> For example I want to select all the lines of the matrix for which 
> values in
> colb and colc are more than 4.
>
> I tried several ways that did not work:
>
> R> mat2 <- subset(mat, ("colb">4 & "colc">4))
> R> mat2
> [1] 1 2 3 4 5 6 7 8 9
>
> it is a vector, not a matrix.
>
>> mat2 <- subset(mat, mat[,2:3]>4)
>> mat2
> [1] 2 3 4 5 6 8 9
>
> tha same: it is a vector; so I tried:
>
>> mat2 <- as.matrix(subset(mat, mat[,("colb">4 & "colc">4)]))
>> mat2
> [,1]
> [1,] 1
> [2,] 2
> [3,] 3
> [4,] 4
> [5,] 5
> [6,] 6
> [7,] 7
> [8,] 8
> [9,] 9
>
> not good :(
>
> Did someone have an idea of how to select the only the lines 2 and 3 
> of mat
> by a selection on "colb" and "colc" >4 ?
>
> Thanks a lot,
>
> Florence.
>
> Version 2.0.1 (2004-11-15)
> (Linux Debian).
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From Mark.Steer at bristol.ac.uk  Fri Sep  9 16:40:08 2005
From: Mark.Steer at bristol.ac.uk (Mark Steer)
Date: Fri, 09 Sep 2005 15:40:08 +0100
Subject: [R] GlmmPQL help
Message-ID: <49D2E484453227FDAB3D0228@bio-ahustonpost.bio.bris.ac.uk>


Hi,

I'm running a GLMM on binomial choice data.  The outputs I receive are 
sensible except for the degrees of freedom, which come out much larger than 
expected.  Can anyone advise please?

Exptl design:
Response = Choice
Fixed Factors = Position, Treatment and Sex
Random Factor = ID nested within Treatment and Sex
Covariate = Delay

The model:
glmmPQL(FreeChoice ~ Position * Treatment + Sex + Delay, random = list(~Sex 
+ Treatment|Name), family = binomial)

Thanks, Mark

------------------------------------------------------------------------

Mark Steer
School of Biological Sciences
University of Bristol
Woodland Road
Bristol, BS8 1UG

tel - ++44 (0)117 9545945 (int. 45945)



From e.pebesma at geo.uu.nl  Fri Sep  9 16:48:53 2005
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Fri, 09 Sep 2005 16:48:53 +0200
Subject: [R]  adding text to the corner of a lattice plot
Message-ID: <4321A0D5.1060303@geo.uu.nl>

The following worked for me:

 > xyplot(rnorm(100)~ rnorm(100), 
key=list(text=list(title="mytext"),corner=c(0,0)))

but I'm sure there are better ways of doing this (esp. if you need
the key for the key!)
--
Edzer

Matt Wiener wrote:

I think this message from the help archives might address your question
(found using the query "lattice mtext" on R site search:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/51605.html.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch <https://stat.ethz.ch/mailman/listinfo/r-help>
[mailto:r-help-bounces at stat.math.ethz.ch <https://stat.ethz.ch/mailman/listinfo/r-help>] On Behalf Of RINNER Heinrich
Sent: Friday, September 09, 2005 4:16 AM
To: r-help at stat.math.ethz.ch <https://stat.ethz.ch/mailman/listinfo/r-help>
Subject: [R] adding text to the corner of a lattice plot


Dear R community,

I am using R 2.1.1 on Windows XP, package lattice Version 0.12-5, and
want to add text (sort of a dat-stamp actually) to the lower left corner
of a lattice plot, prefarably _after_ the plot has been created.

Here is a simple example what I do in base graphics:

# base graphics:
> plot(rnorm(100), rnorm(100))
> mtext(as.character(Sys.Date()), side = 1,line = -2, outer = T, adj =
0, font = 1, cex = 0.7)

How can I get the same using lattice?
# lattice:
> require(lattice)
> xyplot(rnorm(100) ~ rnorm(100))
> ???



From dimitris.rizopoulos at med.kuleuven.be  Fri Sep  9 17:09:22 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 9 Sep 2005 17:09:22 +0200
Subject: [R] Off-topic: Comparing standard errors from simulation
	andanalytical model
References: <88EAF3512A55DF46B06B1954AEF73F740A0A4159@dc1ex2.air.org>
Message-ID: <009401c5b550$757dd880$0540210a@www.domain>

since you are interested especially in the standard errors, I think 
that you probably need something like a double simulation procedure, 
e.g.,

1. simulate data D[b] and "contaminate" them.

2. fit the model (with parameters \theta) using D[b], get \theta[b] 
and also compute the standard errors se.a[b] using the asymptotic 
method.

3. using \theta[b] simulate M new data sets, "contaminate" them, fit 
the model in each one, obtain \theta[m] and calculate the standard 
deviation of these estimates se.mc[b]

4. keep res[b] = (se.mc[b] - se.a[b]) / se.mc[b]

5. repeat steps 1-4 B times and calculate, e.g., a 95% CI for res 
using the sample quantiles.


of course this is going to be much more time consuming (depending on 
the choices of B and M), but I think it will give you better a picture 
of how your method performs.


I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Doran, Harold" <HDoran at air.org>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, September 09, 2005 4:03 PM
Subject: [R] Off-topic: Comparing standard errors from simulation 
andanalytical model


> Dear list:
>
> I'm hoping to tap in to the statistical expertise in the group,
> especially those familiar with simulation techniques. I'm finalizing 
> a
> study where I obtain standard errors from two sources. The first 
> source
> is a monte carlo simulation and the other source is an analytical 
> model
> I have developed that appears to recover the standard errors from 
> the
> simulation. All analysis are performed in R using MASS, nlme, and
> Matrix.
>
> Here is a very brief description. In the monte carlo, I first sample
> from a multivariate distribution to create data. The data are
> hypothetical student scores on an achievement test over time and the 
> aim
> is to examine what happens to standard errors under certain 
> psychometric
> conditions. The data are then "contaminated" to reflect a certain
> psychometric problem that occurs in longitudinal analyses of student
> achievement scores.
>
> These data are then analyzed using a linear model to obtain 
> parameter
> estimates. This is replicated 250 times.
>
> For example, the model equation used is
>
> Y_{ti} =  \mu + \beta \cdot t + \epsilon_{ti}
>
> So, I obtain 250 estimates of \mu and \beta. I take the standard
> deviation of these estimates to get the sampling distribution of the
> parameter (standard errors). Next, I take a single data set, 
> contaminate
> the scores, and then use the analytical approach to obtain standard
> errors. So, I end up with two sets of standards errors, those 
> obtained
> under simulated conditions and those obtained from the analytical 
> model.
>
> My question is what are the most acceptable techniques for comparing 
> the
> standard errors in order to say that the analytical approach 
> actually
> "recovers" the monte carlo standard errors? For the most part, the
> standard errors appear to be exactly the same, save rounding error.
>
> One idea I am toying with is to average the standard errors of \mu 
> and
> \beta from the simulation and then do a t-test between the two 
> standard
> errors which might be something along these lines
>
> t = (SE_{analytical} - SE_{mc} )/  \bar se
>
> Where \bar se is the average of the standard errors.
>
> But I'm not certain this is correct. Can anyone suggest a more
> appropriate method for comparing the results?
>
> Many thanks. I can also send a copy of the paper to anyone who would
> like more information or details.
>
> -Harold
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From Francisco.Redelico at fi.austral.edu.ar  Fri Sep  9 17:11:19 2005
From: Francisco.Redelico at fi.austral.edu.ar (Francisco.Redelico@fi.austral.edu.ar)
Date: Fri, 9 Sep 2005 12:11:19 -0300
Subject: [R] Probit-normal model and ROC Curve
Message-ID: <OFF6C4C093.737E5034-ON03257077.005281B9-03257077.00536F43@austral.edu.ar>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/26dc19c8/attachment.pl

From maustin at amgen.com  Fri Sep  9 17:26:35 2005
From: maustin at amgen.com (Austin, Matt)
Date: Fri, 9 Sep 2005 08:26:35 -0700 
Subject: [R] Plot of multiple data sets
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD0F8@teal-exch.amgen.com>

The book is out--I received mine a few weeks ago.  It is very useful.

--Matt

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Chris 
> Buddenhagen
> Sent: Friday, September 09, 2005 6:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Plot of multiple data sets
> 
> 
> I found a lot of answers at this type of problem website wrt 
> graphics and
> multiple plots- I bet the book will be useful when it comes out.
> 
>  
> 
> http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html
> 
>  
> 
> Chris Buddenhagen, Botany Department, Charles Darwin Research 
> Station, Santa
> Cruz,Galapagos. Mail: Charles Darwin Foundation, Casilla 
> 17-01-3891 Avenida
> 6 de Diciembre N36-109 y Pasaje California Quito, ECUADOR
> 
>  
> 
> 
> 
> 
> 
> ______________________________________________________________________
> EL CONTENIDO DE ESTE MENSAJE ES DE ABSOLUTA RESPONSABILIDAD DEL AUTOR.
> FUNDACION CHARLES DARWIN
> WWW.DARWINFOUNDATION.ORG
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From paul.bliese at us.army.mil  Fri Sep  9 17:27:45 2005
From: paul.bliese at us.army.mil (Bliese, Paul D LTC USAMH)
Date: Fri, 9 Sep 2005 17:27:45 +0200
Subject: [R] Simulate phi-coefficient
Message-ID: <FADCFAA8BA80C748890C1D3893C198D98A7844@amedmlmhah01.eur.amed.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/6a8dd069/attachment.pl

From tlumley at u.washington.edu  Fri Sep  9 17:36:22 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 9 Sep 2005 08:36:22 -0700 (PDT)
Subject: [R] change in read.spss, package foreing?
In-Reply-To: <3.0.6.32.20050909015829.007bed60@pop.gmx.net>
References: <3.0.6.32.20050909015829.007bed60@pop.gmx.net>
Message-ID: <Pine.LNX.4.63a.0509090830230.619@homer23.u.washington.edu>

On Fri, 9 Sep 2005, Heinz Tuechler wrote:

> Dear All,
>
> it seems to me that the function read.spss of package foreign changed its
> behaviour regarding factors. I noted that in version 0.8-8 variables with
> value labels in SPSS were transformed in factors with the labels in
> alphabetic order.

No, they were in the order they were stored in the file

> In version 0.8-10 they seem to be ordered preserving the order
> corresponding to their numerical codes in SPSS.

They are now in the opposite order to their order in the file, which 
appears to be numerical order in SPSS (we can't be sure that this is 
always true since the file format is undocumented).

> However I could not find a description of this supposed change.

It's in the ChangeLog file in the foreign package.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From neo27 at t-online.de  Fri Sep  9 17:43:32 2005
From: neo27 at t-online.de (Mark Hempelmann)
Date: Fri, 09 Sep 2005 17:43:32 +0200
Subject: [R] regression with restrictions - optimization problem
Message-ID: <4321ADA4.6010704@t-online.de>

Dear WizaRds!

I am sorry to ask for some help, but I have come to a complete stop in 
my efforts. I hope, though, that some of you might find the problem 
quite interesting to look at.

I have been trying to estimate parameters for lotteries, the so called 
utility of chance, i.e. the "felt" probability compared to a rational 
given probability. A real brief example: Given is a lottery payoff 
matrix of the type

x1    x2 ...     x10     median
1000    5000 ... 5000    3750
0    1000 ... 5000    2250 etc.

The actual data frame consists of 11 columns and 28 rows.

Each entry x1 ... x10 gives the amount of money resp. the utility of 
that amount you receive playing the lottery. The probability for each 
column is 10%. The median represents the empirical answers of players 
where the person is indifferent if they prefer to receive the lottery or 
the sum of money as a sure payoff.

I try to determine the probability people feel instead of the known 10% 
probability of each column payoff entry. But here's the catch:

People also give different utilities to each amount of money, which 
basically gives us some sort of function like this:
u(x1...x10) = u(x1)*pi(p1) + u(x2)*pi(p2) +...+u(x10)*pi(p10)=y
u() - unknown utility function
pi() - unknown probability function
y - empirical answer
p1..p10 - probabilities, here always 0.1

To keep it simple, I set u(0)=0 and u(5000)=5000 and vary u(1000) 
between a start and end point. On each cycle R computes the regression 
coefficients that serve as the pi(p) estimators for every 10% step.
Then I minimize the residual sum of squares which should give the best 
estimators for every 10% step.

How can I possibly calculate a "smooth" pi(p) curve, a curve that should 
look like an "S", plotted against the cumulative 10% probabilities? I 
only have my ten estimators. How can I possibly tell R the necessary 
restrictions of nonnegative estimators and their sum to equal one? Here 
is my quite naive approach:

a70 <- matrix(c(1000,5000,5000,5000,2150, 0,1000,5000,5000,1750, 
0,0,1000,5000,1150, 0,0,0,1000,200, 1000,1000,5000,5000,2050, 
0,1000,1000,5000,1972), ncol=5, byrow=T)
colnames(a70)=c(paste("x", 1:4, sep=""), "med")
a70 <- as.data.frame(a70)

start=800; end=2000
step=10; u1000=start-step

u1000 <- u1000+step # varying the 1000 entry
a70[a70==1000] <- u1000
reg70 <- lm(a70$med ~ -1+x1+x2+x3+x4, data=a70)
res <- sum( (reg70$residuals^2) )

for (i in 1:( (end-start)/step) ){
         a70[a70==u1000]    <- u1000+step
     u1000 <- u1000+step
     reg70 <- lm(a70$med ~ -1+x1+x2+x3+x4, data=a70)
     if (res >= sum( (reg70$residuals^2) )) {
         res <- sum( (reg70$residuals^2) )
         print(paste("cycle", i, "u1000=", u1000, "RSS=", res))
         final70 <- a70
         finalreg <- reg70
         }
}
print(final70)
summary(finalreg)

     Maybe a better approach works with optim(stats) or dfp(Bhat), but I 
have no idea how to correctly approach such a restricted optimization 
problem.
     Thank you su much for your help and support.

Mark Hempelmann



From tlumley at u.washington.edu  Fri Sep  9 17:53:31 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 9 Sep 2005 08:53:31 -0700 (PDT)
Subject: [R] change in read.spss, package foreign?
In-Reply-To: <3.0.6.32.20050909145010.007c2a70@pop.gmx.net>
References: <3.0.6.32.20050909015829.007bed60@pop.gmx.net>
	<3.0.6.32.20050909015829.007bed60@pop.gmx.net>
	<3.0.6.32.20050909145010.007c2a70@pop.gmx.net>
Message-ID: <Pine.LNX.4.63a.0509090845050.619@homer23.u.washington.edu>

On Fri, 9 Sep 2005, Heinz Tuechler wrote:
> Dear Martin,
> Thank you for your answer. As I said, I appreciate this change. The
> documentation does not explain precisely, how variables with labels are
> treated now. It only tells "If SPSS value labels are converted to factors
> the underlying numerical codes will not in general be the same as the SPSS
> numerical values, since the numerical codes in R are always 1,2,3,...".
> Will now the created factor levels in any case be ordered according to the
> order of the original numerical codes in SPSS?

We don't know. We think so, based on a reasonable amount of 
experimentation, but the file format isn't documented.  We do know that 
the numerical codes R uses will always be 1,2,3,... so that there is no 
hope for having the same codes as SPSS unless the SPSS codes were also 
1,2,3...

> In general I wonder, how I could get to know such critical changes before I
> update a package instead of finding it out by chance. Is there a place,
> where a responsible R-user should look, when updating the program?

Many packages have a NEWS or ChangeLog file describing changes.  You would 
typically have to look at the source package to find them, since by Unix 
tradition they are usually in the top-level directory and so are not 
included in the binary build.

The foreign package is on svn.r-project.org, so you can see its Changelog 
there. There have been suggestions to extract these files and put them in 
the CRAN listing, but one obstacle is the lack of standardisation.

 	-thomas



From caobg at email.uc.edu  Fri Sep  9 17:49:15 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Fri, 9 Sep 2005 11:49:15 -0400
Subject: [R] usage of the trianed networks by nnet without R enviroment
Message-ID: <200509091549.AJH23176@mprelay4.uc.edu>

Dear All,

The nnet function in R is wonderful. I used it to train a network, and I'd like to use that network without R enviroment. Any suggestion so that I can start?

Best, 
 Baoqiang Cao



From phgrosjean at sciviews.org  Fri Sep  9 18:15:55 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 09 Sep 2005 18:15:55 +0200
Subject: [R] writing data to sheet in excel workbook
In-Reply-To: <971536df05090906367f8f92e2@mail.gmail.com>
References: <20050908181558.5990.qmail@web35903.mail.mud.yahoo.com>	<971536df0509081226768f2c56@mail.gmail.com>	<1115a2b00509090408545b72f2@mail.gmail.com>	<43217345.1050704@sciviews.org>
	<971536df05090906367f8f92e2@mail.gmail.com>
Message-ID: <4321B53B.7050000@sciviews.org>

Hello Gabor and Thomas and all,

OK, I see: I recently changed the disposition of the directories in 
tcltk2. This is the error. I just uploaded the latest version 
(tcltk2_0.8-9) on http://www.sciviews.org/SciViews-R, which should work 
now. Thank you for pointing me this error.
Best,

Philippe

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Gabor Grothendieck wrote:
> Hi, 
> I tried downloading tcltk2 and using 
>     Packages | Install Packages from Local Zip(s)
> under R 2.2.0 but got this:
> 
> 
>>utils:::menuInstallLocal()
> 
> package 'tcltk2' successfully unpacked and MD5 sums checked
> updating HTML package descriptions
> 
>>library(tcltk2)
> 
> Loading required package: tcltk
> Loading Tcl/Tk interface ... done
> Error in structure(.External("dotTclObjv", objv, PACKAGE = "tcltk"),
> class = "tclObj") :
>         [tcl] couldn't load library
> "C:/PROGRA~1/R/R-22~1.0DE/library/tcltk2/tklibs/winico0.5/winico05.dll":
> this library or a dependent library could not be found in library
> path.
> In addition: Warning message:
> Tcl package 'tile' not found in: tclRequire("tile") 
> Error: .onLoad failed in 'loadNamespace' for 'tcltk2'
> Error: package/namespace load failed for 'tcltk2'
> 
>>R.version.string
> 
> [1] "R version 2.2.0, 2005-09-03" 
> 
> On 9/9/05, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
> 
>>Hello,
>>
>>You cvould also look at http://www.sciviews.org/SciViews-R and download
>>tcltk2 package there. ?tk2dde gives you some examples on how to
>>manipulate Excel from R easily.
>>Best,
>>
>>Philippe Grosjean
>>
>>..............................................<??}))><........
>> ) ) ) ) )
>>( ( ( ( (    Prof. Philippe Grosjean
>> ) ) ) ) )
>>( ( ( ( (    Numerical Ecology of Aquatic Systems
>> ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
>>( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>> ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
>>( ( ( ( (
>> ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
>>( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>> ) ) ) ) )
>>( ( ( ( (    web:   http://www.umh.ac.be/~econum
>> ) ) ) ) )          http://www.sciviews.org
>>( ( ( ( (
>>..............................................................
>>
>>Wensui Liu wrote:
>>
>>>RDCOMClient package works great. But it might take lots of coding if you
>>>want to write a big report.
>>>
>>>On 9/8/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>>
>>>
>>>>On 9/8/05, adalbert duerrer <adi_due at yahoo.ca> wrote:
>>>>
>>>>
>>>>>Hi,
>>>>>
>>>>>I believe to remember there is a package that lets you
>>>>>write data from R to different sheets in a Excel
>>>>>workbook. I've been looking around on CRAN but could
>>>>>not find what I am looking for.
>>>>>
>>>>
>>>>See
>>>>
>>>>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/58249.html
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>>
>>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From jporzak at gmail.com  Fri Sep  9 18:19:16 2005
From: jporzak at gmail.com (Jim Porzak)
Date: Fri, 9 Sep 2005 09:19:16 -0700
Subject: [R] Plot of multiple data sets
In-Reply-To: <000501c5b542$8feead60$4c01a8c0@Chris>
References: <000501c5b542$8feead60$4c01a8c0@Chris>
Message-ID: <2a9c000c050909091916b3724e@mail.gmail.com>

Paul's book has been available in the states since mid-Aug. Is on my
local bookseller's shelf (&, of course, mine)

I would recommend it to anyone doing more than "off the shelf"
graphics in R. As expected, an especially good look at grid.

Hopefully it will be available in Europe soon.

-- 
Best,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA


On 9/9/05, Chris Buddenhagen <chrisb at fcdarwin.org.ec> wrote:
> I found a lot of answers at this type of problem website wrt graphics and
> multiple plots- I bet the book will be useful when it comes out.
> 
> 
> 
> http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html
> 
> 
> 
> Chris Buddenhagen, Botany Department, Charles Darwin Research Station, Santa
> Cruz,Galapagos. Mail: Charles Darwin Foundation, Casilla 17-01-3891 Avenida
> 6 de Diciembre N36-109 y Pasaje California Quito, ECUADOR
> 
> 
> 
> 
> 
> 
> 
> ______________________________________________________________________
> EL CONTENIDO DE ESTE MENSAJE ES DE ABSOLUTA RESPONSABILIDAD DEL AUTOR.
> FUNDACION CHARLES DARWIN
> WWW.DARWINFOUNDATION.ORG
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tuechler at gmx.at  Fri Sep  9 18:47:46 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Fri, 09 Sep 2005 18:47:46 +0200
Subject: [R] change in read.spss, package foreign?
In-Reply-To: <Pine.LNX.4.63a.0509090845050.619@homer23.u.washington.edu>
References: <3.0.6.32.20050909145010.007c2a70@pop.gmx.net>
	<3.0.6.32.20050909015829.007bed60@pop.gmx.net>
	<3.0.6.32.20050909015829.007bed60@pop.gmx.net>
	<3.0.6.32.20050909145010.007c2a70@pop.gmx.net>
Message-ID: <3.0.6.32.20050909184746.007c8100@pop.gmx.net>

Dear Thomas,
Thanks a lot for your extensive answer.
Heinz

At 08:53 09.09.2005 -0700, Thomas Lumley wrote:
>On Fri, 9 Sep 2005, Heinz Tuechler wrote:
>> Dear Martin,
>> Thank you for your answer. As I said, I appreciate this change. The
>> documentation does not explain precisely, how variables with labels are
>> treated now. It only tells "If SPSS value labels are converted to factors
>> the underlying numerical codes will not in general be the same as the SPSS
>> numerical values, since the numerical codes in R are always 1,2,3,...".
>> Will now the created factor levels in any case be ordered according to the
>> order of the original numerical codes in SPSS?
>
>We don't know. We think so, based on a reasonable amount of 
>experimentation, but the file format isn't documented.  We do know that 
>the numerical codes R uses will always be 1,2,3,... so that there is no 
>hope for having the same codes as SPSS unless the SPSS codes were also 
>1,2,3...
>
>> In general I wonder, how I could get to know such critical changes before I
>> update a package instead of finding it out by chance. Is there a place,
>> where a responsible R-user should look, when updating the program?
>
>Many packages have a NEWS or ChangeLog file describing changes.  You would 
>typically have to look at the source package to find them, since by Unix 
>tradition they are usually in the top-level directory and so are not 
>included in the binary build.
>
>The foreign package is on svn.r-project.org, so you can see its Changelog 
>there. There have been suggestions to extract these files and put them in 
>the CRAN listing, but one obstacle is the lack of standardisation.
>
> 	-thomas
>
>



From andy_liaw at merck.com  Fri Sep  9 19:50:35 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 9 Sep 2005 13:50:35 -0400
Subject: [R] usage of the trianed networks by nnet without R enviromen t
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED41E@usctmx1106.merck.com>

One possibility is to look at predict.nnet(), and

- Write an R function that write out parts of an nnet object that are needed
by predict.nnet() to an external file.

- Re-write predict.nnet() in C, reading the model information from the
external file.  Obviously you'll also need the C source for the code that
predict.nnet() calls, and modify those as needed to strip out dependency on
R, if possible.

Andy

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Baoqiang Cao
> Sent: Friday, September 09, 2005 11:49 AM
> To: R-help
> Subject: [R] usage of the trianed networks by nnet without R 
> enviroment
> 
> 
> Dear All,
> 
> The nnet function in R is wonderful. I used it to train a 
> network, and I'd like to use that network without R 
> enviroment. Any suggestion so that I can start?
> 
> Best, 
>  Baoqiang Cao
> 
>



From chrysopa at insecta.ufv.br  Fri Sep  9 19:55:04 2005
From: chrysopa at insecta.ufv.br (Ronaldo Reis-Jr.)
Date: Fri, 9 Sep 2005 14:55:04 -0300
Subject: [R] best way to fit a model
Message-ID: <200509091455.04898.chrysopa@insecta.ufv.br>

Hi,

I have some data that have this behaviour:

|
|*******
|        *
|          * 
|            *
|              *
|----------------

What is the best and simpler way to fit this in R?

Thanks
Ronaldo
-- 
Ela pilotava um Continenal 2001 com igni????o autom??tica Magiclic...
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From reid_huntsinger at merck.com  Fri Sep  9 20:21:01 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 9 Sep 2005 14:21:01 -0400
Subject: [R] best way to fit a model
Message-ID: <355C35514FEAC9458F75947F5270974D076CB2@usctmx1103.merck.com>

You might want to look at the "segmented" package on
http://cran.r-project.org and the accompanying paper. Some important
questions are: is the point at which the data changes from flat to
decreasing known? (I presume not...) Does it correspond to some change in
the process being measured? Do you want to estimate it? 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ronaldo Reis-Jr.
Sent: Friday, September 09, 2005 1:55 PM
To: R-Help
Subject: [R] best way to fit a model


Hi,

I have some data that have this behaviour:

|
|*******
|        *
|          * 
|            *
|              *
|----------------

What is the best and simpler way to fit this in R?

Thanks
Ronaldo
-- 
Ela pilotava um Continenal 2001 com igni????o autom??tica Magiclic...
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Achim.Zeileis at wu-wien.ac.at  Fri Sep  9 20:25:48 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 9 Sep 2005 20:25:48 +0200
Subject: [R] best way to fit a model
In-Reply-To: <200509091455.04898.chrysopa@insecta.ufv.br>
References: <200509091455.04898.chrysopa@insecta.ufv.br>
Message-ID: <20050909202548.75ed4fdd.Achim.Zeileis@wu-wien.ac.at>

On Fri, 9 Sep 2005 14:55:04 -0300 Ronaldo Reis-Jr. wrote:

> Hi,
> 
> I have some data that have this behaviour:
> 
> |
> |*******
> |        *
> |          * 
> |            *
> |              *
> |----------------
> 
> What is the best and simpler way to fit this in R?

If the changepoint is known, then this is straightforward using lm:

## generate example data
set.seed(20050909)
x <- seq(0, 10, by = 0.25)
y.mean <- ifelse(x >= 5, x, 5)
y <- y.mean + rnorm(41)
plot(y ~ x)
lines(y.mean ~ x)

## fit linear model with break
fm <- lm(y ~ I((x-5) * (x >= 5)))
fm
y.fit1 <- fitted(fm)
lines(y.fit1 ~ x, col = 2)

If it is unknown, it can be estimated using Vito Muggeo's segmented
package:

## estimate change point in x
library("segmented")
sfm <- segmented(lm(y ~ x), x, 6)
sfm
y.fit2 <- fitted(sfm)
lines(y.fit2 ~ x, col = 3)

This fits a continuous mean function. Alternatively, breakpoints() in
strucchange can be used to estimate a break point:

## estimate break point in x
library("strucchange")
bp <- breakpoints(y ~ x)
summary(bp)
y.fit3 <- fitted(bp)
lines(y.fit3 ~ x, col = 4)

This does not enforce that the line is continuous, hence the jump in the
fitted mean. Of course, the estimated breakpoint could be used to fit a
continuous line model, but this is not what is optimized in
breakpoints().
Z

> Thanks
> Ronaldo
> -- 
> Ela pilotava um Continenal 2001 com igni????o autom??tica Magiclic...
> --
> |>   // | \\   [***********************************]
> |   ( ??   ?? )  [Ronaldo Reis J??nior                ]
> |>      V      [UFV/DBA-Entomologia                ]
> |    /     \   [36570-000 Vi??osa - MG              ]
> |>  /(.''`.)\  [Fone: 31-3899-4007                 ]
> |  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |    ( `-  )   [***********************************]
> |>>  _/   \_Powered by GNU/Debian Woody/Sarge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ychung4 at uiuc.edu  Fri Sep  9 21:29:59 2005
From: ychung4 at uiuc.edu (Larsen Chung)
Date: Fri, 9 Sep 2005 14:29:59 -0500
Subject: [R] Question about plotting discontinuous data
Message-ID: <b05c08bb.393bcf90.aefe400@expms3.cites.uiuc.edu>

Hi, I have a simple question that I just cannot figure out. I
have 2 corresponding columns of data, one column (X-axis) for
time (formatted thus: 8:30:01am = 830.1, 12:30:05pm = 1230.5,
and one column (Y-axis) for values.

When I attempt to plot the data using something like
plot(inputdata[,1],inputdata[,2],type="l");
I get breaks in the plot (since the time essentially jumps
from 8:59:59am = 859.59 to 9:00:00am = 900.0).

Essentially, I get the plot shape I want if I just plot the
'values' column using plot(inputdata[,2],type="l");
however, then I don't get the corresponding 'time' labels. Is
there a way I can plot these two columns together without the
'breaks'?



From Achim.Zeileis at wu-wien.ac.at  Fri Sep  9 21:39:27 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 9 Sep 2005 21:39:27 +0200 (CEST)
Subject: [R] Question about plotting discontinuous data
In-Reply-To: <b05c08bb.393bcf90.aefe400@expms3.cites.uiuc.edu>
References: <b05c08bb.393bcf90.aefe400@expms3.cites.uiuc.edu>
Message-ID: <Pine.LNX.4.58.0509092137070.7743@thorin.ci.tuwien.ac.at>

On Fri, 9 Sep 2005, Larsen Chung wrote:

> Hi, I have a simple question that I just cannot figure out. I
> have 2 corresponding columns of data, one column (X-axis) for
> time (formatted thus: 8:30:01am = 830.1, 12:30:05pm = 1230.5,
> and one column (Y-axis) for values.

R comes with support for various time formats, use them! Look at
particular at "chron" and "POSIXct". In addition, you might want to define
a time series object which might make plotting (and other operations) even
easier. Look in particular at the "zoo" package which supports both time
formats above.
Z

> When I attempt to plot the data using something like
> plot(inputdata[,1],inputdata[,2],type="l");
> I get breaks in the plot (since the time essentially jumps
> from 8:59:59am = 859.59 to 9:00:00am = 900.0).
>
> Essentially, I get the plot shape I want if I just plot the
> 'values' column using plot(inputdata[,2],type="l");
> however, then I don't get the corresponding 'time' labels. Is
> there a way I can plot these two columns together without the
> 'breaks'?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jfox at mcmaster.ca  Fri Sep  9 21:37:50 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 9 Sep 2005 15:37:50 -0400
Subject: [R] [R-pkgs] Internationalized version of the Rcmdr package
Message-ID: <20050909193749.TNWQ1799.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear R-packages list members,

I've recently uploaded to CRAN version 1.1-1 of the Rcmdr ("R Commander")
package. Based on the tcltk package, the Rcmdr provides a basic-statistics
graphical interface to R.

The general functionality of the R Commander has not changed much since
version 1.0-0. There are some small improvements to the package, but the
reason that I'm re-announcing it is that the latest version supports
translation into other languages, using the new internationalization and
localization features of R 2.1.x. 

A Catalan translation (courtesy of Manel Salamero), a French translation
(courtesy of Philippe Grosjean), a Japanese translations (courtesy of
Takaharu Araki), and a Slovenian translation (courtesy of Jaro Lajovic), are
provided with the Rcmdr package. I understand that Portuguese, Russian, and
Spanish translations are in the works, and these will be incorporated when
they are completed. 

If you're interested in translating the Rcmdr into another language, please
contact me.

As before, bug reports, suggestions, and other feedback are appreciated.

 John 

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ggrothendieck at gmail.com  Fri Sep  9 21:58:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Sep 2005 15:58:55 -0400
Subject: [R] Question about plotting discontinuous data
In-Reply-To: <b05c08bb.393bcf90.aefe400@expms3.cites.uiuc.edu>
References: <b05c08bb.393bcf90.aefe400@expms3.cites.uiuc.edu>
Message-ID: <971536df05090912587104a60d@mail.gmail.com>

On 9/9/05, Larsen Chung <ychung4 at uiuc.edu> wrote:
> Hi, I have a simple question that I just cannot figure out. I
> have 2 corresponding columns of data, one column (X-axis) for
> time (formatted thus: 8:30:01am = 830.1, 12:30:05pm = 1230.5,
> and one column (Y-axis) for values.
> 
> When I attempt to plot the data using something like
> plot(inputdata[,1],inputdata[,2],type="l");
> I get breaks in the plot (since the time essentially jumps
> from 8:59:59am = 859.59 to 9:00:00am = 900.0).
> 
> Essentially, I get the plot shape I want if I just plot the
> 'values' column using plot(inputdata[,2],type="l");
> however, then I don't get the corresponding 'time' labels. Is
> there a way I can plot these two columns together without the
> 'breaks'?

Suppose this is your input data:

xx <- 11:12
tt <- c(830.1, 1230.5)

# use the times class in chron library
library(chron)
tt.times <- times(paste(tt %/% 100, floor(tt %% 100), (100 * tt) %%
100, sep = ":"))
plot(tt.times, xx)

# or represent it as a zoo object and plot that
library(zoo)
z <- zoo(xx, tt.times)
plot(z)



From fcombes at gmail.com  Fri Sep  9 22:29:35 2005
From: fcombes at gmail.com (Florence Combes)
Date: Fri, 9 Sep 2005 22:29:35 +0200
Subject: [R] how to do something like " subset(mat,
	("col1">4 & "col2">4)) " -- THANKS
In-Reply-To: <x2d5nicobp.fsf@turmalin.kubism.ku.dk>
References: <73dae306050909070918483ad8@mail.gmail.com>
	<x2d5nicobp.fsf@turmalin.kubism.ku.dk>
Message-ID: <73dae30605090913296d352b9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/8bd65c52/attachment.pl

From deepayan.sarkar at gmail.com  Fri Sep  9 22:51:22 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 9 Sep 2005 15:51:22 -0500
Subject: [R] adding text to the corner of a lattice plot
In-Reply-To: <4E9A692D8755DF478B56A2892388EE1F06B717@usctmx1118.merck.com>
References: <4E9A692D8755DF478B56A2892388EE1F06B717@usctmx1118.merck.com>
Message-ID: <eb555e6605090913514e8ac7e3@mail.gmail.com>

On 9/9/05, Wiener, Matthew <matthew_wiener at merck.com> wrote:
> I think this message from the help archives might address your question
> (found using the query "lattice mtext" on R site search:
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/51605.html.

That's what I would recommend. You can even set 

lattice.options(default.args = list(page = function(n) 
    grid::grid.text(date(), x = 0.01, y = 0.01, 
                    default.units = "npc", 
                    just = c("left", "bottom"))))

to do this for all plots by default without having to change any code.

Deepayan

> Hope this helps,
> 
> Matt Wiener
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of RINNER Heinrich
> Sent: Friday, September 09, 2005 4:16 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] adding text to the corner of a lattice plot
> 
> 
> Dear R community,
> 
> I am using R 2.1.1 on Windows XP, package lattice Version 0.12-5, and
> want to add text (sort of a dat-stamp actually) to the lower left corner
> of a lattice plot, prefarably _after_ the plot has been created.
> 
> Here is a simple example what I do in base graphics:
> 
> # base graphics:
> > plot(rnorm(100), rnorm(100))
> > mtext(as.character(Sys.Date()), side = 1,line = -2, outer = T, adj =
> 0, font = 1, cex = 0.7)
> 
> How can I get the same using lattice?
> # lattice:
> > require(lattice)
> > xyplot(rnorm(100) ~ rnorm(100))
> > ???
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From larry_sonna at hotmail.com  Fri Sep  9 16:10:06 2005
From: larry_sonna at hotmail.com (Larry A Sonna)
Date: Fri, 9 Sep 2005 10:10:06 -0400
Subject: [R] Discrepancy between R and SPSS in 2-way, repeated measures ANOVA
Message-ID: <BAY106-DAV1044EA3896BE9BA8617C2099980@phx.gbl>

Dear R community,

I am trying to resolve a discrepancy between the way SPSS and R handle 
2-way, repeated measures ANOVA.

An experiment was performed in which samples were drawn before and after 
treatment of four groups of subjects (control and disease states 1, 2 and 
3).  Each group contained five subjects.  An experimental measurement was 
performed on each sample to yield a "signal".  The before and after 
treatment signals for each subject were treated as repeated measures.  We 
desire to obtain P values for disease state ("CONDITION"), and the 
interaction between signal over time and disease state ("CONDITION*TIME").

Using SPSS, the following output was obtained:
                      DF        SumSq (Type 3)    Mean Sq    F value     P=

COND              3                 42861            14287       3.645 
0.0355

TIME                1                     473               473       0.175 
0.681

COND*TIME     3                     975               325       0.120 
0.947

Error                16                43219             2701



By contrast, using the following R command:

summary(aov(SIGNAL~(COND+TIME+COND*TIME)+Error(EXPNO/COND), Type="III"))

the output was as follows:

                  Df     Sum Sq     Mean Sq     F value  Pr(>F)

COND          3          26516       8839      3.2517     0.03651 *

TIME            1            473         473      0.1739     0.67986

COND:TIME  3            975         325      0.1195     0.94785

Residuals     28        76107      2718



I don't understand why the two results are discrepant.  In particular, I'm 
not sure why R is yielding 28 DF for the residuals whereas SPSS only yields 
16.  Can anyone help?



E-mail replies would be much appreciated.  I can be reached at 
larry_sonna at yahoo.com and at larry_sonna at hotmail.com





Thanks in advance,



Larry Sonna



From mzarkov at EUnet.yu  Fri Sep  9 23:10:28 2005
From: mzarkov at EUnet.yu (Milos Zarkovic)
Date: Fri, 9 Sep 2005 23:10:28 +0200
Subject: [R] Interpolating / smoothing missing time series data
References: <BAY103-F10EBC58AD14F15DEB52901A6990@phx.gbl>
	<431FD766.6070600@hhbio.wasser.tu-dresden.de>
Message-ID: <009501c5b583$16fd4f10$0401a8c0@milos>

I agree that excessive interpolation might cause problems.
Maybe  Lomb-Scargle periodogram could be used (spectral analysis on unevenly 
spaced data).
Another option would be to use Kalman filtering to interpolate data.
I belive that both are implemented in R.

Milos Zarkovic
----- Original Message ----- 
From: "Thomas Petzoldt" <thpe at hhbio.wasser.tu-dresden.de>
To: "Francisco J. Zagmutt" <gerifalte28 at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, September 08, 2005 8:17 AM
Subject: Re: [R] Interpolating / smoothing missing time series data


> Francisco J. Zagmutt wrote:
>> I don't have much experience in the subject but it seems that 
>> library(akima)
>> should be useful for your problem. Try library(help="akima") to see a 
>> list
>> of the functions available in the library.
>>
>> I hope this helps
>>
>> Francisco
>
> Yes, function aspline() of package akima is well suited for such things:
> no wiggles like in spline() and less variance reducing than approx().
> But in any case: excessive interpolation will definitely lead to biased
> results, in particular artificial autocorrelations.
>
> If ever possible, David should look for methods, capable of dealing with
> missing data directly.
>
> Thomas P.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ccameron at Princeton.EDU  Sat Sep 10 00:04:56 2005
From: ccameron at Princeton.EDU (Charles M Cameron)
Date: Fri, 9 Sep 2005 18:04:56 -0400
Subject: [R] "Chow Test" for classification and regression trees
Message-ID: <D6D1F322F8FCE44C9ED6CFBC405FFB270EDF38@EXCLUSTER.pu.win.princeton.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/ae883518/attachment.pl

From sdshlxh at gmail.com  Sat Sep 10 00:19:12 2005
From: sdshlxh at gmail.com (Ping Yao)
Date: Fri, 9 Sep 2005 15:19:12 -0700
Subject: [R] R-help Digest, Vol 31, Issue 9
In-Reply-To: <mailman.13.1126260002.17027.r-help@stat.math.ethz.ch>
References: <mailman.13.1126260002.17027.r-help@stat.math.ethz.ch>
Message-ID: <e99f98a705090915196accb6f2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050909/e5de99a2/attachment.pl

From leif at reflectivity.com  Sat Sep 10 01:40:29 2005
From: leif at reflectivity.com (Leif Kirschenbaum)
Date: Fri, 9 Sep 2005 16:40:29 -0700
Subject: [R] PowerPoint graph insertion
Message-ID: <200509092340.j89NeYZC013173@hypatia.math.ethz.ch>

Yes:  I have Tufte's monograph on my desk. (along with 4 statistics texts)
Yes:  I am not the biggest fan of PowerPoint.
Yes:  I am using R to generate charts, plots, trends, etc. I have to summarize them each week.

  When I consider how to organize this data my first thought is to generate an HTML file with links to the R-generated plots, which HTML file organizes the plots in the required order.
However:
 * Each week we annotate one PowerPoint slide in the weekly presentation with action items -- we don't only use PP as a presentation tool. This is convenient, as then the action items resulting from particular data trends are associated in a single document with the plots of the data trends.
 * Other (non-R) users insert data into the weekly PP presentation: from other plotting software and images from various sources (microscope, SEM, TEM, etc.), which I cannot easily incorporate into a generated HTML file before-the-fact.
 * I'm not sure how to create an HTML file which allows one to page forward and backward through it easily, as with PowerPoint (a minor point: and there is probably a way to write HTML to respond to such)

So:
 Can R insert plots into an existing PowerPoint presentation?
(actually, I'd copy last week's presentation and then update with new plots)

 I'll guess that it cannot, as there probably is not a Microsoft supplied interface (ODBC or otherwise) with PowerPoint as there is with Excel.

-Leif Kirschenbaum, Ph.D.
 Sr. Yield Integration Engineer (I'm a physicist)
 leif at reflectivity.com



From prm at runbox.us  Sat Sep 10 01:41:25 2005
From: prm at runbox.us (Paul MacManus)
Date: Fri, 09 Sep 2005 19:41:25 -0400 (EDT)
Subject: [R] less precision, please!
Message-ID: <E1EDsUn-0003Nb-BN@garm.runbox.com>

I need to run qbeta on a set of 500K different parameter pairs (with a fixed quantile). For most pairs qbeta finds the solution very quickly but for a substantial minority of the cases qbeta is very slow. This occurs when the solution is very close to zero. qbeta is getting answers to a precision of about 16 decimal places. I don't need that accuracy. Is there any way to set the precision of R's calculations to, say, 9 decimal places and so speed up the whole process?

I could, of course, avoid this problem by not running qbeta when I know the solution is going to be sufficiently small but I'm more interested in ways to adjust the precision of calculations in R.

          Thanks, Paul



From murdoch at stats.uwo.ca  Sat Sep 10 01:55:23 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Sep 2005 19:55:23 -0400
Subject: [R] less precision, please!
In-Reply-To: <E1EDsUn-0003Nb-BN@garm.runbox.com>
References: <E1EDsUn-0003Nb-BN@garm.runbox.com>
Message-ID: <432220EB.5030204@stats.uwo.ca>

On 9/9/2005 7:41 PM, Paul MacManus wrote:
> I need to run qbeta on a set of 500K different parameter pairs (with a fixed quantile). For most pairs qbeta finds the solution very quickly but for a substantial minority of the cases qbeta is very slow. This occurs when the solution is very close to zero. qbeta is getting answers to a precision of about 16 decimal places. I don't need that accuracy. Is there any way to set the precision of R's calculations to, say, 9 decimal places and so speed up the whole process?
> 
> I could, of course, avoid this problem by not running qbeta when I know the solution is going to be sufficiently small but I'm more interested in ways to adjust the precision of calculations in R.

There's no general way to do this.  The function that implements qbeta 
may have some tuning parameters (I haven't looked), but they aren't 
usually needed, and aren't exposed in R.

If you want a quick approximation, I'd suggest doing your calculation on 
a grid of values and using approx() to interpolate.

Duncan Murdoch



From jahumada at usgs.gov  Sat Sep 10 02:38:56 2005
From: jahumada at usgs.gov (Jorge Ahumada)
Date: Fri, 9 Sep 2005 19:38:56 -0500
Subject: [R] adding labels to a multiple plot in lattice
Message-ID: <9B06A25E-7AEE-4E74-90CD-7DE0610E33F8@usgs.gov>

Hello,

I am trying to a create a plot in lattice that has four panels, but  
all of them have the same xlab and ylab, so instead of labeling each  
plot  separately I want to create one single label for the y series  
and one for the x series, somehow along the left and bottom margins  
of the plot. Can anybody help? This is the pseudocode:

print(xyplot(x1~y1),split=c(1,1,2,2),more=T)
print(xyplot(x2~y2),split=c(2,1,2,2),more=T)
print(xyplot(x3~y3),split=c(1,2,2,2),more=T)
print(xyplot(x4~y4),split=c(2,2,2,2))

thanks,

Jorge



From jeff.hamann at forestinformatics.com  Sat Sep 10 03:08:34 2005
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Fri, 9 Sep 2005 18:08:34 -0700
Subject: [R] transparent backgound in lattice plots
Message-ID: <000901c5b5a4$311e23d0$0a00a8c0@rodan>

I couldn't find this is in the FAQs or in the R archives and I've poked 
around the lattice package manual, but...

I've been trying to figure out how to generate graphics files with a 
transparent background with the lattice package. The code to generate the 
levelplot is fine and I can produce a simple png file with a transparent 
background just fine, but I'm not sure how to generate a png of a lattice 
plot with a transparent backgound.

png( "../images/opt_harvest_pattern_pred.png", bg="transparent" )
main=expression( paste( "Optimal Harvest Pattern ", widehat(V)[saw], ", in 
", m^3 ) )
labs <- paste( rep( "Day", 12 ), 1:12 )
opt.harv.pat.pred <- levelplot( var1.pred~x+y,
                 data=saw.comb,
                 aspect=mapasp(saw.comb),
                 col.regions=terrain.colors(80),
                 main=main,
                 panel = function(...) {
                   panel.levelplot(...)
                   panel.grid(h=-1, v=-1, col="black", lwd=2)
                   panel.text( from.x[par[1:12]],
                              from.y[par[1:12]]+20,
                              labs, color="black", lwd=2, cex=1 )
                   panel.arrows( from.x[par[1:11]], from.y[par[1:11]],
                                from.x[par[2:12]], from.y[par[2:12]],
                                angle=10, col="black", lwd=1 )
                 }
                 )
print( opt.harv.pat.pred )
dev.off()

the png bg="transparent" seems to have no effect here and I'm not sure where 
in the lattice levelplot call to set the background to transparent. I'm sure 
it's obvious....

Help?
Jeff.

---
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon USA 97339-1421
541-754-1428
jeff.hamann at forestinformatics.com
www.forestinformatics.com



From deepayan.sarkar at gmail.com  Sat Sep 10 03:59:52 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 9 Sep 2005 20:59:52 -0500
Subject: [R] transparent backgound in lattice plots
In-Reply-To: <000901c5b5a4$311e23d0$0a00a8c0@rodan>
References: <000901c5b5a4$311e23d0$0a00a8c0@rodan>
Message-ID: <eb555e66050909185940fb3070@mail.gmail.com>

On 9/9/05, Jeff D. Hamann <jeff.hamann at forestinformatics.com> wrote:
> I couldn't find this is in the FAQs or in the R archives and I've poked 
> around the lattice package manual, but...
> 
> I've been trying to figure out how to generate graphics files with a 
> transparent background with the lattice package. The code to generate the 
> levelplot is fine and I can produce a simple png file with a transparent 
> background just fine, but I'm not sure how to generate a png of a lattice 
> plot with a transparent backgound.

The short answer is 

trellis.par.set(background = list(col = "transparent"))

but see ?trellis.device for a better answer.

Deepayan



From jfox at mcmaster.ca  Sat Sep 10 04:33:15 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 9 Sep 2005 22:33:15 -0400
Subject: [R] Plotting an ellipse in 3D
In-Reply-To: <43218802.6090401@stats.uwo.ca>
Message-ID: <20050910023314.ZTTQ26550.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Duncan and Mike,

For some time I've been meaning to add data (concentration) ellipsoids to
the scatter3d function in the Rcmdr package, which uses rgl.

The functions below are a first crack at this. I'm pretty sure that the
approach I've taken is correct -- I deform a unit sphere, adapting a bit of
the code in the rgl demo to generate the sphere -- but I haven't checked it
carefully. As well, I'm sure that someone more conversant with the new tools
in rgl (like Duncan!) could render the ellipsoids better than I've done. You
can try the following examples to see how this works:

library(rgl)
library(MASS)

R <- matrix(c(1, .5, .5, .5, 1, .5, .5, .5, 1), 3, 3)
data <- mvrnorm(n=200, mu=c(0,0,0), Sigma=R)
scatter3d(data[,1], data[,2], data[,3], ellipsoid=TRUE)

data1 <- mvrnorm(n=100, mu=c(0,0,0), Sigma=R)
data2 <- mvrnorm(n=100, mu=c(1,1,1), Sigma=R)
data <- rbind(data1, data2)
groups <- as.factor(c(rep("a", 100), rep("b", 100)))
scatter3d(data[,1], data[,2], data[,3], ellipsoid=TRUE, 
    surface=FALSE, groups=groups)


Regards,
 John

---------- snip ---------------

ellipsoid <- function(center=c(0, 0, 0), radius=1, shape=diag(3),
segments=51) {
  angles <- (0:segments)*2*pi/segments
  ecoord2 <- function(p) {
    c(cos(p[1])*sin(p[2]), sin(p[1])*sin(p[2]), cos(p[2])) }
  unit.sphere <- t(apply(expand.grid(angles, angles), 1, ecoord2))
  t(center + radius * t(unit.sphere %*% chol(shape))) 
}


scatter3d <- function(x, y, z, xlab=deparse(substitute(x)),
ylab=deparse(substitute(y)),
                      zlab=deparse(substitute(z)), revolutions=0,
bg.col=c("white", "black"), 
                      axis.col=if (bg.col == "white") "black" else "white",
                      surface.col=c("blue", "green", "orange", "magenta",
"cyan", "red", "yellow", "gray"),
                      neg.res.col="red", pos.res.col="green",
point.col="yellow",
                      text.col=axis.col, grid.col=if (bg.col == "white")
"black" else "gray",
                      fogtype=c("exp2", "linear", "exp", "none"),
                      residuals=(length(fit) == 1), surface=TRUE, grid=TRUE,
grid.lines=26,
                      df.smooth=NULL, df.additive=NULL,
                      sphere.size=1, threshold=0.01, speed=1, fov=60,
                      fit="linear", groups=NULL, parallel=TRUE,
ellipsoid=FALSE, level=0.5, model.summary=FALSE){
    require(rgl)
    require(mgcv)
    summaries <- list()
    if ((!is.null(groups)) && (nlevels(groups) > length(surface.col))) 
        stop(sprintf(gettextRcmdr("Number of groups (%d) exceeds number of
colors (%d)."),
            nlevels(groups), length(surface.col)))
    if ((!is.null(groups)) && (!is.factor(groups)))
stop(gettextRcmdr("groups variable must be a factor."))
    bg.col <- match.arg(bg.col)
    fogtype <- match.arg(fogtype)
    if ((length(fit) > 1) && residuals && surface)
        stop(gettextRcmdr("cannot plot both multiple surfaces and
residuals"))
    xlab  # cause these arguments to be evaluated
    ylab
    zlab
    rgl.clear()
    rgl.viewpoint(fov=fov)
    rgl.bg(col=bg.col, fogtype=fogtype)
    valid <- if (is.null(groups)) complete.cases(x, y, z)
        else complete.cases(x, y, z, groups)
    x <- x[valid]
    y <- y[valid]
    z <- z[valid]
    if (!is.null(groups)) groups <- groups[valid]
    x <- (x - min(x))/(max(x) - min(x))
    y <- (y - min(y))/(max(y) - min(y))
    z <- (z - min(z))/(max(z) - min(z))
    size <- sphere.size*((100/length(x))^(1/3))*0.015
    if (is.null(groups)){
        if (size > threshold) rgl.spheres(x, y, z, color=point.col,
radius=size)
            else rgl.points(x, y, z, color=point.col)
            }
    else {
        if (size > threshold) rgl.spheres(x, y, z,
color=surface.col[as.numeric(groups)], radius=size)
            else rgl.points(x, y, z, color=surface.col[as.numeric(groups)])
            }
    rgl.lines(c(0,1), c(0,0), c(0,0), color=axis.col)
    rgl.lines(c(0,0), c(0,1), c(0,0), color=axis.col)
    rgl.lines(c(0,0), c(0,0), c(0,1), color=axis.col)
    rgl.texts(1, 0, 0, xlab, adj=1, color=text.col)
    rgl.texts(0, 1, 0, ylab, adj=1, color=text.col)
    rgl.texts(0, 0, 1, zlab, adj=1, color=text.col)
    if (ellipsoid) {
        dfn <- 3
        if (is.null(groups)){
            dfd <- length(x) - 1
            radius <- sqrt(dfn * qf(level, dfn, dfd))
            ellips <- ellipsoid(center=c(mean(x), mean(y), mean(z)),
shape=cov(cbind(x,y,z)), radius=radius)
            quads3d(ellips[,1], ellips[,2], ellips[,3], front="lines",
back="lines", alpha=.5, 
                lit=FALSE, col=surface.col[1])
            }
        else{
            levs <- levels(groups)
            for (j in 1:length(levs)){
                group <- levs[j]
                select.obs <- groups == group
                xx <- x[select.obs]
                yy <- y[select.obs]
                zz <- z[select.obs]
                dfd <- length(xx) - 1
                radius <- sqrt(dfn * qf(level, dfn, dfd))
                ellips <- ellipsoid(center=c(mean(xx), mean(yy), mean(zz)),
shape=cov(cbind(xx,yy,zz)), radius=radius)
                quads3d(ellips[,1], ellips[,2], ellips[,3], front="lines",
back="lines", alpha=.5, 
                    lit=FALSE, col=surface.col[j])
                }
            }
        }               
    if (surface){
        vals <- seq(0, 1, length=grid.lines)
        dat <- expand.grid(x=vals, z=vals)
        for (i in 1:length(fit)){
            f <- match.arg(fit[i], c("linear", "quadratic", "smooth",
"additive"))
            if (is.null(groups)){
                mod <- switch(f,
                    linear = lm(y ~ x + z),
                    quadratic = lm(y ~ (x + z)^2 + I(x^2) + I(z^2)),
                    smooth = if (is.null(df.smooth)) gam(y ~ s(x, z))
                        else gam(y ~ s(x, z, fx=TRUE, k=df.smooth)),
                    additive = if (is.null(df.additive)) gam(y ~ s(x) +
s(z))
                        else gam(y ~ s(x, fx=TRUE, k=df.additive[1]+1) +
                            s(z, fx=TRUE, k=(rev(df.additive+1)[1]+1)))
                    )
                if (model.summary) summaries[[f]] <- summary(mod)
                yhat <- matrix(predict(mod, newdata=dat), grid.lines,
grid.lines)
                rgl.surface(vals, vals, yhat, color=surface.col[i],
alpha=0.5, lit=FALSE)
                if(grid) rgl.surface(vals, vals, yhat, color=grid.col,
alpha=0.5, lit=FALSE, front="lines", back="lines")
                if (residuals){
                    n <- length(y)
                    fitted <- fitted(mod)
                    colors <- ifelse(residuals(mod) > 0, pos.res.col,
neg.res.col)
                    rgl.lines(as.vector(rbind(x,x)),
as.vector(rbind(y,fitted)), as.vector(rbind(z,z)),
                        color=as.vector(rbind(colors,colors)))
                    }
                }
            else{
                if (parallel){
                    mod <- switch(f,
                        linear = lm(y ~ x + z + groups),
                        quadratic = lm(y ~ (x + z)^2 + I(x^2) + I(z^2) +
groups),
                        smooth = if (is.null(df.smooth)) gam(y ~ s(x, z) +
groups)
                            else gam(y ~ s(x, z, fx=TRUE, k=df.smooth) +
groups),
                        additive = if (is.null(df.additive)) gam(y ~ s(x) +
s(z) + groups)
                            else gam(y ~ s(x, fx=TRUE, k=df.additive[1]+1) +
                                s(z, fx=TRUE, k=(rev(df.additive+1)[1]+1)) +
groups)
                        )
                    if (model.summary) summaries[[f]] <- summary(mod)
                    levs <- levels(groups)
                    for (j in 1:length(levs)){
                        group <- levs[j]
                        select.obs <- groups == group
                        yhat <- matrix(predict(mod, newdata=cbind(dat,
groups=group)), grid.lines, grid.lines)
                        rgl.surface(vals, vals, yhat, color=surface.col[j],
alpha=0.5, lit=FALSE)
                        if (grid) rgl.surface(vals, vals, yhat,
color=grid.col, alpha=0.5, lit=FALSE, front="lines", back="lines")
                        rgl.texts(0, predict(mod, newdata=data.frame(x=0,
z=0, groups=group)), 0,
                            paste(group, " "), adj=1, color=surface.col[j])
                        if (residuals){
                            yy <- y[select.obs]
                            xx <- x[select.obs]
                            zz <- z[select.obs]
                            fitted <- fitted(mod)[select.obs]
                            rgl.lines(as.vector(rbind(xx,xx)),
as.vector(rbind(yy,fitted)), as.vector(rbind(zz,zz)),
                                col=surface.col[j])
                            }
                        }
                    }
                else {
                    levs <- levels(groups)
                    for (j in 1:length(levs)){
                        group <- levs[j]
                        select.obs <- groups == group
                        mod <- switch(f,
                            linear = lm(y ~ x + z, subset=select.obs),
                            quadratic = lm(y ~ (x + z)^2 + I(x^2) + I(z^2),
subset=select.obs),
                            smooth = if (is.null(df.smooth)) gam(y ~ s(x,
z), subset=select.obs)
                                else gam(y ~ s(x, z, fx=TRUE, k=df.smooth),
subset=select.obs),
                            additive = if (is.null(df.additive)) gam(y ~
s(x) + s(z), subset=select.obs)
                                else gam(y ~ s(x, fx=TRUE,
k=df.additive[1]+1) +
                                    s(z, fx=TRUE,
k=(rev(df.additive+1)[1]+1)), subset=select.obs)
                            )
                        if (model.summary) summaries[[paste(f, ".", group,
sep="")]] <- summary(mod)
                        yhat <- matrix(predict(mod, newdata=dat),
grid.lines, grid.lines)
                        rgl.surface(vals, vals, yhat, color=surface.col[j],
alpha=0.5, lit=FALSE)
                        rgl.surface(vals, vals, yhat, color=grid.col,
alpha=0.5, lit=FALSE, front="lines", back="lines")
                        rgl.texts(0, predict(mod, newdata=data.frame(x=0,
z=0, groups=group)), 0,
                            paste(group, " "), adj=1, color=surface.col[j])
                        if (residuals){
                            yy <- y[select.obs]
                            xx <- x[select.obs]
                            zz <- z[select.obs]
                            fitted <- fitted(mod)
                            rgl.lines(as.vector(rbind(xx,xx)),
as.vector(rbind(yy,fitted)), as.vector(rbind(zz,zz)),
                                col=surface.col[j])
                            }
                        }
                    }
                }
            }
        }
    if (revolutions > 0) {
        for (i in 1:revolutions){
            for (angle in seq(1, 360, length=360/speed))
rgl.viewpoint(-angle, fov=fov)
            }
        }
    if (model.summary) return(summaries) else return(invisible(NULL))
    }

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
> Sent: Friday, September 09, 2005 8:03 AM
> To: Mike White
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Plotting an ellipse in 3D
> 
> Mike White wrote:
> > I have been using the ellipse function from the car package and the 
> > covariance matrix to draw an ellipse around a group of 
> points to show 
> > the confidence limits.  However, the points are actually 
> represented 
> > by 3 variables so rather than plot each pairwise combination of 
> > variables in 2D I would like to plot the 'ellipse' in 3D using the 
> > djmrgl package.  Can anyone offer advice on how I can plot 
> the surface 
> > of  a 3D 'ellipse' using the covariance matrix to define 
> the shape, so 
> > that the points inside can also be seen.
> 
> You should use rgl, rather than djmrgl.  It now has most of 
> the same functions plus a lot more.
> 
> Then you can plot the ellipse as a wireframe or transparent 
> object.  See the demo(regression) example for that kind of 
> drawing; demo(shapes3d) for ellipses.  (The demo names are 
> from memory, I don't have access to it right now.)
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Sat Sep 10 05:14:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Sep 2005 23:14:26 -0400
Subject: [R] PowerPoint graph insertion
In-Reply-To: <200509092340.j89NeYZC013173@hypatia.math.ethz.ch>
References: <200509092340.j89NeYZC013173@hypatia.math.ethz.ch>
Message-ID: <971536df050909201460e70b24@mail.gmail.com>

On 9/9/05, Leif Kirschenbaum <leif at reflectivity.com> wrote:
> Yes:  I have Tufte's monograph on my desk. (along with 4 statistics texts)
> Yes:  I am not the biggest fan of PowerPoint.
> Yes:  I am using R to generate charts, plots, trends, etc. I have to summarize them each week.
> 
>  When I consider how to organize this data my first thought is to generate an HTML file with links to the R-generated plots, which HTML file organizes the plots in the required order.
> However:
>  * Each week we annotate one PowerPoint slide in the weekly presentation with action items -- we don't only use PP as a presentation tool. This is convenient, as then the action items resulting from particular data trends are associated in a single document with the plots of the data trends.
>  * Other (non-R) users insert data into the weekly PP presentation: from other plotting software and images from various sources (microscope, SEM, TEM, etc.), which I cannot easily incorporate into a generated HTML file before-the-fact.
>  * I'm not sure how to create an HTML file which allows one to page forward and backward through it easily, as with PowerPoint (a minor point: and there is probably a way to write HTML to respond to such)
> 
> So:
>  Can R insert plots into an existing PowerPoint presentation?
> (actually, I'd copy last week's presentation and then update with new plots)
> 
>  I'll guess that it cannot, as there probably is not a Microsoft supplied interface (ODBC or otherwise) with PowerPoint as there is with Excel.
> 

You can do it in VBA or you can do it in R using the RDCOMClient or 
rcom packages, either of which provide an interface to Microsoft COM 
objects, in general (these are not specific to any particular application).

I would first do it manually in PP with the macro recorder on so you can
see what VBA code is generated by the recorder.  Then you can
use that as a base for your VBA code or if you like you can translate it 
to R using either of the above mentioned R packages.



From wuming.gong at gmail.com  Sat Sep 10 10:40:45 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Sat, 10 Sep 2005 16:40:45 +0800
Subject: [R] R-help Digest, Vol 31, Issue 9
In-Reply-To: <e99f98a705090915196accb6f2@mail.gmail.com>
References: <mailman.13.1126260002.17027.r-help@stat.math.ethz.ch>
	<e99f98a705090915196accb6f2@mail.gmail.com>
Message-ID: <b428d06d050910014077203f8e@mail.gmail.com>

?summary.lm and check the Value section.

Wuming

On 9/10/05, Ping Yao <sdshlxh at gmail.com> wrote:
> Hi:
> I use lm (linear model) to analyze 47 variables , 8 responses
> So I use loop to finish it .
> I want the program to show the results that P-value is less than 0.05.
> How can I cite the P-valus from lm result ?
> 
> Ping
> 
> The code:
> 
> 
> #using LM to model general fati
> for (j in 48:52) {
> for (i in 3:46){
> gen.fat<-y_x[,j]
> gen.fat<-as.numeric(gen.fat)
> 
> snp_marker<-y_x[,i]
> 
> x<-colnames(y_x)
> 
> #snp_marker<-as.matrix(snp_marker)
> #mode(snp_marker)
> cat("phenotype is = ",x[j] , "\n")
> cat("snp marker is = ",x[i] , "\n")
> 
> zz<-summary(lm.D9 <- lm(gen.fat~snp_marker))
> 
> print(zz)
> 
> return
> }
> }
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tmlammail at yahoo.com  Sat Sep 10 12:04:40 2005
From: tmlammail at yahoo.com (Martin Lam)
Date: Sat, 10 Sep 2005 03:04:40 -0700 (PDT)
Subject: [R] Re-evaluating the tree in the random forest
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED41B@usctmx1106.merck.com>
Message-ID: <20050910100440.65845.qmail@web40525.mail.yahoo.com>

Hi Andy,

Thank you for your help but it was not really a
solution to my problem. Following from your iris
example, if I do "iris.rf$forest$xbestsplit[1,1] <-
1.1" instead of "iris.rf$forest$xbestsplit[1,1] <-
3.5" then the training instances in node 2 (left node
of the root node) aren't correctly split any more,
since there are training instances that have
Petal.Length > 1.1. 

So, I wondered if it was possible that after I've made
a change in a splitpoint that the tree only put
training instances with Petal.Length < 1.1 in node 2
and the others in node 3, from node 3 on the training
instances from node 2 with Petal.Length >= 1.1 are
passed down the tree until they reach the leafs and
finally the classification in the leafs are updated.

Thanks in advance,

Martin


--- "Liaw, Andy" <andy_liaw at merck.com> wrote:

> Here's an example, using the iris data:
> 
> > ## Grow one tree, using all data, and try all
> variables at all splits,
> > ## using large nodesize to get smaller tree.
> > iris.rf <- randomForest(iris[-5], iris[[5]],
> ntree=1, nodesize=20, mtry=4,
> +                         sampsize=150,
> replace=FALSE)
> > getTree(iris.rf, 1)
>    left daughter right daughter split var split
> point status prediction
> 1              2              3         3       
> 2.45      1          0
> 2              0              0         0       
> 0.00     -1          1
> 3              4              5         4       
> 1.75      1          0
> 4              6              7         3       
> 4.95      1          0
> 5              8              9         3       
> 4.85      1          0
> 6             10             11         4       
> 1.65      1          0
> 7              0              0         0       
> 0.00     -1          3
> 8              0              0         0       
> 0.00     -1          3
> 9              0              0         0       
> 0.00     -1          3
> 10             0              0         0       
> 0.00     -1          2
> 11             0              0         0       
> 0.00     -1          3
> > idx <- with(iris, Petal.Length > 2.45 &
> Petal.Length < 3.5)
> > predict(iris.rf, iris[idx, -5])
> [1] versicolor versicolor versicolor
> Levels: setosa versicolor virginica
> > iris.rf$forest$xbestsplit[1,1] <- 3.5
> > predict(iris.rf, iris[newiris, -5])
> [1] setosa setosa setosa
> Levels: setosa versicolor virginica
> 
> Note how the predictions have changed.
> 
> HTH,
> Andy
> 
> > -----Original Message-----
> > From: Martin Lam [mailto:tmlammail at yahoo.com] 
> > Sent: Friday, September 09, 2005 9:04 AM
> > To: Liaw, Andy; r-help at stat.math.ethz.ch
> > Subject: RE: [R] Re-evaluating the tree in the
> random forest
> > 
> > 
> > Hi,
> > 
> > Let me give a simple example, assume a dataset
> > containing 5 instances  with 1 variable and the
> class
> > label:
> > 
> > [x1, y]:
> > [0.5, A]
> > [3.2, B]
> > [4.5, B]
> > [1.4, C]
> > [1.6, C]
> > [1.9, C]
> > 
> > Assume that the randomForest algorithm create this
> (2
> > levels deep) tree:
> > 
> > Root node: question: x1 < 2.2?
> > 
> > Left terminal node:
> > [0.5, A]
> > [1.4, C]
> > [1.6, C]
> > [1.9, C]
> > Leaf classification: C
> > 
> > Right terminal node:
> > [3.2, B]
> > [4.5, B]
> > Leaf classification: B
> > 
> > If I change the question at the root node to "x1 <
> > 1?", the instances in the left leaf node are not
> > correctly passed down the tree anymore.  
> > My original question was if there was a way to
> > re-evaluate the instances again into:
> > 
> > Root node: question: x1 < 1?
> > 
> > Left terminal node:
> > [0.5, A]
> > Leaf classification: A
> > 
> > Right terminal node:
> > [3.2, B]
> > [4.5, B]
> > [1.4, C]
> > [1.6, C]
> > [1.9, C]
> > Leaf classification: C
> > 
> > Cheers,
> > 
> > Martin
> > 
> > --- "Liaw, Andy" <andy_liaw at merck.com> wrote:
> > 
> > > > From: Martin Lam
> > > > 
> > > > Dear mailinglist members,
> > > > 
> > > > I was wondering if there was a way to
> re-evaluate
> > > the
> > > > instances of a tree (in the forest) again
> after I
> > > have
> > > > manually changed a splitpoint (or split
> variable)
> > > of a
> > > > decision node. Here's an illustration:
> > > > 
> > > > library("randomForest")
> > > > 
> > > > forest.rf <- randomForest(formula = Species ~
> .,
> > > data
> > > > = iris, do.trace = TRUE, ntree = 3, mtry = 2,
> > > > norm.votes = FALSE)
> > > > 
> > > > # I am going to change the splitpoint of the
> root
> > > node
> > > > of the first tree to 1
> > > > forest.rf$forest$xbestsplit[1,]
> > > > forest.rf$forest$xbestsplit[1,1] <- 1
> > > > forest.rf$forest$xbestsplit[1,]
> > > > 
> > > > Because I've changed the splitpoint, some
> > > instances in
> > > > the leafs are not supposed where they should
> be.
> > > Is
> > > > there a way to reappoint them to the correct
> leaf?
> > > 
> > > I'm not sure what you want to do exactly, but I
> > > suspect you can use
> > > predict().
> > >  
> > > > I was also wondering how I should interpret
> the
> > > output
> > > > of do.trace:
> > > > 
> > > > ntree      OOB      1      2      3
> > > >     1:   3.70%  0.00%  6.25%  5.88%
> > > >     2:   3.49%  0.00%  3.85%  7.14%
> > > >     3:   3.57%  0.00%  5.56%  5.26%
> > > > 
> > > > What's OOB and what does the percentages mean?
> > > 
> > > OOB stands for `Out-of-bag'.  Read up on random
> > > forests (e.g., the article
> > > in R News) to learn about it.  Those numbers are
> > > estimated error rates.  The
> > > `OOB' column is across all data, while the
> others
> > > are for the classes.
> > > 
> > > Andy
> > > 
> > >  
> > > > Thanks in advance,
> > > > 
> > > > Martin
> > > > 
> > > > 
> > > > 	
> > > > 		
> > > >
> > >
> >
>
______________________________________________________
> > > > Click here to donate to the Hurricane Katrina
> > > relief effort.
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > > 
> > > > 
> > > > 
> > > 
> > > 
> > > 
> > >
> >
>
--------------------------------------------------------------
> > ----------------
> > > Notice:  This e-mail message, together with any
> > > attachments, contains information of Merck &
> Co.,
> > > Inc. (One Merck Drive, Whitehouse Station, New
> > > Jersey, USA 08889), and/or its affiliates (which
> may
> > > be known outside the United States as Merck
> Frosst,
> > > Merck Sharp & Dohme or MSD and in Japan, as
> Banyu)
> > > that may be confidential, proprietary
> copyrighted
> > > and/or legally privileged. It is intended solely
> for
> > > the use of the individual or entity named on
> this
> > > message.  If you are not the intended recipient,
> and
> > > have received this message in error, please
> notify
> > > us immediately by reply e-mail and then delete
> it
> > > from your system.
> > >
> >
>
--------------------------------------------------------------
> > ----------------
> > > 
> > 
> > 
> > 
> > 	
> > 		
> >
>
______________________________________________________
> > Click here to donate to the Hurricane Katrina
> relief effort.
> > http://store.yahoo.com/redcross-donate3/
> > 
> > 
> > 
> 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any
> attachments, contains information of Merck & Co.,
> Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may
> be known outside the United States as Merck Frosst,
> Merck Sharp & Dohme or MSD and in Japan, as Banyu)
> that may be confidential, proprietary copyrighted
> and/or legally privileged. It is intended solely for
> the use of the individual or entity named on this
> message.  If you are not the intended recipient, and
> have received this message in error, please notify
> us immediately by reply e-mail and then delete it
> from your system.
>
------------------------------------------------------------------------------
> 



	
		
______________________________________________________
Click here to donate to the Hurricane Katrina relief effort.



From john.maindonald at anu.edu.au  Sat Sep 10 14:17:01 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sat, 10 Sep 2005 22:17:01 +1000
Subject: [R] Discrepancy between R and SPSS in 2-way,
	repeated measures ANOVA
In-Reply-To: <mailman.12.1126346401.25910.r-help@stat.math.ethz.ch>
References: <mailman.12.1126346401.25910.r-help@stat.math.ethz.ch>
Message-ID: <92A77164-3D8B-4DFA-A592-883DECE0EC63@anu.edu.au>

There are 20 distinct individuals, right? expno breaks the 20
individuals into five groups of 4, right? Is this a blocking factor?

If expno is treated as a blocking factor, the following is what you get:

 > xy <- expand.grid(expno=letters[1:5],cond=letters[1:4],
+                                    time=factor(paste(1:2)))
 > xy$subj <- factor(paste(xy$expno, xy$cond, sep=":"))
 > xy$cond <- factor(xy$cond)
 > xy$expno <- factor(xy$expno)
 > xy$y <- rnorm(40)
 > summary(aov(y~cond*time+Error(expno/cond), data=xy))

Error: expno
           Df Sum Sq Mean Sq F value Pr(>F)
Residuals  4   3.59    0.90

Error: expno:cond
           Df Sum Sq Mean Sq F value Pr(>F)
cond       3   1.06    0.35    0.36   0.78
Residuals 12  11.86    0.99

Error: Within
           Df Sum Sq Mean Sq F value Pr(>F)
time       1   2.27    2.27    1.38   0.26
cond:time  3   3.27    1.09    0.67   0.59
Residuals 16  26.19    1.64


If on the other hand this is analyzed as for a complete
randomized design, the following is the output:

 > summary(aov(y~cond*time+Error(subj), data=xy))

Error: subj
           Df Sum Sq Mean Sq F value Pr(>F)
cond       3   1.06    0.35    0.37   0.78
Residuals 16  15.46    0.97

Error: Within
           Df Sum Sq Mean Sq F value Pr(>F)
time       1   2.27    2.27    1.38   0.26
cond:time  3   3.27    1.09    0.67   0.59
Residuals 16  26.19    1.64



On 10 Sep 2005, at 8:00 PM, Larry A Sonna wrote:

> From: "Larry A Sonna" <larry_sonna at hotmail.com>
> Date: 10 September 2005 12:10:06 AM
> To: <r-help at stat.math.ethz.ch>
> Subject: [R] Discrepancy between R and SPSS in 2-way, repeated  
> measures ANOVA
>
>
> Dear R community,
>
> I am trying to resolve a discrepancy between the way SPSS and R  
> handle 2-way, repeated measures ANOVA.
>
> An experiment was performed in which samples were drawn before and  
> after treatment of four groups of subjects (control and disease  
> states 1, 2 and 3).  Each group contained five subjects.  An  
> experimental measurement was performed on each sample to yield a  
> "signal".  The before and after treatment signals for each subject  
> were treated as repeated measures.  We desire to obtain P values  
> for disease state ("CONDITION"), and the interaction between signal  
> over time and disease state ("CONDITION*TIME").
>
> Using SPSS, the following output was obtained:
>                      DF        SumSq (Type 3)    Mean Sq    F  
> value     P=
>
> COND              3                 42861            14287        
> 3.645 0.0355
>
> TIME                1                     473                
> 473       0.175 0.681
>
> COND*TIME     3                     975               325        
> 0.120 0.947
>
> Error                16                43219             2701
>
>
>
> By contrast, using the following R command:
>
> summary(aov(SIGNAL~(COND+TIME+COND*TIME)+Error(EXPNO/COND),  
> Type="III"))
>
> the output was as follows:
>
>                  Df     Sum Sq     Mean Sq     F value  Pr(>F)
>
> COND          3          26516       8839      3.2517     0.03651 *
>
> TIME            1            473         473      0.1739     0.67986
>
> COND:TIME  3            975         325      0.1195     0.94785
>
> Residuals     28        76107      2718
>
>
>
> I don't understand why the two results are discrepant.  In  
> particular, I'm not sure why R is yielding 28 DF for the residuals  
> whereas SPSS only yields 16.  Can anyone help?
>
>

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From Achim.Zeileis at wu-wien.ac.at  Sat Sep 10 15:59:01 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 10 Sep 2005 15:59:01 +0200 (CEST)
Subject: [R] "Chow Test" for classification and regression trees
In-Reply-To: <D6D1F322F8FCE44C9ED6CFBC405FFB270EDF38@EXCLUSTER.pu.win.princeton.edu>
References: <D6D1F322F8FCE44C9ED6CFBC405FFB270EDF38@EXCLUSTER.pu.win.princeton.edu>
Message-ID: <Pine.LNX.4.58.0509101547220.9175@thorin.ci.tuwien.ac.at>

On Fri, 9 Sep 2005, Charles M Cameron wrote:

> Suppose one estimates a classification or regression tree (CART) for one
> group or one time period; and then estimates a CART for another group or
> time period. Is there a way to test for a structural change or break
> across the two groups or between the two time periods, in other words,
> is there an analogue of a Chow Test for CART? Has anyone ever seen
> anything like this or have any ideas how one could do it? Thanks for any
> suggestions.

A couple of ideas could come to mind here:
  1. Just include the grouping variable (or time variable) as a potential
     explanatory variable into your tree-growing algorithm and then you
     could see whether this is picked up by the tree or not.
  2. If you've got two predictive models grown on different subsets of
     data (sorted by grouping or time) you could try to predict the values
     in the other subset for each model to check whether there are
     structural differences or not. Combining it with bootstrapping (or
     something like that) could give you an inference procedure.
  3. To do some advertising of our work: there is a working paper that
     I've written with Kurt Hornik and Torsten Hothorn about `Model-based
     recursive partitioning' that tries to combine recursive partitioning
     ideas with structural change methods that could be relevant here.
     You could fit one model tree on the whole data and then check whether
     there are instabilities with respect to the time or grouping
     variable. The paper can be obtained from
     http://epub.wu-wien.ac.at/dyn/virlib/wp/showentry?ID=epub-wu-01_86e&back=/

hth,
Z

> Charles Cameron
> Professor of Politics & Public Affairs
> Princeton University
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Sat Sep 10 18:25:12 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 10 Sep 2005 18:25:12 +0200
Subject: [R] less precision, please!
In-Reply-To: <432220EB.5030204@stats.uwo.ca>
References: <E1EDsUn-0003Nb-BN@garm.runbox.com> <432220EB.5030204@stats.uwo.ca>
Message-ID: <17187.2280.225482.62964@stat.math.ethz.ch>

>>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca>
>>>>>     on Fri, 09 Sep 2005 19:55:23 -0400 writes:

    Duncan> On 9/9/2005 7:41 PM, Paul MacManus wrote:
    >> I need to run qbeta on a set of 500K different parameter
    >> pairs (with a fixed quantile). For most pairs qbeta finds
    >> the solution very quickly but for a substantial minority
    >> of the cases qbeta is very slow. This occurs when the
    >> solution is very close to zero. qbeta is getting answers
    >> to a precision of about 16 decimal places. I don't need
    >> that accuracy. Is there any way to set the precision of
    >> R's calculations to, say, 9 decimal places and so speed
    >> up the whole process?
    >> 
    >> I could, of course, avoid this problem by not running
    >> qbeta when I know the solution is going to be
    >> sufficiently small but I'm more interested in ways to
    >> adjust the precision of calculations in R.

    Duncan> There's no general way to do this.  The function
    Duncan> that implements qbeta may have some tuning
    Duncan> parameters (I haven't looked), but they aren't
    Duncan> usually needed, and aren't exposed in R.

Yes.

However, I've had thoughts in the past on possibly providing such
a possibility from both R and C level.  One problem is that
``for symmetry reasons'' you would want to have this ``for all functions'' 
which would need a lot of work, for something that's really not
of too high a need.   
I agree that qbeta() can be particularly "nasty".  I'm open to
more in-depth discussion on this -- after R 2.2.0 is out

    Duncan> If you want a quick approximation, I'd suggest doing
    Duncan> your calculation on a grid of values and using
    Duncan> approx() to interpolate.

yes, or approxfun() {which prefer for its UI},
or even more smoothly  using  spline() or splinefun() {again
preferably the latter}.

One problem may be that these are only for 1-D interpolation and
qbeta() depends on three principal arguments.
Package 'akima' provides somewhat smooth 2-D interpolation.

Martin Maechler



From dmbates at gmail.com  Sat Sep 10 18:58:32 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sat, 10 Sep 2005 11:58:32 -0500
Subject: [R] summary of problem with mCall function.
In-Reply-To: <003d01c5b3d2$e9318d50$a01ad284@BIO041>
References: <003d01c5b3d2$e9318d50$a01ad284@BIO041>
Message-ID: <40e66e0b050910095820dd3b7d@mail.gmail.com>

On 9/7/05, Bill Shipley <bill.shipley at usherbrooke.ca> wrote:
> Last week I posted a question concerning the mCall function, which is
> used to create self-starting functions and is described in the book by
> Pinheiro, J.C. and Bates, D.M. (Mixed-effects models in S and S-PLUS).
> On page 345 one finds the following call:
> 
> 
> 
> xy<-sortedXyData(mCall[["x"]], LHS,data)
> 
> 
> 
> It is necessary to replace the "x" in the call to mCall by the actual
> variable name for the dependent variable.  The error message that I was
> getting was due to the fact that I had called by dependent variable in
> the data frame (data) by another name than x without changing this in
> the call to mCall.

mCall isn't a function.  It's a matched call created by the match.call
function.  The purpose of having such a matched call is to be able to
map the actual arguments to the formal arguments.  You need to know
which names to evaluate in the data frame and you can only get that
information from the matched call.



From tlumley at u.washington.edu  Sat Sep 10 19:16:17 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 10 Sep 2005 10:16:17 -0700 (PDT)
Subject: [R] [R-pkgs] survey: version 3.3
Message-ID: <Pine.LNX.4.63a.0509101007370.19273@homer24.u.washington.edu>


Version 3.3 of "survey" is percolating through CRAN.  Since the last 
announcement on this list, version 2.9, the main additions are calibration 
estimators: linear, bounded linear, raking ratio, bounded raking ratio, 
logit.

Other updates and bug fixes are described in
         http://faculty.washington.edu/tlumley/survey/NEWS


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From dieter.menne at menne-biomed.de  Sat Sep 10 19:27:20 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 10 Sep 2005 17:27:20 +0000 (UTC)
Subject: [R] data manipulation
References: <20050908161733.30565.qmail@web25805.mail.ukl.yahoo.com>
Message-ID: <loom.20050910T192528-660@post.gmane.org>

Marc Bernard <bernarduse1 <at> yahoo.fr> writes:

> I would be grateful if you can help me. My problem is the following:
> I have a data set like:
> 
> ID  time      X1          X2
> 1    1          x111      x211
> 1    2          x112      x212
....
 
> where X1 and X2 are 2 covariates and "time" is the time of observation and ID 
indicates the cluster.
> 
> I want to merge the above data by creating a new variable  "X" and "type" as 
follows:
> 
> ID   time    X            type
> 1     1      x111         X1
....

Try reshape. And have courage, this is one of the more complex interfaces in R, 
very powerful, but intimidating.

Dieter



From janpaulr at yahoo.com  Sat Sep 10 20:10:06 2005
From: janpaulr at yahoo.com (Jan-Paul Roodbol)
Date: Sat, 10 Sep 2005 19:10:06 +0100 (BST)
Subject: [R] Missing
Message-ID: <20050910181006.79300.qmail@web36201.mail.mud.yahoo.com>

Does anyone know if randomForest in R can handle
dataset with missings?

Thank you

Kind regards

Jan-Paul



From dieter.menne at menne-biomed.de  Sat Sep 10 22:23:18 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 10 Sep 2005 20:23:18 +0000 (UTC)
Subject: [R] R API call from delphi
References: <18668981.1126194469884.JavaMail.www@wwinf1615>
Message-ID: <loom.20050910T222142-898@post.gmane.org>

Laurent TESSIER <famille.tessier <at> wanadoo.fr> writes:

> Has anyone tried to call R API from Delphi under windows ? 

I have written a wrapper for Erich Neuwirth's RDCOM.

http://www.menne-biomed.de/download/download.html

Dieter



From deepayan.sarkar at gmail.com  Sat Sep 10 22:54:50 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 10 Sep 2005 15:54:50 -0500
Subject: [R] adding labels to a multiple plot in lattice
In-Reply-To: <9B06A25E-7AEE-4E74-90CD-7DE0610E33F8@usgs.gov>
References: <9B06A25E-7AEE-4E74-90CD-7DE0610E33F8@usgs.gov>
Message-ID: <eb555e6605091013543949eddf@mail.gmail.com>

On 9/9/05, Jorge Ahumada <jahumada at usgs.gov> wrote:
> Hello,
> 
> I am trying to a create a plot in lattice that has four panels, but  
> all of them have the same xlab and ylab, so instead of labeling each  
> plot  separately I want to create one single label for the y series  
> and one for the x series, somehow along the left and bottom margins  
> of the plot. Can anybody help? This is the pseudocode:
> 
> print(xyplot(x1~y1),split=c(1,1,2,2),more=T)
> print(xyplot(x2~y2),split=c(2,1,2,2),more=T)
> print(xyplot(x3~y3),split=c(1,2,2,2),more=T)
> print(xyplot(x4~y4),split=c(2,2,2,2))

The obvious thing to do would be

a <- factor(rep(1:4, c(length(x1), length(x2), length(x3), length(x4))))
xyplot(c(x1, x2, x3, x4) ~ c(y1, y2, y3, y4) | a,
       scales = "free", xlab = "whatever")

Deepayan



From Arkady.Sherman at ksu.ru  Sun Sep 11 00:00:18 2005
From: Arkady.Sherman at ksu.ru (Arkady Sherman)
Date: Sun, 11 Sep 2005 02:00:18 +0400
Subject: [R] Output of warnings inside the source function
Message-ID: <43235772.1010302@ksu.ru>

Hello, all.
There is a problem to get an output of warnings() function to sink in a 
file specified.
There are to files

1. File "test" with content:

source("test_foo",local=T)

2. and file "test_foo" with content:

options(warn = 1)
sink("c:/temp/foo.txt",append=F)
warning("Foo warning")
warnings()
sink()

3. If I run R as

"c:\Program Files\R\rw2011\bin\R.exe" --no-save < test > out.txt

the file "c:/temp/foo.txt" will contain nothing.
But I'd like it should contain the warning message "Foo warning".
Is the behavior a bug of R or there is another way to get it working.
         Thanks in advance, Alex



From jahumada at usgs.gov  Sat Sep 10 23:48:02 2005
From: jahumada at usgs.gov (Jorge Ahumada)
Date: Sat, 10 Sep 2005 16:48:02 -0500
Subject: [R] adding labels to a multiple plot in lattice
In-Reply-To: <eb555e6605091013543949eddf@mail.gmail.com>
References: <9B06A25E-7AEE-4E74-90CD-7DE0610E33F8@usgs.gov>
	<eb555e6605091013543949eddf@mail.gmail.com>
Message-ID: <ADF3A9FC-975E-4E82-A9E8-230AA89ACF52@usgs.gov>

Thanks. Well the pseudocode was a simple illustration but in reality  
each plot is a levelplot with contourplot panels, etc. This is how  
one of them looks:

levelplot(mean~SI+slope,data=sens.ker.new,at=c 
(0.01,0.05,0.1,0.25,0.5,1,2),cont=T,
xlab="",ylab="",main="A", panel=function(...,at,region=T,contour=T) {
panel.levelplot(...,at=at,region=region,contour=contour,labels=list 
(cex=0.8))
panel.xyplot(0.9,0.00554,pch=16,cex=1.5,col='black')
})

Will the principle that you outlined apply to this more complex  
situation? I'll try it out.

thanks..

J.

On Sep 10, 2005, at 3:54 PM, Deepayan Sarkar wrote:

> xyplot(c(x1, x2, x3, x4) ~ c(y1, y2, y3, y4) | a,
>        scales = "free", xlab = "whatever")



From spencer.graves at pdf.com  Sun Sep 11 01:56:38 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Sep 2005 16:56:38 -0700
Subject: [R] Time series ARIMAX and multivariate models
In-Reply-To: <1390.139.133.94.35.1126179199.squirrel@www.abdn.ac.uk>
References: <1390.139.133.94.35.1126179199.squirrel@www.abdn.ac.uk>
Message-ID: <432372B6.3040903@pdf.com>

	  If you have not received an adequate reply to this and would still 
like help, please submit a more specific question, preferably with a 
small self-contained example that someone can copy from your email, 
paste into R, and try some alternatives in a minute or two.  R has many 
capabililities for time series.  A more specific question might produce 
more useful replies.  The posting guide 
(http://www.R-project.org/posting-guide.html) might help increase the 
chances that someone else will provide the help you seek.

	  Good Luck,  	
	  Spencer Graves

nhy303 at abdn.ac.uk wrote:

> Dear List,
> 
> The purpose of this e-mail is to ask about R time series procedures - as a
> biologist with only basic time series knowledge and about a year's
> experience in R.
> 
> I have been using ARIMAX models with seasonal components on seasonal data.
>  However I am now moving on to annual data (with only 34 time points) and
> understand that ARIMA is not suitable for these shorter time periods -
> does R have other, more robust, methods?
> 
> I have tried looking through the R help pages & documentation for packages
> but am unsure what model type is suitable.
> 
> Secondly, I wish to start building multivariate time series models in R to
> look at how fish condition (for several sizes of fish) is affected by
> environmental factors and numbers of prey.  It would be great if someone
> could suggest what R packages/documentation would be useful to research?
> 
> Thankyou,
> 
> Lillian.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From joseclaudio.faria at terra.com.br  Sun Sep 11 02:24:24 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Sat, 10 Sep 2005 21:24:24 -0300
Subject: [R] help with one matrix
Message-ID: <43237938.7050007@terra.com.br>

Dear R-list,

Could anybody tell me how to make one matrix as the below:

      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    -    2    3    4    5    6
[2,]    2    -    2    3    4    5
[3,]    3    2    -    2    3    4
[4,]    4    3    2    -    2    3
[5,]    5    4    3    2    -    2
[6,]    6    5    4    3    2    -

Thanks in advance,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From kerbo2004 at yahoo.com  Sun Sep 11 02:42:29 2005
From: kerbo2004 at yahoo.com (A Das)
Date: Sat, 10 Sep 2005 17:42:29 -0700 (PDT)
Subject: [R] random effects plot
Message-ID: <20050911004230.8492.qmail@web35606.mail.mud.yahoo.com>

Hi, does anyone know how to do random effects plots
for multi-level models? 
"plot(ranef(model))" doesn't seem to work in the
latest release. Here's the error message:
Error in table(x) : argument "x" is missing, with no
default
In addition: Warning message:
The 'formula' argument has been renamed to 'x'. See
?xyplot in: dotplot(formula = .groups ~ .pars |
.enames, data = list(.pars = c(-0.0388163041843026,  

Help a poor sociologist out... 
                          -Bobby



From spencer.graves at pdf.com  Sun Sep 11 02:43:02 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Sep 2005 17:43:02 -0700
Subject: [R] Multinomial Logit and p-values
In-Reply-To: <200509081723.j88HN2Wo000793@phaenicia.ucdavis.edu>
References: <200509081723.j88HN2Wo000793@phaenicia.ucdavis.edu>
Message-ID: <43237D96.9070608@pdf.com>

	  What have you tried?  'RSiteSearch("multinomial logit")' produced 120 
hits for me just now.  'help.search("multinomial logit")' suggests the 
presence of something related to multinomial logits in packages bayesm, 
MCMCpack, and nnet.  If you are more specific, you might get a better 
response quicker.  I believe that many people have found that the 
posting guide! (http://www.R-project.org/posting-guide.html) was quite 
useful for helping them prepare a question that produced more 
informative responses quicker.

	  Best Wishes,
	  spencer graves	

Sangick Jeon wrote:

> Hi,
> 
> I am trying to obtain p-values for coefficient estimates in a multinomial
> logit model.  Although I am able to test for significance using other
> methods (e.g., Wald statistics), I can't seem to get R to give me simple
> p-values. I am sure there is a very simple solution to this, but the R
> archives seem to have nothing on this issue. I would appreciate any help. 
> Thanks in advance!
> 
> Best,
> Sangick Jeon
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From bitwrit at ozemail.com.au  Sun Sep 11 13:08:43 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Sun, 11 Sep 2005 11:08:43 +0000
Subject: [R] help with one matrix
Message-ID: <4324103B.80801@ozemail.com.au>

Is this what you want?

make.odd.matrix<-function(matsize) {
  newmat<-matrix(0,nrow=matsize,ncol=matsize)
  for(i in 1:matsize) {
   for(j in 1:matsize)
    newmat[i,j]<-ifelse(i==j,NA,abs(i-j)+1)
  }
  return(newmat)
}

Jim



From wuming.gong at gmail.com  Sun Sep 11 04:06:17 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Sun, 11 Sep 2005 10:06:17 +0800
Subject: [R] R-help Digest, Vol 31, Issue 9
In-Reply-To: <e99f98a705091011134c94fe15@mail.gmail.com>
References: <mailman.13.1126260002.17027.r-help@stat.math.ethz.ch>
	<e99f98a705090915196accb6f2@mail.gmail.com>
	<b428d06d050910014077203f8e@mail.gmail.com>
	<e99f98a705091011134c94fe15@mail.gmail.com>
Message-ID: <b428d06d05091019062bb4d8ba@mail.gmail.com>

Hi Ping, 

You can use zz$coefficients[,4] to get the p values for each estimated
coefficients in your context.

Wuming 

On 9/11/05, Ping Yao <sdshlxh at gmail.com> wrote:
> Wuming:
>             Thanks for your help.
>            I use the fuction:
>    call("fstatistic",zz)
>   call("p-value",zz)
>  
>  I can get each variable P-values,but I can't  get P-value of the model.
>  How can I do ?
>           
>        one of the results is following :
>  
>  Call:
>  lm(formula = gen.fat ~ snp_marker)
>  
>  Residuals:
>       Min       1Q   Median       3Q      Max 
>  -10.5455  -3.0481   0.4545   3.9519   6.9519 
>  
>  Coefficients:
>                    Estimate Std. Error t value Pr(>|t|)    
>  (Intercept)        13.0481     0.4518  28.881   <2e-16 ***
>  snp_markerallele2   0.5107     0.9102   0.561   0.5753    
>  snp_markerBoth      1.4974     0.6927   2.162   0.0318 *  
>  ---
>  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>  
>  Residual standard error: 4.607 on 212 degrees of freedom
>  Multiple R-Squared: 0.02166,    Adjusted R-squared: 0.01244 
>  F-statistic: 2.347 on 2 and 212 DF,  p-value: 0.0981 
>  
>  I use the code :
>  
>  zz<-summary(lm.D9 <- lm(gen.fat~snp_marker))
>    coe<-coef(lm.D9)# the bare coefficients
>  if (coe[2]<=.05||coe[3]<=.05||coe[4]<=.05||coe[5]<=.05) {
>  cat("phenotype is  = ",x[j] , "\n")
>  cat("snp marker is  = ",x[i] , "\n")
>     sign<-call("fstatistic",zz)
>   call("p-value",zz)
>     
>    #print(coe)
>    print(zz)
> 
>  }
>  
>  
>  
>  
>  
> 
> On 9/10/05, Wuming Gong <wuming.gong at gmail.com> wrote:
> > ?summary.lm and check the Value section.
> > 
> > Wuming
> > 
> > On 9/10/05, Ping Yao <sdshlxh at gmail.com> wrote:
> > > Hi:
> > > I use lm (linear model) to analyze 47 variables , 8 responses 
> > > So I use loop to finish it .
> > > I want the program to show the results that P-value is less than 0.05.
> > > How can I cite the P-valus from lm result ?
> > >
> > > Ping
> > >
> > > The code:
> > > 
> > >
> > > #using LM to model general fati
> > > for (j in 48:52) {
> > > for (i in 3:46){
> > > gen.fat<-y_x[,j]
> > > gen.fat<-as.numeric(gen.fat)
> > >
> > > snp_marker<-y_x[,i]
> > >
> > > x<-colnames(y_x) 
> > >
> > > #snp_marker<-as.matrix(snp_marker)
> > > #mode(snp_marker)
> > > cat("phenotype is = ",x[j] , "\n")
> > > cat("snp marker is = ",x[i] , "\n")
> > >
> > > zz<-summary( lm.D9 <- lm(gen.fat~snp_marker))
> > >
> > > print(zz)
> > >
> > > return
> > > }
> > > }
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________ 
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > >
> > 
> 
>



From deepayan.sarkar at gmail.com  Sun Sep 11 04:43:07 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 10 Sep 2005 21:43:07 -0500
Subject: [R] random effects plot
In-Reply-To: <20050911004230.8492.qmail@web35606.mail.mud.yahoo.com>
References: <20050911004230.8492.qmail@web35606.mail.mud.yahoo.com>
Message-ID: <eb555e6605091019435fce2468@mail.gmail.com>

On 9/10/05, A Das <kerbo2004 at yahoo.com> wrote:
> Hi, does anyone know how to do random effects plots
> for multi-level models? 
> "plot(ranef(model))" doesn't seem to work in the
> latest release. Here's the error message:
> Error in table(x) : argument "x" is missing, with no
> default
> In addition: Warning message:
> The 'formula' argument has been renamed to 'x'. See
> ?xyplot in: dotplot(formula = .groups ~ .pars |
> .enames, data = list(.pars = c(-0.0388163041843026,  

That's a bad bug that got through. Try updating again once the mirrors
catch up (you want lattice != 0.12-6).

Deepayan



From deepayan.sarkar at gmail.com  Sun Sep 11 04:53:33 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 10 Sep 2005 21:53:33 -0500
Subject: [R] adding labels to a multiple plot in lattice
In-Reply-To: <ADF3A9FC-975E-4E82-A9E8-230AA89ACF52@usgs.gov>
References: <9B06A25E-7AEE-4E74-90CD-7DE0610E33F8@usgs.gov>
	<eb555e6605091013543949eddf@mail.gmail.com>
	<ADF3A9FC-975E-4E82-A9E8-230AA89ACF52@usgs.gov>
Message-ID: <eb555e660509101953593b7506@mail.gmail.com>

On 9/10/05, Jorge Ahumada <jahumada at usgs.gov> wrote:
> Thanks. Well the pseudocode was a simple illustration but in reality  
> each plot is a levelplot with contourplot panels, etc. This is how  
> one of them looks:
> 
> levelplot(mean~SI+slope,data=sens.ker.new,at=c 
> (0.01,0.05,0.1,0.25,0.5,1,2),cont=T,
> xlab="",ylab="",main="A", panel=function(...,at,region=T,contour=T) {
> panel.levelplot(...,at=at,region=region,contour=contour,labels=list 
> (cex=0.8))
> panel.xyplot(0.9,0.00554,pch=16,cex=1.5,col='black')
> })
> 
> Will the principle that you outlined apply to this more complex  
> situation? I'll try it out.

If it doesn't, this  might help:

https://stat.ethz.ch/pipermail/r-help/2005-March/066012.html

Deepayan



From spencer.graves at pdf.com  Sun Sep 11 04:55:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Sep 2005 19:55:08 -0700
Subject: [R] test for exponential,lognormal and gammadistribution
In-Reply-To: <1126274609.43219631dccdc@www.cx.unibe.ch>
References: <1126274609.43219631dccdc@www.cx.unibe.ch>
Message-ID: <43239C8C.5080901@pdf.com>

	  I just got 73 hits for 'RSiteSearch("test for exponential 
distribution")'.  In skimming quickly the first 20, the one that seemed 
most relevant to me was a reply I wrote to a similar question two months 
ago (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/37055.html).  Of 
course, some of the remaining 72 may help you more.

	  spencer graves

riedwyl at giub.unibe.ch wrote:

> hello!
> i don't want to test my sample data for normality, but exponential- lognormal- 
> or gammadistribution.
> as i've learnt the anderson-darling-test in R is only for normality and i am 
> not supposed to use the kolmogorov-smirnov test of R for parameter estimates 
> from sample data, is that true?
> can you help me, how to do this anyway!
> thank you very much!
> nadja
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Sep 11 05:05:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Sep 2005 20:05:08 -0700
Subject: [R] GlmmPQL help
In-Reply-To: <49D2E484453227FDAB3D0228@bio-ahustonpost.bio.bris.ac.uk>
References: <49D2E484453227FDAB3D0228@bio-ahustonpost.bio.bris.ac.uk>
Message-ID: <43239EE4.4030803@pdf.com>

	  Have you received a reply to this question?  If you still want an 
answer, please submit a much simpler, self-contained example that 
someone can copy from your email into R, examine possibly with other 
tools and offer comments in a minute or two.  Please include the output, 
labeling clearly the degrees of freedom in question, what you think the 
number should be and why.  I believe that will increase the chances that 
you will get a useful reply quickly.

	  I'm sorry I couldn't help more.
	  spencer graves

Mark Steer wrote:

> Hi,
> 
> I'm running a GLMM on binomial choice data.  The outputs I receive are 
> sensible except for the degrees of freedom, which come out much larger than 
> expected.  Can anyone advise please?
> 
> Exptl design:
> Response = Choice
> Fixed Factors = Position, Treatment and Sex
> Random Factor = ID nested within Treatment and Sex
> Covariate = Delay
> 
> The model:
> glmmPQL(FreeChoice ~ Position * Treatment + Sex + Delay, random = list(~Sex 
> + Treatment|Name), family = binomial)
> 
> Thanks, Mark
> 
> ------------------------------------------------------------------------
> 
> Mark Steer
> School of Biological Sciences
> University of Bristol
> Woodland Road
> Bristol, BS8 1UG
> 
> tel - ++44 (0)117 9545945 (int. 45945)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Sep 11 05:17:42 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Sep 2005 20:17:42 -0700
Subject: [R] Simulate phi-coefficient
In-Reply-To: <FADCFAA8BA80C748890C1D3893C198D98A7844@amedmlmhah01.eur.amed.ds.army.mil>
References: <FADCFAA8BA80C748890C1D3893C198D98A7844@amedmlmhah01.eur.amed.ds.army.mil>
Message-ID: <4323A1D6.10600@pdf.com>

	  Please pardon my ignorance, but what's a "phi coefficient" in this 
context?  RSiteSearch("phi coefficient") produced 36 hits that seemed to 
discuss issues unrelated to your question.
	
	  If it refers to correlation, have you considered something like the 
following:


VECTOR1<-rep(c(1,0),c(15,10))
n <- length(VECTOR1)
library(MASS)

Sig <- array(c(1,.5, .5, 1), dim=c(2,2))
z <- mvrnorm(n, mu=c(0,0), Sigma=Sig)

p <- mean(VECTOR1)
q. <- qnorm(p)

Z <- 0+(z<q.)

	  hope this helps.
	  spencer graves

Bliese, Paul D LTC USAMH wrote:

> Looking for help with the following problem.
> 
>  
> 
> Given a sample of zeros and ones, for example:
> 
>  
> 
> 
>>VECTOR1<-rep(c(1,0),c(15,10))
> 
> 
>>VECTOR1
> 
> 
>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
> 
>  
> 
> How would I create a new sample (VECTOR2) also containing zeros and
> ones, in which the phi-coefficient between the two sample vectors was
> drawn from a population with a known phi-coefficient value?
> 
>  
> 
> Basically, I have a vector of zeros and ones and want to simulate
> another vector such that the two vectors have a known phi-coefficient.
> 
>  
> 
> I know there are ways to do this with normally distributed numbers (for
> example the mvrnorm function in MASS), but am stumped when dealing with
> dichotomous variables.
> 
>  
> 
> Appreciate any thoughts.
> 
>  
> 
> Paul
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Sun Sep 11 08:23:19 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Sep 2005 02:23:19 -0400
Subject: [R] help with one matrix
In-Reply-To: <43237938.7050007@terra.com.br>
References: <43237938.7050007@terra.com.br>
Message-ID: <971536df05091023233c55c2d7@mail.gmail.com>

On 9/10/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> Dear R-list,
> 
> Could anybody tell me how to make one matrix as the below:
> 
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    -    2    3    4    5    6
> [2,]    2    -    2    3    4    5
> [3,]    3    2    -    2    3    4
> [4,]    4    3    2    -    2    3
> [5,]    5    4    3    2    -    2
> [6,]    6    5    4    3    2    -
> 

Assuming that - means NA 

dd <- diag(NA, 6)
abs(col(dd) - row(dd)) + 1 + dd

or

abs(outer(1:6, 1:6, "-")) + 1 + diag(NA,6)

or

f <- function(x,y) ifelse(x==y, NA, abs(x-y)+1)
outer(1:6, 1:6, f)



From joseclaudio.faria at terra.com.br  Sun Sep 11 12:13:49 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Sun, 11 Sep 2005 07:13:49 -0300
Subject: [R] help with one matrix
In-Reply-To: <971536df05091023233c55c2d7@mail.gmail.com>
References: <43237938.7050007@terra.com.br>
	<971536df05091023233c55c2d7@mail.gmail.com>
Message-ID: <4324035D.7080402@terra.com.br>

Gabor Grothendieck wrote:

> On 9/10/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> 
>>Dear R-list,
>>
>>Could anybody tell me how to make one matrix as the below:
>>
>>     [,1] [,2] [,3] [,4] [,5] [,6]
>>[1,]    -    2    3    4    5    6
>>[2,]    2    -    2    3    4    5
>>[3,]    3    2    -    2    3    4
>>[4,]    4    3    2    -    2    3
>>[5,]    5    4    3    2    -    2
>>[6,]    6    5    4    3    2    -
>>
> 
> 
> Assuming that - means NA 
> 
> dd <- diag(NA, 6)
> abs(col(dd) - row(dd)) + 1 + dd
> 
> or
> 
> abs(outer(1:6, 1:6, "-")) + 1 + diag(NA,6)
> 
> or
> 
> f <- function(x,y) ifelse(x==y, NA, abs(x-y)+1)
> outer(1:6, 1:6, f)

Hi,

You are always solving (and teaching) my R doubts: thanks Gabor, very much!
Because I need one, I've been trying to make a more flexible function for 
multiple comparison test of means (Tukey, SNK and Duncan). The matrix above is 
necessary for SNK and Duncan tests. So, when running I will to sent it for you 
for suggestions.

Best,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From ligges at statistik.uni-dortmund.de  Sun Sep 11 12:43:54 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 11 Sep 2005 12:43:54 +0200
Subject: [R] [handling] Missing [values in randomForest]
In-Reply-To: <20050910181006.79300.qmail@web36201.mail.mud.yahoo.com>
References: <20050910181006.79300.qmail@web36201.mail.mud.yahoo.com>
Message-ID: <43240A6A.4030906@statistik.uni-dortmund.de>

Jan-Paul Roodbol wrote:

> Does anyone know if randomForest in R can handle
> dataset with missings?

See ?randomForest, you can omit observations including NAs by specifying 
na.action=na.omit

Please do not cross-post!
Please specify a sensible subject!

Uwe Ligges


> Thank you
> 
> Kind regards
> 
> Jan-Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun Sep 11 12:45:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 11 Sep 2005 12:45:36 +0200
Subject: [R] Output of warnings inside the source function
In-Reply-To: <43235772.1010302@ksu.ru>
References: <43235772.1010302@ksu.ru>
Message-ID: <43240AD0.7010905@statistik.uni-dortmund.de>

Arkady Sherman wrote:

> Hello, all.
> There is a problem to get an output of warnings() function to sink in a 
> file specified.
> There are to files
> 
> 1. File "test" with content:
> 
> source("test_foo",local=T)
> 
> 2. and file "test_foo" with content:
> 
> options(warn = 1)
> sink("c:/temp/foo.txt",append=F)
> warning("Foo warning")
> warnings()
> sink()
> 
> 3. If I run R as
> 
> "c:\Program Files\R\rw2011\bin\R.exe" --no-save < test > out.txt
> 
> the file "c:/temp/foo.txt" will contain nothing.
> But I'd like it should contain the warning message "Foo warning".
> Is the behavior a bug of R or there is another way to get it working.

See ?sink how to handle messages such as warnings.

Uwe Ligges


>          Thanks in advance, Alex
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bernarduse1 at yahoo.fr  Sun Sep 11 14:16:21 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Sun, 11 Sep 2005 14:16:21 +0200 (CEST)
Subject: [R] Create New variable
Message-ID: <20050911121621.36232.qmail@web25807.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050911/56f7a322/attachment.pl

From p.dalgaard at biostat.ku.dk  Sun Sep 11 15:25:02 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Sep 2005 15:25:02 +0200
Subject: [R] Create New variable
In-Reply-To: <20050911121621.36232.qmail@web25807.mail.ukl.yahoo.com>
References: <20050911121621.36232.qmail@web25807.mail.ukl.yahoo.com>
Message-ID: <x2mzmjenyp.fsf@turmalin.kubism.ku.dk>

Marc Bernard <bernarduse1 at yahoo.fr> writes:

> Dear All,
>  
> I want to create one numeric variable from a factor one. here is a simple example:
>  
> x1      x2     factor
> x11    x21    f1
> x12    x22    f1
> x13    x23    f2
> x14    x24    f3
> .        .         .
> .        .         .
>  
> Suppose that the variable factor has 3 levels (f1,f2,f3). I want to
> add to this data frame a numeric variable x4 in (v1,v2,v3) such that
> x4=v1 if factor = f1, x4 = v2 if factor = f2 and x4 = v3 if factor =
> f3.

Just use the factor as an index:

   x4 <- c(v1, v2, v3)[factor]

-- 

   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ggrothendieck at gmail.com  Sun Sep 11 15:49:24 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Sep 2005 09:49:24 -0400
Subject: [R] help with one matrix
In-Reply-To: <4324035D.7080402@terra.com.br>
References: <43237938.7050007@terra.com.br>
	<971536df05091023233c55c2d7@mail.gmail.com>
	<4324035D.7080402@terra.com.br>
Message-ID: <971536df0509110649780941ff@mail.gmail.com>

On 9/11/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> Gabor Grothendieck wrote:
> 
> > On 9/10/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> >
> >>Dear R-list,
> >>
> >>Could anybody tell me how to make one matrix as the below:
> >>
> >>     [,1] [,2] [,3] [,4] [,5] [,6]
> >>[1,]    -    2    3    4    5    6
> >>[2,]    2    -    2    3    4    5
> >>[3,]    3    2    -    2    3    4
> >>[4,]    4    3    2    -    2    3
> >>[5,]    5    4    3    2    -    2
> >>[6,]    6    5    4    3    2    -
> >>
> >
> >
> > Assuming that - means NA
> >
> > dd <- diag(NA, 6)
> > abs(col(dd) - row(dd)) + 1 + dd
> >
> > or
> >
> > abs(outer(1:6, 1:6, "-")) + 1 + diag(NA,6)
> >
> > or
> >
> > f <- function(x,y) ifelse(x==y, NA, abs(x-y)+1)
> > outer(1:6, 1:6, f)
> 
> Hi,
> 
> You are always solving (and teaching) my R doubts: thanks Gabor, very much!
> Because I need one, I've been trying to make a more flexible function for
> multiple comparison test of means (Tukey, SNK and Duncan). The matrix above is
> necessary for SNK and Duncan tests. So, when running I will to sent it for you
> for suggestions.
> 

Note that there already exists a TukeyHSD function and you might
want to do an RSiteSearch for the others to see what
is available.



From jporzak at gmail.com  Sun Sep 11 18:50:07 2005
From: jporzak at gmail.com (Jim Porzak)
Date: Sun, 11 Sep 2005 09:50:07 -0700
Subject: [R] [handling] Missing [values in randomForest]
In-Reply-To: <43240A6A.4030906@statistik.uni-dortmund.de>
References: <20050910181006.79300.qmail@web36201.mail.mud.yahoo.com>
	<43240A6A.4030906@statistik.uni-dortmund.de>
Message-ID: <2a9c000c0509110950240d100d@mail.gmail.com>

On 9/11/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Jan-Paul Roodbol wrote:
> 
> > Does anyone know if randomForest in R can handle
> > dataset with missings?
> 
> See ?randomForest, you can omit observations including NAs by specifying
> na.action=na.omit

Uwe, 
While strictly true, this tells randomForest to ignore any rows with
one or more NAs in the predictor variables.

Since, randomForest is often used for problems with a lot of
(canidate) predictors, na.omit can result in a lot of rows being
discarded. Hence, my reply to Jan-Paul's original posting suggesting
the impute functions in randomForest.

JIm Porzak

> Please do not cross-post!
> Please specify a sensible subject!
> 
> Uwe Ligges
> 
> 
> > Thank you
> >
> > Kind regards
> >
> > Jan-Paul
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Arkady.Sherman at ksu.ru  Sun Sep 11 21:36:59 2005
From: Arkady.Sherman at ksu.ru (Arkady Sherman)
Date: Sun, 11 Sep 2005 23:36:59 +0400
Subject: [R] Output of warnings inside the source function
In-Reply-To: <43240AD0.7010905@statistik.uni-dortmund.de>
References: <43235772.1010302@ksu.ru>
	<43240AD0.7010905@statistik.uni-dortmund.de>
Message-ID: <4324875B.7000106@ksu.ru>

Uwe Ligges wrote:
> Arkady Sherman wrote:
> 
>> Hello, all.
>> There is a problem to get an output of warnings() function to sink in 
>> a file specified.
>> There are to files
>>
>> 1. File "test" with content:
>>
>> source("test_foo",local=T)
>>
>> 2. and file "test_foo" with content:
>>
>> options(warn = 1)
>> sink("c:/temp/foo.txt",append=F)
>> warning("Foo warning")
>> warnings()
>> sink()
>>
>> 3. If I run R as
>>
>> "c:\Program Files\R\rw2011\bin\R.exe" --no-save < test > out.txt
>>
>> the file "c:/temp/foo.txt" will contain nothing.
>> But I'd like it should contain the warning message "Foo warning".
>> Is the behavior a bug of R or there is another way to get it working.
> 
> See ?sink how to handle messages such as warnings.
It's clear I could use sink(f, type="message"), but if I run "c:\Program 
Files\R\rw2011\bin\R.exe" --no-save < test_foo > out.txt instead, the 
warning is written to foo.txt without sinking with type="message". Also 
the problem is that in the later case the warnings will appear in the 
output file in places when just produced, but I need them to appear when 
I call warnings().

The problem (and question) is that the last.warning variable has not 
been set after the warning call (even when options(warn = 1) is defined) 
and warnings() can't output it, so - is that a right behavior?



From Gregor.Gorjanc at bfro.uni-lj.si  Sun Sep 11 22:25:23 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sun, 11 Sep 2005 22:25:23 +0200
Subject: [R] Backtransforming regression coefficient for scaled covariate
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31A47@pollux.bfro.uni-lj.si>

Hello!

Scaling i.e. (x - mean(x)) / sd(x) of covariates in the model 
can improve the efficiency of estimation. That is nice, but 
sometimes one needs to report estimates for original scale. I
was able to backtransform estimates of linear regression quite
easily but I stumped on higher polynomials. Is there a general
rule that I am not aware of or is my algebra so bad?

I appologize for not pure R question but I hope others will also
benefit. I attached the R code for example bellow.

## --- Generate data for linear regression ---
e <- rnorm(n = 100, sd = 10)
x <- rnorm(n = 100, mean = 100, sd = 10)
b <- 3
mu <- 2
y <- mu + b * x + e
plot(y = y, x = x)

## Fit linear regression
(lm1 <- lm(y ~ x))

## Fit linear regression with transformed i.e. standardized covariate
(lm2 <- lm(y ~ scale(x)))

## Backtransform estimate of regression coefficient
coef(lm2)[2] / sd(x)

## --- Generate data for quadratic regression ---
e <- rnorm(n = 100, sd = 10)
x <- runif(n = 100, min = 1, max = 100)
b1 <- 2
b2 <- -0.01
mu <- 2
y <- mu + b1 * x + b2 * x^2 + e
plot(y = y, x = x)

## Fit regression
(lm1 <- lm(y ~ x + I(x^2)))

## Fit regression with transformed i.e. standardized covariate
(lm2 <- lm(y ~ scale(x) + I(scale(x)^2)))

## Backtransform estimates of regression coefficients
## ??

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From tlumley at u.washington.edu  Sun Sep 11 23:39:06 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 11 Sep 2005 14:39:06 -0700 (PDT)
Subject: [R] Output of warnings inside the source function
In-Reply-To: <43235772.1010302@ksu.ru>
References: <43235772.1010302@ksu.ru>
Message-ID: <Pine.LNX.4.63a.0509111437190.31806@homer23.u.washington.edu>

On Sun, 11 Sep 2005, Arkady Sherman wrote:
>
> "c:\Program Files\R\rw2011\bin\R.exe" --no-save < test > out.txt
>
> the file "c:/temp/foo.txt" will contain nothing.
> But I'd like it should contain the warning message "Foo warning".
> Is the behavior a bug of R or there is another way to get it working.

The help page for sink() documents the behaviour you are seeing, so it 
isn't a bug. The help page also describes how to do what you want, and 
even gives an example.

 	-thomas



From David.Duffy at qimr.edu.au  Mon Sep 12 02:34:20 2005
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Mon, 12 Sep 2005 10:34:20 +1000 (EST)
Subject: [R]  Simulate phi-coefficient
In-Reply-To: <mailman.13.1126346401.25910.r-help@stat.math.ethz.ch>
References: <mailman.13.1126346401.25910.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0509121006190.6052@orpheus.qimr.edu.au>

> From: "Bliese, Paul D LTC USAMH" <paul.bliese at us.army.mil>
>
> Given a sample of zeros and ones, for example:
> > VECTOR1<-rep(c(1,0),c(15,10))
> How would I create a new sample (VECTOR2) also containing zeros and
> ones, in which the phi-coefficient between the two sample vectors was
> drawn from a population with a known phi-coefficient value?
>
> I know there are ways to do this with normally distributed numbers (for
> example the mvrnorm function in MASS), but am stumped when dealing with
> dichotomous variables.
>
> Paul

One way is to sample from the 2x2 table with the specified means and pearson
correlation (phi):

for a fourfold table, a b
                      c d
with marginal proportions p1 and p2
cov <- phi * sqrt(p1*(1-p1)*p2*(1-p2))
a <- p1*p2 + cov
b <- p1*(1-p2) - cov
c <- (1-p1)*p2 - cov
d <- (1-p1)*(1-p2) + cov
expand.grid(0:1,0:1)[sample(1:4, size=25, replace=TRUE, prob=c(a,b,c,d)),]

David.

| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From sun at cae.wisc.edu  Mon Sep 12 03:12:00 2005
From: sun at cae.wisc.edu (Hongyu Sun)
Date: Sun, 11 Sep 2005 20:12:00 -0500
Subject: [R] how to do multiple comparisons in R?
References: <20050624085859.4d2c5ce0@localhost.localdomain>
	<Pine.LNX.4.61.0506240733200.25946@gannet.stats>
Message-ID: <013401c5b736$fd49db00$d3c16880@DOS2333>

Hi, Sorry I have to bother a question.

Does R have the functions to do lsd, tukey, bonferonni, contrast etc. like 
in SAS?

Many thanks,

HS



From ramasamy at cancer.org.uk  Mon Sep 12 03:37:56 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 12 Sep 2005 02:37:56 +0100
Subject: [R] how to do multiple comparisons in R?
In-Reply-To: <013401c5b736$fd49db00$d3c16880@DOS2333>
References: <20050624085859.4d2c5ce0@localhost.localdomain>
	<Pine.LNX.4.61.0506240733200.25946@gannet.stats>
	<013401c5b736$fd49db00$d3c16880@DOS2333>
Message-ID: <1126489076.30576.15.camel@dhcp-123.wolf.ox.ac.uk>

Certainly yes and more. 

Try checking under http://finzi.psych.upenn.edu/nmz.html and 
http://cran.r-project.org/manuals.html

Regards, Adai


On Sun, 2005-09-11 at 20:12 -0500, Hongyu Sun wrote:
> Hi, Sorry I have to bother a question.
> 
> Does R have the functions to do lsd, tukey, bonferonni, contrast etc. like 
> in SAS?
> 
> Many thanks,
> 
> HS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From blomsp at ozemail.com.au  Mon Sep 12 03:40:26 2005
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Mon, 12 Sep 2005 11:40:26 +1000
Subject: [R] how to do multiple comparisons in R?
In-Reply-To: <013401c5b736$fd49db00$d3c16880@DOS2333>
References: <20050624085859.4d2c5ce0@localhost.localdomain>
	<Pine.LNX.4.61.0506240733200.25946@gannet.stats>
	<013401c5b736$fd49db00$d3c16880@DOS2333>
Message-ID: <6.2.1.2.0.20050912113238.01cee458@mail.ozemail.com.au>

R has various methods for multiple comparison procedures. See package 
multcomp, or ?TukeyHSD or ?pairwise.t.test for example. An 
RSiteSearch("multiple comparison") returned 187 results. A priori contrasts 
can be constructed using the make.contrasts function in the gmodels 
package, for example. We try not to do anything like in SAS.

Simon.

At 11:12 AM 12/09/2005, Hongyu Sun wrote:
>Hi, Sorry I have to bother a question.
>
>Does R have the functions to do lsd, tukey, bonferonni, contrast etc. like
>in SAS?
>
>Many thanks,
>
>HS
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From joseclaudio.faria at terra.com.br  Mon Sep 12 05:27:36 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Mon, 12 Sep 2005 00:27:36 -0300
Subject: [R] Help with a more flexible funtion for multiple comparision of
	means
Message-ID: <4324F5A8.6080704@terra.com.br>

Dear R-list,

Could anybody tell me (or give me a tip) of how to implement the Duncan 
distribution in R?

I've been trying to make a new and more flexible function for multiple 
comparison of means: Tukey, SNK and Duncan, from 'aov' objects, like TukeyHSD 
function.

For while, it is running nice (Tukey and SNK), for simple design (completely 
randomized, randomized block and Latin squares) and simple experimental schemes 
  (one factor).

I'm needing only two informations: 'qduncan' and 'pduncan',
similar to already available in R 'qtukey' and 'ptukey'. The basic algorithm 
implemented with SNK test will be used for Duncan test.

Below a sample:
a) Generating data and calling the function:

tra = gl(4, 5, label = c('A', 'B', 'C', 'D'))
blo   = rep(1:5, 4)
pro = c(NA, 26, 20, 23, 21, 31, 25, 28, 27, 24, 22, 26, NA, 25, 29, 33, 29, 31, 
34, NA)

x   = aov(pro ~ tra) #or x= aov(pro ~ blo + tra)
res = mctm(x, which='tra', test='SNK', conf.level=0.95)
print(res)

b) The R output:

$Table
Tables of means
Grand mean

26.70588

  tra
        A  B    C     D
     22.5 27 25.5 31.75
rep  4.0  5  4.0  4.00

$Ordered means
tra
     D     B     C     A
31.75 27.00 25.50 22.50

$Result
   D  B  C  A
D -  *  *  *
B *  - ns ns
C * ns  - ns
A * ns ns  -

$Test
[1] "SNK"

$Conf.level
[1] 0.95

$Mean differences
      D    B    C    A
D 0.00 4.75 6.25 9.25
B 4.75 0.00 1.50 4.50
C 6.25 1.50 0.00 3.00
A 9.25 4.50 3.00 0.00

$Minimum Significative Differences - MSD
      D    B    C    A
D 0.00 3.83 4.93 5.48
B 3.83 0.00 3.83 4.68
C 4.93 3.83 0.00 4.04
A 5.48 4.68 4.04 0.00

$Replicates number
     D   B   C   A
D   - 4:5 4:4 4:4
B 5:4   - 5:4 5:4
C 4:4 4:5   - 4:4
A 4:4 4:5 4:4   -

Thanks in advance,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From joseclaudio.faria at terra.com.br  Mon Sep 12 05:57:39 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Mon, 12 Sep 2005 00:57:39 -0300
Subject: [R] help with one matrix
Message-ID: <4324FCB3.8040705@terra.com.br>

Hi Jim,

Many thanks for the function!
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From buser at stat.math.ethz.ch  Mon Sep 12 08:20:20 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 12 Sep 2005 08:20:20 +0200
Subject: [R] test for exponential,lognormal and gammadistribution
In-Reply-To: <1126274609.43219631dccdc@www.cx.unibe.ch>
References: <1126274609.43219631dccdc@www.cx.unibe.ch>
Message-ID: <17189.7716.189792.802640@stat.math.ethz.ch>

Hi Nadja

It depends on your purpose. Often people are using tests to show
that a sample follows a distribution (normal, exponential,
lognormal, ...).
If a test rejects the null hypothesis that the sample comes from
the specified distribution, you are on the safe side, since you
are controlling the significance level (e.g. 5%) and therefore
know the alpha error.
But if a test do not reject the null hypothesis, generally you
have NOT shown that the sample has the specified
distribution. This is related to the power of your test (to
detect differences). 
If the power of a test is lousy, the conclusion that "your
sample has the distribution ...". based on the nonsignificant
test result is misleading or even wrong.

As you mentioned below the kolmogorov-smirnov test does not
adapt for the fact that the parameters of the distribution you
test against are estimated from the data sample.
It assumes that the parameters are know. But in practice that's
not the case in general.
Since the parameter are estimated from the data, but the test do
not have this information, but assumes that these parameters are
a fixed known quantity, the test is to conservative and has a
small power to detect differences.
Therefore it is quite dangerous to conclude that a sample has a
specified distribution, based on the kolmogorov-smirnov test.

An alternative way might be using graphical tools, e.g. quantile
plots (see ?qqplot and ?qqnorm).
Obviously you have the same difficulty by interpreting the
plots, since nobody can tell you for sure if a deviation from
the straight line is significant or just by chance.
But if you conclude that a sample has a distribution by looking
at the plot you will be aware of this subjectivity that can not
be avoided.
The test result will often give you the wrong impression of
objectivity.
The best example to see this is if you have a very small
sample. In general any test has a small power if your sample is
small and it is most probable that the test is
nonsignificant. If we look at the quantile plot (with a small
sample) we often can not judge if it is a straight line or not
(since the sample is to small) and in this case it is the
correct conclusion that we can not say anything about the
distribution. 

I hope this will be helpful.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


riedwyl at giub.unibe.ch writes:
 > 
 > hello!
 > i don't want to test my sample data for normality, but exponential- lognormal- 
 > or gammadistribution.
 > as i've learnt the anderson-darling-test in R is only for normality and i am 
 > not supposed to use the kolmogorov-smirnov test of R for parameter estimates 
 > from sample data, is that true?
 > can you help me, how to do this anyway!
 > thank you very much!
 > nadja
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
 > 
 > 
 > !DSPAM:43219660106581956711619!



From alegarra at neiker.net  Mon Sep 12 08:53:02 2005
From: alegarra at neiker.net (Andres Legarra)
Date: Mon, 12 Sep 2005 08:53:02 +0200
Subject: [R] Backtransforming regression coefficient for scaled covariate
References: <7FFEE688B57D7346BC6241C55900E730F31A47@pollux.bfro.uni-lj.si>
Message-ID: <000301c5b766$9e7d0db0$0802a8c0@iktlan.net>

[R] Backtransforming regression coefficient for scaled covariate

Your
covariate in the second part of the polynomial is x^2 and not x. Therefore
the transformation should be applied to x^2.
Like this:
(lm2 <- lm(y ~ scale(x) + I(scale(x^2)) )
then you would use
coef(lm2)[3]/sd(x^2)

Andres
--
Andres Legarra 
NEIKER
Apdo. 46 
Vitoria-Gasteiz 01080 Spain
--



----- Original Message ----- 
From: Gorjanc Gregor
To: r-help at stat.math.ethz.ch
Sent: Sunday, September 11, 2005 10:25 PM
Subject: [R] Backtransforming regression coefficient for scaled covariate


Hello!
Scaling i.e. (x - mean(x)) / sd(x) of covariates in the model
can improve the efficiency of estimation. That is nice, but
sometimes one needs to report estimates for original scale. I
was able to backtransform estimates of linear regression quite
easily but I stumped on higher polynomials. Is there a general
rule that I am not aware of or is my algebra so bad?
I appologize for not pure R question but I hope others will also
benefit. I attached the R code for example bellow.
## --- Generate data for linear regression --- 
e <- rnorm(n = 100, sd = 10)
x <- rnorm(n = 100, mean = 100, sd = 10)
b <- 3
mu <- 2
y <- mu + b * x + e
plot(y = y, x = x)
## Fit linear regression
(lm1 <- lm(y ~ x))
## Fit linear regression with transformed i.e. standardized covariate
(lm2 <- lm(y ~ scale(x)))
## Backtransform estimate of regression coefficient
coef(lm2)[2] / sd(x)
## --- Generate data for quadratic regression --- 
e <- rnorm(n = 100, sd = 10)
x <- runif(n = 100, min = 1, max = 100)
b1 <- 2
b2 <- -0.01
mu <- 2
y <- mu + b1 * x + b2 * x^2 + e
plot(y = y, x = x)
## Fit regression
(lm1 <- lm(y ~ x + I(x^2)))
## Fit regression with transformed i.e. standardized covariate
(lm2 <- lm(y ~ scale(x) + I(scale(x)^2)))
## Backtransform estimates of regression coefficients
## ??
Lep pozdrav / With regards,
    Gregor Gorjanc
---------------------------------------------------------------------- 
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
---------------------------------------------------------------------- 
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Jan.Wijffels at ucs.kuleuven.be  Mon Sep 12 10:01:53 2005
From: Jan.Wijffels at ucs.kuleuven.be (Jan Wijffels)
Date: Mon, 12 Sep 2005 10:01:53 +0200
Subject: [R] poisson mean hypothesis
Message-ID: <00c401c5b770$3d3d12c0$2c70210a@UCSPC32>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/8d8f9e91/attachment.pl

From Gregor.Gorjanc at bfro.uni-lj.si  Mon Sep 12 10:12:23 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 12 Sep 2005 10:12:23 +0200
Subject: [R] Backtransforming regression coefficient for scaled covariate
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31A49@pollux.bfro.uni-lj.si>

Andres, this seems not to be the case. Look bellow 
the coefficients. They are not the same as in unscaled
regression.

R> (lm1 <- lm(y ~ x + I(x^2)))

Call:
lm(formula = y ~ x + I(x^2))

Coefficients:
(Intercept)            x       I(x^2)  
    4.62069      1.78811     -0.00751  

R> ## Fit regression with transformed i.e. standardized covariate
R> (lm2 <- lm(y ~ scale(x) + I(scale(x)^2)))

Call:
lm(formula = y ~ scale(x) + I(scale(x)^2))

Coefficients:
  (Intercept)       scale(x)  I(scale(x)^2)  
        75.12          29.86          -6.21  

R> coef(lm2)[3]/sd(x^2)
I(scale(x)^2) 
   -0.0020519 

R> coef(lm2)[2]/sd(x)
scale(x) 
  1.0384 

-----Original Message-----
From: Andres Legarra [mailto:alegarra at neiker.net]
Sent: Mon 2005-09-12 08:53
To: Gorjanc Gregor; r-help at stat.math.ethz.ch
Subject: Re: [R] Backtransforming regression coefficient for scaled covariate
 
[R] Backtransforming regression coefficient for scaled covariate

Your
covariate in the second part of the polynomial is x^2 and not x. Therefore
the transformation should be applied to x^2.
Like this:
(lm2 <- lm(y ~ scale(x) + I(scale(x^2)) )
then you would use
coef(lm2)[3]/sd(x^2)

Andres
--
Andres Legarra 
NEIKER
Apdo. 46 
Vitoria-Gasteiz 01080 Spain
--



----- Original Message ----- 
From: Gorjanc Gregor
To: r-help at stat.math.ethz.ch
Sent: Sunday, September 11, 2005 10:25 PM
Subject: [R] Backtransforming regression coefficient for scaled covariate


Hello!
Scaling i.e. (x - mean(x)) / sd(x) of covariates in the model
can improve the efficiency of estimation. That is nice, but
sometimes one needs to report estimates for original scale. I
was able to backtransform estimates of linear regression quite
easily but I stumped on higher polynomials. Is there a general
rule that I am not aware of or is my algebra so bad?
I appologize for not pure R question but I hope others will also
benefit. I attached the R code for example bellow.
## --- Generate data for linear regression --- 
e <- rnorm(n = 100, sd = 10)
x <- rnorm(n = 100, mean = 100, sd = 10)
b <- 3
mu <- 2
y <- mu + b * x + e
plot(y = y, x = x)
## Fit linear regression
(lm1 <- lm(y ~ x))
## Fit linear regression with transformed i.e. standardized covariate
(lm2 <- lm(y ~ scale(x)))
## Backtransform estimate of regression coefficient
coef(lm2)[2] / sd(x)
## --- Generate data for quadratic regression --- 
e <- rnorm(n = 100, sd = 10)
x <- runif(n = 100, min = 1, max = 100)
b1 <- 2
b2 <- -0.01
mu <- 2
y <- mu + b1 * x + b2 * x^2 + e
plot(y = y, x = x)
## Fit regression
(lm1 <- lm(y ~ x + I(x^2)))
## Fit regression with transformed i.e. standardized covariate
(lm2 <- lm(y ~ scale(x) + I(scale(x)^2)))
## Backtransform estimates of regression coefficients
## ??
Lep pozdrav / With regards,
    Gregor Gorjanc
---------------------------------------------------------------------- 
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
---------------------------------------------------------------------- 
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From buser at stat.math.ethz.ch  Mon Sep 12 10:43:42 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 12 Sep 2005 10:43:42 +0200
Subject: [R] Backtransforming regression coefficient for scaled covariate
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730F31A49@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730F31A49@pollux.bfro.uni-lj.si>
Message-ID: <17189.16318.978695.775138@stat.math.ethz.ch>

Dear Gregor

The solution of Andres was correct, but by reproducing his
example you did a copy paste error. In the model lm2 you should
scale your variable after the polynomial transformation

I(scale(x^2)) and not I(scale(x)^2)

Then the backtransformation in your example should work.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Gorjanc Gregor writes:
 > Andres, this seems not to be the case. Look bellow 
 > the coefficients. They are not the same as in unscaled
 > regression.
 > 
 > R> (lm1 <- lm(y ~ x + I(x^2)))
 > 
 > Call:
 > lm(formula = y ~ x + I(x^2))
 > 
 > Coefficients:
 > (Intercept)            x       I(x^2)  
 >     4.62069      1.78811     -0.00751  
 > 
 > R> ## Fit regression with transformed i.e. standardized covariate

Wrong line!!!!

 > R> (lm2 <- lm(y ~ scale(x) + I(scale(x)^2)))

Correct one

 > R> (lm2 <- lm(y ~ scale(x) + I(scale(x^2))))

 > 
 > Call:
 > lm(formula = y ~ scale(x) + I(scale(x)^2))
 > 
 > Coefficients:
 >   (Intercept)       scale(x)  I(scale(x)^2)  
 >         75.12          29.86          -6.21  
 > 
 > R> coef(lm2)[3]/sd(x^2)
 > I(scale(x)^2) 
 >    -0.0020519 
 > 
 > R> coef(lm2)[2]/sd(x)
 > scale(x) 
 >   1.0384 
 > 
 > -----Original Message-----
 > From: Andres Legarra [mailto:alegarra at neiker.net]
 > Sent: Mon 2005-09-12 08:53
 > To: Gorjanc Gregor; r-help at stat.math.ethz.ch
 > Subject: Re: [R] Backtransforming regression coefficient for scaled covariate
 >  
 > [R] Backtransforming regression coefficient for scaled covariate
 > 
 > Your
 > covariate in the second part of the polynomial is x^2 and not x. Therefore
 > the transformation should be applied to x^2.
 > Like this:
 > (lm2 <- lm(y ~ scale(x) + I(scale(x^2)) )
 > then you would use
 > coef(lm2)[3]/sd(x^2)
 > 
 > Andres
 > --
 > Andres Legarra 
 > NEIKER
 > Apdo. 46 
 > Vitoria-Gasteiz 01080 Spain
 > --
 > 
 > 
 > 
 > ----- Original Message ----- 
 > From: Gorjanc Gregor
 > To: r-help at stat.math.ethz.ch
 > Sent: Sunday, September 11, 2005 10:25 PM
 > Subject: [R] Backtransforming regression coefficient for scaled covariate
 > 
 > 
 > Hello!
 > Scaling i.e. (x - mean(x)) / sd(x) of covariates in the model
 > can improve the efficiency of estimation. That is nice, but
 > sometimes one needs to report estimates for original scale. I
 > was able to backtransform estimates of linear regression quite
 > easily but I stumped on higher polynomials. Is there a general
 > rule that I am not aware of or is my algebra so bad?
 > I appologize for not pure R question but I hope others will also
 > benefit. I attached the R code for example bellow.
 > ## --- Generate data for linear regression --- 
 > e <- rnorm(n = 100, sd = 10)
 > x <- rnorm(n = 100, mean = 100, sd = 10)
 > b <- 3
 > mu <- 2
 > y <- mu + b * x + e
 > plot(y = y, x = x)
 > ## Fit linear regression
 > (lm1 <- lm(y ~ x))
 > ## Fit linear regression with transformed i.e. standardized covariate
 > (lm2 <- lm(y ~ scale(x)))
 > ## Backtransform estimate of regression coefficient
 > coef(lm2)[2] / sd(x)
 > ## --- Generate data for quadratic regression --- 
 > e <- rnorm(n = 100, sd = 10)
 > x <- runif(n = 100, min = 1, max = 100)
 > b1 <- 2
 > b2 <- -0.01
 > mu <- 2
 > y <- mu + b1 * x + b2 * x^2 + e
 > plot(y = y, x = x)
 > ## Fit regression
 > (lm1 <- lm(y ~ x + I(x^2)))
 > ## Fit regression with transformed i.e. standardized covariate
 > (lm2 <- lm(y ~ scale(x) + I(scale(x)^2)))
 > ## Backtransform estimates of regression coefficients
 > ## ??
 > Lep pozdrav / With regards,
 >     Gregor Gorjanc
 > ---------------------------------------------------------------------- 
 > University of Ljubljana
 > Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
 > Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
 > Groblje 3                   tel: +386 (0)1 72 17 861
 > SI-1230 Domzale             fax: +386 (0)1 72 17 888
 > Slovenia, Europe
 > ---------------------------------------------------------------------- 
 > "One must learn by doing the thing; for though you think you know it,
 >  you have no certainty until you try." Sophocles ~ 450 B.C.
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide!
 > http://www.R-project.org/posting-guide.html
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
 > 
 > 
 > !DSPAM:43253a39268411607118103!



From alegarra at neiker.net  Mon Sep 12 11:09:49 2005
From: alegarra at neiker.net (Andres Legarra)
Date: Mon, 12 Sep 2005 11:09:49 +0200
Subject: [R] Backtransforming regression coefficient for scaled covariate
References: <7FFEE688B57D7346BC6241C55900E730F31A49@pollux.bfro.uni-lj.si>
Message-ID: <001d01c5b779$ba7dce60$0802a8c0@iktlan.net>

RE: [R] Backtransforming regression coefficient for scaled covariateYep it
is the the same.
Scaling  does both dividing and centering (this is the reason intercept
changes) and scaling is not with sd but with sum of squares divided by n-1;
so, it is not s.d. because average is not substracted
But the appropriate parameters are returned by scale()in form of attributes
(quite awful to extract)

x=1:100
a=rnorm(100)*200+x+.5*x^2+100
lm1=lm(formula= a ~ x + I(x^2))
Coefficients:
(Intercept)            x       I(x^2)
     96.463       -1.062        0.528

 lm2=lm(formula= a ~ scale(x,center=F) + scale(I(x^2),center=F) )
lm2$coeff
              (Intercept)      scale(x, center = F) scale(I(x^2), center =
F)
                 96.46283                 -62.07923
2403.00364

 lm2$coeff[2]/attributes(scale(x,center=F))$"scaled:scale"
scale(x, center = F)
           -1.061893

 lm2$coeff[3]/attributes(scale(x^2,center=F))$"scaled:scale"
scale(I(x^2), center = F)
                0.5280315

If you use the default center=T, then you also have to consider the means of
x and x^2 which are involved in the intercept.
I find this scale() a bit complicated. I never used it and perhaps I've read
the doc too fast

Regards
Andres

--
Andres Legarra Albizu
NEIKER
Apdo. 46
Vitoria-Gasteiz 01080 Spain
--

----- Original Message ----- 
From: Gorjanc Gregor
To: Andr??s Legarra ; r-help at stat.math.ethz.ch
Sent: Monday, September 12, 2005 10:12 AM
Subject: RE: [R] Backtransforming regression coefficient for scaled
covariate


Andres, this seems not to be the case. Look bellow
the coefficients. They are not the same as in unscaled
regression.
R> (lm1 <- lm(y ~ x + I(x^2)))
Call:
lm(formula = y ~ x + I(x^2))
Coefficients:
(Intercept)            x       I(x^2)
    4.62069      1.78811     -0.00751
R> ## Fit regression with transformed i.e. standardized covariate
R> (lm2 <- lm(y ~ scale(x) + I(scale(x)^2)))
Call:
lm(formula = y ~ scale(x) + I(scale(x)^2))
Coefficients:
  (Intercept)       scale(x)  I(scale(x)^2)
        75.12          29.86          -6.21
R> coef(lm2)[3]/sd(x^2)
I(scale(x)^2)
   -0.0020519
R> coef(lm2)[2]/sd(x)
scale(x)
  1.0384
-----Original Message----- 
From: Andres Legarra [mailto:alegarra at neiker.net]
Sent: Mon 2005-09-12 08:53
To: Gorjanc Gregor; r-help at stat.math.ethz.ch
Subject: Re: [R] Backtransforming regression coefficient for scaled
covariate

[R] Backtransforming regression coefficient for scaled covariate
Your
covariate in the second part of the polynomial is x^2 and not x. Therefore
the transformation should be applied to x^2.
Like this:
(lm2 <- lm(y ~ scale(x) + I(scale(x^2)) )
then you would use
coef(lm2)[3]/sd(x^2)
Andres
-- 
Andres Legarra
NEIKER
Apdo. 46
Vitoria-Gasteiz 01080 Spain
-- 



----- Original Message ----- 
From: Gorjanc Gregor
To: r-help at stat.math.ethz.ch
Sent: Sunday, September 11, 2005 10:25 PM
Subject: [R] Backtransforming regression coefficient for scaled covariate


Hello!
Scaling i.e. (x - mean(x)) / sd(x) of covariates in the model
can improve the efficiency of estimation. That is nice, but
sometimes one needs to report estimates for original scale. I
was able to backtransform estimates of linear regression quite
easily but I stumped on higher polynomials. Is there a general
rule that I am not aware of or is my algebra so bad?
I appologize for not pure R question but I hope others will also
benefit. I attached the R code for example bellow.
## --- Generate data for linear regression --- 
e <- rnorm(n = 100, sd = 10)
x <- rnorm(n = 100, mean = 100, sd = 10)
b <- 3
mu <- 2
y <- mu + b * x + e
plot(y = y, x = x)
## Fit linear regression
(lm1 <- lm(y ~ x))
## Fit linear regression with transformed i.e. standardized covariate
(lm2 <- lm(y ~ scale(x)))
## Backtransform estimate of regression coefficient
coef(lm2)[2] / sd(x)
## --- Generate data for quadratic regression --- 
e <- rnorm(n = 100, sd = 10)
x <- runif(n = 100, min = 1, max = 100)
b1 <- 2
b2 <- -0.01
mu <- 2
y <- mu + b1 * x + b2 * x^2 + e
plot(y = y, x = x)
## Fit regression
(lm1 <- lm(y ~ x + I(x^2)))
## Fit regression with transformed i.e. standardized covariate
(lm2 <- lm(y ~ scale(x) + I(scale(x)^2)))
## Backtransform estimates of regression coefficients
## ??
Lep pozdrav / With regards,
    Gregor Gorjanc
---------------------------------------------------------------------- 
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
---------------------------------------------------------------------- 
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.be  Mon Sep 12 11:33:13 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 12 Sep 2005 11:33:13 +0200
Subject: [R] poisson mean hypothesis
References: <00c401c5b770$3d3d12c0$2c70210a@UCSPC32>
Message-ID: <007901c5b77c$ff3f3f90$0540210a@www.domain>

you could use something like the following (in case of two-group 
comparisons make the proper adjustements):

pois.test <- function(x, alternative = c("two.sided", "less", 
"greater"), mu){
    alternative <- match.arg(alternative)
    if (missing(mu) || (length(mu) != 1 || is.na(mu)))
        stop("'mu' must be a single number")
    nx <- length(x)
    mu.x <- mean(x)
    stat <- (mu.x - mu) / sqrt(mu.x/nx)
    p.value <- switch(alternative,
        "two.sided" = 2 * pnorm(-abs(stat)),
        "less" = pnorm(stat),
        "greater" = pnorm(stat, lower = FALSE))
    list("sample mean" = mu.x, "null mean" = mu, "alternative" = 
alternative,
            statistic = stat, p.value = p.value)
}
################

y <- rpois(50, 5)
pois.test(y, alt = "g", mu = 4)

y <- rpois(30, 15)
pois.test(y, alt = "l", mu = 16)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Jan Wijffels" <Jan.Wijffels at ucs.kuleuven.be>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, September 12, 2005 10:01 AM
Subject: [R] poisson mean hypothesis


> Dear R-users,
> Is there a way to get p-values for a one-sided hypothesis test about 
> a
> poisson mean?
>
> Thanks,
>
> Jan Wijffels
> University Center for Statistics
> W. de Croylaan 54
> 3001 Heverlee
> Belgium
> tel: +32 (0)16 322784
> fax: +32 (0)16 322831
> <http://www.kuleuven.be/ucs> http://www.kuleuven.be/ucs
>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From Jan.Wijffels at ucs.kuleuven.be  Mon Sep 12 12:11:50 2005
From: Jan.Wijffels at ucs.kuleuven.be (Jan Wijffels)
Date: Mon, 12 Sep 2005 12:11:50 +0200
Subject: [R] poisson mean hypothesis
In-Reply-To: <007901c5b77c$ff3f3f90$0540210a@www.domain>
Message-ID: <00e001c5b782$644bd510$2c70210a@UCSPC32>

Dimitris,
Thanks for the test. But you are using a normal approximation to the
poisson distribution. This is not applicable in my case. I was more
looking for an exact test. Probably based on Garwood's (1936) confidence
interval. If there is already an R implementation available, than this
would be helpful for me.

Jan

-----Original Message-----
From: Dimitris Rizopoulos [mailto:dimitris.rizopoulos at med.kuleuven.be] 
Sent: maandag 12 september 2005 11:33
To: Jan Wijffels
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] poisson mean hypothesis

you could use something like the following (in case of two-group 
comparisons make the proper adjustements):

pois.test <- function(x, alternative = c("two.sided", "less", 
"greater"), mu){
    alternative <- match.arg(alternative)
    if (missing(mu) || (length(mu) != 1 || is.na(mu)))
        stop("'mu' must be a single number")
    nx <- length(x)
    mu.x <- mean(x)
    stat <- (mu.x - mu) / sqrt(mu.x/nx)
    p.value <- switch(alternative,
        "two.sided" = 2 * pnorm(-abs(stat)),
        "less" = pnorm(stat),
        "greater" = pnorm(stat, lower = FALSE))
    list("sample mean" = mu.x, "null mean" = mu, "alternative" = 
alternative,
            statistic = stat, p.value = p.value)
}
################

y <- rpois(50, 5)
pois.test(y, alt = "g", mu = 4)

y <- rpois(30, 15)
pois.test(y, alt = "l", mu = 16)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Jan Wijffels" <Jan.Wijffels at ucs.kuleuven.be>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, September 12, 2005 10:01 AM
Subject: [R] poisson mean hypothesis


> Dear R-users,
> Is there a way to get p-values for a one-sided hypothesis test about 
> a
> poisson mean?
>
> Thanks,
>
> Jan Wijffels
> University Center for Statistics
> W. de Croylaan 54
> 3001 Heverlee
> Belgium
> tel: +32 (0)16 322784
> fax: +32 (0)16 322831
> <http://www.kuleuven.be/ucs> http://www.kuleuven.be/ucs
>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From riedwyl at giub.unibe.ch  Mon Sep 12 12:58:00 2005
From: riedwyl at giub.unibe.ch (Nadja Riedwyl)
Date: Mon, 12 Sep 2005 12:58:00 +0200
Subject: [R] fit data with gammadistribution
Message-ID: <200509121258.00432.riedwyl@giub.unibe.ch>

hello
my data is
data2:2743  4678 21427  6194 10286  1505 12811  2161  6853  2625 14542   694
11491 14924 28640 17097  2136  5308  3477 91301 11488  3860 64114 14334

by calculating
shape<-(mean(data2))^2/var(data2)
scale<-var(data2)/mean(data2)

i get the idea what the parameters of the gammadistribution would be.
but if i try using the method mle() i get stock and i don't know, how to make 
it work. can anybody help me? thank you very much, indeed.
Nadja
I tried so fare

ll<-function(lambda,alfa) 
{n<-24
x<-data2 
-n*alfa*log(lambda)+n*log(gamma(alfa))-(alfa-1)*sum(log(x))+lambda*sum(x)
est<-mle(minuslogl=ll,start=list(lambda=29827.51,alfa=0.4954725))
summary(est)

NaN's are produced with optim, i just don't know how to avoid this!



From Marco.Salvini at sanpaoloam.com  Mon Sep 12 14:04:10 2005
From: Marco.Salvini at sanpaoloam.com (Marco Salvini)
Date: Mon, 12 Sep 2005 14:04:10 +0200
Subject: [R] [R-sig-finance] Missing
In-Reply-To: <20050910181006.79300.qmail@web36201.mail.mud.yahoo.com>
Message-ID: <OF42DDB084.C5AF684D-ONC125707A.004234A5-C125707A.00424CEB@sanpaoloam.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/e9129710/attachment.pl

From jpritikin at pobox.com  Mon Sep 12 14:15:53 2005
From: jpritikin at pobox.com (Joshua N Pritikin)
Date: Mon, 12 Sep 2005 17:45:53 +0530
Subject: [R] remedial stats education
Message-ID: <20050912121553.GG31014@always.joy.eth.net>

In short:

I didn't take enough stats courses in college.  Now I am working on scientific
research and I feel somewhat lost when it comes to designing the statistical
framework.  I have looked through the books at:

  http://www.r-project.org/doc/bib/R-books.html

I even tried to read [17] Julian J. Faraway. Linear Models with R.  This book
is too advanced.  It helped a little bit but I still feel lost.  Can somebody
recommend a textbook or textbooks suitable for a self-study stats course?

Brief bio:

I have 20 years background in software development.  I know lots of
computer languages including C++ and Perl.  The computer language aspects of R
seems fairly simple.  I did some calculus in college but not more than 1-2
courses.  I have a basic understanding of probability.  I mostly understand
descriptive statistics.  I feel somewhat lost when it comes to statistical
inference.  I am good at self-study.  I happily spend 12 hours a day reading
dry technical manuals.

About the research:

I have designed a web-based questionaire.  http://shared.openheartlogic.org
My collaborator (equally stats inept) is working on a similar web-based
questionaire http://ruminate.openheartlogic.org

Ultimately, we want to publish in a peer-reviewed journal such as Emotion &
Cognition or, at least, get a paper accepted at the annual Cognitive Science
conference.  Something like that.  We have already started collecting data but
not on a large scale since we are not confident about our statistical
approach.

This is a shot in the dark, but if a stats expert wants to collaborate with us
then we would welcome that. We don't have much to offer except, what we think
is, exciting research.

In any case, a few textbook recommendations would probably help me a lot.

-- 
Make April 15 just another day, visit http://fairtax.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050912/b7182cf1/attachment.bin

From HDoran at air.org  Mon Sep 12 14:26:40 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 12 Sep 2005 08:26:40 -0400
Subject: [R] remedial stats education
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A0A42AD@dc1ex2.air.org>

There is a Springer publication "All of Statistics: a concise course in
statistical inference" by Larry Wasserman that might be what you are
looking for. The book also has an emphasis on R and his web site has
code and data sets for analysis of the examples used throughout.

-Harold
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Joshua N Pritikin
Sent: Monday, September 12, 2005 8:16 AM
To: r-help at stat.math.ethz.ch
Cc: heartlogic-dev at nongnu.org
Subject: [R] remedial stats education

In short:

I didn't take enough stats courses in college.  Now I am working on
scientific research and I feel somewhat lost when it comes to designing
the statistical framework.  I have looked through the books at:

  http://www.r-project.org/doc/bib/R-books.html

I even tried to read [17] Julian J. Faraway. Linear Models with R.  This
book is too advanced.  It helped a little bit but I still feel lost.
Can somebody recommend a textbook or textbooks suitable for a self-study
stats course?

Brief bio:

I have 20 years background in software development.  I know lots of
computer languages including C++ and Perl.  The computer language
aspects of R seems fairly simple.  I did some calculus in college but
not more than 1-2 courses.  I have a basic understanding of probability.
I mostly understand descriptive statistics.  I feel somewhat lost when
it comes to statistical inference.  I am good at self-study.  I happily
spend 12 hours a day reading dry technical manuals.

About the research:

I have designed a web-based questionaire.
http://shared.openheartlogic.org My collaborator (equally stats inept)
is working on a similar web-based questionaire
http://ruminate.openheartlogic.org

Ultimately, we want to publish in a peer-reviewed journal such as
Emotion & Cognition or, at least, get a paper accepted at the annual
Cognitive Science conference.  Something like that.  We have already
started collecting data but not on a large scale since we are not
confident about our statistical approach.

This is a shot in the dark, but if a stats expert wants to collaborate
with us then we would welcome that. We don't have much to offer except,
what we think is, exciting research.

In any case, a few textbook recommendations would probably help me a
lot.

--
Make April 15 just another day, visit http://fairtax.org



From Charles.Annis at StatisticalEngineering.com  Mon Sep 12 14:57:24 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 12 Sep 2005 08:57:24 -0400
Subject: [R] remedial stats education
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F740A0A42AD@dc1ex2.air.org>
Message-ID: <200509121257.j8CCvH6x019722@hypatia.math.ethz.ch>

Given that your goal is understanding the fundamentals (a wise choice as it
is problematic attempting to build on an inadequate foundation, and
dangerous to use tools that you don't understand), I enthusiastically
recommend Peter Dalgaard's book, _Introductory Statistics with R_. Springer,
2002. ISBN 0-387-95475-9. http://www.biostat.ku.dk/~pd/ISwR.html.  It is
inexpensive, well written, lucid and very helpful.  After you've mastered
that (and since this is remedial work for you it will not take very long) I
further recommend Venables and Ripley, _Modern Applied Statistics with S_,
Fourth Edition. Springer, 2002. ISBN 0-387-95457-0.
http://www.stats.ox.ac.uk/pub/MASS4/.  This book is more demanding and
covers a broad spectrum of contemporary statistical practice.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
Sent: Monday, September 12, 2005 8:27 AM
To: Joshua N Pritikin; r-help at stat.math.ethz.ch
Cc: heartlogic-dev at nongnu.org
Subject: Re: [R] remedial stats education

There is a Springer publication "All of Statistics: a concise course in
statistical inference" by Larry Wasserman that might be what you are
looking for. The book also has an emphasis on R and his web site has
code and data sets for analysis of the examples used throughout.

-Harold
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Joshua N Pritikin
Sent: Monday, September 12, 2005 8:16 AM
To: r-help at stat.math.ethz.ch
Cc: heartlogic-dev at nongnu.org
Subject: [R] remedial stats education

In short:

I didn't take enough stats courses in college.  Now I am working on
scientific research and I feel somewhat lost when it comes to designing
the statistical framework.  I have looked through the books at:

  http://www.r-project.org/doc/bib/R-books.html

I even tried to read [17] Julian J. Faraway. Linear Models with R.  This
book is too advanced.  It helped a little bit but I still feel lost.
Can somebody recommend a textbook or textbooks suitable for a self-study
stats course?

Brief bio:

I have 20 years background in software development.  I know lots of
computer languages including C++ and Perl.  The computer language
aspects of R seems fairly simple.  I did some calculus in college but
not more than 1-2 courses.  I have a basic understanding of probability.
I mostly understand descriptive statistics.  I feel somewhat lost when
it comes to statistical inference.  I am good at self-study.  I happily
spend 12 hours a day reading dry technical manuals.

About the research:

I have designed a web-based questionaire.
http://shared.openheartlogic.org My collaborator (equally stats inept)
is working on a similar web-based questionaire
http://ruminate.openheartlogic.org

Ultimately, we want to publish in a peer-reviewed journal such as
Emotion & Cognition or, at least, get a paper accepted at the annual
Cognitive Science conference.  Something like that.  We have already
started collecting data but not on a large scale since we are not
confident about our statistical approach.

This is a shot in the dark, but if a stats expert wants to collaborate
with us then we would welcome that. We don't have much to offer except,
what we think is, exciting research.

In any case, a few textbook recommendations would probably help me a
lot.

--
Make April 15 just another day, visit http://fairtax.org

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From thpe at hhbio.wasser.tu-dresden.de  Mon Sep 12 15:13:45 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 12 Sep 2005 15:13:45 +0200
Subject: [R] remedial stats education
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F740A0A42AD@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A42AD@dc1ex2.air.org>
Message-ID: <43257F09.6010800@hhbio.wasser.tu-dresden.de>

Doran, Harold wrote:
> There is a Springer publication "All of Statistics: a concise course in
> statistical inference" by Larry Wasserman that might be what you are
> looking for. The book also has an emphasis on R and his web site has
> code and data sets for analysis of the examples used throughout.
> 
> -Harold

My personal recommendation for students and collegues is as follows:

1) Dalgaard, P.: Introductory Statistics with R.
(for beginners, very understandable, R without too much programming)

2) Crawley, M.J. Statistical Computing. An introduction to data analysis 
using S-Plus. (great for ANOVA like methods and very good self-teaching 
how the methods work)

... and for further reading:

* Venables, W.N. and Ripley, B.D. Modern applied statistics with S.
(Systematic introduction into the S language and comprehensive reference 
over many classical and modern statistical techniques. Some [but not 
all] chapters are relatively demanding. The book, mostly called "MASS", 
is a must for serious S/R users.)

* Pinheiro and Bates: Mixed-effects models in S and S-Plus.

* Box, Jenkins and Reinsel: Time series analysis. Forecasting and control.

* Legendre and Legendre: Numerical Ecology. 2nd edition.

... and many more ;-) depending on your skills and interests.

Thomas P.



From e9826064 at student.tuwien.ac.at  Mon Sep 12 15:23:51 2005
From: e9826064 at student.tuwien.ac.at (Thomas Steiner)
Date: Mon, 12 Sep 2005 15:23:51 +0200
Subject: [R] oma and sub-title
Message-ID: <imph7r.95wgmf@webmail.tuwien.ac.at>

I want to add an outer subtitle to my 2x3-plot, but it's distance to the
lowest plots is very high: 6 lines or more?!
If I choose oma=c(5,0,2,0), it lies out of the plotting region and
disapprears. Obviously I make something wrong here. Help appreciated,
Thomas

x=seq(from=1, to=3.5, length=100)
par(mfrow = c(2, 3), oma=c(6,0,2,0))
## oma=c(5,0,2,0) will not work?!
plot(x,x^2)
plot(x,sin(x))
plot(x,exp(x))
plot(x,1/x)
plot(x,log(x))
plot(x,sin(1/x))
title(main="My Test", outer=TRUE, sub="my subtitle")



From anette at geoplus.dk  Mon Sep 12 15:36:28 2005
From: anette at geoplus.dk (=?iso-8859-1?Q?Anette_N=F8rgaard?=)
Date: Mon, 12 Sep 2005 15:36:28 +0200
Subject: [R] barplot with multiple columns
Message-ID: <000901c5b79e$fab419b0$35d0e182@anette>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/e6369612/attachment.pl

From thpe at hhbio.wasser.tu-dresden.de  Mon Sep 12 15:44:08 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 12 Sep 2005 15:44:08 +0200
Subject: [R] oma and sub-title
In-Reply-To: <imph7r.95wgmf@webmail.tuwien.ac.at>
References: <imph7r.95wgmf@webmail.tuwien.ac.at>
Message-ID: <43258628.4000303@hhbio.wasser.tu-dresden.de>

Thomas Steiner schrieb:
> I want to add an outer subtitle to my 2x3-plot, but it's distance to the
> lowest plots is very high: 6 lines or more?!
> If I choose oma=c(5,0,2,0), it lies out of the plotting region and
> disapprears. Obviously I make something wrong here. Help appreciated,
> Thomas
> 
> x=seq(from=1, to=3.5, length=100)
> par(mfrow = c(2, 3), oma=c(6,0,2,0))
> ## oma=c(5,0,2,0) will not work?!
> plot(x,x^2)
> plot(x,sin(x))
> plot(x,exp(x))
> plot(x,1/x)
> plot(x,log(x))
> plot(x,sin(1/x))
> title(main="My Test", outer=TRUE, sub="my subtitle")
> 

The line parameter may help you, see ?title

title(main="My Test", outer=TRUE, sub="my subtitle", line=0)

HTH Thomas P.



From kkiely at insightful.com  Mon Sep 12 15:49:37 2005
From: kkiely at insightful.com (Kathy Kiely)
Date: Mon, 12 Sep 2005 14:49:37 +0100
Subject: [R] Applied Quantitative Analytics in Finance
Message-ID: <B796B8C05975394DA24E457D1985BDB466E4EE@uk2kexch01.insightful.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/0396f186/attachment.pl

From vincent at 7d4.com  Mon Sep 12 15:51:07 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Mon, 12 Sep 2005 15:51:07 +0200
Subject: [R] remedial stats education
In-Reply-To: <20050912121553.GG31014@always.joy.eth.net>
References: <20050912121553.GG31014@always.joy.eth.net>
Message-ID: <432587CB.8010403@7d4.com>

The Wonnacott & Wonnacott
http://www.amazon.com/exec/obidos/tg/detail/-/0471615188/qid=1126532904/sr=8-1/ref=pd_bbs_1/104-8196380-9551909?v=glance&s=books&n=507846
may be of interest
hih
Vincent



From Achim.Zeileis at wu-wien.ac.at  Mon Sep 12 15:59:15 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 12 Sep 2005 15:59:15 +0200
Subject: [R] barplot with multiple columns
In-Reply-To: <000901c5b79e$fab419b0$35d0e182@anette>
References: <000901c5b79e$fab419b0$35d0e182@anette>
Message-ID: <20050912155915.62e6ff1a.Achim.Zeileis@wu-wien.ac.at>

On Mon, 12 Sep 2005 15:36:28 +0200 Anette N??rgaard wrote:

> I have a large dataset looking like this (as an example):
>  
> doy<-c(178,179,180,181,182,183,184,185,186,187,188)
> s1<-c(0 , 0, 2.4 , 0 , 3.34 , 0 , 5.34 , 0 , 0 , 0 , 6.9)
> s2<-c(0 , 9.72, 0, 10.56 , 2.67 , 0 , 6.45 ,0 , 0 , 9, 3.6)
>  
> dat<-cbind(doy,s1,s2)
>  
> dat
>  
> I need to make a barplot where the two time series s1 and s2 are
> plottet beside each other for each doy. How can I do that?

Probably you want

x <- rbind(s1, s2)
colnames(x) <- doy
barplot(x, beside = TRUE)

or maybe

barplot(t(x), beside = TRUE)

hth,
Z

> Anette
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Sep 12 16:00:24 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Sep 2005 16:00:24 +0200
Subject: [R] barplot with multiple columns
In-Reply-To: <000901c5b79e$fab419b0$35d0e182@anette>
References: <000901c5b79e$fab419b0$35d0e182@anette>
Message-ID: <x2psretmh3.fsf@turmalin.kubism.ku.dk>

Anette N??rgaard <anette at geoplus.dk> writes:

> I have a large dataset looking like this (as an example):
>  
> doy<-c(178,179,180,181,182,183,184,185,186,187,188)
> s1<-c(0 , 0, 2.4 , 0 , 3.34 , 0 , 5.34 , 0 , 0 , 0 , 6.9)
> s2<-c(0 , 9.72, 0, 10.56 , 2.67 , 0 , 6.45 ,0 , 0 , 9, 3.6)
>  
> dat<-cbind(doy,s1,s2)
>  
> dat
>  
> I need to make a barplot where the two time series s1 and s2 are plottet
> beside each other for each doy. How can I do that?

barplot(rbind(s1,s2), beside=T, names=doy)


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From stgries_lists at arcor.de  Mon Sep 12 16:24:30 2005
From: stgries_lists at arcor.de (Stefan Th. Gries)
Date: Mon, 12 Sep 2005 16:24:30 +0200 (CEST)
Subject: [R] grepping and splitting (with R 2.1.1)
Message-ID: <25441990.1126535070811.JavaMail.ngmail@webmail-05.arcor-online.net>

Hi R experts

I have the following regular expression problem. I am writing a basic corpus retrieval program, i.e. a concordancer/function where a user enters
- a set or a directory of text files to search;
- a regular expression to search for in these files.

I want to provide an output in which the matches of the regular expression are listed in one central column and the neighboring columns given the words before and after the matching word. For example, a concordance of the word "the" for the previous sentence with a user-defined span of 3 would lool like this:
-3	-2	-1	0	1	2	3
output	in	which	the	matches	of	the
the	matches	of	the	regular	expression	are
central	column	and	the	neighboring	columns	given
neighboring	columns	given	the	words	before	and
before	and	after	the	matching	word	.

As you can see, there may be multiple hits per line. This works all perfectly fine for cases where the regular expression matches just one of the kind of elements to be separated in the table. 'Unfortunately', apart from 'normal' text files, I also have text files in which every word is preceded by a tag giving its word class, for example

a<-c("<w TO0>to <w VV1>find <w VVN>expected <w TO0>to <w VV2>skivvy <w DT0>much <c PUN>.",
     "<w VVN>seen <w TO0>to <w VV3>kill <w DT0>many")

Now, as long as the regular expression entered by the user is something like
   b<-<w TO0>to
or even
   b<-(?Ui)<w VVN>[^<]*<
this works fine: I identify hits using grep(b, a, perl=T), split up the line using strsplit, and provide as many words before and after my search string as are necessary (and available in the line).

But if the regular expression entered by a user (when prompted by scan(nmax=1, what="char")) is
   b<-b<-"(?Ui)(<w TO0>to <w VV.>[^<]*<)"
I run into several related problems. As you all know, grep and regexpr will only give me the first hit anyway - which is how I identified the lines in the first place - but for the desired output I need all the hits per line together with their context. But, obviously, when I split up the line using strplit and "<w " as a separator so that I can get all hits and all words for the columns -3 to -1 and 1 to 3, the expression matched by the search string b is also split up and cannot be put into one tab-separated central column anymore and I don't seem to be able to extract all hits to store them and insert them again at a later stage ... Basically, I need to split up the element of the vector containing at least one match into x parts, where x is the number of hits plus the number of elements when the surrounding material is split up so that I can generate this kind of display (I leave aside the issue of spaces for now and transpose the above kind of display for expository reasons):

(the first hit in a[1])
-3	
-2	
-1	
0	<w TO0>to <w VV1>find
1	<w VVN>expected
2	<w TO0>to
3	<w VV2>skivvy

and the next line of the output would be the second hit in a[1]:

-3	<w TO0>to
-2	<w VV1>find
-1	<w VVN>expected
0	<w TO0>to <w VV2>skivvy
1	
2	
3	

and the next line would be the only hit in a[2]. The short question after this long intro now is, is there any way of splitting up the elements containing matches in such a way?

I use R 2.1.1 on a Windows XP Pro SP2 machine (with Perl 5.8.7 in case that matters for PRCE). Thanks,
STG


Machen Sie aus 14 Cent spielend bis zu 100 Euro!
Die neue Gaming-Area von Arcor - ??ber 50 Onlinespiele im Angebot.
http://www.arcor.de/rd/emf-gaming-1



From tlumley at u.washington.edu  Mon Sep 12 16:26:37 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Sep 2005 07:26:37 -0700 (PDT)
Subject: [R] poisson mean hypothesis
In-Reply-To: <00e001c5b782$644bd510$2c70210a@UCSPC32>
References: <00e001c5b782$644bd510$2c70210a@UCSPC32>
Message-ID: <Pine.LNX.4.63a.0509120716110.26674@homer24.u.washington.edu>


Use ppois(x,lambda), which gives P(X<=x) for mean=lambda.

Eg: lower one-sided test for observing no events with mean of 3.4
> ppois(0,3.4)
[1] 0.03337327

upper one-sided test for observing 8 events with a mean of 3.4 (need the 
-1 to include 8 in the rejection region)

> ppois(8-1,3.4,lower.tail=FALSE)
[1] 0.02307394

If you want an exact confidence interval there is a formula involving the 
quantiles of the gamma distribution (ie the qgamma() function) that I 
can't remember off hand. It might even be Garwood's formula.


 	-thomas



On Mon, 12 Sep 2005, Jan Wijffels wrote:

> Dimitris,
> Thanks for the test. But you are using a normal approximation to the
> poisson distribution. This is not applicable in my case. I was more
> looking for an exact test. Probably based on Garwood's (1936) confidence
> interval. If there is already an R implementation available, than this
> would be helpful for me.
>
> Jan
>
> -----Original Message-----
> From: Dimitris Rizopoulos [mailto:dimitris.rizopoulos at med.kuleuven.be]
> Sent: maandag 12 september 2005 11:33
> To: Jan Wijffels
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] poisson mean hypothesis
>
> you could use something like the following (in case of two-group
> comparisons make the proper adjustements):
>
> pois.test <- function(x, alternative = c("two.sided", "less",
> "greater"), mu){
>    alternative <- match.arg(alternative)
>    if (missing(mu) || (length(mu) != 1 || is.na(mu)))
>        stop("'mu' must be a single number")
>    nx <- length(x)
>    mu.x <- mean(x)
>    stat <- (mu.x - mu) / sqrt(mu.x/nx)
>    p.value <- switch(alternative,
>        "two.sided" = 2 * pnorm(-abs(stat)),
>        "less" = pnorm(stat),
>        "greater" = pnorm(stat, lower = FALSE))
>    list("sample mean" = mu.x, "null mean" = mu, "alternative" =
> alternative,
>            statistic = stat, p.value = p.value)
> }
> ################
>
> y <- rpois(50, 5)
> pois.test(y, alt = "g", mu = 4)
>
> y <- rpois(30, 15)
> pois.test(y, alt = "l", mu = 16)
>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.be/biostat/
>     http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
> ----- Original Message -----
> From: "Jan Wijffels" <Jan.Wijffels at ucs.kuleuven.be>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, September 12, 2005 10:01 AM
> Subject: [R] poisson mean hypothesis
>
>
>> Dear R-users,
>> Is there a way to get p-values for a one-sided hypothesis test about
>> a
>> poisson mean?
>>
>> Thanks,
>>
>> Jan Wijffels
>> University Center for Statistics
>> W. de Croylaan 54
>> 3001 Heverlee
>> Belgium
>> tel: +32 (0)16 322784
>> fax: +32 (0)16 322831
>> <http://www.kuleuven.be/ucs> http://www.kuleuven.be/ucs
>>
>>
>>
>> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>>
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From chrisb at fcdarwin.org.ec  Mon Sep 12 15:48:28 2005
From: chrisb at fcdarwin.org.ec (Chris Buddenhagen)
Date: Mon, 12 Sep 2005 08:48:28 -0500
Subject: [R] remedial stats education
In-Reply-To: <432587CB.8010403@7d4.com>
Message-ID: <003101c5b7a0$a7beb1f0$4c01a8c0@Chris>

I was in a similar situation-I recommend (Crawley 2005) which I found to be
quite inspiring with many excellent examples.
http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470022981.html  

Crawley, M. J. 2005. Statistics: An introduction using R. John Wiley & Sons
Ltd., Chichester, England.






______________________________________________________________________
EL CONTENIDO DE ESTE MENSAJE ES DE ABSOLUTA RESPONSABILIDAD DEL AUTOR.
FUNDACION CHARLES DARWIN
WWW.DARWINFOUNDATION.ORG



From ecoinformatics at gmail.com  Mon Sep 12 16:49:34 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Mon, 12 Sep 2005 16:49:34 +0200
Subject: [R] help for linear-circular correlation
Message-ID: <15f8e67d0509120749537f819@mail.gmail.com>

Hi R-profs,

Maybe my question is a little off topic. Could any one tell me how to
calculate a linear-circular correlation coefficient and its p-values?
I had a quick look at circular and CircStats packages and did not find
the related function.

Thanks for any kindly help.

Xiaohua

-- 
Xiaohua Dai, Dr.

Centre for Systems Research, Durban Institute of Technology
P.O.Box 953, Durban 4000, South Africa
Tel: +27-31-2042737(O) Fax: +27-31-2042736(O)
Mobile: +27-723682954
Publications: http://www.getcited.org/?MBR=11061629



From p.dalgaard at biostat.ku.dk  Mon Sep 12 16:50:42 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Sep 2005 16:50:42 +0200
Subject: [R] poisson mean hypothesis
In-Reply-To: <Pine.LNX.4.63a.0509120716110.26674@homer24.u.washington.edu>
References: <00e001c5b782$644bd510$2c70210a@UCSPC32>
	<Pine.LNX.4.63a.0509120716110.26674@homer24.u.washington.edu>
Message-ID: <x2hdcqtk59.fsf@turmalin.kubism.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> Use ppois(x,lambda), which gives P(X<=x) for mean=lambda.
> 
> Eg: lower one-sided test for observing no events with mean of 3.4
> > ppois(0,3.4)
> [1] 0.03337327
> 
> upper one-sided test for observing 8 events with a mean of 3.4 (need the 
> -1 to include 8 in the rejection region)
> 
> > ppois(8-1,3.4,lower.tail=FALSE)
> [1] 0.02307394
> 
> If you want an exact confidence interval there is a formula involving the 
> quantiles of the gamma distribution (ie the qgamma() function) that I 
> can't remember off hand. It might even be Garwood's formula.

Or you can clone the procedure in binom.test. In fact, using
binom.test with a sufficiently large n is a rather decent "cheat":

> binom.test(5,1e6)$conf * 1e6

[1]  1.623488 11.668293
attr(,"conf.level")
[1] 0.95

> ppois(4,1.623488)
[1] 0.975
> ppois(5,11.668293)
[1] 0.02500060

The qgamma-based interval would seem to be from

> qgamma(.025, 5)
[1] 1.623486

to

> qgamma(1 - .025, 5 + 1)
[1] 11.66833


Notice that this is obtained as the intersection of the two one-sided
intervals, each at half the nominal alpha level. There are other
possible definitions (e.g. invert the two sided test), but this one
has the advantage of being computable.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From pscacher at mail.nih.gov  Mon Sep 12 16:51:23 2005
From: pscacher at mail.nih.gov (Peter Scacheri)
Date: Mon, 12 Sep 2005 10:51:23 -0400
Subject: [R] heatmap question
Message-ID: <p06200706bf4b4513a758@[128.231.144.61]>

I'm having trouble with the heatmap function in R.  When I try and 
heatmap something, my graphics window does not open.  Does anyone 
know if this is a glitch in the version of R that I'm using?  I've 
listed my version of R below, as well as a simple heatmap command. 
I'm running the program on a Mac, OS 10 v 10.3.9.  Any suggestions??

Thanks!
Peter



>  version
          _
platform powerpc-apple-darwin6.8
arch     powerpc
os       darwin6.8
system   powerpc, darwin6.8
status
major    2
minor    0.1
year     2004
month    11
day      15
language R
>  x<-matrix(rnorm(100),nr=10)
>  heatmap(x)
>



From James.A.Rogers at pfizer.com  Mon Sep 12 16:55:24 2005
From: James.A.Rogers at pfizer.com (Rogers, James A [PGRD Groton])
Date: Mon, 12 Sep 2005 10:55:24 -0400
Subject: [R] Help with a more flexible funtion for multiple comparisio	n
	of	means
Message-ID: <2A8DE2E40F52E049B8FBB4634D32AFE602CA1C6F@groamrexm01.amer.pfizer.com>

Jose - 

Before implementing SNK and Duncan's, you may want to be aware of some
criticisms of these methods:

>From Hsu (1996), 

"Newman-Keuls multiple range test is not a confident inequalities method and
cannot be recommended."

"Duncan's multiple range test is not a confident inequalities method and
cannot be recommended either. In the words of Tukey (1991), Duncan's
multiple range test was a 'distraction' in the history of multiple
comparisons, amounting to 'talking 5% while using more than 5%
simultaneous'"

@Book{Hsu1996,
  author = 	 {Jason C. Hsu},
  title = 	 {Multiple Comparisons: Theory and Methods},
  publisher = 	 {Chapman & Hall},
  year = 	 {1996}
}


-- Jim 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From ramasamy at cancer.org.uk  Mon Sep 12 18:06:34 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 12 Sep 2005 17:06:34 +0100
Subject: [R] heatmap question
In-Reply-To: <p06200706bf4b4513a758@[128.231.144.61]>
References: <p06200706bf4b4513a758@[128.231.144.61]>
Message-ID: <1126541195.5998.4.camel@ipc143004.lif.icnet.uk>

Does any other plotting function work as they should e.g. plot(1:10) or
are you connecting remotely to a server ?

Regards, Adai



On Mon, 2005-09-12 at 10:51 -0400, Peter Scacheri wrote:
> I'm having trouble with the heatmap function in R.  When I try and 
> heatmap something, my graphics window does not open.  Does anyone 
> know if this is a glitch in the version of R that I'm using?  I've 
> listed my version of R below, as well as a simple heatmap command. 
> I'm running the program on a Mac, OS 10 v 10.3.9.  Any suggestions??
> 
> Thanks!
> Peter
> 
> 
> 
> >  version
>           _
> platform powerpc-apple-darwin6.8
> arch     powerpc
> os       darwin6.8
> system   powerpc, darwin6.8
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
> >  x<-matrix(rnorm(100),nr=10)
> >  heatmap(x)
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From vincent at 7d4.com  Mon Sep 12 17:30:08 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Mon, 12 Sep 2005 17:30:08 +0200
Subject: [R] heatmap question
In-Reply-To: <p06200706bf4b4513a758@[128.231.144.61]>
References: <p06200706bf4b4513a758@[128.231.144.61]>
Message-ID: <43259F00.60703@7d4.com>

Peter Scacheri a ??crit :

> I'm having trouble with the heatmap function in R.  When I try and 
> heatmap something, my graphics window does not open.  Does anyone 
> know if this is a glitch in the version of R that I'm using?  I've 
> listed my version of R below, as well as a simple heatmap command. 
> I'm running the program on a Mac, OS 10 v 10.3.9.  Any suggestions??

perhaps try
image(x, col = heat.colors(100))
and let us know the result
hih



From pscacher at mail.nih.gov  Mon Sep 12 17:53:02 2005
From: pscacher at mail.nih.gov (Peter Scacheri)
Date: Mon, 12 Sep 2005 11:53:02 -0400
Subject: [R] heatmap question
In-Reply-To: <43259F00.60703@7d4.com>
References: <43259F00.60703@7d4.com>
Message-ID: <p0620070abf4b54b0500b@[128.231.144.61]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/2b69cdfc/attachment.pl

From sms13+ at pitt.edu  Mon Sep 12 18:19:48 2005
From: sms13+ at pitt.edu (sms13+@pitt.edu)
Date: Mon, 12 Sep 2005 12:19:48 -0400
Subject: [R] set differences
Message-ID: <1473598046.1126527588@Lab26.DOMAIN.IE.PITT.EDU>

Can anyone tell me how to do set 
differences in R?
e.g., if I have a vector 
a<-c(1,2,3,4,5) and another vector 
b<-c(2,5), how can I do something like 
a/b = (1,3,4)?

Thanks!

---------------------------------------
-------------------------
Steven Shechter
PhD Candidate in Industrial Engineering
University of Pittsburgh
www.pitt.edu/~sms13



From sundar.dorai-raj at pdf.com  Mon Sep 12 18:29:34 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 12 Sep 2005 11:29:34 -0500
Subject: [R] set differences
In-Reply-To: <1473598046.1126527588@Lab26.DOMAIN.IE.PITT.EDU>
References: <1473598046.1126527588@Lab26.DOMAIN.IE.PITT.EDU>
Message-ID: <4325ACEE.70200@pdf.com>

sms13+ at pitt.edu wrote:
> Can anyone tell me how to do set 
> differences in R?
> e.g., if I have a vector 
> a<-c(1,2,3,4,5) and another vector 
> b<-c(2,5), how can I do something like 
> a/b = (1,3,4)?
> 
> Thanks!
> 

a[!a %in% b]

*or*

setdiff(a, b)

--sundar



From abderrahim.oulhaj at pharmacology.oxford.ac.uk  Mon Sep 12 18:59:39 2005
From: abderrahim.oulhaj at pharmacology.oxford.ac.uk (Abderrahim Oulhaj)
Date: Mon, 12 Sep 2005 17:59:39 +0100
Subject: [R] Glmm for multiple outcomes
Message-ID: <023301c5b7bb$5ccb4580$adca01a3@optima.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/cdf8ea24/attachment.pl

From gerifalte28 at hotmail.com  Mon Sep 12 19:09:05 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 12 Sep 2005 17:09:05 +0000
Subject: [R] fit data with gammadistribution
In-Reply-To: <200509121258.00432.riedwyl@giub.unibe.ch>
Message-ID: <BAY103-F37E15294E231ECA529F42CA69D0@phx.gbl>

Somebody already did the job for you.  Try fitdistr{MASS} i.e.

x=scan("clipboard")#Read your data from clipboard
sh=(mean(x))^2/var(x)
sc=var(x)/mean(x)
fitdistr(x,"gamma", list(shape=sh, scale=sc))

Now you probably know that you have to be carfeul when estimating 
distribution parameters from such a small number of observations.

PS: this is a very "popular" question so in the future before you post a 
question try RSiteSearch() i.e.  RSiteSearch("fit gamma") gave me 273 hits

Cheers

Francisco

>From: Nadja Riedwyl <riedwyl at giub.unibe.ch>
>To: r-help at stat.math.ethz.ch
>Subject: [R] fit data with gammadistribution
>Date: Mon, 12 Sep 2005 12:58:00 +0200
>
>hello
>my data is
>data2:2743  4678 21427  6194 10286  1505 12811  2161  6853  2625 14542   
>694
>11491 14924 28640 17097  2136  5308  3477 91301 11488  3860 64114 14334
>
>by calculating
>shape<-(mean(data2))^2/var(data2)
>scale<-var(data2)/mean(data2)
>
>i get the idea what the parameters of the gammadistribution would be.
>but if i try using the method mle() i get stock and i don't know, how to 
>make
>it work. can anybody help me? thank you very much, indeed.
>Nadja
>I tried so fare
>
>ll<-function(lambda,alfa)
>{n<-24
>x<-data2
>-n*alfa*log(lambda)+n*log(gamma(alfa))-(alfa-1)*sum(log(x))+lambda*sum(x)
>est<-mle(minuslogl=ll,start=list(lambda=29827.51,alfa=0.4954725))
>summary(est)
>
>NaN's are produced with optim, i just don't know how to avoid this!
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From Gregor.Gorjanc at bfro.uni-lj.si  Mon Sep 12 19:42:44 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 12 Sep 2005 19:42:44 +0200
Subject: [R] Backtransforming regression coefficient for scaled covariate
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31A4E@pollux.bfro.uni-lj.si>

Andres,

thanks for detailed explanation. Things make sense now.

Regards, Gregor



From sledepi at operamail.com  Mon Sep 12 20:58:47 2005
From: sledepi at operamail.com (sloan jones)
Date: Mon, 12 Sep 2005 19:58:47 +0100
Subject: [R] trouble with reading data from excel
Message-ID: <20050912185847.BBC9F23D09@ws5-3.us4.outblaze.com>

I have been trying to open data that I have saved in an excel spread sheet.  I saved it as a csv.  Then I tried using the read.csv command. However, everytime I do this--

diseasedat<-read.csv("M:/sloan/R/disease/disease.csv", sep=,  header = TRUE, fill= TRUE)--

I get an error message:  

Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file 'M:/sloan/R/disease/disease.csv'

What am I doing wrong or what should I look for to correct this?

Sloan

-- 
_______________________________________________
Surf the Web in a faster, safer and easier way:
Download Opera 8 at http://www.opera.com



From Roger.Bivand at nhh.no  Mon Sep 12 21:11:02 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 12 Sep 2005 21:11:02 +0200 (CEST)
Subject: [R] trouble with reading data from excel
In-Reply-To: <20050912185847.BBC9F23D09@ws5-3.us4.outblaze.com>
Message-ID: <Pine.LNX.4.44.0509122109220.27759-100000@reclus.nhh.no>

On Mon, 12 Sep 2005, sloan jones wrote:

> I have been trying to open data that I have saved in an excel spread sheet.  I saved it as a csv.  Then I tried using the read.csv command. However, everytime I do this--
> 
> diseasedat<-read.csv("M:/sloan/R/disease/disease.csv", sep=,  header = TRUE, fill= TRUE)--
> 
> I get an error message:  
> 
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file 'M:/sloan/R/disease/disease.csv'
> 
> What am I doing wrong or what should I look for to correct this?
> 

Try using file.choose() instead of typing in your file name - read.csv()
cannot find a file with that name in that directory.

> Sloan
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From p.dalgaard at biostat.ku.dk  Mon Sep 12 21:14:36 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Sep 2005 21:14:36 +0200
Subject: [R] trouble with reading data from excel
In-Reply-To: <20050912185847.BBC9F23D09@ws5-3.us4.outblaze.com>
References: <20050912185847.BBC9F23D09@ws5-3.us4.outblaze.com>
Message-ID: <x2hdcqxfmr.fsf@turmalin.kubism.ku.dk>

"sloan jones" <sledepi at operamail.com> writes:

> I have been trying to open data that I have saved in an excel spread sheet.  I saved it as a csv.  Then I tried using the read.csv command. However, everytime I do this--
> 
> diseasedat<-read.csv("M:/sloan/R/disease/disease.csv", sep=,  header = TRUE, fill= TRUE)--
> 
> I get an error message:  
> 
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file 'M:/sloan/R/disease/disease.csv'
> 
> What am I doing wrong or what should I look for to correct this?

First (must be said) check that you got the filename right...

If the name is right: Did you close Excel before trying to read the
file? Sometimes Windows applications hang onto their files and prevent
others from opening them. Can you open it in other applications?

The read.csv call looks a little odd -- you shouldn't need extra
arguments, just read.csv or read.csv2 depending on locale. However,
that shouldn't cause that sort of error message.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From liuwensui at gmail.com  Mon Sep 12 21:23:23 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 12 Sep 2005 15:23:23 -0400
Subject: [R] trouble with reading data from excel
In-Reply-To: <20050912185847.BBC9F23D09@ws5-3.us4.outblaze.com>
References: <20050912185847.BBC9F23D09@ws5-3.us4.outblaze.com>
Message-ID: <1115a2b005091212236c84e0ea@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/164a6951/attachment.pl

From mbock at Environcorp.com  Mon Sep 12 21:32:54 2005
From: mbock at Environcorp.com (Mike Bock)
Date: Mon, 12 Sep 2005 14:32:54 -0500
Subject: [R] Covert list of list to dataframe for export or outputting
	by(test) output
Message-ID: <AA564451B2A8A147B653D20C3E481D62D5451B@emloop02.environchicago.environ.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/ef55d634/attachment.pl

From pon3 at llnl.gov  Mon Sep 12 21:47:49 2005
From: pon3 at llnl.gov (Raymond K Pon)
Date: Mon, 12 Sep 2005 12:47:49 -0700
Subject: [R] Document clustering for R
Message-ID: <4325DB65.4090504@llnl.gov>

I'm working on a project related to document clustering. I know that R 
has clustering algorithms such as clara, but only supports two distance 
metrics: euclidian and manhattan, which are not very useful for 
clustering documents. I was wondering how easy it would be to extend the 
clustering package in R to support other distance metrics, such as 
cosine distance, or if there was an API for custom distance metrics.

Best regards,
Raymond Pon
pon3 at llnl.gov
x43062



From p.dalgaard at biostat.ku.dk  Mon Sep 12 22:21:55 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Sep 2005 22:21:55 +0200
Subject: [R] Covert list of list to dataframe for export or outputting
	by(test) output
In-Reply-To: <AA564451B2A8A147B653D20C3E481D62D5451B@emloop02.environchicago.environ.local>
References: <AA564451B2A8A147B653D20C3E481D62D5451B@emloop02.environchicago.environ.local>
Message-ID: <x2d5nexcik.fsf@turmalin.kubism.ku.dk>

"Mike Bock" <mbock at Environcorp.com> writes:

> Greetings,
> I am running a buch of wilcox tests and need to be able to rapidly
> export the results into a csv file. I have attached example code as well
> as my attempts to get what I need. I have tried unlist,cbind,rbind etc
> but I am obvously missing something simple. FYI I am actually running
> about 50 WRS tests per dataset, this is just an example.
....
> WRS <- by(AKCCR, AKCCR$Compound.Name, function(AKCCR) wilcox.test(AdjRes
> ~ ExposureUnit, data = AKCCR))
> #just one of many attempts to get the output into a form I can write to
> a file and import into excel
> WRSlist <- cbind(WRS)
> WRSFinal <- data.frame(WRSlist)

How about do.call("rbind", WRS) instead? (Or just the first and
hird column of it.)

A fine point: This gives a character matrix. If the function inside
by() returned a data frame, then rbind()'ing would give one too, but
one component of the return value of wilcox.test is NULL, so you need

> WRS <- by(AKCCR, AKCCR$Compound.Name, function(AKCCR)
+     as.data.frame(wilcox.test(AdjRes~ ExposureUnit, data = AKCCR)[-2]))
Warning messages:
1: cannot compute exact p-value with ties in: wilcox.test.default(x =
   c(0.03, 0.24, 0.0082, 0.29, 0.01, 0.19,
2: cannot compute exact p-value with ties in: wilcox.test.default(x =
   c(0.15, 0.09, 0.16, 0.08, 0.15, 0.17,
> class(do.call("rbind", WRS)) 
[1] "data.frame"


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From rggefrm at ucl.ac.uk  Mon Sep 12 23:17:11 2005
From: rggefrm at ucl.ac.uk (rggefrm@ucl.ac.uk)
Date: Mon, 12 Sep 2005 22:17:11 +0100
Subject: [R] binary for Hmisc library OS-X
Message-ID: <1126559831.4325f057ec7af@www.webmail.ucl.ac.uk>


I'm looking for the binary of the Hmisc package (required for the Design librar)
for OS-X (R 2.1)
I think it is not available for download because it didn't pass all the checks.
I know, it should be easy to compile from the source, but I have no access to a
Mac which has installed the X-tools.
Itt would be great if someone could send me the binaries of the library

Yours
Frank Mattes



From chrisb at fcdarwin.org.ec  Mon Sep 12 22:26:05 2005
From: chrisb at fcdarwin.org.ec (Chris Buddenhagen)
Date: Mon, 12 Sep 2005 15:26:05 -0500
Subject: [R] Bonferroni test?
In-Reply-To: <x2d5nexcik.fsf@turmalin.kubism.ku.dk>
Message-ID: <000701c5b7d8$33a4bb60$4c01a8c0@Chris>



Chris Buddenhagen, Botany Department, Charles Darwin Research Station, Santa
Cruz,Galapagos. Mail: Charles Darwin Foundation, Casilla 17-01-3891 Avenida
6 de Diciembre N36-109 y Pasaje California Quito, ECUADOR
Dear R experts

Is there a simple means of doing this multiple comparison test in R?

I did a search and found information about it at
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/44566.html but it did not
include information about doing it in R.

Chris Buddenhagen, Botany Department, Charles Darwin Research Station, Santa
Cruz,Galapagos. Mail: Charles Darwin Foundation, Casilla 17-01-3891 Avenida
6 de Diciembre N36-109 y Pasaje California Quito, ECUADOR






______________________________________________________________________
EL CONTENIDO DE ESTE MENSAJE ES DE ABSOLUTA RESPONSABILIDAD DEL AUTOR.
FUNDACION CHARLES DARWIN
WWW.DARWINFOUNDATION.ORG



From tlumley at u.washington.edu  Mon Sep 12 23:33:46 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Sep 2005 14:33:46 -0700 (PDT)
Subject: [R] Bonferroni test?
In-Reply-To: <000701c5b7d8$33a4bb60$4c01a8c0@Chris>
References: <000701c5b7d8$33a4bb60$4c01a8c0@Chris>
Message-ID: <Pine.LNX.4.63a.0509121433320.22754@homer22.u.washington.edu>

On Mon, 12 Sep 2005, Chris Buddenhagen wrote:
>
> Is there a simple means of doing this multiple comparison test in R?
>

p.adjust()

 	-thomas



From jhallman at frb.gov  Mon Sep 12 23:52:44 2005
From: jhallman at frb.gov (jhallman@frb.gov)
Date: Mon, 12 Sep 2005 17:52:44 -0400
Subject: [R] Trying to reach Frank Harrell
Message-ID: <20050912215244.D5CA253584@mail.rsma.frb.gov>

Does anyone have a valid email address for Frank Harrell of Hmisc fame?  I've
tried getting in touch with him at both fharrell at virginia.edu and
f.harrell at vanderbilt.edu, but messages to either of those addresses get
bounced.  Frank, if you're reading this, please email me from an account that
will accept a reply.

Jeff Hallman



From tahrens at stanford.edu  Tue Sep 13 00:22:56 2005
From: tahrens at stanford.edu (Mr Toby Daniel Ahrens)
Date: Mon, 12 Sep 2005 15:22:56 -0700
Subject: [R] Running DOS command prompt from R 2.0.1 Windows?
Message-ID: <1126563776.4325ffc02e76a@webmail.stanford.edu>

I am trying to use R (windows version 2.0.1) to manage runs of a program
that is run from a DOS command prompt.  Is R able to call a DOS prompt?  I
am hoping that there is something analogous to the "spawn" command in IDL,
but I can't see to find any help in the R archives...

Thanks very much,

Toby



From chrisb at fcdarwin.org.ec  Tue Sep 13 00:18:23 2005
From: chrisb at fcdarwin.org.ec (Chris Buddenhagen)
Date: Mon, 12 Sep 2005 17:18:23 -0500
Subject: [R] Multiple comparisons like a Chi2 or Fisher's exact test
Message-ID: <001901c5b7e7$e68fa870$4c01a8c0@Chris>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/9b3dd662/attachment.pl

From bartzk at yahoo-inc.com  Tue Sep 13 01:17:20 2005
From: bartzk at yahoo-inc.com (Kevin Bartz)
Date: Mon, 12 Sep 2005 16:17:20 -0700
Subject: [R] [handling] Missing [values in randomForest]
Message-ID: <E4EBBAD66D826C41BDED4CCCDCC6D0F1013B4322@EXCHG01-BUR>

Hi Jan-Paul,

You definitely want to be careful with na.omit in randomForest -- that
wipes out any row with even one NA. If NAs are sprawled throughout your
dataset, na.omit might end up killing a lot of rows. Here's my usual MO
for missing values:

1) "impute" in Hmisc fills in gaps with the mean, median, most common
value, etc.
2) rfImpute: fits a forest on the rows available and uses it to predict
the missing values.
3) aregImpute: similar to rfImpute, but using a linear model.
4) You may want to consider using a single tree ("rpart" package) in
this case instead of a forest. Single trees deal with missing values
cleanly through surrogate splits.

Good luck!

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
Sent: Sunday, September 11, 2005 3:44 AM
To: Jan-Paul Roodbol
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] [handling] Missing [values in randomForest]

Jan-Paul Roodbol wrote:

> Does anyone know if randomForest in R can handle
> dataset with missings?

See ?randomForest, you can omit observations including NAs by specifying

na.action=na.omit

Please do not cross-post!
Please specify a sensible subject!

Uwe Ligges


> Thank you
> 
> Kind regards
> 
> Jan-Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From joseclaudio.faria at terra.com.br  Tue Sep 13 01:23:00 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Mon, 12 Sep 2005 20:23:00 -0300
Subject: [R] Help with a more flexible funtion for multiple comparisio n
 of	means
In-Reply-To: <2A8DE2E40F52E049B8FBB4634D32AFE602CA1C6F@groamrexm01.amer.pfizer.com>
References: <2A8DE2E40F52E049B8FBB4634D32AFE602CA1C6F@groamrexm01.amer.pfizer.com>
Message-ID: <43260DD4.4020308@terra.com.br>

Rogers, James A [PGRD Groton] wrote:
> Jose - 
> 
> Before implementing SNK and Duncan's, you may want to be aware of some
> criticisms of these methods:
> 
>>From Hsu (1996), 
> 
> "Newman-Keuls multiple range test is not a confident inequalities method and
> cannot be recommended."
> 
> "Duncan's multiple range test is not a confident inequalities method and
> cannot be recommended either. In the words of Tukey (1991), Duncan's
> multiple range test was a 'distraction' in the history of multiple
> comparisons, amounting to 'talking 5% while using more than 5%
> simultaneous'"
> 
> @Book{Hsu1996,
>   author = 	 {Jason C. Hsu},
>   title = 	 {Multiple Comparisons: Theory and Methods},
>   publisher = 	 {Chapman & Hall},
>   year = 	 {1996}
> }
> 
> 
> -- Jim 

Ok Jim, I know these limitations, but these tests exists.
BTW, I am making this function for academic reasons and to learning how to make 
more advanced R functions.

Thanks for all.

Best,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From vinum at iinet.net.au  Tue Sep 13 01:29:03 2005
From: vinum at iinet.net.au (John Charles Considine)
Date: Tue, 13 Sep 2005 07:29:03 +0800
Subject: [R] trouble with reading data from excel
In-Reply-To: <20050912185847.BBC9F23D09@ws5-3.us4.outblaze.com>
References: <20050912185847.BBC9F23D09@ws5-3.us4.outblaze.com>
Message-ID: <1126567743.7043.10.camel@Tardis.considine.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050913/c7fc5a8b/attachment.pl

From Murraypu at aimnsw.com.au  Tue Sep 13 01:43:12 2005
From: Murraypu at aimnsw.com.au (Murray Pung)
Date: Tue, 13 Sep 2005 09:43:12 +1000
Subject: [R] trouble with reading data from excel
Message-ID: <3028F4C4647C9043B870276E28C69FD6B5680C@syd05.aimnsw.com.au>

Alternatively, you can set the working directory to your folder:

File > Change dir...

Select your directory, and then:

diseasedat <- read.csv("disease.csv")

Murray


-----Original Message-----
From: John Charles Considine [mailto:vinum at iinet.net.au]
Sent: Tuesday, 13 September 2005 9:29 AM
To: r-help
Subject: Re: [R] trouble with reading data from excel


Sloane, 

try,

diseasedat<-read.csv("M:\\sloan\\R\\disease\\disease.csv")

and read http://cran.r-project.org/doc/manuals/R-data.html

JC


On Mon, 2005-09-12 at 19:58 +0100, sloan jones wrote:

> I have been trying to open data that I have saved in an excel spread sheet.  I saved it as a csv.  Then I tried using the read.csv command. However, everytime I do this--
> 
> diseasedat<-read.csv("M:/sloan/R/disease/disease.csv", sep=,  header = TRUE, fill= TRUE)--
> 
> I get an error message:  
> 
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file 'M:/sloan/R/disease/disease.csv'
> 
> What am I doing wrong or what should I look for to correct this?
> 
> Sloan
> 

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue Sep 13 02:27:54 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 13 Sep 2005 02:27:54 +0200 (CEST)
Subject: [R] R news: Call for Papers
Message-ID: <Pine.LNX.4.51.0509130226290.16273@artemis.imbe.med.uni-erlangen.de>


Dear useRs and developeRs,

the next issue of `R news' is scheduled for the beginning of November
and we are now  accepting submissions for this last issue in 2005.
For more information see

      http://cran.r-project.org/doc/Rnews/

If you are the author of a package on CRAN and you would like to promote
it a little bit, or if you simply have an interesting application using
R, we hope you can find some time to write a short article on it. We
suggest that it be approximately 3 pages or less. The idea of the
newsletter is that it be interesting to R users without being too
technical. For example an article describing a package could begin by
briefly outlining the statistical background and go on to demonstrate
the usage on some typical data set. Of course graphics are more than
welcome!

Bill Venables <Bill.Venables at csiro.au> is also encouraging submissions
to the more specialist Programmer's Niche column. In this case the
technical level could be a little higher, of course, but not necessarily:
ingeniousness is the key.

The R Help Desk column is intended to present answers to frequently
asked questions as well as tricks that are useful to the majority of
useRs. Please send submissions to Uwe Ligges <Uwe.Ligges at R-project.org>.

The deadline for submissions is

	October, 8th, 2005

Keep the contributions rolling in!

The Editorial Board,

Doug Bates, Paul Murrell and Torsten Hothorn



From Tom.Mulholland at dpi.wa.gov.au  Tue Sep 13 02:40:59 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 13 Sep 2005 08:40:59 +0800
Subject: [R] Document clustering for R
Message-ID: <4702645135092E4497088F71D9C8F51A128C27@afhex01.dpi.wa.gov.au>

I searched the help for "cosine distance" and this was the first hit http://finzi.psych.upenn.edu/R/Rhelp02a/archive/3946.html


Tom
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Raymond K Pon
> Sent: Tuesday, 13 September 2005 3:48 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Document clustering for R
> 
> 
> I'm working on a project related to document clustering. I 
> know that R 
> has clustering algorithms such as clara, but only supports 
> two distance 
> metrics: euclidian and manhattan, which are not very useful for 
> clustering documents. I was wondering how easy it would be to 
> extend the 
> clustering package in R to support other distance metrics, such as 
> cosine distance, or if there was an API for custom distance metrics.
> 
> Best regards,
> Raymond Pon
> pon3 at llnl.gov
> x43062
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jsorkin at grecc.umaryland.edu  Tue Sep 13 02:47:28 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 12 Sep 2005 20:47:28 -0400
Subject: [R] Fisher's exact test vs Chi-square
Message-ID: <s325e976.029@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/a6d663cf/attachment.pl

From f.harrell at vanderbilt.edu  Tue Sep 13 02:58:41 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 12 Sep 2005 19:58:41 -0500
Subject: [R] Fisher's exact test vs Chi-square
In-Reply-To: <s325e976.029@grecc.umaryland.edu>
References: <s325e976.029@grecc.umaryland.edu>
Message-ID: <43262441.6000001@vanderbilt.edu>

John Sorkin wrote:
> Timothy,
> I believe you are mistaken. Fisher's exact test give the correct answer
> even in the face of small expected values for the cell counts. Pearson's
> Chi-square approximates Fisher's exact test and can give the wrong
> answer when expected cell counts are low. Chi-square was developed
> because it is computationally "simple". Fisher's exact test,
> particularly with tables larger than 2 by 2 can be computationally
> complex. The value of the Chi-square statistic becomes closer and closer
> to Fisher's exact test as the expected cell counts become larger.
> John

John,

I'll have to disagree a bit.  Pearson's can still work with low expected 
frequencies.  It was not intended to approximate Fisher's test.  And 
Fisher's is conservative.  The Pearson chi-square with Yates' continuity 
correction was intended to approximate the more conservative Fisher test.

Cheers,

Frank

>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>  
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>  
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 
>>>>Timothy Mak <Timothy.Mak at IOP.KCL.AC.UK> 9/12/2005 11:45:28 AM >>>
> 
> 
> I have heard that people favour the Pearson's Chi-square over Fisher's
> 
> exact test because the latter is more conservative. Some people
> therefore 
> only use Fisher's exact test when some of the expected counts are too 
> small. But nowadays we can quite easily calculate the exact p-value
> based 
> on the Pearson statistic, provided it's not a huge table (SPSS can do
> it). 
> Is there any place for Fisher's exact test then? 
> 
> Tim 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From john.maindonald at anu.edu.au  Tue Sep 13 02:48:02 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Tue, 13 Sep 2005 10:48:02 +1000
Subject: [R] Discrepancy between R and SPSS in 2-way,
	repeated measures ANOVA
Message-ID: <4FAF8F23-3FE1-41E9-9581-5C1B8DECDB1F@anu.edu.au>

For the record, it turns out that EXPNO ran from 1 to 20, i.e., it  
identified
subject.

Thus EXPNO/COND parsed into the two error terms (additional to residual)
EXPNO and EXPNO:COND.  This second error term accounts for all
variation between levels of COND; so there is no COND sum of squares.
(In SPSS the fixed effect COND may have taken precedence; I do not
know for sure.)

In R, if this was a complete randomized design, the term Error(EXPO),
or in the mock-up example I gave Error(subj), would be enough on its  
own.

The R implementation can handle error terms akin to Error(REPNO/subj),
but because there are redundant model matrix columns generated by the
REPNO:subj term, complains that the Error() model is singular.

In general, terms of the form a/b should be used only if b is nested  
within a,
i.e.,
REPNO/IndividualWithinBlock
(where IndividualWithinBlock runs from 1 to 4)
not REPNO/subj.
(Either of these cause REPNO to be treated as a blocking factor).

 > xy <- expand.grid(REPNO=letters[1:5], COND=letters[1:4],
+                                    TIME=factor(paste(1:2)))
 > xy$subj <- factor(paste(xy$REPNO, xy$COND, sep=":"))
 > ## Below subj becomes EXPNO
 > xy$COND <- factor(xy$COND)
 > xy$REPNO <- factor(xy$REPNO)
 > xy$y <- rnorm(40)

Plea to those who post such questions to the list:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Please Include either a toy data set or, if the actual data set is  
small,
lists of factor values.  If you are happy to make the information  
public,
give the result vector also (this is less important!)  Or you can put  
the
data and, where relevant, your code, on a web site.

Be careful about the use of the word "groups" in an experimental
design context; speak of "treatment groups" if that is the meaning,
or "blocks" if that is what is intended.  I suspect that confusion
between these two contexts in which the word groups is wont to
be used lay behind the use of the EXPNO/COND form of
model formula.

John Maindonald.


On 10 Sep 2005, at 8:00 PM, Larry A Sonna wrote:


> From: "Larry A Sonna" <larry_sonna at hotmail.com>
> Date: 10 September 2005 12:10:06 AM
> To: <r-help at stat.math.ethz.ch>
> Subject: [R] Discrepancy between R and SPSS in 2-way, repeated  
> measures ANOVA
>
>
> Dear R community,
>
> I am trying to resolve a discrepancy between the way SPSS and R  
> handle 2-way, repeated measures ANOVA.
>
> An experiment was performed in which samples were drawn before and  
> after treatment of four groups of subjects (control and disease  
> states 1, 2 and 3).  Each group contained five subjects.  An  
> experimental measurement was performed on each sample to yield a  
> "signal".  The before and after treatment signals for each subject  
> were treated as repeated measures.  We desire to obtain P values  
> for disease state ("CONDITION"), and the interaction between signal  
> over time and disease state ("CONDITION*TIME").
>
> Using SPSS, the following output was obtained:
>                      DF        SumSq (Type 3)    Mean Sq    F  
> value     P=
>
> COND              3                 42861            14287        
> 3.645 0.0355
>
> TIME                1                     473                
> 473       0.175 0.681
>
> COND*TIME     3                     975               325        
> 0.120 0.947
>
> Error                16                43219             2701
>
>
>
> By contrast, using the following R command:
>
> summary(aov(SIGNAL~(COND+TIME+COND*TIME)+Error(EXPNO/COND),  
> Type="III"))
>
> the output was as follows:
>
>                  Df     Sum Sq     Mean Sq     F value  Pr(>F)
>
> COND          3          26516       8839      3.2517     0.03651 *
>
> TIME            1            473         473      0.1739     0.67986
>
> COND:TIME  3            975         325      0.1195     0.94785
>
> Residuals     28        76107      2718
>
>
>
> I don't understand why the two results are discrepant.  In  
> particular, I'm not sure why R is yielding 28 DF for the residuals  
> whereas SPSS only yields 16.  Can anyone help?

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From r.darnell at uq.edu.au  Tue Sep 13 03:28:04 2005
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Tue, 13 Sep 2005 11:28:04 +1000
Subject: [R] Translating lme model call to lme4
Message-ID: <43262B24.1020803@uq.edu.au>

I would appreciate help translating the following lme model to an lmer 
function.

lme(lognrms ~ Group*Rotation*muscle*side*support*arms,
                 random=~1|Subject/Stratum2/rep, data=Data)



Many thanks

Ross Darnell
r.darnell at uq.edu.au



From HDoran at air.org  Tue Sep 13 03:42:46 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 12 Sep 2005 21:42:46 -0400
Subject: [R] Translating lme model call to lme4
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407E41C6F@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050912/b06979ca/attachment.pl

From blomsp at ozemail.com.au  Tue Sep 13 04:24:50 2005
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Tue, 13 Sep 2005 12:24:50 +1000
Subject: [R] Translating lme model call to lme4
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7407E41C6F@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7407E41C6F@dc1ex2.air.org>
Message-ID: <6.2.1.2.0.20050913122113.01cbcd78@mail.ozemail.com.au>

There is a slight caveat in that lmer does not respect implicit nesting, so 
you need to make sure your nested groups have unique levels:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/47413.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/47423.html

Simon.

At 11:42 AM 13/09/2005, Doran, Harold wrote:
>Only the random portion will differ as in:
>
>lmer(lognrms ~ Group*Rotation*muscle*side*support*arms + (1|Subject) + 
>(1|Stratum) + (1|rep), Data)
>
>
>-----Original Message-----
>From:   r-help-bounces at stat.math.ethz.ch on behalf of Ross Darnell
>Sent:   Mon 9/12/2005 9:28 PM
>To:     r-help at stat.math.ethz.ch
>Cc:
>Subject:        [R] Translating lme model call to lme4
>
>I would appreciate help translating the following lme model to an lmer
>function.
>
>lme(lognrms ~ Group*Rotation*muscle*side*support*arms,
>                  random=~1|Subject/Stratum2/rep, data=Data)
>
>
>
>Many thanks
>
>Ross Darnell
>r.darnell at uq.edu.au
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C



From weigand.stephen at charter.net  Tue Sep 13 06:09:37 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Mon, 12 Sep 2005 23:09:37 -0500
Subject: [R] Finding a decision tree's leaf node from a new value
In-Reply-To: <001601c5b544$bd9ffeb0$5e7aa8c0@FEUPsig.fe.up.pt>
References: <001601c5b544$bd9ffeb0$5e7aa8c0@FEUPsig.fe.up.pt>
Message-ID: <c7b7b3852b32ec07ea7b2097ebf071aa@charter.net>

It sounds to me like you're looking for the function predict.rpart().

Hope this helps,

Stephen

On Sep 9, 2005, at 8:45 AM, Jo??o Mendes Moreira wrote:

> Dear mailinglist members,
>
> I have the following problem: I run a decision tree using the rpart 
> function and, afterwords, I try to find to which leaf node a new 
> register (not used to build the decision tree) belongs to.
>
> I will try to explain better:
>
> rpart.tree <- rpart(target.value ~., data)
> leaf.node <- new.function(rpart.tree, new.register)
>
> The new register has all the explanatory values used to build the tree 
> but has not the target value.
> The new.function is the function I am trying to write and shall return 
> the leaf.node from the decision tree where it should belong.
>
> What I need ( and I am am not able to) is to reproduce the decision 
> path over my new.register.
>
> Thanks in advance
>
> Joao Moreira



From mcclatchie.sam at saugov.sa.gov.au  Tue Sep 13 07:47:29 2005
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Tue, 13 Sep 2005 15:17:29 +0930
Subject: [R] R CMD INSTALL -l /path/to/library packagename
Message-ID: <BEA6A7E18959A04385DC14D24619F89F01D73C8B@sagemsg0008.sagemsmrd01.sa.gov.au>

Background:
OS: Linux Mandrake 10.1
release: R 2.1.1
editor: GNU Emacs 21.3.2
front-end: ESS 5.2.3

Colleagues

Since I upgraded to R 2.1.1, I am getting a an error message from R CMD
INSTALL packagename that says
R_HOME ('/usr/local/lib/R') not found. 

That's not too surprising, since R is now in /usr/lib/R, but what is
confusing me is that 
R CMD INSTALL -l /path/to/library packagename is giving me the same message,
suggesting that I am misusing the -lib switch when I enter
R CMD INSTALL -l /usr/lib/R/library waveslim_1.5.tar.gz

I'm sure the fix is trivial, but I can't see it for looking in the manual.
Any help appreciated.

Sam
----
Sam McClatchie,
Biological oceanography 
South Australian Aquatic Sciences Centre
PO Box 120, Henley Beach 5022
Adelaide, South Australia
email <mcclatchie.sam at saugov.sa.gov.au>
Cellular: 0431 304 497 
Telephone: (61-8) 8207 5448
FAX: (61-8) 8207 5481
Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
  
                   /\
      ...>><xX(??> 
                //// \\\\
                   <??)Xx><<
              /////  \\\\\\
                        ><(((??> 
  >><(((??>   ...>><xX(??>O<??)Xx><<



From ggrothendieck at gmail.com  Tue Sep 13 08:10:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Sep 2005 02:10:42 -0400
Subject: [R] R CMD INSTALL -l /path/to/library packagename
In-Reply-To: <BEA6A7E18959A04385DC14D24619F89F01D73C8B@sagemsg0008.sagemsmrd01.sa.gov.au>
References: <BEA6A7E18959A04385DC14D24619F89F01D73C8B@sagemsg0008.sagemsmrd01.sa.gov.au>
Message-ID: <971536df0509122310119ce30@mail.gmail.com>

I don't use Linux but perhaps you should check
what environment variables you have defined
and also if you have anything in your *.site
files, if you have them, that could cause that.

On 9/13/05, McClatchie, Sam (PIRSA-SARDI)
<mcclatchie.sam at saugov.sa.gov.au> wrote:
> Background:
> OS: Linux Mandrake 10.1
> release: R 2.1.1
> editor: GNU Emacs 21.3.2
> front-end: ESS 5.2.3
> 
> Colleagues
> 
> Since I upgraded to R 2.1.1, I am getting a an error message from R CMD
> INSTALL packagename that says
> R_HOME ('/usr/local/lib/R') not found.
> 
> That's not too surprising, since R is now in /usr/lib/R, but what is
> confusing me is that
> R CMD INSTALL -l /path/to/library packagename is giving me the same message,
> suggesting that I am misusing the -lib switch when I enter
> R CMD INSTALL -l /usr/lib/R/library waveslim_1.5.tar.gz
> 
> I'm sure the fix is trivial, but I can't see it for looking in the manual.
> Any help appreciated.
> 
> Sam
> ----
> Sam McClatchie,
> Biological oceanography
> South Australian Aquatic Sciences Centre
> PO Box 120, Henley Beach 5022
> Adelaide, South Australia
> email <mcclatchie.sam at saugov.sa.gov.au>
> Cellular: 0431 304 497
> Telephone: (61-8) 8207 5448
> FAX: (61-8) 8207 5481
> Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
> 
>                   /\
>      ...>><xX(??>
>                //// \\\\
>                   <??)Xx><<
>              /////  \\\\\\
>                        ><(((??>
>  >><(((??>   ...>><xX(??>O<??)Xx><<
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From thpe at hhbio.wasser.tu-dresden.de  Tue Sep 13 09:05:44 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Tue, 13 Sep 2005 09:05:44 +0200
Subject: [R] Running DOS command prompt from R 2.0.1 Windows?
In-Reply-To: <1126563776.4325ffc02e76a@webmail.stanford.edu>
References: <1126563776.4325ffc02e76a@webmail.stanford.edu>
Message-ID: <43267A48.6030107@hhbio.wasser.tu-dresden.de>

Mr Toby Daniel Ahrens wrote:
> I am trying to use R (windows version 2.0.1) to manage runs of a program
> that is run from a DOS command prompt.  Is R able to call a DOS prompt?  I
> am hoping that there is something analogous to the "spawn" command in IDL,
> but I can't see to find any help in the R archives...
> 
> Thanks very much,
> 
> Toby

Yes, e.g. system("cmd") to get a "DOS" prompt. See ?system or ?shell for 
details. You may consider to use wait=FALSE if you want a non-modal 
behavior.

HTH Thomas P.



From antoine at ruetter.ch  Tue Sep 13 09:25:34 2005
From: antoine at ruetter.ch (Antoine de Bary)
Date: Tue, 13 Sep 2005 09:25:34 +0200
Subject: [R] Collineariy Diagnostics
Message-ID: <BF4C4B8E.1686%antoine@ruetter.ch>

Hi, and thanks for your help

in order to do collinearity analysis I downloaded the perturb package. I run
a lm (regression) and on that the ??calldiag?? commad to get condition numbers
but i get the following message: the variable XY with modus ??numeric?? was
not found (it does the same with all predictors despite all variables are
numeric and exists).

Can anyone tell me how can I go arround this problem? Is there another way
to have ??condition numbers??? What about VIF?

Please return message to: antoine at ruetter.ch

Thanks a lot

Antoine



From Ted.Harding at nessie.mcc.ac.uk  Tue Sep 13 10:23:24 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Sep 2005 09:23:24 +0100 (BST)
Subject: [R] Floating-point arithmetic
Message-ID: <XFMail.050913092324.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

A recent exchange on the 'octave' list led to the following
paper being cited, which I had not met before:

  What Every Computer Scientist Should Know About
  Floating-Point Arithmetic, by David Goldberg,

originally published in the March, 1991 issue of Computing Surveys.

PDF and HTML versions are widely available on the web (see Google),
e.g. at

http://www.physics.ohio-state.edu/~dws/grouplinks/floating_point_math.pdf

Though over the years I've many times been there, done that,
got bitten by the bugs, and bought the T-shirt [I have one from
Florida depicting a large and joyful mosquito with proboscis
dripping with blood] I still found this a revealing read, written
in a style well-suited to general users of numerical computer
languages.

So I'm writing to bring it to the notice of R users who may not
have come across it.

(And also, maybe, to pre-empt that question that some of you may
have been thinking of asking ... ).

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Sep-05                                       Time: 08:30:03
------------------------------ XFMail ------------------------------



From chrish at stats.ucl.ac.uk  Tue Sep 13 11:05:31 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Tue, 13 Sep 2005 10:05:31 +0100 (BST)
Subject: [R] Document clustering for R
In-Reply-To: <4325DB65.4090504@llnl.gov>
References: <4325DB65.4090504@llnl.gov>
Message-ID: <Pine.LNX.4.58.0509131000500.30170@egon.stats.ucl.ac.uk>

If you are able to implement the computation of the distance matrix, you
can use methods such as pam, agnes and hclust, which operate on
dissimilarity matrices of any kind. You may also perform a
multidimensional scaling with isoMDS, sammon or cmdscale and use any
clustering algorithm for n*p data on the outcome.

Best,
Christian

On Mon, 12 Sep 2005, Raymond K Pon wrote:

> I'm working on a project related to document clustering. I know that R
> has clustering algorithms such as clara, but only supports two distance
> metrics: euclidian and manhattan, which are not very useful for
> clustering documents. I was wondering how easy it would be to extend the
> clustering package in R to support other distance metrics, such as
> cosine distance, or if there was an API for custom distance metrics.
>
> Best regards,
> Raymond Pon
> pon3 at llnl.gov
> x43062
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From jpritikin at pobox.com  Tue Sep 13 11:24:52 2005
From: jpritikin at pobox.com (Joshua N Pritikin)
Date: Tue, 13 Sep 2005 14:54:52 +0530
Subject: [R] remedial stats education
In-Reply-To: <432587CB.8010403@7d4.com>
References: <20050912121553.GG31014@always.joy.eth.net>
	<432587CB.8010403@7d4.com>
Message-ID: <20050913092452.GL31014@always.joy.eth.net>

Everybody, thanks for your suggestions.  I am planning to order at least these
two:

  Peter Dalgaard _Introductory Statistics with R_
  Andrew Gelman _Bayesian Data Analysis_

Also, while stumbling around amazon.com, I found this book:

  Statistical Reasoning in Psychology and Education, 4th edition
  by Edward W. Minium, Bruce M. King  ISBN: 0471211877

This book looks perfect for me -- I can do math, learn R, etc, but WHY?  Why
use one method and not another?  Why structure a problem one way and not
another?  These are the questions which really stump me.

So, has anybody read this book?  Is there a similar book which is even better?

-- 
Make April 15 just another day, visit http://fairtax.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050913/a4cc3fee/attachment.bin

From David.Ruau at rwth-aachen.de  Tue Sep 13 11:50:03 2005
From: David.Ruau at rwth-aachen.de (David Ruau)
Date: Tue, 13 Sep 2005 11:50:03 +0200
Subject: [R] Document clustering for R
In-Reply-To: <4325DB65.4090504@llnl.gov>
References: <4325DB65.4090504@llnl.gov>
Message-ID: <4564cee8b2df8071f6dc53f451129cbf@rwth-aachen.de>

Hi,
We discovered that the package "amap" contain a distance calculation 
function call Dist which can calculate the distance according to a 
method call "pearson" which is in fact the "not centered Pearson" which 
seems to be the cosine distance.
Could you tell me what do you think on that?

Best regards,
David

On Sep 12, 2005, at 21:47, Raymond K Pon wrote:

> I'm working on a project related to document clustering. I know that R
> has clustering algorithms such as clara, but only supports two distance
> metrics: euclidian and manhattan, which are not very useful for
> clustering documents. I was wondering how easy it would be to extend 
> the
> clustering package in R to support other distance metrics, such as
> cosine distance, or if there was an API for custom distance metrics.
>
> Best regards,
> Raymond Pon
> pon3 at llnl.gov
> x43062
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Tue Sep 13 12:16:22 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 13 Sep 2005 05:16:22 -0500
Subject: [R] Collineariy Diagnostics
In-Reply-To: <BF4C4B8E.1686%antoine@ruetter.ch>
References: <BF4C4B8E.1686%antoine@ruetter.ch>
Message-ID: <4326A6F6.9020609@pdf.com>



Antoine de Bary wrote:
> Hi, and thanks for your help
> 
> in order to do collinearity analysis I downloaded the perturb package. I run
> a lm (regression) and on that the ??calldiag?? commad to get condition numbers
> but i get the following message: the variable XY with modus ??numeric?? was
> not found (it does the same with all predictors despite all variables are
> numeric and exists).
> 
> Can anyone tell me how can I go arround this problem? Is there another way
> to have ??condition numbers??? What about VIF?
> 
> Please return message to: antoine at ruetter.ch

I cannot comment on the "perturb" package. However for condition numbers 
see ?kappa.lm, and for variance inflation factors see ?vif. The latter 
is in the Design package.

set.seed(1)
x1 <- rnorm(100)
x2 <- x1 + 0.1 * rnorm(100)
y  <- rnorm(100)
f  <- lm(y ~ x1 + x2)

vif(f)
kappa(f)

HTH,

--sundar



From jfox at mcmaster.ca  Tue Sep 13 13:00:49 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 13 Sep 2005 07:00:49 -0400
Subject: [R] Collineariy Diagnostics
In-Reply-To: <4326A6F6.9020609@pdf.com>
Message-ID: <20050913110048.XHUB1799.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Sundar and Antoine,

In addition, the vif function in the car package will calculate generalized
variance inflation factors.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sundar 
> Dorai-Raj
> Sent: Tuesday, September 13, 2005 5:16 AM
> To: Antoine de Bary
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Collineariy Diagnostics
> 
> 
> 
> Antoine de Bary wrote:
> > Hi, and thanks for your help
> > 
> > in order to do collinearity analysis I downloaded the 
> perturb package. 
> > I run a lm (regression) and on that the ??calldiag?? commad to get 
> > condition numbers but i get the following message: the variable XY 
> > with modus ??numeric?? was not found (it does the same with all 
> > predictors despite all variables are numeric and exists).
> > 
> > Can anyone tell me how can I go arround this problem? Is 
> there another 
> > way to have ??condition numbers??? What about VIF?
> > 
> > Please return message to: antoine at ruetter.ch
> 
> I cannot comment on the "perturb" package. However for 
> condition numbers see ?kappa.lm, and for variance inflation 
> factors see ?vif. The latter is in the Design package.
> 
> set.seed(1)
> x1 <- rnorm(100)
> x2 <- x1 + 0.1 * rnorm(100)
> y  <- rnorm(100)
> f  <- lm(y ~ x1 + x2)
> 
> vif(f)
> kappa(f)
> 
> HTH,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From v.schlecht at arcor.de  Tue Sep 13 13:08:55 2005
From: v.schlecht at arcor.de (v.schlecht@arcor.de)
Date: Tue, 13 Sep 2005 13:08:55 +0200 (CEST)
Subject: [R] How to erase objects
Message-ID: <17831268.1126609735403.JavaMail.ngmail@webmail-03.arcor-online.net>


Hi, I admit that I rather carelessly built lots of large objects and therefore ran out of memory. Most objects, which I have at the moment are now unnecessary, but it would take a lot of time to recreate the last few ones from scratch. So I would like to erase (get rid of) selected objects in order to be able to continue. Does anyone know how I could do this? 

Machen Sie aus 14 Cent spielend bis zu 100 Euro!
Die neue Gaming-Area von Arcor - ??ber 50 Onlinespiele im Angebot.
http://www.arcor.de/rd/emf-gaming-1



From liuwensui at gmail.com  Tue Sep 13 13:27:31 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 13 Sep 2005 07:27:31 -0400
Subject: [R] VB and R
Message-ID: <1115a2b0050913042747ff73de@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050913/9198a70d/attachment.pl

From Luisr at frs.fo  Tue Sep 13 13:32:13 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Tue, 13 Sep 2005 12:32:13 +0100
Subject: [R] How to erase objects
Message-ID: <s326c6d0.029@ffdata.setur.fo>

?rm


>>> <v.schlecht at arcor.de> 13/09/2005 12:08:55 >>>

Hi, I admit that I rather carelessly built lots of large objects and therefore ran out of memory. Most objects, which I have at the moment are now unnecessary, but it would take a lot of time to recreate the last few ones from scratch. So I would like to erase (get rid of) selected objects in order to be able to continue. Does anyone know how I could do this? 

Machen Sie aus 14 Cent spielend bis zu 100 Euro!
Die neue Gaming-Area von Arcor - ??ber 50 Onlinespiele im Angebot.
http://www.arcor.de/rd/emf-gaming-1 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From JAROSLAW.W.TUSZYNSKI at saic.com  Tue Sep 13 13:38:23 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Tue, 13 Sep 2005 07:38:23 -0400
Subject: [R] VB and R
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD503858801@us-arlington-0668.mail.saic.com>

I belive R, which is platform independent, does not work with any platform
specific software or languages, like Visual Basic. Can you write your code
by passing information through input/output files (CSV & HML might be good
formats)?

Jarek Tuszynski
SAIC

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] 
Sent: Tuesday, September 13, 2005 7:28 AM
To: r-help at stat.math.ethz.ch
Subject: [R] VB and R

Dear Listers,

Is there any good paper about how to use R together with VB?

Thank you so much!

--
WenSui Liu
(http://statcompute.blogspot.com)
Senior Decision Support Analyst
Cincinnati Children Hospital Medical Center

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tmlammail at yahoo.com  Tue Sep 13 13:39:45 2005
From: tmlammail at yahoo.com (Martin Lam)
Date: Tue, 13 Sep 2005 04:39:45 -0700 (PDT)
Subject: [R] How to erase objects
In-Reply-To: <s326c6d0.029@ffdata.setur.fo>
Message-ID: <20050913113945.44037.qmail@web40527.mail.yahoo.com>

# to see the objects that are currently in memory
objects()

# to remove everything
rm(list = ls())

HTH,

Martin

--- Luis Ridao Cruz <Luisr at frs.fo> wrote:

> ?rm
> 
> 
> >>> <v.schlecht at arcor.de> 13/09/2005 12:08:55 >>>
> 
> Hi, I admit that I rather carelessly built lots of
> large objects and therefore ran out of memory. Most
> objects, which I have at the moment are now
> unnecessary, but it would take a lot of time to
> recreate the last few ones from scratch. So I would
> like to erase (get rid of) selected objects in order
> to be able to continue. Does anyone know how I could
> do this? 
> 
> Machen Sie aus 14 Cent spielend bis zu 100 Euro!
> Die neue Gaming-Area von Arcor - ??ber 50
> Onlinespiele im Angebot.
> http://www.arcor.de/rd/emf-gaming-1 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Tue Sep 13 13:54:14 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 13 Sep 2005 13:54:14 +0200 (CEST)
Subject: [R] VB and R
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD503858801@us-arlington-0668.mail.saic.com>
Message-ID: <Pine.LNX.4.44.0509131348130.28550-100000@reclus.nhh.no>

On Tue, 13 Sep 2005, Tuszynski, Jaroslaw W. wrote:

> I belive R, which is platform independent, does not work with any platform
> specific software or languages, like Visual Basic. Can you write your code
> by passing information through input/output files (CSV & HML might be good
> formats)?

R can be used in a (D)COM context - see:

http://cran.r-project.org/contrib/extra/dcom

and can be embedded in other software systems too. To quote from:

http://cran.r-project.org/contrib/extra/dcom/RSrv135.html

"R (D)COM server provides a mechanism for standard applications like 
Microsoft Excel or custom applications written in any language serving as 
a COM client (e.g. Visual Basic, Perl) to use the R as a powerful 
computational engine and renderer for graphics and text output."

Because there are so many possibilities, the "good paper" is in the 
examples, there isn't (as far as I know) a complete manual.

> 
> Jarek Tuszynski
> SAIC
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] 
> Sent: Tuesday, September 13, 2005 7:28 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] VB and R
> 
> Dear Listers,
> 
> Is there any good paper about how to use R together with VB?
> 
> Thank you so much!
> 
> --
> WenSui Liu
> (http://statcompute.blogspot.com)
> Senior Decision Support Analyst
> Cincinnati Children Hospital Medical Center
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ramasamy at cancer.org.uk  Tue Sep 13 14:59:19 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 13 Sep 2005 13:59:19 +0100
Subject: [R] How to erase objects
In-Reply-To: <20050913113945.44037.qmail@web40527.mail.yahoo.com>
References: <20050913113945.44037.qmail@web40527.mail.yahoo.com>
Message-ID: <1126616360.5996.48.camel@ipc143004.lif.icnet.uk>

You can also use ls() which is alias of objects().

One way is to remove unwanted objects by hand
 rm(a,b,c,d)

Another way is to save the required objects, remove everything and then
load the saved objects.
 save(x,y,z, file="out.rda", compress=TRUE)
 rm( list=ls() )
 load("out.rda")

The second approach allows you to load the objects in a fresh R session,
which is one way to release memory if gc() fails. But which approach you
prefer depends on the number of objects to be removed and saved.

Regards, Adai



On Tue, 2005-09-13 at 04:39 -0700, Martin Lam wrote:
> # to see the objects that are currently in memory
> objects()
> 
> # to remove everything
> rm(list = ls())
> 
> HTH,
> 
> Martin
> 
> --- Luis Ridao Cruz <Luisr at frs.fo> wrote:
> 
> > ?rm
> > 
> > 
> > >>> <v.schlecht at arcor.de> 13/09/2005 12:08:55 >>>
> > 
> > Hi, I admit that I rather carelessly built lots of
> > large objects and therefore ran out of memory. Most
> > objects, which I have at the moment are now
> > unnecessary, but it would take a lot of time to
> > recreate the last few ones from scratch. So I would
> > like to erase (get rid of) selected objects in order
> > to be able to continue. Does anyone know how I could
> > do this? 
> > 
> > Machen Sie aus 14 Cent spielend bis zu 100 Euro!
> > Die neue Gaming-Area von Arcor - Ã¼ber 50
> > Onlinespiele im Angebot.
> > http://www.arcor.de/rd/emf-gaming-1 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Tue Sep 13 14:36:21 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Sep 2005 08:36:21 -0400
Subject: [R] VB and R
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD503858801@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD503858801@us-arlington-0668.mail.saic.com>
Message-ID: <4326C7C5.8010207@stats.uwo.ca>

On 9/13/2005 7:38 AM, Tuszynski, Jaroslaw W. wrote:
> I belive R, which is platform independent, does not work with any platform
> specific software or languages, like Visual Basic. 

That's not true at all.  R works with many platform specific packages. 
I think Roger Bivand's answer covers the easiest way (COM) to link to 
VB, but you can also use shared libraries (DLL's in Windows) created in 
just about any package through the dyn.load() facility.

Duncan Murdoch

 > Can you write your code
> by passing information through input/output files (CSV & HML might be good
> formats)?
> 
> Jarek Tuszynski
> SAIC
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] 
> Sent: Tuesday, September 13, 2005 7:28 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] VB and R
> 
> Dear Listers,
> 
> Is there any good paper about how to use R together with VB?
> 
> Thank you so much!
> 
> --
> WenSui Liu
> (http://statcompute.blogspot.com)
> Senior Decision Support Analyst
> Cincinnati Children Hospital Medical Center
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Tue Sep 13 14:46:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Sep 2005 08:46:47 -0400
Subject: [R] VB and R
In-Reply-To: <1115a2b0050913042747ff73de@mail.gmail.com>
References: <1115a2b0050913042747ff73de@mail.gmail.com>
Message-ID: <971536df0509130546450bb3bd@mail.gmail.com>

On 9/13/05, Wensui Liu <liuwensui at gmail.com> wrote:
> Dear Listers,
> 
> Is there any good paper about how to use R together with VB?

I assume you are looking to write a VB GUI with an R backend.
You might want to look at the R GUIs page and its mailing list to
see what other people are doing in that area.

>From the R home page click on Related Projects in the left pane
and then click on R GUIs.



From phgrosjean at sciviews.org  Tue Sep 13 15:05:57 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 13 Sep 2005 15:05:57 +0200
Subject: [R] VB and R
In-Reply-To: <971536df0509130546450bb3bd@mail.gmail.com>
References: <1115a2b0050913042747ff73de@mail.gmail.com>
	<971536df0509130546450bb3bd@mail.gmail.com>
Message-ID: <4326CEB5.9030600@sciviews.org>

Hello,

You can look at SciViews-R (http://www.sciviews.org/Sciviews-R). It is 
written in VB6. source code is available.
Best,

Philippe Grosjean

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Gabor Grothendieck wrote:
> On 9/13/05, Wensui Liu <liuwensui at gmail.com> wrote:
> 
>>Dear Listers,
>>
>>Is there any good paper about how to use R together with VB?
> 
> 
> I assume you are looking to write a VB GUI with an R backend.
> You might want to look at the R GUIs page and its mailing list to
> see what other people are doing in that area.
> 
>>From the R home page click on Related Projects in the left pane
> and then click on R GUIs.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From jarioksa at sun3.oulu.fi  Tue Sep 13 15:30:22 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Tue, 13 Sep 2005 16:30:22 +0300
Subject: [R] Document clustering for R
In-Reply-To: <4325DB65.4090504@llnl.gov>
References: <4325DB65.4090504@llnl.gov>
Message-ID: <1126618222.9521.5.camel@biol102145.oulu.fi>

On Mon, 2005-09-12 at 12:47 -0700, Raymond K Pon wrote:
> I'm working on a project related to document clustering. I know that R 
> has clustering algorithms such as clara, but only supports two distance 
> metrics: euclidian and manhattan, which are not very useful for 
> clustering documents. I was wondering how easy it would be to extend the 
> clustering package in R to support other distance metrics, such as 
> cosine distance, or if there was an API for custom distance metrics.
> 
You don't have to extend the "clustering package in R to support other
distance metrics", but you should take care that you produce your
dissimilarities (or distances) in the standard format so that they can
be used in "clustering package" or in cmdscale or in isoMDS or any other
function excepting a "dist" object.  "Clustering package" will support
new dissimilarities if they were written in standard conforming way.
There are several packages that offer alternative dissimilarities (and
some even distances) that can be used in clustering functions. Look for
"distances" or "dissimilarities" in the R Site. Some of these could be
the one for you... I would be surprised if cosine index is missing (and
if needed, I could write it for you in C, but I don't think that is
necessary).

cheers, jari oksanen



From mcardeal at ufba.br  Tue Sep 13 15:29:12 2005
From: mcardeal at ufba.br (=?ISO-8859-1?Q?Carlos_Maur=EDcio_Cardeal_Mendes?=)
Date: Tue, 13 Sep 2005 10:29:12 -0300
Subject: [R] if() command
Message-ID: <4326D428.2070700@ufba.br>

Hi everyone !

Could you please help me with this problem ?

I??ve trying to write a code that assign to a variable the content from 
another, but all I??ve got is a message error. For example:

if (age <=10) {group == 1}
else if (age > 10 & age <= 20) {group == 2}
else {group == 3}

Syntax error

Or

if (age <=10) {group == 1}
else (age > 10 & age <= 20) {group == 2}
else {group == 3}

Syntax error

I know that is possible to find the solution by ifelse command or even 
recode command, but I??d like to use this way, because I can add another 
variable as a new condition and I believe to expand the possibilites.

Thanks,
Mauricio



From sundar.dorai-raj at pdf.com  Tue Sep 13 15:41:23 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 13 Sep 2005 08:41:23 -0500
Subject: [R] if() command
In-Reply-To: <4326D428.2070700@ufba.br>
References: <4326D428.2070700@ufba.br>
Message-ID: <4326D703.5060706@pdf.com>



Carlos Maur??cio Cardeal Mendes wrote:
> Hi everyone !
> 
> Could you please help me with this problem ?
> 
> I??ve trying to write a code that assign to a variable the content from 
> another, but all I??ve got is a message error. For example:
> 
> if (age <=10) {group == 1}
> else if (age > 10 & age <= 20) {group == 2}
> else {group == 3}
> 
> Syntax error
> 
> Or
> 
> if (age <=10) {group == 1}
> else (age > 10 & age <= 20) {group == 2}
> else {group == 3}
> 
> Syntax error
> 
> I know that is possible to find the solution by ifelse command or even 
> recode command, but I??d like to use this way, because I can add another 
> variable as a new condition and I believe to expand the possibilites.
> 
> Thanks,
> Mauricio
> 

Because the following line is syntatically correct:

if (age <=10) {group == 1}

the R parser does not expect the following:

else (age > 10 & age <= 20) {group == 2}
else {group == 3}

causing a sytax error. Instead, you want:

if (age <=10) {
   group == 1
} else (age > 10 & age <= 20) {
   group == 2
} else {
   group == 3
}

HTH,

--sundar



From chrish at stats.ucl.ac.uk  Tue Sep 13 15:50:08 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Tue, 13 Sep 2005 14:50:08 +0100 (BST)
Subject: [R] Document clustering for R
In-Reply-To: <1126618222.9521.5.camel@biol102145.oulu.fi>
References: <4325DB65.4090504@llnl.gov>
	<1126618222.9521.5.camel@biol102145.oulu.fi>
Message-ID: <Pine.LNX.4.58.0509131447130.30170@egon.stats.ucl.ac.uk>

On Tue, 13 Sep 2005, Jari Oksanen wrote:

> On Mon, 2005-09-12 at 12:47 -0700, Raymond K Pon wrote:
> > I'm working on a project related to document clustering. I know that R
> > has clustering algorithms such as clara, but only supports two distance
> > metrics: euclidian and manhattan, which are not very useful for
> > clustering documents. I was wondering how easy it would be to extend the
> > clustering package in R to support other distance metrics, such as
> > cosine distance, or if there was an API for custom distance metrics.
> >
> You don't have to extend the "clustering package in R to support other
> distance metrics", but you should take care that you produce your
> dissimilarities (or distances) in the standard format so that they can
> be used in "clustering package" or in cmdscale or in isoMDS or any other
> function excepting a "dist" object.  "Clustering package" will support
> new dissimilarities if they were written in standard conforming way.
> There are several packages that offer alternative dissimilarities (and
> some even distances) that can be used in clustering functions. Look for
> "distances" or "dissimilarities" in the R Site. Some of these could be
> the one for you... I would be surprised if cosine index is missing (and
> if needed, I could write it for you in C, but I don't think that is
> necessary).

Generation of the standard dist format out of a distance
matrix m works simply by as.dist(m).

Christian


*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From bernarduse1 at yahoo.fr  Tue Sep 13 16:05:39 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Tue, 13 Sep 2005 16:05:39 +0200 (CEST)
Subject: [R] plot glmmPQL
Message-ID: <20050913140540.48048.qmail@web25801.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050913/b4749be4/attachment.pl

From vincent at 7d4.com  Tue Sep 13 16:08:09 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Tue, 13 Sep 2005 16:08:09 +0200
Subject: [R] if() command
In-Reply-To: <4326D428.2070700@ufba.br>
References: <4326D428.2070700@ufba.br>
Message-ID: <4326DD49.6090204@7d4.com>

not enough parenthesis
hih



From petr.pikal at precheza.cz  Tue Sep 13 16:10:11 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 13 Sep 2005 16:10:11 +0200
Subject: [R] trouble with reading data from excel
In-Reply-To: <1115a2b005091212236c84e0ea@mail.gmail.com>
References: <20050912185847.BBC9F23D09@ws5-3.us4.outblaze.com>
Message-ID: <4326F9E3.6157.166CFF5@localhost>

Hi

On 12 Sep 2005 at 15:23, Wensui Liu wrote:

> Sloan,
> 
> You don't need to save xls as csv. Actually, R reads data in excel
> very well. Following code is cutted from my blog and HTH.
> 
> library(RODBC);
> 
> ###########################################################
> # 1. READ DATA FROM EXCEL INTO R #
> ###########################################################
> xlsConnect<-odbcConnectExcel("C:\\temp\\demo.xls");
> demo<-sqlFetch(xlsConnect, "Sheet1");
> odbcClose(xlsConnect);
> rm(demo);

or if you are on Windows

open Excel
select what you want to read, including header
press ctrl-C
in R
mydata<-read.delim("clipboard")

to write from R

write.table(tab, "clipboard", sep = "\t", row.names = F)
open Excel
press ctrl-V

HTH
Petr

> 
> 
> On 9/12/05, sloan jones <sledepi at operamail.com> wrote:
> > 
> > I have been trying to open data that I have saved in an excel spread
> > sheet. I saved it as a csv. Then I tried using the read.csv command.
> > However, everytime I do this--
> > 
> > diseasedat<-read.csv("M:/sloan/R/disease/disease.csv", sep=, header
> > = TRUE, fill= TRUE)--
> > 
> > I get an error message:
> > 
> > Error in file(file, "r") : unable to open connection
> > In addition: Warning message:
> > cannot open file 'M:/sloan/R/disease/disease.csv'
> > 
> > What am I doing wrong or what should I look for to correct this?
> > 
> > Sloan
> > 
> > --
> > _______________________________________________
> > Surf the Web in a faster, safer and easier way:
> > Download Opera 8 at http://www.opera.com
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> 
> 
> -- 
> WenSui Liu
> (http://statcompute.blogspot.com)
> Senior Decision Support Analyst
> Cincinnati Children Hospital Medical Center
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From jpritikin at pobox.com  Tue Sep 13 16:25:02 2005
From: jpritikin at pobox.com (Joshua N Pritikin)
Date: Tue, 13 Sep 2005 19:55:02 +0530
Subject: [R] [kjbeath@kagi.com: Remedial stats]
Message-ID: <20050913142502.GQ31014@always.joy.eth.net>

It seems like the Bayesian folks like to hide.  Hey, I just want to get the
job done whether using frequency or Bayesian stats.  Can anybody suggest
a good introduction to Bayesian Analysis?

----- Forwarded message from Ken Beath <kjbeath at kagi.com> -----

To: Joshua N Pritikin <jpritikin at pobox.com>
From: Ken Beath <kjbeath at kagi.com>
Subject: Remedial stats

Hi,

Gelman's book "Bayesian Data Analysis' while excellent tends to move  
through the introductory material at a rapid pace.  You will need to  
have a good understanding of things like conditional probability.  
There are other books which may be better. Peter Lee "Bayesian  
Statistics: an introduction" is good on the basic theory but may be a  
bit light on the Gibbs sampler.  There are probably other good books,  
I'm not really a Bayesian so I don't know them off hand.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050913/eb1f815a/attachment.bin

From petr.pikal at precheza.cz  Tue Sep 13 16:24:25 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 13 Sep 2005 16:24:25 +0200
Subject: [R] if() command
In-Reply-To: <4326D428.2070700@ufba.br>
Message-ID: <4326FD39.7418.173D7F4@localhost>



From tlumley at u.washington.edu  Tue Sep 13 16:27:19 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 13 Sep 2005 07:27:19 -0700 (PDT)
Subject: [R] Floating-point arithmetic
In-Reply-To: <XFMail.050913092324.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050913092324.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.63a.0509130724450.2610@homer23.u.washington.edu>

On Tue, 13 Sep 2005, Ted.Harding at nessie.mcc.ac.uk wrote:
> A recent exchange on the 'octave' list led to the following
> paper being cited, which I had not met before:
>
>  What Every Computer Scientist Should Know About
>  Floating-Point Arithmetic, by David Goldberg,
<snip>
>
> So I'm writing to bring it to the notice of R users who may not
> have come across it.
>
> (And also, maybe, to pre-empt that question that some of you may
> have been thinking of asking ... ).

Since the paper is already linked from the relevant R FAQ question, there 
surely wouldn't be any of these ;-)


 	-thomas



From petr.pikal at precheza.cz  Tue Sep 13 16:58:59 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 13 Sep 2005 16:58:59 +0200
Subject: [R] if() command
Message-ID: <43270553.7807.1937D65@localhost>

Hallo 


On 13 Sep 2005 at 10:29, Carlos Maur??cio Cardeal Mende wrote: 

> Hi everyone ! 
>  
> Could you please help me with this problem ? 
>  
> I??ve trying to write a code that assign to a variable the content 
from 
> another, but all I??ve got is a message error. For example: 
>  
> if (age <=10) {group == 1} 
> else if (age > 10 & age <= 20) {group == 2} 
> else {group == 3} 

if you put your statement on one line it works (at least it does not  
give you syntax error) but the result is hardly what you really  
expect 

age<-sample(seq(10,50,10), 20, replace=T) 

if (age <=10) {group <- 1} else if (age > 10 & age <= 20) {group 
<- 2} else {group <- 3} 
if (age <=10) {group == 1} else if (age > 10 & age <= 20) {group 
== 2} else {group == 3} 

Maybe you want something like 

group<-as.numeric(cut(age,c(0,10,20,100))) 

but it is only guess 

HTH 
Petr 

>  
> Syntax error 
>  
> Or 
>  
> if (age <=10) {group == 1} 
> else (age > 10 & age <= 20) {group == 2} 
> else {group == 3} 
>  
> Syntax error 
>  
> I know that is possible to find the solution by ifelse command or 
even 
> recode command, but I??d like to use this way, because I can add 
> another variable as a new condition and I believe to expand the 
> possibilites. 
>  
> Thanks, 
> Mauricio 
>  
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 
Petr Pikal
petr.pikal at precheza.cz



From reid_huntsinger at merck.com  Tue Sep 13 17:01:03 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 13 Sep 2005 11:01:03 -0400
Subject: [R] if() command
Message-ID: <355C35514FEAC9458F75947F5270974D076CBF@usctmx1103.merck.com>

First, "==" is logical comparison, so if you want to create a variable based
on both "age" and "group" you can do that. However, it looks like you want
to define the variable "group", so you want to use "<-" or "=" for that. 

Second, if you're typing this at a command prompt, you need to make sure you
tell R you're not finished when it looks like you could be. There are
several ways to do this. One is to put everything inside braces; another is
to deliberately leave lines incomplete, like

if (age <= 10) {
   group <- 1
} else {
   if (age <= 20) {
      group <- 2
   } else group <- 3
}

Third, this will work for a vector of length 1. If you want to take a vector
"age" and produce a corresponding vector "group", you'll need to put this in
a loop, or use "lapply", or some iteration.

Fourth, you can also write the above as 

> group <- if (age <= 10) 1 else if (age <= 20) 2 else 3

that is, if() returns a value you can assign.

Finally, besides "ifelse" you can use "cut" for this particular task.

Reid Huntsinger


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Carlos Maur??cio
Cardeal Mendes
Sent: Tuesday, September 13, 2005 9:29 AM
To: r-help at stat.math.ethz.ch
Subject: [R] if() command


Hi everyone !

Could you please help me with this problem ?

I??ve trying to write a code that assign to a variable the content from 
another, but all I??ve got is a message error. For example:

if (age <=10) {group == 1}
else if (age > 10 & age <= 20) {group == 2}
else {group == 3}

Syntax error

Or

if (age <=10) {group == 1}
else (age > 10 & age <= 20) {group == 2}
else {group == 3}

Syntax error

I know that is possible to find the solution by ifelse command or even 
recode command, but I??d like to use this way, because I can add another 
variable as a new condition and I believe to expand the possibilites.

Thanks,
Mauricio

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From kingroi at hotmail.com  Tue Sep 13 17:32:23 2005
From: kingroi at hotmail.com (paul king)
Date: Tue, 13 Sep 2005 15:32:23 +0000
Subject: [R] Any S-plus/R doc or manual for SAS user out there?
In-Reply-To: <6.2.1.2.0.20050913133124.01c523b0@mail.pse.unige.ch>
Message-ID: <BAY107-F8B9A6E549C4F27B0249CEB09C0@phx.gbl>

Dear list,

I am looking for an Splus or R doc / manual for SAS user.

Thank you in advance. PK



From S.O.Nyangoma at amc.uva.nl  Tue Sep 13 17:59:24 2005
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Tue, 13 Sep 2005 17:59:24 +0200
Subject: [R] where is eda library (Prof. Ripley's Exploratory Data Analysis
	Library)
Message-ID: <59c8c659fd20.59fd2059c8c6@amc.uva.nl>

Hi 
I want to install eda library. Where can I download it?
I use R version 2.1.0 on Linux.

Thanks.
Stephen.



----- Original Message -----
From: Luis Ridao Cruz <Luisr at frs.fo>
Date: Tuesday, September 13, 2005 1:32 pm
Subject: Re: [R] How to erase objects

> ?rm
> 
> 
> >>> <v.schlecht at arcor.de> 13/09/2005 12:08:55 >>>
> 
> Hi, I admit that I rather carelessly built lots of large objects 
> and therefore ran out of memory. Most objects, which I have at the 
> moment are now unnecessary, but it would take a lot of time to 
> recreate the last few ones from scratch. So I would like to erase 
> (get rid of) selected objects in order to be able to continue. 
> Does anyone know how I could do this? 
> 
> Machen Sie aus 14 Cent spielend bis zu 100 Euro!
> Die neue Gaming-Area von Arcor - ??ber 50 Onlinespiele im Angebot.
> http://www.arcor.de/rd/emf-gaming-1 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From chrisb at fcdarwin.org.ec  Tue Sep 13 17:02:00 2005
From: chrisb at fcdarwin.org.ec (Chris Buddenhagen)
Date: Tue, 13 Sep 2005 10:02:00 -0500
Subject: [R] multiple comparisons for proportions
Message-ID: <003301c5b874$17fd4ac0$4c01a8c0@Chris>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050913/18a3ad04/attachment.pl

From ernesto at ipimar.pt  Tue Sep 13 18:06:57 2005
From: ernesto at ipimar.pt (ernesto)
Date: Tue, 13 Sep 2005 17:06:57 +0100
Subject: [R] where is eda library (Prof. Ripley's Exploratory Data
 Analysis Library)
In-Reply-To: <59c8c659fd20.59fd2059c8c6@amc.uva.nl>
References: <59c8c659fd20.59fd2059c8c6@amc.uva.nl>
Message-ID: <4326F921.8040606@ipimar.pt>

S.O. Nyangoma wrote:

>Hi 
>I want to install eda library. Where can I download it?
>I use R version 2.1.0 on Linux.
>
>Thanks.
>Stephen.
>
>
>  
>
> library(eda)
Warning message:
package 'eda' has been merged into 'stats'

EJ



From vdemart1 at tin.it  Tue Sep 13 18:07:46 2005
From: vdemart1 at tin.it (vittorio)
Date: Tue, 13 Sep 2005 18:07:46 +0200
Subject: [R] Reading data from a serial port
Message-ID: <200509131807.46497.vdemart1@tin.it>

I need to read data from from a medical appliance via the serial port. This 
medical appliance produces streams of set data at regular intervals.
What commands,  packages are available for this purpose?

Vittorio



From ligges at statistik.uni-dortmund.de  Tue Sep 13 18:08:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 13 Sep 2005 18:08:56 +0200
Subject: [R] where is eda library (Prof. Ripley's Exploratory Data
 Analysis Library)
In-Reply-To: <59c8c659fd20.59fd2059c8c6@amc.uva.nl>
References: <59c8c659fd20.59fd2059c8c6@amc.uva.nl>
Message-ID: <4326F998.80401@statistik.uni-dortmund.de>

S.O. Nyangoma wrote:

> Hi 
> I want to install eda library. Where can I download it?
> I use R version 2.1.0 on Linux.


What about typing

 > library(eda)
Warning message:
package 'eda' has been merged into 'stats'


Now we see immediately that we alreeady have access to the functions 
that *were* included in *package* "eda".

Uwe Ligges



> Thanks.
> Stephen.
> 
> 
> 
> ----- Original Message -----
> From: Luis Ridao Cruz <Luisr at frs.fo>
> Date: Tuesday, September 13, 2005 1:32 pm
> Subject: Re: [R] How to erase objects
> 
> 
>>?rm
>>
>>
>>
>>>>><v.schlecht at arcor.de> 13/09/2005 12:08:55 >>>
>>
>>Hi, I admit that I rather carelessly built lots of large objects 
>>and therefore ran out of memory. Most objects, which I have at the 
>>moment are now unnecessary, but it would take a lot of time to 
>>recreate the last few ones from scratch. So I would like to erase 
>>(get rid of) selected objects in order to be able to continue. 
>>Does anyone know how I could do this? 
>>
>>Machen Sie aus 14 Cent spielend bis zu 100 Euro!
>>Die neue Gaming-Area von Arcor - ??ber 50 Onlinespiele im Angebot.
>>http://www.arcor.de/rd/emf-gaming-1 
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help 
>>PLEASE do read the posting guide! http://www.R-project.org/posting-
>>guide.html
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-
>>guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Tue Sep 13 18:16:12 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 13 Sep 2005 11:16:12 -0500
Subject: [R] where is eda library (Prof. Ripley's Exploratory Data
 Analysis Library)
In-Reply-To: <59c8c659fd20.59fd2059c8c6@amc.uva.nl>
References: <59c8c659fd20.59fd2059c8c6@amc.uva.nl>
Message-ID: <4326FB4C.5050409@pdf.com>

Three things:

1. Please do not hit the reply button to an unrelated topic and change 
the subject. As the signature tells you, "PLEASE do read the posting 
guide! http://www.R-project.org/posting-guide.html"

2. You are looking for the "eda" *package* and not a *library*.

3. Since "eda" is not on CRAN (just tried install.packages("eda")) 
perhaps you can tell us what functions you're looking for. eda, which 
I'm not familiar with, may have been merged with other packages or is 
defunct. Either way, there may be alternatives out there that will 
satisfy your needs.

Thanks,

--sundar

S.O. Nyangoma wrote:
> Hi 
> I want to install eda library. Where can I download it?
> I use R version 2.1.0 on Linux.
> 
> Thanks.
> Stephen.
> 
> 
> 
> ----- Original Message -----
> From: Luis Ridao Cruz <Luisr at frs.fo>
> Date: Tuesday, September 13, 2005 1:32 pm
> Subject: Re: [R] How to erase objects
> 
> 
>>?rm
>>
>>
>>
>>>>><v.schlecht at arcor.de> 13/09/2005 12:08:55 >>>
>>
>>Hi, I admit that I rather carelessly built lots of large objects 
>>and therefore ran out of memory. Most objects, which I have at the 
>>moment are now unnecessary, but it would take a lot of time to 
>>recreate the last few ones from scratch. So I would like to erase 
>>(get rid of) selected objects in order to be able to continue. 
>>Does anyone know how I could do this? 
>>
>>Machen Sie aus 14 Cent spielend bis zu 100 Euro!
>>Die neue Gaming-Area von Arcor - ??ber 50 Onlinespiele im Angebot.
>>http://www.arcor.de/rd/emf-gaming-1 
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help 
>>PLEASE do read the posting guide! http://www.R-project.org/posting-
>>guide.html
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-
>>guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chrysopa at insecta.ufv.br  Tue Sep 13 18:24:02 2005
From: chrysopa at insecta.ufv.br (Ronaldo Reis-Jr.)
Date: Tue, 13 Sep 2005 13:24:02 -0300
Subject: [R] best way to fit a model
In-Reply-To: <20050909202548.75ed4fdd.Achim.Zeileis@wu-wien.ac.at>
References: <200509091455.04898.chrysopa@insecta.ufv.br>
	<20050909202548.75ed4fdd.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <200509131324.02926.chrysopa@insecta.ufv.br>

Em Sex 09 Set 2005 15:25, Achim Zeileis escreveu:
> On Fri, 9 Sep 2005 14:55:04 -0300 Ronaldo Reis-Jr. wrote:
> > Hi,
> >
> > I have some data that have this behaviour:
> > |*******
> > |        *
> > |          *
> > |            *
> > |              *
> > |----------------
> >
> > What is the best and simpler way to fit this in R?
>
> If the changepoint is known, then this is straightforward using lm:
>

I try this. But my doubt now is:

How to justify this kind of analysis? Why dont use any linearized or non 
linear regression to fit this? Something like a log function (I try but is 
not a good function).

Thanks
Ronaldo
-- 
Your reasoning is excellent -- it's only your basic assumptions that are 
wrong.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From greg.snow at ihc.com  Tue Sep 13 18:28:22 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Tue, 13 Sep 2005 10:28:22 -0600
Subject: [R] Reading data from a serial port
Message-ID: <s326a9da.006@lp-msg1.co.ihc.com>

I don't know of a specific package, but there is support for various 
connections in R.  look for help on connections by typing:

?connection

There is also a section in the R Data Import/Export document on 
connections.  You will probably use a fifo or a pipe.

Good luck,

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> vittorio <vdemart1 at tin.it> 09/13/05 10:07AM >>>
I need to read data from from a medical appliance via the serial port.
This 
medical appliance produces streams of set data at regular intervals.
What commands,  packages are available for this purpose?

Vittorio

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Tue Sep 13 18:40:07 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 13 Sep 2005 11:40:07 -0500
Subject: [R] [S] Any S-plus/R doc or manual for SAS user out there?
In-Reply-To: <BAY107-F8B9A6E549C4F27B0249CEB09C0@phx.gbl>
References: <BAY107-F8B9A6E549C4F27B0249CEB09C0@phx.gbl>
Message-ID: <432700E7.5040406@vanderbilt.edu>

paul king wrote:
> Dear list,
> 
> I am looking for an Splus or R doc / manual for SAS user.
> 
> Thank you in advance. PK

Somewhat helpful in that regard: 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/RS/sintro.pdf

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Ted.Harding at nessie.mcc.ac.uk  Tue Sep 13 19:06:01 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Sep 2005 18:06:01 +0100 (BST)
Subject: [R] Floating-point arithmetic
In-Reply-To: <Pine.LNX.4.63a.0509130724450.2610@homer23.u.washington.edu>
Message-ID: <XFMail.050913180601.Ted.Harding@nessie.mcc.ac.uk>

On 13-Sep-05 Thomas Lumley wrote:
> On Tue, 13 Sep 2005, Ted.Harding at nessie.mcc.ac.uk wrote:
>> A recent exchange on the 'octave' list led to the following
>> paper being cited, which I had not met before:
>>
>>  What Every Computer Scientist Should Know About
>>  Floating-Point Arithmetic, by David Goldberg,
> <snip>
>>
>> So I'm writing to bring it to the notice of R users who may not
>> have come across it.
>>
>> (And also, maybe, to pre-empt that question that some of you may
>> have been thinking of asking ... ).
> 
> Since the paper is already linked from the relevant R FAQ question,
> there surely wouldn't be any of these ;-)
> 
> 
>       -thomas

Except, perhaps, from people (like me) who had not read that FAQ
-- and who knows what legions these may be?

;)
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Sep-05                                       Time: 16:06:25
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Tue Sep 13 19:14:05 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Sep 2005 18:14:05 +0100 (BST)
Subject: [R] Floating-point arithmetic
In-Reply-To: <Pine.LNX.4.63a.0509130724450.2610@homer23.u.washington.edu>
Message-ID: <XFMail.050913180601.Ted.Harding@nessie.mcc.ac.uk>

On 13-Sep-05 Thomas Lumley wrote:
> On Tue, 13 Sep 2005, Ted.Harding at nessie.mcc.ac.uk wrote:
>> A recent exchange on the 'octave' list led to the following
>> paper being cited, which I had not met before:
>>
>>  What Every Computer Scientist Should Know About
>>  Floating-Point Arithmetic, by David Goldberg,
> <snip>
>>
>> So I'm writing to bring it to the notice of R users who may not
>> have come across it.
>>
>> (And also, maybe, to pre-empt that question that some of you may
>> have been thinking of asking ... ).
> 
> Since the paper is already linked from the relevant R FAQ question,
> there surely wouldn't be any of these ;-)
> 
> 
>       -thomas

Except, perhaps, from people (like me) who had not read that FAQ
-- and who knows what legions these may be?

;)
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Sep-05                                       Time: 16:06:25
------------------------------ XFMail ------------------------------



From Achim.Zeileis at wu-wien.ac.at  Tue Sep 13 20:04:19 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 13 Sep 2005 20:04:19 +0200
Subject: [R] best way to fit a model
In-Reply-To: <200509131324.02926.chrysopa@insecta.ufv.br>
References: <200509091455.04898.chrysopa@insecta.ufv.br>
	<20050909202548.75ed4fdd.Achim.Zeileis@wu-wien.ac.at>
	<200509131324.02926.chrysopa@insecta.ufv.br>
Message-ID: <20050913200419.38944e27.Achim.Zeileis@wu-wien.ac.at>

On Tue, 13 Sep 2005 13:24:02 -0300 Ronaldo Reis-Jr. wrote:

> Em Sex 09 Set 2005 15:25, Achim Zeileis escreveu:
> > On Fri, 9 Sep 2005 14:55:04 -0300 Ronaldo Reis-Jr. wrote:
> > > Hi,
> > >
> > > I have some data that have this behaviour:
> > > |*******
> > > |        *
> > > |          *
> > > |            *
> > > |              *
> > > |----------------
> > >
> > > What is the best and simpler way to fit this in R?
> >
> > If the changepoint is known, then this is straightforward using lm:
> >
> 
> I try this. But my doubt now is:
> 
> How to justify this kind of analysis? Why dont use any linearized or
> non linear regression to fit this? Something like a log function (I
> try but is not a good function).

The motivation usually comes from subject-matter knowledge. For example,
such changepoint models are accepted by practitioners as good proxies
for certain biological processes. If you think that the data-generating
process might contain a structural change, why not model it as such?
Z

> Thanks
> Ronaldo
> -- 
> Your reasoning is excellent -- it's only your basic assumptions that
> are wrong.
> --
> |>   // | \\   [***********************************]
> |   ( ??   ?? )  [Ronaldo Reis J??nior                ]
> |>      V      [UFV/DBA-Entomologia                ]
> |    /     \   [36570-000 Vi??osa - MG              ]
> |>  /(.''`.)\  [Fone: 31-3899-4007                 ]
> |  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |    ( `-  )   [***********************************]
> |>>  _/   \_Powered by GNU/Debian Woody/Sarge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From uofiowa at gmail.com  Tue Sep 13 20:29:55 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Tue, 13 Sep 2005 14:29:55 -0400
Subject: [R] what OS
Message-ID: <3f87cc6d0509131129499b4fd7@mail.gmail.com>

How can I determine whT OS I am running under?

if WINDOWS do this 
if LINUX do that



From sundar.dorai-raj at pdf.com  Tue Sep 13 20:40:37 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 13 Sep 2005 13:40:37 -0500
Subject: [R] what OS
In-Reply-To: <3f87cc6d0509131129499b4fd7@mail.gmail.com>
References: <3f87cc6d0509131129499b4fd7@mail.gmail.com>
Message-ID: <43271D25.7030905@pdf.com>



Omar Lakkis wrote:
> How can I determine whT OS I am running under?
> 
> if WINDOWS do this 
> if LINUX do that
> 

Hi, Omar,

Look at version$os.

 > # windows
 > version$os
[1] "mingw32"

 > # linux
 > version$os
[1] "linux-gnu"

HTH,

--sundar



From gunter.berton at gene.com  Tue Sep 13 20:40:39 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 13 Sep 2005 11:40:39 -0700
Subject: [R] what OS
In-Reply-To: <3f87cc6d0509131129499b4fd7@mail.gmail.com>
Message-ID: <200509131840.j8DIedpm029226@hertz.gene.com>

?.Platform

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omar Lakkis
> Sent: Tuesday, September 13, 2005 11:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] what OS
> 
> How can I determine whT OS I am running under?
> 
> if WINDOWS do this 
> if LINUX do that
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ecu at info.fundp.ac.be  Tue Sep 13 20:40:52 2005
From: ecu at info.fundp.ac.be (Cuvelier Etienne)
Date: Tue, 13 Sep 2005 20:40:52 +0200
Subject: [R] what OS
References: <3f87cc6d0509131129499b4fd7@mail.gmail.com>
Message-ID: <039501c5b892$ab3991e0$6500a8c0@winXP>

Try this
 Sys.info()["sysname"]

Etienne

----- Original Message ----- 
From: "Omar Lakkis" <uofiowa at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, September 13, 2005 8:29 PM
Subject: [R] what OS


> How can I determine whT OS I am running under?
>
> if WINDOWS do this
> if LINUX do that
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>
>
> -- 
> No virus found in this incoming message.
> Checked by AVG Anti-Virus.
> Version: 7.0.344 / Virus Database: 267.10.23/99 - Release Date: 12/09/2005
>
>



-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From maustin at amgen.com  Tue Sep 13 20:42:30 2005
From: maustin at amgen.com (Austin, Matt)
Date: Tue, 13 Sep 2005 11:42:30 -0700
Subject: [R] what OS
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD13F@teal-exch.amgen.com>

.Platform$OS.type

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Omar Lakkis
> Sent: Tuesday, September 13, 2005 11:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] what OS
> 
> 
> How can I determine whT OS I am running under?
> 
> if WINDOWS do this 
> if LINUX do that
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From reid_huntsinger at merck.com  Tue Sep 13 20:45:04 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 13 Sep 2005 14:45:04 -0400
Subject: [R] what OS
Message-ID: <355C35514FEAC9458F75947F5270974D076CC8@usctmx1103.merck.com>

.Platform contains this and more.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omar Lakkis
Sent: Tuesday, September 13, 2005 2:30 PM
To: r-help at stat.math.ethz.ch
Subject: [R] what OS


How can I determine whT OS I am running under?

if WINDOWS do this 
if LINUX do that

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Sep 13 20:47:09 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 13 Sep 2005 14:47:09 -0400
Subject: [R] what OS
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED42D@usctmx1106.merck.com>

> From: Sundar Dorai-Raj
> 
> Omar Lakkis wrote:
> > How can I determine whT OS I am running under?
> > 
> > if WINDOWS do this 
> > if LINUX do that
> > 
> 
> Hi, Omar,
> 
> Look at version$os.
> 
>  > # windows
>  > version$os
> [1] "mingw32"
> 
>  > # linux
>  > version$os
> [1] "linux-gnu"

Or see ?.Platform.

Andy

 
> HTH,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From uofiowa at gmail.com  Tue Sep 13 21:09:17 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Tue, 13 Sep 2005 15:09:17 -0400
Subject: [R] is library loaded
Message-ID: <3f87cc6d05091312095f3c3b7d@mail.gmail.com>

Is there a way to test if a library has been loaded?

is.loaded does not give me what I want, I am looking to say:

if loaded(Rdbi) dbSendQuery(conn, q)
if loaded(RODBC) sqlQuery(conn, q)

I need this to support both unix and windows platforms as I could not
find a windows distribution for RdbiPgSQL. I am using R 2.1.0 and
postgresql. I will be connecting to the database using Rdbi and
RdbiPgSQL and have other developers using windows connect with RODBC,
unless someone can suggest a better solution.



From momozilla at gmail.com  Tue Sep 13 21:34:39 2005
From: momozilla at gmail.com (Zhen Zhang)
Date: Tue, 13 Sep 2005 21:34:39 +0200
Subject: [R] coxph.detail() does not work
Message-ID: <6bfea57f050913123432025d49@mail.gmail.com>

Hello everyone,

I tried to use coxph.detail() to get the hazard function.  But a warning
messge always returns to me, even in the example provided by its help
document:

> ?coxph.detail
> fit   <- coxph(Surv(futime,fustat) ~ age + rx + ecog.ps, ovarian, x=TRUE)
> fitd  <- coxph.detail(fit)
Warning message:
data length [37] is not a sub-multiple or multiple of the number of
rows [12] in matrix

Can anyone suggest why it does not work?


I use R 2.1.1 with Windows XP


Thanks a lot.


Zhen Zhang



From Ted.Harding at nessie.mcc.ac.uk  Tue Sep 13 21:26:16 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Sep 2005 20:26:16 +0100 (BST)
Subject: [R] Reading data from a serial port
In-Reply-To: <200509131807.46497.vdemart1@tin.it>
Message-ID: <XFMail.050913202616.Ted.Harding@nessie.mcc.ac.uk>

On 13-Sep-05 vittorio wrote:
> I need to read data from from a medical appliance via the serial port.
> This medical appliance produces streams of set data at regular
> intervals.
> What commands,  packages are available for this purpose?
> 
> Vittorio

Have a look at "?scan".

You will have to do several things which depend on your hardware
setup, your operating system, the behaviour of your medical applicance,
and how you want to handle the data as it comes through. Some of this
can be worked out from "?scan"; the rest is up to you!

Example: On my Linux machine here, the serial port at the back is
/dev/ttyS0. I have used 'minicom' to set its data parameters to
4800 baud, 7-bit data, space parity, 1 stop bit ("4800 7S1").

Next, I have connected my GPS gadget (which has a serial output
in ASCII text format at the above characteristics) to the serial
port.

Then, in R (with permissions on /dev/ttyS0 set to allow user read/write,
namely "rw-rw-rw-"), in R I have executed, for example,

scan(file="/dev/ttyS0",n=1,what="character")
Read 1 items
[1] "@050913192752N5228545E00023023G007-00004E0000N0000D0000"

which tells me that it is 2005/09/13 at 19:27:52 UTC, that I
am at 52deg 28.545minN and 000deg 23.023min E, that I have a
Good stellite fix, have a potential horizontal position error
of 007 metres, am at 0000.4 metres below sea level, and am
moving at 000.0m/s Eastwards, 000.0m/s Northwards, and 00.00m/s
Downwards. (Illustrating that such data is parsed by position;
'scan' does not seem to have a mechanism for splitting a line
into fields by position, but it can be done after reading by
using 'substr').

That command read just one line, so by repeating the command
I can read a line at a time, do something with it, read the next ...

Again,

scan(file="/dev/ttyS0",n=5,what="character")
Read 5 items
[1] "@050913191942N5228544E00023023G010-00001E0000N0000U0001"
[2] "@050913191943N5228544E00023023G010-00001E0000N0000U0001"
[3] "@050913191944N5228544E00023023G010-00001E0000N0000U0001"
[4] "@050913191945N5228544E00023023G010-00001E0000N0000U0001"
[5] "@050913191946N5228544E00023023G010-00001E0000N0000U0001"

reads a batch of lines, which can be assigned to a vector.

Variants on this depend on what data format your apparatus
puts out, and on what you want to do. In particular, if you
want to process the output in real time, then probably you
are best off reading a line at a time. But if you simply
want to store a batch of lines for later processing, then
set a (possibly large) number of lines to be read at a time.

An so on.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Sep-05                                       Time: 20:26:09
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Tue Sep 13 20:01:49 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Sep 2005 19:01:49 +0100 (BST)
Subject: [R] Floating-point arithmetic
In-Reply-To: <XFMail.050913180601.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050913190149.Ted.Harding@nessie.mcc.ac.uk>

On 13-Sep-05 Ted Harding wrote:
> [..]
> Except, perhaps, from people (like me) who had not read that FAQ
> -- and who knows what legions these may be?
> 
> ;)
> Ted.

Apologies that people may have received two copies of the above message.

When I first sent it there was a bounce on the grounds that
the ISP's mailrouter IP was blacklisted, so I assumed it had
failed to be delivered and so re-sent it.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Sep-05                                       Time: 19:01:37
------------------------------ XFMail ------------------------------



From chabotd at globetrotter.net  Tue Sep 13 21:53:39 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Tue, 13 Sep 2005 15:53:39 -0400
Subject: [R] inconsistant decimal marker with write.table
Message-ID: <FF6A3C3A-D222-4992-973F-2B3F817A90AE@globetrotter.net>

Hi,

My problem does not happen all the time, nor with all files I save to  
csv format. I can send my test file (format rda or csv) to whoever  
would like to replicate (and hopefully explain) my problem.

In short, I have a dataset with mostly numerical variables. One of my  
variable is called pfi2 and is definitely numerical, as shown by this:

 > summary(test$pfi2)
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
0.00000 0.00000 0.00000 0.01208 0.01500 0.08900

Yet, when I export to a text file and asking for comma as the decimal  
separator, I get a period as the separator for this variable, whereas  
all the other variables have the desired comma as separator.

write.table(test, "out1.csv", sep = ";", dec = ",", row.names = F,  
col.names = TRUE)
write.csv2(test, "out2.csv")

First 5 lines of out1.csv:

"trait";"tfi";"sto_wt";"pfi2";"pfi_st";"focc";"sed";"mob";"date";"latitu 
de";"longitud";"hre_deb";"temp_fon";"strate";"lat.km";"long.km";"temp_.2 
5C"
10;0,764;5,1;0.007;0,213;0,143;0,048;0,095;"05/08/99"; 
47,49;-58,9267;"20:11";4,9;302;-167,791200000000;230,761439786593;4,88
106;0,566;3,3;0.089;4,762;0,25;0;0,25;"14/08/99"; 
49,84;-59,655;"23:25";4,4;814;93,3408000000004;168,052052432345;4,38
110;1,517;8,3;0.003;0,172;0,25;0;0;"15/08/99"; 
49,88667;-60,2167;"04:57";0,3;833;98,5267704000003;127,674990442535;0,38
111;1,232;7,5;0.016;1,309;0,25;0;0;"15/08/99";49,84;-60,1617;"06:14"; 
0,1;833;93,3408000000004;131,739909589075;0,12

I'm sorry but I wanted to make this lighter by removing more  
variables from my original dataset, but the problem disappeared, so I  
had to keep these variables to show you the problem SOMETIMES happen.  
You'll notice that the 4th variable has periods for decimal markers.  
The write.csv2 command produced the same problem.

With other files I sometimes get commas for all variables, sometimes  
I get more than one variable with periods. It is frustrating.

So let me know if you'd like the data file,

Sincerely,

Denis Chabot
platform powerpc-apple-darwin7.9.0
arch     powerpc
os       darwin7.9.0
system   powerpc, darwin7.9.0
status
major    2
minor    1.1
year     2005
month    06
day      20
language R



From murdoch at stats.uwo.ca  Tue Sep 13 22:00:11 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Sep 2005 16:00:11 -0400
Subject: [R] is library loaded
In-Reply-To: <3f87cc6d05091312095f3c3b7d@mail.gmail.com>
References: <3f87cc6d05091312095f3c3b7d@mail.gmail.com>
Message-ID: <43272FCB.2050203@stats.uwo.ca>

On 9/13/2005 3:09 PM, Omar Lakkis wrote:
> Is there a way to test if a library has been loaded?
> 
> is.loaded does not give me what I want, I am looking to say:
> 
> if loaded(Rdbi) dbSendQuery(conn, q)
> if loaded(RODBC) sqlQuery(conn, q)
> 
> I need this to support both unix and windows platforms as I could not
> find a windows distribution for RdbiPgSQL. I am using R 2.1.0 and
> postgresql. I will be connecting to the database using Rdbi and
> RdbiPgSQL and have other developers using windows connect with RODBC,
> unless someone can suggest a better solution.

You can use the strangely named "require" function, as in

if (require(Rdbi)) dbSendQuery(conn, q)
else if (require(RODBC) sqlQuery(conn, q)
else stop("Nothing works")

Duncan Murdoch



From tlumley at u.washington.edu  Tue Sep 13 22:04:57 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 13 Sep 2005 13:04:57 -0700 (PDT)
Subject: [R] coxph.detail() does not work
In-Reply-To: <6bfea57f050913123432025d49@mail.gmail.com>
References: <6bfea57f050913123432025d49@mail.gmail.com>
Message-ID: <Pine.LNX.4.63a.0509131258070.29030@homer24.u.washington.edu>

On Tue, 13 Sep 2005, Zhen Zhang wrote:

> Hello everyone,
>
> I tried to use coxph.detail() to get the hazard function.  But a warning
> messge always returns to me, even in the example provided by its help
> document:
>
>> ?coxph.detail
>> fit   <- coxph(Surv(futime,fustat) ~ age + rx + ecog.ps, ovarian, x=TRUE)
>> fitd  <- coxph.detail(fit)
> Warning message:
> data length [37] is not a sub-multiple or multiple of the number of
> rows [12] in matrix
>
> Can anyone suggest why it does not work?
>

It does work. That's a warning, not an error.

If you want to know why the warning is there, it's because the code turns 
a 37-element vector into a 12x3 matrix. This drops the last element. In 
S-PLUS, and in some older versions of R, there is no warning, but in 
current R there is a warning.  The answer is the same, though.

 	-thomas



From jfox at mcmaster.ca  Tue Sep 13 22:11:32 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 13 Sep 2005 16:11:32 -0400
Subject: [R] is library loaded
In-Reply-To: <3f87cc6d05091312095f3c3b7d@mail.gmail.com>
Message-ID: <20050913201131.LBRI16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Omar,

The following function tests whether a package is in the search path (with
the package name given in quotes):

packageLoaded <- function(name) 0 != length(grep(paste("^package:", name,
"$", sep=""), search()))

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omar Lakkis
> Sent: Tuesday, September 13, 2005 2:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] is library loaded
> 
> Is there a way to test if a library has been loaded?
> 
> is.loaded does not give me what I want, I am looking to say:
> 
> if loaded(Rdbi) dbSendQuery(conn, q)
> if loaded(RODBC) sqlQuery(conn, q)
> 
> I need this to support both unix and windows platforms as I 
> could not find a windows distribution for RdbiPgSQL. I am 
> using R 2.1.0 and postgresql. I will be connecting to the 
> database using Rdbi and RdbiPgSQL and have other developers 
> using windows connect with RODBC, unless someone can suggest 
> a better solution.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Sep 13 22:32:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 13 Sep 2005 22:32:47 +0200
Subject: [R] what OS
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED42D@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED42D@usctmx1106.merck.com>
Message-ID: <4327376F.8050208@statistik.uni-dortmund.de>

Liaw, Andy wrote:
>>From: Sundar Dorai-Raj
>>
>>Omar Lakkis wrote:
>>
>>>How can I determine whT OS I am running under?
>>>
>>>if WINDOWS do this 
>>>if LINUX do that
>>>
>>
>>Hi, Omar,
>>
>>Look at version$os.
>>
>> > # windows
>> > version$os
>>[1] "mingw32"
>>
>> > # linux
>> > version$os
>>[1] "linux-gnu"
> 
> 
> Or see ?.Platform.

Yes, please follow Andy and use .Platform, and in particular nothing 
else for programming!
It is *not* guaranteed that on Windows version$os == "mingw32".

Uwe Ligges




> Andy
> 
>  
> 
>>HTH,
>>
>>--sundar
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dlvanbrunt at gmail.com  Tue Sep 13 22:29:08 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Tue, 13 Sep 2005 15:29:08 -0500
Subject: [R] Anyone have any code for importing data from NAMCS?
Message-ID: <d332d3e105091313296daf0fcb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050913/1f4c0a2b/attachment.pl

From rgentlem at fhcrc.org  Tue Sep 13 22:53:16 2005
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Tue, 13 Sep 2005 13:53:16 -0700
Subject: [R] is library loaded
In-Reply-To: <20050913201131.LBRI16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>
References: <20050913201131.LBRI16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <43273C3C.5020402@fhcrc.org>

Hi,

  Almost surely this is a bad name. With the advent of name spaces it is 
important to distinguish between loading and attaching. This function 
tests for attached packages. To test for loaded packages we already have
  loadedNamespaces.

  Best wishes,
    Robert

John Fox wrote:
> Dear Omar,
> 
> The following function tests whether a package is in the search path (with
> the package name given in quotes):
> 
> packageLoaded <- function(name) 0 != length(grep(paste("^package:", name,
> "$", sep=""), search()))
> 
> I hope this helps,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omar Lakkis
>>Sent: Tuesday, September 13, 2005 2:09 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] is library loaded
>>
>>Is there a way to test if a library has been loaded?
>>
>>is.loaded does not give me what I want, I am looking to say:
>>
>>if loaded(Rdbi) dbSendQuery(conn, q)
>>if loaded(RODBC) sqlQuery(conn, q)
>>
>>I need this to support both unix and windows platforms as I 
>>could not find a windows distribution for RdbiPgSQL. I am 
>>using R 2.1.0 and postgresql. I will be connecting to the 
>>database using Rdbi and RdbiPgSQL and have other developers 
>>using windows connect with RODBC, unless someone can suggest 
>>a better solution.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Robert Gentleman, PhD
Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M2-B876
PO Box 19024
Seattle, Washington 98109-1024
206-667-7700
rgentlem at fhcrc.org



From 8-T at gmx.net  Tue Sep 13 23:17:28 2005
From: 8-T at gmx.net (=?ISO-8859-1?Q?Ram=F3n_Casero_Ca=F1as?=)
Date: Tue, 13 Sep 2005 22:17:28 +0100
Subject: [R] logistic regression with nominal predictors
Message-ID: <432741E8.1000702@gmx.net>


(Sorry for obvious mistakes, as I am quite a newby with no Statistics
background).

My question is going to be what is the gain of logistic regression over
odds ratios when none of the input variables is continuous.


My experiment:
 Outcome: ordinal scale, ``quality'' (QUA=1,2,3)
 Predictors: ``segment'' (SEG) and ``stress'' (STR). SEG is
             nominal scale with 24 levels, and STR is dychotomous (0,1).



Considering the outcome continuous, two-way ANOVA with

aov(as.integer(QUA) ~ SEG * STR)

doesn't find evidence of interaction between SEG and STR, and they are
significant on their own. This is the result that we would expect from
clinical knowledge.



I use

xtabs(~QUA+SEG, data=data2.df, subset=STR==0)
xtabs(~QUA+SEG, data=data2.df, subset=STR==0)

for the contingency tables. There are zero cells, and for some values of
SEG, there is only one none-zero cell, i.e. some values of SEG determine
the output with certainty.

So initially I was thinking of a proportional odds logistic regression
model, but following Hosmer and Lemeshow [1], zero cells are
problematic. So I take out of the data table the deterministic values of
SEG, and I pool QUA=2 and QUA=3, and now I have a dychotomous outcome
(QUA = Good/Bad) and no zero cells.

The following model doesn't find evidence of interaction

glm(QUA ~ STR * SEG, data=data3.df, family=binomial)

so I go for

glm(QUA ~ STR + SEG, data=data3.df, family=binomial)


(I suppose that what glm does is to create design variables for SEG,
where 0 0 ... 0 is for the first value of SEG, 1 0 ... 0 for the second
value, 0 1 0 ... 0 for the third, etc).

Coefficients:
              Estimate Std. Error   z value Pr(>|z|)
(Intercept) -1.085e+00  1.933e-01    -5.614 1.98e-08 ***
STR.L        2.112e-01  6.373e-02     3.314 0.000921 ***
SEGP2C.MI   -9.869e-01  3.286e-01    -3.004 0.002669 **
SEGP2C.AI   -1.306e+00  3.585e-01    -3.644 0.000269 ***
SEGP2C.AA   -1.743e+00  4.123e-01    -4.227 2.37e-05 ***
[shortened]
SEGP4C.ML   -5.657e-01  2.990e-01    -1.892 0.058485 .
SEGP4C.BL   -2.908e-16  2.734e-01 -1.06e-15 1.000000
SEGSAX.MS    1.092e-01  2.700e-01     0.405 0.685772
SEGSAX.MAS  -5.441e-16  2.734e-01 -1.99e-15 1.000000
SEGSAX.MA    7.130e-01  2.582e-01     2.761 0.005758 **
SEGSAX.ML    1.199e+00  2.565e-01     4.674 2.96e-06 ***
SEGSAX.MP    1.313e+00  2.570e-01     5.108 3.26e-07 ***
SEGSAX.MI    8.865e-01  2.569e-01     3.451 0.000558 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 3462.0  on 3123  degrees of freedom
Residual deviance: 3012.6  on 3101  degrees of freedom
AIC: 3058.6

Number of Fisher Scoring iterations: 6


Even though some coefficients have no evidence of statistical
significance, the model requires them from a clinical point of view.

At this point, the question would be how to interpret these results, and
what advantage they offer over odds ratios. From [1] I can understand
that in the case of a dychotomous and a continuous predictor, you can
adjust for the continuous variable.

But when all predictors are dychotomous (due to the design variables), I
don't quite see the effect of adjustment. Wouldn't it be better just to
split the data in two groups (STR=0 and STR=1), and instead of using
logistic regression, use odds ratios for each value of SEG?

Cheers,

Ram??n.

[1] D.W. Hosmer and S. Lemeshow. ``Applied Logistic Regression''.
John-Wiley. 2000.

-- 
Ram??n Casero Ca??as

web:    http://www.robots.ox.ac.uk/~rcasero/



From vdemart1 at tin.it  Tue Sep 13 23:29:36 2005
From: vdemart1 at tin.it (vittorio)
Date: Tue, 13 Sep 2005 23:29:36 +0200
Subject: [R] Reading data from a serial port
In-Reply-To: <XFMail.050913202616.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050913202616.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200509132329.36578.vdemart1@tin.it>

Alle 21:26, marted?? 13 settembre 2005, Ted Harding ha scritto:
...................................>
> Have a look at "?scan".
>
> You will have to do several things which depend on your hardware
> setup, your operating system, the behaviour of your medical applicance,
> and how you want to handle the data as it comes through. Some of this
> can be worked out from "?scan"; the rest is up to you!
>
> Example: On my Linux machine here, the serial port at the back is
> /dev/ttyS0. I have used 'minicom' to set its data parameters to
> 4800 baud, 7-bit data, space parity, 1 stop bit ("4800 7S1").
>
> Next, I have connected my GPS gadget (which has a serial output
> in ASCII text format at the above characteristics) to the serial
> port.
>
> Then, in R (with permissions on /dev/ttyS0 set to allow user read/write,
> namely "rw-rw-rw-"), in R I have executed, for example,
>
> scan(file="/dev/ttyS0",n=1,what="character")
> Read 1 items
> [1] "@050913192752N5228545E00023023G007-00004E0000N0000D0000"
>
> which tells me that it is 2005/09/13 at 19:27:52 UTC, that I
> am at 52deg 28.545minN and 000deg 23.023min E, that I have a
> Good stellite fix, have a potential horizontal position error
> of 007 metres, am at 0000.4 metres below sea level, and am
> moving at 000.0m/s Eastwards, 000.0m/s Northwards, and 00.00m/s
> Downwards. (Illustrating that such data is parsed by position;
> 'scan' does not seem to have a mechanism for splitting a line
> into fields by position, but it can be done after reading by
> using 'substr').
>
> That command read just one line, so by repeating the command
> I can read a line at a time, do something with it, read the next ...
>.
> Again,
>
> scan(file="/dev/ttyS0",n=5,what="character")
> Read 5 items
> [1] "@050913191942N5228544E00023023G010-00001E0000N0000U0001"
> [2] "@050913191943N5228544E00023023G010-00001E0000N0000U0001"
> [3] "@050913191944N5228544E00023023G010-00001E0000N0000U0001"
> [4] "@050913191945N5228544E00023023G010-00001E0000N0000U0001"
> [5] "@050913191946N5228544E00023023G010-00001E0000N0000U0001"
>
> reads a batch of lines, which can be assigned to a vector.
>
> Variants on this depend on what data format your apparatus
> puts out, and on what you want to do. In particular, if you
> want to process the output in real time, then probably you
> are best off reading a line at a time. But if you simply
> want to store a batch of lines for later processing, then
> set a (possibly large) number of lines to be read at a time.
>
> An so on.
>
> Hoping this helps,

Thanks, it helps! But Ted,
how do you let R know the parameters of the serial connection  (e.g. "4800 
7S1") ?
Ciao
Vittorio



From jfox at mcmaster.ca  Tue Sep 13 23:56:48 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 13 Sep 2005 17:56:48 -0400
Subject: [R] is library loaded
In-Reply-To: <43273C3C.5020402@fhcrc.org>
Message-ID: <20050913215647.KHFI21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Robert,

packageLoaded() may well be a bad name but loadedNamespaces() won't detect a
package without a namespace. It therefore seemed safe to me to check the
path, which would include both packages with and without namespaces. With
respect to loading and attaching, I thought that library() both loaded a
package (with or without a namespace) and attached it to the search path,
but I must admit that I'm easily confused about these distinctions. 

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Robert Gentleman [mailto:rgentlem at fhcrc.org] 
> Sent: Tuesday, September 13, 2005 3:53 PM
> To: John Fox
> Cc: uofiowa at gmail.com; r-help at stat.math.ethz.ch
> Subject: Re: [R] is library loaded
> 
> Hi,
> 
>   Almost surely this is a bad name. With the advent of name 
> spaces it is important to distinguish between loading and 
> attaching. This function tests for attached packages. To test 
> for loaded packages we already have
>   loadedNamespaces.
> 
>   Best wishes,
>     Robert
> 
> John Fox wrote:
> > Dear Omar,
> > 
> > The following function tests whether a package is in the 
> search path 
> > (with the package name given in quotes):
> > 
> > packageLoaded <- function(name) 0 != length(grep(paste("^package:", 
> > name, "$", sep=""), search()))
> > 
> > I hope this helps,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > 
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omar Lakkis
> >>Sent: Tuesday, September 13, 2005 2:09 PM
> >>To: r-help at stat.math.ethz.ch
> >>Subject: [R] is library loaded
> >>
> >>Is there a way to test if a library has been loaded?
> >>
> >>is.loaded does not give me what I want, I am looking to say:
> >>
> >>if loaded(Rdbi) dbSendQuery(conn, q)
> >>if loaded(RODBC) sqlQuery(conn, q)
> >>
> >>I need this to support both unix and windows platforms as I 
> could not 
> >>find a windows distribution for RdbiPgSQL. I am using R 2.1.0 and 
> >>postgresql. I will be connecting to the database using Rdbi and 
> >>RdbiPgSQL and have other developers using windows connect 
> with RODBC, 
> >>unless someone can suggest a better solution.
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> --
> Robert Gentleman, PhD
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M2-B876
> PO Box 19024
> Seattle, Washington 98109-1024
> 206-667-7700
> rgentlem at fhcrc.org



From ljlayne at unm.edu  Wed Sep 14 00:10:24 2005
From: ljlayne at unm.edu (Larry Layne)
Date: Tue, 13 Sep 2005 16:10:24 -0600
Subject: [R] package Spcmdr
Message-ID: <D47C0C2EB7A56B86F1B737DA@dhcp-129-24-91-249.unm.edu>

In the R Console (ver 2.1.1) I typed the command help("ppinit") and 
received the error message:

Error in help.search("ppinit") : could not find package 'Spcmdr'

After typing library(spatial) I could get help on the ppinit function. 
However, I have tried in vain to find any information on the Spcmdr 
package. This has included searching the full R search site (Jonathan Baron 
and U. Penn) and looking under the RGUI's and Rgeo pages.

Does anyone know what the package 'Spcmdr' is and where I can find either 
the package or documentation about it?

Larry Layne
ljlayne at unm.edu



From murdoch at stats.uwo.ca  Wed Sep 14 00:15:27 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Sep 2005 18:15:27 -0400
Subject: [R] package Spcmdr
In-Reply-To: <D47C0C2EB7A56B86F1B737DA@dhcp-129-24-91-249.unm.edu>
References: <D47C0C2EB7A56B86F1B737DA@dhcp-129-24-91-249.unm.edu>
Message-ID: <43274F7F.8050107@stats.uwo.ca>

Larry Layne wrote:
> In the R Console (ver 2.1.1) I typed the command help("ppinit") and 
> received the error message:
> 
> Error in help.search("ppinit") : could not find package 'Spcmdr'

That sounds like a local installation problem.  Perhaps you had a 
package with that name installed at some point, and then deleted it, but 
didn't update the indices?  It probably has nothing to do with ppinit.

Duncan Murdoch
> 
> After typing library(spatial) I could get help on the ppinit function. 
> However, I have tried in vain to find any information on the Spcmdr 
> package. This has included searching the full R search site (Jonathan Baron 
> and U. Penn) and looking under the RGUI's and Rgeo pages.
> 
> Does anyone know what the package 'Spcmdr' is and where I can find either 
> the package or documentation about it?
> 
> Larry Layne
> ljlayne at unm.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ljlayne at unm.edu  Wed Sep 14 00:30:29 2005
From: ljlayne at unm.edu (Larry Layne)
Date: Tue, 13 Sep 2005 16:30:29 -0600
Subject: [R] package Spcmdr
In-Reply-To: <43274F7F.8050107@stats.uwo.ca>
References: <D47C0C2EB7A56B86F1B737DA@dhcp-129-24-91-249.unm.edu>
	<43274F7F.8050107@stats.uwo.ca>
Message-ID: <D685CF18007105E5888AA640@dhcp-129-24-91-249.unm.edu>

Thank you very much for the quick response!

As far as I know I did not install the Spcmdr explicitly. When I installed 
R ver 2.1.1, I uninstalled the previous version of R and deleted the R 
directory and all directories under R in C:\Program Files. (BTW, I am using 
the windows binaries of R). In the R Console, if I click on the Packages 
pull-down menu and select Load Packages..., Spcmdr does appear in the list 
of packages. Selecting this I get an error that says there is no package 
called 'Spcmdr'. Should I be directing my question to a windows mailing 
list?

--On Tuesday, September 13, 2005 6:15 PM -0400 Duncan Murdoch 
<murdoch at stats.uwo.ca> wrote:

> Larry Layne wrote:
>> In the R Console (ver 2.1.1) I typed the command help("ppinit") and
>> received the error message:
>>
>> Error in help.search("ppinit") : could not find package 'Spcmdr'
>
> That sounds like a local installation problem.  Perhaps you had a package
> with that name installed at some point, and then deleted it, but didn't
> update the indices?  It probably has nothing to do with ppinit.
>
> Duncan Murdoch
>>
>> After typing library(spatial) I could get help on the ppinit function.
>> However, I have tried in vain to find any information on the Spcmdr
>> package. This has included searching the full R search site (Jonathan
>> Baron  and U. Penn) and looking under the RGUI's and Rgeo pages.
>>
>> Does anyone know what the package 'Spcmdr' is and where I can find
>> either  the package or documentation about it?
>>
>> Larry Layne
>> ljlayne at unm.edu
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Wed Sep 14 00:42:25 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Sep 2005 18:42:25 -0400
Subject: [R] package Spcmdr
In-Reply-To: <D685CF18007105E5888AA640@dhcp-129-24-91-249.unm.edu>
References: <D47C0C2EB7A56B86F1B737DA@dhcp-129-24-91-249.unm.edu>
	<43274F7F.8050107@stats.uwo.ca>
	<D685CF18007105E5888AA640@dhcp-129-24-91-249.unm.edu>
Message-ID: <432755D1.2080506@stats.uwo.ca>

Larry Layne wrote:
> Thank you very much for the quick response!
> 
> As far as I know I did not install the Spcmdr explicitly. When I installed 
> R ver 2.1.1, I uninstalled the previous version of R and deleted the R 
> directory and all directories under R in C:\Program Files. (BTW, I am using 
> the windows binaries of R). In the R Console, if I click on the Packages 
> pull-down menu and select Load Packages..., Spcmdr does appear in the list 
> of packages. Selecting this I get an error that says there is no package 
> called 'Spcmdr'. Should I be directing my question to a windows mailing 
> list?

That sounds like you have a messed up your R_HOME/library directory. 
There's apparently a subdirectory called Spcmdr, with a file named 
DESCRIPTION in it, but it's not a correctly installed R package.  No 
idea how it would have got there.

Duncan Murdoch
> 
> --On Tuesday, September 13, 2005 6:15 PM -0400 Duncan Murdoch 
> <murdoch at stats.uwo.ca> wrote:
> 
> 
>>Larry Layne wrote:
>>
>>>In the R Console (ver 2.1.1) I typed the command help("ppinit") and
>>>received the error message:
>>>
>>>Error in help.search("ppinit") : could not find package 'Spcmdr'
>>
>>That sounds like a local installation problem.  Perhaps you had a package
>>with that name installed at some point, and then deleted it, but didn't
>>update the indices?  It probably has nothing to do with ppinit.
>>
>>Duncan Murdoch
>>
>>>After typing library(spatial) I could get help on the ppinit function.
>>>However, I have tried in vain to find any information on the Spcmdr
>>>package. This has included searching the full R search site (Jonathan
>>>Baron  and U. Penn) and looking under the RGUI's and Rgeo pages.
>>>
>>>Does anyone know what the package 'Spcmdr' is and where I can find
>>>either  the package or documentation about it?
>>>
>>>Larry Layne
>>>ljlayne at unm.edu
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>
> 
> 
>



From ljlayne at unm.edu  Wed Sep 14 00:47:13 2005
From: ljlayne at unm.edu (Larry Layne)
Date: Tue, 13 Sep 2005 16:47:13 -0600
Subject: [R] package Spcmdr
In-Reply-To: <432755D1.2080506@stats.uwo.ca>
References: <D47C0C2EB7A56B86F1B737DA@dhcp-129-24-91-249.unm.edu>
	<43274F7F.8050107@stats.uwo.ca>
	<D685CF18007105E5888AA640@dhcp-129-24-91-249.unm.edu>
	<432755D1.2080506@stats.uwo.ca>
Message-ID: <08D2D02710BE8498C6DBCD40@dhcp-129-24-91-249.unm.edu>

> That sounds like you have a messed up your R_HOME/library directory.
> There's apparently a subdirectory called Spcmdr, with a file named
> DESCRIPTION in it, but it's not a correctly installed R package.  No idea
> how it would have got there.

Yes there is and now I remember putting it there, although there is no such 
package. Sorry for wasting your time with my confusion.



From vantini at mate.polimi.it  Wed Sep 14 01:17:09 2005
From: vantini at mate.polimi.it (Simone Vantini)
Date: Wed, 14 Sep 2005 01:17:09 +0200 (CEST)
Subject: [R] mvpart: 'weights' argument
Message-ID: <14582.62.101.126.224.1126653429.squirrel@webmail.mate.polimi.it>

Hallo dear R-users!
Does anyone know if it's possible to use the weights argument (available
in rpart() ) in mvpart() function too?Thanks a lot
Simone Vantini


-- 
Simone Vantini
MOX (Modelling and Scientific Computing)
Dipartimento di Matematica "F. Brioschi"
Politecnico di Milano
P.za Leonardo da Vinci, 32
20133 Milano (Italy)
tel: +39 02 2399 4604
email: simone.vantini at mate.polimi.it



From murdoch at stats.uwo.ca  Wed Sep 14 02:19:15 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Sep 2005 20:19:15 -0400
Subject: [R] package Spcmdr
In-Reply-To: <08D2D02710BE8498C6DBCD40@dhcp-129-24-91-249.unm.edu>
References: <D47C0C2EB7A56B86F1B737DA@dhcp-129-24-91-249.unm.edu>	<43274F7F.8050107@stats.uwo.ca>	<D685CF18007105E5888AA640@dhcp-129-24-91-249.unm.edu>	<432755D1.2080506@stats.uwo.ca>
	<08D2D02710BE8498C6DBCD40@dhcp-129-24-91-249.unm.edu>
Message-ID: <43276C83.9030503@stats.uwo.ca>

Larry Layne wrote:
>>That sounds like you have a messed up your R_HOME/library directory.
>>There's apparently a subdirectory called Spcmdr, with a file named
>>DESCRIPTION in it, but it's not a correctly installed R package.  No idea
>>how it would have got there.
> 
> 
> Yes there is and now I remember putting it there, although there is no such 
> package. Sorry for wasting your time with my confusion.

Generally if you're writing your own package, it's a good idea to do it 
in a directory separate from R, then use "Rcmd install <pkg>" to install 
it.  If it's missing something it won't install, and you'll avoid 
problems like this.

Duncan Murdoch



From Ted.Harding at nessie.mcc.ac.uk  Wed Sep 14 02:13:26 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 14 Sep 2005 01:13:26 +0100 (BST)
Subject: [R] Reading data from a serial port
In-Reply-To: <200509132329.36578.vdemart1@tin.it>
Message-ID: <XFMail.050914011326.Ted.Harding@nessie.mcc.ac.uk>

On 13-Sep-05 vittorio wrote:
> Alle 21:26, marted?? 13 settembre 2005, Ted Harding ha scritto:
> ...................................>
>> Have a look at "?scan".
>>
>> You will have to do several things which depend on your hardware
>> setup, your operating system, the behaviour of your medical
>> applicance,
>> and how you want to handle the data as it comes through. Some of this
>> can be worked out from "?scan"; the rest is up to you!
>>
>> Example: On my Linux machine here, the serial port at the back is
>> /dev/ttyS0. I have used 'minicom' to set its data parameters to
>> 4800 baud, 7-bit data, space parity, 1 stop bit ("4800 7S1").
>>
>> Next, I have connected my GPS gadget (which has a serial output
>> in ASCII text format at the above characteristics) to the serial
>> port.
>>
>> Then, in R (with permissions on /dev/ttyS0 set to allow user
>> read/write,
>> namely "rw-rw-rw-"), in R I have executed, for example,
>>
>> scan(file="/dev/ttyS0",n=1,what="character")
>> Read 1 items
>> [1] "@050913192752N5228545E00023023G007-00004E0000N0000D0000"
>>
>> [...]
>> 
>> Hoping this helps,
> 
> Thanks, it helps! But Ted,
> how do you let R know the parameters of the serial connection
>  (e.g. "4800 7S1") ?
> 
> Ciao
> Vittorio

I didn't, as it happens -- I used the 'minicom' program to set
the serial port to these characteristics before starting the
business with 'scan' in R, so it was an "external" operation.
In fact I have a ready-made configuration file for this purpose
called "/etc/minirc.etrex" which contains

# Machine-generated file - use "minicom -s" to change parameters.
pr port             /dev/ttyS0
pu baudrate         4800
pu bits             7
pu parity           S
pu stopbits         1

so when I run

  mincom etrex

the serial port is set to those parameters. Unfortunately, minicom
does not have an option causing it to quit after initialising
the serial port, so it has to be killed explicitly, since otherwise
it will continue reading the serial port and thereby "steal" data
from R! This I do by pressing Ctrl-C on the keyboard, but it could
be wrapped in a script which identified minicom's process ID and
then sent a 'kill -15 <pid>' to it.

If I did it that way, say the script were called "set.etrex".
Then, using the 'system' command

  system("set.etrex")

I could set it up from within an R session.

On Unix/Linux systems there is a program 'setserial' which can be
used to set up the serial port, but it seems it can only be used
to set baud rate (and in a somewhat obscure way); I've seen nothing
which indicates how to use it to set data bits, parity, and stop
bits. So as far as I know 'minicom' is the only program I have
available which can do the lot (short of getting out a serial port
manual and writing a C program which will talk directly to the
hardware port!).

If other readers know better, I'd be very grateful to hear of it!

(Come to think of it, that's just the sort of question to ask my
mates on Linux lists ... )

Once that was done (and checked by reading a few lines of output
to screen by 'cat /dev/ttyS0', closed by Ctrl-C) I then went into
R, and did the things like

  scan(file="/dev/ttyS0",n=1,what="character")

Since the serial port was already set up, and receiving the output
from the instrument, all R was doing was reading this from the
serial port. 

Now all this of course is written in terms of a Linux system,
and we don't know yet what sort of system you are using. On Windows,
I find one can navigate by hand through

  My Computer -> Control Panel -> System -> Device Manager
  -> Ports -> Communications Port (COM1) -> Port settings

where again one can manually set "Bits per second", "Data bits",
"Stop bits" and "Flow control" but, again, I don't know of a
program which can be used to set these "non-manually".

Anyway, the summary of the way I did it is to set the serial
port parameters independently of R, connect the device and get
it sending data, and then within R use 'scan' to read the port.

Initially one may capture an incomplete first line from the
port, since R will start with whatever is sitting on the serial
port's data lines when 'scan' is invoked. However, I have rarely
seen this and indeed, even at 4800 baud, it is not very likely.
The GPS device sends a line of data for every second of time,
consisting of 57 ASCII characters including the terminal
CRLF, which is a total of 9*57 = 413 bits at "7S1" taking
513/4800 approx= 1/10 seconds, so it is only about 1/10 chance
of catching an incomplete first line.

However, there are various ways you can arrange to avoid such
incomplete data being processed:

a) Get R reading the serial port before the device starts
   sending data. The R will already be looking for data before
   the first line starts to come through;
b) Simply dump the first line read by R;
c) Read it anyway, but if its length (as a character string) is
   too short, then dump it (use the R function 'nchar' for this);

This is always the issue when dealing with hardware interfaces:
mere logic is not enough -- you have to plan how to cope with
unruly behaviour as well!

Of course, the data string output from your device may already
consist of distinct fields separated by whitespace or by a separator
character such as "," (as opposed to my GPS data I used as an
example, where the fields are defined by position). In that case,
since 'scan' does have the capability to split the string into
fields by the separator, you can use the "what=list(...)" option.
E.g.

> E<-scan(what=list(Name="",Num=as.integer(1),Wt=1),n=1,sep=",")
1: "Ted",1,70.1
2: 
Read 1 records
> E
$Name
[1] "Ted"

$Num
[1] 1

$Wt
[1] 70.1

> typeof(E$Name)
[1] "character"
> typeof(E$Num)
[1] "integer"
> typeof(E$Wt)
[1] "double"

I'm a bit unsure from the documentation "?scan" exactly how this
works, but emprically it seems that if in

  "what=list(name1=A,name2=B,name3=C)"

you put instances of types (in this example A is "" (string),
B is as,integer(1) (integer), C is 1 (number)) then the result
of 'scan' is a list whose elements have these types, as verified
above by 'typeof'.

But I confess I have few general ideas about how to handle the
situation where the output of the device is lines which are
variable-lengh records, or even worse variable-type. Here, I think,
you have to roll up your sleeves and do "intelligent programming"!

Feel free to ask further questions -- it's an interesting topic,
not often discussed, and I hope that R experts in this field will
intervene!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 14-Sep-05                                       Time: 01:11:39
------------------------------ XFMail ------------------------------



From mcclatchie.sam at saugov.sa.gov.au  Wed Sep 14 03:36:00 2005
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Wed, 14 Sep 2005 11:06:00 +0930
Subject: [R] R CMD INSTALL -l /path/to/library packagename/ fixed
Message-ID: <BEA6A7E18959A04385DC14D24619F89F01D73C97@sagemsg0008.sagemsmrd01.sa.gov.au>

Background:
OS: Linux Mandrake 10.1
release: R 2.1.1
editor: GNU Emacs 21.3.2
front-end: ESS 5.2.3
---------------------------------

Colleagues

The environment variables checked with Sys.getenv() all appeared to be in
the right place.

The easy fix was to copy /usr/lib/R into /usr/local/lib/R
then run the R CMD INSTALL packagename
then just copy the package subdirectory back to  /usr/lib/R/library

A cheat fix, but it worked.

Sam

>-----Original Message-----
>From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
>Sent: Tuesday, 13 September 2005 3:41 PM
>To: McClatchie, Sam (PIRSA-SARDI)
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] R CMD INSTALL -l /path/to/library packagename
>
>
>I don't use Linux but perhaps you should check
>what environment variables you have defined
>and also if you have anything in your *.site
>files, if you have them, that could cause that.
>


----
Sam McClatchie,
Biological oceanography 
South Australian Aquatic Sciences Centre
PO Box 120, Henley Beach 5022
Adelaide, South Australia
email <mcclatchie.sam at saugov.sa.gov.au>
Cellular: 0431 304 497 
Telephone: (61-8) 8207 5448
FAX: (61-8) 8207 5481
Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
  
                   /\
      ...>><xX(??> 
                //// \\\\
                   <??)Xx><<
              /////  \\\\\\
                        ><(((??> 
  >><(((??>   ...>><xX(??>O<??)Xx><<



From kerryrekky at yahoo.com  Wed Sep 14 05:34:18 2005
From: kerryrekky at yahoo.com (Cunningham Kerry)
Date: Tue, 13 Sep 2005 20:34:18 -0700 (PDT)
Subject: [R] matrix calculation?
Message-ID: <20050914033418.30550.qmail@web51802.mail.yahoo.com>

Hi,
  I am wondering if anybody has a simpler solution to
calculate the
following:

> ma
     a b
[1,] 1 4
[2,] 2 3
[3,] 3 2
[4,] 4 5
> pa   
  [,1] [,2] [,3] [,4]
a    1    2    3    4
b    4    3    2    5
> diag(ma%*%pa)
[1] 17 13 13 41

I only want to calculate the product of the row of the
first matrix and
the corresponding column of the second matrix (instead
of other
columns). However, what I did was to multiply these
two and then extract
the diagonal components. Is there a simpler way to do
this in R? (ma and pa may not be the same matrix.)

Thanks!


	
		
______________________________________________________ 

Donate to the Hurricane Katrina relief effort.



From sfalcon at fhcrc.org  Wed Sep 14 06:28:48 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 13 Sep 2005 21:28:48 -0700
Subject: [R] is library loaded
In-Reply-To: <20050913215647.KHFI21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>
	(John Fox's message of "Tue, 13 Sep 2005 17:56:48 -0400")
References: <20050913215647.KHFI21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <m2acigw9vj.fsf@fhcrc.org>

On 13 Sep 2005, jfox at mcmaster.ca wrote:
> packageLoaded() may well be a bad name but loadedNamespaces() won't
> detect a package without a namespace. 

Right, that's a problem.

> It therefore seemed safe to me to check the path, which would
> include both packages with and without namespaces. With respect to
> loading and attaching, I thought that library() both loaded a
> package (with or without a namespace) and attached it to the search
> path, but I must admit that I'm easily confused about these
> distinctions.

As I understand it, library(foo) will load and attach package "foo".
If foo has a namespace, some of foo's dependencies may get loaded but
not attached.  This is only possible if said dependencies also use
namespaces.

So it is possible for a package to be loaded and not attached.  In
this case, the loaded package is not visible via search(), but is
visible via loadedNamespaces() since only packages with namespaces can
be loaded and not attached.

Clear as mud?

HTH,

+ seth



From ggrothendieck at gmail.com  Wed Sep 14 06:29:33 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Sep 2005 00:29:33 -0400
Subject: [R] matrix calculation?
In-Reply-To: <20050914033418.30550.qmail@web51802.mail.yahoo.com>
References: <20050914033418.30550.qmail@web51802.mail.yahoo.com>
Message-ID: <971536df05091321294df2029a@mail.gmail.com>

Try this:

rowSums(ma * t(pa))

On 9/13/05, Cunningham Kerry <kerryrekky at yahoo.com> wrote:
> Hi,
>  I am wondering if anybody has a simpler solution to
> calculate the
> following:
> 
> > ma
>     a b
> [1,] 1 4
> [2,] 2 3
> [3,] 3 2
> [4,] 4 5
> > pa
>  [,1] [,2] [,3] [,4]
> a    1    2    3    4
> b    4    3    2    5
> > diag(ma%*%pa)
> [1] 17 13 13 41
> 
> I only want to calculate the product of the row of the
> first matrix and
> the corresponding column of the second matrix (instead
> of other
> columns). However, what I did was to multiply these
> two and then extract
> the diagonal components. Is there a simpler way to do
> this in R? (ma and pa may not be the same matrix.)
> 
> Thanks!
> 
> 
> 
> 
> ______________________________________________________
> 
> Donate to the Hurricane Katrina relief effort.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Wed Sep 14 08:25:13 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 14 Sep 2005 14:25:13 +0800
Subject: [R] Reading data from a serial port
Message-ID: <4702645135092E4497088F71D9C8F51A128C2F@afhex01.dpi.wa.gov.au>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
> Ted.Harding at nessie.mcc.ac.uk
> Sent: Wednesday, 14 September 2005 8:13 AM
> To: vittorio
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Reading data from a serial port
> 
snip

> Now all this of course is written in terms of a Linux system,
> and we don't know yet what sort of system you are using. On Windows,
> I find one can navigate by hand through
> 
>   My Computer -> Control Panel -> System -> Device Manager
>   -> Ports -> Communications Port (COM1) -> Port settings
> 
> where again one can manually set "Bits per second", "Data bits",
> "Stop bits" and "Flow control" but, again, I don't know of a
> program which can be used to set these "non-manually".
> 
I think that windows would use com1 for /dev/ttyS0 so one could use a batch file in windows with something like
mode com1:4800,0,7,1 

I recall somewhere about setting parity to 0 for space parity, but I don't have a DOS manual here at work.

snip
> 

Tom



From dieter.menne at menne-biomed.de  Wed Sep 14 09:23:24 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 14 Sep 2005 07:23:24 +0000 (UTC)
Subject: [R] Anyone have any code for importing data from NAMCS?
References: <d332d3e105091313296daf0fcb@mail.gmail.com>
Message-ID: <loom.20050914T092206-981@post.gmane.org>

David L. Van Brunt, Ph.D. <dlvanbrunt <at> gmail.com> writes:

> 
> The National Ambulatory and Medical Care Survey is a free data set from the 
> CDC that I'd like to analyze using the "Survey" package in R. Before I dive 
> in, though, it occurred to me that someone may already have gone to the 
> trouble of writing code that will bring in the data and assign the variable 
> names and value labels.


At least the 2002 and 2003 files are available as SPSS macros (not as SPSS 
file, as far I know). So if you know someone with SPSS, ask her to run the 
macros, and read in the data with R and package foreign.

Dieter Menne



From h.andersson at nioo.knaw.nl  Wed Sep 14 10:14:59 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Wed, 14 Sep 2005 10:14:59 +0200
Subject: [R] Long lines with Sweave
Message-ID: <dg8ma8$e3h$1@sea.gmane.org>

I have used Sweave a lot the latest year, but never really used any long 
function calls.


If I have code which look like this

-------------------------------------------------------------
gof <- benthic.flux(ID="Gulf of Finland",
                     meas.conc=conc,
                     bw.conc=bw.conc,
                     time=times,
                     substance=expression(DIC~(mmol~m^{-3}))
                     )
-------------------------------------------------------------

I get the output by Sweave in my pdf file, like this:

---------------------------------------------------------------
 > gof <- benthic.flux(ID = "Gulf of Finland", meas.conc = conc,
+ bw.conc = bw.conc, time = times, substance = expression(DIC ~
+ (mmol ~ m^{
+ -3
+ })))
----------------------------------------------------------------

I can understand that it will not look exactly as entered but why is the 
'-3' on a line of it's own?

Can anyone suggest a idea to how I can make this more readable.

---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From maechler at stat.math.ethz.ch  Wed Sep 14 10:40:02 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 14 Sep 2005 10:40:02 +0200
Subject: [R] is *package* loaded
In-Reply-To: <m2acigw9vj.fsf@fhcrc.org>
References: <20050913215647.KHFI21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>
	<m2acigw9vj.fsf@fhcrc.org>
Message-ID: <17191.57826.937020.670090@stat.math.ethz.ch>

>>>>> "Seth" == Seth Falcon <sfalcon at fhcrc.org>
>>>>>     on Tue, 13 Sep 2005 21:28:48 -0700 writes:

    Seth> On 13 Sep 2005, jfox at mcmaster.ca wrote:

    >> packageLoaded() may well be a bad name but loadedNamespaces() won't
    >> detect a package without a namespace. 

    Seth> Right, that's a problem.

    >> It therefore seemed safe to me to check the path, which would
    >> include both packages with and without namespaces. With respect to
    >> loading and attaching, I thought that library() both loaded a
    >> package (with or without a namespace) and attached it to the search
    >> path, 

that's correct.  But still your proposed function isn't doing
what its name suggests; so its name is really very misleading 
or "bad" as Robert said.
OTOH, the name could be quite good if it's implementation
changed:

packageLoaded <- function(name)
{
    ## Purpose: is package 'name' loaded?
    ## --------------------------------------------------
    (paste("package:", name, sep="") %in% search()) ||
    (name %in% loadedNamespaces())
}    



    >> but I must admit that I'm easily confused about these distinctions.

    Seth> As I understand it, library(foo) will load and attach package "foo".

correct

    Seth> If foo has a namespace, some of foo's dependencies may get loaded but
    Seth> not attached.  This is only possible if said dependencies also use
    Seth> namespaces.
    Seth> So it is possible for a package to be loaded and not attached.

Yes.  There's another maybe even more common case of package
loading without attaching:
e.g.  using   MASS::rlm(...)  anywhere in your code silently
loads the MASS package but doesn't attach it.

    Seth> In this case, the loaded package is not visible via search(), but is
    Seth> visible via loadedNamespaces() since only packages with namespaces can
    Seth> be loaded and not attached.

Indeed.
Further note that "package loading" is more than just loading the
exported R symbols from the namespace.  E.g., it also dyn.load()s
the ./src/ stuff [ such that in the example, MASS::rlm() can
work at all ].

    Seth> Clear as mud?

    Seth> HTH,

    Seth> + seth

Martin



From nhy303 at abdn.ac.uk  Wed Sep 14 10:49:03 2005
From: nhy303 at abdn.ac.uk (nhy303@abdn.ac.uk)
Date: Wed, 14 Sep 2005 09:49:03 +0100 (BST)
Subject: [R] adf test and cross-correlation with missing values
Message-ID: <1248.81.178.117.154.1126687743.squirrel@www.abdn.ac.uk>

Dear List,

I have multiple time series, all of which (excepting 1) have missing
values.  These run for ~30 years, with monthly sampling.  I need to
determine stationarity, and have tried to use the Augmented Dickey-Fuller
test (adf.test), but this cannot handle missing values.  The same problem
occurs when attempting cross-correlation (ccf).

Could someone please suggest any suitable functions in R to check for
stationarity and to look at cross-correlation when NAs are present in a
time series (and also, which packages these would be in) - or, do I have
to interpolate the missing values first in order to perform these tests on
my time series?

Thankyou,

Lillian.



From jtk at cmp.uea.ac.uk  Wed Sep 14 11:13:01 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 14 Sep 2005 10:13:01 +0100
Subject: [R] Long lines with Sweave
In-Reply-To: <dg8ma8$e3h$1@sea.gmane.org>
References: <dg8ma8$e3h$1@sea.gmane.org>
Message-ID: <20050914091301.GA14109@jtkpc.cmp.uea.ac.uk>

On Wed, Sep 14, 2005 at 10:14:59AM +0200, Henrik Andersson wrote:
> I have used Sweave a lot the latest year, but never really used any long 
> function calls.
> 
> 
> If I have code which look like this
> 
> -------------------------------------------------------------
> gof <- benthic.flux(ID="Gulf of Finland",
>                      meas.conc=conc,
>                      bw.conc=bw.conc,
>                      time=times,
>                      substance=expression(DIC~(mmol~m^{-3}))
>                      )
> -------------------------------------------------------------
> 
> I get the output by Sweave in my pdf file, like this:
> 
> ---------------------------------------------------------------
>  > gof <- benthic.flux(ID = "Gulf of Finland", meas.conc = conc,
> + bw.conc = bw.conc, time = times, substance = expression(DIC ~
> + (mmol ~ m^{
> + -3
> + })))
> ----------------------------------------------------------------
> 
> I can understand that it will not look exactly as entered but why is the 
> '-3' on a line of it's own?
> 
> Can anyone suggest a idea to how I can make this more readable.

It seems you've been thinking LaTeX rather than R ;-)  :
The exponent "-3" in the expression should be enclosed by parentheses
rather than by curly braces.

The code formatting done by the print method inserts the newline after
"{" and before "}".

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From Colin.Beale at rspb.org.uk  Wed Sep 14 12:40:00 2005
From: Colin.Beale at rspb.org.uk (Beale, Colin)
Date: Wed, 14 Sep 2005 11:40:00 +0100
Subject: [R] Graphical presentation of logistic regression
Message-ID: <E71A2CDB5DFFD14383BAC2E0612811BA4B6D22@ADARA.RSPB.ORG.UK>

Hi,

I wonder if anyone has written any code to implement the suggestions of
Smart et al (2004) in the Bulletin of the Ecological Society of America
for a new way of graphically presenting the results of logistic
regression (see
www.esapubs.org/bulletin/backissues/085-3/bulletinjuly2004_2column.htm#t
ools1 for the full text)? I couldn't find anything relating to this sort
of graphical representation of logistic models in the archives, but
maybe someone has solved it already? In short, Smart et al suggest that
a logistic regression be presented as a combination of the two
histograms for successes and failures (with one presented upside down at
the top of the figure, the other the right way up at the bottom)
overlaid by the probability function (ie logistic curve). It's somewhat
hard to describe, but is nicely illustrated in the full text version
above. I think it is a sensible way of presenting these results and am
keen to do so - at the moment I can only do this by generating the two
histograms and the logistic curve separately (using hist() and lines()),
then copying and pasting the graphs out of R and inverting one in a
graphics package, before overlying the others. I'm sure this could be
done within R and would be a handy plotting function to develop. Has
anyone done so, or can anyone give me any pointers to doing this? I
really nead to know how to invert a histogram and how to overlay this
with another histogram "the right way up".

Any thoughts would be welcome.

Thanks in advance,
Colin

...



From bob.ohara at helsinki.fi  Wed Sep 14 12:58:50 2005
From: bob.ohara at helsinki.fi (Anon.)
Date: Wed, 14 Sep 2005 13:58:50 +0300
Subject: [R] Graphical presentation of logistic regression
References: <E71A2CDB5DFFD14383BAC2E0612811BA4B6D22@ADARA.RSPB.ORG.UK>
Message-ID: <4328026A.7040801@helsinki.fi>

Beale, Colin wrote:
> Hi,
> 
> I wonder if anyone has written any code to implement the suggestions of
> Smart et al (2004) in the Bulletin of the Ecological Society of America
> for a new way of graphically presenting the results of logistic
> regression (see
> www.esapubs.org/bulletin/backissues/085-3/bulletinjuly2004_2column.htm#t
> ools1 for the full text)? I couldn't find anything relating to this sort
> of graphical representation of logistic models in the archives, but
> maybe someone has solved it already? In short, Smart et al suggest that
> a logistic regression be presented as a combination of the two
> histograms for successes and failures (with one presented upside down at
> the top of the figure, the other the right way up at the bottom)
> overlaid by the probability function (ie logistic curve). It's somewhat
> hard to describe, but is nicely illustrated in the full text version
> above. I think it is a sensible way of presenting these results and am
> keen to do so - at the moment I can only do this by generating the two
> histograms and the logistic curve separately (using hist() and lines()),
> then copying and pasting the graphs out of R and inverting one in a
> graphics package, before overlying the others. I'm sure this could be
> done within R and would be a handy plotting function to develop. Has
> anyone done so, or can anyone give me any pointers to doing this? I
> really nead to know how to invert a histogram and how to overlay this
> with another histogram "the right way up".
> 
> Any thoughts would be welcome.
> 
My reaction was that I had seen some R code in a Bulletin of the ESA 
that someone sent me.  A quick search revealed this:
<http://www.esapubs.org/bulletin/backissues/086-1/bulletinjan2005.htm#et>
which has the code.

Bob

-- 
Bob O'Hara

Dept. of Mathematics and Statistics
P.O. Box 68 (Gustaf H??llstr??min katu 2b)
FIN-00014 University of Helsinki
Finland

Telephone: +358-9-191 51479
Mobile: +358 50 599 0540
Fax:  +358-9-191 51400
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: http://www.jnr-eeb.org



From f.harrell at vanderbilt.edu  Wed Sep 14 13:29:11 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 14 Sep 2005 06:29:11 -0500
Subject: [R] Graphical presentation of logistic regression
In-Reply-To: <E71A2CDB5DFFD14383BAC2E0612811BA4B6D22@ADARA.RSPB.ORG.UK>
References: <E71A2CDB5DFFD14383BAC2E0612811BA4B6D22@ADARA.RSPB.ORG.UK>
Message-ID: <43280987.1080403@vanderbilt.edu>

Beale, Colin wrote:
> Hi,
> 
> I wonder if anyone has written any code to implement the suggestions of
> Smart et al (2004) in the Bulletin of the Ecological Society of America
> for a new way of graphically presenting the results of logistic
> regression (see
> www.esapubs.org/bulletin/backissues/085-3/bulletinjuly2004_2column.htm#t
> ools1 for the full text)? I couldn't find anything relating to this sort
> of graphical representation of logistic models in the archives, but
> maybe someone has solved it already? In short, Smart et al suggest that
> a logistic regression be presented as a combination of the two
> histograms for successes and failures (with one presented upside down at
> the top of the figure, the other the right way up at the bottom)
> overlaid by the probability function (ie logistic curve). It's somewhat
> hard to describe, but is nicely illustrated in the full text version
> above. I think it is a sensible way of presenting these results and am
> keen to do so - at the moment I can only do this by generating the two
> histograms and the logistic curve separately (using hist() and lines()),
> then copying and pasting the graphs out of R and inverting one in a
> graphics package, before overlying the others. I'm sure this could be
> done within R and would be a handy plotting function to develop. Has
> anyone done so, or can anyone give me any pointers to doing this? I
> really nead to know how to invert a histogram and how to overlay this
> with another histogram "the right way up".
> 
> Any thoughts would be welcome.
> 
> Thanks in advance,
> Colin

 From what you describe, that is a poor way to represent the model 
except for judging discrimination ability (if the model is calibrated 
well).  Effect plots, odds ratio charts, and nomograms are better.  See 
the Design package for details.


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From B.Rowlingson at lancaster.ac.uk  Wed Sep 14 14:10:34 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 14 Sep 2005 13:10:34 +0100
Subject: [R] Graphical presentation of logistic regression
In-Reply-To: <43280987.1080403@vanderbilt.edu>
References: <E71A2CDB5DFFD14383BAC2E0612811BA4B6D22@ADARA.RSPB.ORG.UK>
	<43280987.1080403@vanderbilt.edu>
Message-ID: <4328133A.6090307@lancaster.ac.uk>

Frank E Harrell Jr wrote:

>>I
>>really nead to know how to invert a histogram and how to overlay this
>>with another histogram "the right way up".
>>

  Flipping the last two numbers in par()$usr has this effect...

>  From what you describe, that is a poor way to represent the model 
> except for judging discrimination ability (if the model is calibrated 
> well).  Effect plots, odds ratio charts, and nomograms are better.  See 
> the Design package for details.

 From the poor-diagnostics-R-us dept, here's something to work from:


logDiag <- function(x,y){

   d0=x[y==0]
   d1=x[y==1]

   h0=hist(d0,plot=FALSE)
   h1=hist(d1,plot=FALSE, breaks=h0$breaks)

# set the xlim so the stalactites dont hit the stalagmites:
   plot(h0, ylim=c(0,max(c(h0$counts,h1$counts))*2))
   pu=par()$usr

# flip the Y-axis limits
   par(usr=pu[c(1,2,4,3)])

# draw the stalactites
   lines(h1)

# reset axis
   par(usr=pu)

}

sample:

  xyd=data.frame(x=runif(1000),y=as.numeric(runif(1000)>.2))
  logDiag(xyd$x,xyd$y)

  lets just hope this plot isn't patented like that baseball diamond 
plot a few years ago. My lawyer is ready...

Baz



From Ted.Harding at nessie.mcc.ac.uk  Wed Sep 14 14:15:11 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 14 Sep 2005 13:15:11 +0100 (BST)
Subject: [R] Reading data from a serial port
In-Reply-To: <200509132329.36578.vdemart1@tin.it>
Message-ID: <XFMail.050914131511.Ted.Harding@nessie.mcc.ac.uk>

On 13-Sep-05 vittorio wrote:
>> Hoping this helps,
> 
> Thanks, it helps! But Ted,
> how do you let R know the parameters of the serial connection
> (e.g. "4800 7S1") ?
> Ciao
> Vittorio

Following up, I've now had info from people pointing out the
following.

For Windows, there's a simple DOS utility which, for a serial port,
is one the lines of

  MODE COM1:<speed>,<parity>,<databits>,<stopbuts>[,P]

where the optional "P" is to allow infinite retries to send data
to a non-responding device. This shouldn't be necessary when
passively reading data being output from external equipment.

Any of the above can be omitted (in which case the corresponding
setting is not changed) provided the requisite commas are present.
Example:

  MODE COM1:4800,E,7,1

will set 4800 baud, Even parity, 7 databits and 1 stop bit.

Thanks to Tom Mulholland for reminding me of this!

For Linux, there is the 'stty' command (which can set far more
things as well, since it is designed for terminal consoles
connected via serial lines). Something like

  stty -F /dev/ttyS0 4800 parenb -parodd cs7 -cstopb

would have the same effect as the above. See "man stty" for more
details.

So, since there is a simple command foreither Windows or Linux,
this can be sent from within an R session using the 'system'
command, which will set up the serial port. After this, 'scan'
should simply read the incoming data (as discussed earlier).

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 14-Sep-05                                       Time: 13:09:55
------------------------------ XFMail ------------------------------



From h.andersson at nioo.knaw.nl  Wed Sep 14 14:49:56 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Wed, 14 Sep 2005 14:49:56 +0200
Subject: [R] Long lines with Sweave
In-Reply-To: <20050914091301.GA14109@jtkpc.cmp.uea.ac.uk>
References: <dg8ma8$e3h$1@sea.gmane.org>
	<20050914091301.GA14109@jtkpc.cmp.uea.ac.uk>
Message-ID: <dg96dl$uiu$1@sea.gmane.org>

Jan T. Kim wrote:
> On Wed, Sep 14, 2005 at 10:14:59AM +0200, Henrik Andersson wrote:
> 
>>I have used Sweave a lot the latest year, but never really used any long 
>>function calls.
>>
>>
>>If I have code which look like this
>>
>>-------------------------------------------------------------
>>gof <- benthic.flux(ID="Gulf of Finland",
>>                     meas.conc=conc,
>>                     bw.conc=bw.conc,
>>                     time=times,
>>                     substance=expression(DIC~(mmol~m^{-3}))
>>                     )
>>-------------------------------------------------------------
>>
>>I get the output by Sweave in my pdf file, like this:
>>
>>---------------------------------------------------------------
>> > gof <- benthic.flux(ID = "Gulf of Finland", meas.conc = conc,
>>+ bw.conc = bw.conc, time = times, substance = expression(DIC ~
>>+ (mmol ~ m^{
>>+ -3
>>+ })))
>>----------------------------------------------------------------
>>
>>I can understand that it will not look exactly as entered but why is the 
>>'-3' on a line of it's own?
>>
>>Can anyone suggest a idea to how I can make this more readable.
> 
> 
> It seems you've been thinking LaTeX rather than R ;-)  :
> The exponent "-3" in the expression should be enclosed by parentheses
> rather than by curly braces.
> 
> The code formatting done by the print method inserts the newline after
> "{" and before "}".
> 
> Best regards, Jan

If you look at demo(plotmath), I get the impression that m^(-3) does not 
give me the desired behavior.

I want to have -3 in superscript without visible parentheses.

Tricky!

---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From KINLEY_ROBERT at Lilly.com  Wed Sep 14 15:24:15 2005
From: KINLEY_ROBERT at Lilly.com (Robert Kinley)
Date: Wed, 14 Sep 2005 14:24:15 +0100
Subject: [R] non-central t   : R v.Splus
Message-ID: <OF502E8887.DB3685DC-ON8025707C.004426FE-8025707C.0049A6B9@EliLilly.lilly.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050914/d592c68b/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Wed Sep 14 15:42:10 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 14 Sep 2005 15:42:10 +0200
Subject: [R] [S] non-central t   : R v.Splus
References: <OF502E8887.DB3685DC-ON8025707C.004426FE-8025707C.0049A6B9@EliLilly.lilly.com>
Message-ID: <00d901c5b932$1b014aa0$0540210a@www.domain>

I think that you might find the following usefull:

http://www.biostat.wustl.edu/archives/html/s-news/2002-11/msg00079.html


I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Robert Kinley" <KINLEY_ROBERT at LILLY.COM>
To: <s-news at lists.biostat.wustl.edu>; <r-help at stat.math.ethz.ch>
Sent: Wednesday, September 14, 2005 3:24 PM
Subject: [S] non-central t : R v.Splus


> Hi
>
> For bureaucratic reasons beyond my control I need to rewrite an R 
> function
> (for producing operating characteristic curves) as an Splus function 
> (
> version 6 , windows XP ).
>
> The R function makes extensive use of the fact that the student's t
> distribution function  pt()  has a non-centrality parameter built in 
> ...
> sadly that parameter is not present in the Splus pt() function .
>
> However, the Splus f distribution function pf() does have such a 
> parameter
> , so I have tried to write my own non-central version of pt() based 
> around
> pf() , using the relationship between the t and F distributions.
>
> Unfortunately my success has been limited  ... I can only get 
> correct
> probabilities for part of the range of the quantile space , failing 
> when
> the quantile becomes small ... and I'm beginning to wonder whether 
> it's
> actually possible to do what I want at all , given that the range of 
> x in
> F(x) is [ 0:Inf ] while that in t(x) is [-Inf , Inf ] , and the
> non-central t  distribution is not symmetric ...
>
> Do any wiser heads than mine have any experience or advice to offer 
> ...
> ?
>
>        thanks          Bob Kinley
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From keiwani at arago.de  Wed Sep 14 15:42:38 2005
From: keiwani at arago.de (Shahrokh Keiwani)
Date: Wed, 14 Sep 2005 15:42:38 +0200
Subject: [R] *** saving files ***
Message-ID: <"H00001a9002d1ee8.1126705357.scalix-prod.arago.de*"@MHS>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050914/8001bb71/attachment.pl

From jtk at cmp.uea.ac.uk  Wed Sep 14 15:46:20 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 14 Sep 2005 14:46:20 +0100
Subject: [R] Long lines with Sweave
In-Reply-To: <dg96dl$uiu$1@sea.gmane.org>
References: <dg8ma8$e3h$1@sea.gmane.org>
	<20050914091301.GA14109@jtkpc.cmp.uea.ac.uk>
	<dg96dl$uiu$1@sea.gmane.org>
Message-ID: <20050914134620.GB16826@jtkpc.cmp.uea.ac.uk>

On Wed, Sep 14, 2005 at 02:49:56PM +0200, Henrik Andersson wrote:
> Jan T. Kim wrote:
> > On Wed, Sep 14, 2005 at 10:14:59AM +0200, Henrik Andersson wrote:
> > 
> >>I have used Sweave a lot the latest year, but never really used any long 
> >>function calls.
> >>
> >>
> >>If I have code which look like this
> >>
> >>-------------------------------------------------------------
> >>gof <- benthic.flux(ID="Gulf of Finland",
> >>                     meas.conc=conc,
> >>                     bw.conc=bw.conc,
> >>                     time=times,
> >>                     substance=expression(DIC~(mmol~m^{-3}))
> >>                     )
> >>-------------------------------------------------------------
> >>
> >>I get the output by Sweave in my pdf file, like this:
> >>
> >>---------------------------------------------------------------
> >> > gof <- benthic.flux(ID = "Gulf of Finland", meas.conc = conc,
> >>+ bw.conc = bw.conc, time = times, substance = expression(DIC ~
> >>+ (mmol ~ m^{
> >>+ -3
> >>+ })))
> >>----------------------------------------------------------------
> >>
> >>I can understand that it will not look exactly as entered but why is the 
> >>'-3' on a line of it's own?
> >>
> >>Can anyone suggest a idea to how I can make this more readable.
> > 
> > 
> > It seems you've been thinking LaTeX rather than R ;-)  :
> > The exponent "-3" in the expression should be enclosed by parentheses
> > rather than by curly braces.
> > 
> > The code formatting done by the print method inserts the newline after
> > "{" and before "}".
> > 
> > Best regards, Jan
> 
> If you look at demo(plotmath), I get the impression that m^(-3) does not 
> give me the desired behavior.
> 
> I want to have -3 in superscript without visible parentheses.
> 
> Tricky!

Ok, I see.

It seems to me that you could omit the curly braces in the example, I
don't see any differences between the title in the plots produced by

    plot(1:10, main = expression(DIC~(mmol~m^-3)))

and

    plot(1:10, main = expression(DIC~(mmol~m^{-3})))

For more complex exponents, you could try plain() to prevent them from
being wrongly grouped by operator precedence, as in

    plot(1:10, main = expression(DIC~(mmol~m^plain(-3 + t))))

Not exactly ideal for readability, however...

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From Matthias.Templ at statistik.gv.at  Wed Sep 14 16:04:23 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 14 Sep 2005 16:04:23 +0200
Subject: [R] *** saving files ***
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BABEF@xchg1.statistik.local>

Hi,

write.table(result, paste("path/file_",i,sep=""))

inside the for-loop should done this in a for( i in ... ) loop.

Or:
save(result, file=paste("path/file_",i,sep="")

See ?read.table and ?load for loading the files.

Best,
Matthias

> 
>    Hi,
> 
>    
> 
>    I need help  :o(
> 
>    
> 
>    I want that my function saves result files in a for()-loop 
> while it runs
>    automatically.
> 
>    
> 
>    the filenames must be saved like:
> 
>    
> 
>    file_1  -> for 1. result
> 
>    file_2  -> for 2. result
> 
>    file_3  -> for 3. result
> 
>    
> 
>    and
> 
>    .
> 
>    .
> 
>    .  
> 
>    
> 
>    file_n -> for n. result
> 
>    
> 
>    the file names are the same identified by _1, _2 , _3, ... , _n
> 
>    
> 
>    these files will loaded by a second function later in the 
> same sequence
>    (_1 to _n). 
> 
>    
> 
>    how can I do that ...
> 
>    
> 
>    
> 
>     
> 
>    
> 
>    
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From vincent at 7d4.com  Wed Sep 14 16:08:24 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Wed, 14 Sep 2005 16:08:24 +0200
Subject: [R] *** saving files ***
In-Reply-To: <"H00001a9002d1ee8.1126705357.scalix-prod.arago.de*"@MHS>
References: <"H00001a9002d1ee8.1126705357.scalix-prod.arago.de*"@MHS>
Message-ID: <43282ED8.5080002@7d4.com>

Shahrokh Keiwani a ??crit :

>    the filenames must be saved like:
>    file_1  -> for 1. result
>    file_2  -> for 2. result
>    file_n -> for n. result

for (i in 1:n)
{
myfilename = paste("file_" , i , sep="");
...
}
hih



From matthew_wiener at merck.com  Wed Sep 14 16:10:36 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 14 Sep 2005 10:10:36 -0400
Subject: [R] *** saving files ***
Message-ID: <4E9A692D8755DF478B56A2892388EE1F06B743@usctmx1118.merck.com>

Depending on the size of your objects, you may be able to just keep them in
a list, especially as you say you will need them in the same order later.

If because of memory constraints or for some other reason you really need to
have a separate file for each round, you can generate object and file names
using "paste", assign the object to the name with assign, and save the file:

(warning:  untested code)
for(i in 1:n){
	obj.name <- paste(base.obj.name, i, sep = ".")
	file.name <- paste(obj.name, "rda", sep = ".")  # or whatever other
file name you want
	this.obj <- my.fn.that.creates.object(my.arguments)
	assign(obj.name, this.obj)
	save(list = obj.name, file = file.name)
}

When retrieving the files, you can use a similar loop, but instead of using
"assign", you can use "get" to retrieve the value and assign it to another
variable.  Or, in interactive use, you know the name so you can refer to the
variable.

Hope this helps,

Matt


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shahrokh Keiwani
Sent: Wednesday, September 14, 2005 9:43 AM
To: r-help at stat.math.ethz.ch
Subject: [R] *** saving files ***


   Hi,

   

   I need help  :o(

   

   I want that my function saves result files in a for()-loop while it runs
   automatically.

   

   the filenames must be saved like:

   

   file_1  -> for 1. result

   file_2  -> for 2. result

   file_3  -> for 3. result

   

   and

   .

   .

   .  

   

   file_n -> for n. result

   

   the file names are the same identified by _1, _2 , _3, ... , _n

   

   these files will loaded by a second function later in the same sequence
   (_1 to _n). 

   

   how can I do that ...

   

   

    

   

   

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Wed Sep 14 16:16:14 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 14 Sep 2005 10:16:14 -0400
Subject: [R] *** saving files ***
In-Reply-To: <"H00001a9002d1ee8.1126705357.scalix-prod.arago.de*"@MHS>
Message-ID: <Pine.SGI.4.40.0509140958400.1336296-100000@origin.chass.utoronto.ca>


If your loop is from 1:n then you can do the following. suppose you call
the resulslts results 1:n using assign or something. so like this

for(i in 1:n){
assign("results_", i, sep=""), lm(bla bla))
save(get(


> for(i in 1:10){
+ temp <- paste("results_", i, sep="")
+ assign(temp, rnorm(i))
+ save(list=temp, file=temp)
+ }
> dir()
 [1] "results_1"  "results_10" "results_2"  "results_3"  "results_4"
 [6] "results_5"  "results_6"  "results_7"  "results_8"  "results_9"

P.S. If you need to call your results within the loop do something like
temp2<-get(temp)

and you can use temp2 as any regular object

HTH

On Wed, 14 Sep 2005, Shahrokh Keiwani wrote:

>    Hi,
>
>
>
>    I need help  :o(
>
>
>
>    I want that my function saves result files in a for()-loop while it runs
>    automatically.
>
>
>
>    the filenames must be saved like:
>
>
>
>    file_1  -> for 1. result
>
>    file_2  -> for 2. result
>
>    file_3  -> for 3. result
>
>
>
>    and
>
>    .
>
>    .
>
>    .
>
>
>
>    file_n -> for n. result
>
>
>
>    the file names are the same identified by _1, _2 , _3, ... , _n
>
>
>
>    these files will loaded by a second function later in the same sequence
>    (_1 to _n).
>
>
>
>    how can I do that ...
>
>
>
>
>
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From r-stats at arcriswell.com  Wed Sep 14 17:36:31 2005
From: r-stats at arcriswell.com (Andrew R. Criswell)
Date: Wed, 14 Sep 2005 17:36:31 +0200
Subject: [R] specification for glmmPQL
In-Reply-To: <40e66e0b05090410192b8bfb06@mail.gmail.com>
References: <431AB4DA.5040606@arcriswell.com>	
	<40e66e0b05090408265bdcfbae@mail.gmail.com>	
	<431B13CA.7000908@arcriswell.com>
	<40e66e0b05090410192b8bfb06@mail.gmail.com>
Message-ID: <4328437F.6090709@arcriswell.com>

  Dear Prof. Bates and Group:

I hope it is not to late to revisit this thread. My concern is with the 
difference in standard errors estimated from data that is arranged as 
grouped (data.1) and ungrouped (data.2). With the grouped data set, the 
effect of treatment is highly significant; with the data ungrouped, is 
is only marginally significant. My empirical findings depend on the 
choice of how to construct the data frame. Which is correct?

Best wishes,
Andrew

 > summary(fm.5 <- lmer(cbind(response, 100 - response) ~ expt +
+                      (1 | subject), data = data.1, family = binomial,
+                      method = "AGQ"))

Generalized linear mixed model fit using AGQ
Formula: cbind(response, 100 - response) ~ expt + (1 | subject)
   Data: data.1
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 2437.298 2443.161 -1214.649 2429.298
Random effects:
     Groups        Name    Variance    Std.Dev.
    subject (Intercept)    0.026600     0.16309
# of obs: 32, groups: subject, 8

Estimated scale (compare to 1)  8.669802

Fixed effects:
               Estimate Std. Error z value  Pr(>|z|)   
(Intercept)   0.3082489  0.0081604  37.774 < 2.2e-16 ***
expttreatment 0.2160440  0.0115933  18.635 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr)
expttretmnt -0.704
 >
 > summary(fm.6 <- lmer(response ~ expt + (1 | subject), data = data.2,
+                      family = binomial, method = "AGQ"))
Generalized linear mixed model fit using AGQ
Formula: response ~ expt + (1 | subject)
   Data: data.2
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 4298.023 4322.306 -2145.011 4290.023
Random effects:
     Groups        Name    Variance    Std.Dev.
    subject (Intercept)    0.015878     0.12601
# of obs: 3200, groups: subject, 8

Estimated scale (compare to 1)  1.007666

Fixed effects:
              Estimate Std. Error z value  Pr(>|z|)   
(Intercept)    0.30813    0.08075  3.8159 0.0001357 ***
expttreatment  0.21350    0.11473  1.8609 0.0627583 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr)
expttretmnt -0.704


Douglas Bates wrote:

>On 9/4/05, Andrew R. Criswell <r-stats at arcriswell.com> wrote:
>  
>
>>Hello Dr. Bates and group,
>>
>>I understand, the attached data file did not accompany my original
>>message. I have listed below the code used to create that file.
>>
>>data.1 <- data.frame(subject  = factor(rep(c("one", "two", "three", "four",
>>                                             "five", "six", "seven",
>>"eight"),
>>                                           each = 4),
>>                                       levels = c("one", "two", "three",
>>                                                  "four", "five", "six",
>>                                                  "seven", "eight")),
>>                     day      = factor(rep(c("one", "two", "three", "four"),
>>                                           times = 8),
>>                                       levels = c("one", "two", "three",
>>                                                  "four")),
>>                     expt     = rep(c("control", "treatment"), each = 16),
>>                     response = c(58, 63, 57, 54, 63, 59, 61, 53, 52, 62,
>>                                  46, 55, 59, 63, 58, 59, 62, 59, 64, 53,
>>                                  63, 75, 62, 64, 53, 58, 62, 53, 64, 72,
>>                                  65, 74))
>>
>>mtrx.1 <- matrix(apply(data.1[, -4], 2, function(x)
>>                 rep(x, 100 - data.1$response)), ncol = 3, byrow = F)
>>mtrx.2 <- matrix(apply(data.1[, -4], 2, function(x)
>>                 rep(x, data.1$response)), ncol = 3, byrow = F)
>>
>>data.2 <- data.frame(subject  = factor(c(mtrx.1[,1], mtrx.2[,1]),
>>                                       levels = c("one", "two", "three",
>>                                                  "four", "five", "six",
>>                                                  "seven", "eight")),
>>                     day      = factor(c(mtrx.1[,2], mtrx.2[,2]),
>>                                       levels = c("one", "two", "three",
>>                                                  "four")),
>>                     expt     = factor(c(mtrx.1[,3], mtrx.2[,3]),
>>                                       levels = c("control", "treatment")),
>>                     response = factor(c(rep("yes", nrow(mtrx.1)),
>>                                         rep("no", nrow(mtrx.2))),
>>                                       levels = c("yes", "no")))
>>
>>#-------------------------------------------------------------------------------#
>>    
>>
>
>Thanks for sending the data.
>
>In your first message you said that you got completely different
>results from glmmPQL when fitting the two models.  When I fit these
>models with glmmPQL I got quite similar parameter estimates.  The
>reported log-likelihood or AIC or BIC values are quite different but
>these values apply to a different model (the list weighted linear
>mixed model used in the PQL algorithm) and should not be used for a
>glmm model in any case.
>
>The fm4 results from lmer in the lme4 package (actually lmer is now in
>the Matrix package but that is only temporary) confirm those from
>glmmPQL.  The model fm3 when fit by lmer provides different standard
>errors but that is because the weights are not being appropriately
>adjusted in lmer.  We will fix that.
>
>In general I think it is safest to use the long form of the data as in
>your data.2.
>
>Here are the results from lmer applied to the long form.  The results
>from the Adaptive Gauss-Hermite Quadrature (AGQ) method are preferred
>to those from the PQL method because AGQ is a more accurate
>approximation to the log-likelihood of the GLMM model.  In this case
>the differences are minor.
>
>The log-likelihood reported here is an approximation to the
>log-likelihood of the GLMM model.
>
>  
>
>>(fm.4 <- lmer(response ~ expt + (1|subject), data.2, binomial))
>>    
>>
>Generalized linear mixed model fit using PQL 
>Formula: response ~ expt + (1 | subject) 
>   Data: data.2 
> Family: binomial(logit link)
>      AIC     BIC    logLik deviance
> 4298.026 4322.31 -2145.013 4290.026
>Random effects:
>     Groups        Name    Variance    Std.Dev. 
>    subject (Intercept)    0.015835     0.12584 
># of obs: 3200, groups: subject, 8
>
>Estimated scale (compare to 1)  0.9990621 
>
>Fixed effects:
>              Estimate Std. Error z value  Pr(>|z|)
>(Intercept)    0.30764    0.08075  3.8098 0.0001391
>expttreatment  0.21319    0.11473  1.8582 0.0631454
>  
>
>>(fm.4a <- lmer(response ~ expt + (1|subject), data.2, binomial, method = "AGQ"))
>>    
>>
>Generalized linear mixed model fit using AGQ 
>Formula: response ~ expt + (1 | subject) 
>   Data: data.2 
> Family: binomial(logit link)
>      AIC      BIC    logLik deviance
> 4298.023 4322.306 -2145.011 4290.023
>Random effects:
>     Groups        Name    Variance    Std.Dev. 
>    subject (Intercept)    0.015855     0.12592 
># of obs: 3200, groups: subject, 8
>
>Estimated scale (compare to 1)  1.007675 
>
>Fixed effects:
>              Estimate Std. Error z value  Pr(>|z|)
>(Intercept)    0.30811    0.08075  3.8156 0.0001358
>expttreatment  0.21352    0.11473  1.8611 0.0627322
>
>
>  
>
>>Douglas Bates wrote:
>>
>>    
>>
>>>On 9/4/05, Andrew R. Criswell <r-stats at arcriswell.com> wrote:
>>>
>>>      
>>>
>>>>Hello All,
>>>>
>>>>I have a question regarding how glmmPQL should be specified. Which of
>>>>these two is correct?
>>>>
>>>>summary(fm.3 <- glmmPQL(cbind(response, 100 - response) ~ expt,
>>>>                       data = data.1, random = ~ 1 | subject,
>>>>                       family = binomial))
>>>>
>>>>summary(fm.4 <- glmmPQL(response ~ expt, data = data.2,
>>>>                       random = ~ 1 | subject, family = binomial))
>>>>
>>>>One might think it makes no difference, but it does.
>>>>
>>>>I have an experiment in which 8 individuals were subjected to two types
>>>>of treatment, 100 times per day for 4 consecutive days. The response
>>>>given is binary--yes or no--for each treatment.
>>>>
>>>>I constructed two types of data sets. On Rfile-01.Rdata (attached here)
>>>>are data frames, data.1 and data.2. The information is identical but the
>>>>data are arranged differently between these two data frames. Data frame,
>>>>data.1, groups frequencies by subject, day and treatment. Data frame,
>>>>data.2, is ungrouped.
>>>>
>>>>        
>>>>
>>>I don't think your attached .Rdata file made it through the various
>>>filters on the lists or on my receiving email.  Could you send me a
>>>copy in a separate email message?
>>>
>>>
>>>      
>>>
>>>>Consistency of these data frames is substantiated by computing these
>>>>tables:
>>>>
>>>>ftable(xtabs(response ~ expt + subject + day,
>>>>            data = data.1))
>>>>ftable(xtabs(as.numeric(response) - 1 ~ expt + subject + day,
>>>>            data = data.2))
>>>>
>>>>If I ignore the repeated measurement aspect of the data, I get, using
>>>>glm, identical results (but for deviance and df).
>>>>
>>>>summary(fm.1 <- glm(cbind(response, 100 - response) ~ expt,
>>>>                   data = data.1, family = binomial))
>>>>
>>>>summary(fm.2 <- glm(response ~ expt, data = data.2,
>>>>                   family = binomial))
>>>>
>>>>However, if I estimate these two equations as a mixed model using
>>>>glmPQL, I get completely different results between the two
>>>>specifications, fm.3 and fm.4. Which one is right? The example which
>>>>accompanies help(glmmPQL) suggests fm.4.
>>>>
>>>>summary(fm.3 <- glmmPQL(cbind(response, 100 - response) ~ expt,
>>>>                       data = data.1, random = ~ 1 | subject,
>>>>                       family = binomial))
>>>>
>>>>summary(fm.4 <- glmmPQL(response ~ expt, data = data.2,
>>>>                       random = ~ 1 | subject, family = binomial))
>>>>
>>>>Thank you,
>>>>Andrew
>>>>
>>>>
>>>>
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>>
>>>>        
>>>>
>>>      
>>>
>>    
>>
>
>
>
>  
>



From maechler at stat.math.ethz.ch  Wed Sep 14 17:49:10 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 14 Sep 2005 17:49:10 +0200
Subject: [R] correlation as distance/dissimilarity
In-Reply-To: <a06230904bf4dd677966d@[160.111.236.58]>
References: <a06230904bf4dd677966d@[160.111.236.58]>
Message-ID: <17192.18038.349462.453663@stat.math.ethz.ch>

I've been asked (privately)

>>>>> "CarlosJ" == jaramilloc  <jaramilloc at si.edu>
>>>>>     on Wed, 14 Sep 2005 09:40:22 -0400 writes:

     ..........

    CarlosJ> In Kaufman & Rousseeuw 2000 book on Cluster Analysis, it says that 
    CarlosJ> Daisy can compute Pearson correlation between variables and then 
    CarlosJ> transform these to dissimilarities.  

I don't think it does say this.  But it does talk about doing it
"your self", e.g., on pages 17--19.

    CarlosJ> Has this capability being 
    CarlosJ> implemented in the Cluster package for R?  It seems that is not 
    CarlosJ> there.  How could I do that using R?
    CarlosJ> I would appreciate your help.

It has never been explicitly in R, because in the past 'everyone'
has thought this was obvious and trivial.  The "past" here was
when S was used by statisticians, mathematicians or engineers...

Anyway, here is an example on how to do this.

> dd <- as.dist((1 - cor(USJudgeRatings))/2)
> plot(hclust(dd))
> round(1000 * dd)
     CONT INTG DMNR DILG CFMG DECI PREP FAMI ORAL WRIT PHYS
INTG  567                                                  
DMNR  577   18                                             
DILG  494   64   82                                        
CFMG  432   93   93   21                                   
DECI  457   99   98   22    9                              
PREP  494   61   72   11   21   21                         
FAMI  513   66   79   21   32   29    5                    
ORAL  506   44   47   23   25   26    8    9               
WRIT  522   46   53   20   29   27    7    5    3          
PHYS  473  129  106   94   60   64   76   78   54   72     
RTEN  517   31   28   35   36   38   25   29    9   16   47

I'm going to add the example to the help page for 'dist' in R-2.2.0 

Martin Maechler



From Hathaikan.Chootrakool at newcastle.ac.uk  Wed Sep 14 18:01:04 2005
From: Hathaikan.Chootrakool at newcastle.ac.uk (Hathaikan Chootrakool)
Date: Wed, 14 Sep 2005 17:01:04 +0100 (BST)
Subject: [R] Random effect model
Message-ID: <1885.128.240.6.80.1126713664.squirrel@sws2.ncl.ac.uk>

Dear R-help group,

I would like to model directly following random effect model:

  Y_ik = M_ik +  E_ik  where M_ik  ~ N(Mew_k,tau_k^2)
                             E_ik  ~ N(0,s_ik^2)
  i = number of study
  k = number of treatment
---------------------------------------------------------------------------

I have practiced using the command  from 'Mixed -Effects models in S and
S-plus' as follow

fm1logit.lme <- lme(logitp~1, data=logit, random = ~1|factor(Tr))

It can be written in this model

Y_ik = Mew + B_i + E_ik  where M_i ~ N(0,sigma_b^2)
                               E_ik ~ N(0,sigma^2)


 but it is not the same what my model is.


Could somebody please point me in the right direction ?

 Sorry if this turns out to be an extreamly simple question, I'm a
 new user to R.

 Thank you very much,

 Ae



From bernarduse1 at yahoo.fr  Wed Sep 14 18:18:07 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Wed, 14 Sep 2005 18:18:07 +0200 (CEST)
Subject: [R] Apply a function for each Row
Message-ID: <20050914161807.77912.qmail@web25801.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050914/334df8ec/attachment.pl

From B.Rowlingson at lancaster.ac.uk  Wed Sep 14 18:21:23 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 14 Sep 2005 17:21:23 +0100
Subject: [R] Apply a function for each Row
In-Reply-To: <20050914161807.77912.qmail@web25801.mail.ukl.yahoo.com>
References: <20050914161807.77912.qmail@web25801.mail.ukl.yahoo.com>
Message-ID: <43284E03.80207@lancaster.ac.uk>

Marc Bernard wrote:
> Dear All,
>  
> I wonder how to apply a given function to  each row of a data frame. I've seen this function before  but don't remember its name....

  You've just said it twice!

'apply'!

Baz



From mcardeal at ufba.br  Wed Sep 14 19:09:52 2005
From: mcardeal at ufba.br (Carlos Mauricio Cardeal Mendes)
Date: Wed, 14 Sep 2005 14:09:52 -0300
Subject: [R] if() command
In-Reply-To: <4326FD39.7418.173D7F4@localhost>
References: <4326FD39.7418.173D7F4@localhost>
Message-ID: <43285960.3020507@ufba.br>

Ok Petr, I run your suggestion and I got this message:

 > age<-sample(seq(10,50,10), 20, replace=T)
 >
 > if (age <=10) {group <- 1} else if (age > 10 & age <= 20) {group <- 
2} else {group <- 3}
Warning message:
the condition has length > 1 and only the first element will be used in: 
if (age <= 10) {

What does it means ?

And when I look to the database I have no new classification !

Could you help please ?

Mauricio

Petr Pikal escreveu:

> Hallo
>
>
> On 13 Sep 2005 at 10:29, Carlos Maur??cio Cardeal Mende wrote:
>
>> Hi everyone !
>>
>> Could you please help me with this problem ?
>>
>> I??ve trying to write a code that assign to a variable the content from
>> another, but all I??ve got is a message error. For example:
>>
>> if (age <=10) {group == 1}
>> else if (age > 10 & age <= 20) {group == 2}
>> else {group == 3}
>
> if you put your statement on one line it works (at least it does not 
> give you syntax error) but the result is hardly what you really expect
>
> age<-sample(seq(10,50,10), 20, replace=T)
>
> if (age <=10) {group <- 1} else if (age > 10 & age <= 20) {group <- 2} 
> else {group <- 3}
> if (age <=10) {group == 1} else if (age > 10 & age <= 20) {group == 2} 
> else {group == 3}
>
> Maybe you want something like
>
> group<-as.numeric(cut(age,c(0,10,20,100)))
>
> but it is only guess
>
> HTH
> Petr
>
>>
>> Syntax error
>>
>> Or
>>
>> if (age <=10) {group == 1}
>> else (age > 10 & age <= 20) {group == 2}
>> else {group == 3}
>>
>> Syntax error
>>
>> I know that is possible to find the solution by ifelse command or even
>> recode command, but I??d like to use this way, because I can add
>> another variable as a new condition and I believe to expand the
>> possibilites.
>>
>> Thanks,
>> Mauricio
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>------------------------------------------------------------------------
>
>No virus found in this incoming message.
>Checked by AVG Anti-Virus.
>Version: 7.0.344 / Virus Database: 267.10.21/96 - Release Date: 10/9/2005
>  
>



From mcardeal at ufba.br  Wed Sep 14 19:20:52 2005
From: mcardeal at ufba.br (Carlos Mauricio Cardeal Mendes)
Date: Wed, 14 Sep 2005 14:20:52 -0300
Subject: [R] if() command
In-Reply-To: <355C35514FEAC9458F75947F5270974D076CBF@usctmx1103.merck.com>
References: <355C35514FEAC9458F75947F5270974D076CBF@usctmx1103.merck.com>
Message-ID: <43285BF4.80402@ufba.br>

Hello reid ! About your third explanation, could you please write the 
complete code including that option: a loop ?

Forgiveme,  I'm trying to learn R and my mind is full of  other 
statistical program syntax. And I'd like very very much to improve my 
knowledge using R and maybe contribute to someone, someday, somehow.

Thanks, again

Mauricio

Huntsinger, Reid escreveu:

>First, "==" is logical comparison, so if you want to create a variable based
>on both "age" and "group" you can do that. However, it looks like you want
>to define the variable "group", so you want to use "<-" or "=" for that. 
>
>Second, if you're typing this at a command prompt, you need to make sure you
>tell R you're not finished when it looks like you could be. There are
>several ways to do this. One is to put everything inside braces; another is
>to deliberately leave lines incomplete, like
>
>if (age <= 10) {
>   group <- 1
>} else {
>   if (age <= 20) {
>      group <- 2
>   } else group <- 3
>}
>
>Third, this will work for a vector of length 1. If you want to take a vector
>"age" and produce a corresponding vector "group", you'll need to put this in
>a loop, or use "lapply", or some iteration.
>
>Fourth, you can also write the above as 
>
>  
>
>>group <- if (age <= 10) 1 else if (age <= 20) 2 else 3
>>    
>>
>
>that is, if() returns a value you can assign.
>
>Finally, besides "ifelse" you can use "cut" for this particular task.
>
>Reid Huntsinger
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Carlos Maur??cio
>Cardeal Mendes
>Sent: Tuesday, September 13, 2005 9:29 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] if() command
>
>
>Hi everyone !
>
>Could you please help me with this problem ?
>
>I??ve trying to write a code that assign to a variable the content from 
>another, but all I??ve got is a message error. For example:
>
>if (age <=10) {group == 1}
>else if (age > 10 & age <= 20) {group == 2}
>else {group == 3}
>
>Syntax error
>
>Or
>
>if (age <=10) {group == 1}
>else (age > 10 & age <= 20) {group == 2}
>else {group == 3}
>
>Syntax error
>
>I know that is possible to find the solution by ifelse command or even 
>recode command, but I??d like to use this way, because I can add another 
>variable as a new condition and I believe to expand the possibilites.
>
>Thanks,
>Mauricio
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>
>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments...{{dropped}}



From sundar.dorai-raj at pdf.com  Wed Sep 14 19:20:44 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 14 Sep 2005 12:20:44 -0500
Subject: [R] if() command
In-Reply-To: <43285960.3020507@ufba.br>
References: <4326FD39.7418.173D7F4@localhost> <43285960.3020507@ufba.br>
Message-ID: <43285BEC.50002@pdf.com>

"if" is not vectorised and "age" is a vector. Try the following test:

if(c(TRUE, FALSE)) "TRUE" else "FALSE"

You really need to use "ifelse".

ifelse(c(TRUE, FALSE), "TRUE", "FALSE")

As others have suggested, you might want to look at ?cut.

--sundar

Carlos Mauricio Cardeal Mendes wrote:
> Ok Petr, I run your suggestion and I got this message:
> 
>  > age<-sample(seq(10,50,10), 20, replace=T)
>  >
>  > if (age <=10) {group <- 1} else if (age > 10 & age <= 20) {group <- 
> 2} else {group <- 3}
> Warning message:
> the condition has length > 1 and only the first element will be used in: 
> if (age <= 10) {
> 
> What does it means ?
> 
> And when I look to the database I have no new classification !
> 
> Could you help please ?
> 
> Mauricio
> 
> Petr Pikal escreveu:
> 
> 
>>Hallo
>>
>>
>>On 13 Sep 2005 at 10:29, Carlos Maur??cio Cardeal Mende wrote:
>>
>>
>>>Hi everyone !
>>>
>>>Could you please help me with this problem ?
>>>
>>>I??ve trying to write a code that assign to a variable the content from
>>>another, but all I??ve got is a message error. For example:
>>>
>>>if (age <=10) {group == 1}
>>>else if (age > 10 & age <= 20) {group == 2}
>>>else {group == 3}
>>
>>if you put your statement on one line it works (at least it does not 
>>give you syntax error) but the result is hardly what you really expect
>>
>>age<-sample(seq(10,50,10), 20, replace=T)
>>
>>if (age <=10) {group <- 1} else if (age > 10 & age <= 20) {group <- 2} 
>>else {group <- 3}
>>if (age <=10) {group == 1} else if (age > 10 & age <= 20) {group == 2} 
>>else {group == 3}
>>
>>Maybe you want something like
>>
>>group<-as.numeric(cut(age,c(0,10,20,100)))
>>
>>but it is only guess
>>
>>HTH
>>Petr
>>
>>
>>>Syntax error
>>>
>>>Or
>>>
>>>if (age <=10) {group == 1}
>>>else (age > 10 & age <= 20) {group == 2}
>>>else {group == 3}
>>>
>>>Syntax error
>>>
>>>I know that is possible to find the solution by ifelse command or even
>>>recode command, but I??d like to use this way, because I can add
>>>another variable as a new condition and I believe to expand the
>>>possibilites.
>>>
>>>Thanks,
>>>Mauricio
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>
>>
>>Petr Pikal
>>petr.pikal at precheza.cz
>>
>>------------------------------------------------------------------------
>>
>>No virus found in this incoming message.
>>Checked by AVG Anti-Virus.
>>Version: 7.0.344 / Virus Database: 267.10.21/96 - Release Date: 10/9/2005
>> 
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jeff.breiwick at noaa.gov  Wed Sep 14 19:19:13 2005
From: jeff.breiwick at noaa.gov (J.M. Breiwick)
Date: Wed, 14 Sep 2005 10:19:13 -0700
Subject: [R] nls()
References: <BF4C4B8E.1686%antoine@ruetter.ch>
Message-ID: <dg9m2q$pgi$1@sea.gmane.org>

Hi,

I am using nls() with the form: nls(~my.fcn(...)) because I have to 
iteratively compute the expected y values. The function my.fcn() returns 
y.obs-y.pred

However, I want to fix some of the parameters in my.fcn at various values 
and compute the parameter estimates. In Splus there is such a thing as a 
parameterized dataframe. I don't think this exists in R so does anyone know 
how to set one or more of the parameters as constants in the model? Thank 
you.

Jeff Breiwick



From Roger.Bivand at nhh.no  Wed Sep 14 19:40:54 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 14 Sep 2005 19:40:54 +0200 (CEST)
Subject: [R] if() command
In-Reply-To: <43285960.3020507@ufba.br>
Message-ID: <Pine.LNX.4.44.0509141928200.2914-100000@reclus.nhh.no>

On Wed, 14 Sep 2005, Carlos Mauricio Cardeal Mendes wrote:

> Ok Petr, I run your suggestion and I got this message:
> 
>  > age<-sample(seq(10,50,10), 20, replace=T)
>  >
>  > if (age <=10) {group <- 1} else if (age > 10 & age <= 20) {group <- 
> 2} else {group <- 3}
> Warning message:
> the condition has length > 1 and only the first element will be used in: 
> if (age <= 10) {
> 
> What does it means ?
> 
> And when I look to the database I have no new classification !
> 

Although the syntax issue is real, if() is not the way to go if you are 
comparing a vector with a scalar; if() will only compare the first element 
of the vector with the scalar. The ifelse() function is vectorised:

> age<-sample(seq(10,50,10), 20, replace=T)
> group_ifelse <- ifelse(age > 10, ifelse(age > 20, 3, 2), 1)
> group_ifelse
 [1] 3 2 3 3 3 2 3 3 1 2 3 1 1 1 3 2 3 2 3 3

or maybe even better, use the cut function to create a grouping factor:

> group_cut <- cut(age, breaks=c(0,10,20,100), include.lowest=TRUE)
> group_cut
 [1] (20,100] (10,20]  (20,100] (20,100] (20,100] (10,20]  (20,100] (20,100]
 [9] [0,10]   (10,20]  (20,100] [0,10]   [0,10]   [0,10]   (20,100] (10,20] 
[17] (20,100] (10,20]  (20,100] (20,100]
Levels: [0,10] (10,20] (20,100]
> age
 [1] 30 20 30 40 30 20 50 50 10 20 30 10 10 10 30 20 40 20 50 40
> as.integer(group_cut)
 [1] 3 2 3 3 3 2 3 3 1 2 3 1 1 1 3 2 3 2 3 3

Sometimes you need to enclose cut() within ordered(), and if there are 
empty intervals, you may not get what you expect from the integer 
representation of the result. Yet another elegant function is 
findInterval():

> group_findInterval <- findInterval(age, c(0,10.001,20.001,100))
> group_findInterval
 [1] 3 2 3 3 3 2 3 3 1 2 3 1 1 1 3 2 3 2 3 3

Hope this helps

> Could you help please ?
> 
> Mauricio
> 
> Petr Pikal escreveu:
> 
> > Hallo
> >
> >
> > On 13 Sep 2005 at 10:29, Carlos Maur??cio Cardeal Mende wrote:
> >
> >> Hi everyone !
> >>
> >> Could you please help me with this problem ?
> >>
> >> I??ve trying to write a code that assign to a variable the content from
> >> another, but all I??ve got is a message error. For example:
> >>
> >> if (age <=10) {group == 1}
> >> else if (age > 10 & age <= 20) {group == 2}
> >> else {group == 3}
> >
> > if you put your statement on one line it works (at least it does not 
> > give you syntax error) but the result is hardly what you really expect
> >
> > age<-sample(seq(10,50,10), 20, replace=T)
> >
> > if (age <=10) {group <- 1} else if (age > 10 & age <= 20) {group <- 2} 
> > else {group <- 3}
> > if (age <=10) {group == 1} else if (age > 10 & age <= 20) {group == 2} 
> > else {group == 3}
> >
> > Maybe you want something like
> >
> > group<-as.numeric(cut(age,c(0,10,20,100)))
> >
> > but it is only guess
> >
> > HTH
> > Petr
> >
> >>
> >> Syntax error
> >>
> >> Or
> >>
> >> if (age <=10) {group == 1}
> >> else (age > 10 & age <= 20) {group == 2}
> >> else {group == 3}
> >>
> >> Syntax error
> >>
> >> I know that is possible to find the solution by ifelse command or even
> >> recode command, but I??d like to use this way, because I can add
> >> another variable as a new condition and I believe to expand the
> >> possibilites.
> >>
> >> Thanks,
> >> Mauricio
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >------------------------------------------------------------------------
> >
> >No virus found in this incoming message.
> >Checked by AVG Anti-Virus.
> >Version: 7.0.344 / Virus Database: 267.10.21/96 - Release Date: 10/9/2005
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From reid_huntsinger at merck.com  Wed Sep 14 20:04:12 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 14 Sep 2005 14:04:12 -0400
Subject: [R] if() command
Message-ID: <355C35514FEAC9458F75947F5270974D076CCF@usctmx1103.merck.com>

Looping would look like:

group <- vector(length=length(age))

for (i in 1:length(age)) {
  group[i] <- if (age[i] <= 10) 1 else if (age[i] <= 20) 2 else 3
}

Another way to do this is to write a function, say "category", like

category <- function(x) if(x <= 10) 1 else if (x <= 20) 2 else 3

and then apply the function to all elements of "age" like

group <- sapply(age,category)

(This is a common way to vectorize a function.)

Reid Huntsinger

-----Original Message-----
From: Carlos Mauricio Cardeal Mendes [mailto:mcardeal at ufba.br] 
Sent: Wednesday, September 14, 2005 1:21 PM
To: Huntsinger, Reid
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] if() command


Hello reid ! About your third explanation, could you please write the 
complete code including that option: a loop ?

Forgiveme,  I'm trying to learn R and my mind is full of  other 
statistical program syntax. And I'd like very very much to improve my 
knowledge using R and maybe contribute to someone, someday, somehow.

Thanks, again

Mauricio

Huntsinger, Reid escreveu:

>First, "==" is logical comparison, so if you want to create a variable
based
>on both "age" and "group" you can do that. However, it looks like you want
>to define the variable "group", so you want to use "<-" or "=" for that. 
>
>Second, if you're typing this at a command prompt, you need to make sure
you
>tell R you're not finished when it looks like you could be. There are
>several ways to do this. One is to put everything inside braces; another is
>to deliberately leave lines incomplete, like
>
>if (age <= 10) {
>   group <- 1
>} else {
>   if (age <= 20) {
>      group <- 2
>   } else group <- 3
>}
>
>Third, this will work for a vector of length 1. If you want to take a
vector
>"age" and produce a corresponding vector "group", you'll need to put this
in
>a loop, or use "lapply", or some iteration.
>
>Fourth, you can also write the above as 
>
>  
>
>>group <- if (age <= 10) 1 else if (age <= 20) 2 else 3
>>    
>>
>
>that is, if() returns a value you can assign.
>
>Finally, besides "ifelse" you can use "cut" for this particular task.
>
>Reid Huntsinger
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Carlos Maur??cio
>Cardeal Mendes
>Sent: Tuesday, September 13, 2005 9:29 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] if() command
>
>
>Hi everyone !
>
>Could you please help me with this problem ?
>
>I??ve trying to write a code that assign to a variable the content from 
>another, but all I??ve got is a message error. For example:
>
>if (age <=10) {group == 1}
>else if (age > 10 & age <= 20) {group == 2}
>else {group == 3}
>
>Syntax error
>
>Or
>
>if (age <=10) {group == 1}
>else (age > 10 & age <= 20) {group == 2}
>else {group == 3}
>
>Syntax error
>
>I know that is possible to find the solution by ifelse command or even 
>recode command, but I??d like to use this way, because I can add another 
>variable as a new condition and I believe to expand the possibilites.
>
>Thanks,
>Mauricio
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>
>
>
>
>---------------------------------------------------------------------------
---
>Notice:  This e-mail message, together with any attachments...{{dropped}}



From par at hunter-gatherer.org  Wed Sep 14 20:17:59 2005
From: par at hunter-gatherer.org (Par Leijonhufvud)
Date: Wed, 14 Sep 2005 20:17:59 +0200
Subject: [R] Forcing hist()
Message-ID: <20050914181759.GF38765@absaroka.eryn-lasgalen.org>

I'm trying to create histogram (using hist()) that fullfill the following
criteria:

        * data is on a ordinal scale (1, 2, 3, 4, 5)
        * I want bars centered over the number on the x-axis
        * I want 5 bars of equal width

I have tried various versions of the hist() command, with no luck. what
am I missing?

/Par

-- 
Par Leijonhufvud                               par at hunter-gatherer.org
If you're not part of the solution, be part of the problem!



From andy_liaw at merck.com  Wed Sep 14 20:20:28 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Sep 2005 14:20:28 -0400
Subject: [R] Apply a function for each Row
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED42F@usctmx1106.merck.com>

> From: Barry Rowlingson
> 
> Marc Bernard wrote:
> > Dear All,
> >  
> > I wonder how to apply a given function to  each row of a 
> data frame. I've seen this function before  but don't 
> remember its name....
> 
>   You've just said it twice!
> 
> 'apply'!

A small catch:  Marc wants to apply the function to rows of a data frame,
but apply() expects a matrix or array, and will coerce to such if given a
data frame, which may (or may not) be problematic...

Andy

> Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From mschwartz at mn.rr.com  Wed Sep 14 20:32:27 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 14 Sep 2005 13:32:27 -0500
Subject: [R] Forcing hist()
In-Reply-To: <20050914181759.GF38765@absaroka.eryn-lasgalen.org>
References: <20050914181759.GF38765@absaroka.eryn-lasgalen.org>
Message-ID: <1126722747.4185.35.camel@localhost.localdomain>

On Wed, 2005-09-14 at 20:17 +0200, Par Leijonhufvud wrote:
> I'm trying to create histogram (using hist()) that fullfill the following
> criteria:
> 
>         * data is on a ordinal scale (1, 2, 3, 4, 5)
>         * I want bars centered over the number on the x-axis
>         * I want 5 bars of equal width
> 
> I have tried various versions of the hist() command, with no luck. what
> am I missing?
> 
> /Par


More than likely, you want to use barplot() and not hist():

Try:

  barplot(1:5, names.arg = 1:5)

See ?barplot for more information.

HTH,

Marc Schwartz

P.S. To R Core: It probably makes sense to add barplot to the See Also
section of ?hist, since hist is listed in the See Also for ?barplot.



From tinypenguin at gmail.com  Wed Sep 14 20:43:17 2005
From: tinypenguin at gmail.com (Ruixiao Lu)
Date: Wed, 14 Sep 2005 11:43:17 -0700
Subject: [R] Can I use "lme" to deal with grouping data when I only get one
	data point per group?
Message-ID: <a175c71d05091411437ec9c3e0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050914/dc9f66e3/attachment.pl

From pebrewin at sdsc.edu  Wed Sep 14 21:28:50 2005
From: pebrewin at sdsc.edu (Paul Brewin)
Date: Wed, 14 Sep 2005 12:28:50 -0700
Subject: [R] Converting coordinates to actual distances
Message-ID: <845AADAC1106E44996327D62097E4C6B6F5369@et.ad.sdsc.edu>

Hello,

I've been searching for a method of converting Lat/Lon decimal
coordinates into actual distances between points, and taking into
account the curvature of the earth.  Is there such a package in R?  I've
looked at the GeoR package, but this does not seem to contain what I am
looking for.  Ideally the output would be a triangular matrix of
distances.  

Thanks in advance, 
Paul Brewin



Paul E Brewin (PhD)
Center for Research in Biological Systems
University of California San Diego
9500 Gilman Drive MC 0505
La Jolla CA, 92093-0505
USA
 
Ph: 858-822-0871
Fax: 858-822-3631



From wdmccoy at geo.umass.edu  Wed Sep 14 22:01:33 2005
From: wdmccoy at geo.umass.edu (William McCoy)
Date: Wed, 14 Sep 2005 16:01:33 -0400
Subject: [R] maximum string length in RdbiPgSQL and in R
Message-ID: <4328819D.7060007@geo.umass.edu>

Because my problem involves the RdbiPgSQL package, I sent a message 
similar to this one to the Bioconductor list.  But while awaiting 
moderator approval of my message (because I am not a member of that 
list), it occurred to me to send it to R-help as the problem may be more 
general than just RdbiPgSQL.

Here's my situation:

I have been using RdbiPgSQL successfully for a year or two.  I commonly 
save my queries in text files that I can use either in PostgreSQL's psql 
(useful for testing and editing) or in R using readLines().  For example 
(in R):

library(RdbiPgSQL)
conn <- dbConnect(PgSQL(), host = "localhost", dbname = "agdb")
test.sql < readLines("queryfile")
test.df <- dbGetQuery(conn, paste(test.sql, collapse = " "))

This works fine for all the multiline files I have tried -- except one.
I have recently encountered a problem with a moderately complex, 
moderately long query (12 lines, 459 characters).  I can execute the 
query with no problem in psql and it returns the 14 rows that I expect. 
  When I execute the query in R as above, I get a dataframe with the 
expected column names, but no rows.  I get no error message.  I am 
wondering if the query string is too long.  Is there a maximum length 
for queries in RdbiPgSQL or for strings in R?

By the way, I can use collapse = "\n" in paste() and get the same 
result, so I don't think line length is the problem.

Or maybe someone has a better idea of how to read (in R) the file 
containing the query and sending it to my database.  Of course, I know I 
can execute the query outside of R and use read.table to make my 
dataframe, but I want to do this inside R.

Thanks for any ideas.


-- 

William D. McCoy
Geosciences
University of Massachusetts, Amherst
wdmccoy at geo.umass.edu



From dieter.menne at menne-biomed.de  Wed Sep 14 22:08:44 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 14 Sep 2005 20:08:44 +0000 (UTC)
Subject: [R]
	=?utf-8?q?Can_I_use_=22lme=22_to_deal_with_grouping_data_when?=
	=?utf-8?q?_I_only_get_one=09data_point_per_group=3F?=
References: <a175c71d05091411437ec9c3e0@mail.gmail.com>
Message-ID: <loom.20050914T220611-93@post.gmane.org>

Ruixiao Lu <tinypenguin <at> gmail.com> writes:

> I have a question for using "lme". Say, I have 6 data points and they belong 
> to six groups (one group factor). So there is no replicates for each group 
> and I cannot separate the with-in group variation from the between group 
> variation. 
> But when I try to use "lme" to deal with it, it gave the answers for both 
> with-in group variation and the between group variation! The statement is as 
> below:

((Slightly modified by DM))

fac=as.factor(c(1:6))
y=c(-0.3465181, -0.2019839, -0.7610653, -0.1992943, -0.1663348, 0.2811794)
data.y=data.frame(y,fac)
y.g=groupedData(y~1|fac)
intervals(y.g) ## and you will see ....

Similar example on page 27 of Pinheiro/Bates

Dieter Menne



From bwheeler at echip.com  Wed Sep 14 22:28:22 2005
From: bwheeler at echip.com (Bob Wheeler)
Date: Wed, 14 Sep 2005 16:28:22 -0400
Subject: [R] Converting coordinates to actual distances
In-Reply-To: <845AADAC1106E44996327D62097E4C6B6F5369@et.ad.sdsc.edu>
References: <845AADAC1106E44996327D62097E4C6B6F5369@et.ad.sdsc.edu>
Message-ID: <432887E6.2070600@echip.com>

Paul Brewin wrote:
> Hello,
> 
> I've been searching for a method of converting Lat/Lon decimal
> coordinates into actual distances between points, and taking into
> account the curvature of the earth.  Is there such a package in R?  I've
> looked at the GeoR package, but this does not seem to contain what I am
> looking for.  Ideally the output would be a triangular matrix of
> distances.  
> 
> Thanks in advance, 
> Paul Brewin
> 
> 
> 
> Paul E Brewin (PhD)
> Center for Research in Biological Systems
> University of California San Diego
> 9500 Gilman Drive MC 0505
> La Jolla CA, 92093-0505
> USA
>  
> Ph: 858-822-0871
> Fax: 858-822-3631
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

See http://www.meridianworlddata.com/Distance-Calculation.asp. The 
calculations are trivial.


-- 
Bob Wheeler --- http://www.bobwheeler.com/
         ECHIP, Inc. ---
Randomness comes in bunches.



From ggrothendieck at gmail.com  Wed Sep 14 22:37:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Sep 2005 16:37:15 -0400
Subject: [R] Converting coordinates to actual distances
In-Reply-To: <845AADAC1106E44996327D62097E4C6B6F5369@et.ad.sdsc.edu>
References: <845AADAC1106E44996327D62097E4C6B6F5369@et.ad.sdsc.edu>
Message-ID: <971536df05091413373f51e845@mail.gmail.com>

See

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/29117.html

On 9/14/05, Paul Brewin <pebrewin at sdsc.edu> wrote:
> Hello,
> 
> I've been searching for a method of converting Lat/Lon decimal
> coordinates into actual distances between points, and taking into
> account the curvature of the earth.  Is there such a package in R?  I've
> looked at the GeoR package, but this does not seem to contain what I am
> looking for.  Ideally the output would be a triangular matrix of
> distances.
> 
> Thanks in advance,
> Paul Brewin
> 
> 
> 
> Paul E Brewin (PhD)
> Center for Research in Biological Systems
> University of California San Diego
> 9500 Gilman Drive MC 0505
> La Jolla CA, 92093-0505
> USA
> 
> Ph: 858-822-0871
> Fax: 858-822-3631
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Wed Sep 14 23:05:38 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 14 Sep 2005 23:05:38 +0200 (CEST)
Subject: [R] Converting coordinates to actual distances
In-Reply-To: <845AADAC1106E44996327D62097E4C6B6F5369@et.ad.sdsc.edu>
Message-ID: <Pine.LNX.4.44.0509142254470.2982-100000@reclus.nhh.no>

On Wed, 14 Sep 2005, Paul Brewin wrote:

> Hello,
> 
> I've been searching for a method of converting Lat/Lon decimal
> coordinates into actual distances between points, and taking into
> account the curvature of the earth.  Is there such a package in R?  I've
> looked at the GeoR package, but this does not seem to contain what I am
> looking for.  Ideally the output would be a triangular matrix of
> distances.  
> 

Using C code in the sp package (which will be exposed at the R level 
shortly) and sp > 0.8 and maptools > 0.5:

> library(maptools)
Loading required package: foreign
Loading required package: sp
> xx <- readShapePoly(system.file("shapes/sids.shp", package="maptools")[1])
> ll <- getSpPPolygonsLabptSlots(xx)
# ll is a matrix of long-lat centroids of North Carolina county polygons
> str(ll)
 num [1:100, 1:2] -81.5 -81.1 -79.3 -79.8 -78.7 ...
> plot(ll)
> x <- as.double(ll[,1])
> y <- as.double(ll[,2])
> n <- as.integer(length(x))
> dists <- vector(mode="double", length=n)
> lonlat <- as.integer(1)
> res <- matrix(as.double(NA), 100, 100)
> for (i in 1:100) res[i,] <- .C("sp_dists", x, y, x[i], y[i], n, dists, 
+ lonlat)[[6]]

gives a full matrix measured in kilometers for the WGS-84 ellipsoid. 
Accessing the C function like this puts the responsibility for checking 
the argument modes on the user.

If this seems scary, rdist.earth() in the fields package has an R version 
of this. But maybe you need the actual functions to use great circle 
distance instead of Euclidean, rather than just to generate a distance 
matrix?

Hope this helps,

Roger Bivand

> Thanks in advance, 
> Paul Brewin
> 
> 
> 
> Paul E Brewin (PhD)
> Center for Research in Biological Systems
> University of California San Diego
> 9500 Gilman Drive MC 0505
> La Jolla CA, 92093-0505
> USA
>  
> Ph: 858-822-0871
> Fax: 858-822-3631
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From d.scott at auckland.ac.nz  Thu Sep 15 00:06:53 2005
From: d.scott at auckland.ac.nz (David Scott)
Date: Thu, 15 Sep 2005 10:06:53 +1200 (NZST)
Subject: [R] Graphical presentation of logistic regression
In-Reply-To: <E71A2CDB5DFFD14383BAC2E0612811BA4B6D22@ADARA.RSPB.ORG.UK>
References: <E71A2CDB5DFFD14383BAC2E0612811BA4B6D22@ADARA.RSPB.ORG.UK>
Message-ID: <Pine.LNX.4.60.0509150954120.18590@stat71.stat.auckland.ac.nz>

On Wed, 14 Sep 2005, Beale, Colin wrote:

> Hi,
>
> I wonder if anyone has written any code to implement the suggestions of
> Smart et al (2004) in the Bulletin of the Ecological Society of America
> for a new way of graphically presenting the results of logistic
> regression (see
> www.esapubs.org/bulletin/backissues/085-3/bulletinjuly2004_2column.htm#t
> ools1 for the full text)? I couldn't find anything relating to this sort
> of graphical representation of logistic models in the archives, but
> maybe someone has solved it already? In short, Smart et al suggest that
> a logistic regression be presented as a combination of the two
> histograms for successes and failures (with one presented upside down at
> the top of the figure, the other the right way up at the bottom)
> overlaid by the probability function (ie logistic curve). It's somewhat
> hard to describe, but is nicely illustrated in the full text version
> above. I think it is a sensible way of presenting these results and am
> keen to do so - at the moment I can only do this by generating the two
> histograms and the logistic curve separately (using hist() and lines()),
> then copying and pasting the graphs out of R and inverting one in a
> graphics package, before overlying the others. I'm sure this could be
> done within R and would be a handy plotting function to develop. Has
> anyone done so, or can anyone give me any pointers to doing this? I
> really nead to know how to invert a histogram and how to overlay this
> with another histogram "the right way up".
>
I think if you take a peek at hist.default you will find it is pretty 
straightforward. All that happens in hist.default is there is a lot of 
stuff about choosing the breaks for the bins, then some C code is called 
to get the counts, then the information is assembled and plot is called 
where the object plotted is of class histogram.

If you then look at plot.histogram (getAnywhere(plot.histogram)) you find 
all it really does is plot some rectangles. Just change the plotting bit.

If you want an example of how it might be done, you can look at log.hist 
in my package HyperbDist (or a more recent version logHist.R on my 
homepage at http://www.stat.auckland.ac.nz/~dscott/)

David Scott


_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From ericpante at hotmail.com  Thu Sep 15 00:08:52 2005
From: ericpante at hotmail.com (Eric Pante)
Date: Wed, 14 Sep 2005 18:08:52 -0400
Subject: [R] bootstrapping for clustering
Message-ID: <BAY106-DAV21127B15BD10856513EB37BC9F0@phx.gbl>

Dear R-listers,

Is anyone familiar with a package that would perform bootstrapping on 
species/site matrices for clustering ? So far I have been using the 
vegan package to generate trees (Bray-Curtis index), but I would like 
to associate a certainty to each node (similarly to a phylogenetic 
tree).
If no package exist, I would know how to generate bootstrapped matrices 
of distance, but how could I plot the results ? Graphically, I am 
looking for something similar to what is available from the pvclust 
package (my understanding is that pvclust makes covariance clustering 
from dataframes, and it cannot be used with matrices to generate 
Bray-Curtis similarity dendrograms?)

Please forgive my ignorance!

Thank you in advance,
eric pante



From lefsky at gmail.com  Wed Sep 14 23:06:17 2005
From: lefsky at gmail.com (Michael Lefsky)
Date: Wed, 14 Sep 2005 15:06:17 -0600
Subject: [R] Scan and Lists
Message-ID: <10e410e5050914140630da87d@mail.gmail.com>

This may be a newbie question - although I did search for this error
message in the archives and via google and didn't see this error:

The help page for "scan" indicates that among the types of data
capable of being read are:

> "The supported types are 'logical', 'integer', 'numeric', 'complex', 'character', 'raw' and 'list': 
> 'list' values should have elements which are one of the first six types listed or 'NULL'.

I have tried to use a list within a "what" list : 

f <- scan(file="c:/test/testout.csv",what=list(hi=0.0,bye="",wave=list(1:1000)),sep=",",skip=1)

and the following error is returned: 

 "c:/test/testout.csv", what = list(hi = 0, bye = "",  : 
        unimplemented type 'list' in 'extractItem'

So, is my syntax confusing R, or is the documentation wrong, or is it
some other, third, option?

Thanks

M
-- 
Michael Lefsky
College of Natural Resources
Colorado State University
---------------------------------------------------------------------
Out of the crooked timber of humanity, 
no straight thing was ever made- Immanuel Kant



From lefsky at gmail.com  Wed Sep 14 23:25:24 2005
From: lefsky at gmail.com (Michael Lefsky)
Date: Wed, 14 Sep 2005 15:25:24 -0600
Subject: [R] Importing IDL Structures
Message-ID: <10e410e50509141425b12716c@mail.gmail.com>

I am trying to get started with R, but before I do, I need to find a
way to import my existing datasets, which are currently stored as
arrays of IDL structures (RSI's IDL, not the other one).

The problem I has is this: the IDL structures contain scalar items, as
well as n-dimensional arrays. I can export the data in a number of
ways, including as separate files for scalars and for each of the
arrays.

Has anyone tackled this problem? If not, can you advise me on the best
data structure(s) to hold such data in R? Data frames seemed to be the
most obvious choice, but I prefer the syntax used for lists (it is
more similar to IDL), if that is possible. And of course (as I said in
a previous message) there is the question of how to import the data
into the structure (it looks like I will need to export each array
separately , import them into R and then assemble the final
structure).

Any assistance will be appreciated. 

M

-- 
Michael Lefsky
College of Natural Resources
Colorado State University
---------------------------------------------------------------------
Out of the crooked timber of humanity, 
no straight thing was ever made- Immanuel Kant



From gerifalte28 at hotmail.com  Thu Sep 15 01:45:08 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 14 Sep 2005 23:45:08 +0000
Subject: [R] Scan and Lists
In-Reply-To: <10e410e5050914140630da87d@mail.gmail.com>
Message-ID: <BAY103-F17F18E908B8773C99E58DAA69F0@phx.gbl>

Hi Michael

An example of your list would have helped.  Anyhow, why do you want to read 
a list? If you created a list object in R and want to save it and then read 
it back in other session or in some other time a good option is to write an 
ASCII representation of the object using dput and then recreate it using 
dget i.e.

mylist= list(x=cars[,1], y=cars[,2])
dput(mylist,"mylist")
mylistback=dget("mylist")
$x
[1]  4  4  7  7  8  9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 
15
[26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 
25

$y
[1]   2  10   4  22  16  10  18  26  34  17  28  14  20  24  28  26  34  34  
46
[20]  26  36  60  80  20  26  54  32  40  32  40  50  42  56  76  84  36  46 
  68
[39]  32  48  52  56  64  66  54  70  92  93 120  85

If you want to read some other type of data take a look at the higher lever 
functions listed under ?read.table and the functions at 
library(help="foreign")

I hope this helps

Francisco

PS: Nasty weather in Fort Collins today!

>From: Michael Lefsky <lefsky at gmail.com>
>Reply-To: lefsky at gmail.com
>To: r-help at stat.math.ethz.ch
>Subject: [R] Scan and Lists
>Date: Wed, 14 Sep 2005 15:06:17 -0600
>
>This may be a newbie question - although I did search for this error
>message in the archives and via google and didn't see this error:
>
>The help page for "scan" indicates that among the types of data
>capable of being read are:
>
> > "The supported types are 'logical', 'integer', 'numeric', 'complex', 
>'character', 'raw' and 'list':
> > 'list' values should have elements which are one of the first six types 
>listed or 'NULL'.
>
>I have tried to use a list within a "what" list :
>
>f <- 
>scan(file="c:/test/testout.csv",what=list(hi=0.0,bye="",wave=list(1:1000)),sep=",",skip=1)
>
>and the following error is returned:
>
>  "c:/test/testout.csv", what = list(hi = 0, bye = "",  :
>         unimplemented type 'list' in 'extractItem'
>
>So, is my syntax confusing R, or is the documentation wrong, or is it
>some other, third, option?
>
>Thanks
>
>M
>--
>Michael Lefsky
>College of Natural Resources
>Colorado State University
>---------------------------------------------------------------------
>Out of the crooked timber of humanity,
>no straight thing was ever made- Immanuel Kant
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From dushoff at eno.princeton.edu  Thu Sep 15 04:42:38 2005
From: dushoff at eno.princeton.edu (Jonathan Dushoff)
Date: Wed, 14 Sep 2005 22:42:38 -0400 (EDT)
Subject: [R] Log scale in histograms
Message-ID: <Pine.LNX.4.61.0509142237080.2700@tahawus.Princeton.EDU>

Can't find any information about this, but others must want to do it.

In the example below, the second plot has the desired log scale, but the
first does not.

Any help appreciated.

JD

----------------------------------------------------------------------


data(state)
area_Mh = 259*state.area/1000000

histlogarea = hist(log(area_Mh), 13, xlab="Area (Mh)", main="")
histlogarea$mids = exp(histlogarea$mids)
histlogarea$breaks = exp(histlogarea$breaks)

plot(histlogarea, log="x")

plot(histlogarea$mids, histlogarea$density, log="x")



From p.dalgaard at biostat.ku.dk  Thu Sep 15 08:49:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Sep 2005 08:49:27 +0200
Subject: [R] Importing IDL Structures
In-Reply-To: <10e410e50509141425b12716c@mail.gmail.com>
References: <10e410e50509141425b12716c@mail.gmail.com>
Message-ID: <x2br2uakqw.fsf@turmalin.kubism.ku.dk>

Michael Lefsky <lefsky at gmail.com> writes:

> I am trying to get started with R, but before I do, I need to find a
> way to import my existing datasets, which are currently stored as
> arrays of IDL structures (RSI's IDL, not the other one).
> 
> The problem I has is this: the IDL structures contain scalar items, as
> well as n-dimensional arrays. I can export the data in a number of
> ways, including as separate files for scalars and for each of the
> arrays.
> 
> Has anyone tackled this problem? If not, can you advise me on the best
> data structure(s) to hold such data in R? Data frames seemed to be the
> most obvious choice, but I prefer the syntax used for lists (it is
> more similar to IDL), if that is possible. And of course (as I said in
> a previous message) there is the question of how to import the data
> into the structure (it looks like I will need to export each array
> separately , import them into R and then assemble the final
> structure).

I don't know IDL, but it seems to do HDF. Would the hdf5 package help,
I wonder?

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From jarioksa at sun3.oulu.fi  Thu Sep 15 09:17:06 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Thu, 15 Sep 2005 10:17:06 +0300
Subject: [R] Graphical presentation of logistic regression
In-Reply-To: <43280987.1080403@vanderbilt.edu>
References: <E71A2CDB5DFFD14383BAC2E0612811BA4B6D22@ADARA.RSPB.ORG.UK>
	<43280987.1080403@vanderbilt.edu>
Message-ID: <1126768626.17535.14.camel@biol102145.oulu.fi>

On Wed, 2005-09-14 at 06:29 -0500, Frank E Harrell Jr wrote:
> Beale, Colin wrote:
> > Hi,
> > 
> > I wonder if anyone has written any code to implement the suggestions of
> > Smart et al (2004) in the Bulletin of the Ecological Society of America
> > for a new way of graphically presenting the results of logistic
> > regression (see
> > www.esapubs.org/bulletin/backissues/085-3/bulletinjuly2004_2column.htm#t
> > ools1 for the full text)? I couldn't find anything relating to this sort
> > of graphical representation of logistic models in the archives, but
> > maybe someone has solved it already? In short, Smart et al suggest that
> > a logistic regression be presented as a combination of the two
> > histograms for successes and failures (with one presented upside down at
> > the top of the figure, the other the right way up at the bottom)
> > overlaid by the probability function (ie logistic curve). It's somewhat
> > hard to describe, but is nicely illustrated in the full text version
> > above. I think it is a sensible way of presenting these results and am
> > keen to do so - at the moment I can only do this by generating the two
> > histograms and the logistic curve separately (using hist() and lines()),
> > then copying and pasting the graphs out of R and inverting one in a
> > graphics package, before overlying the others. I'm sure this could be
> > done within R and would be a handy plotting function to develop. Has
> > anyone done so, or can anyone give me any pointers to doing this? I
> > really nead to know how to invert a histogram and how to overlay this
> > with another histogram "the right way up".
> > 
> > Any thoughts would be welcome.
> > 
> > Thanks in advance,
> > Colin
> 
>  From what you describe, that is a poor way to represent the model 
> except for judging discrimination ability (if the model is calibrated 
> well).  Effect plots, odds ratio charts, and nomograms are better.  See 
> the Design package for details.
> 

You're correct when you say that this is a poor way to represent the
model. However, you should have some understanding to us ecologists who
are simple creatures working with tangible subjects such as animals and
plants (microbiologists work with less tangible things). Therefore we
want to have a concrete and simple representation. After all, the
example was about occurrence of an animal against a concrete
environmental variable, and a concrete representation was suggested.
Nomograms and things are abstractions that you understand first after
long education and training (I tried the Design package and I didn't
understand the nomogram plot). 

I tried with one concrete example with my own data, and the inverted
histogram method was patently misleading (with Baz Rowlingson's neat and
compact code, sorry for the repetition). The method would be useful with
dense and regular data only, but now the clearest visual cue was the
uneven sampling intensity. With my limited knowledge on R facilities, I
can now remember only two ways two preserve the concreteness of display
in the base R: jitter() to avoid overplotting of observations, and
sunflowerplot() to show the amount of overplotting.

I think Ecological Society of America would be happy to receive papers
to suggest better ways to represent binary response data, if some of the
knowledgeable persons in this groups would decided to educate them (I'm
not an ESA member, so I wouldn't be educated: therefore 'them' instead
of 'us'). The ESA bulletin will be influential in manuscript submitted
to the Society journals in the future, and the time for action is now.

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
Ph. +358 8 5531526, cell +358 40 5136529, fax +358 8 5531061
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/



From maechler at stat.math.ethz.ch  Thu Sep 15 09:35:25 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 15 Sep 2005 09:35:25 +0200
Subject: [R] Long lines with Sweave
In-Reply-To: <20050914134620.GB16826@jtkpc.cmp.uea.ac.uk>
References: <dg8ma8$e3h$1@sea.gmane.org>
	<20050914091301.GA14109@jtkpc.cmp.uea.ac.uk>
	<dg96dl$uiu$1@sea.gmane.org>
	<20050914134620.GB16826@jtkpc.cmp.uea.ac.uk>
Message-ID: <17193.9277.127838.915033@stat.math.ethz.ch>

>>>>> "Jan" == Jan T Kim <jtk at cmp.uea.ac.uk>
>>>>>     on Wed, 14 Sep 2005 14:46:20 +0100 writes:

    Jan> On Wed, Sep 14, 2005 at 02:49:56PM +0200, Henrik Andersson wrote:
    >> Jan T. Kim wrote:
    >> > On Wed, Sep 14, 2005 at 10:14:59AM +0200, Henrik Andersson wrote:
    >> > 
    >> >>I have used Sweave a lot the latest year, but never really used any long 
    >> >>function calls.
    >> >>
    >> >>
    >> >>If I have code which look like this
    >> >>
    >> >>-------------------------------------------------------------
    >> >>gof <- benthic.flux(ID="Gulf of Finland",
    >> >>                     meas.conc=conc,
    >> >>                     bw.conc=bw.conc,
    >> >>                     time=times,
    >> >>                     substance=expression(DIC~(mmol~m^{-3}))
    >> >>                     )
    >> >>-------------------------------------------------------------
    >> >>
    >> >>I get the output by Sweave in my pdf file, like this:
    >> >>
    >> >>---------------------------------------------------------------
    >> >> > gof <- benthic.flux(ID = "Gulf of Finland", meas.conc = conc,
    >> >>+ bw.conc = bw.conc, time = times, substance = expression(DIC ~
    >> >>+ (mmol ~ m^{
    >> >>+ -3
    >> >>+ })))
    >> >>----------------------------------------------------------------
    >> >>
    >> >>I can understand that it will not look exactly as entered but why is the 
    >> >>'-3' on a line of it's own?
    >> >>
    >> >>Can anyone suggest a idea to how I can make this more readable.
    >> > 
    >> > 
    >> > It seems you've been thinking LaTeX rather than R ;-)  :
    >> > The exponent "-3" in the expression should be enclosed by parentheses
    >> > rather than by curly braces.
    >> > 
    >> > The code formatting done by the print method inserts the newline after
    >> > "{" and before "}".
    >> > 
    >> > Best regards, Jan
    >> 
    >> If you look at demo(plotmath), I get the impression that m^(-3) does not 
    >> give me the desired behavior.
    >> 
    >> I want to have -3 in superscript without visible parentheses.
    >> 
    >> Tricky!

    Jan> Ok, I see.

    Jan> It seems to me that you could omit the curly braces in the example, I
    Jan> don't see any differences between the title in the plots produced by

    Jan> plot(1:10, main = expression(DIC~(mmol~m^-3)))

    Jan> and

    Jan> plot(1:10, main = expression(DIC~(mmol~m^{-3})))

    Jan> For more complex exponents, you could try plain() to prevent them from
    Jan> being wrongly grouped by operator precedence, as in

    Jan> plot(1:10, main = expression(DIC~(mmol~m^plain(-3 + t))))

neat idea, but

    Jan> Not exactly ideal for readability, however...

indeed.  And really only a workaround:

You shouldn't have to uglify your R code in order to work around
Sweave "pecularities".

Martin



From maechler at stat.math.ethz.ch  Thu Sep 15 10:19:05 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 15 Sep 2005 10:19:05 +0200
Subject: [R] Scan and Lists
In-Reply-To: <10e410e5050914140630da87d@mail.gmail.com>
References: <10e410e5050914140630da87d@mail.gmail.com>
Message-ID: <17193.11897.791448.545228@stat.math.ethz.ch>

>>>>> "Michael" == Michael Lefsky <lefsky at gmail.com>
>>>>>     on Wed, 14 Sep 2005 15:06:17 -0600 writes:

    Michael> This may be a newbie question - although I did
    Michael> search for this error message in the archives and
    Michael> via google and didn't see this error:

I know how useful google can be - - still, sometimes one would
better spend the time differently ... You know the old IBM
motto? If not, google for "IBM motto"  ;-)

    Michael> The help page for "scan" indicates that among the types of data
    Michael> capable of being read are:

    >> "The supported types are 'logical', 'integer', 'numeric',
    >> 'complex', 'character', 'raw' and 'list': 'list' values
    >> should have elements which are one of the first six types
    >> listed or 'NULL'.

    Michael> I have tried to use a list within a "what" list : 

which was wrong:

    Michael> f <- scan(file="c:/test/testout.csv",
    Michael>           what=list(hi=0.0,bye="",wave=list(1:1000)),
    Michael>           sep=",",skip=1)
    Michael> and the following error is returned: 

    Michael> "c:/test/testout.csv", what = list(hi = 0, bye = "",  : 
    Michael> unimplemented type 'list' in 'extractItem'

    Michael> So, is my syntax confusing R, or is the documentation wrong, or is it
    Michael> some other, third, option?

3rd: You didn't read the documentation carefully enough
    (though I agree that the current wording leaves a non-small
     possibility for confusion):

In your above citation from the help page, you've left off
crucial context. Here is a bit more

 >> what: the type of 'what' gives the type of data to be read. If
 >>        'what' is a list, it is assumed that the lines of the data
 >>        file are records each containing 'length(what)' items
 >>        ("fields"). The supported types are 'logical', 'integer',
 >>        'numeric', 'complex', 'character', 'raw' and 'list': 'list'
 >>        values should have elements which are one of the first six
 >>        types listed or 'NULL'.

So 'what' has a *type* and that type can be logical, ...., and list.
where list should have elements from the first six types ---
which do *NOT* include list.

In short: It does exclude using a *list* inside the list.

Is your data
     <double> <character> <int> <int> .... <int>
with 1000 integers, i.e. you have 1002 columns?

If yes, you'd probably get what you want by

    whatCols <- c(list(hi=0.0, bye=""), as.list(1:1000))
    f <- scan(file = "c:/test/testout.csv",
              what = whatCols, sep= ",", skip=1)

{The point here is that c(l1, l2) is used to concatenate two
 lists l1 and l2;
 and yes: Please do use spaces {and indentation} to make your
	  more readable !
}

    Michael> Thanks

You're welcome,
Martin Maechler, ETH Zurich



From petr.pikal at precheza.cz  Thu Sep 15 10:22:33 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Sep 2005 10:22:33 +0200
Subject: [R] if() command
Message-ID: <43294B69.25787.429104@localhost>



From Allan at STATS.uct.ac.za  Thu Sep 15 10:22:38 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Thu, 15 Sep 2005 10:22:38 +0200
Subject: [R] R: deleting rows
Message-ID: <43292F4E.EE5081AB@STATS.uct.ac.za>

hi all

hopefully some one can help.


assume that i imported the following data into R (say the data frame is
called a)

x1	x2	x3
1	NA	3
1	2	NA
1	2	3
3	NA	6
4	5	9
7	5	6
7	8	9
NA	7	9


How can i construct a new data frame that only contains those rows that
does not contain the NA's? is these a quick way?

ie

x1	x2	x3
1	2	3
4	5	9
7	5	6
7	8	9


in this example we can simple use a[c(-1,-2,-4,-8),] but happens if we
have a larger dataframe?

we need to construct some kind of row indicator telling R which rows
contains NA'S.

is there an easier method?

/
allan

From dimitris.rizopoulos at med.kuleuven.be  Thu Sep 15 10:34:07 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 15 Sep 2005 10:34:07 +0200
Subject: [R] R: deleting rows
References: <43292F4E.EE5081AB@STATS.uct.ac.za>
Message-ID: <008001c5b9d0$3cac2d20$0540210a@www.domain>

look at function ?complete.cases(), e.g.,

a[complete.cases(a), ]

will do the work in your case.


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Clark Allan" <Allan at stats.uct.ac.za>
To: <r-help at stat.math.ethz.ch>
Cc: "Birgit Erni" <berni at stats.uct.ac.za>
Sent: Thursday, September 15, 2005 10:22 AM
Subject: [R] R: deleting rows


> hi all
>
> hopefully some one can help.
>
>
> assume that i imported the following data into R (say the data frame 
> is
> called a)
>
> x1 x2 x3
> 1 NA 3
> 1 2 NA
> 1 2 3
> 3 NA 6
> 4 5 9
> 7 5 6
> 7 8 9
> NA 7 9
>
>
> How can i construct a new data frame that only contains those rows 
> that
> does not contain the NA's? is these a quick way?
>
> ie
>
> x1 x2 x3
> 1 2 3
> 4 5 9
> 7 5 6
> 7 8 9
>
>
> in this example we can simple use a[c(-1,-2,-4,-8),] but happens if 
> we
> have a larger dataframe?
>
> we need to construct some kind of row indicator telling R which rows
> contains NA'S.
>
> is there an easier method?
>
> /
> allan


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From ligges at statistik.uni-dortmund.de  Thu Sep 15 10:34:30 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Sep 2005 10:34:30 +0200
Subject: [R] R: deleting rows
In-Reply-To: <43292F4E.EE5081AB@STATS.uct.ac.za>
References: <43292F4E.EE5081AB@STATS.uct.ac.za>
Message-ID: <43293216.4000205@statistik.uni-dortmund.de>

Clark Allan wrote:

> hi all
> 
> hopefully some one can help.
> 
> 
> assume that i imported the following data into R (say the data frame is
> called a)
> 
> x1	x2	x3
> 1	NA	3
> 1	2	NA
> 1	2	3
> 3	NA	6
> 4	5	9
> 7	5	6
> 7	8	9
> NA	7	9
> 
> 
> How can i construct a new data frame that only contains those rows that
> does not contain the NA's? is these a quick way?
> 
> ie
> 
> x1	x2	x3
> 1	2	3
> 4	5	9
> 7	5	6
> 7	8	9
> 
> 
> in this example we can simple use a[c(-1,-2,-4,-8),] but happens if we
> have a larger dataframe?
> 
> we need to construct some kind of row indicator telling R which rows
> contains NA'S.
> 
> is there an easier method?


na.omit(a)

Uwe Ligges


> /
> allan
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Friedrich.Leisch at tuwien.ac.at  Thu Sep 15 10:36:43 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Thu, 15 Sep 2005 10:36:43 +0200
Subject: [R] Long lines with Sweave
In-Reply-To: <17193.9277.127838.915033@stat.math.ethz.ch>
References: <dg8ma8$e3h$1@sea.gmane.org>
	<20050914091301.GA14109@jtkpc.cmp.uea.ac.uk>
	<dg96dl$uiu$1@sea.gmane.org>
	<20050914134620.GB16826@jtkpc.cmp.uea.ac.uk>
	<17193.9277.127838.915033@stat.math.ethz.ch>
Message-ID: <17193.12955.331882.122374@galadriel.ci.tuwien.ac.at>

>>>>> On Thu, 15 Sep 2005 09:35:25 +0200,
>>>>> Martin Maechler (MM) wrote:

>>>>> "Jan" == Jan T Kim <jtk at cmp.uea.ac.uk>
>>>>>     on Wed, 14 Sep 2005 14:46:20 +0100 writes:

  Jan> On Wed, Sep 14, 2005 at 02:49:56PM +0200, Henrik Andersson wrote:
  >>> Jan T. Kim wrote:
  >>> > On Wed, Sep 14, 2005 at 10:14:59AM +0200, Henrik Andersson wrote:
  >>> > 
  >>> >>I have used Sweave a lot the latest year, but never really used any long 
  >>> >>function calls.
  >>> >>
  >>> >>
  >>> >>If I have code which look like this
  >>> >>
  >>> >>-------------------------------------------------------------
  >>> >>gof <- benthic.flux(ID="Gulf of Finland",
  >>> >>                     meas.conc=conc,
  >>> >>                     bw.conc=bw.conc,
  >>> >>                     time=times,
  >>> >>                     substance=expression(DIC~(mmol~m^{-3}))
  >>> >>                     )
  >>> >>-------------------------------------------------------------
  >>> >>
  >>> >>I get the output by Sweave in my pdf file, like this:
  >>> >>
  >>> >>---------------------------------------------------------------
  >>> >> > gof <- benthic.flux(ID = "Gulf of Finland", meas.conc = conc,
  >>> >>+ bw.conc = bw.conc, time = times, substance = expression(DIC ~
  >>> >>+ (mmol ~ m^{
  >>> >>+ -3
  >>> >>+ })))
  >>> >>----------------------------------------------------------------
  >>> >>
  >>> >>I can understand that it will not look exactly as entered but why is the 
  >>> >>'-3' on a line of it's own?
  >>> >>
  >>> >>Can anyone suggest a idea to how I can make this more readable.
  >>> > 
  >>> > 
  >>> > It seems you've been thinking LaTeX rather than R ;-)  :
  >>> > The exponent "-3" in the expression should be enclosed by parentheses
  >>> > rather than by curly braces.
  >>> > 
  >>> > The code formatting done by the print method inserts the newline after
  >>> > "{" and before "}".
  >>> > 
  >>> > Best regards, Jan
  >>> 
  >>> If you look at demo(plotmath), I get the impression that m^(-3) does not 
  >>> give me the desired behavior.
  >>> 
  >>> I want to have -3 in superscript without visible parentheses.
  >>> 
  >>> Tricky!

  Jan> Ok, I see.

  Jan> It seems to me that you could omit the curly braces in the example, I
  Jan> don't see any differences between the title in the plots produced by

  Jan> plot(1:10, main = expression(DIC~(mmol~m^-3)))

  Jan> and

  Jan> plot(1:10, main = expression(DIC~(mmol~m^{-3})))

  Jan> For more complex exponents, you could try plain() to prevent them from
  Jan> being wrongly grouped by operator precedence, as in

  Jan> plot(1:10, main = expression(DIC~(mmol~m^plain(-3 + t))))

  > neat idea, but

  Jan> Not exactly ideal for readability, however...

  > indeed.  And really only a workaround:

  > You shouldn't have to uglify your R code in order to work around
  > Sweave "pecularities".


Hmm, it's not really an "Sweave peculiarity", but one of the R
parser. After saving Henrik's code in file test.R I get

R> x  = parse("test.R")
R> x
expression(gof <- benthic.flux(ID = "Gulf of Finland", meas.conc = conc, 
    bw.conc = bw.conc, time = times, substance = expression(DIC ~ 
        (mmol ~ m^{
            -3
        }))))
R> deparse(x)
[1] "expression(gof <- benthic.flux(ID = \"Gulf of Finland\", meas.conc = conc, "
[2] "    bw.conc = bw.conc, time = times, substance = expression(DIC ~ "         
[3] "        (mmol ~ m^{"                                                        
[4] "            -3"                                                             
[5] "        }))))"

and the latter is used by Sweave. The code chunks need to be parsed,
because otherwise there is no way how to know where to insert
output. A source(..., echo=TRUE) will suffer from the same problem.

Best,
Fritz

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From maechler at stat.math.ethz.ch  Thu Sep 15 10:48:25 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 15 Sep 2005 10:48:25 +0200
Subject: [R] Long lines with Sweave
In-Reply-To: <17193.12955.331882.122374@galadriel.ci.tuwien.ac.at>
References: <dg8ma8$e3h$1@sea.gmane.org>
	<20050914091301.GA14109@jtkpc.cmp.uea.ac.uk>
	<dg96dl$uiu$1@sea.gmane.org>
	<20050914134620.GB16826@jtkpc.cmp.uea.ac.uk>
	<17193.9277.127838.915033@stat.math.ethz.ch>
	<17193.12955.331882.122374@galadriel.ci.tuwien.ac.at>
Message-ID: <17193.13657.951894.214377@stat.math.ethz.ch>

>>>>> "Fritz" == Friedrich Leisch <Friedrich.Leisch at tuwien.ac.at>
>>>>>     on Thu, 15 Sep 2005 10:36:43 +0200 writes:

>>>>> On Thu, 15 Sep 2005 09:35:25 +0200,
>>>>> Martin Maechler (MM) wrote:

>>>>> "Jan" == Jan T Kim <jtk at cmp.uea.ac.uk>
>>>>>     on Wed, 14 Sep 2005 14:46:20 +0100 writes:

    Jan> On Wed, Sep 14, 2005 at 02:49:56PM +0200, Henrik Andersson wrote:
    >>>> Jan T. Kim wrote:
    >>>> > On Wed, Sep 14, 2005 at 10:14:59AM +0200, Henrik Andersson wrote:
    >>>> > 
    >>>> >>I have used Sweave a lot the latest year, but never really used any long 
    >>>> >>function calls.
    >>>> >>
    >>>> >>
    >>>> >>If I have code which look like this
    >>>> >>
    >>>> >>-------------------------------------------------------------
    >>>> >>gof <- benthic.flux(ID="Gulf of Finland",
    >>>> >>                     meas.conc=conc,
    >>>> >>                     bw.conc=bw.conc,
    >>>> >>                     time=times,
    >>>> >>                     substance=expression(DIC~(mmol~m^{-3}))
    >>>> >>                     )
    >>>> >>-------------------------------------------------------------
    >>>> >>
    >>>> >>I get the output by Sweave in my pdf file, like this:
    >>>> >>
    >>>> >>---------------------------------------------------------------
    >>>> >> > gof <- benthic.flux(ID = "Gulf of Finland", meas.conc = conc,
    >>>> >>+ bw.conc = bw.conc, time = times, substance = expression(DIC ~
    >>>> >>+ (mmol ~ m^{
    >>>> >>+ -3
    >>>> >>+ })))
    >>>> >>----------------------------------------------------------------
    >>>> >>
    >>>> >>I can understand that it will not look exactly as entered but why is the 
    >>>> >>'-3' on a line of it's own?
    >>>> >>
    >>>> >>Can anyone suggest a idea to how I can make this more readable.
    >>>> > 
    >>>> > 
    >>>> > It seems you've been thinking LaTeX rather than R ;-)  :
    >>>> > The exponent "-3" in the expression should be enclosed by parentheses
    >>>> > rather than by curly braces.
    >>>> > 
    >>>> > The code formatting done by the print method inserts the newline after
    >>>> > "{" and before "}".
    >>>> > 
    >>>> > Best regards, Jan
    >>>> 
    >>>> If you look at demo(plotmath), I get the impression that m^(-3) does not 
    >>>> give me the desired behavior.
    >>>> 
    >>>> I want to have -3 in superscript without visible parentheses.
    >>>> 
    >>>> Tricky!

    Jan> Ok, I see.

    Jan> It seems to me that you could omit the curly braces in the example, I
    Jan> don't see any differences between the title in the plots produced by

    Jan> plot(1:10, main = expression(DIC~(mmol~m^-3)))

    Jan> and

    Jan> plot(1:10, main = expression(DIC~(mmol~m^{-3})))

    Jan> For more complex exponents, you could try plain() to prevent them from
    Jan> being wrongly grouped by operator precedence, as in

    Jan> plot(1:10, main = expression(DIC~(mmol~m^plain(-3 + t))))

    >> neat idea, but

    Jan> Not exactly ideal for readability, however...

    >> indeed.  And really only a workaround:

    >> You shouldn't have to uglify your R code in order to work around
    >> Sweave "pecularities".


    Fritz> Hmm, it's not really an "Sweave peculiarity", but one of the R
    Fritz> parser. 

Indeed, of course -- it's R's  internal deparse(.) :

Here's a smaller example:

> (cc <- expression(x ^ {-3})[[1]])
> str(as.list(cc))
List of 3
 $ : symbol ^
 $ : symbol x
 $ : language {  -3 }
> 

Please apologize for any bad light that I might have shed on
Sweave.  I do love its concept!

Martin



From tmlammail at yahoo.com  Thu Sep 15 12:18:18 2005
From: tmlammail at yahoo.com (Martin Lam)
Date: Thu, 15 Sep 2005 03:18:18 -0700 (PDT)
Subject: [R]  How to sort data sets?
Message-ID: <20050915101818.1453.qmail@web40525.mail.yahoo.com>

Hi,

I was wondering if someone know how to sort a data set
by column.
I've tried sort() but without luck. I would think
there should be a function for it somewhere. An
example with the iris data set would be appreciated.

Thanks,

Martin



From vincent at 7d4.com  Thu Sep 15 12:31:14 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Thu, 15 Sep 2005 12:31:14 +0200
Subject: [R] How to sort data sets?
In-Reply-To: <20050915101818.1453.qmail@web40525.mail.yahoo.com>
References: <20050915101818.1453.qmail@web40525.mail.yahoo.com>
Message-ID: <43294D72.1000607@7d4.com>

see : order, sort.list, sort and rank
hih



From ligges at statistik.uni-dortmund.de  Thu Sep 15 12:31:52 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Sep 2005 12:31:52 +0200
Subject: [R] How to sort data sets?
In-Reply-To: <20050915101818.1453.qmail@web40525.mail.yahoo.com>
References: <20050915101818.1453.qmail@web40525.mail.yahoo.com>
Message-ID: <43294D98.4050700@statistik.uni-dortmund.de>

Martin Lam wrote:

> Hi,
> 
> I was wondering if someone know how to sort a data set
> by column.
> I've tried sort() but without luck. I would think
> there should be a function for it somewhere. An
> example with the iris data set would be appreciated.


See ?order.

Uwe Ligges

> Thanks,
> 
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jost at cict.fr  Thu Sep 15 12:39:34 2005
From: jost at cict.fr (Christian Jost)
Date: Thu, 15 Sep 2005 12:39:34 +0200
Subject: [R] Rcommander and simple chisquare
Message-ID: <a0600200abf4efe8dda47@[130.120.104.141]>

In this years biostat teaching I will include Rcommander (it indeed 
simplifies syntax problems that makes students frequently miss the 
core statistical problems). But I could not find how to make a simple 
chisquare comparison between observed frequencies and expected 
frequencies (eg in genetics where you expect phenotypic frequencies 
corresponding to 3:1 in standard dominant/recessif alleles). Any idea 
where this feature might be hidden? Or could it be added to 
Rcommander?

Thanks, Christian.

ps: in case I am not making myself clear, can Rcommander be made to perform
>  chisq.test(c(61,39),p=c(0.75,0.25))



From felipe at unileon.es  Thu Sep 15 12:47:50 2005
From: felipe at unileon.es (Felipe)
Date: Thu, 15 Sep 2005 12:47:50 +0200
Subject: [R] means comparison in R (post-hoc test)
Message-ID: <43295156.7040806@unileon.es>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi.

I have been using SAS for some time, and now I have discovered R. I am
very happy with it, but I have not found out how to perform some of the
multiple comparisons I was used to do in SAS.

With the SAS/STAT, I generally used the MEANS (for comparison of
arithmetic means) and the LSMEANS (for adjusted means) statements of the
GLM procedure (I think it is equivalent to lm in R). They provided a lot
of tests: LSD, Duncan, Tukey-Kramer, Bonferroni, Scheff??, SNK, etc.
However, in R I have only discovered Tukey-HSD.

I have searched for information about this, but I was not successful. I
wonder if anybody knows where I could learn about this. I would like to
use these tests in R, and also obtain the adjusted means like these
produced by the LSMEANS statement.

Thank you.

Felipe
- ----------------------------------oOo----------------------------------
Felipe Mart??nez-Pastor (BSc, PhD)
Animal Reproduction and Obstetrics
Veterinary Clinic Hospital
24071-Le??n (Spain)
Phone: 987 291 430 / 987 291 000 + 5203
Fax: 987 295 203
e-mail: dbcfmp at unileon.es

- ---
This message has been electronically signed using GPG, a free system to
sign and encrypt documents. If you want to learn more, visit
http://www.gnupg.org
-----BEGIN PGP SIGNATURE-----

iEYEARECAAYFAkMpUVYACgkQWtdQtNzjBl4rywCePiSxJw8/N6HzdZ7C+YMHf2K6
YSYAniw5GSo0ihrt4+OabHJ4c2PKNHkp
=aF38
-----END PGP SIGNATURE-----



From petr.pikal at precheza.cz  Thu Sep 15 13:27:43 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Sep 2005 13:27:43 +0200
Subject: [R] if() command
Message-ID: <432976CF.12660.EC2668@localhost>

Hi 

Sorry, I am not sure why sometimes is a text from my answeres 
stripped off.


On 14 Sep 2005 at 14:09, Carlos Mauricio Cardeal Mende wrote: 

> Ok Petr, I run your suggestion and I got this message: 
>  
>  > age<-sample(seq(10,50,10), 20, replace=T) 
>  > 
>  > if (age <=10) {group <- 1} else if (age > 10 & age <= 20)  
{group <- 
>   
> 2} else {group <- 3} 
> Warning message: 
> the condition has length > 1 and only the first element will be  
used 
> in: if (age <= 10) { 
>  
> What does it means ? 

Others has already answered it but I told you that if command is  
not vhat you probably want and suggested to use cut e.g.

cutvector <- c(0,10,20,100) 
group <- as.numeric(cut(age,cutvector)) 

You can change your cutvector according to your wish anytime  
before calling the second function 

HTH 
Petr 



>  
> And when I look to the database I have no new classification ! 
>  
> Could you help please ? 
>  
> Mauricio 
>  
> Petr Pikal escreveu: 
>  
> > Hallo 
> > 
> > 
> > On 13 Sep 2005 at 10:29, Carlos Maur??cio Cardeal Mende  
wrote: 
> > 
> >> Hi everyone ! 
> >> 
> >> Could you please help me with this problem ? 
> >> 
> >> I??ve trying to write a code that assign to a variable the  
content 
> >> from another, but all I??ve got is a message error. For  
example: 
> >> 
> >> if (age <=10) {group == 1} 
> >> else if (age > 10 & age <= 20) {group == 2} 
> >> else {group == 3} 
> > 
> > if you put your statement on one line it works (at least it does  
not 
> > give you syntax error) but the result is hardly what you really 
> > expect 
> > 
> > age<-sample(seq(10,50,10), 20, replace=T) 
> > 
> > if (age <=10) {group <- 1} else if (age > 10 & age <= 20)  
{group <- 
> > 2} else {group <- 3} if (age <=10) {group == 1} else if (age >  
10 & 
> > age <= 20) {group == 2} else {group == 3} 
> > 
> > Maybe you want something like 
> > 
> > group<-as.numeric(cut(age,c(0,10,20,100))) 
> > 
> > but it is only guess 
> > 
> > HTH 
> > Petr 
> > 
> >> 
> >> Syntax error 
> >> 
> >> Or 
> >> 
> >> if (age <=10) {group == 1} 
> >> else (age > 10 & age <= 20) {group == 2} 
> >> else {group == 3} 
> >> 
> >> Syntax error 
> >> 
> >> I know that is possible to find the solution by ifelse command  
or 
> >> even recode command, but I??d like to use this way, because I  
can 
> >> add another variable as a new condition and I believe to  
expand the 
> >> possibilites. 
> >> 
> >> Thanks, 
> >> Mauricio 
> >> 
> >> ______________________________________________ 
> >> R-help at stat.math.ethz.ch mailing list 
> >> https://stat.ethz.ch/mailman/listinfo/r-help 
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html 
> > 
Petr Pikal
petr.pikal at precheza.cz



From maechler at stat.math.ethz.ch  Thu Sep 15 14:30:42 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 15 Sep 2005 14:30:42 +0200
Subject: [R] stripped postings when not following the posting guide
In-Reply-To: <432976CF.12660.EC2668@localhost>
References: <432976CF.12660.EC2668@localhost>
Message-ID: <17193.26994.112485.929967@stat.math.ethz.ch>

>>>>> "Petr" == Petr Pikal <petr.pikal at precheza.cz>
>>>>>     on Thu, 15 Sep 2005 13:27:43 +0200 writes:

    Petr> Hi 
    Petr> Sorry, I am not sure why sometimes is a text from my answeres 
    Petr> stripped off.

Hi Petr,
it's when you don't follow the posting guide _and_ simultaneously
happen to fool the filters so that something is posted at all:

The one you mention from this morning was full of HTML crap,
(wc on the body gave "302  1491 17036"), but wasn't correctly
recognized as HTML by one part of the filters but by the others
(that strip HTML).

      [ If the filters worked as intended currently, such a message
        would be completely swallowed by the filters! ]

Regards, Martin



From phgrosjean at sciviews.org  Thu Sep 15 14:32:13 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 15 Sep 2005 14:32:13 +0200
Subject: [R] Rcommander and simple chisquare
In-Reply-To: <a0600200abf4efe8dda47@[130.120.104.141]>
References: <a0600200abf4efe8dda47@[130.120.104.141]>
Message-ID: <432969CD.1090608@sciviews.org>

Hello,

Just look at Statistics -> Contingency tables. There is an option for 
making the chi square test there.
Best,

Philippe Grosjean,

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Christian Jost wrote:
> In this years biostat teaching I will include Rcommander (it indeed 
> simplifies syntax problems that makes students frequently miss the 
> core statistical problems). But I could not find how to make a simple 
> chisquare comparison between observed frequencies and expected 
> frequencies (eg in genetics where you expect phenotypic frequencies 
> corresponding to 3:1 in standard dominant/recessif alleles). Any idea 
> where this feature might be hidden? Or could it be added to 
> Rcommander?
> 
> Thanks, Christian.
> 
> ps: in case I am not making myself clear, can Rcommander be made to perform
> 
>> chisq.test(c(61,39),p=c(0.75,0.25))
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From Charles.Annis at StatisticalEngineering.com  Thu Sep 15 14:43:20 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 15 Sep 2005 08:43:20 -0400
Subject: [R] Graphical presentation of logistic regression
In-Reply-To: <1126768626.17535.14.camel@biol102145.oulu.fi>
Message-ID: <200509151243.j8FChN1w030817@hypatia.math.ethz.ch>

If a graphical presentation provides improved insight then that is
sufficient justification.  The existence of "better" more precise methods,
does not change that.

I, too, sometimes use jitter() to avoid overplotting of observations, but I
think the dot-plots in de la Cruz's code are even better.  It is the
histogram that is misleading (due to paucity of data), not the effort to
elucidate the joint behavior of zeros and ones. 
http://www.esapubs.org/bulletin/backissues/086-1/bulletinjan2005.htm#et

Please try a variation that his code provides:

plot.logi.hist(independ = altitude, depend = tree, logi.mod = 1, type =
"dit", boxp = TRUE, rug = TRUE, las.h = 1)

which does not use the histograms but instead uses "dit plots" to provide a
helpful, visceral feel for the behavior of the observations.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jari Oksanen
Sent: Thursday, September 15, 2005 3:17 AM
To: Frank E Harrell Jr
Cc: r-help at stat.math.ethz.ch; Beale, Colin
Subject: Re: [R] Graphical presentation of logistic regression

On Wed, 2005-09-14 at 06:29 -0500, Frank E Harrell Jr wrote:
> Beale, Colin wrote:
> > Hi,
> > 
> > I wonder if anyone has written any code to implement the suggestions of
> > Smart et al (2004) in the Bulletin of the Ecological Society of America
> > for a new way of graphically presenting the results of logistic
> > regression (see
> > www.esapubs.org/bulletin/backissues/085-3/bulletinjuly2004_2column.htm#t
> > ools1 for the full text)? I couldn't find anything relating to this sort
> > of graphical representation of logistic models in the archives, but
> > maybe someone has solved it already? In short, Smart et al suggest that
> > a logistic regression be presented as a combination of the two
> > histograms for successes and failures (with one presented upside down at
> > the top of the figure, the other the right way up at the bottom)
> > overlaid by the probability function (ie logistic curve). It's somewhat
> > hard to describe, but is nicely illustrated in the full text version
> > above. I think it is a sensible way of presenting these results and am
> > keen to do so - at the moment I can only do this by generating the two
> > histograms and the logistic curve separately (using hist() and lines()),
> > then copying and pasting the graphs out of R and inverting one in a
> > graphics package, before overlying the others. I'm sure this could be
> > done within R and would be a handy plotting function to develop. Has
> > anyone done so, or can anyone give me any pointers to doing this? I
> > really nead to know how to invert a histogram and how to overlay this
> > with another histogram "the right way up".
> > 
> > Any thoughts would be welcome.
> > 
> > Thanks in advance,
> > Colin
> 
>  From what you describe, that is a poor way to represent the model 
> except for judging discrimination ability (if the model is calibrated 
> well).  Effect plots, odds ratio charts, and nomograms are better.  See 
> the Design package for details.
> 

You're correct when you say that this is a poor way to represent the
model. However, you should have some understanding to us ecologists who
are simple creatures working with tangible subjects such as animals and
plants (microbiologists work with less tangible things). Therefore we
want to have a concrete and simple representation. After all, the
example was about occurrence of an animal against a concrete
environmental variable, and a concrete representation was suggested.
Nomograms and things are abstractions that you understand first after
long education and training (I tried the Design package and I didn't
understand the nomogram plot). 

I tried with one concrete example with my own data, and the inverted
histogram method was patently misleading (with Baz Rowlingson's neat and
compact code, sorry for the repetition). The method would be useful with
dense and regular data only, but now the clearest visual cue was the
uneven sampling intensity. With my limited knowledge on R facilities, I
can now remember only two ways two preserve the concreteness of display
in the base R: jitter() to avoid overplotting of observations, and
sunflowerplot() to show the amount of overplotting.

I think Ecological Society of America would be happy to receive papers
to suggest better ways to represent binary response data, if some of the
knowledgeable persons in this groups would decided to educate them (I'm
not an ESA member, so I wouldn't be educated: therefore 'them' instead
of 'us'). The ESA bulletin will be influential in manuscript submitted
to the Society journals in the future, and the time for action is now.

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
Ph. +358 8 5531526, cell +358 40 5136529, fax +358 8 5531061
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From fcombes at gmail.com  Thu Sep 15 14:45:33 2005
From: fcombes at gmail.com (Florence Combes)
Date: Thu, 15 Sep 2005 14:45:33 +0200
Subject: [R] how to do sthg like "mat[!=(ind),]"
Message-ID: <73dae3060509150545461e92dd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050915/3a06b21c/attachment.pl

From petr.pikal at precheza.cz  Thu Sep 15 14:47:01 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Sep 2005 14:47:01 +0200
Subject: [R] stripped postings when not following the posting guide
In-Reply-To: <17193.26994.112485.929967@stat.math.ethz.ch>
References: <432976CF.12660.EC2668@localhost>
Message-ID: <43298965.29223.134BE16@localhost>

Hi Martin

It sometimes happens when I respond to somebodys question. 
When I post my own I try to follow the posting guide closely and 
do not use HTML crap at all. 

I just do not know how to recognize that my response has this 
unwanted residues and how to get rid of them before I actually 
find that the message is stripped off.

I am using Pegassus Mail 4.21c.

Thanks
Petr






On 15 Sep 2005 at 14:30, Martin Maechler wrote:

> >>>>> "Petr" == Petr Pikal <petr.pikal at precheza.cz>
> >>>>>     on Thu, 15 Sep 2005 13:27:43 +0200 writes:
> 
>     Petr> Hi 
>     Petr> Sorry, I am not sure why sometimes is a text from my
>     answeres Petr> stripped off.
> 
> Hi Petr,
> it's when you don't follow the posting guide _and_ simultaneously
> happen to fool the filters so that something is posted at all:
> 
> The one you mention from this morning was full of HTML crap,
> (wc on the body gave "302  1491 17036"), but wasn't correctly
> recognized as HTML by one part of the filters but by the others
> (that strip HTML).
> 
>       [ If the filters worked as intended currently, such a message
>         would be completely swallowed by the filters! ]
> 
> Regards, Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ligges at statistik.uni-dortmund.de  Thu Sep 15 14:52:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Sep 2005 14:52:39 +0200
Subject: [R] how to do sthg like "mat[!=(ind),]"
In-Reply-To: <73dae3060509150545461e92dd@mail.gmail.com>
References: <73dae3060509150545461e92dd@mail.gmail.com>
Message-ID: <43296E97.1090006@statistik.uni-dortmund.de>

Florence Combes wrote:

> Hi 
> 
> I want to do something which seems straightforward, but I couldn't find the 
> way to do this. 
> I have a matrix called m for example, and a vector of values (let's call ind 
> this vector) which are indices of lines I don't want to keep. 
> 
> for example I have:
> 
> 
>>m
> 
> v1 v2 v3
> [1,] 1 4 7
> [2,] 2 5 8
> [3,] 3 6 9
> [4,] 10 11 12
> 
> 
>>ind
> 
> [1] 2 4
> 
> 
> I would like to obtain this:
> 
> 
>>m2
> 
> v1 v2 v3
> [1,] 1 4 7
> [3,] 3 6 9
> 
> by saying something like 
> 
> m2<-m[!=(ind),] 



See help("[") and learn about negative indices:

m2 <- m[-ind,]

Uwe Ligges


> but this line does not work. 
> 
> Any idea or suggestion highly welcome !
> 
> Florence.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dieter.menne at menne-biomed.de  Thu Sep 15 14:48:09 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 15 Sep 2005 12:48:09 +0000 (UTC)
Subject: [R] means comparison in R (post-hoc test)
References: <43295156.7040806@unileon.es>
Message-ID: <loom.20050915T144646-454@post.gmane.org>

Felipe <felipe <at> unileon.es> writes:

> With the SAS/STAT, I generally used the MEANS (for comparison of
> arithmetic means) and the LSMEANS (for adjusted means) statements of the
> GLM procedure (I think it is equivalent to lm in R). They provided a lot
> of tests: LSD, Duncan, Tukey-Kramer, Bonferroni, ScheffÃ©, SNK, etc.
> However, in R I have only discovered Tukey-HSD.

Package multcomp with the workhorse-function simint comes close to what you 
want.


Dieter



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Sep 15 15:01:57 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 15 Sep 2005 09:01:57 -0400
Subject: [R] Splitting the string at the last sub-string
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD503914EC9@us-arlington-0668.mail.saic.com>


Hi,

I need to split a string into 2 strings, with the split point defined by the
last occurrence of some substring. I come up with some convoluted code to do
so:

str = "Chance favors the prepared mind"
sub = "e"
y = unlist(strsplit(str,sub))
z = cbind(paste(y[-length(y)], sub,  sep="", collapse = ""), y[length(y)]);

y
z
z[1]
z[2]

Is there a simpler way to do so? I think ~8 function calls to do such a
simple operation is an overkill.

Jarek 
====================================================\====                 
 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">  \ 
 Jaroslaw.W.Tuszynski at saic.com                    `    \



From f.harrell at vanderbilt.edu  Thu Sep 15 15:05:01 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 15 Sep 2005 08:05:01 -0500
Subject: [R] Graphical presentation of logistic regression
In-Reply-To: <1126768626.17535.14.camel@biol102145.oulu.fi>
References: <E71A2CDB5DFFD14383BAC2E0612811BA4B6D22@ADARA.RSPB.ORG.UK>	
	<43280987.1080403@vanderbilt.edu>
	<1126768626.17535.14.camel@biol102145.oulu.fi>
Message-ID: <4329717D.3050307@vanderbilt.edu>

Jari Oksanen wrote:
> On Wed, 2005-09-14 at 06:29 -0500, Frank E Harrell Jr wrote:
> 
>>Beale, Colin wrote:
>>
>>>Hi,
>>>
>>>I wonder if anyone has written any code to implement the suggestions of
>>>Smart et al (2004) in the Bulletin of the Ecological Society of America
>>>for a new way of graphically presenting the results of logistic
>>>regression (see
>>>www.esapubs.org/bulletin/backissues/085-3/bulletinjuly2004_2column.htm#t
>>>ools1 for the full text)? I couldn't find anything relating to this sort
>>>of graphical representation of logistic models in the archives, but
>>>maybe someone has solved it already? In short, Smart et al suggest that
>>>a logistic regression be presented as a combination of the two
>>>histograms for successes and failures (with one presented upside down at
>>>the top of the figure, the other the right way up at the bottom)
>>>overlaid by the probability function (ie logistic curve). It's somewhat
>>>hard to describe, but is nicely illustrated in the full text version
>>>above. I think it is a sensible way of presenting these results and am
>>>keen to do so - at the moment I can only do this by generating the two
>>>histograms and the logistic curve separately (using hist() and lines()),
>>>then copying and pasting the graphs out of R and inverting one in a
>>>graphics package, before overlying the others. I'm sure this could be
>>>done within R and would be a handy plotting function to develop. Has
>>>anyone done so, or can anyone give me any pointers to doing this? I
>>>really nead to know how to invert a histogram and how to overlay this
>>>with another histogram "the right way up".
>>>
>>>Any thoughts would be welcome.
>>>
>>>Thanks in advance,
>>>Colin
>>
>> From what you describe, that is a poor way to represent the model 
>>except for judging discrimination ability (if the model is calibrated 
>>well).  Effect plots, odds ratio charts, and nomograms are better.  See 
>>the Design package for details.
>>
> 
> 
> You're correct when you say that this is a poor way to represent the
> model. However, you should have some understanding to us ecologists who
> are simple creatures working with tangible subjects such as animals and
> plants (microbiologists work with less tangible things). Therefore we
> want to have a concrete and simple representation. After all, the
> example was about occurrence of an animal against a concrete
> environmental variable, and a concrete representation was suggested.
> Nomograms and things are abstractions that you understand first after
> long education and training (I tried the Design package and I didn't
> understand the nomogram plot). 

I don't understand why you think the histograms are "representing the 
model".  That approach even seems to be interchanging the roles of the 
independent and dependent variables.

> 
> I tried with one concrete example with my own data, and the inverted
> histogram method was patently misleading (with Baz Rowlingson's neat and
> compact code, sorry for the repetition). The method would be useful with
> dense and regular data only, but now the clearest visual cue was the
> uneven sampling intensity. With my limited knowledge on R facilities, I
> can now remember only two ways two preserve the concreteness of display
> in the base R: jitter() to avoid overplotting of observations, and
> sunflowerplot() to show the amount of overplotting.
> 
> I think Ecological Society of America would be happy to receive papers
> to suggest better ways to represent binary response data, if some of the
> knowledgeable persons in this groups would decided to educate them (I'm
> not an ESA member, so I wouldn't be educated: therefore 'them' instead
> of 'us'). The ESA bulletin will be influential in manuscript submitted
> to the Society journals in the future, and the time for action is now.

See

@Article{gui00ord,
   author = 		 {Guisan, Antoine and Harrell, Frank E.},
   title = 		 {Ordinal response regression models in
ecology},
   journal = 	 {Journal of Vegetation Science},
   year = 		 2000,
   volume = 11,
   pages = {617-626},
   annote =		 {teaching;ordinal logistic model}
}

This is more complex than needed (ordinal instead of binary) but binary 
is a special case of ordinal.

Cheers,

Frank

> 
> cheers, jari oksanen


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ripley at stats.ox.ac.uk  Thu Sep 15 15:29:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Sep 2005 14:29:28 +0100 (BST)
Subject: [R] Splitting the string at the last sub-string
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD503914EC9@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD503914EC9@us-arlington-0668.mail.saic.com>
Message-ID: <Pine.LNX.4.61.0509151425340.2920@gannet.stats>

> regexpr(".*e", str)
[1] 1
attr(,"match.length")
[1] 25

tells you you need

> substring(str, c(1, 26), c(25,length(str)))
[1] "Chance favors the prepare" "d mind"

to reproduce your answer (I don't know what you want to do with the 
substring, but you included it in the first string, which is not what 
split() does).

On Thu, 15 Sep 2005, Tuszynski, Jaroslaw W. wrote:

>
> Hi,
>
> I need to split a string into 2 strings, with the split point defined by the
> last occurrence of some substring. I come up with some convoluted code to do
> so:
>
> str = "Chance favors the prepared mind"
> sub = "e"
> y = unlist(strsplit(str,sub))
> z = cbind(paste(y[-length(y)], sub,  sep="", collapse = ""), y[length(y)]);
>
> y
> z
> z[1]
> z[2]
>
> Is there a simpler way to do so? I think ~8 function calls to do such a
> simple operation is an overkill.
>
> Jarek
> ====================================================\====
> Jarek Tuszynski, PhD.                           o / \
> Science Applications International Corporation  <\__,|
> (703) 676-4192                                   ">  \
> Jaroslaw.W.Tuszynski at saic.com                    `    \
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ccthomas at joimail.com  Thu Sep 15 17:10:53 2005
From: ccthomas at joimail.com (Courtney Thomas)
Date: Thu, 15 Sep 2005 10:10:53 -0500
Subject: [R] make check FAILS -> Error code 1...comparing d-p-q-r-tests.Rout
Message-ID: <1126797053.533.139.camel@bsdbox.SAMBA>

Under FreeBSD 5.3, attempting to properly install R-2.1.1, I get the
following response when I.....

% make		;all finishes without error, then...

% make check <ret>
.
.
----------------------------------------------
comparing d-p-q-r-tests.Rout
	to
	  d-p-q-r-tests.Rout.save

1004c1004

< [1] mean relative difference 1.2848649e-08
  [1] TRUE
.....Error code 1

stop in ~R/R-2.1.1/tests
-----------------------------------------------

I assume a computed value is out of bounds regarding a predetermined
range of accuracy. Not being a statistician nor programmer, how might I
fix this, please ?

Appreciatively,

Courtney



From B.Rowlingson at lancaster.ac.uk  Thu Sep 15 16:01:36 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 15 Sep 2005 15:01:36 +0100
Subject: [R] Splitting the string at the last sub-string
In-Reply-To: <Pine.LNX.4.61.0509151425340.2920@gannet.stats>
References: <CA0BCF3BED56294AB91E3AD74B849FD503914EC9@us-arlington-0668.mail.saic.com>
	<Pine.LNX.4.61.0509151425340.2920@gannet.stats>
Message-ID: <43297EC0.7080305@lancaster.ac.uk>

Prof Brian Ripley wrote:

>>substring(str, c(1, 26), c(25,length(str)))

  nchar(str) surely?

  regexps can be rather slow though. Here's two functions:

byRipley =
function(str,sub){
   lp=attr(regexpr(paste(".*",sub,sep=""),str),'match.length')
   return(substring(str, c(1, lp+1), c(lp,nchar(str))))
}

byJarek =
function(str,sub){
   y = unlist(strsplit(str,sub))
   return(cbind(paste(y[-length(y)], sub,  sep="", collapse = ""), 
y[length(y)]))
}

  and a quick test:

 > system.time(for(i in 1:100000){byJarek(str,sub)})
[1] 15.55  0.10 16.06  0.00  0.00

 > system.time(for(i in 1:100000){byRipley(str,sub)})
[1] 30.28  0.07 31.86  0.00  0.00

Baz



From kalleswedens at hotmail.com  Tue Sep 13 14:35:17 2005
From: kalleswedens at hotmail.com (Karsten Luder)
Date: Tue, 13 Sep 2005 14:35:17 +0200
Subject: [R] Remove vector elements from another vector
Message-ID: <BAY12-F51E657A81E8BED92ECA0DDD9C0@phx.gbl>

Hello,

I have two vectors of different lengths. Fx a <- 1:9; b <- c(4, 5).
What is the best way to remove the elements in vector b from vector a so 
that the result would be a vector with elements c(1,2,3,6,7,8,9)?

Best regards,
Kalle

_________________________________________________________________
Find masser af gode tilbud p?? MSN Shopping http://shopping.msn.dk/



From ligges at statistik.uni-dortmund.de  Thu Sep 15 16:26:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Sep 2005 16:26:12 +0200
Subject: [R] Remove vector elements from another vector
In-Reply-To: <BAY12-F51E657A81E8BED92ECA0DDD9C0@phx.gbl>
References: <BAY12-F51E657A81E8BED92ECA0DDD9C0@phx.gbl>
Message-ID: <43298484.9000306@statistik.uni-dortmund.de>

Karsten Luder wrote:

> Hello,
> 
> I have two vectors of different lengths. Fx a <- 1:9; b <- c(4, 5).
> What is the best way to remove the elements in vector b from vector a so 
> that the result would be a vector with elements c(1,2,3,6,7,8,9)?


setdiff(a, b)

Uwe Ligges


> Best regards,
> Kalle
> 
> _________________________________________________________________
> Find masser af gode tilbud p?? MSN Shopping http://shopping.msn.dk/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.be  Thu Sep 15 16:32:31 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 15 Sep 2005 16:32:31 +0200
Subject: [R] Remove vector elements from another vector
References: <BAY12-F51E657A81E8BED92ECA0DDD9C0@phx.gbl>
Message-ID: <008101c5ba02$4e4e9180$0540210a@www.domain>

look at function ?setdiff(), e.g.,

setdiff(a, b)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Karsten Luder" <kalleswedens at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, September 13, 2005 2:35 PM
Subject: [SPAM?] [R] Remove vector elements from another vector


Hello,

I have two vectors of different lengths. Fx a <- 1:9; b <- c(4, 5).
What is the best way to remove the elements in vector b from vector a 
so
that the result would be a vector with elements c(1,2,3,6,7,8,9)?

Best regards,
Kalle

_________________________________________________________________
Find masser af gode tilbud p?? MSN Shopping http://shopping.msn.dk/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From p.dalgaard at biostat.ku.dk  Thu Sep 15 16:31:03 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Sep 2005 16:31:03 +0200
Subject: [R] Remove vector elements from another vector
In-Reply-To: <BAY12-F51E657A81E8BED92ECA0DDD9C0@phx.gbl>
References: <BAY12-F51E657A81E8BED92ECA0DDD9C0@phx.gbl>
Message-ID: <x2d5nawggo.fsf@turmalin.kubism.ku.dk>

"Karsten Luder" <kalleswedens at hotmail.com> writes:

> Hello,
> 
> I have two vectors of different lengths. Fx a <- 1:9; b <- c(4, 5).
> What is the best way to remove the elements in vector b from vector a so 
> that the result would be a vector with elements c(1,2,3,6,7,8,9)?

I think we had this on the list no more than a week ago...

setdiff(a,b) 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From petr.pikal at precheza.cz  Thu Sep 15 16:31:43 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Sep 2005 16:31:43 +0200
Subject: [R] Remove vector elements from another vector
In-Reply-To: <BAY12-F51E657A81E8BED92ECA0DDD9C0@phx.gbl>
Message-ID: <4329A1EF.26591.19498C8@localhost>

Hi

On 13 Sep 2005 at 14:35, Karsten Luder wrote:

> Hello,
> 
> I have two vectors of different lengths. Fx a <- 1:9; b <- c(4, 5).
> What is the best way to remove the elements in vector b from vector a
> so that the result would be a vector with elements c(1,2,3,6,7,8,9)?

> which(!a%in%b)
[1] 1 2 3 6 7 8 9
> a[which(!a%in%b)]
[1] 1 2 3 6 7 8 9
> a[(!a%in%b)]
[1] 1 2 3 6 7 8 9
>


HTH
Petr

> 
> Best regards,
> Kalle
> 
> _________________________________________________________________ Find
> masser af gode tilbud p?? MSN Shopping http://shopping.msn.dk/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From tom at maladmin.com  Thu Sep 15 12:42:58 2005
From: tom at maladmin.com (tom wright)
Date: Thu, 15 Sep 2005 06:42:58 -0400
Subject: [R] newbie question
Message-ID: <1126780978.4393.1.camel@localhost.localdomain>

Can someone tell me how I create a vector of numbers where the step
isn't 1? 
i.e. x<-(0.0,0.5,1.0,1.5....)

Thanks
tom



From ripley at stats.ox.ac.uk  Thu Sep 15 16:42:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Sep 2005 15:42:43 +0100 (BST)
Subject: [R] Splitting the string at the last sub-string
In-Reply-To: <43297EC0.7080305@lancaster.ac.uk>
References: <CA0BCF3BED56294AB91E3AD74B849FD503914EC9@us-arlington-0668.mail.saic.com>
	<Pine.LNX.4.61.0509151425340.2920@gannet.stats>
	<43297EC0.7080305@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.61.0509151538340.3964@gannet.stats>

On Thu, 15 Sep 2005, Barry Rowlingson wrote:

> Prof Brian Ripley wrote:
>
>>> substring(str, c(1, 26), c(25,length(str)))
>
>  nchar(str) surely?

Yes, or anything larger:  I actually tested 10000.

>  regexps can be rather slow though. Here's two functions:

But that's not the way to do this repeatedly for the same pattern. (It is 
normally compiling regexps that is slow, and regexpr is vectorized.) Not 
that I would call 300us `slow'.

> byRipley =
> function(str,sub){
>   lp=attr(regexpr(paste(".*",sub,sep=""),str),'match.length')
>   return(substring(str, c(1, lp+1), c(lp,nchar(str))))
> }
>
> byJarek =
> function(str,sub){
>   y = unlist(strsplit(str,sub))
>   return(cbind(paste(y[-length(y)], sub,  sep="", collapse = ""),
> y[length(y)]))
> }
>
>  and a quick test:
>
> > system.time(for(i in 1:100000){byJarek(str,sub)})
> [1] 15.55  0.10 16.06  0.00  0.00
>
> > system.time(for(i in 1:100000){byRipley(str,sub)})
> [1] 30.28  0.07 31.86  0.00  0.00
>
> Baz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tyler.smith at mail.mcgill.ca  Thu Sep 15 16:43:29 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Thu, 15 Sep 2005 10:43:29 -0400
Subject: [R] Graphics 'snapshots' in Linux?
Message-ID: <43298891.3010103@mail.mcgill.ca>

Hi,

I'm working on a MEPIS (Debian-based Linux) computer, using the 
emacs/ESS package to do my R work. I've got some plots that I label 
interactively using the locate function. With the Windows GUI there is 
an option to take a snapshot of the graphics output, saving it as an 
image file. Is there a way to do this with emacs/ESS?

Thanks,

Tyler



From sdavis2 at mail.nih.gov  Thu Sep 15 16:46:41 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 15 Sep 2005 10:46:41 -0400
Subject: [R] newbie question
In-Reply-To: <1126780978.4393.1.camel@localhost.localdomain>
Message-ID: <BF4F0191.EA00%sdavis2@mail.nih.gov>

On 9/15/05 6:42 AM, "tom wright" <tom at maladmin.com> wrote:

> Can someone tell me how I create a vector of numbers where the step
> isn't 1? 
> i.e. x<-(0.0,0.5,1.0,1.5....)

See ?seq

Sean



From ripley at stats.ox.ac.uk  Thu Sep 15 16:49:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Sep 2005 15:49:54 +0100 (BST)
Subject: [R] make check FAILS -> Error code 1...comparing
	d-p-q-r-tests.Rout
In-Reply-To: <1126797053.533.139.camel@bsdbox.SAMBA>
References: <1126797053.533.139.camel@bsdbox.SAMBA>
Message-ID: <Pine.LNX.4.61.0509151547330.4181@gannet.stats>

The problem is a known one and solved in R 2.1.1-patched.  On your OS the 
result is not as accurate as most, but the tolerance set was too tight so 
the test failure is not something to worry about.

Please install R-patched instead, as it has many bug fixes in place.

On Thu, 15 Sep 2005, Courtney Thomas wrote:

> Under FreeBSD 5.3, attempting to properly install R-2.1.1, I get the
> following response when I.....
>
> % make		;all finishes without error, then...
>
> % make check <ret>
> .
> .
> ----------------------------------------------
> comparing d-p-q-r-tests.Rout
> 	to
> 	  d-p-q-r-tests.Rout.save
>
> 1004c1004
>
> < [1] mean relative difference 1.2848649e-08
>  [1] TRUE
> .....Error code 1
>
> stop in ~R/R-2.1.1/tests
> -----------------------------------------------
>
> I assume a computed value is out of bounds regarding a predetermined
> range of accuracy. Not being a statistician nor programmer, how might I
> fix this, please ?
>
> Appreciatively,
>
> Courtney
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lamkelj at yahoo.com  Thu Sep 15 16:52:33 2005
From: lamkelj at yahoo.com (Kel Lam)
Date: Thu, 15 Sep 2005 07:52:33 -0700 (PDT)
Subject: [R] Combine vector of different length
Message-ID: <20050915145233.65595.qmail@web52713.mail.yahoo.com>

Hi group,  

Is there a quick way to cbind vector of different
length?  I should have checked out the previous
postings but somehow I can't access the list from CRAN
website.  Thanks for your help.

Regards,
Kelvin



From vincent at 7d4.com  Thu Sep 15 16:53:52 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Thu, 15 Sep 2005 16:53:52 +0200
Subject: [R] newbie question
In-Reply-To: <1126780978.4393.1.camel@localhost.localdomain>
References: <1126780978.4393.1.camel@localhost.localdomain>
Message-ID: <43298B00.9090307@7d4.com>

tom wright a ??crit :
> Can someone tell me how I create a vector of numbers 
 > where the step isn't 1?
> i.e. x<-(0.0,0.5,1.0,1.5....)

seq(a, b, 0.5)
?seq
hih



From ligges at statistik.uni-dortmund.de  Thu Sep 15 16:55:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Sep 2005 16:55:27 +0200
Subject: [R] sequences; was: newbie question
In-Reply-To: <1126780978.4393.1.camel@localhost.localdomain>
References: <1126780978.4393.1.camel@localhost.localdomain>
Message-ID: <43298B5F.9020604@statistik.uni-dortmund.de>

tom wright wrote:

> Can someone tell me how I create a vector of numbers where the step
> isn't 1? 
> i.e. x<-(0.0,0.5,1.0,1.5....)


Folks,

please read the posting guide and basic documentation!
Please use a sensible subject.

Don't know how many of therelike questions I have read today...

See ?seq

Uwe Ligges


> Thanks
> tom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfontain at free.fr  Thu Sep 15 16:54:56 2005
From: jfontain at free.fr (jfontain@free.fr)
Date: Thu, 15 Sep 2005 16:54:56 +0200
Subject: [R] Graphics 'snapshots' in Linux?
In-Reply-To: <43298891.3010103@mail.mcgill.ca>
References: <43298891.3010103@mail.mcgill.ca>
Message-ID: <1126796096.43298b400d438@imp6-g19.free.fr>

Quoting Tyler Smith <tyler.smith at mail.mcgill.ca>:

> Hi,
>
> I'm working on a MEPIS (Debian-based Linux) computer, using the
> emacs/ESS package to do my R work. I've got some plots that I label
> interactively using the locate function. With the Windows GUI there is
> an option to take a snapshot of the graphics output, saving it as an
> image file. Is there a way to do this with emacs/ESS?

In the graphics menu, you could use ksnapshot.

--
Jean-Luc



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Sep 15 17:00:17 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 15 Sep 2005 11:00:17 -0400
Subject: [R] Splitting the string at the last sub-string
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD50391505D@us-arlington-0668.mail.saic.com>

Thanks for suggestions. I suspect the "regexpr" version will be better than
my version, since I use it to find an string towards the end of a large (up
to ~30Mb) test/XML file.

Thanks again.

Jarek
====================================================\==== 
 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">  \ 
 Jaroslaw.W.Tuszynski at saic.com                     `   \ 

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
Sent: Thursday, September 15, 2005 10:43 AM
To: Barry Rowlingson
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Splitting the string at the last sub-string

On Thu, 15 Sep 2005, Barry Rowlingson wrote:

> Prof Brian Ripley wrote:
>
>>> substring(str, c(1, 26), c(25,length(str)))
>
>  nchar(str) surely?

Yes, or anything larger:  I actually tested 10000.

>  regexps can be rather slow though. Here's two functions:

But that's not the way to do this repeatedly for the same pattern. (It is
normally compiling regexps that is slow, and regexpr is vectorized.) Not
that I would call 300us `slow'.

> byRipley =
> function(str,sub){
>   lp=attr(regexpr(paste(".*",sub,sep=""),str),'match.length')
>   return(substring(str, c(1, lp+1), c(lp,nchar(str)))) }
>
> byJarek =
> function(str,sub){
>   y = unlist(strsplit(str,sub))
>   return(cbind(paste(y[-length(y)], sub,  sep="", collapse = ""),
> y[length(y)]))
> }
>
>  and a quick test:
>
> > system.time(for(i in 1:100000){byJarek(str,sub)})
> [1] 15.55  0.10 16.06  0.00  0.00
>
> > system.time(for(i in 1:100000){byRipley(str,sub)})
> [1] 30.28  0.07 31.86  0.00  0.00
>
> Baz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Thu Sep 15 17:18:35 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 15 Sep 2005 10:18:35 -0500
Subject: [R] Combine vector of different length
In-Reply-To: <20050915145233.65595.qmail@web52713.mail.yahoo.com>
References: <20050915145233.65595.qmail@web52713.mail.yahoo.com>
Message-ID: <432990CB.1080507@pdf.com>



Kel Lam wrote:
> Hi group,  
> 
> Is there a quick way to cbind vector of different
> length?  I should have checked out the previous
> postings but somehow I can't access the list from CRAN
> website.  Thanks for your help.
> 
> Regards,
> Kelvin
> 

Hi, Kelvin,

The following thread gives a few suggestions.

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/59298.html

HTH,

--sundar



From caobg at email.uc.edu  Thu Sep 15 17:26:12 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Thu, 15 Sep 2005 11:26:12 -0400 (EDT)
Subject: [R] about cutree
Message-ID: <20050915112612.CTU05362@mirapoint.uc.edu>

Hi Everyone,

I'm trying to use cutree to get the clusters after hclust. What I used is: mycluster<-cutree(cnclust,h=0.5)
Now, my problem is, how can I get the actual clusters? Thanks!

Best,
 Baoqiang Cao



From jfox at mcmaster.ca  Thu Sep 15 17:31:01 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 15 Sep 2005 11:31:01 -0400
Subject: [R] Rcommander and simple chisquare
In-Reply-To: <a0600200abf4efe8dda47@[130.120.104.141]>
Message-ID: <20050915153115.SCYG26102.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Christian,

>From the Rcmdr menus, select "Statistics -> Summaries -> Frequency
distributions", and check the "Chisquare goodness of fit test" box in the
resulting dialog. This will bring up a dialog box where you can enter
hypothesized probabilities from which expected frequencies will be
calculated.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christian Jost
> Sent: Thursday, September 15, 2005 5:40 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Rcommander and simple chisquare
> 
> In this years biostat teaching I will include Rcommander (it 
> indeed simplifies syntax problems that makes students 
> frequently miss the core statistical problems). But I could 
> not find how to make a simple chisquare comparison between 
> observed frequencies and expected frequencies (eg in genetics 
> where you expect phenotypic frequencies corresponding to 3:1 
> in standard dominant/recessif alleles). Any idea where this 
> feature might be hidden? Or could it be added to Rcommander?
> 
> Thanks, Christian.
> 
> ps: in case I am not making myself clear, can Rcommander be 
> made to perform
> >  chisq.test(c(61,39),p=c(0.75,0.25))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Thu Sep 15 17:32:17 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 15 Sep 2005 11:32:17 -0400
Subject: [R] Rcommander and simple chisquare
In-Reply-To: <a0600200abf4efe8dda47@[130.120.104.141]>
Message-ID: <20050915153217.ZYWK21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Christian,

>From the Rcmdr menus, select "Statistics -> Summaries -> Frequency
distributions", and check the "Chisquare goodness of fit test" box in the
resulting dialog. This will bring up a dialog box where you can enter
hypothesized probabilities from which expected frequencies will be
calculated.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christian Jost
> Sent: Thursday, September 15, 2005 5:40 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Rcommander and simple chisquare
> 
> In this years biostat teaching I will include Rcommander (it 
> indeed simplifies syntax problems that makes students 
> frequently miss the core statistical problems). But I could 
> not find how to make a simple chisquare comparison between 
> observed frequencies and expected frequencies (eg in genetics 
> where you expect phenotypic frequencies corresponding to 3:1 
> in standard dominant/recessif alleles). Any idea where this 
> feature might be hidden? Or could it be added to Rcommander?
> 
> Thanks, Christian.
> 
> ps: in case I am not making myself clear, can Rcommander be 
> made to perform
> >  chisq.test(c(61,39),p=c(0.75,0.25))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dushoff at eno.princeton.edu  Thu Sep 15 18:02:20 2005
From: dushoff at eno.princeton.edu (Jonathan Dushoff)
Date: Thu, 15 Sep 2005 12:02:20 -0400 (EDT)
Subject: [R] Log scale in histograms
In-Reply-To: <Pine.LNX.4.61.0509142237080.2700@tahawus.Princeton.EDU>
References: <Pine.LNX.4.61.0509142237080.2700@tahawus.Princeton.EDU>
Message-ID: <Pine.LNX.4.61.0509151200170.2700@tahawus.Princeton.EDU>



> Can't find any information about this, but others must want to do it.

> In the example below, the second plot has the desired log scale,
> but the first does not.

> Any help appreciated.

Well, I had to solve this problem myself.  I hope that doesn't prove I
should have posted it.  I did spend a lot of time on it, both before and
after posting.

Here is a reasonable-looking histogram based on logged data:

hist(log10(area_Mh), 12, xlab="Area (Mh)", main="", axes=FALSE)
axis(1, labels=formatC(10^(axTicks(1)), digits=3))
axis(2)

JD



From mschwartz at mn.rr.com  Thu Sep 15 18:31:16 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 15 Sep 2005 11:31:16 -0500
Subject: [R] Graphics 'snapshots' in Linux?
In-Reply-To: <43298891.3010103@mail.mcgill.ca>
References: <43298891.3010103@mail.mcgill.ca>
Message-ID: <1126801876.3340.23.camel@localhost.localdomain>

On Thu, 2005-09-15 at 10:43 -0400, Tyler Smith wrote:
> Hi,
> 
> I'm working on a MEPIS (Debian-based Linux) computer, using the 
> emacs/ESS package to do my R work. I've got some plots that I label 
> interactively using the locate function. With the Windows GUI there is 
> an option to take a snapshot of the graphics output, saving it as an 
> image file. Is there a way to do this with emacs/ESS?
> 
> Thanks,
> 
> Tyler

Tyler,

Take a look at ?dev.copy2eps and on the same page dev.copy(), which
enable you to copy the current X11 plot supported output devices.

You could do something like the following for an EPS file:

plot(1:5)
text(locator(1), "Place Text Here")
dev.copy2eps(file = "MyPlot.eps")


or the following for a PNG file:

par(bg = "white")
plot(1:5)
text(locator(1), "Place Text Here")
dev.copy(device = png, file = "MyPlot.png")
dev.off()


Note that in the first example, dev.off() is not required, as the EPS
output device is closed after the call.

Also, note in the second example, you will need to set the background to
white (unless already specified for whatever color you may be using), as
the default output file will have a transparent background, even though
the png() function shows the default as white. If my memory is correct
this is because the X11 device itself has a transparent background by
default and this is what is copied.

HTH,

Marc Schwartz



From felipe at unileon.es  Thu Sep 15 18:39:01 2005
From: felipe at unileon.es (Felipe)
Date: Thu, 15 Sep 2005 18:39:01 +0200
Subject: [R] means comparison in R (post-hoc test)
In-Reply-To: <loom.20050915T144646-454@post.gmane.org>
References: <43295156.7040806@unileon.es>
	<loom.20050915T144646-454@post.gmane.org>
Message-ID: <4329A3A5.6060600@unileon.es>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Thank you, I think multcomp is very near to what I was looking for.
However, I am still looking for a mean to obtain least-squares
(adjusted) means and std. errors of these means, and performing
comparisons among these means, as the LSMEANS do in SAS. I have read
other messages and have looked at car, effects and Design manuals, but I
am not sure if this is what I am looking for. Any clue?

Felipe

Dieter Menne wrote:
| Felipe <felipe <at> unileon.es> writes:
|
|
|>With the SAS/STAT, I generally used the MEANS (for comparison of
|>arithmetic means) and the LSMEANS (for adjusted means) statements of the
|>GLM procedure (I think it is equivalent to lm in R). They provided a lot
|>of tests: LSD, Duncan, Tukey-Kramer, Bonferroni, ScheffÃ©, SNK, etc.
|>However, in R I have only discovered Tukey-HSD.
|
|
| Package multcomp with the workhorse-function simint comes close to
what you
| want.
|
|
| Dieter
|
| __
-----BEGIN PGP SIGNATURE-----

iEYEARECAAYFAkMpo6UACgkQWtdQtNzjBl72rwCcCuw5qxD1BWsensDI71RzhNgL
MUcAnA0Iq4tfoKSr/ymIV1nEZHZijvLW
=pavA
-----END PGP SIGNATURE-----



From johanfaux at yahoo.com  Thu Sep 15 18:39:18 2005
From: johanfaux at yahoo.com (johan Faux)
Date: Thu, 15 Sep 2005 09:39:18 -0700 (PDT)
Subject: [R] what's the best way to save global variables?
Message-ID: <20050915163918.94165.qmail@web31411.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050915/5e4dd189/attachment.pl

From jost at cict.fr  Thu Sep 15 18:37:57 2005
From: jost at cict.fr (Christian Jost)
Date: Thu, 15 Sep 2005 18:37:57 +0200
Subject: [R] Rcommander and simple chisquare
In-Reply-To: <20050915153113.SCXX26102.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <20050915153113.SCXX26102.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <a06002010bf4f52237190@[130.120.104.141]>

Dear John and Philippe,

thanks for your replys, I finally  found this 
menu, but I am somewhat at a loss how I should 
enter the observed frequencies. To take my 
example below, If I enter a one-column data.frame 
with the numbers 61 and 39, John's indicated menu 
is not highlighted. If I add a second column 
containing some factor, the menu is highlighted 
by I cannot select the first column. However, if 
I edit the data and declare the first column to 
be of type 'character' I can select it in the 
menu dialog and declare the expected frequencies, 
but the chisquare output doesn't make any sense. 
For the moment I cannot make any sense of that 
:-( Any help most appreciated, or a link to the 
tutorial/faq that explains such kind of problems.

Thanks, Christian.

At 11:31 -0400 15/09/05, John Fox wrote:
>Dear Philippe,
>
>This does a chi-square test of independence in a contingency table, not a
>chi-square goodness-of-fit test (which is done in the Rcmdr via Statistics
>-> Summaries -> Frequency distribution).
>
>Regards,
>  John
>
>--------------------------------
>John Fox
>Department of Sociology
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>905-525-9140x23604
>http://socserv.mcmaster.ca/jfox
>--------------------------------
>
>>  -----Original Message-----
>>  From: r-help-bounces at stat.math.ethz.ch
>>  [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
>>  Philippe Grosjean
>>  Sent: Thursday, September 15, 2005 7:32 AM
>>  To: Christian Jost
>>  Cc: r-help at stat.math.ethz.ch
>>  Subject: Re: [R] Rcommander and simple chisquare
>>
>>  Hello,
>>
>>  Just look at Statistics -> Contingency tables. There is an
>>  option for making the chi square test there.
>>  Best,
>>
>>  Philippe Grosjean,
>>
>>  ..............................................<??}))><........
>>    ) ) ) ) )
>  > ( ( ( ( (    Prof. Philippe Grosjean
>  > ..............................................................
>>
>>  Christian Jost wrote:
>>  > In this years biostat teaching I will include Rcommander (it indeed
>>  > simplifies syntax problems that makes students frequently miss the
>>  > core statistical problems). But I could not find how to
>>  make a simple
>>  > chisquare comparison between observed frequencies and expected
>>  > frequencies (eg in genetics where you expect phenotypic frequencies
>>  > corresponding to 3:1 in standard dominant/recessif
>>  alleles). Any idea
>>  > where this feature might be hidden? Or could it be added to
>>  > Rcommander?
>>  >
>>  > Thanks, Christian.
>>  >
>>  > ps: in case I am not making myself clear, can Rcommander be made to
>>  > perform
>>  >
>>  >> chisq.test(c(61,39),p=c(0.75,0.25))
>>  >
>>  >
>  > > ______________________________________________



From andy_liaw at merck.com  Thu Sep 15 18:43:15 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 15 Sep 2005 12:43:15 -0400
Subject: [R] means comparison in R (post-hoc test)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED438@usctmx1106.merck.com>

Do RSiteSearch("lsmeans") and go from there.

Andy

> From: Felipe
> 
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Thank you, I think multcomp is very near to what I was looking for.
> However, I am still looking for a mean to obtain least-squares
> (adjusted) means and std. errors of these means, and performing
> comparisons among these means, as the LSMEANS do in SAS. I have read
> other messages and have looked at car, effects and Design 
> manuals, but I
> am not sure if this is what I am looking for. Any clue?
> 
> Felipe
> 
> Dieter Menne wrote:
> | Felipe <felipe <at> unileon.es> writes:
> |
> |
> |>With the SAS/STAT, I generally used the MEANS (for comparison of
> |>arithmetic means) and the LSMEANS (for adjusted means) 
> statements of the
> |>GLM procedure (I think it is equivalent to lm in R). They 
> provided a lot
> |>of tests: LSD, Duncan, Tukey-Kramer, Bonferroni, Scheff??, SNK, etc.
> |>However, in R I have only discovered Tukey-HSD.
> |
> |
> | Package multcomp with the workhorse-function simint comes close to
> what you
> | want.
> |
> |
> | Dieter
> |
> | __
> -----BEGIN PGP SIGNATURE-----
> 
> iEYEARECAAYFAkMpo6UACgkQWtdQtNzjBl72rwCcCuw5qxD1BWsensDI71RzhNgL
> MUcAnA0Iq4tfoKSr/ymIV1nEZHZijvLW
> =pavA
> -----END PGP SIGNATURE-----
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jfox at mcmaster.ca  Thu Sep 15 18:59:11 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 15 Sep 2005 12:59:11 -0400
Subject: [R] Rcommander and simple chisquare
In-Reply-To: <a06002010bf4f52237190@[130.120.104.141]>
Message-ID: <20050915165911.SZRC1799.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Christian,

The Rcmdr assumes that you have a data frame with the original data, in
which the variable in question is a factor. The frequency distribution is
constructed for the factor. Thus, in your example, you'd have 100
observations classified on a two-category factor. What you enter directly
are the hypothesized probabilities.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Christian Jost [mailto:jost at cict.fr] 
> Sent: Thursday, September 15, 2005 11:38 AM
> To: John Fox; 'Philippe Grosjean'
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] Rcommander and simple chisquare
> 
> Dear John and Philippe,
> 
> thanks for your replys, I finally  found this menu, but I am 
> somewhat at a loss how I should enter the observed 
> frequencies. To take my example below, If I enter a 
> one-column data.frame with the numbers 61 and 39, John's 
> indicated menu is not highlighted. If I add a second column 
> containing some factor, the menu is highlighted by I cannot 
> select the first column. However, if I edit the data and 
> declare the first column to be of type 'character' I can 
> select it in the menu dialog and declare the expected 
> frequencies, but the chisquare output doesn't make any sense. 
> For the moment I cannot make any sense of that :-( Any help 
> most appreciated, or a link to the tutorial/faq that explains 
> such kind of problems.
> 
> Thanks, Christian.
> 
> At 11:31 -0400 15/09/05, John Fox wrote:
> >Dear Philippe,
> >
> >This does a chi-square test of independence in a contingency 
> table, not 
> >a chi-square goodness-of-fit test (which is done in the Rcmdr via 
> >Statistics
> >-> Summaries -> Frequency distribution).
> >
> >Regards,
> >  John
> >
> >--------------------------------
> >John Fox
> >Department of Sociology
> >McMaster University
> >Hamilton, Ontario
> >Canada L8S 4M4
> >905-525-9140x23604
> >http://socserv.mcmaster.ca/jfox
> >--------------------------------
> >
> >>  -----Original Message-----
> >>  From: r-help-bounces at stat.math.ethz.ch  
> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of  Philippe 
> >> Grosjean
> >>  Sent: Thursday, September 15, 2005 7:32 AM
> >>  To: Christian Jost
> >>  Cc: r-help at stat.math.ethz.ch
> >>  Subject: Re: [R] Rcommander and simple chisquare
> >>
> >>  Hello,
> >>
> >>  Just look at Statistics -> Contingency tables. There is 
> an  option 
> >> for making the chi square test there.
> >>  Best,
> >>
> >>  Philippe Grosjean,
> >>
> >>  ..............................................<??}))><........
> >>    ) ) ) ) )
> >  > ( ( ( ( (    Prof. Philippe Grosjean
> >  > ..............................................................
> >>
> >>  Christian Jost wrote:
> >>  > In this years biostat teaching I will include Rcommander (it 
> >> indeed  > simplifies syntax problems that makes students 
> frequently 
> >> miss the  > core statistical problems). But I could not 
> find how to  
> >> make a simple  > chisquare comparison between observed frequencies 
> >> and expected  > frequencies (eg in genetics where you expect 
> >> phenotypic frequencies  > corresponding to 3:1 in standard 
> >> dominant/recessif  alleles). Any idea  > where this 
> feature might be 
> >> hidden? Or could it be added to  > Rcommander?
> >>  >
> >>  > Thanks, Christian.
> >>  >
> >>  > ps: in case I am not making myself clear, can 
> Rcommander be made 
> >> to  > perform  >  >> chisq.test(c(61,39),p=c(0.75,0.25))
> >>  >
> >>  >
> >  > > ______________________________________________
>



From palvarez7777 at yahoo.es  Thu Sep 15 18:59:40 2005
From: palvarez7777 at yahoo.es (Alvarez Pedro)
Date: Thu, 15 Sep 2005 18:59:40 +0200 (CEST)
Subject: [R] How to label a plot of a tree dendrogram with text
Message-ID: <20050915165940.59294.qmail@web25201.mail.ukl.yahoo.com>

Dear R list,

I have generated a object of class rpart in R 2.1.1
(using the wrapper function "mvpart" of the
mvpart-package version 1.0-1). In order to label the
plot of the tree dendrogram with text I used
"text.rpart". In this function it is possible to set
the argument "label" to determine which values will
label the nodes. In the case of my tree-object I have
the following possibilities to chose the
label-argument:

> prunedtree2$frame[1:1,]
  var    n   wt       dev     yval complexity ncompete
nsurrogate   yval2.1   yval2.2   yval2.3   yval2.4
1 EST 4258 4258 377874512 113.6452  0.2399082        4
         1 230.56740 118.06599  78.31141  27.63598

In order to produce labels with variable "n" I
entered:

> plot(prunedtree2)
> text.rpart(prunedtree2,label="n")

Although I set label="n", it always produces labels
with variable "dev". I tried also to set label="yval",
label="yval2.1", ... without any success, everytime it
produces labels with "dev". What am I doing wrong?

Thanks, Pedro.



From tom_colson at ncsu.edu  Thu Sep 15 20:08:56 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Thu, 15 Sep 2005 14:08:56 -0400
Subject: [R] Error in vector("double",
	length) : vector size specified is too large....VLDs
Message-ID: <200509151808.j8FI8qoj000525@uni08mr.unity.ncsu.edu>

I have what R seems to consider a very large dataset, a 12MB text file of
lat,long,and height values, 130,000 rows to be exact. 

Here's what I get:


Thomas Colson
North Carolina State University
Department of Forestry and Environmental Resources
(919) 673 8023
tom_colson at ncsu.edu

Calendar:
www4.ncsu.edu/~tpcolson



From tom_colson at ncsu.edu  Thu Sep 15 20:11:23 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Thu, 15 Sep 2005 14:11:23 -0400
Subject: [R] Error in vector("double",
	length) : vector size specified is too large....VLDs
Message-ID: <200509151811.j8FIBJco001013@uni08mr.unity.ncsu.edu>

I have what R seems to consider a very large dataset, a 12MB text file of
lat,long,and height values, 130,000 rows to be exact. 

Here's what I get:
> data1 <- data.frame(read.table("BE3720078500WC20020828.txt",sep=",",
header=T))
> raw.data <- as.geodata(data1)
> variog.1.b <- variog(raw.data)
variog: computing omnidirectional variogram
Error in vector("double", length) : vector size specified is too large
> round(memory.limit()/1048576.0, 2)
[1] 4000



The "Vector size specified is too large" seems to be a common error, but I
haven't seen any workarounds posted...and the help.archive web site seems to
be down. I can plot the dataset, do some elementary stats on it...no
variogram though. 


Any ideas on how to compute variograms on datasets with 100 to 300k points? 
Thanks 

Thomas Colson
North Carolina State University
Department of Forestry and Environmental Resources
(919) 673 8023
tom_colson at ncsu.edu

Calendar:
www4.ncsu.edu/~tpcolson



From tom_colson at ncsu.edu  Thu Sep 15 20:38:57 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Thu, 15 Sep 2005 14:38:57 -0400
Subject: [R] Error in vector("double",
	length) : vector size specified is too large....VLDs
In-Reply-To: <200509151833.j8FIXeJS019490@hertz.gene.com>
Message-ID: <200509151838.j8FIcrij006238@uni08mr.unity.ncsu.edu>

 
At 4 GB, I'm at the 32bit windows limit.....

Thomas Colson
North Carolina State University
Department of Forestry and Environmental Resources
(919) 673 8023
tom_colson at ncsu.edu

Calendar:
www4.ncsu.edu/~tpcolson
 

-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com] 
Sent: Thursday, September 15, 2005 2:34 PM
To: 'Tom Colson'
Subject: RE: [R] Error in vector("double",length) : vector size specified is
too large....VLDs

> 
> Any ideas on how to compute variograms on datasets with 100 to 300k 
> points?
> Thanks

Get more memory? ... it's cheap! :-)

-- Bert Gunter
Genentech



From tom_colson at ncsu.edu  Thu Sep 15 20:53:33 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Thu, 15 Sep 2005 14:53:33 -0400
Subject: [R] Error in vector("double",
	length) : vector size specified is too large....VLDs
In-Reply-To: <Pine.LNX.4.61.0509151121100.18165@echidna.fhcrc.org>
Message-ID: <200509151853.j8FIrT1H009135@uni08mr.unity.ncsu.edu>

 
> rm(data1)
> variog.1.b <- variog(raw.data)
variog: computing omnidirectional variogram
Error in vector("double", length) : vector size specified is too large

Turns out I was wrong re: # of rows...it's 304,000


Same problem. Version is 2.1.1, hardware is Dual Xeon 3.6 4 GB RAM, XP Pro
64 Bit. Can reproduce the problem with 64Bit R 2.1.1 running on Fedora 4,
same hardware. 



Thomas Colson
North Carolina State University
Department of Forestry and Environmental Resources
(919) 673 8023
tom_colson at ncsu.edu

Calendar:
www4.ncsu.edu/~tpcolson
 

-----Original Message-----
From: Douglas Grove [mailto:dgrove at fhcrc.org] 
Sent: Thursday, September 15, 2005 2:23 PM
To: Tom Colson
Subject: Re: [R] Error in vector("double", length) : vector size specified
is too large....VLDs

Well you could start by removing large objects that you aren't using (e.g.
'data1') and seeing if that helps. 

There may be other suggestions but you haven't told us what platform you're
working on, as the posting guide requests:

> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

Doug


On Thu, 15 Sep 2005, Tom Colson wrote:

> I have what R seems to consider a very large dataset, a 12MB text file 
> of lat,long,and height values, 130,000 rows to be exact.
> 
> Here's what I get:
> > data1 <- data.frame(read.table("BE3720078500WC20020828.txt",sep=",",
> header=T))
> > raw.data <- as.geodata(data1)
> > variog.1.b <- variog(raw.data)
> variog: computing omnidirectional variogram Error in vector("double", 
> length) : vector size specified is too large
> > round(memory.limit()/1048576.0, 2)
> [1] 4000
> 
> 
> 
> The "Vector size specified is too large" seems to be a common error, 
> but I haven't seen any workarounds posted...and the help.archive web 
> site seems to be down. I can plot the dataset, do some elementary 
> stats on it...no variogram though.
> 
> 
> Any ideas on how to compute variograms on datasets with 100 to 300k
points? 
> Thanks
> 
> Thomas Colson
> North Carolina State University
> Department of Forestry and Environmental Resources
> (919) 673 8023
> tom_colson at ncsu.edu
> 
> Calendar:
> www4.ncsu.edu/~tpcolson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Thu Sep 15 21:08:02 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Sep 2005 21:08:02 +0200
Subject: [R] Error in vector("double",
	length) : vector size specified is too large....VLDs
In-Reply-To: <200509151853.j8FIrT1H009135@uni08mr.unity.ncsu.edu>
References: <200509151853.j8FIrT1H009135@uni08mr.unity.ncsu.edu>
Message-ID: <x2zmqeup2l.fsf@turmalin.kubism.ku.dk>

"Tom Colson" <tom_colson at ncsu.edu> writes:

>  
> > rm(data1)
> > variog.1.b <- variog(raw.data)
> variog: computing omnidirectional variogram
> Error in vector("double", length) : vector size specified is too large
> 
> Turns out I was wrong re: # of rows...it's 304,000
> 
> 
> Same problem. Version is 2.1.1, hardware is Dual Xeon 3.6 4 GB RAM, XP Pro
> 64 Bit. Can reproduce the problem with 64Bit R 2.1.1 running on Fedora 4,
> same hardware. 
> 

Variograms involve the differences between all pairs of points which
can become a rather large number of values. 304000*303999/2 in your
case, about 344GB by my reckoning. And the distances between them
makes for a similar quantity.

Now, some algorithms may be smarter than to keep all values in memory,
but you haven't even told us where you got the variog() from. It
doesn't seem to be in the standard packages, although we do have
variogram() and Variogram() in spatial and nlme.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From cgwiita at ucla.edu  Thu Sep 15 21:14:03 2005
From: cgwiita at ucla.edu (Chris Wiita)
Date: Thu, 15 Sep 2005 12:14:03 -0700
Subject: [R] Copying from graphics window in OS X
Message-ID: <4329C7FB.2070408@ucla.edu>

I'm running R from an Xterm window is OSX-Tiger.  Graphical windows 
appear as they should, but I'm having trouble copying from them--using 
cmd+c or the Copy option in the Edit menu won't place the graph in the 
clipboard (when I paste into a running OS X app, I get whatever was the 
last copied thing from a non-x11 window).  Any ideas on how to copy from 
a xterm-launched graphical window?  I can copy/paste into and out of the 
xterm command line, but I can't get anything from a graphical window.

Thanks!



From sdavis2 at mail.nih.gov  Thu Sep 15 21:21:09 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 15 Sep 2005 15:21:09 -0400
Subject: [R] Copying from graphics window in OS X
In-Reply-To: <4329C7FB.2070408@ucla.edu>
Message-ID: <BF4F41E5.EA58%sdavis2@mail.nih.gov>


On 9/15/05 3:14 PM, "Chris Wiita" <cgwiita at ucla.edu> wrote:

> I'm running R from an Xterm window is OSX-Tiger.  Graphical windows
> appear as they should, but I'm having trouble copying from them--using
> cmd+c or the Copy option in the Edit menu won't place the graph in the
> clipboard (when I paste into a running OS X app, I get whatever was the
> last copied thing from a non-x11 window).  Any ideas on how to copy from
> a xterm-launched graphical window?  I can copy/paste into and out of the
> xterm command line, but I can't get anything from a graphical window.

I don't think it is possible, but I would love to be corrected.  You can
simple make a png, pdf, etc. if you want to save the graphic.

Sean



From Roger.Bivand at nhh.no  Thu Sep 15 21:28:20 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 15 Sep 2005 21:28:20 +0200 (CEST)
Subject: [R] Error in vector("double",
 length) : vector size specified is too large....VLDs
In-Reply-To: <x2zmqeup2l.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.44.0509152125250.3778-100000@reclus.nhh.no>

On 15 Sep 2005, Peter Dalgaard wrote:

> "Tom Colson" <tom_colson at ncsu.edu> writes:
> 
> >  
> > > rm(data1)
> > > variog.1.b <- variog(raw.data)
> > variog: computing omnidirectional variogram
> > Error in vector("double", length) : vector size specified is too large
> > 
> > Turns out I was wrong re: # of rows...it's 304,000
> > 
> > 
> > Same problem. Version is 2.1.1, hardware is Dual Xeon 3.6 4 GB RAM, XP Pro
> > 64 Bit. Can reproduce the problem with 64Bit R 2.1.1 running on Fedora 4,
> > same hardware. 
> > 
> 
> Variograms involve the differences between all pairs of points which
> can become a rather large number of values. 304000*303999/2 in your
> case, about 344GB by my reckoning. And the distances between them
> makes for a similar quantity.
> 
> Now, some algorithms may be smarter than to keep all values in memory,
> but you haven't even told us where you got the variog() from. It
> doesn't seem to be in the standard packages, although we do have
> variogram() and Variogram() in spatial and nlme.

Right, this is from geoR, which uses full matrices. I think both fields 
and gstat can work with larger data sets. Whether model-based 
geostatistics is what you need for interpolating a digital elevation model 
is another question.

> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From cgwiita at ucla.edu  Thu Sep 15 21:49:43 2005
From: cgwiita at ucla.edu (Chris Wiita)
Date: Thu, 15 Sep 2005 12:49:43 -0700
Subject: [R] Copying from graphics window in OS X
In-Reply-To: <BF4F41E5.EA58%sdavis2@mail.nih.gov>
References: <BF4F41E5.EA58%sdavis2@mail.nih.gov>
Message-ID: <4329D057.3080107@ucla.edu>

I hope there is a way...at the moment an os X screen grab is the only 
way to get a quick copy. When I ran Cygwin on a PC, copying from graphic 
windows was as easy as ctrl+c--so it doesn't sound like an X11 
limitation.  I'd like to know what Cygwin was doing in the background...

Sean Davis wrote:

>On 9/15/05 3:14 PM, "Chris Wiita" <cgwiita at ucla.edu> wrote:
>
>  
>
>>I'm running R from an Xterm window is OSX-Tiger.  Graphical windows
>>appear as they should, but I'm having trouble copying from them--using
>>cmd+c or the Copy option in the Edit menu won't place the graph in the
>>clipboard (when I paste into a running OS X app, I get whatever was the
>>last copied thing from a non-x11 window).  Any ideas on how to copy from
>>a xterm-launched graphical window?  I can copy/paste into and out of the
>>xterm command line, but I can't get anything from a graphical window.
>>    
>>
>
>I don't think it is possible, but I would love to be corrected.  You can
>simple make a png, pdf, etc. if you want to save the graphic.
>
>Sean
>
>
>  
>



From ssrnmail at gmail.com  Thu Sep 15 22:08:59 2005
From: ssrnmail at gmail.com (Pablo Gonzalez)
Date: Thu, 15 Sep 2005 21:08:59 +0100
Subject: [R] Coefficients from LM
In-Reply-To: <mailman.9.1126778401.29836.r-help@stat.math.ethz.ch>
Message-ID: <4329d4e1.4864c655.2f19.7572@mx.gmail.com>

Hi everyone,

Can anyone tell me if its possibility to extract the coefficients from the
lm() command?
For instance, imagine that we have the following data set (the number of
observations for each company is actually larger than the one showed...):

Company	Y	X1	X2
1		y_1	x1_1	x2_1
1		y_2	x1_2	x2_2
1		y_3	x1_3	x2_3
(...)
2		y_4	x1_4	x2_4
2		y_5	x1_5	x2_5	
2		y_6	x1_6	x2_6
(...)
n		y_n	x1_n	x2_n
n		y_n1	x1_n1	x2_n1
n		y_n2	x1_n2	x2_n2
(...)

I need to run a regression of Y=b0+b1*X1+b2*X2 for EACH company in the
dataset and then retrieve the coefficients for each regression obtained (and
t-stats and R^2) for each company and put it in another dataset/table. The
procedure can be done easily done with a loop statement, but i need to
retrieve each individual coefficient, t-stat, R^2, etc... I know that, using
the $coefficients command will return the vector of coeffcients but I'm
having trouble to assignt it to the correct row in the final dataset.
Furthermore, I can't find any way of retrieving the R^2 and t-stats...

Thanks for any help,

Pablo.



From dbcfmp at unileon.es  Thu Sep 15 22:13:26 2005
From: dbcfmp at unileon.es (Felipe)
Date: Thu, 15 Sep 2005 22:13:26 +0200
Subject: [R] Copying from graphics window in OS X
In-Reply-To: <4329C7FB.2070408@ucla.edu>
References: <4329C7FB.2070408@ucla.edu>
Message-ID: <4329D5E6.90704@unileon.es>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

I'm using R for Mac OS X Aqua GUI. It allows me to copy graphs directly,
simply with command-c.
It's pretty nice, you can give it a try.
Greetings.
Felipe

Chris Wiita wrote:
| I'm running R from an Xterm window is OSX-Tiger.  Graphical windows
| appear as they should, but I'm having trouble copying from them--using
| cmd+c or the Copy option in the Edit menu won't place the graph in the
| clipboard (when I paste into a running OS X app, I get whatever was the
| last copied thing from a non-x11 window).  Any ideas on how to copy from
| a xterm-launched graphical window?  I can copy/paste into and out of the
| xterm command line, but I can't get anything from a graphical window.
|
| Thanks!
|
| ______________________________________________
| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
|
|
-----BEGIN PGP SIGNATURE-----

iEYEARECAAYFAkMp1eYACgkQWtdQtNzjBl59JgCfV7JR8kbsbYvMQG6OVt/plNTd
1S8AnRTHe6cRrwr0mxbJJGXmdkRibDvV
=Pc3t
-----END PGP SIGNATURE-----



From cobleigh at gmail.com  Thu Sep 15 22:15:12 2005
From: cobleigh at gmail.com (Jamieson Cobleigh)
Date: Thu, 15 Sep 2005 16:15:12 -0400
Subject: [R] Copying from graphics window in OS X
In-Reply-To: <4329C7FB.2070408@ucla.edu>
References: <4329C7FB.2070408@ucla.edu>
Message-ID: <7f50836c05091513155f82cb8@mail.gmail.com>

You can try FreeSnap, a screen capture program for OS X:
    http://www.efritz.net/software.html

Also, why are you running R from an XTerm?  There is an OS X native
version of R that might be better integrated with OS X for doing
screen captures:
   http://cran.stat.ucla.edu/bin/macosx/

Jamie

On 9/15/05, Chris Wiita <cgwiita at ucla.edu> wrote:
> I'm running R from an Xterm window is OSX-Tiger.  Graphical windows
> appear as they should, but I'm having trouble copying from them--using
> cmd+c or the Copy option in the Edit menu won't place the graph in the
> clipboard (when I paste into a running OS X app, I get whatever was the
> last copied thing from a non-x11 window).  Any ideas on how to copy from
> a xterm-launched graphical window?  I can copy/paste into and out of the
> xterm command line, but I can't get anything from a graphical window.
> 
> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tom_colson at ncsu.edu  Thu Sep 15 22:15:55 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Thu, 15 Sep 2005 16:15:55 -0400
Subject: [R] Error in vector("double",
	length) : vector size specified is too large....VLDs
In-Reply-To: <Pine.LNX.4.44.0509152125250.3778-100000@reclus.nhh.no>
Message-ID: <200509152015.j8FKFp4r020633@uni01mr.unity.ncsu.edu>

Yes, using geoR. 

I can interpolate the DEM quite easily in Grass (v.surf.rst, kriging) and
block kriging in ArcInfo. What we need, though, is to be able to "estimate"
or even nail down the variogram for these data sets. Where am I going with
this? I'm guessing that variables such as slope, ruggedness, etc.. are going
to produce different sill, range, and nugget values, which I can then use to
fine tune the interpolation process, rather than using the same spline or
kriging parameters on say, a whole state boundary worth of Lidar data.  And
yes, I can estimate the variogram in ArcInfo (limited to 10000 points) and
can also import the DEM from grass into R using spgrass....but the point is
to analyze the point data BEFORE I make the DEM. 

So I'm guessing the geoR isn't ever going to handle this size data, and I
need to be using gstat? (As I write this, gstat(variogram) is plugging away
for last 10 minute with no errors.....)

Thanks for quick replies


Thomas Colson
North Carolina State University
Department of Forestry and Environmental Resources
(919) 673 8023
tom_colson at ncsu.edu

Calendar:
www4.ncsu.edu/~tpcolson
 

-----Original Message-----
From: Roger Bivand [mailto:Roger.Bivand at nhh.no] 
Sent: Thursday, September 15, 2005 3:28 PM
To: Peter Dalgaard
Cc: Tom Colson; r-help at stat.math.ethz.ch
Subject: Re: [R] Error in vector("double", length) : vector size specified
is too large....VLDs

On 15 Sep 2005, Peter Dalgaard wrote:

> "Tom Colson" <tom_colson at ncsu.edu> writes:
> 
> >  
> > > rm(data1)
> > > variog.1.b <- variog(raw.data)
> > variog: computing omnidirectional variogram Error in 
> > vector("double", length) : vector size specified is too large
> > 
> > Turns out I was wrong re: # of rows...it's 304,000
> > 
> > 
> > Same problem. Version is 2.1.1, hardware is Dual Xeon 3.6 4 GB RAM, 
> > XP Pro
> > 64 Bit. Can reproduce the problem with 64Bit R 2.1.1 running on 
> > Fedora 4, same hardware.
> > 
> 
> Variograms involve the differences between all pairs of points which 
> can become a rather large number of values. 304000*303999/2 in your 
> case, about 344GB by my reckoning. And the distances between them 
> makes for a similar quantity.
> 
> Now, some algorithms may be smarter than to keep all values in memory, 
> but you haven't even told us where you got the variog() from. It 
> doesn't seem to be in the standard packages, although we do have
> variogram() and Variogram() in spatial and nlme.

Right, this is from geoR, which uses full matrices. I think both fields and
gstat can work with larger data sets. Whether model-based geostatistics is
what you need for interpolating a digital elevation model is another
question.

> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From cobleigh at gmail.com  Thu Sep 15 22:17:07 2005
From: cobleigh at gmail.com (Jamieson Cobleigh)
Date: Thu, 15 Sep 2005 16:17:07 -0400
Subject: [R] Copying from graphics window in OS X
In-Reply-To: <7f50836c05091513155f82cb8@mail.gmail.com>
References: <4329C7FB.2070408@ucla.edu>
	<7f50836c05091513155f82cb8@mail.gmail.com>
Message-ID: <7f50836c050915131755e57308@mail.gmail.com>

I just tried using Ctrl-C to do a copy a plot from a graphic window
using the Cocoa version of OS X that you can download from the link
below and I was able to paste the plot into a document.

Jamie

On 9/15/05, Jamieson Cobleigh <cobleigh at gmail.com> wrote:

> Also, why are you running R from an XTerm?  There is an OS X native
> version of R that might be better integrated with OS X for doing
> screen captures:
>    http://cran.stat.ucla.edu/bin/macosx/



From cgwiita at ucla.edu  Thu Sep 15 22:41:41 2005
From: cgwiita at ucla.edu (Chris Wiita)
Date: Thu, 15 Sep 2005 13:41:41 -0700
Subject: [R] Copying from graphics window in OS X
In-Reply-To: <7f50836c05091513155f82cb8@mail.gmail.com>
References: <4329C7FB.2070408@ucla.edu>
	<7f50836c05091513155f82cb8@mail.gmail.com>
Message-ID: <4329DC85.40709@ucla.edu>

I'm connecting over SSH to a linux server to do calculations, so I need 
to be able to run the remote linux console on the Mac.

Jamieson Cobleigh wrote:

>You can try FreeSnap, a screen capture program for OS X:
>    http://www.efritz.net/software.html
>
>Also, why are you running R from an XTerm?  There is an OS X native
>version of R that might be better integrated with OS X for doing
>screen captures:
>   http://cran.stat.ucla.edu/bin/macosx/
>
>Jamie
>
>On 9/15/05, Chris Wiita <cgwiita at ucla.edu> wrote:
>  
>
>>I'm running R from an Xterm window is OSX-Tiger.  Graphical windows
>>appear as they should, but I'm having trouble copying from them--using
>>cmd+c or the Copy option in the Edit menu won't place the graph in the
>>clipboard (when I paste into a running OS X app, I get whatever was the
>>last copied thing from a non-x11 window).  Any ideas on how to copy from
>>a xterm-launched graphical window?  I can copy/paste into and out of the
>>xterm command line, but I can't get anything from a graphical window.
>>
>>Thanks!
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>  
>



From bitwrit at ozemail.com.au  Fri Sep 16 09:35:27 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 16 Sep 2005 07:35:27 +0000
Subject: [R] Converting coordinates to actual distances
In-Reply-To: <845AADAC1106E44996327D62097E4C6B6F5369@et.ad.sdsc.edu>
References: <845AADAC1106E44996327D62097E4C6B6F5369@et.ad.sdsc.edu>
Message-ID: <432A75BF.8010504@ozemail.com.au>

Paul Brewin wrote:
> Hello,
> 
> I've been searching for a method of converting Lat/Lon decimal
> coordinates into actual distances between points, and taking into
> account the curvature of the earth.  Is there such a package in R?  I've
> looked at the GeoR package, but this does not seem to contain what I am
> looking for.  Ideally the output would be a triangular matrix of
> distances.  

Hi Paul,

Below is an implementation of the Haversine formula that I once had to 
use for calculating distances between telephone exchanges. It's in C, 
but could be translated to R fairly easily. I've included a degrees to 
radians conversion as well.

Jim

#define APOSTROPHE 39
#define EARTH_RADIUS 6367

typedef struct {
  char *basename;
  char *number[2];
  double lat[2];
  double lon[2];
  char delimiter[4];
  char *data02;
  char *data03;
  char *data07;
  char *data08;
  FILE *input;
  FILE *output;
} PSTS_INFO;

/* DegreesToRadians
Converts a string of the form nnndnn'nn"[NSEW] to a value in radians.
East (E) and South (S) are arbitrarily negative.  Note that this routine
will generate garbage if not passed a string of the correct form, 
although it is unlikely to do too much damage.
         Return value    value of the string in radians
                         -10     input not correct format
*/

double DegreesToRadians(char *dms) {
  int index = 0;
  double degrees;
  double minutes = 0;
  double seconds = 0;
  int mult;

  degrees = atof(dms);
  while(isdigit(dms[index])) index++;
  if(dms[index++] == 'd') {
   minutes = atof(&dms[index]);
   minutes /= 60;
   while(isdigit(dms[index])) index++;
   if(dms[index++] == APOSTROPHE) {
    seconds = atof(&dms[index]);
    seconds /= 360;
    while(isdigit(dms[index]) && dms[index]) index++;
    if(dms[index++] == QUOTES) {
     mult = (dms[index] == 'S' || dms[index] == 'E') ? -1 : 1;
     degrees += minutes + seconds;
     return(mult * degrees * M_PI/180);
    }
   }
  }
  return(-10);
}

/* GreatCircleDistance
(Formula and recommendations for calculation taken from:
http://www.census.gov/cgi-bin/geo/gisfaq?Q5.1
by Bob Chamberlain - rgc at solstice.jpl.nasa.gov)
Calculates 'great circle' distances using the Haversine formula,
based upon a spherical earth of EARTH_RADIUS radius.
This version extracts the two latitude/longitude pairs (as radians)
from the PSTS_INFO structure.
*/

double GreatCircleDist(PSTS_INFO *psts_info) {
  double a,d,dlon,dlat;

  dlon = psts_info->lon[1] - psts_info->lon[0];
  dlat = psts_info->lat[1] - psts_info->lat[0];
  a = pow(sin(dlat / 2),2) + cos(psts_info->lat[0]) *
   cos(psts_info->lat[1]) * pow(sin(dlon / 2),2);
  /* The penultimate result may be calculated either way - the first
  is bulletproof, the second is a bit faster */
  d = 2 * atan2(sqrt(a),sqrt(1 - a));
/* d = 2 * asin(sqrt(a));*/
  return(d * EARTH_RADIUS);
}



From Charles.Annis at StatisticalEngineering.com  Thu Sep 15 23:36:12 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 15 Sep 2005 17:36:12 -0400
Subject: [R] Coefficients from LM
In-Reply-To: <4329d4e1.4864c655.2f19.7572@mx.gmail.com>
Message-ID: <200509152136.j8FLaDsH027628@hypatia.math.ethz.ch>

Tsk, tsk.  You don't seem to be looking very hard.

Here's an example with a glm; lm() works the same way but has fewer internal
objects.

mod3 <- glm(tree ~ altitude, family = binomial)

You can use names() to find out what's inside:

> names(mod3)
 [1] "coefficients"      "residuals"         "fitted.values"     "effects"
"R"                
 [6] "rank"              "qr"                "family"
"linear.predictors" "deviance"         
[11] "aic"               "null.deviance"     "iter"              "weights"
"prior.weights"    
[16] "df.residual"       "df.null"           "y"                 "converged"
"boundary"         
[21] "model"             "call"              "formula"           "terms"
"data"             
[26] "offset"            "control"           "method"            "contrasts"
"xlevels"          

There are lots of ways to retrieve the parameter estimates:

> coefficients(mod3)
(Intercept)    altitude 
13.43360163 -0.01220884
 
> mod3$coeff
(Intercept)    altitude 
13.43360163 -0.01220884
 
> mod3$coef[2]
   altitude 
-0.01220884 

> as.numeric(mod3$coef[2])
[1] -0.01220884
>


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pablo Gonzalez
Sent: Thursday, September 15, 2005 4:09 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Coefficients from LM

Hi everyone,

Can anyone tell me if its possibility to extract the coefficients from the
lm() command?
For instance, imagine that we have the following data set (the number of
observations for each company is actually larger than the one showed...):

Company	Y	X1	X2
1		y_1	x1_1	x2_1
1		y_2	x1_2	x2_2
1		y_3	x1_3	x2_3
(...)
2		y_4	x1_4	x2_4
2		y_5	x1_5	x2_5	
2		y_6	x1_6	x2_6
(...)
n		y_n	x1_n	x2_n
n		y_n1	x1_n1	x2_n1
n		y_n2	x1_n2	x2_n2
(...)

I need to run a regression of Y=b0+b1*X1+b2*X2 for EACH company in the
dataset and then retrieve the coefficients for each regression obtained (and
t-stats and R^2) for each company and put it in another dataset/table. The
procedure can be done easily done with a loop statement, but i need to
retrieve each individual coefficient, t-stat, R^2, etc... I know that, using
the $coefficients command will return the vector of coeffcients but I'm
having trouble to assignt it to the correct row in the final dataset.
Furthermore, I can't find any way of retrieving the R^2 and t-stats...

Thanks for any help,

Pablo.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mceronm at agronica.udea.edu.co  Fri Sep 16 03:23:06 2005
From: mceronm at agronica.udea.edu.co (=?iso-8859-1?Q?Mario_Fernando_Cer=F3n_Mu=F1oz?=)
Date: Thu, 15 Sep 2005 20:23:06 -0500
Subject: [R]  Nolinear mixed-effects models (nlme)
Message-ID: <200509160120.j8G1KUqm013628@agronica.udea.edu.co>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050915/940f0856/attachment.pl

From weigand.stephen at charter.net  Fri Sep 16 05:27:52 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Thu, 15 Sep 2005 22:27:52 -0500
Subject: [R] about cutree
In-Reply-To: <20050915112612.CTU05362@mirapoint.uc.edu>
References: <20050915112612.CTU05362@mirapoint.uc.edu>
Message-ID: <87ee39ef840601b96ee1af3a0a9812c4@charter.net>

On Sep 15, 2005, at 10:26 AM, Baoqiang Cao wrote:

> Hi Everyone,
>
> I'm trying to use cutree to get the clusters after hclust. What I used 
> is: mycluster<-cutree(cnclust,h=0.5)
> Now, my problem is, how can I get the actual clusters? Thanks!
>
> Best,
>  Baoqiang Cao

Doesn't print(mycluster) give you the clusters? You could
use something like:

R> split(names(mycluster), mycluster)

You may need to better define "get the actual clusters" to get
a better answer.

Stephen



From weigand.stephen at charter.net  Fri Sep 16 05:41:20 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Thu, 15 Sep 2005 22:41:20 -0500
Subject: [R] what's the best way to save global variables?
In-Reply-To: <20050915163918.94165.qmail@web31411.mail.mud.yahoo.com>
References: <20050915163918.94165.qmail@web31411.mail.mud.yahoo.com>
Message-ID: <6852b00e52ecc0d1f8d659afc23da69a@charter.net>

Johan,

On Sep 15, 2005, at 11:39 AM, johan Faux wrote:

> I am writing a kind of long program in R and I have some variables 
> which I want to be globals. Where should I save them?  I was thinking 
> to create a function wich initialize all the global variables and then 
> whenever I need them, I call this function.

In most cases, you would write a function that would
return an object of class list, the components of
which would be the values you want to use later.
For example

myfun <- function([stuff]){

  [stuff]

  return(list = (var1 = val1, var2 = val2)
}

and your call would be

glob <- myfun([stuff])

and you would access val1 with

glob$var1


> What if I create a file glob.R with
> var1<-val1
> var2<-val2
> .....
> etc.
>
> How do I include this file in my other files/function . Is there in R 
> some kind of   include("glob.R") or something?
> thank you,
> Johan
>

Use

source("glob.R")

Check out "An introduction to R" 
(http://cran.r-project.org/doc/manuals/R-intro.html).

Stephen



From Fiona.Evans at csiro.au  Fri Sep 16 07:34:05 2005
From: Fiona.Evans at csiro.au (Fiona.Evans@csiro.au)
Date: Fri, 16 Sep 2005 15:34:05 +1000
Subject: [R] Searchable archives
Message-ID: <C8DFDC5896F19C49BC1DD5F2E0F56B38CB33F6@exqld2-bne.qld.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050916/2ee268eb/attachment.pl

From Murraypu at aimnsw.com.au  Fri Sep 16 07:43:13 2005
From: Murraypu at aimnsw.com.au (Murray Pung)
Date: Fri, 16 Sep 2005 15:43:13 +1000
Subject: [R] Searchable archives
Message-ID: <3028F4C4647C9043B870276E28C69FD6B56851@syd05.aimnsw.com.au>

Yes, I've had the same trouble. 

Robert may be able to sort this out.

-----Original Message-----
From: Fiona.Evans at csiro.au [mailto:Fiona.Evans at csiro.au]
Sent: Friday, 16 September 2005 3:34 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Searchable archives



I cannot access the searchable archives at
www.tolstoy.newcastle.au/~rking/R.
Does anyone else have this problem?

--
Fiona H. Evans
http://www.cmis.csiro.au/Fiona.Evans


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vincent at 7d4.com  Fri Sep 16 08:02:46 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Fri, 16 Sep 2005 08:02:46 +0200
Subject: [R] Copying from graphics window in OS X
In-Reply-To: <4329C7FB.2070408@ucla.edu>
References: <4329C7FB.2070408@ucla.edu>
Message-ID: <432A6006.3000707@7d4.com>

you may try :
bmp("myimage.bmp");
plot(...);
dev.off();
# png(), jpeg() also available
hih



From Sebastian.Leuzinger at unibas.ch  Fri Sep 16 08:22:11 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Fri, 16 Sep 2005 08:22:11 +0200
Subject: [R] significance of spectal peak with spectrum()
Message-ID: <200509160822.11737.Sebastian.Leuzinger@unibas.ch>

Hello, has anybody got a simple recepie to test the significance level of the 
peaks after using spectrum() ?

(R-version 2.0.1, linux SuSE9.3)
-- 
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From maechler at stat.math.ethz.ch  Fri Sep 16 08:40:49 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 16 Sep 2005 08:40:49 +0200
Subject: [R] Searchable mailing list archives -- not reachable
In-Reply-To: <3028F4C4647C9043B870276E28C69FD6B56851@syd05.aimnsw.com.au>
References: <3028F4C4647C9043B870276E28C69FD6B56851@syd05.aimnsw.com.au>
Message-ID: <17194.26865.242120.778616@stat.math.ethz.ch>

AFAIK, the correct URL --- as also used from CRAN's search page ---
is
    http://tolstoy.newcastle.edu.au/~rking/R/

However you are both correct that it is not reachable anymore;
It seems because it's been firewalled off the world :

 PING tolstoy.newcastle.edu.au (134.148.237.146) 56(84) bytes of data.
 From newcastle-atm.nswrno.net.au (203.15.123.42) icmp_seq=1 Packet filtered
 From newcastle-atm.nswrno.net.au (203.15.123.42) icmp_seq=14 Packet filtered
 From newcastle-atm.nswrno.net.au (203.15.123.42) icmp_seq=17 Packet filtered

Martin
                    
>>>>> "Murray" == Murray Pung <Murraypu at aimnsw.com.au>
>>>>>     on Fri, 16 Sep 2005 15:43:13 +1000 writes:

    Murray> Yes, I've had the same trouble.  Robert may be able
    Murray> to sort this out.

    Murray> -----Original Message----- From:
    Murray> Fiona.Evans at csiro.au [mailto:Fiona.Evans at csiro.au]
    Murray> Sent: Friday, 16 September 2005 3:34 PM To:
    Murray> r-help at stat.math.ethz.ch Subject: [R] Searchable
    Murray> archives

    > I cannot access the searchable archives at
    > www.tolstoy.newcastle.au/~rking/R.  Does anyone else
    > have this problem?

    > -- Fiona H. Evans
    > http://www.cmis.csiro.au/Fiona.Evans



From dieter.menne at menne-biomed.de  Fri Sep 16 09:19:09 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 16 Sep 2005 07:19:09 +0000 (UTC)
Subject: [R] Nolinear mixed-effects models (nlme)
References: <200509160120.j8G1KUqm013628@agronica.udea.edu.co>
Message-ID: <loom.20050916T091538-854@post.gmane.org>

Mario Fernando CerÃ³n MuÃ±oz <mceronm <at> agronica.udea.edu.co> writes:

> 
> Do you send information about lactation curve analyse with no linear mixed
> model, with fixed effects (herd, year season, parity) and random effects
> (cow)?.

There is little hope that you get much more information with such a diffuse 
question. For a starter, you may check chapter 5.3.4 in Pinheiro/Bates, where  
a similar problem is handled by linear methods.

Dieter



From ligges at statistik.uni-dortmund.de  Fri Sep 16 09:28:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 16 Sep 2005 09:28:17 +0200
Subject: [R] significance of spectal peak with spectrum()
In-Reply-To: <200509160822.11737.Sebastian.Leuzinger@unibas.ch>
References: <200509160822.11737.Sebastian.Leuzinger@unibas.ch>
Message-ID: <432A7411.7080709@statistik.uni-dortmund.de>

Sebastian Leuzinger wrote:

> Hello, has anybody got a simple recepie to test the significance level of the 
> peaks after using spectrum() ?

What is you null hypothesis?

- Kind of noise?
- One particular frequency is noisy or all noisy?
- ...

Uwe Ligges



> (R-version 2.0.1, linux SuSE9.3)



From keineantwortadresse at web.de  Fri Sep 16 09:30:25 2005
From: keineantwortadresse at web.de (Ihr WEB.DE-Kundenservice)
Date: Fri, 16 Sep 2005 09:30:25 +0200
Subject: [R] Ihre Anfrage: Mail System (keineantwortadresse@web.de)
Message-ID: <200509160730.j8G7UPxc015976@mailgate6.cinetic.de>


Sehr geehrte Damen und Herren,
 
diese E-Mail Adresse ist keine gueltige Antwortadresse von WEB.DE.
 
Anfragen, die Sie an diese E-Mail Adresse richten, werden nicht bearbeitet.
 
Weitere Informationen zu den Angeboten von WEB.DE und Antworten auf Fragen zu unseren Produkten 
finden Sie unter dem folgenden Link : http://www.kundenservice.web.de.
 
Mit freundlichen Gruessen
 
WEB.DE  Kundenservice
http://kundenservice.web.de/
 
Festnetz: 01212-82 00 000*
Handy: 0190-88 98 88*
* jeweils 1,86 Euro/Min. Mo-Fr 08-21 Uhr
                                   Sa-So 10-18 Uhr
 
 Fuer Lob, Kritik und Anregungen stehen wir Ihnen im Feedback-Bereich gerne  
 zur Verfuegung: zu FreeMail unter http://kundenservice.web.de/
 
 Alle relevanten Informationen zu FreeMail finden Sie auch unter
 http://hilfe.web.de/freemail/Hilfe/Inhalt/
 
Wir wuenschen Ihnen weiterhin viel Spass und gute Kommunikation mit WEB.DE.



From dieter.menne at menne-biomed.de  Fri Sep 16 09:21:12 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 16 Sep 2005 07:21:12 +0000 (UTC)
Subject: [R] Coefficients from LM
References: <4329d4e1.4864c655.2f19.7572@mx.gmail.com>
	<200509152136.j8FLaDsH027628@hypatia.math.ethz.ch>
Message-ID: <loom.20050916T092027-385@post.gmane.org>

Charles Annis, P.E. <Charles.Annis <at> StatisticalEngineering.com> writes:

> Here's an example with a glm; lm() works the same way but has fewer internal
> objects.
> 
> mod3 <- glm(tree ~ altitude, family = binomial)
> 
> You can use names() to find out what's inside:
> 
> > names(mod3)
>  [1] "coefficients"      "residuals"         "fitted.values"     "effects"

You get more information about the internals by using str(mod3) instead of names
(mod3).

Dieter



From dieter.menne at menne-biomed.de  Fri Sep 16 09:41:26 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 16 Sep 2005 07:41:26 +0000 (UTC)
Subject: [R] significance of spectal peak with spectrum()
References: <200509160822.11737.Sebastian.Leuzinger@unibas.ch>
Message-ID: <loom.20050916T093415-408@post.gmane.org>

Sebastian Leuzinger <Sebastian.Leuzinger <at> unibas.ch> writes:

> 
> Hello, has anybody got a simple recepie to test the significance level of the 
> peaks after using spectrum() ?
> 
Having worked in circadian rhythmic consultancy for a few years, I know how 
popular this question is and how difficult it is to tell people that there is 
no simple answer. It may be possible to find simple answers for astrophysical 
signals, where almost all background is white noise and long signals are 
available, but in biology the null-hypothesis is so badly defined that checking 
the pre-conditions is the most important part of the job.

See also http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33423.html.

This said, you may try package GeneTS where you can find some methods.

Dieter



From francoisromain at free.fr  Fri Sep 16 09:58:16 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 16 Sep 2005 09:58:16 +0200
Subject: [R] about cutree
In-Reply-To: <20050915112612.CTU05362@mirapoint.uc.edu>
References: <20050915112612.CTU05362@mirapoint.uc.edu>
Message-ID: <432A7B18.5020603@free.fr>

Le 15.09.2005 17:26, Baoqiang Cao a ??crit :

>Hi Everyone,
>
>I'm trying to use cutree to get the clusters after hclust. What I used is: mycluster<-cutree(cnclust,h=0.5)
>Now, my problem is, how can I get the actual clusters? Thanks!
>
>Best,
> Baoqiang Cao
>  
>
Hello,

If you mean getting the labels in the same order than presented by the 
dendrogram (( Why isn't it the default BTW )), try using cutree.order in 
package A2R here http://addictedtor.free.fr/packages/A2R/

Regards,

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From Sebastian.Leuzinger at unibas.ch  Fri Sep 16 10:00:42 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Fri, 16 Sep 2005 10:00:42 +0200
Subject: [R] significance of spectal peak with spectrum()
In-Reply-To: <432A7411.7080709@statistik.uni-dortmund.de>
References: <200509160822.11737.Sebastian.Leuzinger@unibas.ch>
	<432A7411.7080709@statistik.uni-dortmund.de>
Message-ID: <200509161000.42913.Sebastian.Leuzinger@unibas.ch>

the null hypothesis would be: one particular frequency peak is not 
significantly different from the background noise.

On Friday 16 September 2005 09:28, you wrote:
> Sebastian Leuzinger wrote:
> > Hello, has anybody got a simple recepie to test the significance level of
> > the peaks after using spectrum() ?
>
> What is you null hypothesis?
>
> - Kind of noise?
> - One particular frequency is noisy or all noisy?
> - ...
>
> Uwe Ligges
>
> > (R-version 2.0.1, linux SuSE9.3)



From alegarra at neiker.net  Fri Sep 16 10:15:44 2005
From: alegarra at neiker.net (Andres Legarra)
Date: Fri, 16 Sep 2005 10:15:44 +0200
Subject: [R] Nolinear mixed-effects models (nlme)
References: <loom.20050916T091538-854@post.gmane.org>
Message-ID: <001c01c5ba96$d66410b0$0802a8c0@iktlan.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050916/36951a23/attachment.pl

From ligges at statistik.uni-dortmund.de  Fri Sep 16 10:36:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 16 Sep 2005 10:36:18 +0200
Subject: [R] significance of spectal peak with spectrum()
In-Reply-To: <200509161000.42913.Sebastian.Leuzinger@unibas.ch>
References: <200509160822.11737.Sebastian.Leuzinger@unibas.ch>
	<432A7411.7080709@statistik.uni-dortmund.de>
	<200509161000.42913.Sebastian.Leuzinger@unibas.ch>
Message-ID: <432A8402.7080404@statistik.uni-dortmund.de>

Sebastian Leuzinger wrote:

> the null hypothesis would be: one particular frequency peak is not 
> significantly different from the background noise.

So you want to know, e.g., whether there is something going on at 1000 
Hz? This is difficult: If you are considering the periodogram to be a 
density, then you do not know the distribution of the value of a single 
frequency, because it depends on the stuff going on at other frequencies.

Second point is (and already asked): "Kind of [background] noise"?

The only really easy test is for the Null "signal is white noise", hence 
H1 is "at least one non-white-noisy frequency".

[If somebody knows a really good book or papers that cover other cases 
than the trivial one mentioned above, I am very interested to hear about 
them, BTW.]

If you have another kind of noise (such as blue or pink noise), things 
become even worse.

Uwe Ligges


> On Friday 16 September 2005 09:28, you wrote:
> 
>>Sebastian Leuzinger wrote:
>>
>>>Hello, has anybody got a simple recepie to test the significance level of
>>>the peaks after using spectrum() ?
>>
>>What is you null hypothesis?
>>
>>- Kind of noise?
>>- One particular frequency is noisy or all noisy?
>>- ...
>>
>>Uwe Ligges
>>
>>
>>>(R-version 2.0.1, linux SuSE9.3)



From christian.hoffmann at wsl.ch  Fri Sep 16 10:38:51 2005
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Fri, 16 Sep 2005 10:38:51 +0200
Subject: [R]  setkeys and Sweave
In-Reply-To: <20050914191934.GA3535@irwin.vpn.uni-freiburg.de>
References: <200509142043.18959.friedrich@hattendoerfer.de>
	<20050914191934.GA3535@irwin.vpn.uni-freiburg.de>
Message-ID: <432A849B.4030603@wsl.ch>

Hi there:

Using

\setkeys{Gin}{width=1.0\textwidth}
\setkeys{Gin}{height=10cm}
\setkeys{Gin}{height=0.8\textwidth}

all seem to work under R-2.1.1 under sparc, solaris2.9, but

\setkeys{Gin}{scale=0.3}
\setkeys{Gin}{angle=90}

do not work. I have not been able to find relevant information, googling 
on setkeys proved confusing, at best.

http://cnlart.web.cern.ch/cnlart/218/node85.html states:
---
  Setting key values globally

If you want to specify a global value for a set of keys, then you can 
use the \setkeys command defined in the keyval package (described below).

As an example let us consider the case where you would like your figure 
to be scaled to the width of the line. Then you could specify the following:

   \setkeys{Gin}{width=\linewidth}

The first argument Gin of the \setkeys command refers to the 
\includegraphics command. The result will be that all images include 
with this command (when the graphicx is loaded) will be set to the 
desired width inside the current group or environment.

In the similar way one could specify any of the possible arguments of 
the \rotatebox command by using the Grot specifier, e.g.,

   \setkeys{Grot}{origin=tc}
---

Question:

Which variants of \setkeys are there? Is there a list stating these?

Can several \setkeys{of the same sort} be used inside one document?

Thanks for pointers
Christian


-- 
Dr. Christian W. Hoffmann,
Swiss Federal Research Institute WSL
Mathematics + Statistical Computing
Zuercherstrasse 111
CH-8903 Birmensdorf, Switzerland

Tel +41-44-7392-277  (office)   -111(exchange)
Fax +41-44-7392-215  (fax)
christian.hoffmann at wsl.ch
http://www.wsl.ch/staff/christian.hoffmann



From Sebastian.Leuzinger at unibas.ch  Fri Sep 16 10:53:58 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Fri, 16 Sep 2005 10:53:58 +0200
Subject: [R] significance of spectal peak with spectrum()
In-Reply-To: <432A8402.7080404@statistik.uni-dortmund.de>
References: <200509160822.11737.Sebastian.Leuzinger@unibas.ch>
	<200509161000.42913.Sebastian.Leuzinger@unibas.ch>
	<432A8402.7080404@statistik.uni-dortmund.de>
Message-ID: <200509161053.58729.Sebastian.Leuzinger@unibas.ch>

thanks a lot. I am interested in the more complex case where the interest is 
about a specific frequency being significant, not "at least one frequency 
being significantly different from the backgrond white noise".

I have discussed this issue with very knowledgable people in the field who 
could not help me either. I would be interested in any references as well.


On Friday 16 September 2005 10:36, you wrote:
> Sebastian Leuzinger wrote:
> > the null hypothesis would be: one particular frequency peak is not
> > significantly different from the background noise.
>
> So you want to know, e.g., whether there is something going on at 1000
> Hz? This is difficult: If you are considering the periodogram to be a
> density, then you do not know the distribution of the value of a single
> frequency, because it depends on the stuff going on at other frequencies.
>
> Second point is (and already asked): "Kind of [background] noise"?
>
> The only really easy test is for the Null "signal is white noise", hence
> H1 is "at least one non-white-noisy frequency".
>
> [If somebody knows a really good book or papers that cover other cases
> than the trivial one mentioned above, I am very interested to hear about
> them, BTW.]
>
> If you have another kind of noise (such as blue or pink noise), things
> become even worse.
>
> Uwe Ligges
>
> > On Friday 16 September 2005 09:28, you wrote:
> >>Sebastian Leuzinger wrote:
> >>>Hello, has anybody got a simple recepie to test the significance level
> >>> of the peaks after using spectrum() ?
> >>
> >>What is you null hypothesis?
> >>
> >>- Kind of noise?
> >>- One particular frequency is noisy or all noisy?
> >>- ...
> >>
> >>Uwe Ligges
> >>
> >>>(R-version 2.0.1, linux SuSE9.3)

-- 
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From e.pebesma at geog.uu.nl  Fri Sep 16 11:27:26 2005
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Fri, 16 Sep 2005 11:27:26 +0200
Subject: [R]  Error in vector("double",
 length) : vector size specified is too large....VLDs
Message-ID: <432A8FFE.8030503@geog.uu.nl>

Tom,

please try to use the variogram function in package gstat;
it doesn't (try to) store all pairwise differences, but rather
accumulates them for distance intervals.

It will take a while to do this, and there is a chance that
you overflow the counter that keeps the number of point
pairs for each interval: 304000^2 > 2^32; it is stored as
a C long, so may work on a 64 bit architecture. Otherwise,
I'd suggest to sample your data set.

I'd be interested to hear whether you succeed (or not).
--
Edzer



From felipe at unileon.es  Fri Sep 16 11:31:09 2005
From: felipe at unileon.es (Felipe)
Date: Fri, 16 Sep 2005 11:31:09 +0200
Subject: [R] Problems compiling Hmisc
Message-ID: <432A90DD.5010201@unileon.es>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi.
I am trying to install Hmisc from source on MacOS 10.4.2, using the R
for Mac OS X Aqua GUI 1.12. I have installed g77(using Fink). I am using
the Package installer provided by the GUI, and this is the output I obtain:

* Installing *source* package 'Hmisc' ...
** libs
g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
gcc-3.3 -no-cpp-precomp
- -I/Library/Frameworks/R.framework/Resources/include
- -I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
g77   -fno-common  -g -O2 -c rcorr.f -o rcorr.o
g77   -fno-common  -g -O2 -c wclosest.f -o wclosest.o
gcc-3.3 -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o
Hmisc.so cidxcn.o cidxcp.o hoeffd.o jacklins.o largrec.o ranksort.o
rcorr.o wclosest.o  -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2
- -lg2c -lSystem -framework R

The downloaded packages are in
	/private/tmp/Rtmpl1pDLj/downloaded_packages
ld: can't locate file for: -lg2c
make: *** [Hmisc.so] Error 1
ERROR: compilation failed for package 'Hmisc'

It seems to be a problem with the linker, but no idea. Do I need to
install any library or another Fink package?

Thank you.

Felipe

- ----------------------------------oOo----------------------------------
Felipe Mart??nez-Pastor (BSc, PhD)
Animal Reproduction and Obstetrics
Veterinary Clinic Hospital
24071-Le??n (Spain)
Phone: 987 291 430 / 987 291 000 + 5203
Fax: 987 295 203
e-mail: dbcfmp at unileon.es

- ---
This message has been electronically signed using GPG, a free system to
sign and encrypt documents. If you want to learn more, visit
http://www.gnupg.org
-----BEGIN PGP SIGNATURE-----

iEYEARECAAYFAkMqkN0ACgkQWtdQtNzjBl5kFACfSBopAnm/S2PTEbSyZGhOt4q0
zeQAn3MajbnFXWWhVd7gT+fQ5fGE85Ro
=lssZ
-----END PGP SIGNATURE-----



From Hans-Georg.Krauthaeuser at E-Technik.Uni-Magdeburg.DE  Fri Sep 16 12:05:37 2005
From: Hans-Georg.Krauthaeuser at E-Technik.Uni-Magdeburg.DE (Dr. Hans Georg Krauthaeuser)
Date: Fri, 16 Sep 2005 12:05:37 +0200
Subject: [R] corr.test -- use a different null hypothesis
Message-ID: <432A98F1.2020101@et.uni-magdeburg.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

First of all, I'm a physicist and therefore I'm not much used to use
statistics. So, please forgive me if this is a FAQ or stupid, but I
failed to find the answer by myself.

I want to use corr.test to test for the correlation of two data sets
(actually I have a lot of data set and perform pairwise testing). But I
wanted to find sets where the correlation is less than a certain limit,
i.e. |r_exp| < r_0. As far as I can see corr.test is designed to work
only for the null hypothesis r_exp = 0. I would need H0 as |r_exp|>= r_0.

Is there any way to specify this null-hypothesis to corr.test ?
Any other idea how to test that hypothesis?

Regards
Hans Georg Krauthaeuser
- --
Otto-von-Guericke-Universitaet Magdeburg
IGET            |   fon: +49 391 67 12195
Postfach 4120   |   fax: +49 391 67 11236
39016 Magdeburg | email: hgk at et.Uni-Magdeburg.DE
Germany         |   www: www.uni-magdeburg.de/krauthae
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.1 (GNU/Linux)
Comment: Using GnuPG with Thunderbird - http://enigmail.mozdev.org

iD8DBQFDKpjxyoYnJ0CXgngRAgzZAJ0adkXej5BADbEu9tH9GzHg7pzzwQCggmvY
zha4F0DF/pDSk6Tx0z59m30=
=zxkJ
-----END PGP SIGNATURE-----



From ssk2031 at columbia.edu  Fri Sep 16 12:06:05 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Fri, 16 Sep 2005 06:06:05 -0400
Subject: [R] significance of spectal peak with spectrum()
In-Reply-To: <432A8402.7080404@statistik.uni-dortmund.de>
References: <200509160822.11737.Sebastian.Leuzinger@unibas.ch>	<432A7411.7080709@statistik.uni-dortmund.de>	<200509161000.42913.Sebastian.Leuzinger@unibas.ch>
	<432A8402.7080404@statistik.uni-dortmund.de>
Message-ID: <432A990D.5000300@columbia.edu>


I am very much a naive and interested beginner, so I am not at all sure 
if you will find this reference

http://snipurl.com/hq2j

interesting....

S.

Uwe Ligges wrote:
> Sebastian Leuzinger wrote:
> 
> 
>>the null hypothesis would be: one particular frequency peak is not 
>>significantly different from the background noise.
> 
> 
> So you want to know, e.g., whether there is something going on at 1000 
> Hz? This is difficult: If you are considering the periodogram to be a 
> density, then you do not know the distribution of the value of a single 
> frequency, because it depends on the stuff going on at other frequencies.
> 
> Second point is (and already asked): "Kind of [background] noise"?
> 
> The only really easy test is for the Null "signal is white noise", hence 
> H1 is "at least one non-white-noisy frequency".
> 
> [If somebody knows a really good book or papers that cover other cases 
> than the trivial one mentioned above, I am very interested to hear about 
> them, BTW.]
> 
> If you have another kind of noise (such as blue or pink noise), things 
> become even worse.
> 
> Uwe Ligges
> 
> 
> 
>>On Friday 16 September 2005 09:28, you wrote:
>>
>>
>>>Sebastian Leuzinger wrote:
>>>
>>>
>>>>Hello, has anybody got a simple recepie to test the significance level of
>>>>the peaks after using spectrum() ?
>>>
>>>What is you null hypothesis?
>>>
>>>- Kind of noise?
>>>- One particular frequency is noisy or all noisy?
>>>- ...
>>>
>>>Uwe Ligges
>>>
>>>
>>>
>>>>(R-version 2.0.1, linux SuSE9.3)
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Mohamad.Taliaa at gmx.at  Fri Sep 16 12:36:51 2005
From: Mohamad.Taliaa at gmx.at (Mohamad Taliaa)
Date: Fri, 16 Sep 2005 12:36:51 +0200 (MEST)
Subject: [R] (no subject)
Message-ID: <21393.1126867011@www54.gmx.net>

Dear Sirs!

My name is Mohamad TALIAA and I am supposed to be the BUDDY for Viola
Rossini at the Univeristy of Vienna!

The ERASMUS Bureau at Vienna just provided me with a wrong email address! I
reported to them the mistake but I just was told to look for it maybe on the
internet or change the extension at the end.

Now I just came to your site as i looked for Viola Rossini at google.at

I also tried to write to her to the email address at: viola_rossini at yahoo.it
but this also did not work.

Therefor I would really appreciate if you could provide me her correct email
address!

Thank you very much for your support!

Sincerely Yours

Mohamad TALIAA

PS: email addresses I already tried out: violarossini at hotmail.com ,
viola_rossini at yahoo.it , viola.rossini at hotmail.com(.it)(.net) ...

--



From dieter.menne at menne-biomed.de  Fri Sep 16 13:00:41 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 16 Sep 2005 11:00:41 +0000 (UTC)
Subject: [R] corr.test -- use a different null hypothesis
References: <432A98F1.2020101@et.uni-magdeburg.de>
Message-ID: <loom.20050916T125636-844@post.gmane.org>

Dr. Hans Georg Krauthaeuser <Hans-Georg.Krauthaeuser <at> E-Technik.Uni-
Magdeburg.DE> writes:

> 
> I want to use corr.test to test for the correlation of two data sets
> (actually I have a lot of data set and perform pairwise testing). But I
> wanted to find sets where the correlation is less than a certain limit,
> i.e. |r_exp| < r_0. As far as I can see corr.test is designed to work
> only for the null hypothesis r_exp = 0. I would need H0 as |r_exp|>= r_0.
> 

cor.test (sic!) for method="pearson" returns a confidence interval for r. You 
could use the upper limit of it for your test

Dieter



From kjetil at acelerate.com  Fri Sep 16 13:12:25 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 16 Sep 2005 07:12:25 -0400
Subject: [R] significance of spectal peak with spectrum()
In-Reply-To: <200509161053.58729.Sebastian.Leuzinger@unibas.ch>
References: <200509160822.11737.Sebastian.Leuzinger@unibas.ch>	<200509161000.42913.Sebastian.Leuzinger@unibas.ch>	<432A8402.7080404@statistik.uni-dortmund.de>
	<200509161053.58729.Sebastian.Leuzinger@unibas.ch>
Message-ID: <432AA899.9090003@acelerate.com>

Sebastian Leuzinger wrote:

>thanks a lot. I am interested in the more complex case where the interest is 
>about a specific frequency being significant, not "at least one frequency 
>being significantly different from the backgrond white noise".
>
>I have discussed this issue with very knowledgable people in the field who 
>could not help me either. I would be interested in any references as well.
>
>
>  
>

http://bayes.wustl.edu/

where you can download Bretthorst's book "Bayesian Spectrum Analysis and 
Parameter
Estimation"

Kjetil


>On Friday 16 September 2005 10:36, you wrote:
>  
>
>>Sebastian Leuzinger wrote:
>>    
>>
>>>the null hypothesis would be: one particular frequency peak is not
>>>significantly different from the background noise.
>>>      
>>>
>>So you want to know, e.g., whether there is something going on at 1000
>>Hz? This is difficult: If you are considering the periodogram to be a
>>density, then you do not know the distribution of the value of a single
>>frequency, because it depends on the stuff going on at other frequencies.
>>
>>Second point is (and already asked): "Kind of [background] noise"?
>>
>>The only really easy test is for the Null "signal is white noise", hence
>>H1 is "at least one non-white-noisy frequency".
>>
>>[If somebody knows a really good book or papers that cover other cases
>>than the trivial one mentioned above, I am very interested to hear about
>>them, BTW.]
>>
>>If you have another kind of noise (such as blue or pink noise), things
>>become even worse.
>>
>>Uwe Ligges
>>
>>    
>>
>>>On Friday 16 September 2005 09:28, you wrote:
>>>      
>>>
>>>>Sebastian Leuzinger wrote:
>>>>        
>>>>
>>>>>Hello, has anybody got a simple recepie to test the significance level
>>>>>of the peaks after using spectrum() ?
>>>>>          
>>>>>
>>>>What is you null hypothesis?
>>>>
>>>>- Kind of noise?
>>>>- One particular frequency is noisy or all noisy?
>>>>- ...
>>>>
>>>>Uwe Ligges
>>>>
>>>>        
>>>>
>>>>>(R-version 2.0.1, linux SuSE9.3)
>>>>>          
>>>>>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From felipe at unileon.es  Fri Sep 16 13:25:12 2005
From: felipe at unileon.es (Felipe)
Date: Fri, 16 Sep 2005 13:25:12 +0200
Subject: [R] Are least-squares means useful or appropriate?
Message-ID: <432AAB98.4040103@unileon.es>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi.
I have been reading about the convenience of using least-squares means
(a. k. a. adjusted means) in multiple comparisons (I used to resort to
them when using SAS). I even read a post in this list warning against
them, but not giving much detail. What do you think about this?
Greetings.

Felipe
-----BEGIN PGP SIGNATURE-----

iEYEARECAAYFAkMqq5gACgkQWtdQtNzjBl4AigCfQJ64O0wrdYK/1iMReW5RtI1d
tMIAn3DQSdk+4D7AK7VQGtWo0TElrFG7
=j9EX
-----END PGP SIGNATURE-----



From Friedrich.Leisch at tuwien.ac.at  Fri Sep 16 14:22:51 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Fri, 16 Sep 2005 14:22:51 +0200
Subject: [R] setkeys and Sweave
In-Reply-To: <432A849B.4030603@wsl.ch>
References: <200509142043.18959.friedrich@hattendoerfer.de>
	<20050914191934.GA3535@irwin.vpn.uni-freiburg.de>
	<432A849B.4030603@wsl.ch>
Message-ID: <17194.47387.611418.114327@galadriel.ci.tuwien.ac.at>

>>>>> On Fri, 16 Sep 2005 10:38:51 +0200,
>>>>> Christian Hoffmann (CH) wrote:

  > Hi there:
  > Using

  > \setkeys{Gin}{width=1.0\textwidth}
  > \setkeys{Gin}{height=10cm}
  > \setkeys{Gin}{height=0.8\textwidth}

  > all seem to work under R-2.1.1 under sparc, solaris2.9, but

  > \setkeys{Gin}{scale=0.3}
  > \setkeys{Gin}{angle=90}

  > do not work. I have not been able to find relevant information, googling 
  > on setkeys proved confusing, at best.

  > http://cnlart.web.cern.ch/cnlart/218/node85.html states:
  > ---
  >   Setting key values globally

  > If you want to specify a global value for a set of keys, then you can 
  > use the \setkeys command defined in the keyval package (described below).

  > As an example let us consider the case where you would like your figure 
  > to be scaled to the width of the line. Then you could specify the following:

  >    \setkeys{Gin}{width=\linewidth}

  > The first argument Gin of the \setkeys command refers to the 
  > \includegraphics command. The result will be that all images include 
  > with this command (when the graphicx is loaded) will be set to the 
  > desired width inside the current group or environment.

  > In the similar way one could specify any of the possible arguments of 
  > the \rotatebox command by using the Grot specifier, e.g.,

  >    \setkeys{Grot}{origin=tc}
  > ---

  > Question:

  > Which variants of \setkeys are there? Is there a list stating these?


That's not really an R question, but a LaTeX question. Maybe you have
more luck on a LaTeX list. I always use the grfguide.ps manual that
comes with the graphics package (at least on my Debian Luinux box) as
reference, or the LaTeX companion. 


  > Can several \setkeys{of the same sort} be used inside one document?

Yes, I regularly do that. Every \setkeys{} sets the value from the
point in the LaTeX document where it appears, i.e.,

\setkeys{Gin}{width=1.0\textwidth}

<<fig1>>

\setkeys{Gin}{width=0.5\textwidth}

<<fig2>>


does the expected.

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From christoph.lehmann at gmx.ch  Fri Sep 16 14:55:52 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 16 Sep 2005 14:55:52 +0200 (MEST)
Subject: [R] savePlot(type="wmf") in loop fails, since too fast
Message-ID: <20875.1126875352@www17.gmx.net>

hi
working with R-2.1.1 on winxp, in a loop I draw to a trellis.device which
takes some time. After the drawing I call savePlot().
it seems, the loop is too fast for the savePlot() call to finish. Is there
any solution for such a problem? Calling the same steps outside the loop,
works fine.
many thanks

christoph 

--



From bernarduse1 at yahoo.fr  Fri Sep 16 14:58:24 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Fri, 16 Sep 2005 14:58:24 +0200 (CEST)
Subject: [R] plot  spaghetti data
Message-ID: <20050916125824.76778.qmail@web25808.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050916/ecaded8c/attachment.pl

From tom_colson at ncsu.edu  Fri Sep 16 15:12:45 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Fri, 16 Sep 2005 09:12:45 -0400
Subject: [R] Error in vector("double",
	length) : vector size specified is too large....VLDs
In-Reply-To: <432A8FFE.8030503@geog.uu.nl>
Message-ID: <200509161312.j8GDCfF5014556@uni08mr.unity.ncsu.edu>

It took about 2 hours on the 64 bit windows platform. Now I just need  to
find my notes from ST733 and remember how to use GSTAT to estimate the
parameters....


http://www4.ncsu.edu/~tpcolson/variog.jpg

Thomas Colson
North Carolina State University
Department of Forestry and Environmental Resources
(919) 673 8023
tom_colson at ncsu.edu

Calendar:
www4.ncsu.edu/~tpcolson
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Edzer J. Pebesma
Sent: Friday, September 16, 2005 5:27 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Error in vector("double", length) : vector size specified is
too large....VLDs

Tom,

please try to use the variogram function in package gstat; it doesn't (try
to) store all pairwise differences, but rather accumulates them for distance
intervals.

It will take a while to do this, and there is a chance that you overflow the
counter that keeps the number of point pairs for each interval: 304000^2 >
2^32; it is stored as a C long, so may work on a 64 bit architecture.
Otherwise, I'd suggest to sample your data set.

I'd be interested to hear whether you succeed (or not).
--
Edzer

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From subianto at gmail.com  Fri Sep 16 15:16:59 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Fri, 16 Sep 2005 15:16:59 +0200
Subject: [R] How to make two figures in one plot - package vcd
Message-ID: <432AC5CB.6070007@gmail.com>

Dear all,
I have a problem to make figures with two columns in package vcd.
Here an example code I take from "\library\vcd\html\plot.loglm.html"
What I need, I want to make two figures in one plot.
How could I do that.
I have tried with
layout(rbind(c(1, 1, 2, 2)))
but the same result, two plot.

Best wishes, Muhammad Subianto

library(vcd)
oldpar <- par(mfrow=c(1, 2))
## mosaic display for PreSex model
data(PreSex)
fm <- loglm(~ PremaritalSex * ExtramaritalSex * (Gender + MaritalStatus),
             data = aperm(PreSex, c(3, 2, 4, 1)))
## visualize Pearson statistic
plot(fm, split_vertical = TRUE)
## visualize LR statistic
plot(fm, split_vertical = TRUE, residuals_type = "deviance")
par(oldpar)


 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.1
year     2005
month    06
day      20
language R
 >
 > packageDescription("vcd")
Package: vcd
Version: 0.9-3
Date: 2005-09-01
....



From HDoran at air.org  Fri Sep 16 15:19:36 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 16 Sep 2005 09:19:36 -0400
Subject: [R] plot  spaghetti data
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A0A4837@dc1ex2.air.org>

There is interaction.plot(), but, the trellis graphics in lattice are
much better. You might want to check these out. 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Bernard
Sent: Friday, September 16, 2005 8:58 AM
To: r-help at stat.math.ethz.ch
Subject: [R] plot spaghetti data

Dear All,
 
I  wonder if there is an R function to plot longitudinal data (spaghetti
plots)...
I've seen the function "is.longitudinal" but without any succes...
 
Thanks a lot in advance,
 
Bernard

		
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From kostaskaris at hotmail.com  Fri Sep 16 15:33:45 2005
From: kostaskaris at hotmail.com (kostas karis)
Date: Fri, 16 Sep 2005 13:33:45 +0000
Subject: [R] Question:manipulating spatial data using combination of
	Maptools and Splancs
Message-ID: <BAY21-F304652BBB61389E8875D95BC910@phx.gbl>

Hi,
I have a problem that concerns combination of the package Maptools and 
Splancs
I have 2 shapefiles that i want to manipulate (one of type point and one 
polygon).I import them in R using Maptools but then i can't estimate a 
quartic Kernel using Splancs. The package doesn't recognize the shapes 
(invalid points and poly argument).I don't know if this is an easy task but 
i have read both packages's manual and i can't find a liable solution. Thank 
u for your time.



From r.hankin at noc.soton.ac.uk  Fri Sep 16 15:42:27 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 16 Sep 2005 14:42:27 +0100
Subject: [R] 3d points for objects
Message-ID: <C7F241C3-4662-4590-8FF6-8BF7BB2C5194@soc.soton.ac.uk>

Hi

I am putting together some manpages for my octonion package,
which is nearing completion.

Does anyone have a dataset consisting of 3D coordinates of
points that collectively form a shape reminiscent of a
simple, friendly, recognizable object?  A few dozen points would
be about the right size.

Something like a teapot or a rabbit (or even the R logo!)
would be good.



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From matthieu.cornec at gmail.com  Fri Sep 16 15:51:10 2005
From: matthieu.cornec at gmail.com (Matthieu Cornec)
Date: Fri, 16 Sep 2005 15:51:10 +0200
Subject: [R] About princomp
Message-ID: <8a83e5000509160651635a6ce@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050916/aec0e3be/attachment.pl

From dieter.menne at menne-biomed.de  Fri Sep 16 15:51:58 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 16 Sep 2005 13:51:58 +0000 (UTC)
Subject: [R] savePlot(type="wmf") in loop fails, since too fast
References: <20875.1126875352@www17.gmx.net>
Message-ID: <loom.20050916T154544-972@post.gmane.org>

Christoph Lehmann <christoph.lehmann <at> gmx.ch> writes:

> working with R-2.1.1 on winxp, in a loop I draw to a trellis.device which
> takes some time. After the drawing I call savePlot().
> it seems, the loop is too fast for the savePlot() call to finish. 

I virtually see the computer being overrun by her/his/its own plotting 
function, all semaphores blinking like mad...

When in a loop, you must use print() to output the plot. Easy to forget, I 
remember that even Douglas Bates once was close to calling Deepayan who was far 
away in India one hour before the presentation.

library(lattice)
for (i in 1:10) {
  x = rnorm(10)
  y = rnorm(10)
  p = xyplot(x~y,main=paste("Plot",i))
  print(p)
  savePlot(paste("Plot",i, sep=""))
}

But as far I know, savePlot anyway is not the preferred methode to print 
trellis, is just just happens to work somehow. Here is my preferred method: 
print everything to a postscript file (nice one-file solution), and extract to 
emf-file with pstoedit/Ghostgum when you really need it.

trellis.device("postscript",file="Myplots.ps",color = T)
for (i in 1:10) {
  x = rnorm(10)
  y = rnorm(10)
  p = xyplot(x~y,main=paste("Plot",i))
  print(p)
}
dev.off()

Dieter



From fcombes at gmail.com  Fri Sep 16 15:55:47 2005
From: fcombes at gmail.com (Florence Combes)
Date: Fri, 16 Sep 2005 15:55:47 +0200
Subject: [R] fusion of rows (as in merge()) but from only 1 matrix
Message-ID: <73dae30605091606557bd2d908@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050916/b9c9b787/attachment.pl

From bitwrit at ozemail.com.au  Sat Sep 17 02:12:16 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Sat, 17 Sep 2005 00:12:16 +0000
Subject: [R] Info about plotting functions
Message-ID: <432B5F60.1010406@ozemail.com.au>

Hi Package Maintainers,

I would like to gather all the information I can about which packages 
include plotting functions that do the following:

radial (cyclic, clock, polar) plots
displaying overplotting (like sunflowerplot)
automatically labeling points on a plot
plotting frequency tables
displaying confidence intervals
axis breaks
staggering or rotating long axis labels
Gantt charts
soil texture triangles

This is for an R-news paper, as I would like to know what other packages 
are doing similar things to plotrix. Just send _me_ the package name so 
that I can have a look at its contents. (Please don't copy to the help 
list) I don't think I could do that for all of the packages before the 
end of the year...

Jim



From david.barron at said-business-school.oxford.ac.uk  Fri Sep 16 16:10:04 2005
From: david.barron at said-business-school.oxford.ac.uk (David Barron)
Date: Fri, 16 Sep 2005 15:10:04 +0100
Subject: [R] column-binary data
Message-ID: <6DD6C3FF939D934CBD916EAC13D581E30B9B4B@sbs-mercury4>

I have a number of datasets that are multipunch column-binary format.  Does anyone have any advice on how to read this into R?  Thanks.

David



From drf5n at maplepark.com  Fri Sep 16 16:13:07 2005
From: drf5n at maplepark.com (David Forrest)
Date: Fri, 16 Sep 2005 09:13:07 -0500 (CDT)
Subject: [R] 3d points for objects
In-Reply-To: <C7F241C3-4662-4590-8FF6-8BF7BB2C5194@soc.soton.ac.uk>
References: <C7F241C3-4662-4590-8FF6-8BF7BB2C5194@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.58.0509160911560.5352@maplepark.com>

On Fri, 16 Sep 2005, Robin Hankin wrote:

> Hi
>
> I am putting together some manpages for my octonion package,
> which is nearing completion.
>
> Does anyone have a dataset consisting of 3D coordinates of
> points that collectively form a shape reminiscent of a
> simple, friendly, recognizable object?  A few dozen points would
> be about the right size.
>
> Something like a teapot or a rabbit (or even the R logo!)
> would be good.
>

Try the teapot in netcdf:
http://www.maplepark.com/~drf5n/cgi-bin/wiki.cgi?RMeshVisualization
http://www.maplepark.com/~drf5n/extras/teapot.nc

>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From snvk4u at gmail.com  Fri Sep 16 16:16:30 2005
From: snvk4u at gmail.com (Krishna)
Date: Fri, 16 Sep 2005 19:46:30 +0530
Subject: [R] help required on read.table
Message-ID: <139ef1c2050916071660688974@mail.gmail.com>

Hi all

i am facing a peculiar problem for data input using read.table which i
never faced previously.

i have a data file by name abnew.txt with two coloumns data as depicted below.

A     B
420 422
314 321

the txt file is created using the excel save as option. i issued the
statement as

> a <- read.table("abnew.txt", header=TRUE) 
> a
     X.??S
1      NA
2       2
3       2
4      NA
5       2
6       2
7      NA
8       2
9       2
10     NA

the o/p looks like as copied above. can some one help me on correct
data reading. your earliest response will help me a lot.

thanks and regards

snvk



From vincent at 7d4.com  Fri Sep 16 16:31:16 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Fri, 16 Sep 2005 16:31:16 +0200
Subject: [R] fusion of rows (as in merge()) but from only 1 matrix
In-Reply-To: <73dae30605091606557bd2d908@mail.gmail.com>
References: <73dae30605091606557bd2d908@mail.gmail.com>
Message-ID: <432AD734.40302@7d4.com>

may
union(x, y)
intersect(x, y)
setdiff(x, y)
setequal(x, y)
be of help ?



From macq at llnl.gov  Fri Sep 16 16:33:30 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 16 Sep 2005 07:33:30 -0700
Subject: [R] Copying from graphics window in OS X
In-Reply-To: <4329D057.3080107@ucla.edu>
References: <BF4F41E5.EA58%sdavis2@mail.nih.gov>
 <4329D057.3080107@ucla.edu>
Message-ID: <p06210209bf5084f33953@[128.115.153.6]>

It does look like a Cygwin feature. Possibly, another window manager, 
not Apple's XQuartz, would have something.

You could try xwd, which is available in the usual place, /usr/X11R6/bin.

I have written a couple of helper functions that might make things a 
little less painful if you don't find a satisfactory 
capture-to-clipboard solution.

Use them something like this:
    x11()            ## open a graphics window
    plot(....)        ## make a plot
    pl.copy()       ## copy it to a pdf (pdf is the default)
    pl.email()      ## email it to someone (default recipient is me)

    pl.copy() copies the current graph to a pdf, png, jpeg, eps, or ps file.
    pl.email(), called immediately after pl.copy(), will email that file.

pl.email() assumes that outgoing email can be sent using the 'mail' 
or 'mailx' command, so
you might to modify pl.email() for your linux server. It also assumes 
uuencode is available.

-Don

>  pl.copy
function (file = "currentgraph", gtype = c("pdf", "png", "jpeg", "eps",
     "ps"), width = 9, height = 5, wpixels = 700, hpixels = 400,
     time = FALSE, horizontal = FALSE, enc.for.ps = "ISOLatin1",
     enc.for.pdf = "AdobeStd", ...)
{
     gtype <- match.arg(gtype)
     file <- paste(file, if (time)
         format(Sys.time(), "-%Y-%m-%d-%H-%M")
     else "", ".", gtype, sep = "")
     gr <- recordPlot()
     switch(gtype, pdf = {
         pdf(file, width = width, height = height, onefile = FALSE,
             encoding = enc.for.pdf, ...)
     }, png = {
         png(file, width = wpixels, height = hpixels, ...)
     }, jpeg = {
         jpeg(file, width = wpixels, height = hpixels, ...)
     }, eps = {
         postscript(file, width = width, height = height, horizontal = 
horizontal,
             onefile = FALSE, paper = "special", encoding = enc.for.ps,
             ...)
     }, ps = {
         postscript(file, width = width, height = height, horizontal = 
horizontal,
             onefile = FALSE, paper = "special", encoding = enc.for.ps,
             ...)
     }, stop("invalid gtype\n"))
     replayPlot(gr)
     dev.off()
     assign("current.file", file, pos = ".GlobalEnv")
     cat("file is ", file, "\nfile name is stored in object \"current.file\"\n",
         sep = "")
     invisible(gr)
}
>  pl.email
function (to = "macq at llnl.gov", file = current.file, from = Sys.getenv("USER"),
     subject = paste("Graph from", from), note = "", mail.cmd = {
         if (Sys.info()["sysname"] == "SunOS")
             "mailx"
         else "mail"
     })
{
     cmd <- paste("uuencode ", file, " ", file, " | ", mail.cmd,
         " -s \"", subject, "\" ", to, sep = "")
     cat("mailing with command:\n", cmd, "\n")
     system(cmd)
}



At 12:49 PM -0700 9/15/05, Chris Wiita wrote:
>I hope there is a way...at the moment an os X screen grab is the only
>way to get a quick copy. When I ran Cygwin on a PC, copying from graphic
>windows was as easy as ctrl+c--so it doesn't sound like an X11
>limitation.  I'd like to know what Cygwin was doing in the background...
>
>Sean Davis wrote:
>
>>On 9/15/05 3:14 PM, "Chris Wiita" <cgwiita at ucla.edu> wrote:
>>
>> 
>>
>>>I'm running R from an Xterm window is OSX-Tiger.  Graphical windows
>>>appear as they should, but I'm having trouble copying from them--using
>>>cmd+c or the Copy option in the Edit menu won't place the graph in the
>>>clipboard (when I paste into a running OS X app, I get whatever was the
>>>last copied thing from a non-x11 window).  Any ideas on how to copy from
>>>a xterm-launched graphical window?  I can copy/paste into and out of the
>>>xterm command line, but I can't get anything from a graphical window.
>>>   
>>>
>>
>>I don't think it is possible, but I would love to be corrected.  You can
>>simple make a png, pdf, etc. if you want to save the graphic.
>>
>>Sean
>>
>>
>> 
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From petr.pikal at precheza.cz  Fri Sep 16 16:34:21 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 16 Sep 2005 16:34:21 +0200
Subject: [R] help required on read.table
In-Reply-To: <139ef1c2050916071660688974@mail.gmail.com>
Message-ID: <432AF40D.9706.1D0BC3A@localhost>

Hi

if you still have Excel file and can open it

open Excel
select what you want to read, including header
press ctrl-C
then in R
a<-read.delim("clipboard")

and to write from R

write.table(tab, "clipboard", sep = "\t", row.names = F)
open Excel
press ctrl-V

Looks like Excel left in your txt file  something which prevent 
correct reading. Does the txt file look like you presented in other 
viewer (Notepad,....)

HTH
Petr

On 16 Sep 2005 at 19:46, Krishna wrote:

> Hi all
> 
> i am facing a peculiar problem for data input using read.table which i
> never faced previously.
> 
> i have a data file by name abnew.txt with two coloumns data as
> depicted below.
> 
> A     B
> 420 422
> 314 321
> 
> the txt file is created using the excel save as option. i issued the
> statement as
> 
> > a <- read.table("abnew.txt", header=TRUE) 
> > a
>      X.??S
> 1      NA
> 2       2
> 3       2
> 4      NA
> 5       2
> 6       2
> 7      NA
> 8       2
> 9       2
> 10     NA
> 
> the o/p looks like as copied above. can some one help me on correct
> data reading. your earliest response will help me a lot.
> 
> thanks and regards
> 
> snvk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From fcombes at gmail.com  Fri Sep 16 16:35:55 2005
From: fcombes at gmail.com (Florence Combes)
Date: Fri, 16 Sep 2005 16:35:55 +0200
Subject: [R] fusion of rows (as in merge()) but from only 1 matrix
In-Reply-To: <73dae30605091607357e7f0fe1@mail.gmail.com>
References: <73dae30605091606557bd2d908@mail.gmail.com>
	<432AD734.40302@7d4.com> <73dae30605091607357e7f0fe1@mail.gmail.com>
Message-ID: <73dae3060509160735797bc99a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050916/94e5eee1/attachment.pl

From bernarduse1 at yahoo.fr  Fri Sep 16 16:38:46 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Fri, 16 Sep 2005 16:38:46 +0200 (CEST)
Subject: [R] Replicate
Message-ID: <20050916143846.8691.qmail@web25808.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050916/e872ae97/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Fri Sep 16 16:54:56 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 16 Sep 2005 16:54:56 +0200
Subject: [R] Replicate
References: <20050916143846.8691.qmail@web25808.mail.ukl.yahoo.com>
Message-ID: <009001c5bace$9a991480$0540210a@www.domain>

look at ?unique(), e.g.,

x <- c(1, 1, 1, 2, 2, 2, 3, 3, 3, 3)
unique(x)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Marc Bernard" <bernarduse1 at yahoo.fr>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, September 16, 2005 4:38 PM
Subject: [R] Replicate


> Dear All,
>
> I have a vector x = (1,1,1,2,2,2,3,3,3,3)
> I am looking for a function to return a vector containing  the 
> distinct elements of x i,e y = (1,2,3)
>
> The following code gives the desired results:
>
> as.numeric(levels(as.factor(x)))
>
> Is there any other elegant  way?
>
> Thanks,
>
> B
>
>
>
>
> ---------------------------------
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From Matthias.Templ at statistik.gv.at  Fri Sep 16 16:53:34 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 16 Sep 2005 16:53:34 +0200
Subject: [R] Replicate
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BABFB@xchg1.statistik.local>

x = (1,1,1,2,2,2,3,3,3,3)
unique(x)

Best,
Matthias

> Dear All,
>  
> I have a vector x = (1,1,1,2,2,2,3,3,3,3)
> I am looking for a function to return a vector containing  
> the distinct elements of x i,e y = (1,2,3)
>  
> The following code gives the desired results:
>  
> as.numeric(levels(as.factor(x)))
>  
> Is there any other elegant  way?
>  
> Thanks,
>  
> B
>  
>  
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Fri Sep 16 16:55:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Sep 2005 16:55:27 +0200
Subject: [R] help required on read.table
In-Reply-To: <139ef1c2050916071660688974@mail.gmail.com>
References: <139ef1c2050916071660688974@mail.gmail.com>
Message-ID: <x2hdclf4f4.fsf@turmalin.kubism.ku.dk>

Krishna <snvk4u at gmail.com> writes:

> Hi all
> 
> i am facing a peculiar problem for data input using read.table which i
> never faced previously.
> 
> i have a data file by name abnew.txt with two coloumns data as depicted below.
> 
> A     B
> 420 422
> 314 321

(with A left-justified and B right-ditto???)
 
> the txt file is created using the excel save as option. i issued the
> statement as
> 
> > a <- read.table("abnew.txt", header=TRUE) 
> > a
>      X.??S
> 1      NA
> 2       2
> 3       2
> 4      NA
> 5       2
> 6       2
> 7      NA
> 8       2
> 9       2
> 10     NA
> 
> the o/p looks like as copied above. can some one help me on correct
> data reading. your earliest response will help me a lot.

What do you get from readLines("abnew.txt")? 


You might want to try read.delim("abnew.txt").

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From jporzak at gmail.com  Fri Sep 16 16:58:33 2005
From: jporzak at gmail.com (Jim Porzak)
Date: Fri, 16 Sep 2005 07:58:33 -0700
Subject: [R] Replicate
In-Reply-To: <20050916143846.8691.qmail@web25808.mail.ukl.yahoo.com>
References: <20050916143846.8691.qmail@web25808.mail.ukl.yahoo.com>
Message-ID: <2a9c000c05091607584b98def5@mail.gmail.com>

Hi Marc,

> x = c(1,1,1,2,2,2,3,3,3,3)
> unique(x)
[1] 1 2 3
>

Being a database guy myself, it took me a while to think "unique"
rather than "distinct"

-- 
HTH,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA


On 9/16/05, Marc Bernard <bernarduse1 at yahoo.fr> wrote:
> Dear All,
> 
> I have a vector x = (1,1,1,2,2,2,3,3,3,3)
> I am looking for a function to return a vector containing  the distinct elements of x i,e y = (1,2,3)
> 
> The following code gives the desired results:
> 
> as.numeric(levels(as.factor(x)))
> 
> Is there any other elegant  way?
> 
> Thanks,
> 
> B
> 
> 
> 
> 
> ---------------------------------
> 
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From xprt.wannabe at gmail.com  Fri Sep 16 17:11:05 2005
From: xprt.wannabe at gmail.com (xpRt.wannabe)
Date: Fri, 16 Sep 2005 10:11:05 -0500
Subject: [R] Add lines to density plot
Message-ID: <a4fecdd705091608114fd5477c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050916/75b39a93/attachment.pl

From A.Brennan at sheffield.ac.uk  Fri Sep 16 17:21:00 2005
From: A.Brennan at sheffield.ac.uk (A.Brennan)
Date: Fri, 16 Sep 2005 16:21:00 +0100
Subject: [R] Integrate functions with loops
Message-ID: <432AF0EC.5645.8B6925@localhost>

Hi

i am having a problem with the 'integrate' function
the function i want to integrate has the form
sum(vector^x)

i have defined the function with a for loop first - 
integrandtotest <- function(x)
    {a<-rep(0,len=2)
    for (i in 1:2)
        {a[i]<-t[i]^x}
        sum(a)
        }

the results gives errors
###########
Error in integrate(integrandtotest, lower = 0.1, upper = 2, 
subdivisions = 10000) : 
        evaluation of function gave a result of wrong length
In addition: Warning messages:
1: number of items to replace is not a multiple of replacement 
length 
2: number of items to replace is not a multiple of replacement 
length 
#######

I then tried a vector multiplication instead of the for loop


integrandtotest3 <- function(x)
    {b<-c(t[1],t[2])
    a<-b^x
    sum(a)
        }

which gave errors
########
Error in integrate(integrandtotest3, lower = 0.1, upper = 2, 
subdivisions = 10000) : 
        evaluation of function gave a result of wrong length
In addition: Warning message:
longer object length
        is not a multiple of shorter object length in: b^x 
##########

but when i write the functio out long-hand as follows:

integrandtotest2 <- function(x)
    {t[1]^x+t[2]^x}

the integrate function works perfectly.......
###
> integralresulttotest2<-integrate(integrandtotest2, lower=0.1, 
upper=2, subdivisions=10000)
> integralresulttotest2
1.642369 with absolute error < 1.8e-14
###

Unfortunatley my real life example has the vector a with length at 
least 100.

Is the any way round these errors?

Many thanks for answers to my first question to this list
yours
Alan



Alan Brennan
Director of Health Economics and Decision Science
http://www.shef.ac.uk/scharr/sections/heds
ScHARR
School of Health and Related Research
University of Sheffield
Regent Ct
30 Regent St
Sheffield S1 4DA
Tel:+44 (0)114 2220684
Fax:+44 (0)114 2724095
e-mail:a.brennan at sheffield.ac.uk



From ripley at stats.ox.ac.uk  Fri Sep 16 17:21:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Sep 2005 16:21:34 +0100 (BST)
Subject: [R] About princomp
In-Reply-To: <8a83e5000509160651635a6ce@mail.gmail.com>
References: <8a83e5000509160651635a6ce@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0509161619050.3651@gannet.stats>

You send HTML mail (please see the posting guide and do as we ask) and it 
seems to have not produced a working text version.

My guess is that you have a changed version of USArrests around.  In any 
case, there is no problem in R 2.1.1 (sic), so this is local to you.

On Fri, 16 Sep 2005, Matthieu Cornec wrote:

> Hi,
> I run the example for princomp for R211
> I got the following error for biplot
>
>> ## The variances of the variables in the
>> ## USArrests data vary by orders of magnitude, so scaling is appropriate
>> (pc.cr <http://pc.cr> <- princomp(USArrests)) # inappropriate
> Erreur dans cov.wt(z) : 'x' must contain finite values only
>> princomp(USArrests, cor = TRUE) # =^= prcomp(USArrests, scale=TRUE)
> Erreur dans cov.wt(z) : 'x' must contain finite values only
>> ## Similar, but different:
>> ## The standard deviations differ by a factor of sqrt(49/50)
>>
>> summary(pc.cr <http://pc.cr> <- princomp(USArrests, cor = TRUE))
> Erreur dans cov.wt(z) : 'x' must contain finite values only
>> loadings(pc.cr <http://pc.cr>) ## note that blank entries are small but
> not zero
>
> Loadings:
> Comp.1 Comp.2 Comp.3
> Murder 0.663 0.314 0.679
> Assault 0.694 -0.715
> UrbanPop 0.279 -0.946 0.165
>
> Comp.1 Comp.2 Comp.3
> SS loadings 1.000 1.000 1.000
> Proportion Var 0.333 0.333 0.333
> Cumulative Var 0.333 0.667 1.000
>> plot(pc.cr <http://pc.cr>) # shows a screeplot.
>> biplot(pc.cr <http://pc.cr>)
> Erreur dans plot.window(xlim, ylim, log, asp, ...) :
> 'xlim' n?cessite des valeurs finies
>>
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From gunter.berton at gene.com  Fri Sep 16 17:21:44 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 16 Sep 2005 08:21:44 -0700
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <200509161521.j8GFLiLl005334@hertz.gene.com>

 
Newbies (and others!) may find useful the R Reference Card made available by
Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
the "Contributed" link on CRAN (where some other reference cards are also
linked). It categorizes and organizes a bunch of R's basic, most used
functions so that they can be easily found. For example, paste() is under
the "Strings" heading and expand.grid() is under "Data Creation." For
newbies struggling to find the right R function as well as veterans who
can't quite remember the function name, it's very handy.
 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From francoisromain at free.fr  Fri Sep 16 17:22:18 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 16 Sep 2005 17:22:18 +0200
Subject: [R] Add lines to density plot
In-Reply-To: <a4fecdd705091608114fd5477c@mail.gmail.com>
References: <a4fecdd705091608114fd5477c@mail.gmail.com>
Message-ID: <432AE32A.6050008@free.fr>

Le 16.09.2005 17:11, xpRt.wannabe a ??crit :

>Dear List,
>
>I am using R to learn bootstrapping concepts. Not to be oblivious to 
>the
>contributed packages of boot and bootstrap, I have opted to do the
>following as a way to hone my R skills.
>
>One example I am trying to work through is the following:
>
>x <- 1:20
>Observed <- sample(x,20)
>Resamples <- replicate(1000,sample(Observed,replace=TRUE))
>Boot.Means <- apply(Resamples,2,mean)
>plot(density(Boot.Means))
>
>For the life of me, I can't seem to figure out how to add two lines to 
>the
>density plot:
>
>1. A line that represents the mean of Boot.Means
>2. A line that represents the mean of Observed
>  
>
R> abline(v=mean(Boot.Means))

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From sundar.dorai-raj at pdf.com  Fri Sep 16 17:31:35 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 16 Sep 2005 10:31:35 -0500
Subject: [R] Integrate functions with loops
In-Reply-To: <432AF0EC.5645.8B6925@localhost>
References: <432AF0EC.5645.8B6925@localhost>
Message-ID: <432AE557.9080908@pdf.com>



A.Brennan wrote:
> Hi
> 
> i am having a problem with the 'integrate' function
> the function i want to integrate has the form
> sum(vector^x)
> 
> i have defined the function with a for loop first - 
> integrandtotest <- function(x)
>     {a<-rep(0,len=2)
>     for (i in 1:2)
>         {a[i]<-t[i]^x}
>         sum(a)
>         }
> 
> the results gives errors
> ###########
> Error in integrate(integrandtotest, lower = 0.1, upper = 2, 
> subdivisions = 10000) : 
>         evaluation of function gave a result of wrong length
> In addition: Warning messages:
> 1: number of items to replace is not a multiple of replacement 
> length 
> 2: number of items to replace is not a multiple of replacement 
> length 
> #######
> 
> I then tried a vector multiplication instead of the for loop
> 
> 
> integrandtotest3 <- function(x)
>     {b<-c(t[1],t[2])
>     a<-b^x
>     sum(a)
>         }
> 
> which gave errors
> ########
> Error in integrate(integrandtotest3, lower = 0.1, upper = 2, 
> subdivisions = 10000) : 
>         evaluation of function gave a result of wrong length
> In addition: Warning message:
> longer object length
>         is not a multiple of shorter object length in: b^x 
> ##########
> 
> but when i write the functio out long-hand as follows:
> 
> integrandtotest2 <- function(x)
>     {t[1]^x+t[2]^x}
> 
> the integrate function works perfectly.......
> ###
> 
>>integralresulttotest2<-integrate(integrandtotest2, lower=0.1, 
> 
> upper=2, subdivisions=10000)
> 
>>integralresulttotest2
> 
> 1.642369 with absolute error < 1.8e-14
> ###
> 
> Unfortunatley my real life example has the vector a with length at 
> least 100.
> 
> Is the any way round these errors?
> 
> Many thanks for answers to my first question to this list
> yours
> Alan
> 

Hi, Alan,

It might help if you print(x) in your integrate function.

integrandtotest <- function(x) {
   print(x)
   a <- rep(0, len=2)
   for(i in 1:2) {
     a[i] <- tt[i]^x
   }
   sum(a)
}

integrate(integrandtotest, lower = 0.1, upper = 2)

You will see that "x" is a vector and "tt[i]^x" returns a vector of the 
same length. You are trying to place this vector into "a[i]" which is 
length 1. Try the following *untested* code instead:

<untested>
integrandtotest <- function(x) {
   sum(sapply(x, function(xi) sum(tt^xi)))
}

integrate(integrandtotest, lower = 0.1, upper = 2)
</untested>

Also, I would avoid using "t" as a variable name. "t" is also a 
function. Most of the time R can tell the difference, but sometimes it 
cannot.

HTH,

--sundar



From ripley at stats.ox.ac.uk  Fri Sep 16 17:37:47 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Sep 2005 16:37:47 +0100 (BST)
Subject: [R] Integrate functions with loops
In-Reply-To: <432AF0EC.5645.8B6925@localhost>
References: <432AF0EC.5645.8B6925@localhost>
Message-ID: <Pine.LNX.4.61.0509161632580.3651@gannet.stats>

Try to give a vector result with one element for each of element of x, 
e.g.

 	integrandtotest <- function(x) colSums(outer(t, x, "^"))

works, although in fact this integration can be done analytically (it is a 
sum of exponentials).

On Fri, 16 Sep 2005, A.Brennan wrote:

> Hi
>
> i am having a problem with the 'integrate' function
> the function i want to integrate has the form
> sum(vector^x)
>
> i have defined the function with a for loop first -
> integrandtotest <- function(x)
>    {a<-rep(0,len=2)
>    for (i in 1:2)
>        {a[i]<-t[i]^x}
>        sum(a)
>        }
>
> the results gives errors
> ###########
> Error in integrate(integrandtotest, lower = 0.1, upper = 2,
> subdivisions = 10000) :
>        evaluation of function gave a result of wrong length
> In addition: Warning messages:
> 1: number of items to replace is not a multiple of replacement
> length
> 2: number of items to replace is not a multiple of replacement
> length
> #######
>
> I then tried a vector multiplication instead of the for loop
>
>
> integrandtotest3 <- function(x)
>    {b<-c(t[1],t[2])
>    a<-b^x
>    sum(a)
>        }
>
> which gave errors
> ########
> Error in integrate(integrandtotest3, lower = 0.1, upper = 2,
> subdivisions = 10000) :
>        evaluation of function gave a result of wrong length
> In addition: Warning message:
> longer object length
>        is not a multiple of shorter object length in: b^x
> ##########
>
> but when i write the functio out long-hand as follows:
>
> integrandtotest2 <- function(x)
>    {t[1]^x+t[2]^x}
>
> the integrate function works perfectly.......
> ###
>> integralresulttotest2<-integrate(integrandtotest2, lower=0.1,
> upper=2, subdivisions=10000)
>> integralresulttotest2
> 1.642369 with absolute error < 1.8e-14
> ###
>
> Unfortunatley my real life example has the vector a with length at
> least 100.
>
> Is the any way round these errors?
>
> Many thanks for answers to my first question to this list
> yours
> Alan
>
>
>
> Alan Brennan
> Director of Health Economics and Decision Science
> http://www.shef.ac.uk/scharr/sections/heds
> ScHARR
> School of Health and Related Research
> University of Sheffield
> Regent Ct
> 30 Regent St
> Sheffield S1 4DA
> Tel:+44 (0)114 2220684
> Fax:+44 (0)114 2724095
> e-mail:a.brennan at sheffield.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From marcelino.delacruz at upm.es  Fri Sep 16 17:29:33 2005
From: marcelino.delacruz at upm.es (Marcelino de la Cruz)
Date: Fri, 16 Sep 2005 15:29:33 +0000 (UTC)
Subject: [R] Graphical presentation of logistic regression
Message-ID: <loom.20050916T172824-298@post.gmane.org>


As an anecdotical information about the paper in the ESA Bulletin, the data to 
graph the examples are from Jari Oksanen's  package gravy.
Unfortunately, the graphs were not well edited  and even my favourite one (fig. 
3) was completely missing. You can see in the Erratum from volume 86(2) 
http://www.esapubs.org/bulletin/backissues/086-2/et_bulletin86_2print.pdf what 
I was trying to represent. The code only tries to â€œefficientlyâ€ represent raw 
data in the traditional logistic plot that most ecologist are familiar with.

Cheers 


Marcelino



From rschulz at sonic.net  Fri Sep 16 17:53:36 2005
From: rschulz at sonic.net (Randall R Schulz)
Date: Fri, 16 Sep 2005 08:53:36 -0700
Subject: [R] R Reference Card (especially useful for Newbies)
In-Reply-To: <200509161521.j8GFLiLl005334@hertz.gene.com>
References: <200509161521.j8GFLiLl005334@hertz.gene.com>
Message-ID: <200509160853.36562.rschulz@sonic.net>

Bert,

On Friday 16 September 2005 08:21, Berton Gunter wrote:
> Newbies (and others!) may find useful the R Reference Card made
> available by Tom Short and Rpad at
> http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through the
> "Contributed" link on CRAN (where some other reference cards are also
> linked).

This is truly handy. Thanks for pointing it out.

It's too bad there are five orphaned lines of text on an otherwise blank 
page five. Do you or does anyone know of a way to reformat this 
reference card to fit on four pages? Is the original TeX available?

> ...
>
> -- Bert Gunter


Again, thanks for the pointer.

Randall "Rnewbie" Schulz



From h.y.wong at leeds.ac.uk  Fri Sep 16 17:57:54 2005
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Fri, 16 Sep 2005 16:57:54 +0100
Subject: [R] Possible bug in lmer nested analysis with factors
Message-ID: <50E14D7F-8B7D-4D1A-977E-CB36C499A8E7@leeds.ac.uk>

Hello,

Is this a bug in the lmer routine?

 > library(lme4)
 > ### test case based on rats data from Crawley
 > a<-rnorm(36);b<-rep(1:3,each=12);c<-rep(1:2,each=6,3);d<-rep 
(1:3,each=2,6)
 >
 > ### mixed model works when c & d are numeric, lmer assumes they  
are factors
 > m <- lmer(a ~ b + (1|c/d))
 >
 > ### but bails out when they are actually specified as factors
 > c<-factor(c); d<-factor(d)
 > m <- lmer(a ~ b + (1| c / d))

Error in lmer(a ~ b + (1 | c/d)) : entry 0 in matrix[0,0] has row  
2147483647 and column 2147483647
In addition: Warning message:
/ not meaningful for factors in: Ops.factor(c, d)


Cheers

Yan



From h.y.wong at leeds.ac.uk  Fri Sep 16 18:00:53 2005
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Fri, 16 Sep 2005 17:00:53 +0100
Subject: [R] Possible bug in lmer nested analysis with factors
In-Reply-To: <50E14D7F-8B7D-4D1A-977E-CB36C499A8E7@leeds.ac.uk>
References: <50E14D7F-8B7D-4D1A-977E-CB36C499A8E7@leeds.ac.uk>
Message-ID: <D49E3099-7739-43FD-8789-3E7C5145CDF9@leeds.ac.uk>


On 16 Sep 2005, at 16:57, Yan Wong wrote:

> Hello,
>
> Is this a bug in the lmer routine?
>
> ...
> Error in lmer(a ~ b + (1 | c/d)) : entry 0 in matrix[0,0] has row  
> 2147483647 and column 2147483647
> In addition: Warning message:
> / not meaningful for factors in: Ops.factor(c, d)

Sorry, I forgot to specify:

R 2.1.1, Mac OS X 10.4.2, lme4 version 0.98-1, Matrix version 0.98-7.

Yan



From dieter.menne at menne-biomed.de  Fri Sep 16 17:59:31 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 16 Sep 2005 15:59:31 +0000 (UTC)
Subject: [R] How to make two figures in one plot - package vcd
References: <432AC5CB.6070007@gmail.com>
Message-ID: <loom.20050916T175503-974@post.gmane.org>

Muhammad Subianto <subianto <at> gmail.com> writes:

> I have a problem to make figures with two columns in package vcd.
> Here an example code I take from "\library\vcd\html\plot.loglm.html"
> What I need, I want to make two figures in one plot.
> 
> library(vcd)
> oldpar <- par(mfrow=c(1, 2))
> ## mosaic display for PreSex model
> data(PreSex)
> fm <- loglm(~ PremaritalSex * ExtramaritalSex * (Gender + MaritalStatus),
>              data = aperm(PreSex, c(3, 2, 4, 1)))
> ## visualize Pearson statistic
> plot(fm, split_vertical = TRUE)
> ## visualize LR statistic
> plot(fm, split_vertical = TRUE, residuals_type = "deviance")
> par(oldpar)
..

The example worked in the previous version, but David Meyer has rewritten the 
whole package using grid functions. Figures are much nicer now, but 
documentations is a bit on the sparse side. I don't have the docs at hand 
currently, but I believe you should looks at strucplot to set the layout there, 
or use grid directly.

Greetings from an Ex-Cirebonese

Dieter



From HDoran at air.org  Fri Sep 16 18:12:10 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 16 Sep 2005 12:12:10 -0400
Subject: [R] Possible bug in lmer nested analysis with factors
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A0A4880@dc1ex2.air.org>

I think you might have confused lme code with lmer code. Why do you have
c/d in the random portion?

I think what you want is

> lmer(a ~ b + (1 | c)+(1|d))

Which gives the following using your data


Linear mixed-effects model fit by REML
Formula: a ~ b + (1 | c) + (1 | d) 
      AIC      BIC    logLik MLdeviance REMLdeviance
 108.0239 115.9415 -49.01193   94.57296     98.02386
Random effects:
 Groups   Name        Variance   Std.Dev.  
 d        (Intercept) 4.2877e-10 2.0707e-05
 c        (Intercept) 4.2877e-10 2.0707e-05
 Residual             8.5754e-01 9.2603e-01
# of obs: 36, groups: d, 3; c, 2

Fixed effects:
            Estimate Std. Error DF t value Pr(>|t|)  
(Intercept)  0.82928    0.40834 34  2.0309  0.05015 .
b           -0.37563    0.18903 34 -1.9872  0.05500 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Yan Wong
Sent: Friday, September 16, 2005 11:58 AM
To: R-help
Subject: [R] Possible bug in lmer nested analysis with factors

Hello,

Is this a bug in the lmer routine?

 > library(lme4)
 > ### test case based on rats data from Crawley  >
a<-rnorm(36);b<-rep(1:3,each=12);c<-rep(1:2,each=6,3);d<-rep
(1:3,each=2,6)
 >
 > ### mixed model works when c & d are numeric, lmer assumes they are
factors  > m <- lmer(a ~ b + (1|c/d))  >  > ### but bails out when they
are actually specified as factors  > c<-factor(c); d<-factor(d)  > m <-
lmer(a ~ b + (1| c / d))

Error in lmer(a ~ b + (1 | c/d)) : entry 0 in matrix[0,0] has row
2147483647 and column 2147483647
In addition: Warning message:
/ not meaningful for factors in: Ops.factor(c, d)


Cheers

Yan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Fri Sep 16 18:21:17 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 16 Sep 2005 11:21:17 -0500
Subject: [R] Possible bug in lmer nested analysis with factors
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F740A0A4880@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A4880@dc1ex2.air.org>
Message-ID: <432AF0FD.1030001@pdf.com>

My guess is he wants this:

c1 <- factor(c)
d1 <- factor(d)
m <- lmer(a ~ b + (1|c1:d1)+(1|c1))

which assumes d1 is nested within c1.

Take a look at Section 3 in the "MlmSoftRev" vignette:

library(mlmRev)
vignette("MlmSoftRev")

HTH,

--sundar

Doran, Harold wrote:
> I think you might have confused lme code with lmer code. Why do you have
> c/d in the random portion?
> 
> I think what you want is
> 
> 
>>lmer(a ~ b + (1 | c)+(1|d))
> 
> 
> Which gives the following using your data
> 
> 
> Linear mixed-effects model fit by REML
> Formula: a ~ b + (1 | c) + (1 | d) 
>       AIC      BIC    logLik MLdeviance REMLdeviance
>  108.0239 115.9415 -49.01193   94.57296     98.02386
> Random effects:
>  Groups   Name        Variance   Std.Dev.  
>  d        (Intercept) 4.2877e-10 2.0707e-05
>  c        (Intercept) 4.2877e-10 2.0707e-05
>  Residual             8.5754e-01 9.2603e-01
> # of obs: 36, groups: d, 3; c, 2
> 
> Fixed effects:
>             Estimate Std. Error DF t value Pr(>|t|)  
> (Intercept)  0.82928    0.40834 34  2.0309  0.05015 .
> b           -0.37563    0.18903 34 -1.9872  0.05500 .
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Yan Wong
> Sent: Friday, September 16, 2005 11:58 AM
> To: R-help
> Subject: [R] Possible bug in lmer nested analysis with factors
> 
> Hello,
> 
> Is this a bug in the lmer routine?
> 
>  > library(lme4)
>  > ### test case based on rats data from Crawley  >
> a<-rnorm(36);b<-rep(1:3,each=12);c<-rep(1:2,each=6,3);d<-rep
> (1:3,each=2,6)
>  >
>  > ### mixed model works when c & d are numeric, lmer assumes they are
> factors  > m <- lmer(a ~ b + (1|c/d))  >  > ### but bails out when they
> are actually specified as factors  > c<-factor(c); d<-factor(d)  > m <-
> lmer(a ~ b + (1| c / d))
> 
> Error in lmer(a ~ b + (1 | c/d)) : entry 0 in matrix[0,0] has row
> 2147483647 and column 2147483647
> In addition: Warning message:
> / not meaningful for factors in: Ops.factor(c, d)
> 
> 
> Cheers
> 
> Yan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jsorkin at grecc.umaryland.edu  Fri Sep 16 18:56:06 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Fri, 16 Sep 2005 12:56:06 -0400
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <s32ac107.013@grecc.umaryland.edu>

If you can fix the problem, please let me know. I too have noticed the
same error. In the past the text did not spread to a fifth  page.
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu

>>> Randall R Schulz <rschulz at sonic.net> 09/16 11:53 AM >>>
Bert,

On Friday 16 September 2005 08:21, Berton Gunter wrote:
> Newbies (and others!) may find useful the R Reference Card made
> available by Tom Short and Rpad at
> http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through the
> "Contributed" link on CRAN (where some other reference cards are
also
> linked).

This is truly handy. Thanks for pointing it out.

It's too bad there are five orphaned lines of text on an otherwise
blank 
page five. Do you or does anyone know of a way to reformat this 
reference card to fit on four pages? Is the original TeX available?

> ...
>
> -- Bert Gunter


Again, thanks for the pointer.

Randall "Rnewbie" Schulz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From subianto at gmail.com  Fri Sep 16 19:32:30 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Fri, 16 Sep 2005 19:32:30 +0200
Subject: [R] How to make two figures in one plot - package vcd
In-Reply-To: <loom.20050916T175503-974@post.gmane.org>
References: <432AC5CB.6070007@gmail.com>
	<loom.20050916T175503-974@post.gmane.org>
Message-ID: <432B01AE.5080502@gmail.com>

On this day 16/09/2005 05:59 PM, Dieter Menne wrote:
> Muhammad Subianto <subianto <at> gmail.com> writes:
> 
> 
>>I have a problem to make figures with two columns in package vcd.
>>Here an example code I take from "\library\vcd\html\plot.loglm.html"
>>What I need, I want to make two figures in one plot.
>>
>>library(vcd)
>>oldpar <- par(mfrow=c(1, 2))
>>## mosaic display for PreSex model
>>data(PreSex)
>>fm <- loglm(~ PremaritalSex * ExtramaritalSex * (Gender + MaritalStatus),
>>             data = aperm(PreSex, c(3, 2, 4, 1)))
>>## visualize Pearson statistic
>>plot(fm, split_vertical = TRUE)
>>## visualize LR statistic
>>plot(fm, split_vertical = TRUE, residuals_type = "deviance")
>>par(oldpar)
> 
> ..
> 
> The example worked in the previous version, but David Meyer has rewritten the 
> whole package using grid functions. Figures are much nicer now, but 
> documentations is a bit on the sparse side. I don't have the docs at hand 
> currently, but I believe you should looks at strucplot to set the layout there, 
> or use grid directly.
> 

Thanks you for your respon.
Yes, I know this a new version vcd (mosaic->vcd <> mosaicplot->core,base 
R). This version is look very nice. I am very happy to use this version.
I have tried to look at documentations like 
"\library\vcd\html\strucplot.html" I have used "newpage=FALSE" but it 
didn't change.
I will try to look at grid package.

> Greetings from an Ex-Cirebonese
> 
> Dieter


Best, Muhammad Subianto

PS.
Ex-Cirebonese? Ha ha ha ... I know this ... 
http://en.wikipedia.org/wiki/Cirebon
how about this http://en.wikipedia.org/wiki/Banda_Aceh



From tmlammail at yahoo.com  Fri Sep 16 19:34:57 2005
From: tmlammail at yahoo.com (Martin Lam)
Date: Fri, 16 Sep 2005 10:34:57 -0700 (PDT)
Subject: [R]  How do  I get the row indices?
Message-ID: <20050916173458.33907.qmail@web40522.mail.yahoo.com>

Hi,

I was wondering if it's possible to get the row
numbers from a filtering. Here's an example:

# give me the rows with sepal.length == 6.2
iris[(iris[,1]==6.2),]

# output
    Sepal.Length Sepal.Width Petal.Length Petal.Width 
  Species
69           6.2         2.2          4.5         1.5
versicolor
98           6.2         2.9          4.3         1.3
versicolor
127          6.2         2.8          4.8         1.8 
virginica
149          6.2         3.4          5.4         2.3 
virginica

What I really want is that it return the row numbers:
69, 98, 127, 149.

Thanks in advance,

Martin



From Roger.Bivand at nhh.no  Fri Sep 16 19:36:22 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 16 Sep 2005 19:36:22 +0200 (CEST)
Subject: [R] Question:manipulating spatial data using combination of
 Maptools and Splancs
In-Reply-To: <BAY21-F304652BBB61389E8875D95BC910@phx.gbl>
Message-ID: <Pine.LNX.4.44.0509161923180.7565-100000@reclus.nhh.no>

On Fri, 16 Sep 2005, kostas karis wrote:

> Hi,
> I have a problem that concerns combination of the package Maptools and 
> Splancs
> I have 2 shapefiles that i want to manipulate (one of type point and one 
> polygon).I import them in R using Maptools but then i can't estimate a 
> quartic Kernel using Splancs. The package doesn't recognize the shapes 
> (invalid points and poly argument).I don't know if this is an easy task but 
> i have read both packages's manual and i can't find a liable solution. Thank 
> u for your time.
> 

Say you have a shapefile of points, and a shapefile with one single ring 
polygon, no holes or other geometry objects, then using maptools 0.5-2:

> library(maptools)
Loading required package: foreign
Loading required package: sp
> cardiff_pts <- readShapePoints("cf_pts")
> cardiff_poly <- readShapePoly("cf_poly", verbose=TRUE)
Shapefile type: Polygon, (5), # of Shapes: 1
> plot(cardiff_poly)
> plot(cardiff_pts, add=TRUE)

Conversion is by

> splancs_pts <- coordinates(cardiff_pts)
> splancs_poly <- getPolygonCoordsSlot(getPolygonsPolygonsSlot(
+    getSpPpolygonsSlot(cardiff_poly)[[1]])[[1]])

to unpack the coordinates of the points and the single ring boundary. If 
you have more than one shape, and/or more than one ring in that shape, 
adjust the [[Shape]])[[ring]] indices to suit. Then:

> polymap(splancs_poly)
> image(kernel2d(splancs_pts, splancs_poly, h0=15, nx=100, ny=100), 
+    add=TRUE)
> pointmap(splancs_pts, add=TRUE)

works as expected. Extracting the polygon looks complicated because 
shapefiles have a "richer" geometry than splancs. Wrapper functions for 
splancs to use the sp classes now used by maptools will be available 
before long.



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From h.wickham at gmail.com  Fri Sep 16 19:41:01 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 16 Sep 2005 12:41:01 -0500
Subject: [R] Lists as a column in data.frame
Message-ID: <f8e6ff0505091610413fb8720a@mail.gmail.com>

A data frame is a list of vectors of the same length.  A list is a
vector.  So is it acceptable to use a list as a column of a data
frame?

I'd like to be able to take advantage of all the nice features of data
frames (esp. subsetting) while storing more complicated objects.  An
example of this would be a version of by that returns a data.frame
instead of a list - you would then be able to extract the objects you
wanted using subsetting, rather than having to loop through each
element of the list and see if it matches your criteria.

I have been experimenting a bit with this, and generally it works fine
(apart from the occasional bug with printing, and not being able to
use data.frame(a=1, list(a=1, b=2)) because of the way
as.data.frame.list works).  Is it ok to do this? ie. if it works in
the current version of R, should I expect it to keep working in the
future?  Is it undocumented because it is obvious, or because it's a
bad idea?

Thanks,

Hadley



From Achim.Zeileis at wu-wien.ac.at  Fri Sep 16 19:41:10 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 16 Sep 2005 19:41:10 +0200
Subject: [R] How to make two figures in one plot - package vcd
In-Reply-To: <loom.20050916T175503-974@post.gmane.org>
References: <432AC5CB.6070007@gmail.com>
	<loom.20050916T175503-974@post.gmane.org>
Message-ID: <20050916194110.1aa504a8.Achim.Zeileis@wu-wien.ac.at>

On Fri, 16 Sep 2005 15:59:31 +0000 (UTC) Dieter Menne wrote:

> Muhammad Subianto <subianto <at> gmail.com> writes:
> 
> > I have a problem to make figures with two columns in package vcd.
> > Here an example code I take from "\library\vcd\html\plot.loglm.html"
> > What I need, I want to make two figures in one plot.
> > 
> > library(vcd)
> > oldpar <- par(mfrow=c(1, 2))
> > ## mosaic display for PreSex model
> > data(PreSex)
> > fm <- loglm(~ PremaritalSex * ExtramaritalSex * (Gender +
> > MaritalStatus),
> >              data = aperm(PreSex, c(3, 2, 4, 1)))
> > ## visualize Pearson statistic
> > plot(fm, split_vertical = TRUE)
> > ## visualize LR statistic
> > plot(fm, split_vertical = TRUE, residuals_type = "deviance")
> > par(oldpar)
> ..
> 
> The example worked in the previous version, but David Meyer has
> rewritten the whole package using grid functions.

Exactly and par() sets the base graphics parameters, not grid
parameters. You can use grid.layout() but it's a bit more work than with
par(). Look at 
  example("Ord_plot")
for an illustration.

> Figures are much nicer now, but documentations is a bit on the sparse
> side.

Yes...we're working on a set of vignettes, though. They are almost
done. But then again, we also used to claim that this was the case with
the new (now released) vcd for about two years. It's like the new
Kraftwerk album: it's always coming out `next summer' ;-)

Best,
Z

> I don't have the docs at
> hand currently, but I believe you should looks at strucplot to set the
> layout there, or use grid directly.
> 
> Greetings from an Ex-Cirebonese
> 
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From vincent at 7d4.com  Fri Sep 16 19:42:31 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Fri, 16 Sep 2005 19:42:31 +0200
Subject: [R] How do  I get the row indices?
In-Reply-To: <20050916173458.33907.qmail@web40522.mail.yahoo.com>
References: <20050916173458.33907.qmail@web40522.mail.yahoo.com>
Message-ID: <432B0407.2040002@7d4.com>

have a look at
which()
hih



From Achim.Zeileis at wu-wien.ac.at  Fri Sep 16 19:42:57 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 16 Sep 2005 19:42:57 +0200
Subject: [R] How do  I get the row indices?
In-Reply-To: <20050916173458.33907.qmail@web40522.mail.yahoo.com>
References: <20050916173458.33907.qmail@web40522.mail.yahoo.com>
Message-ID: <20050916194257.4fb02ffe.Achim.Zeileis@wu-wien.ac.at>

On Fri, 16 Sep 2005 10:34:57 -0700 (PDT) Martin Lam wrote:

> Hi,
> 
> I was wondering if it's possible to get the row
> numbers from a filtering. Here's an example:
> 
> # give me the rows with sepal.length == 6.2
> iris[(iris[,1]==6.2),]
> 
> # output
>     Sepal.Length Sepal.Width Petal.Length Petal.Width 
>   Species
> 69           6.2         2.2          4.5         1.5
> versicolor
> 98           6.2         2.9          4.3         1.3
> versicolor
> 127          6.2         2.8          4.8         1.8 
> virginica
> 149          6.2         3.4          5.4         2.3 
> virginica
> 
> What I really want is that it return the row numbers:
> 69, 98, 127, 149.

R> which(iris[,1]==6.2)
[1]  69  98 127 149

hth,
Z

> Thanks in advance,
> 
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From gavin.simpson at ucl.ac.uk  Fri Sep 16 19:49:32 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 16 Sep 2005 18:49:32 +0100
Subject: [R] How do  I get the row indices?
In-Reply-To: <20050916173458.33907.qmail@web40522.mail.yahoo.com>
References: <20050916173458.33907.qmail@web40522.mail.yahoo.com>
Message-ID: <1126892972.29827.73.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2005-09-16 at 10:34 -0700, Martin Lam wrote:
> Hi,
> 
> I was wondering if it's possible to get the row
> numbers from a filtering. Here's an example:
> 
> # give me the rows with sepal.length == 6.2
> iris[(iris[,1]==6.2),]
> 
> # output
>     Sepal.Length Sepal.Width Petal.Length Petal.Width 
>   Species
> 69           6.2         2.2          4.5         1.5
> versicolor
> 98           6.2         2.9          4.3         1.3
> versicolor
> 127          6.2         2.8          4.8         1.8 
> virginica
> 149          6.2         3.4          5.4         2.3 
> virginica
> 
> What I really want is that it return the row numbers:
> 69, 98, 127, 149.
> 
> Thanks in advance,
> 
> Martin

?which,

as in:

> which(iris[ , 1] == 6.2)
[1]  69  98 127 149

HTH

G

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From gunter.berton at gene.com  Fri Sep 16 19:49:32 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 16 Sep 2005 10:49:32 -0700
Subject: [R] How do  I get the row indices?
In-Reply-To: <20050916173458.33907.qmail@web40522.mail.yahoo.com>
Message-ID: <200509161749.j8GHnWV6026696@meitner.gene.com>

?row
row(iris)[iris[,1]==6.2]

##or better yet

?which
which(iris[,1]==6.2)

Note also that both may fail because the test may not be numerically exact
-- you may need to add fuzz.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Lam
> Sent: Friday, September 16, 2005 10:35 AM
> To: R
> Subject: [R] How do I get the row indices?
> 
> Hi,
> 
> I was wondering if it's possible to get the row
> numbers from a filtering. Here's an example:
> 
> # give me the rows with sepal.length == 6.2
> iris[(iris[,1]==6.2),]
> 
> # output
>     Sepal.Length Sepal.Width Petal.Length Petal.Width 
>   Species
> 69           6.2         2.2          4.5         1.5
> versicolor
> 98           6.2         2.9          4.3         1.3
> versicolor
> 127          6.2         2.8          4.8         1.8 
> virginica
> 149          6.2         3.4          5.4         2.3 
> virginica
> 
> What I really want is that it return the row numbers:
> 69, 98, 127, 149.
> 
> Thanks in advance,
> 
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From reid_huntsinger at merck.com  Fri Sep 16 19:50:26 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 16 Sep 2005 13:50:26 -0400
Subject: [R] How do  I get the row indices?
Message-ID: <355C35514FEAC9458F75947F5270974D076CD7@usctmx1103.merck.com>

You can use "which" on your subscript vector.

> which(iris[,1] == 6.2)
[1]  69  98 127 149

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Lam
Sent: Friday, September 16, 2005 1:35 PM
To: R
Subject: [R] How do I get the row indices?


Hi,

I was wondering if it's possible to get the row
numbers from a filtering. Here's an example:

# give me the rows with sepal.length == 6.2
iris[(iris[,1]==6.2),]

# output
    Sepal.Length Sepal.Width Petal.Length Petal.Width 
  Species
69           6.2         2.2          4.5         1.5
versicolor
98           6.2         2.9          4.3         1.3
versicolor
127          6.2         2.8          4.8         1.8 
virginica
149          6.2         3.4          5.4         2.3 
virginica

What I really want is that it return the row numbers:
69, 98, 127, 149.

Thanks in advance,

Martin

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From h.y.wong at leeds.ac.uk  Fri Sep 16 19:57:44 2005
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Fri, 16 Sep 2005 18:57:44 +0100
Subject: [R] Possible bug in lmer nested analysis with factors
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F740A0A4880@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A4880@dc1ex2.air.org>
Message-ID: <2D980A44-FD8D-4F87-9E01-955B66D012EC@leeds.ac.uk>


On 16 Sep 2005, at 17:12, Doran, Harold wrote:

> I think you might have confused lme code with lmer code. Why do you  
> have
> c/d in the random portion?

Apologies. I obviously have done something of the sort. I assumed  
that the 'random' assignment in lme could just be incorporated into  
an lmer call by placing it in brackets and removing the ~, so that

lme(a ~ b, random= ~ 1|c/d)

would be equivalent to

lmer(a ~ b + (1|c/d))

Is there a good guide somewhere to lmer calling conventions? I  
obviously don't understand them. As you can see, I would like to nest  
d within c, (and actually, c is nested in b too).

Perhaps it would be better with some explanation of the Crawley data.  
There are 3 fixed drug treatments ('b') given to 2 rats (6 rats in  
all: 'c'), and 3 samples ('d') are taken from each of the rat's  
livers, with some response variable recorded for each sample ('a':  
here just allocated a Normal distribution for testing purposes). I.e.  
c and d are random effects, with d %in% c and c %in% b.

He analyses it via
aov(a ~ b+c+d+Error(a/b/c))

I'm wondering what the lme and lmer equivalents are. I've been told  
that a reasonable form of analysis using lme is

a<-rnorm(36);b<-rep(1:3,each=12);d<-rep(1:3,each=2,6)
c <- rep(1:6, each=6) #use unique labels for each rat ## I got this  
wrong in my previous example
model1 <- lme(a ~ b, random= ~ 1|c/d)

Which gives what looks to be a reasonable output (but I'm new to all  
this mixed modelling stuff). How would I code the same thing using lmer?

> I think what you want is
>
>> lmer(a ~ b + (1 | c)+(1|d))
>>
>
> Which gives the following using your data

I'm not sure this is what I wanted to do. But thanks for the all the  
help.

Yan



From mschwartz at mn.rr.com  Fri Sep 16 20:05:38 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 16 Sep 2005 13:05:38 -0500
Subject: [R] How do  I get the row indices?
In-Reply-To: <20050916173458.33907.qmail@web40522.mail.yahoo.com>
References: <20050916173458.33907.qmail@web40522.mail.yahoo.com>
Message-ID: <1126893938.4239.21.camel@localhost.localdomain>

On Fri, 2005-09-16 at 10:34 -0700, Martin Lam wrote:
> Hi,
> 
> I was wondering if it's possible to get the row
> numbers from a filtering. Here's an example:
> 
> # give me the rows with sepal.length == 6.2
> iris[(iris[,1]==6.2),]
> 
> # output
>     Sepal.Length Sepal.Width Petal.Length Petal.Width 
>   Species
> 69           6.2         2.2          4.5         1.5
> versicolor
> 98           6.2         2.9          4.3         1.3
> versicolor
> 127          6.2         2.8          4.8         1.8 
> virginica
> 149          6.2         3.4          5.4         2.3 
> virginica
> 
> What I really want is that it return the row numbers:
> 69, 98, 127, 149.
> 
> Thanks in advance,
> 
> Martin

Martin,

First: Be very, very careful when performing exact equalities on
floating point numbers. They won't always result in the answer you
expect. For more information see R FAQ 7.31: Why doesn't R think these
numbers are equal?

Second:

See ?all.equal, ?sapply and ?which. Here is a possible vectorized
solution:

> which(sapply(iris[, 1], function(x) isTRUE(all.equal(x, 6.2))))
[1]  69  98 127 149


The above applies isTRUE(all.equal(x, 6.2)) for each element 'x' in 
iris[, 1], returning the indices of the TRUE results for the near
equality comparison, based upon the tolerance argument in all.equal().

HTH,

Marc Schwartz



From HDoran at air.org  Fri Sep 16 20:19:42 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 16 Sep 2005 14:19:42 -0400
Subject: [R] Possible bug in lmer nested analysis with factors
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A0A48BE@dc1ex2.air.org>

Doug Bates has the following article in R News. To date, it is the only
source I know of documenting the lmer function.

@Article{Rnews:Bates:2005,
  author       = {Douglas Bates},
  title	       = {Fitting Linear Mixed Models in {R}},
  journal      = {R News},
  year	       = 2005,
  volume       = 5,
  number       = 1,
  pages	       = {27--30},
  month	       = {May},
  url	       = {http://CRAN.R-project.org/doc/Rnews/},
} 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Yan Wong
Sent: Friday, September 16, 2005 1:58 PM
To: R-help
Subject: Re: [R] Possible bug in lmer nested analysis with factors


On 16 Sep 2005, at 17:12, Doran, Harold wrote:

> I think you might have confused lme code with lmer code. Why do you 
> have c/d in the random portion?

Apologies. I obviously have done something of the sort. I assumed that
the 'random' assignment in lme could just be incorporated into an lmer
call by placing it in brackets and removing the ~, so that

lme(a ~ b, random= ~ 1|c/d)

would be equivalent to

lmer(a ~ b + (1|c/d))

Is there a good guide somewhere to lmer calling conventions? I obviously
don't understand them. As you can see, I would like to nest d within c,
(and actually, c is nested in b too).

Perhaps it would be better with some explanation of the Crawley data.  
There are 3 fixed drug treatments ('b') given to 2 rats (6 rats in
all: 'c'), and 3 samples ('d') are taken from each of the rat's livers,
with some response variable recorded for each sample ('a':  
here just allocated a Normal distribution for testing purposes). I.e.  
c and d are random effects, with d %in% c and c %in% b.

He analyses it via
aov(a ~ b+c+d+Error(a/b/c))

I'm wondering what the lme and lmer equivalents are. I've been told that
a reasonable form of analysis using lme is

a<-rnorm(36);b<-rep(1:3,each=12);d<-rep(1:3,each=2,6)
c <- rep(1:6, each=6) #use unique labels for each rat ## I got this
wrong in my previous example
model1 <- lme(a ~ b, random= ~ 1|c/d)

Which gives what looks to be a reasonable output (but I'm new to all
this mixed modelling stuff). How would I code the same thing using lmer?

> I think what you want is
>
>> lmer(a ~ b + (1 | c)+(1|d))
>>
>
> Which gives the following using your data

I'm not sure this is what I wanted to do. But thanks for the all the
help.

Yan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jhallman at frb.gov  Fri Sep 16 20:32:00 2005
From: jhallman at frb.gov (jhallman@frb.gov)
Date: Fri, 16 Sep 2005 14:32:00 -0400
Subject: [R] Job opening at the Fed
Message-ID: <20050916183200.63A6F53583@mail.rsma.frb.gov>


The Money and Reserves Analysis section in the Division of Monetary Affairs at
the Federal Reserve Board in Washington has an opening for an Information
Systems Analyst (ISA).  The job listing can be found here: 

http://careers.peopleclick.com/jobposts/Client40_FRBOG/BU1/External/281-813.htm

This is not really a statistics position, although it does involve SAS (and
possibly R, see below) programming.  Much of the daily work will be in SAS and
shell scripts, and some SQL experience would also be helpful.  The retiring
analyst we are trying to replace worked mostly in SAS, but there is quite a
lot of R (and some Splus) code in the section, and because it is a small shop
we can be flexible about how things get done.  Make no mistake, whoever
we hire for this position will have to write and maintain some SAS programs,
but it is all Basic SAS (data step, procs sql, summary, print, etc...) and not
the complicated SAS AF GUI programming. (Of course, knowing the harder stuff
won't hurt your chances.)  Part of the job consists of running and maintaining
programs that clean up and extrapolate reported data in various ways, as well
as handling problems that occur.  

Over time I am moving to put more stuff in Smalltalk, R and relational
databases, so there will be opportunities to work with those environments if
you want to.  We will consider new graduates who can plausibly claim some SAS
experience through coursework. 

This is a nice place to work, with good people and a pretty good variety of
stuff to do.  Our section usually has about 22 members, about half of them
economists and three or four each of financial analysts, information systems
analysts, and research assistant.  Since the programming part of the shop is
quite small, and the section chief is not a programmer, there is quite a bit
of scope to tailor some of your work to suit your interests.   You do have to
be able to get along with colleagues and with sometimes-clueless users.  A
sense of humor goes a long way. 

Jeff Hallman
Senior Economist



From jhallman at frb.gov  Fri Sep 16 20:44:49 2005
From: jhallman at frb.gov (jhallman@frb.gov)
Date: Fri, 16 Sep 2005 14:44:49 -0400
Subject: [R] Pasting into Tk table widget?
Message-ID: <20050916184449.7FDBF53583@mail.rsma.frb.gov>

I'm working on a matrix editor using the Tk table widget, and it is almost
done.  It slices, dices, etc., and it can paste into Excel from an R session
running on Linux.  What I don't know how to do is paste from Excel into a Tk
table, and Google is not helping me.  Can anyone lend me a clue?

Jeff



From Ted.Harding at nessie.mcc.ac.uk  Fri Sep 16 20:58:05 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 16 Sep 2005 19:58:05 +0100 (BST)
Subject: [R] column-binary data
In-Reply-To: <6DD6C3FF939D934CBD916EAC13D581E30B9B4B@sbs-mercury4>
Message-ID: <XFMail.050916195805.Ted.Harding@nessie.mcc.ac.uk>

On 16-Sep-05 David Barron wrote:
> I have a number of datasets that are multipunch column-binary format. 
> Does anyone have any advice on how to read this into R?  Thanks.
> 
> David

Do you mean something like the old

HOLLERITH PUNCHED CARD BINARY FORMAT?
1111111110111111101111011111101111110
0000000001000000010000100000010000001
0000010100110000000010000001100010011
1111001010001010000000001100100101001
0111100100011001100001000100001101011
0100010000001100001010010101001110001
0100101000010101001100001010100101101

(here "1" = hole in card, binary representation of 7-bit ASCII
encoding, high-order bit on top).

If so, or if you precisely describe the binary format you have,
then the above or similar should be easy to get into R.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Sep-05                                       Time: 19:56:01
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Fri Sep 16 20:58:05 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 16 Sep 2005 19:58:05 +0100 (BST)
Subject: [R] Replicate
In-Reply-To: <2a9c000c05091607584b98def5@mail.gmail.com>
Message-ID: <XFMail.050916195805.Ted.Harding@nessie.mcc.ac.uk>

On 16-Sep-05 Jim Porzak wrote:
> Hi Marc,
> 
>> x = c(1,1,1,2,2,2,3,3,3,3)
>> unique(x)
> [1] 1 2 3
> 
> Being a database guy myself, it took me a while to think "unique"
> rather than "distinct"

If you were a Unix guy you would have thought of it at once (though
you would want to spell it "uniq" :).

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Sep-05                                       Time: 19:13:45
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Fri Sep 16 21:20:04 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Sep 2005 21:20:04 +0200
Subject: [R] Pasting into Tk table widget?
In-Reply-To: <20050916184449.7FDBF53583@mail.rsma.frb.gov>
References: <20050916184449.7FDBF53583@mail.rsma.frb.gov>
Message-ID: <x2br2svmzf.fsf@turmalin.kubism.ku.dk>

jhallman at frb.gov writes:

> I'm working on a matrix editor using the Tk table widget, and it is almost
> done.  It slices, dices, etc., and it can paste into Excel from an R session
> running on Linux.  What I don't know how to do is paste from Excel into a Tk
> table, and Google is not helping me.  Can anyone lend me a clue?

That should be completely independent of R, so perhaps ask on the Tcl
newsgroup? Or maybe ask the moodss maintainer who popped in on the r-devel
list a week or so back. Sounds like you are mixing operating systems
and that could be an issue I suppose.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From luke at novum.am.lublin.pl  Fri Sep 16 21:25:00 2005
From: luke at novum.am.lublin.pl (Lukasz Komsta)
Date: Fri, 16 Sep 2005 21:25:00 +0200
Subject: [R] De-data.fram-ize?
Message-ID: <432B1C0C.3040506@novum.am.lublin.pl>

Dear useRs,

Is there any more elegant way to convert dataframe to a vector of all 
its values than as.vector(as.matrix(x)) ? I did not have to do such 
conversion yet, so I am not sure... (of course as.vector() alone does 
not work).

Regards,

-- 
Lukasz Komsta
Department of Medicinal Chemistry
Medical University of Lublin
Jaczewskiego 4, 20-090 Lublin, Poland
Fax +48 81 7425165



From p.dalgaard at biostat.ku.dk  Fri Sep 16 21:36:53 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Sep 2005 21:36:53 +0200
Subject: [R] De-data.fram-ize?
In-Reply-To: <432B1C0C.3040506@novum.am.lublin.pl>
References: <432B1C0C.3040506@novum.am.lublin.pl>
Message-ID: <x27jdgvm7e.fsf@turmalin.kubism.ku.dk>

Lukasz Komsta <luke at novum.am.lublin.pl> writes:

> Dear useRs,
> 
> Is there any more elegant way to convert dataframe to a vector of all 
> its values than as.vector(as.matrix(x)) ? I did not have to do such 
> conversion yet, so I am not sure... (of course as.vector() alone does 
> not work).

unlist(x) should do it. It does spend a fair bit of effort in adding
names, so it could be less efficient.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From jholtman at gmail.com  Fri Sep 16 22:09:42 2005
From: jholtman at gmail.com (jim holtman)
Date: Fri, 16 Sep 2005 16:09:42 -0400
Subject: [R] column-binary data
In-Reply-To: <XFMail.050916195805.Ted.Harding@nessie.mcc.ac.uk>
References: <6DD6C3FF939D934CBD916EAC13D581E30B9B4B@sbs-mercury4>
	<XFMail.050916195805.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <644e1f3205091613096709c683@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050916/0517e8b4/attachment.pl

From mail at joeconway.com  Fri Sep 16 23:08:16 2005
From: mail at joeconway.com (Joe Conway)
Date: Fri, 16 Sep 2005 14:08:16 -0700
Subject: [R] maximum string length in RdbiPgSQL and in R
In-Reply-To: <4328819D.7060007@geo.umass.edu>
References: <4328819D.7060007@geo.umass.edu>
Message-ID: <432B3440.1040708@joeconway.com>

William McCoy wrote:
> library(RdbiPgSQL)
> conn <- dbConnect(PgSQL(), host = "localhost", dbname = "agdb")
> test.sql < readLines("queryfile")
> test.df <- dbGetQuery(conn, paste(test.sql, collapse = " "))
> 
> This works fine for all the multiline files I have tried -- except one.
> I have recently encountered a problem with a moderately complex, 
> moderately long query (12 lines, 459 characters).  I can execute the 
> query with no problem in psql and it returns the 14 rows that I expect. 
>   When I execute the query in R as above, I get a dataframe with the 
> expected column names, but no rows.  I get no error message.  I am 
> wondering if the query string is too long.  Is there a maximum length 
> for queries in RdbiPgSQL or for strings in R?

I tried using this for a "queryfile"

8<----------------
select
length(
'0123456789...repaeted for total length of 500...0123456789'
)
8<----------------

and it works fine for me:

8<----------------
 > conn <- dbConnect(PgSQL(),dbname="regression")
 > sql <- readLines("/tmp/queryfile")
 > df <- dbGetQuery(conn, paste(sql, collapse = " "))
 > df
   length
1    500
8<----------------

so I don't think length is the issue. Maybe you have an embedded control 
character? Or is it possible that you are introducing a space somewhere 
unexpected in your query, preventing a match? Try doing
   paste(test.sql, collapse = " ")
and then cut and paste the result into psql.

HTH,

Joe



From Ted.Harding at nessie.mcc.ac.uk  Fri Sep 16 23:31:52 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 16 Sep 2005 22:31:52 +0100 (BST)
Subject: [R] column-binary data
In-Reply-To: <644e1f3205091613096709c683@mail.gmail.com>
Message-ID: <XFMail.050916223152.Ted.Harding@nessie.mcc.ac.uk>

On 16-Sep-05 jim holtman wrote:
> Each card column had 12 rows, so as binary it comes in as 12 bits. The 
> question is does this come as a 16 bit integer, or a string of 12 bits
> that I have to extract from. Either case is not that difficult to do.

Indeed ... as an example of how one could proceed, I "deconstruct"
my example below (see at end).

> On 9/16/05, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote: 
>> 
>> On 16-Sep-05 David Barron wrote:
>> > I have a number of datasets that are multipunch column-binary
>> > format.
>> > Does anyone have any advice on how to read this into R? Thanks.
>> >
>> > David
>> 
>> Do you mean something like the old
>> 
>> HOLLERITH PUNCHED CARD BINARY FORMAT?
>> 1111111110111111101111011111101111110
>> 0000000001000000010000100000010000001
>> 0000010100110000000010000001100010011
>> 1111001010001010000000001100100101001
>> 0111100100011001100001000100001101011
>> 0100010000001100001010010101001110001
>> 0100101000010101001100001010100101101
>> 
>> (here "1" = hole in card, binary representation of 7-bit ASCII
>> encoding, high-order bit on top).

#First, construct a vector ASCII consiting of the printable
#characters:

ASCII<-c(" ","!","\"","#","$","%","&","'","(",")",
         "*","+",",","-",".","/","0","1","2","3",
         "4","5","6","7","8","9",":",";","<","=",
         ">","?","@","A","B","C","D","E","F","G",
         "H","I","J","K","L","M","N","O","P","Q",
         "R","S","T","U","V","W","X","Y","Z","[",
         "\\","]","^","_","`","a","b","c","d","e",
         "f","g","h","i","j","k","l","m","n","o",
         "p","q","r","s","t","u","v","w","x","y",
         "z","{","|","}","~")


#Next, a vector of powers of 2:

rad<-2^(6:0)


#Read in the data from stdin():

M<-t(matrix(as.integer(unlist((strsplit(scan(stdin(),
     what="character"),split="")))),ncol=7))

#(read 7 lines from stdin by copy&paste:
#1: 1111111110111111101111011111101111110
#2: 0000000001000000010000100000010000001
#3: 0000010100110000000010000001100010011
#4: 1111001010001010000000001100100101001
#5: 0111100100011001100001000100001101011
#6: 0100010000001100001010010101001110001
#7: 0100101000010101001100001010100101101
#8: 
#Read 7 items

#and convert the columns to ASCII codes:

R<-rad%*%M

#and see what you've got:

paste(ASCII[R-31],collapse="")

#[1] "HOLLERITH PUNCHED CARD BINARY FORMAT?"

The above can be adapted to whatever your binary data represent
and to how they are laid out in the input.

Others may find a slicker way of doing this.

The only fly in the above ointment is that I haven't located
in R a character-vector constant which consists of the printable
ASCII characters, or a function to convert numerical ASCII code
to characters, so I made my own.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Sep-05                                       Time: 22:26:16
------------------------------ XFMail ------------------------------



From iaingallagher at btopenworld.com  Sat Sep 17 00:15:49 2005
From: iaingallagher at btopenworld.com (Iain Gallagher)
Date: Fri, 16 Sep 2005 23:15:49 +0100
Subject: [R] horizontal lines on stripchart
Message-ID: <432B4415.1040804@btopenworld.com>

Hi.

I would like to add a horizontal line to each column of data on a 
stripchart (vertical=T) to indicate the mean(rather like the package 
Prism does - for those that have used this) for each dataset. I presume 
the best way to do this would be with "lines" but after much trying  / 
playing about I can't figure it out. Can anyone help?

Thanks



From jerome.lemaitre.1 at ulaval.ca  Sat Sep 17 01:20:23 2005
From: jerome.lemaitre.1 at ulaval.ca (jerome lemaitre)
Date: Fri, 16 Sep 2005 19:20:23 -0400
Subject: [R] Running glm in batch and exporting  results (AIC) to HTML
Message-ID: <000c01c5bb15$36a35bb0$6400a8c0@yourd93e6doqk7>

Dear all,

I'm doing univariate Poisson regressions using the "glm" and "glm.fit"
functions. I have 5 independent datasets and each dataset, has one response
variable and more than 20 factors to test.
Currently, I run one regression at a time and manually take notes of the
results in excel to have a quick overview on what is going on in my data. My
poor method is very time-consuming and I was looking for a faster and more
reliable way to do all the regressions.
I'm quite sure that R could do all of this for my but I can't think of a way
to tell it...

What I want R to do is
1) running one regression at a time in a particular dataset.
2) saving results. Here, I'm interested in AIC, Beta coefficient of the
factor, the z value and the p value of the factor
3) formatting results in a table with column names as follow:
factor; beta coefficient; z value; p value; aic
4) exporting the table in a way that I could read it in excel. In that way,
I would repeat the operation for each of the 5 datasets rather than for the
100 regressions.

I hope you could help me with this.
Thanks a lot for your answers.

J??r??me Lema??tre
Ph.D. student
Dpt biologie
Universit?? Laval
Qu??bec, Canada



From chrisb at fcdarwin.org.ec  Sat Sep 17 01:57:02 2005
From: chrisb at fcdarwin.org.ec (Chris Buddenhagen)
Date: Fri, 16 Sep 2005 17:57:02 -0600
Subject: [R] tickmarks on the inside on y axis and on the outside on the x
	axis
Message-ID: <000801c5bb1a$5536b680$4c01a8c0@Chris>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050916/c31c44ae/attachment.pl

From ripley at stats.ox.ac.uk  Fri Sep 16 11:34:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Sep 2005 10:34:38 +0100 (BST)
Subject: [R] Coefficients from LM
In-Reply-To: <loom.20050916T092027-385@post.gmane.org>
References: <4329d4e1.4864c655.2f19.7572@mx.gmail.com>
	<200509152136.j8FLaDsH027628@hypatia.math.ethz.ch>
	<loom.20050916T092027-385@post.gmane.org>
Message-ID: <Pine.LNX.4.61.0509161031110.1867@gannet.stats>

On Fri, 16 Sep 2005, Dieter Menne wrote:

> Charles Annis, P.E. <Charles.Annis <at> StatisticalEngineering.com> writes:
>
>> Here's an example with a glm; lm() works the same way but has fewer internal
>> objects.
>>
>> mod3 <- glm(tree ~ altitude, family = binomial)
>>
>> You can use names() to find out what's inside:
>>
>>> names(mod3)
>>  [1] "coefficients"      "residuals"         "fitted.values"     "effects"
>
> You get more information about the internals by using str(mod3) instead of names
> (mod3).

Well, yes for a specific object but not for what the function returns in 
general and what it means. In this case (and most others), the help page 
is the best source of information about the return value.

However, in general it is best to use the supplied accessor functions such 
as coef() and residuals(), and these are covered in the introductory 
accounts of model fitting in S/R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Sat Sep 17 02:33:49 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 16 Sep 2005 20:33:49 -0400
Subject: [R] tickmarks on the inside on y axis and on the outside on the
 x	axis
In-Reply-To: <000801c5bb1a$5536b680$4c01a8c0@Chris>
References: <000801c5bb1a$5536b680$4c01a8c0@Chris>
Message-ID: <432B646D.6060102@stats.uwo.ca>

Chris Buddenhagen wrote:
> Hi
> 
>  
> 
> I got both axis doing one or the other, but cannot make one do ticks on the
> inside while the other does it on the outside.

Use axis to draw them separately.  For example,

plot(rnorm(10),rnorm(10), axes=FALSE)
axis(1)
axis(2, tcl=0.5)
box()

You may also want to use the mgp parameter to axis to place the labels 
somewhere else.

Duncan Murdoch



From wdmccoy at geo.umass.edu  Sat Sep 17 02:43:10 2005
From: wdmccoy at geo.umass.edu (William D. McCoy)
Date: Fri, 16 Sep 2005 20:43:10 -0400
Subject: [R] maximum string length in RdbiPgSQL and in R
In-Reply-To: <432B3440.1040708@joeconway.com>
References: <4328819D.7060007@geo.umass.edu> <432B3440.1040708@joeconway.com>
Message-ID: <432B669E.3080709@geo.umass.edu>

Joe, Thanks, for your response.  A few hours ago I sent the following to 
others that had responded to my message on the bioconductor list:

Well I've think I've sorted this out.  First of all, all of the queries 
I have tested on RdbiPgSQL have been ones that worked with psql and I 
later found out they also all work fine with RODBC when sent to my 
PostgreSQL database.

As those who responded to my e-mail supposed, the length of the query 
string was not the problem.  And I found it doesn't matter if I type in 
the queries at the terminal or use readLines() to take the query from a 
file, so there is no problem with hidden characters, etc.

It turns out that the queries that failed in RdbiPgSQL (and worked fine 
in RODBC and psql) are those that used a "date" data type in the "where" 
clause.  Maybe this is a known limitation of RdbiPgSQL -- I hadn't seen 
that documented anywhere and I don't understand it since the query 
presumably is just sent to the database backend and the results should 
be returned.

I do notice that using RdbiPgSQL results in dataframes having columns 
with no attributes.  Whereas when I use RODBC the resulting dataframes 
have appropriate attributes such as class "factor" and class "date". But 
I still don't see why the results don't show up in my dataframe when a 
date field is used as a constraint in a "where" clause when using RdbiPgSQL.

By the way, I should have said this is with R 2.1.1, Rdbi 1.1.2, and 
RdbiPgSQL 1.1.4.

I think for now I will use RODBC.  It appears to be more robust, more 
useful (attribute-wise), and more versatile (should work with other 
databases).

I thank everyone for their help.

Bill


Joe Conway wrote:
> William McCoy wrote:
> 
>>library(RdbiPgSQL)
>>conn <- dbConnect(PgSQL(), host = "localhost", dbname = "agdb")
>>test.sql < readLines("queryfile")
>>test.df <- dbGetQuery(conn, paste(test.sql, collapse = " "))
>>
>>This works fine for all the multiline files I have tried -- except one.
>>I have recently encountered a problem with a moderately complex, 
>>moderately long query (12 lines, 459 characters).  I can execute the 
>>query with no problem in psql and it returns the 14 rows that I expect. 
>>  When I execute the query in R as above, I get a dataframe with the 
>>expected column names, but no rows.  I get no error message.  I am 
>>wondering if the query string is too long.  Is there a maximum length 
>>for queries in RdbiPgSQL or for strings in R?
> 
> 
> I tried using this for a "queryfile"
> 
> 8<----------------
> select
> length(
> '0123456789...repaeted for total length of 500...0123456789'
> )
> 8<----------------
> 
> and it works fine for me:
> 
> 8<----------------
>  > conn <- dbConnect(PgSQL(),dbname="regression")
>  > sql <- readLines("/tmp/queryfile")
>  > df <- dbGetQuery(conn, paste(sql, collapse = " "))
>  > df
>    length
> 1    500
> 8<----------------
> 
> so I don't think length is the issue. Maybe you have an embedded control 
> character? Or is it possible that you are introducing a space somewhere 
> unexpected in your query, preventing a match? Try doing
>    paste(test.sql, collapse = " ")
> and then cut and paste the result into psql.
> 
> HTH,
> 
> Joe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 

William D. McCoy
Geosciences
University of Massachusetts, Amherst
wdmccoy at geo.umass.edu



From spencer.graves at pdf.com  Sat Sep 17 04:55:54 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 16 Sep 2005 19:55:54 -0700
Subject: [R] regression with restrictions - optimization problem
In-Reply-To: <4321ADA4.6010704@t-online.de>
References: <4321ADA4.6010704@t-online.de>
Message-ID: <432B85BA.70307@pdf.com>

	  I have not seen any replies, so I will offer a comment:

	  1.  You speak of x1, x2, ..., x10, but your example includes only 
x1+x2+x3+x4.  I'm confused.  If you could still use help with this, 
could you please simplify your example further so there was only x1+x2, 
say?  Can you solve the problem with only x1?  If no, state your problem 
in terms only of x1.  The answer to that may include a fairly obvious 
generalization to x10.  If not, that can become a follow-on question.

	  2.  What is your objective function?  What do you want to maximize or 
minimize?  Are your y's ("med"?), e.g., some model plus normal error? 
If yes, then some kind of (nonlinear) regression might be appropriate. 
If no, then it might be best to start by writing an objective function. 
  If you have only one parameter to estimate to minimize the objective 
function, then you can just compute the objective function over a range 
for the parameter, create a plot, and be done.  If you want some 
refinement of that, please look at "optimize".  If you only two 
parameters, you can create contour plots, and use "optim" to refine the 
result.  For more unknowns, "optim" can be fairly useful.

	  Good luck.
	  spencer graves

Mark Hempelmann wrote:

> Dear WizaRds!
> 
> I am sorry to ask for some help, but I have come to a complete stop in 
> my efforts. I hope, though, that some of you might find the problem 
> quite interesting to look at.
> 
> I have been trying to estimate parameters for lotteries, the so called 
> utility of chance, i.e. the "felt" probability compared to a rational 
> given probability. A real brief example: Given is a lottery payoff 
> matrix of the type
> 
> x1    x2 ...     x10     median
> 1000    5000 ... 5000    3750
> 0    1000 ... 5000    2250 etc.
> 
> The actual data frame consists of 11 columns and 28 rows.
> 
> Each entry x1 ... x10 gives the amount of money resp. the utility of 
> that amount you receive playing the lottery. The probability for each 
> column is 10%. The median represents the empirical answers of players 
> where the person is indifferent if they prefer to receive the lottery or 
> the sum of money as a sure payoff.
> 
> I try to determine the probability people feel instead of the known 10% 
> probability of each column payoff entry. But here's the catch:
> 
> People also give different utilities to each amount of money, which 
> basically gives us some sort of function like this:
> u(x1...x10) = u(x1)*pi(p1) + u(x2)*pi(p2) +...+u(x10)*pi(p10)=y
> u() - unknown utility function
> pi() - unknown probability function
> y - empirical answer
> p1..p10 - probabilities, here always 0.1
> 
> To keep it simple, I set u(0)=0 and u(5000)=5000 and vary u(1000) 
> between a start and end point. On each cycle R computes the regression 
> coefficients that serve as the pi(p) estimators for every 10% step.
> Then I minimize the residual sum of squares which should give the best 
> estimators for every 10% step.
> 
> How can I possibly calculate a "smooth" pi(p) curve, a curve that should 
> look like an "S", plotted against the cumulative 10% probabilities? I 
> only have my ten estimators. How can I possibly tell R the necessary 
> restrictions of nonnegative estimators and their sum to equal one? Here 
> is my quite naive approach:
> 
> a70 <- matrix(c(1000,5000,5000,5000,2150, 0,1000,5000,5000,1750, 
> 0,0,1000,5000,1150, 0,0,0,1000,200, 1000,1000,5000,5000,2050, 
> 0,1000,1000,5000,1972), ncol=5, byrow=T)
> colnames(a70)=c(paste("x", 1:4, sep=""), "med")
> a70 <- as.data.frame(a70)
> 
> start=800; end=2000
> step=10; u1000=start-step
> 
> u1000 <- u1000+step # varying the 1000 entry
> a70[a70==1000] <- u1000
> reg70 <- lm(a70$med ~ -1+x1+x2+x3+x4, data=a70)
> res <- sum( (reg70$residuals^2) )
> 
> for (i in 1:( (end-start)/step) ){
>          a70[a70==u1000]    <- u1000+step
>      u1000 <- u1000+step
>      reg70 <- lm(a70$med ~ -1+x1+x2+x3+x4, data=a70)
>      if (res >= sum( (reg70$residuals^2) )) {
>          res <- sum( (reg70$residuals^2) )
>          print(paste("cycle", i, "u1000=", u1000, "RSS=", res))
>          final70 <- a70
>          finalreg <- reg70
>          }
> }
> print(final70)
> summary(finalreg)
> 
>      Maybe a better approach works with optim(stats) or dfp(Bhat), but I 
> have no idea how to correctly approach such a restricted optimization 
> problem.
>      Thank you su much for your help and support.
> 
> Mark Hempelmann
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sat Sep 17 05:12:34 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 16 Sep 2005 20:12:34 -0700
Subject: [R] how to do multiple comparisons in R?
In-Reply-To: <013401c5b736$fd49db00$d3c16880@DOS2333>
References: <20050624085859.4d2c5ce0@localhost.localdomain>	<Pine.LNX.4.61.0506240733200.25946@gannet.stats>
	<013401c5b736$fd49db00$d3c16880@DOS2333>
Message-ID: <432B89A2.3030903@pdf.com>

	  Have you received a reply yet?  I haven't seen one.

	  There are functions "TukeyHSD" and "p.adjust" in the "stats" package, 
plus "multcomp" and "multtest" packages.

	  If this is not an adequate answer, please submit another question 
(preferably after reading the posting guide! 
http://www.R-project.org/posting-guide.html).

	  spencer graves

Hongyu Sun wrote:

> Hi, Sorry I have to bother a question.
> 
> Does R have the functions to do lsd, tukey, bonferonni, contrast etc. like 
> in SAS?
> 
> Many thanks,
> 
> HS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From kostaskaris at hotmail.com  Sat Sep 17 09:36:16 2005
From: kostaskaris at hotmail.com (kostas karis)
Date: Sat, 17 Sep 2005 07:36:16 +0000
Subject: [R] Question:manipulating spatial data using combination of
	Maptools and Splancs
In-Reply-To: <Pine.LNX.4.44.0509161923180.7565-100000@reclus.nhh.no>
Message-ID: <BAY21-F2F76D27E061A265A61A93BC900@phx.gbl>

Thank you for your advice but i still have a problem...Everything goes fine 
till the plotting (as usual).When i try the code you posted for the 
conversion i get the following message:

Error in coordinates(quakes) : no direct or inherited method for function 
'coordinates' for this call

i don't have a clue what this means...

_________________________________________________________________
Don't just search. Find. Check out the new MSN Search!



From Roger.Bivand at nhh.no  Sat Sep 17 10:06:07 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 17 Sep 2005 10:06:07 +0200 (CEST)
Subject: [R] Question:manipulating spatial data using combination of
 Maptools and Splancs
In-Reply-To: <BAY21-F2F76D27E061A265A61A93BC900@phx.gbl>
Message-ID: <Pine.LNX.4.44.0509170959050.8255-100000@reclus.nhh.no>

On Sat, 17 Sep 2005, kostas karis wrote:

> Thank you for your advice but i still have a problem...Everything goes fine 
> till the plotting (as usual).When i try the code you posted for the 
> conversion i get the following message:
> 
> Error in coordinates(quakes) : no direct or inherited method for function 
> 'coordinates' for this call
> 
> i don't have a clue what this means...

Please always include the commands you gave, here in particular the
command(s) creating the quakes object - other list participants cannot see
your computer screen. All the error message says is that it is not a
SpatialPoints* object, so you have not done:

> quakes <- readShapePoints("quakes.shp")
> quakes_pts <- coordinates(quakes)

as I suggested. I also suggest moving this thread to the R-sig-geo list - 
subscription details on the "Spatial" Task View on CRAN:

http://cran.r-project.org/src/contrib/Views/Spatial.html

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ligges at statistik.uni-dortmund.de  Sat Sep 17 14:05:48 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 17 Sep 2005 14:05:48 +0200
Subject: [R] horizontal lines on stripchart
In-Reply-To: <432B4415.1040804@btopenworld.com>
References: <432B4415.1040804@btopenworld.com>
Message-ID: <432C069C.6010001@statistik.uni-dortmund.de>

Iain Gallagher wrote:
> Hi.
> 
> I would like to add a horizontal line to each column of data on a 
> stripchart (vertical=T) to indicate the mean(rather like the package 
> Prism does - for those that have used this) for each dataset. I presume 
> the best way to do this would be with "lines" but after much trying  / 
> playing about I can't figure it out. Can anyone help?
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html




Example adapted from ?stripchart:



with(OrchardSprays, {
     stripchart(decrease ~ treatment,
         main = "stripchart(Orchardsprays)", ylab = "decrease",
         vertical = TRUE)
     m <- tapply(decrease, treatment, mean)
     segments(1:nlevels(treatment)-0.25, m, 1:nlevels(treatment)+0.25, m)
})


Uwe Ligges



From lforzani at stat.umn.edu  Sat Sep 17 14:03:56 2005
From: lforzani at stat.umn.edu (lforzani)
Date: Sat, 17 Sep 2005 07:03:56 CDT
Subject: [R] fda
Message-ID: <200509171203.j8HC3uvp032309@trojan.software.umn.edu>

I would like to know where I can download the data Berkeley growth data
from the book S  functional data analysis user's guide, since I want to
perform some analysis in R.

Thanks. Liliana Forzani



From mirko.pham at gmx.de  Sat Sep 17 16:35:40 2005
From: mirko.pham at gmx.de (Mirko Pham)
Date: Sat, 17 Sep 2005 16:35:40 +0200
Subject: [R] multiple binary log regression with R?
Message-ID: <200509171435.j8HEZjXm026817@hypatia.math.ethz.ch>

Dear Group,

could anyone tell me how to perform a binary logistic regression with R? The
data includes 4 continuous independents and I aim at forward-, backward-, as
well as inclusion analyses. 

Does any other model than "glm" exist within R for this purpose? Does anyone
have R-experience with such or similar analysis?

Appreciate any help, thanks!

M. Pham

Department for Neuroradiology
University of W??rzburg
Josef-Schneider Str. 11
97080 Germany



From f.harrell at vanderbilt.edu  Sat Sep 17 16:46:42 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 17 Sep 2005 09:46:42 -0500
Subject: [R] multiple binary log regression with R?
In-Reply-To: <200509171435.j8HEZjXm026817@hypatia.math.ethz.ch>
References: <200509171435.j8HEZjXm026817@hypatia.math.ethz.ch>
Message-ID: <432C2C52.7050708@vanderbilt.edu>

Mirko Pham wrote:
> Dear Group,
> 
> could anyone tell me how to perform a binary logistic regression with R? The
> data includes 4 continuous independents and I aim at forward-, backward-, as
> well as inclusion analyses. 

There are many ways.  Why can't you use the documentation or search all 
the R resources to find out?

Note: Forward and backward stepwise will invalidate almost everything 
you are doing.  Don't know what is inclusion analysis.

Frank

> 
> Does any other model than "glm" exist within R for this purpose? Does anyone
> have R-experience with such or similar analysis?
> 
> Appreciate any help, thanks!
> 
> M. Pham
> 
> Department for Neuroradiology
> University of W??rzburg
> Josef-Schneider Str. 11
> 97080 Germany
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From bernarduse1 at yahoo.fr  Sat Sep 17 17:01:45 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Sat, 17 Sep 2005 17:01:45 +0200 (CEST)
Subject: [R] xyplot and abline
Message-ID: <20050917150145.52245.qmail@web25805.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050917/ca6c00d2/attachment.pl

From ripley at stats.ox.ac.uk  Sat Sep 17 17:36:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 17 Sep 2005 16:36:36 +0100 (BST)
Subject: [R] xyplot and abline
In-Reply-To: <20050917150145.52245.qmail@web25805.mail.ukl.yahoo.com>
References: <20050917150145.52245.qmail@web25805.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0509171632140.23948@gannet.stats>

On Sat, 17 Sep 2005, Marc Bernard wrote:

> Dear All,
>
> I wonderif there is a simple way to draw  a regression line in  the xyplot:
> more specifically, let:
>
> age <- c(20:30, 31:40 )
> age.cut <- cut(age, breaks = 2 )
> y<- rnorm(20)
> x <- rnorm(20,4,1)
>
> xyplot(y  ~  x| age.cut, xlab="x", ylab="y")
>
> How to draw (in the plot given by xyplot)  the two regression lines (y ~ 
> x) corresponding to the two category of age.cut ?

Use a panel function and call panel.lmline from it. Something like

mypanel <- function (x, y, ...)
{
    panel.xyplot(x,y, ...)
    panel.lmline(x,y, ...)
}

xyplot(y  ~  x| age.cut, xlab="x", ylab="y", panel=mypanel)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From h.wickham at gmail.com  Sat Sep 17 18:19:07 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 17 Sep 2005 11:19:07 -0500
Subject: [R] xyplot and abline
In-Reply-To: <20050917150145.52245.qmail@web25805.mail.ukl.yahoo.com>
References: <20050917150145.52245.qmail@web25805.mail.ukl.yahoo.com>
Message-ID: <f8e6ff0505091709195c0d6b25@mail.gmail.com>

> I wonderif there is a simple way to draw  a regression line in  the xyplot:

Try:

xyplot(y  ~  x| age.cut, xlab="x", ylab="y", type=c("p","r"))

Hadley



From vinum at iinet.net.au  Sun Sep 18 03:30:09 2005
From: vinum at iinet.net.au (John Charles Considine)
Date: Sun, 18 Sep 2005 09:30:09 +0800
Subject: [R] lm to an array
Message-ID: <1127007010.7386.14.camel@Tardis.considine.local>

I have an array of dimensions 4x8x15, eg,

, , WA

       1997 1998 1999 2000 2001 2002 2003 2004
actual   19   23   NA   40   62   65   64  100
minus1   NA   20   23   NA   42   57   79   84
minus2   NA   NA   21   27   NA   52   62   83
minus3   NA   NA   NA   24   30   NA   55   65

How do I call lm to evaluate 'minus1:minus3' against 'actual' such that
the results are tabulated by the third dimension?

John Charles Considine



From srini_iyyer_bio at yahoo.com  Sun Sep 18 05:11:14 2005
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Sat, 17 Sep 2005 20:11:14 -0700 (PDT)
Subject: [R] How to update R from ver 1.9.1 to  2
Message-ID: <20050918031114.27924.qmail@web31601.mail.mud.yahoo.com>

Dear group, 
 apologies if this is a stupid question.  I searched
CRAN sites. I am afraid I missed it.  
Can any one help me if I can update my windows version
of 1.9.1 to 2 or higer. 

thanks
srini



From ggrothendieck at gmail.com  Sun Sep 18 05:26:58 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 17 Sep 2005 23:26:58 -0400
Subject: [R] How to update R from ver 1.9.1 to 2
In-Reply-To: <20050918031114.27924.qmail@web31601.mail.mud.yahoo.com>
References: <20050918031114.27924.qmail@web31601.mail.mud.yahoo.com>
Message-ID: <971536df0509172026567c9919@mail.gmail.com>

1. Download and install the new version in the usual way.  You 
do not need to remove the old version.  Both can co-exist.
2. If you have *.site files copy them over to the corresponding
locations in the new R version.
3. Re-download any packages you want from CRAN from within
the new R.

On 9/17/05, Srinivas Iyyer <srini_iyyer_bio at yahoo.com> wrote:
> Dear group,
>  apologies if this is a stupid question.  I searched
> CRAN sites. I am afraid I missed it.
> Can any one help me if I can update my windows version
> of 1.9.1 to 2 or higer.
> 
> thanks
> srini
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dingjun_cn at yahoo.com  Sun Sep 18 05:45:37 2005
From: dingjun_cn at yahoo.com (Jun Ding)
Date: Sat, 17 Sep 2005 20:45:37 -0700 (PDT)
Subject: [R] Variable descriptions of a built-in dataset
In-Reply-To: <mailman.11.1126951201.11292.r-help@stat.math.ethz.ch>
Message-ID: <20050918034537.96749.qmail@web81002.mail.yahoo.com>

Hi, everyone, 
Is there a way to get the detailed variable
descriptions of a built-in dataset?

By using 
 attributes(Boston)
I can get the names of each variables of a builtin
dataset 'Boston'(in package "MASS"), but I don't know
what each variable means. Can some body tell me how I
can get more detailed information about each variable?
Thank you!

Jun



From spencer.graves at pdf.com  Sun Sep 18 07:19:18 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 17 Sep 2005 22:19:18 -0700
Subject: [R] help for linear-circular correlation
In-Reply-To: <15f8e67d0509120749537f819@mail.gmail.com>
References: <15f8e67d0509120749537f819@mail.gmail.com>
Message-ID: <432CF8D6.6090800@pdf.com>

	  Have you received a reply to your question?  I haven't seen one.

	  I got from Google a reference that gave a definition(a);  it doesn't 
seem like it would be hard to program.  However, before I did that, I 
think I would review the contents of the circular and CircStats 
packages.  After figuring out how the circular data are stored and 
manipulated in each package, including functions like "lm.circular" in 
package "circular".  Then I might write a function roughly like what I 
would like to have as part of one of those packages.

	  If you try this and get stuck, feel free to submit another post 
describing what you've tried and what you can't get to work;  you may 
also try an email to Claudio Agostinelli <claudio at unive.it>, the 
maintainer for both the circular and CircStat packages.  (If you try 
r-help again, the posting guide contains hints that might increase the 
speed and utility of replies; 
"http://www.R-project.org/posting-guide.html".)

	  Good Luck,
	  Spencer Graves

(a)
http://www.pstat.ucsb.edu/faculty/jammalam/html/research%20publication_files/article082003.pdf

ecoinfo wrote:

> Hi R-profs,
> 
> Maybe my question is a little off topic. Could any one tell me how to
> calculate a linear-circular correlation coefficient and its p-values?
> I had a quick look at circular and CircStats packages and did not find
> the related function.
> 
> Thanks for any kindly help.
> 
> Xiaohua
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Sun Sep 18 07:33:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 18 Sep 2005 01:33:15 -0400
Subject: [R] Variable descriptions of a built-in dataset
In-Reply-To: <20050918034537.96749.qmail@web81002.mail.yahoo.com>
References: <mailman.11.1126951201.11292.r-help@stat.math.ethz.ch>
	<20050918034537.96749.qmail@web81002.mail.yahoo.com>
Message-ID: <971536df05091722332347e2a1@mail.gmail.com>

?Boston

On 9/17/05, Jun Ding <dingjun_cn at yahoo.com> wrote:
> Hi, everyone,
> Is there a way to get the detailed variable
> descriptions of a built-in dataset?
> 
> By using
>  attributes(Boston)
> I can get the names of each variables of a builtin
> dataset 'Boston'(in package "MASS"), but I don't know
> what each variable means. Can some body tell me how I
> can get more detailed information about each variable?
> Thank you!
> 
> Jun
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From h.y.wong at leeds.ac.uk  Sun Sep 18 12:53:42 2005
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Sun, 18 Sep 2005 11:53:42 +0100
Subject: [R] Possible bug in lmer nested analysis with factors
In-Reply-To: <432AF0FD.1030001@pdf.com>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A4880@dc1ex2.air.org>
	<432AF0FD.1030001@pdf.com>
Message-ID: <A6DDFD3A-7C21-41F1-972E-1DB2597E9AB8@leeds.ac.uk>


On 16 Sep 2005, at 17:21, Sundar Dorai-Raj wrote:

> My guess is he wants this:
>
> c1 <- factor(c)
> d1 <- factor(d)
> m <- lmer(a ~ b + (1|c1:d1)+(1|c1))
>
> which assumes d1 is nested within c1.
>
> Take a look at Section 3 in the "MlmSoftRev" vignette:
>
> library(mlmRev)
> vignette("MlmSoftRev")

Ah, that vignette is extremely useful: it deserves to be more widely  
known.
I mainly intended this reply to be a thank you to yourself and Harold.

Unfortunately, there is (as always), one last thing that is still  
puzzling me:
the df for fixed factors seems to vary between what I currently  
understand to
be equivalent calls to lme and lmer, e.g:

-------

a<-rnorm(36);
b<-factor(rep(1:3,each=12))
c<-factor(rep(1:2,each=6,3))
d<-factor(rep(1:3,each=2,6))
c <- evalq(b:c)[,drop=T] #make unique factor levels
d <- evalq(c:d)[,drop=T] #make unique factor levels

summary(lme(a ~ b, random=~1|c/d))
#  output includes estimates for fixed effects such as
#                    Value Std.Error DF    t-value p-value
#  (Intercept)  0.06908901 0.3318330 18  0.2082041  0.8374
#  b2           0.13279084 0.4692828  3  0.2829655  0.7956
#  b3          -0.26146698 0.4692828  3 -0.5571630  0.6163

# I understand the above lme model to be equivalent to
summary(lmer(a ~ b + (1|c) +(1|c:d))
#but this gives fixed effects estimates with differing DF:
#               Estimate Std. Error DF t value Pr(>|t|)
#  (Intercept)  0.069089   0.331724 33  0.2083   0.8363
#  b2           0.132791   0.469128 33  0.2831   0.7789
#  b3          -0.261467   0.469128 33 -0.5573   0.5811

Again, many thanks for your help: even more so if you or anyone
else can answer this last little niggle of mine.

Yan



From bmw8042 at verizon.net  Sun Sep 18 15:44:15 2005
From: bmw8042 at verizon.net (bmw8042)
Date: Sun, 18 Sep 2005 09:44:15 -0400
Subject: [R] month increment for chron dates
Message-ID: <000701c5bc57$2b74d1e0$1abdfea9@atlantic>

I have a vector of over 7,000 chron dates in the format "mm/dd/yy".  I need
to increment each date in the vector by a standard number of months.
Lapply with seq.dates is working OK; this increments the vector x by 3
months:

dates(unlist(lapply(x, function(g) seq.dates(g, by="months", length=4)[4])))

But this takes about 55 seconds to run on a Windows XP 1.8 Pentium 512 RAM
PC with R version 2.0.0
Can you recommend a more efficient way to do this?

thanks for your advice

Brian Winkler
Controller
BPS Trade Services
CGI



From ggrothendieck at gmail.com  Sun Sep 18 16:42:52 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 18 Sep 2005 10:42:52 -0400
Subject: [R] month increment for chron dates
In-Reply-To: <000701c5bc57$2b74d1e0$1abdfea9@atlantic>
References: <000701c5bc57$2b74d1e0$1abdfea9@atlantic>
Message-ID: <971536df05091807426fa37325@mail.gmail.com>

I don't know if this is any faster but it does not use any loops or apply
so maybe it is.  x is assumed to be a vector chron dates.  Note that
the question is not well specified for days near the end of the month
and I have not addressed that.

with(month.day.year(x), {
	year <- ifelse(month > 9, year + 1, year)
	month <- ifelse(month > 9, month - 9, month + 3)
	chron(paste(month, day, year, sep = "/"))
} )


On 9/18/05, bmw8042 <bmw8042 at verizon.net> wrote:
> I have a vector of over 7,000 chron dates in the format "mm/dd/yy".  I need
> to increment each date in the vector by a standard number of months.
> Lapply with seq.dates is working OK; this increments the vector x by 3
> months:
> 
> dates(unlist(lapply(x, function(g) seq.dates(g, by="months", length=4)[4])))
> 
> But this takes about 55 seconds to run on a Windows XP 1.8 Pentium 512 RAM
> PC with R version 2.0.0
> Can you recommend a more efficient way to do this?
> 
> thanks for your advice
> 
> Brian Winkler
> Controller
> BPS Trade Services
> CGI
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmbates at gmail.com  Sun Sep 18 17:04:40 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 18 Sep 2005 10:04:40 -0500
Subject: [R] Possible bug in lmer nested analysis with factors
In-Reply-To: <A6DDFD3A-7C21-41F1-972E-1DB2597E9AB8@leeds.ac.uk>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A4880@dc1ex2.air.org>
	<432AF0FD.1030001@pdf.com>
	<A6DDFD3A-7C21-41F1-972E-1DB2597E9AB8@leeds.ac.uk>
Message-ID: <40e66e0b050918080436d7e2dc@mail.gmail.com>

On 9/18/05, Yan Wong <h.y.wong at leeds.ac.uk> wrote:
> 
> On 16 Sep 2005, at 17:21, Sundar Dorai-Raj wrote:
> 
> > My guess is he wants this:
> >
> > c1 <- factor(c)
> > d1 <- factor(d)
> > m <- lmer(a ~ b + (1|c1:d1)+(1|c1))
> >
> > which assumes d1 is nested within c1.
> >
> > Take a look at Section 3 in the "MlmSoftRev" vignette:
> >
> > library(mlmRev)
> > vignette("MlmSoftRev")
> 
> Ah, that vignette is extremely useful: it deserves to be more widely
> known.
> I mainly intended this reply to be a thank you to yourself and Harold.
> 
> Unfortunately, there is (as always), one last thing that is still
> puzzling me:
> the df for fixed factors seems to vary between what I currently
> understand to
> be equivalent calls to lme and lmer, e.g:
> 
> -------
> 
> a<-rnorm(36);
> b<-factor(rep(1:3,each=12))
> c<-factor(rep(1:2,each=6,3))
> d<-factor(rep(1:3,each=2,6))
> c <- evalq(b:c)[,drop=T] #make unique factor levels
> d <- evalq(c:d)[,drop=T] #make unique factor levels
> 
> summary(lme(a ~ b, random=~1|c/d))
> #  output includes estimates for fixed effects such as
> #                    Value Std.Error DF    t-value p-value
> #  (Intercept)  0.06908901 0.3318330 18  0.2082041  0.8374
> #  b2           0.13279084 0.4692828  3  0.2829655  0.7956
> #  b3          -0.26146698 0.4692828  3 -0.5571630  0.6163
> 
> # I understand the above lme model to be equivalent to
> summary(lmer(a ~ b + (1|c) +(1|c:d))
> #but this gives fixed effects estimates with differing DF:
> #               Estimate Std. Error DF t value Pr(>|t|)
> #  (Intercept)  0.069089   0.331724 33  0.2083   0.8363
> #  b2           0.132791   0.469128 33  0.2831   0.7789
> #  b3          -0.261467   0.469128 33 -0.5573   0.5811
> 
> Again, many thanks for your help: even more so if you or anyone
> else can answer this last little niggle of mine.

I'm coming to the discussion late and would also like to thank Sundar
and Harold for their explanations.

You are correct that good documentation of the capabilities of lmer
does not currently exist. lmer is still under active development and
documentation is spread in several places.  The vignette in the mlmRev
package explores some of the capabilities of lmer.  Also see the
examples in that package.

You are correct that the denominator degrees of freedom associated
with terms in the fixed effects is different between lme and lmer. 
Neither of them is "right" because there is no correct answer but the
values from lmer are definitely more wrong than the values from lme.
The problem is that lmer allows a wider range of models than does lme.
 In lme the grouping factors can only be nested.  You can fake crossed
grouping factors but you do need to fake them.  Lmer allows nested or
crossed or partially crossed grouping factors.  Most of the subtlety
in the design of lmer is to handle the case of partially crossed
grouping factors in large data sets (think of value-added models that
are applied to longitudinal data on students crossed with teachers in
schools within school districts within states ...).  Some arguments on
degrees of freedom can be made for nested grouping factors but the
question of testing fixed effects terms for models with partially
crossed grouping factors is difficult.  I am aware of the 1997
Biometrics paper by Kenward and Roger but I find it difficult to
translate their formulae into something I can evaluate.  Their
representation of a linear mixed model is as a generalized least
squares problem whereas lmer uses a penalized least squares
representation.  These are equivalent but sometimes the translations
back and forth are difficult to write down.

This area could be a very fruitful research area for people with
strong mathematical and implementation skills.  I have a partially
completed writeup on the details of the lmer representation and
implementation (the description of the linear mixed model is done -
I'm still working on the description of the generalized linear mixed
model and the nonlinear mixed model) which I could forward to anyone
interested in such a challenge.  There are two or three approaches
that could be used and I can provide some references.  An extensive
comparison of the properties of these approaches across a range of
real problems would be a valuable contribution to the literature but
it would involve a lot of work in implementation.

There are already some facilities for lmer models such as mcmcsamp and
simulate which can be used for evaluating the posterior distribution
of a single coefficient or for a parametric bootstrap of the reference
distribution of a quantity like the likelihood ratio statistic for a
hypothesis test.



From dmbates at gmail.com  Sun Sep 18 17:27:48 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 18 Sep 2005 10:27:48 -0500
Subject: [R] Possible bug in lmer nested analysis with factors
In-Reply-To: <A6DDFD3A-7C21-41F1-972E-1DB2597E9AB8@leeds.ac.uk>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A4880@dc1ex2.air.org>
	<432AF0FD.1030001@pdf.com>
	<A6DDFD3A-7C21-41F1-972E-1DB2597E9AB8@leeds.ac.uk>
Message-ID: <40e66e0b050918082734746d08@mail.gmail.com>

I have a couple of other comments.  You can write the nested grouping
factors as Sundar did without having to explicitly evaluate the
interaction term and drop unused levels.  The lmer function, like most
modeling functions in R, uses the optional argument drop.unused.levels
= TRUE when creating the model frame.

John Maindonald has already suggested the use of 

 (1|b/c) => (1|b:c) + (1|b)

as "syntactic sugar" for the lmer formula and it is a reasonable
request.  Unfortunately, implementing this is not high on my priority
list at present. (We just made a massive change in the sparse matrix
implementation in the Matrix package and shaking the bugs out of that
will take a while.)

In any case the general recommendation for nested grouping factors is
first to ensure that they are stored as factors and then to create the
sequence of interaction terms.

On 9/18/05, Yan Wong <h.y.wong at leeds.ac.uk> wrote:
> 
> On 16 Sep 2005, at 17:21, Sundar Dorai-Raj wrote:
> 
> > My guess is he wants this:
> >
> > c1 <- factor(c)
> > d1 <- factor(d)
> > m <- lmer(a ~ b + (1|c1:d1)+(1|c1))
> >
> > which assumes d1 is nested within c1.
> >
> > Take a look at Section 3 in the "MlmSoftRev" vignette:
> >
> > library(mlmRev)
> > vignette("MlmSoftRev")
> 
> Ah, that vignette is extremely useful: it deserves to be more widely
> known.
> I mainly intended this reply to be a thank you to yourself and Harold.
> 
> Unfortunately, there is (as always), one last thing that is still
> puzzling me:
> the df for fixed factors seems to vary between what I currently
> understand to
> be equivalent calls to lme and lmer, e.g:
> 
> -------
> 
> a<-rnorm(36);
> b<-factor(rep(1:3,each=12))
> c<-factor(rep(1:2,each=6,3))
> d<-factor(rep(1:3,each=2,6))
> c <- evalq(b:c)[,drop=T] #make unique factor levels
> d <- evalq(c:d)[,drop=T] #make unique factor levels
> 
> summary(lme(a ~ b, random=~1|c/d))
> #  output includes estimates for fixed effects such as
> #                    Value Std.Error DF    t-value p-value
> #  (Intercept)  0.06908901 0.3318330 18  0.2082041  0.8374
> #  b2           0.13279084 0.4692828  3  0.2829655  0.7956
> #  b3          -0.26146698 0.4692828  3 -0.5571630  0.6163
> 
> # I understand the above lme model to be equivalent to
> summary(lmer(a ~ b + (1|c) +(1|c:d))
> #but this gives fixed effects estimates with differing DF:
> #               Estimate Std. Error DF t value Pr(>|t|)
> #  (Intercept)  0.069089   0.331724 33  0.2083   0.8363
> #  b2           0.132791   0.469128 33  0.2831   0.7789
> #  b3          -0.261467   0.469128 33 -0.5573   0.5811
> 
> Again, many thanks for your help: even more so if you or anyone
> else can answer this last little niggle of mine.



From ligges at statistik.uni-dortmund.de  Sun Sep 18 18:12:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 18 Sep 2005 18:12:03 +0200
Subject: [R] [R-pkgs] tuneR_0.2-0
Message-ID: <432D91D3.6000007@statistik.uni-dortmund.de>

Announcement of a major revision of package tuneR

The package tuneR for the analysis of sound has been updated, improved,
and extended.
Most notably the functionality of the orphaned package
"sound" has been merged into tuneR. Because tuneR's representation and
API is very different, it was not possible to retain the "API" (i.e.
names of functions, argument ordering and expected objects) of package
"sound".

Uwe Ligges

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From dmbates at gmail.com  Sun Sep 18 18:19:42 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 18 Sep 2005 11:19:42 -0500
Subject: [R] plot spaghetti data
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F740A0A4837@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A4837@dc1ex2.air.org>
Message-ID: <40e66e0b050918091917323e61@mail.gmail.com>

To elaborate a bit on Harold's reply, the use of a "spaghetti plot"
for more than 4 or 5 groups is not recommended.  Consider the
longitudinal data "sleepstudy" in the Matrix package.  A spaghetti
plot can be produced by

> data(sleepstudy, package = "Matrix")
> library(lattice)
> xyplot(Reaction ~ Days, sleepstudy, groups = Subject, type = c('g','l'), aspect = 'xy')

Now compare this to the result of

> xyplot(Reaction ~ Days | Subject, sleepstudy, type = c('g','p','r'), aspect = 'xy')

which is a lattice plot with the data for each subject in a separate
panel.  Even better is to use

> xyplot(Reaction ~ Days | Subject, sleepstudy, type = c('g','p','r'), aspect = 'xy',
+        index.cond = function(x, y) coef(lm(y ~ x))[1])

where the panels are ordered according to a characteristic of the data
(the intercept for the per-subject regression line) rather than in the
more-or-less random order created by the subject number.


library(lattice)
xyplot(Reaction ~ Days

On 9/16/05, Doran, Harold <HDoran at air.org> wrote:
> There is interaction.plot(), but, the trellis graphics in lattice are
> much better. You might want to check these out.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Bernard
> Sent: Friday, September 16, 2005 8:58 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] plot spaghetti data
> 
> Dear All,
> 
> I  wonder if there is an R function to plot longitudinal data (spaghetti
> plots)...
> I've seen the function "is.longitudinal" but without any succes...
> 
> Thanks a lot in advance,
> 
> Bernard
> 
> 
> ---------------------------------
> 
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fbameul at wanadoo.fr  Sun Sep 18 19:19:46 2005
From: fbameul at wanadoo.fr (Franck Bameul)
Date: Sun, 18 Sep 2005 19:19:46 +0200
Subject: [R] How to test homogeneity of two covariance matrices?
Message-ID: <432DA1B2.4050701@wanadoo.fr>



From fbameul at wanadoo.fr  Sun Sep 18 19:42:25 2005
From: fbameul at wanadoo.fr (Franck Bameul)
Date: Sun, 18 Sep 2005 19:42:25 +0200
Subject: [R] How to test homogeneity of covariance matrices?
Message-ID: <432DA701.2020304@wanadoo.fr>

  Dear Group Members,

Forgive me if I am a little bit out of subject. I am looking for a good 
way to test the homogeneity of two variance-covariance matrices using R, 
prior to a Hotelling T² test. You’ll probably tell me that it is better 
to use a robust version of T², but I have no precise idea of the 
statistical behaviour of my variables, because they are parameters from 
the harmonics of Fourier series used to describe the outlines of 
specimens. I rather like to explore precisely these harmonics parameters.

It is known that Box’s M-test of homogeneity of variance-covariance 
matrices is oversensitive to heteroscedasticity and to deviation from 
multivariate normality and that it I not useful (Everitt, 2005 ; Seber, 
1984 ; Layard, 1974). I have tried a “quick and dirty” intuitive 
comparison between two covariance matrices and I am seeking the opinion 
of professional statisticians about this stuff. The idea is to compare 
the two matrices using the absolute value of their difference, then to 
make a quadratic form using a unity vector and its transpose. One obtain 
a scalar that must be close to zero if the two covariance matrices are 
homogeneous :

Let S1 and S2 be two variance-covariance matrices of dimension n,

Let a be a vector of n ones : a <- rep(1, times = n)

b = a’ * |S1 – S2| * a, i.e. in R:

b <- a %*% abs(S1 – S2) %*% a

Is b distributed following a chi-square distribution? Is this idea total 
crap? Did someone tried this before and published something?

My data gave two 77 x 77 covariance matrices and b = 0.003243, a value 
close to 0, hence I expect my two covariance matrices are homogeneous. 
Am I right?

If this comparison is incorrect, could someone suggest a useful way to 
make this comparison using R?

Thank you in advance for your comments.

Franck

_______________________________

Dr Franck BAMEUL

Le Clos d'Ornon
7 rue Frédéric Mistral
F-33140 VILLENAVE D'ORNON
France

fbameul at wanadoo.fr

06 89 88 16 73 (personnel)
05 57 19 57 20 (professionnel)
05 57 19 57 27 (fax)



From horebeek at cimat.mx  Sun Sep 18 20:51:25 2005
From: horebeek at cimat.mx (horebeek@cimat.mx)
Date: Sun, 18 Sep 2005 13:51:25 -0500 (CDT)
Subject: [R] replayPlot in loop
Message-ID: <61868.201.128.245.2.1127069485.squirrel@201.128.245.2>


Hi,

In order to make a movie-like animation of different graphs
with replayPlot inside a fast loop:

is there a way to avoid the appearance of some
white stripes/streaks between the different calls to replayPlot?

thanks in advance
     Johan VH



From VINOD at FORDHAM.EDU  Sun Sep 18 21:37:08 2005
From: VINOD at FORDHAM.EDU (VINOD@FORDHAM.EDU)
Date: Sun, 18 Sep 2005 15:37:08 -0400
Subject: [R] trimmed mean in R seems to round the trimming fraction
Message-ID: <OFE45CA777.2D13C7CC-ON85257080.006BC520-85257080.006BC52B@fordham.edu>




subject: trimmed mean in R seems to round the trimming fraction
to r-help at stat.math.ethz.ch.

Consider the following example of 10 numbers.  10% trimmed mean is correct
but you can see that the result is the same for many trimming fractions
till 0.20!

For example 13% trimmed mean should use interpolation of second and
eighth ordered observation.  R does not seem to do this.
The correct 13% trimmed mean is the average of 7.4 middle observations
(28+46+50+52+54+63+82+64.4)/7.4
Answer=(439.4)/7.4= 59.3784


> z=c(23, 40,     46,      50,      52,      54,    63,    82,92,98)
> mean(z, trim=0.10)
[1] 59.875
> mean(z, trim=0.13) # wrong result is  59.875
[1] 59.875
> mean(z, trim=0.15)
[1] 59.875
> mean(z, trim=0.18)
[1] 59.875
> mean(z, trim=0.1999999)
[1] 59.875
> mean(z, trim=0.20)
[1] 57.83333
> mean(z, trim=0.40)
[1] 53


Hrishikesh D. Vinod
Professor of Economics, Fordham University
E-Mail: Vinod at fordham.edu  Tel 718-817-4065,
Secretary 718-817-4048, Fax 718-817-3518
Web page:  http://www.fordham.edu/economics/vinod



From sundar.dorai-raj at pdf.com  Sun Sep 18 22:11:11 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sun, 18 Sep 2005 15:11:11 -0500
Subject: [R] trimmed mean in R seems to round the trimming fraction
In-Reply-To: <OFE45CA777.2D13C7CC-ON85257080.006BC520-85257080.006BC52B@fordham.edu>
References: <OFE45CA777.2D13C7CC-ON85257080.006BC520-85257080.006BC52B@fordham.edu>
Message-ID: <432DC9DF.5070706@pdf.com>



VINOD at FORDHAM.EDU wrote:
> 
> 
> subject: trimmed mean in R seems to round the trimming fraction
> to r-help at stat.math.ethz.ch.
> 
> Consider the following example of 10 numbers.  10% trimmed mean is correct
> but you can see that the result is the same for many trimming fractions
> till 0.20!
> 
> For example 13% trimmed mean should use interpolation of second and
> eighth ordered observation.  R does not seem to do this.
> The correct 13% trimmed mean is the average of 7.4 middle observations
> (28+46+50+52+54+63+82+64.4)/7.4
> Answer=(439.4)/7.4= 59.3784
> 
> 
> 
>>z=c(23, 40,     46,      50,      52,      54,    63,    82,92,98)
>>mean(z, trim=0.10)
> 
> [1] 59.875
> 
>>mean(z, trim=0.13) # wrong result is  59.875
> 
> [1] 59.875
> 
>>mean(z, trim=0.15)
> 
> [1] 59.875
> 
>>mean(z, trim=0.18)
> 
> [1] 59.875
> 
>>mean(z, trim=0.1999999)
> 
> [1] 59.875
> 
>>mean(z, trim=0.20)
> 
> [1] 57.83333
> 
>>mean(z, trim=0.40)
> 
> [1] 53
> 
> 

Hi,

The "trim" argument works as documented. Did you read ?mean:

      If 'trim' is non-zero, a symmetrically trimmed mean is computed
      with a fraction of 'trim' observations deleted from each end
      before the mean is computed.

Also, I don't recall ever learning your method for trimmed mean, but R 
is a programming language and you can always write your own trimmed mean 
function.

HTH,

--sundar



From gerifalte28 at hotmail.com  Mon Sep 19 01:10:18 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Sun, 18 Sep 2005 23:10:18 +0000
Subject: [R] Variable descriptions of a built-in dataset
In-Reply-To: <971536df05091722332347e2a1@mail.gmail.com>
Message-ID: <BAY103-F382CCF38CE263107FF7632A6930@phx.gbl>

Or the longer version help(Boston)

Cheers

Francisco

>From: Gabor Grothendieck <ggrothendieck at gmail.com>
>Reply-To: ggrothendieck at gmail.com
>To: Jun Ding <dingjun_cn at yahoo.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Variable descriptions of a built-in dataset
>Date: Sun, 18 Sep 2005 01:33:15 -0400
>
>?Boston
>
>On 9/17/05, Jun Ding <dingjun_cn at yahoo.com> wrote:
> > Hi, everyone,
> > Is there a way to get the detailed variable
> > descriptions of a built-in dataset?
> >
> > By using
> >  attributes(Boston)
> > I can get the names of each variables of a builtin
> > dataset 'Boston'(in package "MASS"), but I don't know
> > what each variable means. Can some body tell me how I
> > can get more detailed information about each variable?
> > Thank you!
> >
> > Jun
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Mon Sep 19 04:32:00 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 18 Sep 2005 19:32:00 -0700
Subject: [R] Random effect model
In-Reply-To: <1885.128.240.6.80.1126713664.squirrel@sws2.ncl.ac.uk>
References: <1885.128.240.6.80.1126713664.squirrel@sws2.ncl.ac.uk>
Message-ID: <432E2320.2050306@pdf.com>

	  I have not seen a reply to this, so I will offer a comment.

	  1.  You are doing something very wise in reading Pinheiro and Bates 
(2000) Mixed-Effects Models in S and S-Plus.

	  2.  Unfortunately, beyond this, I don't really understand the 
question;  if others had similar difficulties, it might explain why I 
have not seen a reply.

	  3.  There is a famous book by Polya on "How to Solve It".  I regard 
the "posting guide" (www.R-project.org/posting-guide.html) as similar to 
Polya's famous book:  The posting guide can help you prepare a question 
for this group in such a way that will increase the chances of receiving 
a prompt reply.  One important suggestion is to prepare a self-contained 
toy example that someone else can copy from an email into R and test a 
few ideas in a minute or so.  Often, people find answers to their own 
questions while working through the posting guide.  Failing that, the 
resulting questions are often much easier for someone else to understand.

	  4.  Have you tried the "RSiteSearch" function?  Many questions have 
been asked and answered about lme, and this function might help you find 
what you want in a response to a previous question.

	  Good Luck.
	  spencer graves

Hathaikan Chootrakool wrote:

> Dear R-help group,
> 
> I would like to model directly following random effect model:
> 
>   Y_ik = M_ik +  E_ik  where M_ik  ~ N(Mew_k,tau_k^2)
>                              E_ik  ~ N(0,s_ik^2)
>   i = number of study
>   k = number of treatment
> ---------------------------------------------------------------------------
> 
> I have practiced using the command  from 'Mixed -Effects models in S and
> S-plus' as follow
> 
> fm1logit.lme <- lme(logitp~1, data=logit, random = ~1|factor(Tr))
> 
> It can be written in this model
> 
> Y_ik = Mew + B_i + E_ik  where M_i ~ N(0,sigma_b^2)
>                                E_ik ~ N(0,sigma^2)
> 
> 
>  but it is not the same what my model is.
> 
> 
> Could somebody please point me in the right direction ?
> 
>  Sorry if this turns out to be an extreamly simple question, I'm a
>  new user to R.
> 
>  Thank you very much,
> 
>  Ae
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Narcyz.Ghinea at swsahs.nsw.gov.au  Mon Sep 19 04:38:27 2005
From: Narcyz.Ghinea at swsahs.nsw.gov.au (Narcyz Ghinea)
Date: Mon, 19 Sep 2005 12:38:27 +1000
Subject: [R] Extended Hypergeometric Distribution
Message-ID: <588D8BDAAC0BEB4B82DA2CC5AEBA899C3F7A0B@isdex001.intra.swsahs.nsw.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050919/e8997f0f/attachment.pl

From spencer.graves at pdf.com  Mon Sep 19 05:12:10 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 18 Sep 2005 20:12:10 -0700
Subject: [R] How to sort data sets?
In-Reply-To: <20050915101818.1453.qmail@web40525.mail.yahoo.com>
References: <20050915101818.1453.qmail@web40525.mail.yahoo.com>
Message-ID: <432E2C8A.4030405@pdf.com>

?order

Martin Lam wrote:

> Hi,
> 
> I was wondering if someone know how to sort a data set
> by column.
> I've tried sort() but without luck. I would think
> there should be a function for it somewhere. An
> example with the iris data set would be appreciated.
> 
> Thanks,
> 
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From krcabrer at epm.net.co  Mon Sep 19 05:53:16 2005
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Sun, 18 Sep 2005 22:53:16 -0500
Subject: [R] How to sort data sets?
In-Reply-To: <432E2C8A.4030405@pdf.com>
References: <20050915101818.1453.qmail@web40525.mail.yahoo.com>
	<432E2C8A.4030405@pdf.com>
Message-ID: <432E362C.3020208@epm.net.co>

If you want to sort a data frame according a variable, the use it like

datfram[order(datfram$var),]

to sort all the data.frame, not just a variable

Spencer Graves wrote:

>?order
>
>Martin Lam wrote:
>
>  
>
>>Hi,
>>
>>I was wondering if someone know how to sort a data set
>>by column.
>>I've tried sort() but without luck. I would think
>>there should be a function for it somewhere. An
>>example with the iris data set would be appreciated.
>>
>>Thanks,
>>
>>Martin
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>    
>>
>
>  
>

From mjf at ansto.gov.au  Mon Sep 19 06:59:07 2005
From: mjf at ansto.gov.au (FISCHER, Matthew)
Date: Mon, 19 Sep 2005 14:59:07 +1000
Subject: [R] graph tick label size
Message-ID: <283982AD9F3CD211B3AC00A0C983032F11443624@paradise.ansto.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050919/7b7fe592/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Sep 19 08:24:53 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 19 Sep 2005 08:24:53 +0200
Subject: [R] replayPlot in loop
In-Reply-To: <61868.201.128.245.2.1127069485.squirrel@201.128.245.2>
References: <61868.201.128.245.2.1127069485.squirrel@201.128.245.2>
Message-ID: <432E59B5.9000300@statistik.uni-dortmund.de>

horebeek at cimat.mx wrote:

> Hi,
> 
> In order to make a movie-like animation of different graphs
> with replayPlot inside a fast loop:
> 
> is there a way to avoid the appearance of some
> white stripes/streaks between the different calls to replayPlot?


In principle it is device dependend and the devices are generally not 
designed to display movies.
Also drawing of a plot might take a considerable amount of time.

Uwe Ligges



> thanks in advance
>      Johan VH
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Sep 19 09:16:38 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 19 Sep 2005 09:16:38 +0200
Subject: [R] graph tick label size
In-Reply-To: <283982AD9F3CD211B3AC00A0C983032F11443624@paradise.ansto.gov.au>
References: <283982AD9F3CD211B3AC00A0C983032F11443624@paradise.ansto.gov.au>
Message-ID: <432E65D6.3000806@statistik.uni-dortmund.de>

FISCHER, Matthew wrote:

> Hi R-users,
> 
> 
>     I'm running R under Unix and producing postscript output of graphs.
> Soemtimes, some tick axis labels disappear from my output.
>  eg if I have a vector
> 
> months <- c("J","F","M","A","M","J","J","A","S","O","N","D")
> 
> Then the "M" and one or 2 other letters are dropped from the axis.
> 
> This seems to be a size problem, but the spacing looks fine to me (all
> the letters could easily be seen if they were all there).  So my
> question is, can I override what R is doing to the tick labels, without making the
> labels smaller?  I searched the help files but couldn't find anything specifically on this.


Not directly, but you can fake by plotting the stuff in two chunks as in:

plot(1:12, xaxt="n")
months <- c("J","F","M","A","M","J","J","A","S","O","N","D")
temp <- seq(1, 12, 2)
axis(1, temp, labels=months[temp])
temp <- seq(2, 12, 2)
axis(1, temp, labels=months[temp])

Uwe Ligges



> cheers,
> 
> Matt.
> 
> Dr Matt Fischer
> ANSTO - Institute for Nuclear Geophysiology 
> PMB 1 Menai NSW 2234
> Ph: +61 2 9717 9686
> Fax: +61 2 9717 3599
> Mobile: 0428 363 146
> http://www.ansto.gov.au/nugeo/
> http://ipilps.ansto.gov.au/
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From snvk4u at gmail.com  Mon Sep 19 09:26:06 2005
From: snvk4u at gmail.com (Krishna)
Date: Mon, 19 Sep 2005 12:56:06 +0530
Subject: [R] minimal hedge variance ratio
Message-ID: <139ef1c2050919002679e93df0@mail.gmail.com>

Hi all

i have two data sets, spot and futures cash market prices. to estimate
the minimum variance hedge ratio, i first had a glance on the
correlation coefficient of relative price change (ln(St / St-1).
surprizingly the value is just 0.2 compared to actual price
correlation of 0.9. (i did regress the spot change on future change,
co-effi is 0.3, and R2 is only 0.025

a) in such scenario can someone help me in estimating the ratio which
are time varying.
b) is there a way to define the function as the correlation will work
at given level of basis (futures - spot).

thank u for the help and co-operation

rgds

snvk



From ripley at stats.ox.ac.uk  Mon Sep 19 09:33:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Sep 2005 08:33:13 +0100 (BST)
Subject: [R] How to update R from ver 1.9.1 to  2
In-Reply-To: <20050918031114.27924.qmail@web31601.mail.mud.yahoo.com>
References: <20050918031114.27924.qmail@web31601.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0509190831500.21564@gannet.stats>

See the rw-FAQ ..., Q2.8 in the latest version.

On Sat, 17 Sep 2005, Srinivas Iyyer wrote:

> Dear group,
> apologies if this is a stupid question.  I searched
> CRAN sites. I am afraid I missed it.
> Can any one help me if I can update my windows version
> of 1.9.1 to 2 or higer.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From astrzelczak at ps.pl  Mon Sep 19 09:41:41 2005
From: astrzelczak at ps.pl (astrzelczak@ps.pl)
Date: Mon, 19 Sep 2005 09:41:41 +0200
Subject: [R] indicator value in labdsv
Message-ID: <1127115701.432e6bb532815@www.ps.pl>



Hi,

I'm trying to find out what threshold of indicator value in labadsv should be
used to accept a specie as an indicator one? So far I assumed that indval=0.5
is high enough to avoid any mistakes but it was based only in my intuition.

I'd be greatful for any advise

best regards

Agnieszka



From r.hankin at noc.soton.ac.uk  Mon Sep 19 10:08:33 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 19 Sep 2005 09:08:33 +0100
Subject: [R] distance to eye in persp()
Message-ID: <00EF8E24-1DE9-4790-9CDA-68968F091965@soc.soton.ac.uk>

Hi

the manpage for persp() has a wonderful section where a the trans3d 
()  function
is used with points() and lines() to add red dots and a green sinusoid
to the Mexican hat surface.

Does anyone have a way to tell what distance  a point is from the eye/ 
camera?

Take the following line:

lines (trans3d(x, y=10, z= 6 + sin(x), pm = res), col = 3)

Is there a function like trans3d() that returns a vector of distances  
from
the x,y,z point to the camera?  I want this so I can plot clouds of  
points
with the further ones in smaller plotsizes, and perhaps even fading  
to white
(as though viewed through fog).





--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From samir at guidi.ch  Mon Sep 19 11:27:14 2005
From: samir at guidi.ch (samir@guidi.ch)
Date: Mon, 19 Sep 2005 11:27:14 +0200
Subject: [R] dynamic object names?
Message-ID: <1127122034.432e847284e26@www.mail2web.ch>


I am trying to extract data from a matrix. Let's say that i am interested in
extracting 
rows from a 4x4 matrix. Instead of giving a fix name to these 4 rows I would
like to add a number to prefix. As result I should get 4 objects named: 

prefix_1 
prefix_2 
prefix_3
prefix_4

I attepted to solve the problem with a loop, but without success. Any hints??

> matrix(LETTERS[1:16], ncol=4) -> MM
> MM
     [,1] [,2] [,3] [,4]
[1,] "A"  "E"  "I"  "M" 
[2,] "B"  "F"  "J"  "N" 
[3,] "C"  "G"  "K"  "O" 
[4,] "D"  "H"  "L"  "P" 


for (xxx in 1:4) {
MM[xxx,] -> prefix_xxx;
}


Thanks

samir



From ligges at statistik.uni-dortmund.de  Mon Sep 19 11:38:02 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 19 Sep 2005 11:38:02 +0200
Subject: [R] dynamic object names?
In-Reply-To: <1127122034.432e847284e26@www.mail2web.ch>
References: <1127122034.432e847284e26@www.mail2web.ch>
Message-ID: <432E86FA.3020101@statistik.uni-dortmund.de>

samir at guidi.ch wrote:

> I am trying to extract data from a matrix. Let's say that i am interested in
> extracting 
> rows from a 4x4 matrix. Instead of giving a fix name to these 4 rows I would
> like to add a number to prefix. As result I should get 4 objects named: 
> 
> prefix_1 
> prefix_2 
> prefix_3
> prefix_4
> 
> I attepted to solve the problem with a loop, but without success. Any hints??


1. In fact, you do not really want it.
2. You want to use "<-" rather than "->" for assigments. Reading your 
code is really hard when you are using this non-conventional way.
3. In order to answer your question: This is the FAQ "How can I turn a 
string into a variable?". The posting guide asks you to read these FAQs 
before sending questions to R-help. Please do so.

Uwe Ligges




> 
>>matrix(LETTERS[1:16], ncol=4) -> MM
>>MM
> 
>      [,1] [,2] [,3] [,4]
> [1,] "A"  "E"  "I"  "M" 
> [2,] "B"  "F"  "J"  "N" 
> [3,] "C"  "G"  "K"  "O" 
> [4,] "D"  "H"  "L"  "P" 
> 
> 
> for (xxx in 1:4) {
> MM[xxx,] -> prefix_xxx;
> }
> 
> 
> Thanks
> 
> samir
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Mon Sep 19 11:48:31 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 19 Sep 2005 10:48:31 +0100 (BST)
Subject: [R] distance to eye in persp()
In-Reply-To: <00EF8E24-1DE9-4790-9CDA-68968F091965@soc.soton.ac.uk>
Message-ID: <XFMail.050919101803.Ted.Harding@nessie.mcc.ac.uk>


On 19-Sep-05 Robin Hankin wrote:
> Hi
> 
> the manpage for persp() has a wonderful section where a the
> trans3d()  function is used with points() and lines() to add
> red dots and a green sinusoid to the Mexican hat surface.
> 
> Does anyone have a way to tell what distance  a point is from
> the eye/camera?
> 
> Take the following line:
> 
> lines (trans3d(x, y=10, z= 6 + sin(x), pm = res), col = 3)
> 
> Is there a function like trans3d() that returns a vector of
> distances from the x,y,z point to the camera?  I want this so
> I can plot clouds of points with the further ones in smaller
> plotsizes, and perhaps even fading to white (as though viewed
> through fog).

Wonderfully put! That's what statistics is about!

I think you may have to write your own. This is possible given
the values for the parameters xlim, ylim, zlim, r, theta, phi
(default as defined in ?persp, or explicitly user-defined),
since you can then determine the 3D coordinates of the "Eye"
relative to the (X,Y,Z) axes being plotted, after which the
distance to a particular (x,y,z) point is trivial.

E.g.

1. Coordinates of Eye relative to the centre of the box

   xE <- r*sin(theta + pi)*cos(phi)
   yE <- r*cos(theta + pi)*cos(phi)
   zE <- r*sin(phi)

2. Centre of box relative to real (0,0,0)

   xC <- mean(xlim); yC <-mean(ylim); xC <- mean(zlim)

3. Coordinates of (x,y,z) relative to Eye

   x1 <- x - xE - xC; y1 <- y - yE - yC; z1 <- z - zE - zC

4. Distance from Eye to (x,y,z)

   d = sqrt(x1^2 + y1^2 + z1^2)

(Hoping I've not got anything the wrong way round there!)

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 19-Sep-05                                       Time: 10:17:55
------------------------------ XFMail ------------------------------



From pearce.rachel at gmail.com  Mon Sep 19 11:59:15 2005
From: pearce.rachel at gmail.com (Rachel Pearce)
Date: Mon, 19 Sep 2005 10:59:15 +0100
Subject: [R] Problem with tick marks in lines.survfit (package survival)
Message-ID: <59541412050919025937fbff56@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050919/36076a85/attachment.pl

From A.Brennan at sheffield.ac.uk  Mon Sep 19 12:09:18 2005
From: A.Brennan at sheffield.ac.uk (A.Brennan)
Date: Mon, 19 Sep 2005 11:09:18 +0100
Subject: [R] Integrate functions with loops
In-Reply-To: <432AE557.9080908@pdf.com>
References: <432AF0EC.5645.8B6925@localhost>
Message-ID: <432E9C5E.15963.2ADECA@localhost>

Thanks Sundar
what you suggested worked fine 

> You will see that "x" is a vector and "tt[i]^x" returns a vector of the 
> same length. You are trying to place this vector into "a[i]" which is 
> length 1. Try the following *untested* code instead:
> 
> <untested>
> integrandtotest <- function(x) {
>    sum(sapply(x, function(xi) sum(tt^xi)))
> }

except you are summing twice so it should be.........

integrandtotest <- function(x) {(sapply(x, function(xi) sum(tt^xi)))


Alan Brennan
Director of Health Economics and Decision Science
http://www.shef.ac.uk/scharr/sections/heds
ScHARR
School of Health and Related Research
University of Sheffield
Regent Ct
30 Regent St
Sheffield S1 4DA
Tel:+44 (0)114 2220684
Fax:+44 (0)114 2724095
e-mail:a.brennan at sheffield.ac.uk



From A.Brennan at sheffield.ac.uk  Mon Sep 19 12:10:57 2005
From: A.Brennan at sheffield.ac.uk (A.Brennan)
Date: Mon, 19 Sep 2005 11:10:57 +0100
Subject: [R] Integrate functions with loops
In-Reply-To: <Pine.LNX.4.61.0509161632580.3651@gannet.stats>
References: <432AF0EC.5645.8B6925@localhost>
Message-ID: <432E9CC0.14549.2C600B@localhost>

Thankyou Brian

>  	integrandtotest <- function(x) colSums(outer(t, x, "^"))
> 
This worked fine 

>in fact this integration can be done analytically (it is a 
> sum of exponentials).
I know but i have another five terms in the integrand as well as the 
one that was causing the trouble

Many, many thanks
Alan



From felipe at unileon.es  Mon Sep 19 12:39:26 2005
From: felipe at unileon.es (Felipe)
Date: Mon, 19 Sep 2005 12:39:26 +0200
Subject: [R] Use of least-squares means, are they misleading?
Message-ID: <432E955E.3030603@unileon.es>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi.
Sorry for sending this question twice, but I would really like to know
your opinion on this topic.

I have been reading about the (in)convenience of using least-squares
means (a. k. a. adjusted means) in multiple comparisons (I used to
resort to them when using SAS). I even read a post in this list warning
against them, but not giving much detail.

What is your opinion? Should I avoid using LSmeans for comparison (e.g.
after ANOVA)?

Greetings.

Felipe

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


-----BEGIN PGP SIGNATURE-----

iEYEARECAAYFAkMulV4ACgkQWtdQtNzjBl6QpwCfXYweOQWipAYY2ZZFiDYuAiwJ
LQYAnicmypM4yZhzMztRmZvppTZWes1z
=696Z
-----END PGP SIGNATURE-----



From vinum at iinet.net.au  Mon Sep 19 12:42:12 2005
From: vinum at iinet.net.au (John Charles Considine)
Date: Mon, 19 Sep 2005 18:42:12 +0800
Subject: [R] dynamic object names?
In-Reply-To: <432E86FA.3020101@statistik.uni-dortmund.de>
References: <1127122034.432e847284e26@www.mail2web.ch>
	<432E86FA.3020101@statistik.uni-dortmund.de>
Message-ID: <1127126532.10530.27.camel@Tardis.considine.local>

On Mon, 2005-09-19 at 11:38 +0200, Uwe Ligges wrote:
> samir at guidi.ch wrote:
> 
> > I am trying to extract data from a matrix. Let's say that i am interested in
> > extracting 
> > rows from a 4x4 matrix. Instead of giving a fix name to these 4 rows I would
> > like to add a number to prefix. As result I should get 4 objects named: 
> > 
> > prefix_1 
> > prefix_2 
> > prefix_3
> > prefix_4
Samir,
In addition to the other comments, part of the problem may be that you
can't assign the character '_' to standard objects.  Could you use '.'
instead?

JC 

> > 
> > I attepted to solve the problem with a loop, but without success. Any hints??
> 
> 
> 1. In fact, you do not really want it.
> 2. You want to use "<-" rather than "->" for assigments. Reading your 
> code is really hard when you are using this non-conventional way.
> 3. In order to answer your question: This is the FAQ "How can I turn a 
> string into a variable?". The posting guide asks you to read these FAQs 
> before sending questions to R-help. Please do so.
> 
> Uwe Ligges
> 
> 
> 
> 
> > 
> >>matrix(LETTERS[1:16], ncol=4) -> MM
> >>MM
> > 
> >      [,1] [,2] [,3] [,4]
> > [1,] "A"  "E"  "I"  "M" 
> > [2,] "B"  "F"  "J"  "N" 
> > [3,] "C"  "G"  "K"  "O" 
> > [4,] "D"  "H"  "L"  "P" 
> > 
> > 
> > for (xxx in 1:4) {
> > MM[xxx,] -> prefix_xxx;
> > }
> > 
> > 
> > Thanks
> > 
> > samir
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vincent at 7d4.com  Mon Sep 19 12:23:37 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Mon, 19 Sep 2005 12:23:37 +0200
Subject: [R] minimal hedge variance ratio
In-Reply-To: <139ef1c2050919002679e93df0@mail.gmail.com>
References: <139ef1c2050919002679e93df0@mail.gmail.com>
Message-ID: <432E91A9.1080602@7d4.com>

Krishna a ??crit :

> i have two data sets, spot and futures cash market prices. to estimate
> the minimum variance hedge ratio, i first had a glance on the
> correlation coefficient of relative price change (ln(St / St-1).
> surprizingly the value is just 0.2 compared to actual price
> correlation of 0.9.

The correlation coefficient measures the strengh of *linear*
relation between 2 variables X and Y.

Thus when you replace X by X'=aX+b or Y by Y'=cY+d
(for instance use f = function(U) {return((U - mean(U))/sd(U));})
then you always have cor(X,Y)=cor(X',Y')=cor(X,Y')=cor(X',Y).

But when you use X'=f(X), Y'=g(Y) with f,g non linear functions
there is no reason to have cor(X',Y')=cor(X,Y).
... and f(X) = logreturn(X) is not really a linear transformation.

hih



From vincent at 7d4.com  Mon Sep 19 12:26:15 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Mon, 19 Sep 2005 12:26:15 +0200
Subject: [R] dynamic object names?
In-Reply-To: <1127122034.432e847284e26@www.mail2web.ch>
References: <1127122034.432e847284e26@www.mail2web.ch>
Message-ID: <432E9247.6060305@7d4.com>

see
?paste
hih



From ligges at statistik.uni-dortmund.de  Mon Sep 19 12:51:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 19 Sep 2005 12:51:59 +0200
Subject: [R] dynamic object names?
In-Reply-To: <1127126532.10530.27.camel@Tardis.considine.local>
References: <1127122034.432e847284e26@www.mail2web.ch>	<432E86FA.3020101@statistik.uni-dortmund.de>
	<1127126532.10530.27.camel@Tardis.considine.local>
Message-ID: <432E984F.6000508@statistik.uni-dortmund.de>

John Charles Considine wrote:

> On Mon, 2005-09-19 at 11:38 +0200, Uwe Ligges wrote:
> 
>>samir at guidi.ch wrote:
>>
>>
>>>I am trying to extract data from a matrix. Let's say that i am interested in
>>>extracting 
>>>rows from a 4x4 matrix. Instead of giving a fix name to these 4 rows I would
>>>like to add a number to prefix. As result I should get 4 objects named: 
>>>
>>>prefix_1 
>>>prefix_2 
>>>prefix_3
>>>prefix_4
> 
> Samir,
> In addition to the other comments, part of the problem may be that you
> can't assign the character '_' to standard objects.


Why not???
R can handle standard names with "_" in it for ages now.

Uwe Ligges



 > Could you use '.' instead?
> JC 
> 
> 
>>>I attepted to solve the problem with a loop, but without success. Any hints??
>>
>>
>>1. In fact, you do not really want it.
>>2. You want to use "<-" rather than "->" for assigments. Reading your 
>>code is really hard when you are using this non-conventional way.
>>3. In order to answer your question: This is the FAQ "How can I turn a 
>>string into a variable?". The posting guide asks you to read these FAQs 
>>before sending questions to R-help. Please do so.
>>
>>Uwe Ligges
>>
>>
>>
>>
>>
>>>>matrix(LETTERS[1:16], ncol=4) -> MM
>>>>MM
>>>
>>>     [,1] [,2] [,3] [,4]
>>>[1,] "A"  "E"  "I"  "M" 
>>>[2,] "B"  "F"  "J"  "N" 
>>>[3,] "C"  "G"  "K"  "O" 
>>>[4,] "D"  "H"  "L"  "P" 
>>>
>>>
>>>for (xxx in 1:4) {
>>>MM[xxx,] -> prefix_xxx;
>>>}
>>>
>>>
>>>Thanks
>>>
>>>samir
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bmw8042 at verizon.net  Mon Sep 19 12:52:37 2005
From: bmw8042 at verizon.net (bmw8042)
Date: Mon, 19 Sep 2005 06:52:37 -0400
Subject: [R] month increment for chron dates
References: <000701c5bc57$2b74d1e0$1abdfea9@atlantic>
	<971536df05091807426fa37325@mail.gmail.com>
Message-ID: <000901c5bd08$53d938e0$1abdfea9@atlantic>

Gabor, 
This is great.  Thank you.  It runs in just 3 seconds.

Brian



From jarioksa at sun3.oulu.fi  Mon Sep 19 13:09:11 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Mon, 19 Sep 2005 14:09:11 +0300
Subject: [R] indicator value in labdsv
In-Reply-To: <1127115701.432e6bb532815@www.ps.pl>
References: <1127115701.432e6bb532815@www.ps.pl>
Message-ID: <1127128151.2909.13.camel@biol102145.oulu.fi>

On Mon, 2005-09-19 at 09:41 +0200, astrzelczak at ps.pl wrote:
> 
> Hi,
> 
> I'm trying to find out what threshold of indicator value in labadsv should be
> used to accept a specie as an indicator one? So far I assumed that indval=0.5
> is high enough to avoid any mistakes but it was based only in my intuition.
> 
> I'd be greatful for any advise
> 
> best regards
> 

Agnieszka,

R mailing list software appends the following to your message:

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Then about indicator value analysis. You should be more specific: there
seem to be three alternatives functions for "indicator species" in
labdsv. Which did  you mean? At least two of these return an item called
"indval", and these two alternative "indvals" are very different. For
the Dufr??ne-Legendre indvals, you should check the original paper (see
references in the help page), and there you even have an associated "P
value". In indspc, the variance of the indval clearly is dependent on
species frequency. Moreover, in indspc the expected indval (and its
variance) are dependent on the whole set of sites you have: these
reflect the general "homogeneity" of your data set. Therefore you cannot
say there that any certain value would mean that a species is a good
indicator. However, it would be easy to work out standard errors for
indspc indvals.

I think it would be more useful to post to some other mailing group
where people are more concerned about indicator species, or to contact
the package author directly (I CC this message to him).

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
Ph. +358 8 5531526, cell +358 40 5136529, fax +358 8 5531061
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/



From gavin.simpson at ucl.ac.uk  Mon Sep 19 13:16:13 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 19 Sep 2005 12:16:13 +0100
Subject: [R] indicator value in labdsv
In-Reply-To: <1127115701.432e6bb532815@www.ps.pl>
References: <1127115701.432e6bb532815@www.ps.pl>
Message-ID: <1127128573.30858.9.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2005-09-19 at 09:41 +0200, astrzelczak at ps.pl wrote:
> 
> Hi,
> 
> I'm trying to find out what threshold of indicator value in labadsv should be
> used to accept a specie as an indicator one? So far I assumed that indval=0.5
> is high enough to avoid any mistakes but it was based only in my intuition.
> 
> I'd be greatful for any advise
> 
> best regards
> 
> Agnieszka

Have you looked at the paper pertaining to the method? I assume you mean
the duleg() function in package labdsv?

If so, see this reprint on Pierre Legendre's web site:

http://www.bio.umontreal.ca/legendre/reprints/index.html

1997 
      * DufrÃªne, M. & P. Legendre. 1997. Species assemblages and
        indicator species: the need for a flexible asymmetrical
        approach. Ecological Monographs 67: 345-366.

That may help answer your question. Sorry I can't be of further help - I
haven't used this method before.

If it isn't duleg() then you'll need to provide more information...

HTH

G

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From pearce.rachel at gmail.com  Mon Sep 19 14:05:32 2005
From: pearce.rachel at gmail.com (Rachel Pearce)
Date: Mon, 19 Sep 2005 13:05:32 +0100
Subject: [R] Problem with tick marks in lines.survfit (package survival)
	Improved message
In-Reply-To: <59541412050919025937fbff56@mail.gmail.com>
References: <59541412050919025937fbff56@mail.gmail.com>
Message-ID: <5954141205091905057018791c@mail.gmail.com>

Sorry, my previous email seems to have come through in a rather odd
format. Also I forgot to add version and OS information, now included
below. My original query was:

I have attempted to follow posting guidelines but I have failed to
find out what I am doing wrong here.
 
I am trying to use lines.survfit to plot a second curve onto a
survival curve produced by plot.survfit. In my case this is to be a
progression free survival curve superimposed upon an overall survival
curve, but I will illustrate my problem using the example given in the
help for lines.survfit.
 
I would like to have tick marks for censored data points on both
curves, so I would like to do something like:
 
> fit <- survfit(Surv(time, status) ~ sex, pbc,subset=1:312)
> plot(fit[2], mark.time=TRUE, xscale=365.24,
+              xlab='Years', ylab='Survival')
> lines(fit[1], lwd=2, xscale=365.24, mark.time=TRUE)
 
but when I do this, no tick marks appear on the second curve. Can
anyone see what I am doing wrong here?
 
Rachel

Here are the version data:

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.1            
year     2005           
month    06             
day      20             
language R



From murdoch at stats.uwo.ca  Mon Sep 19 14:06:34 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Sep 2005 08:06:34 -0400
Subject: [R] distance to eye in persp()
In-Reply-To: <XFMail.050919101803.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050919101803.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <432EA9CA.3000508@stats.uwo.ca>

(Ted Harding) wrote:
> On 19-Sep-05 Robin Hankin wrote:
> 
>>Hi
>>
>>the manpage for persp() has a wonderful section where a the
>>trans3d()  function is used with points() and lines() to add
>>red dots and a green sinusoid to the Mexican hat surface.
>>
>>Does anyone have a way to tell what distance  a point is from
>>the eye/camera?
>>
>>Take the following line:
>>
>>lines (trans3d(x, y=10, z= 6 + sin(x), pm = res), col = 3)
>>
>>Is there a function like trans3d() that returns a vector of
>>distances from the x,y,z point to the camera?  I want this so
>>I can plot clouds of points with the further ones in smaller
>>plotsizes, and perhaps even fading to white (as though viewed
>>through fog).
> 
> 
> Wonderfully put! That's what statistics is about!
> 
> I think you may have to write your own. This is possible given
> the values for the parameters xlim, ylim, zlim, r, theta, phi
> (default as defined in ?persp, or explicitly user-defined),
> since you can then determine the 3D coordinates of the "Eye"
> relative to the (X,Y,Z) axes being plotted, after which the
> distance to a particular (x,y,z) point is trivial.
> 
> E.g.
> 
> 1. Coordinates of Eye relative to the centre of the box
> 
>    xE <- r*sin(theta + pi)*cos(phi)
>    yE <- r*cos(theta + pi)*cos(phi)
>    zE <- r*sin(phi)
> 
> 2. Centre of box relative to real (0,0,0)
> 
>    xC <- mean(xlim); yC <-mean(ylim); xC <- mean(zlim)
> 
> 3. Coordinates of (x,y,z) relative to Eye
> 
>    x1 <- x - xE - xC; y1 <- y - yE - yC; z1 <- z - zE - zC
> 
> 4. Distance from Eye to (x,y,z)
> 
>    d = sqrt(x1^2 + y1^2 + z1^2)
> 
> (Hoping I've not got anything the wrong way round there!)
> 

I think you forgot the rescaling to [0,1], but it's probably even easier 
than that.  persp returns the 4x4 transformation matrix used to take 
user coordinates into screen coordinates.  trans3d uses that matrix to 
extract the screen x and screen y coordinates (extend the 3-vector to 
homogeneous coordinates by appending a 1, multiply by the projection 
matrix, convert back to Euclidean coordinates by dividing by the last 
coordinate).  You can probably just do what trans3d does, but keep the 
z-coordinate, to get a reasonable measure of depth.  i.e.

depth3d <- function(x,y,z, pmat) {
   tr <- cbind(x, y, z, 1) %*% pmat
   return(tr[,3]/tr[,4])
}

This is sufficient for doing fog calculations.  For perspective 
shrinkage, you'll need to say where the user's eye is in these 
coordinates (or just use depths as distances).

Duncan Murdoch



From murdoch at stats.uwo.ca  Mon Sep 19 14:09:18 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Sep 2005 08:09:18 -0400
Subject: [R] dynamic object names?
In-Reply-To: <1127126532.10530.27.camel@Tardis.considine.local>
References: <1127122034.432e847284e26@www.mail2web.ch>	<432E86FA.3020101@statistik.uni-dortmund.de>
	<1127126532.10530.27.camel@Tardis.considine.local>
Message-ID: <432EAA6E.1060806@stats.uwo.ca>

John Charles Considine wrote:
> On Mon, 2005-09-19 at 11:38 +0200, Uwe Ligges wrote:
> 
>>samir at guidi.ch wrote:
>>
>>
>>>I am trying to extract data from a matrix. Let's say that i am interested in
>>>extracting 
>>>rows from a 4x4 matrix. Instead of giving a fix name to these 4 rows I would
>>>like to add a number to prefix. As result I should get 4 objects named: 
>>>
>>>prefix_1 
>>>prefix_2 
>>>prefix_3
>>>prefix_4
> 
> Samir,
> In addition to the other comments, part of the problem may be that you
> can't assign the character '_' to standard objects.  Could you use '.'
> instead?

Actually underscores have been allowed in names since version 1.9.0.

Duncan Murdoch



From YSIvanova at beeline.ru  Mon Sep 19 15:11:55 2005
From: YSIvanova at beeline.ru (Yuliya S Ivanova)
Date: Mon, 19 Sep 2005 17:11:55 +0400
Subject: [R] Converting the result of classification tree into SQL query
Message-ID: <OFEBCF36F0.734DA89F-ONC3257081.0046CAEA-C3257081.004880BC@beeline.ru>

I built the model of classification tree.Now I want to convert the result
rules of the tree into Sql query for applying received rules to my dataset
in the database managment system.
Can this be done or not ? If it can, tell me please how I can do it.
Thanks for your help.



From Rau at demogr.mpg.de  Mon Sep 19 15:25:14 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Mon, 19 Sep 2005 15:25:14 +0200
Subject: [R] Teaching R - In front of the computer?
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E69FDD44@HERMES.demogr.mpg.de>

Dear R-Users,

given you have been teaching R to students (grad level, mainly social
science background, no previous programming experience, 80% know SPSS),
what are your experiences concerning the style of teaching? Do you
prefer to stand in front of the class like in "normal" lectures and you
show them slides? Or do you you explain some concept (for example things
like mydata[order(var1, var2),]) and show it directly on the computer
via beamer/projector and also the students have to enter it on the
computers in front of them.

Any experiences you can share are highly appreciated.

Thanks,
Roland

+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From vantini at mate.polimi.it  Mon Sep 19 15:30:31 2005
From: vantini at mate.polimi.it (Simone Vantini)
Date: Mon, 19 Sep 2005 15:30:31 +0200 (CEST)
Subject: [R] toLatex
Message-ID: <1503.131.175.23.9.1127136631.squirrel@webmail.mate.polimi.it>

hallo!
does everybody know which kind of objects the function toLatex can manage?
thanks
simone vantini


-- 
Simone Vantini
MOX (Modelling and Scientific Computing)
Dipartimento di Matematica "F. Brioschi"
Politecnico di Milano
P.za Leonardo da Vinci, 32
20133 Milano (Italy)
tel: +39 02 2399 4604
email: simone.vantini at mate.polimi.it



From YSIvanova at beeline.ru  Mon Sep 19 15:32:01 2005
From: YSIvanova at beeline.ru (Yuliya S Ivanova)
Date: Mon, 19 Sep 2005 17:32:01 +0400
Subject: [R] Converting the result of classification tree into SQL query
Message-ID: <OFA1DA1154.EB273F23-ONC3257081.0049CC2A-C3257081.004A57A1@beeline.ru>

I built the model of classification tree.Now I want to convert the result
rules of the tree into Sql query for applying received rules to my dataset
in the database managment system.
Can this be done or not ? If it can, tell me please how I can do it.
Thanks for your help.
Yulia



From rpeng at jhsph.edu  Mon Sep 19 15:39:17 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 19 Sep 2005 09:39:17 -0400
Subject: [R] toLatex
In-Reply-To: <1503.131.175.23.9.1127136631.squirrel@webmail.mate.polimi.it>
References: <1503.131.175.23.9.1127136631.squirrel@webmail.mate.polimi.it>
Message-ID: <432EBF85.3000702@jhsph.edu>

Try 'methods(toLatex)'.

-roger

Simone Vantini wrote:
> hallo!
> does everybody know which kind of objects the function toLatex can manage?
> thanks
> simone vantini
> 
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From r.hankin at noc.soton.ac.uk  Mon Sep 19 15:41:30 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 19 Sep 2005 14:41:30 +0100
Subject: [R] Teaching R - In front of the computer?
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E69FDD44@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E69FDD44@HERMES.demogr.mpg.de>
Message-ID: <7AEEEC5A-B907-4237-A3DA-24C18082A25D@soc.soton.ac.uk>

Hi

the only way I have ever made this work is to have an R
session projected on the screen, and each student to
have their own machine on which they can try out
what I type.

You can then set the students questions to force them
to play around with R syntax.  A good one is to put
up a graph that you have prepared beforehand, and
say "reproduce this".

Just be careful with ad-hoc playing around, in a live public
session.  I got badly bitten by this:

f <- function(x){(x*x -1) - (x-1)*(x+1) }
f(100000:100010)


[everything behaving as documented, but embarrassingly
unexpected!]

best

Robin







On 19 Sep 2005, at 14:25, Rau, Roland wrote:

> Dear R-Users,
>
> given you have been teaching R to students (grad level, mainly social
> science background, no previous programming experience, 80% know  
> SPSS),
> what are your experiences concerning the style of teaching? Do you
> prefer to stand in front of the class like in "normal" lectures and  
> you
> show them slides? Or do you you explain some concept (for example  
> things
> like mydata[order(var1, var2),]) and show it directly on the computer
> via beamer/projector and also the students have to enter it on the
> computers in front of them.
>
> Any experiences you can share are highly appreciated.
>
> Thanks,
> Roland
>
> +++++
> This mail has been sent through the MPI for Demographic Rese... 
> {{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From Jan.Wijffels at ucs.kuleuven.be  Mon Sep 19 15:51:30 2005
From: Jan.Wijffels at ucs.kuleuven.be (Jan Wijffels)
Date: Mon, 19 Sep 2005 15:51:30 +0200
Subject: [R] R(D)COM and Excel compile error
Message-ID: <003201c5bd21$3d382fb0$2c70210a@UCSPC32>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050919/b91a49e5/attachment.pl

From sdavis2 at mail.nih.gov  Mon Sep 19 16:00:54 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 19 Sep 2005 10:00:54 -0400
Subject: [R] Converting the result of classification tree into SQL	query
In-Reply-To: <OFA1DA1154.EB273F23-ONC3257081.0049CC2A-C3257081.004A57A1@beeline.ru>
Message-ID: <BF543CD6.EC5A%sdavis2@mail.nih.gov>

On 9/19/05 9:32 AM, "Yuliya S Ivanova" <YSIvanova at beeline.ru> wrote:

> I built the model of classification tree.Now I want to convert the result
> rules of the tree into Sql query for applying received rules to my dataset
> in the database managment system.
> Can this be done or not ? If it can, tell me please how I can do it.
> Thanks for your help.

This will need to be done by hand, I think.  In other words, you will need
to write the SQL query yourself.  Of course, you can make such a query
dynamic using tools like "paste", but I don't think there is a tool to do
exactly what you want.

Sean



From felipe at unileon.es  Mon Sep 19 16:03:41 2005
From: felipe at unileon.es (Felipe)
Date: Mon, 19 Sep 2005 16:03:41 +0200
Subject: [R] Use of least-squares means, are they misleading?
In-Reply-To: <s32e603a.005@MAIL.NDRI.ORG>
References: <s32e603a.005@MAIL.NDRI.ORG>
Message-ID: <432EC53D.7080505@unileon.es>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Thank you, I will try there.
Felipe

Peter Flom wrote:
| Felipe
|
| I doubt you will get much response on this list, although it's an
| interesting question.  You might try asking it on another list.  Some
| that often have discussions of such topics are
|
| SAS-L
| EDSTAT-L and
| STAT-L
|
| Regards
|
| Peter
|
| Peter L. Flom, PhD
| Assistant Director, Statistics and Data Analysis Core
| Center for Drug Use and HIV Research
| National Development and Research Institutes
| 71 W. 23rd St
| http://cduhr.ndri.org
| www.peterflom.com
| New York, NY 10010
| (212) 845-4485 (voice)
| (917) 438-0894 (fax)
|
|
|
|>>>Felipe <felipe at unileon.es> 09/19/05 6:39 AM >>>
|
| Hi.
| Sorry for sending this question twice, but I would really like to know
| your opinion on this topic.
|
| I have been reading about the (in)convenience of using least-squares
| means (a. k. a. adjusted means) in multiple comparisons (I used to
| resort to them when using SAS). I even read a post in this list warning
| against them, but not giving much detail.
|
| What is your opinion? Should I avoid using LSmeans for comparison (e.g.
| after ANOVA)?
|
| Greetings.
|
| Felipe
|
| ______________________________________________
| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide!
| http://www.R-project.org/posting-guide.html
|
|

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


-----BEGIN PGP SIGNATURE-----

iEYEARECAAYFAkMuxT0ACgkQWtdQtNzjBl7LWACcCddjPgTnZX2+15aDLbxe+qxD
rHcAoIdkgW8ke/M8bb7pSlcptTz9K53j
=eCEC
-----END PGP SIGNATURE-----



From rbaer at atsu.edu  Mon Sep 19 16:13:15 2005
From: rbaer at atsu.edu (Robert Baer)
Date: Mon, 19 Sep 2005 09:13:15 -0500
Subject: [R] Problem with tick marks in lines.survfit (package survival)
References: <59541412050919025937fbff56@mail.gmail.com>
Message-ID: <002e01c5bd24$4761c7f0$6d0d010a@BigBaer>

How about:
plot(fit, mark.time=TRUE, xscale=365.24,xlab='Years',
ylab='Survival',lwd=c(2,1))

Rob
----- Original Message ----- 
From: "Rachel Pearce" <pearce.rachel at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, September 19, 2005 4:59 AM
Subject: [R] Problem with tick marks in lines.survfit (package survival)


> I have attempted to follow posting guidelines but I have failed to find
out
> what I am doing wrong here.
>  I am trying to use lines.survfit to plot a second curve onto a survival
> curve produced by plot.survfit. In my case this is to be a progression
free
> survival curve superimposed upon an overall survival curve, but I will
> illustrate my problem using the example given in the help for
lines.survfit.
>  I would like to have tick marks for censored data points on both curves,
so
> I would like to do something like:
>  > fit <- survfit(Surv(time, status) ~ sex, pbc,subset=1:312)
> > plot(fit[2], mark.time=TRUE, xscale=365.24,
> + xlab='Years', ylab='Survival')
> > lines(fit[1], lwd=2, xscale=365.24, mark.time=TRUE)
>  but when I do this, no tick marks appear on the second curve. Can anyone
> see what I am doing wrong here?
>  Rachel
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Mon Sep 19 16:14:57 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 19 Sep 2005 16:14:57 +0200
Subject: [R] Teaching R - In front of the computer?
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E69FDD44@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E69FDD44@HERMES.demogr.mpg.de>
Message-ID: <432EC7E1.5090907@statistik.uni-dortmund.de>

Rau, Roland wrote:

> Dear R-Users,
> 
> given you have been teaching R to students (grad level, mainly social
> science background, no previous programming experience, 80% know SPSS),
> what are your experiences concerning the style of teaching? Do you
> prefer to stand in front of the class like in "normal" lectures and you
> show them slides? Or do you you explain some concept (for example things
> like mydata[order(var1, var2),]) and show it directly on the computer
> via beamer/projector and also the students have to enter it on the
> computers in front of them.
> 
> Any experiences you can share are highly appreciated.

I like to teach using a projector and people sitting in front of other 
machines beeing able to try out some of the nonsense I am telling.
Of course, they need more exercises than just the few minutes in between 
my sentences.
I am always noth using slides and showing the examples in R directly.


For all other stuff, it is the same as with other courses and lectures.

If this is a one-week course, I'd like to propose 1.5 hours course + 1.5 
hours exercise in the morning and the same again in the afternoon. If 
this is a whole-term course, you may be able to let people do homework, 
but always talk about their "results" and show them hoe to do it in a 
good fashion.

Unfortunately, for courses with lots of poeple (say 90, for the 
first-years' R course I am teaching) it is impossible that people are 
sitting in front of a computer for some reasons: not enough machines, no 
big rooms, and too much noise caused by both machines and people.

The really important fact from my point of view:
Give people many *exercises* and the *time* to work on these exercises. 
Expect that people without any programming experience will be much 
slower than you imagine in your worst nightmares. ;-)

Uwe Ligges




> Thanks,
> Roland
> 
> +++++
> This mail has been sent through the MPI for Demographic Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From r.hankin at noc.soton.ac.uk  Mon Sep 19 16:15:07 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 19 Sep 2005 15:15:07 +0100
Subject: [R] distance to eye in persp()
In-Reply-To: <432EA9CA.3000508@stats.uwo.ca>
References: <XFMail.050919101803.Ted.Harding@nessie.mcc.ac.uk>
	<432EA9CA.3000508@stats.uwo.ca>
Message-ID: <D39CE363-B7E4-4616-9363-C74BC794847B@soc.soton.ac.uk>

Duncan


> You can probably just do what trans3d does, but keep the
> z-coordinate, to get a reasonable measure of depth.  i.e.
>
> depth3d <- function(x,y,z, pmat) {
>    tr <- cbind(x, y, z, 1) %*% pmat
>    return(tr[,3]/tr[,4])
> }
>


fantastic!  This is exactly what I was looking for.


> This is sufficient for doing fog calculations.  For perspective
> shrinkage, you'll need to say where the user's eye is in these
> coordinates (or just use depths as distances).
>


I was going to use hsv(h=1, v=1, s=exp(d/d0) )

(some people believe that red light [ie hsv(h=1, ....)] penetrates  
further
through mist than other colours.  I can now test this !-)

The other idea I had was to plot points with less than the  median
depth with big symbols  and the others  with small symbols.
That would seem to be a nice nonparametric way to do it.
Any other ideas?


very best wishes to everyone

Robin


>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From pearce.rachel at gmail.com  Mon Sep 19 16:22:40 2005
From: pearce.rachel at gmail.com (Rachel Pearce)
Date: Mon, 19 Sep 2005 15:22:40 +0100
Subject: [R] Problem with tick marks in lines.survfit (package survival)
In-Reply-To: <002e01c5bd24$4761c7f0$6d0d010a@BigBaer>
References: <59541412050919025937fbff56@mail.gmail.com>
	<002e01c5bd24$4761c7f0$6d0d010a@BigBaer>
Message-ID: <59541412050919072278b476b3@mail.gmail.com>

Sorry, yes I think that would work, for the example I gave, but I am
trying to superimpose a curve onto an existing plot. The option of
using "plot" to draw both curves is not available, since the data are
(or were) in two different columns. For that reason I was trying to
use "lines" for the second curve.

I used the example given with the package so that I didn't have to
publish my own data (which are not mine to publish, if you see what I
mean).

I have now given up and created a new data frame with OS and PFS
stacked on top of each other, but this is a very time-consuming
workaround.

Do you (or anyone) know how to make "lines.survfit" draw tick marks?

Rachel

On 19/09/05, Robert Baer <rbaer at atsu.edu> wrote:
> How about:
> plot(fit, mark.time=TRUE, xscale=365.24,xlab='Years',
> ylab='Survival',lwd=c(2,1))
> 
> Rob
> ----- Original Message -----
> From: "Rachel Pearce" <pearce.rachel at gmail.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, September 19, 2005 4:59 AM
> Subject: [R] Problem with tick marks in lines.survfit (package survival)
> 
> 
> > I have attempted to follow posting guidelines but I have failed to find
> out
> > what I am doing wrong here.
> >  I am trying to use lines.survfit to plot a second curve onto a survival
> > curve produced by plot.survfit. In my case this is to be a progression
> free
> > survival curve superimposed upon an overall survival curve, but I will
> > illustrate my problem using the example given in the help for
> lines.survfit.
> >  I would like to have tick marks for censored data points on both curves,
> so
> > I would like to do something like:
> >  > fit <- survfit(Surv(time, status) ~ sex, pbc,subset=1:312)
> > > plot(fit[2], mark.time=TRUE, xscale=365.24,
> > + xlab='Years', ylab='Survival')
> > > lines(fit[1], lwd=2, xscale=365.24, mark.time=TRUE)
> >  but when I do this, no tick marks appear on the second curve. Can anyone
> > see what I am doing wrong here?
> >  Rachel
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
>



From tomas.andersson at spotfire.com  Mon Sep 19 16:46:33 2005
From: tomas.andersson at spotfire.com (Tomas Andersson)
Date: Mon, 19 Sep 2005 16:46:33 +0200
Subject: [R] Change in behavior of compare statement
Message-ID: <6EB95FAD50879B408C22FA4E470787F57D1A25@mail-eu.spotfire.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050919/7048be59/attachment.pl

From william.dupont at Vanderbilt.Edu  Mon Sep 19 17:10:26 2005
From: william.dupont at Vanderbilt.Edu (Dupont, William)
Date: Mon, 19 Sep 2005 10:10:26 -0500
Subject: [R] FDR analyses: minimum number of features
Message-ID: <C6E7B149DFD87D4AAD1ADA90B5DE0279493ACE@mailbe11.mc.vanderbilt.edu>

Dear List,

We are planning a genotyping study to be analyzed using false discovery
rates (FDRs) (See Storey and Tibshirani PNAS 2003; 100:9440-5).  I am
interested in learning if there is any consensus as to how many
features (ie. how many P values) need to be studied before reasonably
reliable FDRs can be derived.  Does anyone know of a citation where
this is discussed?

Bill Dupont 

William D. Dupont          phone: 615-343-4100          URL
http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/WilliamDupont



From droberts at montana.edu  Mon Sep 19 18:00:23 2005
From: droberts at montana.edu (Dave Roberts)
Date: Mon, 19 Sep 2005 10:00:23 -0600
Subject: [R] indicator value in labdsv
In-Reply-To: <1127128151.2909.13.camel@biol102145.oulu.fi>
References: <1127115701.432e6bb532815@www.ps.pl>
	<1127128151.2909.13.camel@biol102145.oulu.fi>
Message-ID: <432EE097.4070603@montana.edu>

Agnieszka,

     As Jari indicated, it depends on which function you meant in you 
inquiry.  The duleg() function implements the Dufrene-Legendre 
algorithm, where "indicator" species are indicative of a priori 
communities.  It this requires a classification, and is biased to find 
species which occur in the dataset approximately as often as the mean 
cluster size.

     The indpsc() function calculates the mean similarity of all samples 
a species occurs in.  This is slightly biased because  we know that the 
samples being used to calculate the mean share at least the species that 
defines them, but it is still possible to compare those values to the 
mean similarity of the whole matrix, or to an expectation of maximum 
similarity.  Obviously, as species occur more frequently, the harder it 
is to have a really high similarity (indicator value), with the extreme 
case that a species that occurs in every sample must have the same value 
as the mean of the whole matrix.

     To tell the truth, I forgot that indspc() was included in the 
current version of labdsv.  In the new version (due to be released any 
day), I have included a permutation test that estimates quantiles of 
expected values for different numbers of occurrences.  It works, but is 
pretty slow.  Jari has created a version that uses parametric statistics 
to estimate the same envelope, but I haven't had a chance to try it yet.

     What research are you doing, and what are you really trying to 
determine?  Perhaps something altogether different will work better.

Thanks, Dave Roberts

> On Mon, 2005-09-19 at 09:41 +0200, astrzelczak at ps.pl wrote:
> 
>>Hi,
>>
>>I'm trying to find out what threshold of indicator value in labadsv should be
>>used to accept a specie as an indicator one? So far I assumed that indval=0.5
>>is high enough to avoid any mistakes but it was based only in my intuition.
>>
>>I'd be greatful for any advise
>>
>>best regards
>>
> 
> 
> Agnieszka,
> 
> R mailing list software appends the following to your message:
> 
> 
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> Then about indicator value analysis. You should be more specific: there
> seem to be three alternatives functions for "indicator species" in
> labdsv. Which did  you mean? At least two of these return an item called
> "indval", and these two alternative "indvals" are very different. For
> the Dufr??ne-Legendre indvals, you should check the original paper (see
> references in the help page), and there you even have an associated "P
> value". In indspc, the variance of the indval clearly is dependent on
> species frequency. Moreover, in indspc the expected indval (and its
> variance) are dependent on the whole set of sites you have: these
> reflect the general "homogeneity" of your data set. Therefore you cannot
> say there that any certain value would mean that a species is a good
> indicator. However, it would be easy to work out standard errors for
> indspc indvals.
> 
> I think it would be more useful to post to some other mailing group
> where people are more concerned about indicator species, or to contact
> the package author directly (I CC this message to him).
> 
> cheers, jari oksanen


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From astrzelczak at ps.pl  Mon Sep 19 18:25:00 2005
From: astrzelczak at ps.pl (astrzelczak@ps.pl)
Date: Mon, 19 Sep 2005 18:25:00 +0200
Subject: [R] indicator value in labdsv
Message-ID: <1127147100.432ee65cb8999@www.ps.pl>


Hello,

I was uclear before, I'm sory about it. I forgot to add that I'm using duleg...

I used mvpart for multivariate regression trees. My input variables are
environmental parameters, output variables are macrophyte species
(presence=1,absence=0 in conecutive cases=lakes). For obtained classes I used
duleg to find indicator species for every class. I checked the article Dufrene,
M. and Legendre, P. 1997. Species assemblages and indicator species: the need
for a flexible asymmetrical approach. Ecol. Monogr. 67(3):345-366. The authors
used the threshold of indval=0.25(25%) and that's the only hint I've found in
the literature. This threshod seems to reasonable, but still I have impression
that's too low...

best regards
Agnieszka


> Agnieszka,

>      As Jari indicated, it depends on which function you meant in you
> inquiry.  The duleg() function implements the Dufrene-Legendre
> algorithm, where "indicator" species are indicative of a priori
> communities.  It this requires a classification, and is biased to find
> species which occur in the dataset approximately as often as the mean
> cluster size.

>      The indpsc() function calculates the mean similarity of all samples
> a species occurs in.  This is slightly biased because  we know that the
> samples being used to calculate the mean share at least the species that
> defines them, but it is still possible to compare those values to the
> mean similarity of the whole matrix, or to an expectation of maximum
> similarity.  Obviously, as species occur more frequently, the harder it
> is to have a really high similarity (indicator value), with the extreme
> case that a species that occurs in every sample must have the same value
> as the mean of the whole matrix.

>      To tell the truth, I forgot that indspc() was included in the
> current version of labdsv.  In the new version (due to be released any
> day), I have included a permutation test that estimates quantiles of
> expected values for different numbers of occurrences.  It works, but is
> pretty slow.  Jari has created a version that uses parametric statistics
> to estimate the same envelope, but I haven't had a chance to try it yet.

>      What research are you doing, and what are you really trying to
> determine?  Perhaps something altogether different will work better.

> Thanks, Dave Roberts

>> On Mon, 2005-09-19 at 09:41 +0200, astrzelczak at ps.pl wrote:
>>
>>>Hi,
>>>
>>>I'm trying to find out what threshold of indicator value in labadsv should be
>>>used to accept a specie as an indicator one? So far I assumed that indval=0.5
>>>is high enough to avoid any mistakes but it was based only in my intuition.
>>>
>>>I'd be greatful for any advise
>>>
>>>best regards
>>>
>>
>>
>> Agnieszka,
>>
>> R mailing list software appends the following to your message:
>>
>>
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>
>>
>> Then about indicator value analysis. You should be more specific: there
>> seem to be three alternatives functions for "indicator species" in
>> labdsv. Which did  you mean? At least two of these return an item called
>> "indval", and these two alternative "indvals" are very different. For
>> the Dufr??ne-Legendre indvals, you should check the original paper (see
>> references in the help page), and there you even have an associated "P
>> value". In indspc, the variance of the indval clearly is dependent on
>> species frequency. Moreover, in indspc the expected indval (and its
>> variance) are dependent on the whole set of sites you have: these
>> reflect the general "homogeneity" of your data set. Therefore you cannot
>> say there that any certain value would mean that a species is a good
>> indicator. However, it would be easy to work out standard errors for
>> indspc indvals.
>>
>> I think it would be more useful to post to some other mailing group
>> where people are more concerned about indicator species, or to contact
>> the package author directly (I CC this message to him).
>>
>> cheers, jari oksanen





--
Best regards,
                                                      mailto:astrzelczak at ps.pl

Agnieszka Strzelczak, Research Assistant
mailto:astrzelczak at ps.pl

Institute of Chemistry and Environmental Protection
Faculty of Chemical Engineering
Szczecin University of Technology
Aleja Piastow 42
71-065 Szczecin
Poland



From tomasan at gmail.com  Mon Sep 19 18:25:15 2005
From: tomasan at gmail.com (Tomas Andersson)
Date: Mon, 19 Sep 2005 18:25:15 +0200
Subject: [R]  Change in behavior of compare statement
Message-ID: <f55480e20509190925d85873c@mail.gmail.com>

Dear all, 

I have come across some Windows Script code which calls the
"file.exists" function in R to check for the existence of a particular
file on an R server. This is what the code looks like:

            do
            {
                        // some useful code
            }
            while (m_workspace.session.eval("file.exists(" + dataFile
+ ")") != "1");

It appears that the behavior of this code is different when used with
R version 2.1.1 compared to R version 1.9.1. If the compare statement
s is changed from

            while (s != "1")
to
            while (s != "True")
or
            while !(s)

the behavior of the code is the same with both versions of R.

After reading the R documentation, I have not found any evidence that
the original syntax (s != "1") is in any way recommended or supported
(in version 2.1.1). Still, my questions are:

1. Is it possible that the syntax has been supported in older versions of R?
2. Is the reason for the recent change in behavior known and
documented anywhere?

I have been unable to find an answer to either question, so any
information about this would be appreciated.

Thanks,

T. Andersson



From joris.dewolf at cropdesign.com  Mon Sep 19 18:24:07 2005
From: joris.dewolf at cropdesign.com (Joris De Wolf)
Date: Mon, 19 Sep 2005 18:24:07 +0200
Subject: [R] How to mimic pdMat of lme under lmer?
Message-ID: <432EE627.3020802@cropdesign.com>

Dear members,

I would like to switch from nlme to lme4 and try to translate some of my 
models that worked fine with lme.
I have problems with the pdMat classes.

Below a toy dataset with a fixed effect F and a random effect R. I gave 
also 2 similar lme models.
The one containing pdLogChol (lme1) is easy to translate (as it is an 
explicit notation of the default model)
The more parsimonious model with pdDiag replacing pdLogChol I cannot 
reproduce with lmer. The obvious choice for me would be my model lmer2, 
but this is yielding different result.

Somebody any idea?
Thanks,
Joris

I am using R version 2.1.0 for Linux
and the most recent downloads of Matrix and  nlme

#dataset from McLean, Sanders and Stroup
F <- factor(c(1,1,1,1,1,1,2,2,2,2,2,2))
R <- factor(c(1,1,2,2,3,3,1,1,2,2,3,3))
s <- 
c(51.43,51.28,50.93,50.75,50.47,50.83,51.91,52.43,52.26,52.33,51.58,51.23)
DS <- data.frame(F,R,s)
DS$F <- as.factor(DS$F)
DS$R <- as.factor(DS$R)

library(nlme)
lme1 <- lme(data = DS,s ~ F,random = list(R = pdLogChol(~F)))
lme2 <- lme(data = DS,s ~ F,random = list(R = pdDiag(~F)))
summary(lme1)
summary(lme2)

library(lme4)
lmer1 <- lmer(data = DS,s ~ F + (F|R))
lmer2 <- lmer(data = DS,s ~ F + (1|R) + (1|F))
summary(lmer1)
summary(lmer2)



confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}



From kerbo2004 at yahoo.com  Mon Sep 19 18:27:56 2005
From: kerbo2004 at yahoo.com (A Das)
Date: Mon, 19 Sep 2005 09:27:56 -0700 (PDT)
Subject: [R] anova for random-intercept lmer
Message-ID: <20050919162756.72481.qmail@web35605.mail.mud.yahoo.com>

Hi,
    This might be a silly question, but how do I
create  a pooled model (binomial family) as a baseline
for a likelihood-ratio test for a random-intercept
lmer model? R won't compare a glm model with an lmer
model (or  glmmPQL ). Thanks in advance.
                            -Bobby



From droberts at montana.edu  Mon Sep 19 18:38:35 2005
From: droberts at montana.edu (Dave Roberts)
Date: Mon, 19 Sep 2005 10:38:35 -0600
Subject: [R] indicator value in labdsv
In-Reply-To: <1127147100.432ee65cb8999@www.ps.pl>
References: <1127147100.432ee65cb8999@www.ps.pl>
Message-ID: <432EE98B.6080900@montana.edu>

Wow!  That was fast!

     Unfortunately, Agnieszka, I don't think you will find an objective 
criterion for this.  Clearly, species which do not have a statistically 
significant value are probably less useful, but of the many that are 
significant, many may be marginal.

     Without knowing fully what you are hoping to achieve, I think I 
would rank the species by indicator value, and establish the highest 
threshold for indicator value that gives you a suitable number of 
species for each type.  That way, if you are looking to write a field 
key, for example, you would have sufficient values to identify every 
type I suspect.

Good luck, Dave

astrzelczak at ps.pl wrote:
> Hello,
> 
> I was uclear before, I'm sory about it. I forgot to add that I'm using duleg...
> 
> I used mvpart for multivariate regression trees. My input variables are
> environmental parameters, output variables are macrophyte species
> (presence=1,absence=0 in conecutive cases=lakes). For obtained classes I used
> duleg to find indicator species for every class. I checked the article Dufrene,
> M. and Legendre, P. 1997. Species assemblages and indicator species: the need
> for a flexible asymmetrical approach. Ecol. Monogr. 67(3):345-366. The authors
> used the threshold of indval=0.25(25%) and that's the only hint I've found in
> the literature. This threshod seems to reasonable, but still I have impression
> that's too low...
> 
> best regards
> Agnieszka
> 
> 
> 
>>Agnieszka,
> 
> 
>>     As Jari indicated, it depends on which function you meant in you
>>inquiry.  The duleg() function implements the Dufrene-Legendre
>>algorithm, where "indicator" species are indicative of a priori
>>communities.  It this requires a classification, and is biased to find
>>species which occur in the dataset approximately as often as the mean
>>cluster size.
> 
> 
>>     The indpsc() function calculates the mean similarity of all samples
>>a species occurs in.  This is slightly biased because  we know that the
>>samples being used to calculate the mean share at least the species that
>>defines them, but it is still possible to compare those values to the
>>mean similarity of the whole matrix, or to an expectation of maximum
>>similarity.  Obviously, as species occur more frequently, the harder it
>>is to have a really high similarity (indicator value), with the extreme
>>case that a species that occurs in every sample must have the same value
>>as the mean of the whole matrix.
> 
> 
>>     To tell the truth, I forgot that indspc() was included in the
>>current version of labdsv.  In the new version (due to be released any
>>day), I have included a permutation test that estimates quantiles of
>>expected values for different numbers of occurrences.  It works, but is
>>pretty slow.  Jari has created a version that uses parametric statistics
>>to estimate the same envelope, but I haven't had a chance to try it yet.
> 
> 
>>     What research are you doing, and what are you really trying to
>>determine?  Perhaps something altogether different will work better.
> 
> 
>>Thanks, Dave Roberts
> 
> 
>>>On Mon, 2005-09-19 at 09:41 +0200, astrzelczak at ps.pl wrote:
>>>
>>>
>>>>Hi,
>>>>
>>>>I'm trying to find out what threshold of indicator value in labadsv should be
>>>>used to accept a specie as an indicator one? So far I assumed that indval=0.5
>>>>is high enough to avoid any mistakes but it was based only in my intuition.
>>>>
>>>>I'd be greatful for any advise
>>>>
>>>>best regards
>>>>
>>>
>>>
>>>Agnieszka,
>>>
>>>R mailing list software appends the following to your message:
>>>
>>>
>>>
>>>>PLEASE do read the posting guide!
>>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>>Then about indicator value analysis. You should be more specific: there
>>>seem to be three alternatives functions for "indicator species" in
>>>labdsv. Which did  you mean? At least two of these return an item called
>>>"indval", and these two alternative "indvals" are very different. For
>>>the Dufr??ne-Legendre indvals, you should check the original paper (see
>>>references in the help page), and there you even have an associated "P
>>>value". In indspc, the variance of the indval clearly is dependent on
>>>species frequency. Moreover, in indspc the expected indval (and its
>>>variance) are dependent on the whole set of sites you have: these
>>>reflect the general "homogeneity" of your data set. Therefore you cannot
>>>say there that any certain value would mean that a species is a good
>>>indicator. However, it would be easy to work out standard errors for
>>>indspc indvals.
>>>
>>>I think it would be more useful to post to some other mailing group
>>>where people are more concerned about indicator species, or to contact
>>>the package author directly (I CC this message to him).
>>>
>>>cheers, jari oksanen
> 
> 
> 
> 
> 
> 
> --
> Best regards,
>                                                       mailto:astrzelczak at ps.pl
> 
> Agnieszka Strzelczak, Research Assistant
> mailto:astrzelczak at ps.pl
> 
> Institute of Chemistry and Environmental Protection
> Faculty of Chemical Engineering
> Szczecin University of Technology
> Aleja Piastow 42
> 71-065 Szczecin
> Poland
> 
> 


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From gerifalte28 at hotmail.com  Mon Sep 19 19:01:58 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 19 Sep 2005 17:01:58 +0000
Subject: [R] Teaching R - In front of the computer?
In-Reply-To: <432EC7E1.5090907@statistik.uni-dortmund.de>
Message-ID: <BAY103-F152EDCFFC943CFA0C1F968A6920@phx.gbl>

Adding to Uwe's comment, in my experience is also useful to use a text 
editor that connects to R (i.e. in Windows you have Tinn-R, jgr, SciViews) 
so people can see the function arguments as they type.  People are 
accustomed to this feature from Excel so it helps them to fell more 
comfortable with the syntax.  And yes, some students used to the mainstream 
point-and-click software can be REALLY slow so you may want to plan for 
that!

Good luck!

Francisco



>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>To: "Rau, Roland" <Rau at demogr.mpg.de>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Teaching R - In front of the computer?
>Date: Mon, 19 Sep 2005 16:14:57 +0200
>
>Rau, Roland wrote:
>
> > Dear R-Users,
> >
> > given you have been teaching R to students (grad level, mainly social
> > science background, no previous programming experience, 80% know SPSS),
> > what are your experiences concerning the style of teaching? Do you
> > prefer to stand in front of the class like in "normal" lectures and you
> > show them slides? Or do you you explain some concept (for example things
> > like mydata[order(var1, var2),]) and show it directly on the computer
> > via beamer/projector and also the students have to enter it on the
> > computers in front of them.
> >
> > Any experiences you can share are highly appreciated.
>
>I like to teach using a projector and people sitting in front of other
>machines beeing able to try out some of the nonsense I am telling.
>Of course, they need more exercises than just the few minutes in between
>my sentences.
>I am always noth using slides and showing the examples in R directly.
>
>
>For all other stuff, it is the same as with other courses and lectures.
>
>If this is a one-week course, I'd like to propose 1.5 hours course + 1.5
>hours exercise in the morning and the same again in the afternoon. If
>this is a whole-term course, you may be able to let people do homework,
>but always talk about their "results" and show them hoe to do it in a
>good fashion.
>
>Unfortunately, for courses with lots of poeple (say 90, for the
>first-years' R course I am teaching) it is impossible that people are
>sitting in front of a computer for some reasons: not enough machines, no
>big rooms, and too much noise caused by both machines and people.
>
>The really important fact from my point of view:
>Give people many *exercises* and the *time* to work on these exercises.
>Expect that people without any programming experience will be much
>slower than you imagine in your worst nightmares. ;-)
>
>Uwe Ligges
>
>
>
>
> > Thanks,
> > Roland
> >
> > +++++
> > This mail has been sent through the MPI for Demographic 
>Rese...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From astrzelczak at ps.pl  Mon Sep 19 19:04:32 2005
From: astrzelczak at ps.pl (astrzelczak@ps.pl)
Date: Mon, 19 Sep 2005 19:04:32 +0200
Subject: [R] indicator value in labdsv
Message-ID: <1127149472.432eefa0cafff@www.ps.pl>


That's what I was afraid of.. I'm not a biologist, so I have to involve my
co-workes who are well-versed in biological issues. Thank you very, very much
for help!!!

best regards
Agnieszka


Monday, September 19, 2005, 6:38:35 PM, you wrote:

> Wow!  That was fast!

>      Unfortunately, Agnieszka, I don't think you will find an objective
> criterion for this.  Clearly, species which do not have a statistically
> significant value are probably less useful, but of the many that are
> significant, many may be marginal.

>      Without knowing fully what you are hoping to achieve, I think I
> would rank the species by indicator value, and establish the highest
> threshold for indicator value that gives you a suitable number of
> species for each type.  That way, if you are looking to write a field
> key, for example, you would have sufficient values to identify every
> type I suspect.

> Good luck, Dave

> astrzelczak at ps.pl wrote:
>> Hello,
>>
>> I was uclear before, I'm sory about it. I forgot to add that I'm using
duleg...
>>
>> I used mvpart for multivariate regression trees. My input variables are
>> environmental parameters, output variables are macrophyte species
>> (presence=1,absence=0 in conecutive cases=lakes). For obtained classes I used
>> duleg to find indicator species for every class. I checked the article
Dufrene,
>> M. and Legendre, P. 1997. Species assemblages and indicator species: the need
>> for a flexible asymmetrical approach. Ecol. Monogr. 67(3):345-366. The
authors
>> used the threshold of indval=0.25(25%) and that's the only hint I've found in
>> the literature. This threshod seems to reasonable, but still I have
impression
>> that's too low...
>>
>> best regards
>> Agnieszka
>>
>>
>>
>>>Agnieszka,
>>
>>
>>>     As Jari indicated, it depends on which function you meant in you
>>>inquiry.  The duleg() function implements the Dufrene-Legendre
>>>algorithm, where "indicator" species are indicative of a priori
>>>communities.  It this requires a classification, and is biased to find
>>>species which occur in the dataset approximately as often as the mean
>>>cluster size.
>>
>>
>>>     The indpsc() function calculates the mean similarity of all samples
>>>a species occurs in.  This is slightly biased because  we know that the
>>>samples being used to calculate the mean share at least the species that
>>>defines them, but it is still possible to compare those values to the
>>>mean similarity of the whole matrix, or to an expectation of maximum
>>>similarity.  Obviously, as species occur more frequently, the harder it
>>>is to have a really high similarity (indicator value), with the extreme
>>>case that a species that occurs in every sample must have the same value
>>>as the mean of the whole matrix.
>>
>>
>>>     To tell the truth, I forgot that indspc() was included in the
>>>current version of labdsv.  In the new version (due to be released any
>>>day), I have included a permutation test that estimates quantiles of
>>>expected values for different numbers of occurrences.  It works, but is
>>>pretty slow.  Jari has created a version that uses parametric statistics
>>>to estimate the same envelope, but I haven't had a chance to try it yet.
>>
>>
>>>     What research are you doing, and what are you really trying to
>>>determine?  Perhaps something altogether different will work better.
>>
>>
>>>Thanks, Dave Roberts
>>
>>
>>>>On Mon, 2005-09-19 at 09:41 +0200, astrzelczak at ps.pl wrote:
>>>>
>>>>
>>>>>Hi,
>>>>>
>>>>>I'm trying to find out what threshold of indicator value in labadsv should
be
>>>>>used to accept a specie as an indicator one? So far I assumed that
indval=0.5
>>>>>is high enough to avoid any mistakes but it was based only in my intuition.
>>>>>
>>>>>I'd be greatful for any advise
>>>>>
>>>>>best regards
>>>>>
>>>>
>>>>
>>>>Agnieszka,
>>>>
>>>>R mailing list software appends the following to your message:
>>>>
>>>>
>>>>
>>>>>PLEASE do read the posting guide!
>>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>>Then about indicator value analysis. You should be more specific: there
>>>>seem to be three alternatives functions for "indicator species" in
>>>>labdsv. Which did  you mean? At least two of these return an item called
>>>>"indval", and these two alternative "indvals" are very different. For
>>>>the Dufr??ne-Legendre indvals, you should check the original paper (see
>>>>references in the help page), and there you even have an associated "P
>>>>value". In indspc, the variance of the indval clearly is dependent on
>>>>species frequency. Moreover, in indspc the expected indval (and its
>>>>variance) are dependent on the whole set of sites you have: these
>>>>reflect the general "homogeneity" of your data set. Therefore you cannot
>>>>say there that any certain value would mean that a species is a good
>>>>indicator. However, it would be easy to work out standard errors for
>>>>indspc indvals.
>>>>
>>>>I think it would be more useful to post to some other mailing group
>>>>where people are more concerned about indicator species, or to contact
>>>>the package author directly (I CC this message to him).
>>>>
>>>>cheers, jari oksanen
>>
>>



From Tord.Snall at helsinki.fi  Mon Sep 19 19:34:54 2005
From: Tord.Snall at helsinki.fi (Tord Snall)
Date: Mon, 19 Sep 2005 20:34:54 +0300
Subject: [R] factor as seq() in for loop
Message-ID: <1127151294.432ef6beb14d7@www2.helsinki.fi>

Dear all,
I would like to use the values in vegaggr.BLMCMR02$colony 

str(vegaggr.BLMCMR02)
`data.frame':   1678 obs. of  3 variables:
$ vegtype       : Factor w/ 27 levels "2010","2020",..: 3 4 5 19 4 5 19 5  
$ colony        : Factor w/ 406 levels "0","1","10","100",..: 1 1 1 1 2 2 2 
$ Totvegproparea: num  0.00055 0.03956 0.95705 0.00284 0.05215 ...

as the seq in a for loop (for(var in seq)), and therefore first picked them 
out as 
cols = aggregate(list(A = vegaggr.BLMCMR02$Totvegproparea), 
                list(colony = vegaggr.BLMCMR02$colony), sum)[,1]
> str(cols)
 Factor w/ 406 levels "0","1","10","100",..: 1 2 3 4 5 6 7 8 9 10 ...

I next planned to transform cols using as.integer(cols). However, this 
transformation gives a vector corresponding to seq(from=1,to=length(cols)). 

Could someone please give advice on how to make use of a factor as the seq 
in a for loop (which is strictly not allowed' according to ?Control).

Thanks!

Tord



From murdoch at stats.uwo.ca  Mon Sep 19 19:40:53 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Sep 2005 13:40:53 -0400
Subject: [R] Change in behavior of compare statement
In-Reply-To: <f55480e20509190925d85873c@mail.gmail.com>
References: <f55480e20509190925d85873c@mail.gmail.com>
Message-ID: <432EF825.9070706@stats.uwo.ca>

On 9/19/2005 12:25 PM, Tomas Andersson wrote:
> Dear all, 
> 
> I have come across some Windows Script code which calls the
> "file.exists" function in R to check for the existence of a particular
> file on an R server. This is what the code looks like:
> 
>             do
>             {
>                         // some useful code
>             }
>             while (m_workspace.session.eval("file.exists(" + dataFile
> + ")") != "1");

What is this function m_workspace.session.eval?  It's not a base 
function.  Perhaps its definition has changed?
> 
> It appears that the behavior of this code is different when used with
> R version 2.1.1 compared to R version 1.9.1. If the compare statement
> s is changed from
> 
>             while (s != "1")
> to
>             while (s != "True")
> or
>             while !(s)
> 
> the behavior of the code is the same with both versions of R.
> 
> After reading the R documentation, I have not found any evidence that
> the original syntax (s != "1") is in any way recommended or supported
> (in version 2.1.1). Still, my questions are:

s != "1" is perfectly valid R code.  It will evaluate to TRUE in at 
least the following case:

as.character(s) is not an NA, but isn't "1".

s != "True" will evaluate to TRUE in most cases, unless s happens to 
contain that exact string.
> 
> 1. Is it possible that the syntax has been supported in older versions of R?
> 2. Is the reason for the recent change in behavior known and
> documented anywhere?

The syntax is fine, but it looks like a behaviour change in that 
function m_workspace.session.eval.  You'll need to ask the author of 
that function what is happening.

Duncan Murdoch



From Ted.Harding at nessie.mcc.ac.uk  Mon Sep 19 19:36:41 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 19 Sep 2005 18:36:41 +0100 (BST)
Subject: [R] Teaching R - In front of the computer?
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E69FDD44@HERMES.demogr.mpg.de>
Message-ID: <XFMail.050919183641.Ted.Harding@nessie.mcc.ac.uk>

On 19-Sep-05 Rau, Roland wrote:
> Dear R-Users,
> 
> given you have been teaching R to students (grad level, mainly
> social science background, no previous programming experience,
> 80% know SPSS),what are your experiences concerning the style
> of teaching? Do you prefer to stand in front of the class like
> in "normal" lectures and you show them slides? Or do you you
> explain some concept (for example things like mydata[order(var1,
> var2),]) and show it directly on the computer via beamer/projector
> and also the students have to enter it on the computers in front
> of them.
> 
> Any experiences you can share are highly appreciated.
> 
> Thanks,
> Roland

I'd like to suggest (perhaps more for discussion than for
implementation) thinking of the use of 'xmx' ("X MultipleXer").
See

  http://www.cs.brown.edu/software/xmx/

This work in X Windows on Unix/Linux. The essence of it is that
the software is installed on each of a group of networked computers.
One of these is the "master", who starts it up in "Floor" mode
and nominates any or all of the others to be in "Floor", "Seat"
or "View" mode (terminilogy apparently inspired by what happens
at meetings).

This causes an W window to appear on all nominated computers,
within which is running a virutal X session. Anyone with "Floor"
rights can do anything in that session, and it will be mirrored
to all others. This includes keyboard input and mouse movements.
Anyone in "View" mode an only see what is going on, but cannot
make any input. In "Seat" mode, a user can virtually "raise their
hand" and be granted "Floor" privileges.

Anything anybody does outside the XMX window is private to them,
and will not be seen by others, nor have any effect on the XMX
session.

This software seems not to have undergone further development
in its public form for some time (apparently June 1999). I don't
know if there have been further developments in a private or
proprietary form.

Its creator, John Bazik at Brown University, states that "Xmx
provides reasonable real-time performance on a local 10 Mb/sec
ethernet on 52 UltraSparc-class machines."

It might well provide a good vehicle for expository teaching
in a class that large, since each student can see on their
own screen what the instructor is displaying (and this could
include a "slide-show" presentation alongside code files,
commands being run in an R window, graphics plots, etc.).

However, you would not want a class that size to all be in
"Floor" mode! "Seat" mode could be OK, though, since anyone
who wants to ask a question can go into "Floor" and then thsy
can input things into the XMX session, or even just move their
mouse around to indicate what they are talking about -- something
which in a normal class could only be done by coming out to the
front.

I have used it for teaching on a much smaller scale -- tutorial
sessions with 1 or 2 others, where everyone can be in "Floor"
mode and can make their own spontaneous changes to what's ging
on without any bureaucracy. This can include having everyone
participate in editing a file of R commands, so that everyone
can see what the effect of doing this or that is. I have found
it to work well, and to be appreciated by students.

This mode could work with more than 1 or 2 students.

The main snag with using XMX is that the X display must be the
same on all machines. This was a deliberate design implementation,
since it cuts down enormously on network traffic. However, it
does mean that all machines must have a sufficiently large overlap
in X resouces (i.e. as reported by 'xdpyinfo').

The resources don't have to be absolutely identical, but certainly
colour-depths must be the same all round, and in my experience
you had better have the same display geometry (e.g. 1280x1024),
and there are various other things. One way to find out is to
start it up nominating all the participating machines at once.
If there's a compatible sufficient overlap, it will work.

However, in a teaching Lab with all the machines the same, it
should be straightforward to get it going.

One nice thing about doing things this way is that a student
could be running a private R session alongside the public XMX
one, and copy&paste from the public one into the private one
(or vice versa). They could also be running an editor session
to write their own course notes.

Anyway, there it is if you hadn't heard of it.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 19-Sep-05                                       Time: 18:36:32
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Mon Sep 19 20:01:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Sep 2005 20:01:07 +0200
Subject: [R] factor as seq() in for loop
In-Reply-To: <1127151294.432ef6beb14d7@www2.helsinki.fi>
References: <1127151294.432ef6beb14d7@www2.helsinki.fi>
Message-ID: <x28xxt9bto.fsf@turmalin.kubism.ku.dk>

Tord Snall <Tord.Snall at helsinki.fi> writes:

> Dear all,
> I would like to use the values in vegaggr.BLMCMR02$colony 
> 
> str(vegaggr.BLMCMR02)
> `data.frame':   1678 obs. of  3 variables:
> $ vegtype       : Factor w/ 27 levels "2010","2020",..: 3 4 5 19 4 5 19 5  
> $ colony        : Factor w/ 406 levels "0","1","10","100",..: 1 1 1 1 2 2 2 
> $ Totvegproparea: num  0.00055 0.03956 0.95705 0.00284 0.05215 ...
> 
> as the seq in a for loop (for(var in seq)), and therefore first picked them 
> out as 
> cols = aggregate(list(A = vegaggr.BLMCMR02$Totvegproparea), 
>                 list(colony = vegaggr.BLMCMR02$colony), sum)[,1]
> > str(cols)
>  Factor w/ 406 levels "0","1","10","100",..: 1 2 3 4 5 6 7 8 9 10 ...
> 
> I next planned to transform cols using as.integer(cols). However, this 
> transformation gives a vector corresponding to seq(from=1,to=length(cols)). 

Yes, it would do that... The construction looks a bit complicated,
though. If you want the levels of the factor, wouldn't
levels(vegaggr.BLMCMR02$colony) do it?
 
> Could someone please give advice on how to make use of a factor as the seq 
> in a for loop (which is strictly not allowed' according to ?Control).

You're not exactly being clear about what the for loop should achieve!


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ggrothendieck at gmail.com  Mon Sep 19 20:10:03 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Sep 2005 14:10:03 -0400
Subject: [R] factor as seq() in for loop
In-Reply-To: <1127151294.432ef6beb14d7@www2.helsinki.fi>
References: <1127151294.432ef6beb14d7@www2.helsinki.fi>
Message-ID: <971536df0509191110691d4d5e@mail.gmail.com>

On 9/19/05, Tord Snall <Tord.Snall at helsinki.fi> wrote:
> Dear all,
> I would like to use the values in vegaggr.BLMCMR02$colony
> 
> str(vegaggr.BLMCMR02)
> `data.frame':   1678 obs. of  3 variables:
> $ vegtype       : Factor w/ 27 levels "2010","2020",..: 3 4 5 19 4 5 19 5
> $ colony        : Factor w/ 406 levels "0","1","10","100",..: 1 1 1 1 2 2 2
> $ Totvegproparea: num  0.00055 0.03956 0.95705 0.00284 0.05215 ...
> 
> as the seq in a for loop (for(var in seq)), and therefore first picked them
> out as
> cols = aggregate(list(A = vegaggr.BLMCMR02$Totvegproparea),
>                list(colony = vegaggr.BLMCMR02$colony), sum)[,1]
> > str(cols)
>  Factor w/ 406 levels "0","1","10","100",..: 1 2 3 4 5 6 7 8 9 10 ...
> 
> I next planned to transform cols using as.integer(cols). However, this
> transformation gives a vector corresponding to seq(from=1,to=length(cols)).
> 
> Could someone please give advice on how to make use of a factor as the seq
> in a for loop (which is strictly not allowed' according to ?Control).
> 

What are you trying to loop over?

Species <- head(iris$Species)
for(i in seq(Species)) print(Species[i])

will loop over the elements of the factor Species and you can use

for(lev in levels(Species)) print(lev)

to loop over its levels.



From chrisb at fcdarwin.org.ec  Mon Sep 19 19:20:22 2005
From: chrisb at fcdarwin.org.ec (Chris Buddenhagen)
Date: Mon, 19 Sep 2005 11:20:22 -0600
Subject: [R] library (tree)- which samples belong to each terminal branch?
Message-ID: <006701c5bd3e$6af9e070$4c01a8c0@Chris>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050919/2984ab18/attachment.pl

From jfox at mcmaster.ca  Mon Sep 19 20:24:53 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 19 Sep 2005 14:24:53 -0400
Subject: [R] Teaching R - In front of the computer?
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E69FDD44@HERMES.demogr.mpg.de>
Message-ID: <web-104313520@cgpsrv2.cis.mcmaster.ca>

Dear Roland,

I've taught the use of R to this kind of audience many times. Take a
look at
<http://socserv.socsci.mcmaster.ca/jfox/Courses/UCLA/index.html> for
materials used in such a workshop, and at
<http://socserv.socsci.mcmaster.ca/jfox/Teaching-with-R.pdf> for a
paper on teaching social statistics with R.

As others have suggested, using static slides is not a good idea, and
having at least a live display for the presenter is essential. It also
helps to have the students sitting at computers and able to try things
out for themselves. If this is a workshop devoted to R, I'd strongly
recommend this format.

On the other hand, if you're teaching R in the context of a more
general statistics course, you can cover the basics in a hands-on
workshop and then use the LCD projector to introduce new commands,
etc., during the course as they're needed. I find that once they've
acquired the basics, students are able to work more independently.

I hope this helps,
 John

On Mon, 19 Sep 2005 15:25:14 +0200
 "Rau, Roland" <Rau at demogr.mpg.de> wrote:
> Dear R-Users,
> 
> given you have been teaching R to students (grad level, mainly social
> science background, no previous programming experience, 80% know
> SPSS),
> what are your experiences concerning the style of teaching? Do you
> prefer to stand in front of the class like in "normal" lectures and
> you
> show them slides? Or do you you explain some concept (for example
> things
> like mydata[order(var1, var2),]) and show it directly on the computer
> via beamer/projector and also the students have to enter it on the
> computers in front of them.
> 
> Any experiences you can share are highly appreciated.
> 
> Thanks,
> Roland
> 
> +++++
> This mail has been sent through the MPI for Demographic
> Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From tom at maladmin.com  Mon Sep 19 16:36:04 2005
From: tom at maladmin.com (tom wright)
Date: Mon, 19 Sep 2005 10:36:04 -0400
Subject: [R] waveform filtering
Message-ID: <1127140564.4521.17.camel@localhost.localdomain>

I'm not an engineer so I hope I'm using the correct terminology here. I
have a recorded waveform that I want to apply low and high pass filters
too, are tehre already R functions existing to do this or am I going to
have to program my own?
thanks for any pointers
tom



From tts_boopathy at yahoo.com  Mon Sep 19 21:30:10 2005
From: tts_boopathy at yahoo.com (shanmuha boopathy)
Date: Mon, 19 Sep 2005 12:30:10 -0700 (PDT)
Subject: [R] how to extract the column name or value from the numerical
	value of the matrix
Message-ID: <20050919193010.91321.qmail@web33810.mail.mud.yahoo.com>

Dear sir,
 i have a matrix like
 x<-c(1:20)
A<-matrix(x,4,5)
> A
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    5    9   13   17
[2,]    2    6   10   14   18
[3,]    3    7   11   15   19
[4,]    4    8   12   16   20

I want to extract the column value for the matrix
value 11...

or the row value for 14..
how it is possible?

thanks for your help....

with regards,
boopathy.

Thirumalai Shanmuha Boopathy, 
Zimmer no : 07-15,
R??tscher strasse 165, 
52072  Aachen . 
Germany.
 
Home zone   :  0049 - 241 - 9813409
Mobile zone :  0049 - 176 - 23567867



From ripley at stats.ox.ac.uk  Mon Sep 19 21:41:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Sep 2005 20:41:15 +0100 (BST)
Subject: [R] library (tree)- which samples belong to each terminal
	branch?
In-Reply-To: <006701c5bd3e$6af9e070$4c01a8c0@Chris>
References: <006701c5bd3e$6af9e070$4c01a8c0@Chris>
Message-ID: <Pine.LNX.4.61.0509192038250.5089@gannet.stats>

On Mon, 19 Sep 2005, Chris Buddenhagen wrote:

> What I would like to know is if there is a way to know which specific
> samples fall within a terminal branch created by tree? The classification
> summarizes data, but I want to know which specific samples fall within each
> classification (branch in the dendrogram).

Please read the help page (as we do ask).  Component `where' in the 
returned object tells you the information you are seeking.

If you want this for new data, predict.tree has a type="where" to tell 
you.

> Data contains site codes and multiple fields describing the characters of a
> single specimen of grass growing there. Each sample is an individual plant
> from a site, all plants are similar to each other, but may represent
> different species. I classified species using tree and these are labeled by
> site name. The classification gives me a nice separation of supposed species
> in terms of only 3 sites but with some sites occurring on more than one
> terminal branch, this obviously summarizes one or more species/sites on each
> branch of the dendrogram. So back to my question, how do I find out which
> samples fall within each branch?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From greg.snow at ihc.com  Mon Sep 19 21:56:02 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Mon, 19 Sep 2005 13:56:02 -0600
Subject: [R] how to extract the column name or value from the numerical
 value of the matrix
Message-ID: <s32ec37c.088@lp-msg1.co.ihc.com>

look at ?col and ?row.  One way to use them is:

col(A)[A==11]
row(A)[A==14]

hope this helps,

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> shanmuha boopathy <tts_boopathy at yahoo.com> 09/19/05 01:30PM >>>
Dear sir,
 i have a matrix like
 x<-c(1:20)
A<-matrix(x,4,5)
> A
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    5    9   13   17
[2,]    2    6   10   14   18
[3,]    3    7   11   15   19
[4,]    4    8   12   16   20

I want to extract the column value for the matrix
value 11...

or the row value for 14..
how it is possible?

thanks for your help....

with regards,
boopathy.

Thirumalai Shanmuha Boopathy, 
Zimmer no : 07-15,
R??tscher strasse 165, 
52072  Aachen . 
Germany.
 
Home zone   :  0049 - 241 - 9813409
Mobile zone :  0049 - 176 - 23567867

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mschwartz at mn.rr.com  Mon Sep 19 21:57:17 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 19 Sep 2005 14:57:17 -0500
Subject: [R] how to extract the column name or value from
	the	numerical	value of the matrix
In-Reply-To: <20050919193010.91321.qmail@web33810.mail.mud.yahoo.com>
References: <20050919193010.91321.qmail@web33810.mail.mud.yahoo.com>
Message-ID: <1127159837.20754.26.camel@localhost.localdomain>

On Mon, 2005-09-19 at 12:30 -0700, shanmuha boopathy wrote:
> Dear sir,
>  i have a matrix like
>  x<-c(1:20)
> A<-matrix(x,4,5)
> > A
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    5    9   13   17
> [2,]    2    6   10   14   18
> [3,]    3    7   11   15   19
> [4,]    4    8   12   16   20
> 
> I want to extract the column value for the matrix
> value 11...
> 
> or the row value for 14..
> how it is possible?
> 
> thanks for your help....
> 
> with regards,
> boopathy.


Why did you copy r-devel? r-help is the appropriate forum.


See ?which and take note of the 'arr.ind' argument:

> which(A == 11, arr.ind = TRUE)
     row col
[1,]   3   3


> which(A == 14, arr.ind = TRUE)
     row col
[1,]   2   4


HTH,

Marc Schwartz



From Kristin_Cothern at URSCorp.com  Mon Sep 19 22:29:38 2005
From: Kristin_Cothern at URSCorp.com (Kristin_Cothern@URSCorp.com)
Date: Mon, 19 Sep 2005 13:29:38 -0700
Subject: [R] restore file magic number
Message-ID: <OF87EFFEDB.468C0426-ON85257081.006F758B-88257081.00709427@urscorp.com>

Hello,

I recently lost access to my workspace developed over a long time.  When I
try to load this work space (called Casmalia.Rdata) or double click on it
to open it in R, I see  a popup which says (Embedded image moved to file:
pic17720.jpg)
:
And under this, the last line in the console commands is:   "Bad restore
file magic number.  (File may be corrupted) - no data loaded."

The Casmalia.Rdata file looks the right size.  But is apparently missing a
magic number.  Can this file be fixed in an editor such as Textpad so that
it can be restored?


-----------------------------------------------
Kristin Cothern, URS Corporation
130 Robin Hill Road
Santa Barbara, California 93117
tel.  805-964-6010



 This e-mail and any attachments are confidential. If you receive this
message in error or are not the intended recipient, you should not retain,
distribute, disclose or use any of this information and you should destroy
the e-mail and any attachments or copies.

From murdoch at stats.uwo.ca  Mon Sep 19 22:48:33 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Sep 2005 16:48:33 -0400
Subject: [R] Change in behavior of compare statement
In-Reply-To: <432EF825.9070706@stats.uwo.ca>
References: <f55480e20509190925d85873c@mail.gmail.com>
	<432EF825.9070706@stats.uwo.ca>
Message-ID: <432F2421.3030608@stats.uwo.ca>

On 9/19/2005 1:40 PM, Duncan Murdoch wrote:
> On 9/19/2005 12:25 PM, Tomas Andersson wrote:
>> Dear all, 
>> 
>> I have come across some Windows Script code which calls the
>> "file.exists" function in R to check for the existence of a particular
>> file on an R server. This is what the code looks like:
>> 
>>             do
>>             {
>>                         // some useful code
>>             }
>>             while (m_workspace.session.eval("file.exists(" + dataFile
>> + ")") != "1");
> 
> What is this function m_workspace.session.eval?  It's not a base 
> function.  Perhaps its definition has changed?

This has to be one of the most out-to-lunch answers I ever wrote. 
Sorry, please ignore it.

The code above is "Windows Script", whatever that is, not R. 
file.exists hasn't changed, but whatever interface you're using to R 
presumably has.  In the code you're looking at there, it converted TRUE 
to "1", and now it doesn't.

Duncan Murdoch
>> 
>> It appears that the behavior of this code is different when used with
>> R version 2.1.1 compared to R version 1.9.1. If the compare statement
>> s is changed from
>> 
>>             while (s != "1")
>> to
>>             while (s != "True")
>> or
>>             while !(s)
>> 
>> the behavior of the code is the same with both versions of R.
>> 
>> After reading the R documentation, I have not found any evidence that
>> the original syntax (s != "1") is in any way recommended or supported
>> (in version 2.1.1). Still, my questions are:
> 
> s != "1" is perfectly valid R code.  It will evaluate to TRUE in at 
> least the following case:
> 
> as.character(s) is not an NA, but isn't "1".
> 
> s != "True" will evaluate to TRUE in most cases, unless s happens to 
> contain that exact string.
>> 
>> 1. Is it possible that the syntax has been supported in older versions of R?
>> 2. Is the reason for the recent change in behavior known and
>> documented anywhere?
> 
> The syntax is fine, but it looks like a behaviour change in that 
> function m_workspace.session.eval.  You'll need to ask the author of 
> that function what is happening.
> 
> Duncan Murdoch
>



From murdoch at stats.uwo.ca  Mon Sep 19 22:52:38 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Sep 2005 16:52:38 -0400
Subject: [R] restore file magic number
In-Reply-To: <OF87EFFEDB.468C0426-ON85257081.006F758B-88257081.00709427@urscorp.com>
References: <OF87EFFEDB.468C0426-ON85257081.006F758B-88257081.00709427@urscorp.com>
Message-ID: <432F2516.6060507@stats.uwo.ca>

On 9/19/2005 4:29 PM, Kristin_Cothern at URSCorp.com wrote:
> Hello,
> 
> I recently lost access to my workspace developed over a long time.  When I
> try to load this work space (called Casmalia.Rdata) or double click on it
> to open it in R, I see  a popup which says (Embedded image moved to file:
> pic17720.jpg)
> :
> And under this, the last line in the console commands is:   "Bad restore
> file magic number.  (File may be corrupted) - no data loaded."
> 
> The Casmalia.Rdata file looks the right size.  But is apparently missing a
> magic number.  Can this file be fixed in an editor such as Textpad so that
> it can be restored?

It's probably missing a lot more than a magic number.  That's just a 
signal that the file format isn't really an R workspace any more.

You need to get an old copy from a backup.

If you're desperate and short of a backup, you could try looking at the 
file in a binary dump program to see if there's anything left of the 
contents (perhaps only part of it got trashed), and maybe try to restore 
just the good stuff, but it's not going to be easy, and the only real 
documentation for what to do is to look at the R source code.

Duncan Murdoch



From m_nica at hotmail.com  Mon Sep 19 23:06:40 2005
From: m_nica at hotmail.com (Mihai Nica)
Date: Mon, 19 Sep 2005 16:06:40 -0500
Subject: [R] text editor TinR?
In-Reply-To: <432F2421.3030608@stats.uwo.ca>
Message-ID: <BAY105-F38B0E426C780A0E11C5103F8920@phx.gbl>

Greetings,

Please help me remember the name of the "tiny" text editor that works with 
R.... TinR maybe? I cannot find it at all, and cannot remember it, it is 
really frustrating...

Thanks,

Mihai Nica, ABD
Jackson State University
170 East Griffith Street G5
Jackson, MS 39201
601 914 0361



From khlin at odin.mdacc.tmc.edu  Mon Sep 19 23:33:26 2005
From: khlin at odin.mdacc.tmc.edu (kevin Lin)
Date: Mon, 19 Sep 2005 16:33:26 -0500
Subject: [R] pairwise comparisons among treatments
Message-ID: <a06002002bf54d8b50392@[143.111.133.119]>

Hello R listing,

I did two-way anova on lm. Further question the investigator 
interested in is what two treatments are different?

I am looking for a command which could do pairwise comparison for 
every treatment.

Could anyone help me out?

Thanks a bunch

Kevin



From jfox at mcmaster.ca  Mon Sep 19 23:35:21 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 19 Sep 2005 17:35:21 -0400
Subject: [R] text editor TinR?
In-Reply-To: <BAY105-F38B0E426C780A0E11C5103F8920@phx.gbl>
Message-ID: <20050919213520.HVYP21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Mihai,

It's Tinn-R (with two n's), at <http://www.sciviews.org/Tinn-R/>.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mihai Nica
> Sent: Monday, September 19, 2005 4:07 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] text editor TinR?
> 
> Greetings,
> 
> Please help me remember the name of the "tiny" text editor 
> that works with R.... TinR maybe? I cannot find it at all, 
> and cannot remember it, it is really frustrating...
> 
> Thanks,
> 
> Mihai Nica, ABD
> Jackson State University
> 170 East Griffith Street G5
> Jackson, MS 39201
> 601 914 0361



From p.dalgaard at biostat.ku.dk  Mon Sep 19 23:44:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Sep 2005 23:44:41 +0200
Subject: [R] text editor TinR?
In-Reply-To: <BAY105-F38B0E426C780A0E11C5103F8920@phx.gbl>
References: <BAY105-F38B0E426C780A0E11C5103F8920@phx.gbl>
Message-ID: <x2r7bkg2ba.fsf@turmalin.kubism.ku.dk>

"Mihai Nica" <m_nica at hotmail.com> writes:

> Greetings,
> 
> Please help me remember the name of the "tiny" text editor that works with 
> R.... TinR maybe? I cannot find it at all, and cannot remember it, it is 
> really frustrating...

http://www.sciviews.org/Tinn-R/


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From reid_huntsinger at merck.com  Mon Sep 19 23:48:01 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 19 Sep 2005 17:48:01 -0400
Subject: [R] waveform filtering
Message-ID: <355C35514FEAC9458F75947F5270974D076CDA@usctmx1103.merck.com>

You should probably have a look at the sound packages for R, "tuneR" and
"sound", I believe, on http://cran.r-project.org. 

Applying a filter can be done with filter(), but you need to come up with
filter coefficients. "High-pass" and "low-pass" have simple descriptions in
the Fourier transform space, so you might want to specify the frequency
response of your filter directly there, then do an inverse Fourier transform
(fft() in R) to get coefficients. 

The ingredients are all there in R itself; but the packages tuneR and sound
might have exactly what you want. A book on time series or signal processing
might be helpful.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of tom wright
Sent: Monday, September 19, 2005 10:36 AM
To: R-help mailing list
Subject: [R] waveform filtering


I'm not an engineer so I hope I'm using the correct terminology here. I
have a recorded waveform that I want to apply low and high pass filters
too, are tehre already R functions existing to do this or am I going to
have to program my own?
thanks for any pointers
tom

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Mon Sep 19 23:57:12 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 19 Sep 2005 14:57:12 -0700
Subject: [R] pairwise comparisons among treatments
In-Reply-To: <a06002002bf54d8b50392@[143.111.133.119]>
Message-ID: <200509192157.j8JLvDrF011332@ohm.gene.com>


help.search('pairwise comparison')

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of kevin Lin
> Sent: Monday, September 19, 2005 2:33 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] pairwise comparisons among treatments
> 
> Hello R listing,
> 
> I did two-way anova on lm. Further question the investigator 
> interested in is what two treatments are different?
> 
> I am looking for a command which could do pairwise comparison for 
> every treatment.
> 
> Could anyone help me out?
> 
> Thanks a bunch
> 
> Kevin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From khlin at odin.mdacc.tmc.edu  Tue Sep 20 00:33:32 2005
From: khlin at odin.mdacc.tmc.edu (kevin Lin)
Date: Mon, 19 Sep 2005 17:33:32 -0500
Subject: [R] pairwise comparisons among treatments
In-Reply-To: <200509192157.j8JLvDrF011332@ohm.gene.com>
References: <200509192157.j8JLvDrF011332@ohm.gene.com>
Message-ID: <a06002003bf54eb947039@[143.111.133.119]>

Hello Bert,

Thanks for your response. I don't know why I am getting following 
error messages. I use version of "R version 2.1.1, 2005-06-20, 
powerpc-apple-darwin7.9.0 " in Mac OS X version 10.3.8.
I did try to load following packages before, but it failed.

help.search("pairwise comparison")
Error in help.search("pairwise comparison") :
	could not find package 'arrayQuality.1'
In addition: Warning messages:
1: no Rd contents for package 'R2HTML' in 
'/Library/Frameworks/R.framework/Resources/library' in: 
help.search("pairwise comparison")
2: no Rd contents for package 'affylmGUI' in 
'/Library/Frameworks/R.framework/Resources/library' in: 
help.search("pairwise comparison")

Thanks.

Kevin

>help.search('pairwise comparison')
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
>
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
>
>
>



From m_nica at hotmail.com  Tue Sep 20 00:39:01 2005
From: m_nica at hotmail.com (Mihai Nica)
Date: Mon, 19 Sep 2005 17:39:01 -0500
Subject: [R] text editor TinR?
In-Reply-To: <x2r7bkg2ba.fsf@turmalin.kubism.ku.dk>
Message-ID: <BAY105-F154CD55920E28015A9F91DF8920@phx.gbl>

Thanks to everybody answering my call for help. It is Tinn-R!

Mihai Nica, ABD
Jackson State University
170 East Griffith Street G5
Jackson, MS 39201
601 914 0361



From dmbates at gmail.com  Tue Sep 20 01:27:51 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 19 Sep 2005 18:27:51 -0500
Subject: [R] How to mimic pdMat of lme under lmer?
In-Reply-To: <432EE627.3020802@cropdesign.com>
References: <432EE627.3020802@cropdesign.com>
Message-ID: <40e66e0b05091916272f4229e9@mail.gmail.com>

On 9/19/05, Joris De Wolf <joris.dewolf at cropdesign.com> wrote:
> Dear members,
>
> I would like to switch from nlme to lme4 and try to translate some of my
> models that worked fine with lme.
> I have problems with the pdMat classes.
>
> Below a toy dataset with a fixed effect F and a random effect R. I gave
> also 2 similar lme models.
> The one containing pdLogChol (lme1) is easy to translate (as it is an
> explicit notation of the default model)
> The more parsimonious model with pdDiag replacing pdLogChol I cannot
> reproduce with lmer. The obvious choice for me would be my model lmer2,
> but this is yielding different result.
>
> Somebody any idea?
> Thanks,
> Joris
>
> I am using R version 2.1.0 for Linux
> and the most recent downloads of Matrix and  nlme
>
> #dataset from McLean, Sanders and Stroup
> F <- factor(c(1,1,1,1,1,1,2,2,2,2,2,2))
> R <- factor(c(1,1,2,2,3,3,1,1,2,2,3,3))
> s <-
> c(51.43,51.28,50.93,50.75,50.47,50.83,51.91,52.43,52.26,52.33,51.58,51.23)
> DS <- data.frame(F,R,s)
> DS$F <- as.factor(DS$F)
> DS$R <- as.factor(DS$R)
>
> library(nlme)
> lme1 <- lme(data = DS,s ~ F,random = list(R = pdLogChol(~F)))
> lme2 <- lme(data = DS,s ~ F,random = list(R = pdDiag(~F)))
> summary(lme1)
> summary(lme2)
>
> library(lme4)
> lmer1 <- lmer(data = DS,s ~ F + (F|R))
> lmer2 <- lmer(data = DS,s ~ F + (1|R) + (1|F))
> summary(lmer1)
> summary(lmer2)

You need to generate the two columns of the indicator matrix for F as
separate model matrices.  This will involve some awkward construction
like

> (fm1 <- lmer(s ~ F +(F|R), DS))
Linear mixed-effects model fit by REML
Formula: s ~ F + (F | R)
   Data: DS
      AIC      BIC    logLik MLdeviance REMLdeviance
 20.69259 23.60203 -4.346297    6.03915     8.692593
Random effects:
 Groups   Name        Variance Std.Dev. Corr
 R        (Intercept) 0.108796 0.32984
          F2          0.102008 0.31939  -0.014
 Residual             0.048525 0.22028
# of obs: 12, groups: R, 3

Fixed effects:
            Estimate Std. Error DF  t value  Pr(>|t|)
(Intercept)  50.9483     0.2106 10 241.9188 < 2.2e-16
F2            1.0083     0.2240 10   4.5014  0.001141
> (fm2 <- lmer(s ~ F + (0+as.numeric(F==1)|R) + (0+as.numeric(F==2)|R), DS))
Linear mixed-effects model fit by REML
Formula: s ~ F + (0 + as.numeric(F == 1) | R) + (0 + as.numeric(F ==
2) |      R)
   Data: DS
      AIC      BIC    logLik MLdeviance REMLdeviance
 19.62621 22.05075 -4.813107   7.439584     9.626215
Random effects:
 Groups   Name               Variance Std.Dev.
 R        as.numeric(F == 1) 0.108796 0.32984
 R        as.numeric(F == 2) 0.207896 0.45596
 Residual                    0.048525 0.22028
# of obs: 12, groups: R, 3; R, 3

Fixed effects:
            Estimate Std. Error DF  t value Pr(>|t|)
(Intercept) 50.94833    0.21060 10 241.9188  < 2e-16
F2           1.00833    0.34891 10   2.8899  0.01611

There are probably more elegant ways of doing this but I don't think
there is any really clean way.  If you want to assume that all the
variances are equal then you can estimate the model using an
interaction.

> (fm3 <- lmer(s ~ F + (1|F:R), DS))
Linear mixed-effects model fit by REML
Formula: s ~ F + (1 | F:R)
   Data: DS
      AIC     BIC    logLik MLdeviance REMLdeviance
 17.77917 19.7188 -4.889587   7.669022     9.779175
Random effects:
 Groups   Name        Variance Std.Dev.
 F:R      (Intercept) 0.158346 0.39793
 Residual             0.048525 0.22028
# of obs: 12, groups: F:R, 6

Fixed effects:
            Estimate Std. Error DF  t value Pr(>|t|)
(Intercept) 50.94833    0.24672 10 206.5049  < 2e-16
F2           1.00833    0.34891 10   2.8899  0.01611


>
>
>
> confidentiality notice:
> The information contained in this e-mail is confidential and...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Murraypu at aimnsw.com.au  Tue Sep 20 01:39:29 2005
From: Murraypu at aimnsw.com.au (Murray Pung)
Date: Tue, 20 Sep 2005 09:39:29 +1000
Subject: [R] Teaching R - In front of the computer?
Message-ID: <3028F4C4647C9043B870276E28C69FD6B56867@syd05.aimnsw.com.au>

>From the prospective of a student, I highly recommend the hands on approach. I had the best outcomes when I was provided with examples, and then encouraged to make my own modifications in a lab / class environment, with help available. This approach allows students to build the skills required to program independently, without becoming overwhelmed at the start. 

Murray

Murray Pung | Research Analyst
AIM Research & HR Consulting
PO Box 328, Nth Sydney NSW 2060
P +61 (02) 9956 3951
F +61 (02) 9922 2210
www.aimsurveys.com.au


-----Original Message-----
From: John Fox [mailto:jfox at mcmaster.ca]
Sent: Tuesday, 20 September 2005 4:25 AM
To: Rau, Roland
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Teaching R - In front of the computer?


Dear Roland,

I've taught the use of R to this kind of audience many times. Take a
look at
<http://socserv.socsci.mcmaster.ca/jfox/Courses/UCLA/index.html> for
materials used in such a workshop, and at
<http://socserv.socsci.mcmaster.ca/jfox/Teaching-with-R.pdf> for a
paper on teaching social statistics with R.

As others have suggested, using static slides is not a good idea, and
having at least a live display for the presenter is essential. It also
helps to have the students sitting at computers and able to try things
out for themselves. If this is a workshop devoted to R, I'd strongly
recommend this format.

On the other hand, if you're teaching R in the context of a more
general statistics course, you can cover the basics in a hands-on
workshop and then use the LCD projector to introduce new commands,
etc., during the course as they're needed. I find that once they've
acquired the basics, students are able to work more independently.

I hope this helps,
 John

On Mon, 19 Sep 2005 15:25:14 +0200
 "Rau, Roland" <Rau at demogr.mpg.de> wrote:
> Dear R-Users,
> 
> given you have been teaching R to students (grad level, mainly social
> science background, no previous programming experience, 80% know
> SPSS),
> what are your experiences concerning the style of teaching? Do you
> prefer to stand in front of the class like in "normal" lectures and
> you
> show them slides? Or do you you explain some concept (for example
> things
> like mydata[order(var1, var2),]) and show it directly on the computer
> via beamer/projector and also the students have to enter it on the
> computers in front of them.
> 
> Any experiences you can share are highly appreciated.
> 
> Thanks,
> Roland
> 
> +++++
> This mail has been sent through the MPI for Demographic
> Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Tue Sep 20 06:54:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Sep 2005 21:54:52 -0700
Subject: [R] Are least-squares means useful or appropriate?
In-Reply-To: <432AAB98.4040103@unileon.es>
References: <432AAB98.4040103@unileon.es>
Message-ID: <432F961C.5060404@pdf.com>

Estimado Felipe:

	  If you provide a very simple example (as suggested in the posting 
guide, www.R-project.org/posting-guide.html), it would allow those of 
use who rarely use SAS to respond.  Try to think of the simplest 
possible toy data set and analysis that shows the difference between the 
SAS answer and the answer you get from a certain R function.  If you 
post something simple of that nature that someone can copy from your 
email into R and try other things in a minute or two, it will likely 
increase the speed and utility of a reply.

	  Buena Suerte,
	  spencer graves	

Felipe wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Hi.
> I have been reading about the convenience of using least-squares means
> (a. k. a. adjusted means) in multiple comparisons (I used to resort to
> them when using SAS). I even read a post in this list warning against
> them, but not giving much detail. What do you think about this?
> Greetings.
> 
> Felipe
> -----BEGIN PGP SIGNATURE-----
> 
> iEYEARECAAYFAkMqq5gACgkQWtdQtNzjBl4AigCfQJ64O0wrdYK/1iMReW5RtI1d
> tMIAn3DQSdk+4D7AK7VQGtWo0TElrFG7
> =j9EX
> -----END PGP SIGNATURE-----
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Tue Sep 20 06:59:23 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Sep 2005 21:59:23 -0700
Subject: [R] logistic regression with nominal predictors
In-Reply-To: <432741E8.1000702@gmx.net>
References: <432741E8.1000702@gmx.net>
Message-ID: <432F972B.7090302@pdf.com>

	  This sounds to me like a great research project that could be 
answered relatively easily with a Monte Carlo study.  An execllent 
mathematician might be able to produce simple theoretical limits on the 
error from using ranks or normal scores, but such limits would likely be 
much wider than one could get in a typical case using Monte Carlo.  And 
Monte Carlo in R is normally fairly easy even for quite complicated 
situations.

	  spencer graves

Ram??n Casero Ca??as wrote:

> (Sorry for obvious mistakes, as I am quite a newby with no Statistics
> background).
> 
> My question is going to be what is the gain of logistic regression over
> odds ratios when none of the input variables is continuous.
> 
> 
> My experiment:
>  Outcome: ordinal scale, ``quality'' (QUA=1,2,3)
>  Predictors: ``segment'' (SEG) and ``stress'' (STR). SEG is
>              nominal scale with 24 levels, and STR is dychotomous (0,1).
> 
> 
> 
> Considering the outcome continuous, two-way ANOVA with
> 
> aov(as.integer(QUA) ~ SEG * STR)
> 
> doesn't find evidence of interaction between SEG and STR, and they are
> significant on their own. This is the result that we would expect from
> clinical knowledge.
> 
> 
> 
> I use
> 
> xtabs(~QUA+SEG, data=data2.df, subset=STR==0)
> xtabs(~QUA+SEG, data=data2.df, subset=STR==0)
> 
> for the contingency tables. There are zero cells, and for some values of
> SEG, there is only one none-zero cell, i.e. some values of SEG determine
> the output with certainty.
> 
> So initially I was thinking of a proportional odds logistic regression
> model, but following Hosmer and Lemeshow [1], zero cells are
> problematic. So I take out of the data table the deterministic values of
> SEG, and I pool QUA=2 and QUA=3, and now I have a dychotomous outcome
> (QUA = Good/Bad) and no zero cells.
> 
> The following model doesn't find evidence of interaction
> 
> glm(QUA ~ STR * SEG, data=data3.df, family=binomial)
> 
> so I go for
> 
> glm(QUA ~ STR + SEG, data=data3.df, family=binomial)
> 
> 
> (I suppose that what glm does is to create design variables for SEG,
> where 0 0 ... 0 is for the first value of SEG, 1 0 ... 0 for the second
> value, 0 1 0 ... 0 for the third, etc).
> 
> Coefficients:
>               Estimate Std. Error   z value Pr(>|z|)
> (Intercept) -1.085e+00  1.933e-01    -5.614 1.98e-08 ***
> STR.L        2.112e-01  6.373e-02     3.314 0.000921 ***
> SEGP2C.MI   -9.869e-01  3.286e-01    -3.004 0.002669 **
> SEGP2C.AI   -1.306e+00  3.585e-01    -3.644 0.000269 ***
> SEGP2C.AA   -1.743e+00  4.123e-01    -4.227 2.37e-05 ***
> [shortened]
> SEGP4C.ML   -5.657e-01  2.990e-01    -1.892 0.058485 .
> SEGP4C.BL   -2.908e-16  2.734e-01 -1.06e-15 1.000000
> SEGSAX.MS    1.092e-01  2.700e-01     0.405 0.685772
> SEGSAX.MAS  -5.441e-16  2.734e-01 -1.99e-15 1.000000
> SEGSAX.MA    7.130e-01  2.582e-01     2.761 0.005758 **
> SEGSAX.ML    1.199e+00  2.565e-01     4.674 2.96e-06 ***
> SEGSAX.MP    1.313e+00  2.570e-01     5.108 3.26e-07 ***
> SEGSAX.MI    8.865e-01  2.569e-01     3.451 0.000558 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> (Dispersion parameter for binomial family taken to be 1)
> 
>     Null deviance: 3462.0  on 3123  degrees of freedom
> Residual deviance: 3012.6  on 3101  degrees of freedom
> AIC: 3058.6
> 
> Number of Fisher Scoring iterations: 6
> 
> 
> Even though some coefficients have no evidence of statistical
> significance, the model requires them from a clinical point of view.
> 
> At this point, the question would be how to interpret these results, and
> what advantage they offer over odds ratios. From [1] I can understand
> that in the case of a dychotomous and a continuous predictor, you can
> adjust for the continuous variable.
> 
> But when all predictors are dychotomous (due to the design variables), I
> don't quite see the effect of adjustment. Wouldn't it be better just to
> split the data in two groups (STR=0 and STR=1), and instead of using
> logistic regression, use odds ratios for each value of SEG?
> 
> Cheers,
> 
> Ram??n.
> 
> [1] D.W. Hosmer and S. Lemeshow. ``Applied Logistic Regression''.
> John-Wiley. 2000.
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From 2bingho at stanford.edu  Tue Sep 20 07:03:33 2005
From: 2bingho at stanford.edu (Bing Ho)
Date: Mon, 19 Sep 2005 22:03:33 -0700
Subject: [R] Transform variable number of rows per subject to column
 variables?
Message-ID: <6.2.5.4.2.20050919215545.02cfdc68@wheresmymailserver.com>

Hello,

I am very new to R, but I am having trouble with my dataset.

I have a data frame where a subject has a variable number of multiple 
observations for each row, which I wish the transform these 
observations to column variables.

An example of the data frame
ID 	TEST.A	TEST.B
1	10	1
1	13	2
1	11	1
2	15	2
2	17	3

And I wish to transform it to the following:
ID	TEST.A1	TEST.A2	TEST.A3	TEST.B1	TEST.B2	TEST.B3
1	10		13		11		1		2		1
2	15		17		NA		2		3		NA

In other words, for the variable number of repeated follow up 
studies, a new column variable for each subject, but they are grouped 
by the original test.

Thank you for any help - I'm realizing that I am a terrible programmer!

Bing Ho



From spencer.graves at pdf.com  Tue Sep 20 07:04:18 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Sep 2005 22:04:18 -0700
Subject: [R] nls()
In-Reply-To: <dg9m2q$pgi$1@sea.gmane.org>
References: <BF4C4B8E.1686%antoine@ruetter.ch> <dg9m2q$pgi$1@sea.gmane.org>
Message-ID: <432F9852.8080800@pdf.com>

	  I have passed the identities and values of fixed parameters via the 
"..." arguments in optim;  I don't know about nls.  Then internal to the 
function that optim is to minimize, I combine the "x" argument with the 
fixed parameters to obtain the full set of parameters.  I've used that 
effectively.  It's not trivial, but it can be made to work.

	  spencer graves

J.M. Breiwick wrote:

> Hi,
> 
> I am using nls() with the form: nls(~my.fcn(...)) because I have to 
> iteratively compute the expected y values. The function my.fcn() returns 
> y.obs-y.pred
> 
> However, I want to fix some of the parameters in my.fcn at various values 
> and compute the parameter estimates. In Splus there is such a thing as a 
> parameterized dataframe. I don't think this exists in R so does anyone know 
> how to set one or more of the parameters as constants in the model? Thank 
> you.
> 
> Jeff Breiwick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Tue Sep 20 07:55:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Sep 2005 01:55:13 -0400
Subject: [R] Transform variable number of rows per subject to column
	variables?
In-Reply-To: <6.2.5.4.2.20050919215545.02cfdc68@wheresmymailserver.com>
References: <6.2.5.4.2.20050919215545.02cfdc68@wheresmymailserver.com>
Message-ID: <971536df0509192255163702f8@mail.gmail.com>

On 9/20/05, Bing Ho <2bingho at stanford.edu> wrote:
> Hello,
> 
> I am very new to R, but I am having trouble with my dataset.
> 
> I have a data frame where a subject has a variable number of multiple
> observations for each row, which I wish the transform these
> observations to column variables.
> 
> An example of the data frame
> ID      TEST.A  TEST.B
> 1       10      1
> 1       13      2
> 1       11      1
> 2       15      2
> 2       17      3
> 
> And I wish to transform it to the following:
> ID      TEST.A1 TEST.A2 TEST.A3 TEST.B1 TEST.B2 TEST.B3
> 1       10              13              11              1               2               1
> 2       15              17              NA              2               3               NA
> 
> In other words, for the variable number of repeated follow up
> studies, a new column variable for each subject, but they are grouped
> by the original test.


First manufacture a "time" column and then use reshape:

tt <- sequence(rle(DF$ID)$lengths)
reshape(cbind(tt, DF), idvar = "ID", timevar = "tt", direction = "wide")

Another possibility is to use the reshape package:

library(reshape)
DF.d <- deshape(cbind(tt, DF), id = 1:2)  # same tt as above
reshape(DF.d, ID ~ variable + tt)



From u9370004 at cc.kmu.edu.tw  Tue Sep 20 08:02:16 2005
From: u9370004 at cc.kmu.edu.tw (Chun-Ying Lee)
Date: Tue, 20 Sep 2005 14:02:16 +0800
Subject: [R] Extract data from edit chart
Message-ID: <20050920055951.M23145@cc.kmu.edu.tw>

Dear R users:

I wonder if it is possible to use 
"data.frame" and "edit" to show the 
form like:

parameter   upper  lower
   A 
   B
   C
when I type the command edit(abc), assume
the data.frame named "abc". And then I want to 
extract the number form it, like abc[1,2] 
which mean the upper level of A, to do something.

The following is my example and the warning messages:
> abc<-data.frame(Parameter=c("a","b","c"),Lower=c(" ")
    ,Upper=c(" "))
> par<-edit(abc)
then, I key in some number(1,2,3...) and close the console.
Warning messages:
1: added factor levels in 'Lower' in: edit.data.frame(abc) 
2: added factor levels in 'Upper' in: edit.data.frame(abc) 
> par[1,2]
[1] 1
Levels:   1 2 3
How can I fix the warning messages?
And can I just catch the number 1 without the value of Levels: 1 2 3 ?
Please give me some comments about this.
Thank you in advance!!



From ggrothendieck at gmail.com  Tue Sep 20 08:27:11 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Sep 2005 02:27:11 -0400
Subject: [R] Extract data from edit chart
In-Reply-To: <20050920055951.M23145@cc.kmu.edu.tw>
References: <20050920055951.M23145@cc.kmu.edu.tw>
Message-ID: <971536df050919232735592d28@mail.gmail.com>

By specifying space like that you are specifying
that the columns be factors.  If you want them
to be character columns use 

abc <- data.frame(Parameter = 1:10, lo = I(""), hi = I(""))

and if you want them to be numeric columns specify something
that is numeric like 0, NaN, Inf, -Inf or something like that, e.g.

abc <- data.frame(Parameter = 1:10, lo = 0, hi = 0)

On 9/20/05, Chun-Ying Lee <u9370004 at cc.kmu.edu.tw> wrote:
> Dear R users:
> 
> I wonder if it is possible to use
> "data.frame" and "edit" to show the
> form like:
> 
> parameter   upper  lower
>   A
>   B
>   C
> when I type the command edit(abc), assume
> the data.frame named "abc". And then I want to
> extract the number form it, like abc[1,2]
> which mean the upper level of A, to do something.
> 
> The following is my example and the warning messages:
> > abc<-data.frame(Parameter=c("a","b","c"),Lower=c(" ")
>    ,Upper=c(" "))
> > par<-edit(abc)
> then, I key in some number(1,2,3...) and close the console.
> Warning messages:
> 1: added factor levels in 'Lower' in: edit.data.frame(abc)
> 2: added factor levels in 'Upper' in: edit.data.frame(abc)
> > par[1,2]
> [1] 1
> Levels:   1 2 3
> How can I fix the warning messages?
> And can I just catch the number 1 without the value of Levels: 1 2 3 ?
> Please give me some comments about this.
> Thank you in advance!!
> 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Tue Sep 20 08:58:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 20 Sep 2005 08:58:49 +0200
Subject: [R] waveform filtering
In-Reply-To: <355C35514FEAC9458F75947F5270974D076CDA@usctmx1103.merck.com>
References: <355C35514FEAC9458F75947F5270974D076CDA@usctmx1103.merck.com>
Message-ID: <432FB329.2000606@statistik.uni-dortmund.de>

Huntsinger, Reid wrote:

> You should probably have a look at the sound packages for R, "tuneR" and
> "sound", I believe, on http://cran.r-project.org. 
> 
> Applying a filter can be done with filter(), but you need to come up with
> filter coefficients. "High-pass" and "low-pass" have simple descriptions in
> the Fourier transform space, so you might want to specify the frequency
> response of your filter directly there, then do an inverse Fourier transform
> (fft() in R) to get coefficients. 
> 
> The ingredients are all there in R itself; but the packages tuneR and sound

Unfortunately, methods for filtering are neither in sound nor in tuneR 
(yet). Contributions are welcome, of course!

Uwe Ligges


> might have exactly what you want. A book on time series or signal processing
> might be helpful.
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of tom wright
> Sent: Monday, September 19, 2005 10:36 AM
> To: R-help mailing list
> Subject: [R] waveform filtering
> 
> 
> I'm not an engineer so I hope I'm using the correct terminology here. I
> have a recorded waveform that I want to apply low and high pass filters
> too, are tehre already R functions existing to do this or am I going to
> have to program my own?
> thanks for any pointers
> tom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From JohnField at ozemail.com.au  Tue Sep 20 09:02:40 2005
From: JohnField at ozemail.com.au (John Field)
Date: Tue, 20 Sep 2005 16:32:40 +0930
Subject: [R] Problem installing packages on Windows
Message-ID: <6.2.3.4.2.20050920162605.05ad6948@pop.ozemail.com.au>

I've recently had a series of similar errors trying to install 
packages to R 2.1.1 running under Windows XP.

 > utils:::menuInstallLocal()
package 'randomForest' successfully unpacked and MD5 sums checked
updating HTML package descriptions
Warning message:
unable to move temporary installation 'C:\Program 
Files\R\rw2011\library\file25214\randomForest' to 
'C:\PROGRA~1\R\rw2011\library\randomForest'
 >

I get the same error if I try directly installing from CRAN or from a 
local zip file.

I've fixed it for some earlier package downloads by reinstalling R 
and packages, but I don't want to have to do that every time.

Any ideas about what the problem is?

Many thanks,
John Field

==========================
John Field Consulting Pty Ltd
10 High St, Burnside SA 5066
ph: (08) 8332 5294 or 0409 097 586
fax: (08) 8332 1229
email:  JohnField at ozemail.com.au



From anne.piotet at gmail.com  Tue Sep 20 09:07:40 2005
From: anne.piotet at gmail.com (Anne)
Date: Tue, 20 Sep 2005 08:07:40 +0100
Subject: [R] predicting residual expected survival times
Message-ID: <80102e8805092000073a789e53@mail.gmail.com>

DeaR-Helpers
Is there an implemented method to predict residual expected survival
times for parametric/Cox PH models ? (I have modelled my data using
the survival library)
I would like to predict for a given subject (with a given profile )
having survived up to time Ts the expected residual surviving time (or
the  residual survival time quantiles)

Thanks a lot
-- 
Anne



From frank.schmid at vwi.unibe.ch  Tue Sep 20 09:13:05 2005
From: frank.schmid at vwi.unibe.ch (Frank Schmid)
Date: Tue, 20 Sep 2005 09:13:05 +0200
Subject: [R] scale function within a for-loop?
Message-ID: <0IN300GJJTD953@ubecx01.unibe.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050920/180fc36b/attachment.pl

From m_osm at gmx.net  Tue Sep 20 10:14:47 2005
From: m_osm at gmx.net (Mahdi Osman)
Date: Tue, 20 Sep 2005 10:14:47 +0200 (MEST)
Subject: [R] Xgird, R, parallel computing
Message-ID: <25375.1127204087@www51.gmx.net>

Hi, list,


Sorry if I am bothering you.

I am interested in using xgrid with R for distributed computing. I am using
MacOS X and my R 2.1. 1 is already installed and running. The client
computers are all of the same type and the same version of R has been
installed on them.

We have also set up xgrid on my system and client computers as well.

I am wondering if any one can give me some clues about how I can start using
xgrid with R? Do I need to get some packages? Are there documentations or
examples explainging how to use xgrid with R for parallel computing?


I would be very greatful for  your help and hints


Thanks in advance


Rgards


Mahdi

-- 
-----------------------------------
Mahdi Osman (PhD)
E-mail: m_osm at gmx.net



From christophe.pouzat at univ-paris5.fr  Tue Sep 20 10:32:48 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Tue, 20 Sep 2005 10:32:48 +0200
Subject: [R] Xgird, R, parallel computing
In-Reply-To: <25375.1127204087@www51.gmx.net>
References: <25375.1127204087@www51.gmx.net>
Message-ID: <432FC930.2020402@univ-paris5.fr>

Hi,

Can start by checking out these two addresses:

http://adm.wustl.edu/rcluster.php
http://adm.wustl.edu/xgrid.php

Christophe.

Mahdi Osman wrote:

>Hi, list,
>
>
>Sorry if I am bothering you.
>
>I am interested in using xgrid with R for distributed computing. I am using
>MacOS X and my R 2.1. 1 is already installed and running. The client
>computers are all of the same type and the same version of R has been
>installed on them.
>
>We have also set up xgrid on my system and client computers as well.
>
>I am wondering if any one can give me some clues about how I can start using
>xgrid with R? Do I need to get some packages? Are there documentations or
>examples explainging how to use xgrid with R for parallel computing?
>
>
>I would be very greatful for  your help and hints
>
>
>Thanks in advance
>
>
>Rgards
>
>
>Mahdi
>
>  
>


-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From petr.pikal at precheza.cz  Tue Sep 20 10:39:22 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 20 Sep 2005 10:39:22 +0200
Subject: [R] scale function within a for-loop?
In-Reply-To: <0IN300GJJTD953@ubecx01.unibe.ch>
Message-ID: <432FE6DA.20056.68C6D0@localhost>

Hi

I maybe mistaken but

scale(your.matrix)

gives you matrix scaled in the way you want.

> apply(scale(as.matrix(kalcin[,3:7])), na.rm=T,2,sd)
vodofe  stroz      l      a      b 
     1      1      1      1      1 
> apply(scale(as.matrix(kalcin[,3:7])), na.rm=T,2,mean)
       vodofe         stroz             l             a             b 

 1.990322e-15 -5.025086e-14  8.581765e-14  3.588313e-15 -1.370877e-15 

HTH
Petr


On 20 Sep 2005 at 9:13, Frank Schmid wrote:

> I have a data matrix containing around 180 variables and more than 470
> observations for each and no missing values. 
> 
> Within a for-loop, the first step of calculations is to standardize
> each column, such that the mean of each column is zero and the sd is
> one. The for-loop starts with a subset of the initial matrix and
> includes all columns but only a third of the rows. The loop "works
> itself through" the whole matrix and adds everytimes one row, so in
> the last loop, the whole data matrix is used. The standardization
> within the loop is done using the "scale" function. 
> 
> Now, my problem is that with all the 180 variables, either the
> for-loop or the scale function does not work properly, as the
> resulting matrix after the standardization does not have the same
> dimensions anymore as it had before. The matrix is no longer a 180*470
> matrix, but a 180*130 matrix. If, however, I include only 130
> variables instead of 180, the result is correct, the dimensions are
> right and each column indeed has mean zero and sd one. 
> 
> Can anyone please tell me, why this problem appears? Would there be a
> way that gives the same result without using a for-loop?
> 
> Thanks
> 
> 
> 
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From felipe at unileon.es  Tue Sep 20 10:45:37 2005
From: felipe at unileon.es (Felipe)
Date: Tue, 20 Sep 2005 10:45:37 +0200
Subject: [R] Are least-squares means useful or appropriate?
In-Reply-To: <432F961C.5060404@pdf.com>
References: <432AAB98.4040103@unileon.es> <432F961C.5060404@pdf.com>
Message-ID: <432FCC31.8000407@unileon.es>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi.
My question was just theoric. I was wondering if someone who were using
SAS and R could give me their opinion on the topic. I was trying to use
least-squares means for comparison in R, but then I found some
indications against them, and I wanted to know if they had good basis
(as I told earlier, they were not much detailed).
Greetings.

Felipe

Spencer Graves wrote:
| Estimado Felipe:
|
|       If you provide a very simple example (as suggested in the posting
| guide, www.R-project.org/posting-guide.html), it would allow those of
| use who rarely use SAS to respond.  Try to think of the simplest
| possible toy data set and analysis that shows the difference between the
| SAS answer and the answer you get from a certain R function.  If you
| post something simple of that nature that someone can copy from your
| email into R and try other things in a minute or two, it will likely
| increase the speed and utility of a reply.
|
|       Buena Suerte,
|       spencer graves
|
-----BEGIN PGP SIGNATURE-----

iEYEARECAAYFAkMvzDEACgkQWtdQtNzjBl6NbgCfTg0hPZaSio9tO1iWrKHZY3Os
wzEAn3jdHwqqaHxG0OT8KR6kBlSZDPLp
=KtTd
-----END PGP SIGNATURE-----



From dimitris.rizopoulos at med.kuleuven.be  Tue Sep 20 10:57:04 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 20 Sep 2005 10:57:04 +0200
Subject: [R] scale function within a for-loop?
References: <432FE6DA.20056.68C6D0@localhost>
Message-ID: <00b801c5bdc1$45a24f50$0540210a@www.domain>

Hi Petr,

you are right scale() will produce the standardized version of the 
matrix as documented; a note though, for columns means and standard 
deviations it's better to use colMeans() and sd() directly instead of 
apply(mat, 2, mean) and apply(mat, 2, sd), i.e.,

sc.mat <- scale(as.matrix(kalcin[, 3:7]))

colMeans(sc.mat)
sd(sc.mat)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Petr Pikal" <petr.pikal at precheza.cz>
To: "Frank Schmid" <frank.schmid at vwi.unibe.ch>; 
<r-help at stat.math.ethz.ch>
Sent: Tuesday, September 20, 2005 10:39 AM
Subject: Re: [R] scale function within a for-loop?


> Hi
>
> I maybe mistaken but
>
> scale(your.matrix)
>
> gives you matrix scaled in the way you want.
>
>> apply(scale(as.matrix(kalcin[,3:7])), na.rm=T,2,sd)
> vodofe  stroz      l      a      b
>     1      1      1      1      1
>> apply(scale(as.matrix(kalcin[,3:7])), na.rm=T,2,mean)
>       vodofe         stroz             l             a             b
>
> 1.990322e-15 -5.025086e-14  8.581765e-14  3.588313e-15 -1.370877e-15
>
> HTH
> Petr
>
>
> On 20 Sep 2005 at 9:13, Frank Schmid wrote:
>
>> I have a data matrix containing around 180 variables and more than 
>> 470
>> observations for each and no missing values.
>>
>> Within a for-loop, the first step of calculations is to standardize
>> each column, such that the mean of each column is zero and the sd 
>> is
>> one. The for-loop starts with a subset of the initial matrix and
>> includes all columns but only a third of the rows. The loop "works
>> itself through" the whole matrix and adds everytimes one row, so in
>> the last loop, the whole data matrix is used. The standardization
>> within the loop is done using the "scale" function.
>>
>> Now, my problem is that with all the 180 variables, either the
>> for-loop or the scale function does not work properly, as the
>> resulting matrix after the standardization does not have the same
>> dimensions anymore as it had before. The matrix is no longer a 
>> 180*470
>> matrix, but a 180*130 matrix. If, however, I include only 130
>> variables instead of 180, the result is correct, the dimensions are
>> right and each column indeed has mean zero and sd one.
>>
>> Can anyone please tell me, why this problem appears? Would there be 
>> a
>> way that gives the same result without using a for-loop?
>>
>> Thanks
>>
>>
>>
>>
>>
>>  [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From david.barron at said-business-school.oxford.ac.uk  Tue Sep 20 11:14:48 2005
From: david.barron at said-business-school.oxford.ac.uk (David Barron)
Date: Tue, 20 Sep 2005 10:14:48 +0100
Subject: [R] column-binary data
Message-ID: <6DD6C3FF939D934CBD916EAC13D581E30B9B4F@sbs-mercury4>


Thanks for the replies.  That's not quite what I meant.  These data are multipunched to allow more than one variable to be coded in the same column.  For example, the first 7 columns of the first card of the data I'm trying to read contain the following:

Column   Rows      Description
-------------------------------
1-5                Serial number
6                  Card number
7         Y,X      Sex of respondent
7         0-3      Marital status
7         4-9      Occupational status

I happen to know that the actual punches for the first respondent are 00001,1,Y,1,4.

When I use
> ip <- readBin(ff,what="raw",n=14,signed=FALSE) 
I get
> 08 00 08 00 08 00 08 00 04 00 04 00 24 20
for these seven columns.

When I use raw2bin from package caTools I get:
> binip <- raw2bin(ip,"integer",size=2)
>   8    8    8    8    4    4 8228

Now I can see that the relationship between binary numbers and punches is this:
                             
binary      punch                  binary         punch
------------------                 ------------------------
1            3                     256            9
2            2                     512            8
4            1                     1024           7
8            0                     2048           6
16           X                     4096           5
32           Y                     8192           4
64                                 16384
128                                32768 

I can also see that the binary value for column 7 (8228) is equal to the sum of the values for each of the three punches in that column (Y=32 + 1=4 + 4=8192), but what I don't get is how I can get R to work out the punches either from the raw values or from the binary values.  If anyone can suggest anything I would be very, very grateful!

David

-----Original Message-----
From:	Ted Harding [mailto:Ted.Harding at nessie.mcc.ac.uk]
Sent:	Fri 9/16/2005 14:31
To:	E-Mail
Cc:	David Barron
Subject:	Re: [R] column-binary data
On 16-Sep-05 jim holtman wrote:
> Each card column had 12 rows, so as binary it comes in as 12 bits. The 
> question is does this come as a 16 bit integer, or a string of 12 bits
> that I have to extract from. Either case is not that difficult to do.

Indeed ... as an example of how one could proceed, I "deconstruct"
my example below (see at end).

> On 9/16/05, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote: 
>> 
>> On 16-Sep-05 David Barron wrote:
>> > I have a number of datasets that are multipunch column-binary
>> > format.
>> > Does anyone have any advice on how to read this into R? Thanks.
>> >
>> > David
>> 
>> Do you mean something like the old
>> 
>> HOLLERITH PUNCHED CARD BINARY FORMAT?
>> 1111111110111111101111011111101111110
>> 0000000001000000010000100000010000001
>> 0000010100110000000010000001100010011
>> 1111001010001010000000001100100101001
>> 0111100100011001100001000100001101011
>> 0100010000001100001010010101001110001
>> 0100101000010101001100001010100101101
>> 
>> (here "1" = hole in card, binary representation of 7-bit ASCII
>> encoding, high-order bit on top).

#First, construct a vector ASCII consiting of the printable
#characters:

ASCII<-c(" ","!","\"","#","$","%","&","'","(",")",
         "*","+",",","-",".","/","0","1","2","3",
         "4","5","6","7","8","9",":",";","<","=",
         ">","?","@","A","B","C","D","E","F","G",
         "H","I","J","K","L","M","N","O","P","Q",
         "R","S","T","U","V","W","X","Y","Z","[",
         "\\","]","^","_","`","a","b","c","d","e",
         "f","g","h","i","j","k","l","m","n","o",
         "p","q","r","s","t","u","v","w","x","y",
         "z","{","|","}","~")


#Next, a vector of powers of 2:

rad<-2^(6:0)


#Read in the data from stdin():

M<-t(matrix(as.integer(unlist((strsplit(scan(stdin(),
     what="character"),split="")))),ncol=7))

#(read 7 lines from stdin by copy&paste:
#1: 1111111110111111101111011111101111110
#2: 0000000001000000010000100000010000001
#3: 0000010100110000000010000001100010011
#4: 1111001010001010000000001100100101001
#5: 0111100100011001100001000100001101011
#6: 0100010000001100001010010101001110001
#7: 0100101000010101001100001010100101101
#8: 
#Read 7 items

#and convert the columns to ASCII codes:

R<-rad%*%M

#and see what you've got:

paste(ASCII[R-31],collapse="")

#[1] "HOLLERITH PUNCHED CARD BINARY FORMAT?"

The above can be adapted to whatever your binary data represent
and to how they are laid out in the input.

Others may find a slicker way of doing this.

The only fly in the above ointment is that I haven't located
in R a character-vector constant which consists of the printable
ASCII characters, or a function to convert numerical ASCII code
to characters, so I made my own.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Sep-05                                       Time: 22:26:16
------------------------------ XFMail ------------------------------



From maechler at stat.math.ethz.ch  Tue Sep 20 11:30:36 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 20 Sep 2005 11:30:36 +0200
Subject: [R] Extended Hypergeometric Distribution
In-Reply-To: <588D8BDAAC0BEB4B82DA2CC5AEBA899C3F7A0B@isdex001.intra.swsahs.nsw.gov.au>
References: <588D8BDAAC0BEB4B82DA2CC5AEBA899C3F7A0B@isdex001.intra.swsahs.nsw.gov.au>
Message-ID: <17199.54972.230303.655118@stat.math.ethz.ch>

>>>>> "Narcyz" == Narcyz Ghinea <Narcyz.Ghinea at swsahs.nsw.gov.au>
>>>>>     on Mon, 19 Sep 2005 12:38:27 +1000 writes:

    Narcyz> Dear R Users,

    Narcyz> There exists a non-central hypergeometric
    Narcyz> distribution function in the (MCMCpack) package, and
    Narcyz> a hypergeometric distribution function in the
    Narcyz> (stats) package.

    Narcyz> Is there a function for sampling from an extended
    Narcyz> hypergeometric distribution?

what is "extended" ?
Do you mean "extended to include non-central"?



From chrish at stats.ucl.ac.uk  Tue Sep 20 11:43:55 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Tue, 20 Sep 2005 10:43:55 +0100 (BST)
Subject: [R] Teaching R - In front of the computer?
In-Reply-To: <web-104313520@cgpsrv2.cis.mcmaster.ca>
References: <web-104313520@cgpsrv2.cis.mcmaster.ca>
Message-ID: <Pine.LNX.4.58.0509201038010.367@egon.stats.ucl.ac.uk>

Dear Roland,

I wrote an eight-pages manuscript with basic commands and instructions
to use the help system and gave the students series of many small
exercises. For further concepts such as matrix computations, index
manipulations, coercion, I gave short presentations (15 minutes or so),
again followed by series of exercises including some where they had to
find out about non-introduced stuff. I was available for help (of course
this works with up to 25, but not necessarily with 90 students).
It worked quite well even though it's not exactly a fast way to introduce
a lot of commands...

Christian


On Mon, 19 Sep 2005, John Fox wrote:

> Dear Roland,
>
> I've taught the use of R to this kind of audience many times. Take a
> look at
> <http://socserv.socsci.mcmaster.ca/jfox/Courses/UCLA/index.html> for
> materials used in such a workshop, and at
> <http://socserv.socsci.mcmaster.ca/jfox/Teaching-with-R.pdf> for a
> paper on teaching social statistics with R.
>
> As others have suggested, using static slides is not a good idea, and
> having at least a live display for the presenter is essential. It also
> helps to have the students sitting at computers and able to try things
> out for themselves. If this is a workshop devoted to R, I'd strongly
> recommend this format.
>
> On the other hand, if you're teaching R in the context of a more
> general statistics course, you can cover the basics in a hands-on
> workshop and then use the LCD projector to introduce new commands,
> etc., during the course as they're needed. I find that once they've
> acquired the basics, students are able to work more independently.
>
> I hope this helps,
>  John
>
> On Mon, 19 Sep 2005 15:25:14 +0200
>  "Rau, Roland" <Rau at demogr.mpg.de> wrote:
> > Dear R-Users,
> >
> > given you have been teaching R to students (grad level, mainly social
> > science background, no previous programming experience, 80% know
> > SPSS),
> > what are your experiences concerning the style of teaching? Do you
> > prefer to stand in front of the class like in "normal" lectures and
> > you
> > show them slides? Or do you you explain some concept (for example
> > things
> > like mydata[order(var1, var2),]) and show it directly on the computer
> > via beamer/projector and also the students have to enter it on the
> > computers in front of them.
> >
> > Any experiences you can share are highly appreciated.
> >
> > Thanks,
> > Roland
> >
> > +++++
> > This mail has been sent through the MPI for Demographic
> > Rese...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.mcmaster.ca/jfox/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From h.y.wong at leeds.ac.uk  Tue Sep 20 12:04:33 2005
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Tue, 20 Sep 2005 11:04:33 +0100
Subject: [R] Possible bug in lmer nested analysis with factors
In-Reply-To: <40e66e0b050918080436d7e2dc@mail.gmail.com>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A4880@dc1ex2.air.org>
	<432AF0FD.1030001@pdf.com>
	<A6DDFD3A-7C21-41F1-972E-1DB2597E9AB8@leeds.ac.uk>
	<40e66e0b050918080436d7e2dc@mail.gmail.com>
Message-ID: <76680610-59BA-4FC9-A7D9-2D9311BB54DB@leeds.ac.uk>


On 18 Sep 2005, at 16:04, Douglas Bates wrote:

> You are correct that good documentation of the capabilities of lmer
> does not currently exist. lmer is still under active development and
> documentation is spread in several places.  The vignette in the mlmRev
> package explores some of the capabilities of lmer.  Also see the
> examples in that package.

Yes. Thanks for this, and indeed for the development of the package.
I'm currently trying to do GLMMs (binary response), so I thought that
I should learn mixed modelling using a library with these capabilities.


> You are correct that the denominator degrees of freedom associated
> with terms in the fixed effects is different between lme and lmer.
> ...
> Some arguments on
> degrees of freedom can be made for nested grouping factors but the
> question of testing fixed effects terms for models with partially
> crossed grouping factors is difficult.

Would it not be possible to recognise when the model is fully nested,
and make this a special case? I was imagining using lmer as a
replacement for lme, so finding that they differ in this way came
as some surprise. When learning to use a new, relatively undescribed
routine, I usually try to see if I can reproduce known results. This
is where I was coming unstuck when trying to reproduce lme results
using lmer.

I suspect that many people (I know of one other in my group) will use
lmer as a drop-in replacement for lme specifically for its GLMM
capabilities rather than for its partial nesting. I realise, however,
that this might not be your priority.


> This area could be a very fruitful research area for people with
> strong mathematical and implementation skills.

That's not me, I'm afraid. I am only just working through Chapter 1
of your (excellent) "mixed effects models in S" book.

> There are already some facilities for lmer models such as mcmcsamp and
> simulate which can be used for evaluating the posterior distribution
> of a single coefficient or for a parametric bootstrap of the reference
> distribution of a quantity like the likelihood ratio statistic for a
> hypothesis test.

This, again, is beyond me at the moment. But I do hope that someone
else can respond to the call, especially for "textbook" as well as
more complex examples of lmer usage.

Best wishes

Yan Wong



From h.y.wong at leeds.ac.uk  Tue Sep 20 12:16:05 2005
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Tue, 20 Sep 2005 11:16:05 +0100
Subject: [R] Possible bug in lmer nested analysis with factors
In-Reply-To: <40e66e0b050918082734746d08@mail.gmail.com>
References: <88EAF3512A55DF46B06B1954AEF73F740A0A4880@dc1ex2.air.org>
	<432AF0FD.1030001@pdf.com>
	<A6DDFD3A-7C21-41F1-972E-1DB2597E9AB8@leeds.ac.uk>
	<40e66e0b050918082734746d08@mail.gmail.com>
Message-ID: <87E689AA-634B-4867-A830-57E4B3D66C66@leeds.ac.uk>


On 18 Sep 2005, at 16:27, Douglas Bates wrote:

> I have a couple of other comments.  You can write the nested grouping
> factors as Sundar did without having to explicitly evaluate the
> interaction term and drop unused levels.  The lmer function, like most
> modeling functions in R, uses the optional argument drop.unused.levels
> = TRUE when creating the model frame.

In other words, the use of "b:c" in a model formula, where both b and c
are factors, results in an internal call to evalq(b:c)[,drop=T] (or
equivalent), which is treated as a factor in a temporary model data
frame? I know little of the internals to R - that is new to me,
but does make sense for factors.

Thus I could use |a:b and |a:b:c as random terms in lme or lmer, even
though 'a' is a fixed, unnested factor.

Notation like this in the model formula does indeed aid clarity. By the
way, I noticed that in your mlmRev vignette you recommended this as good
practice for lea:school (page 3), but then omitted to do it on page 4.

> John Maindonald has already suggested the use of
>
>  (1|b/c) => (1|b:c) + (1|b)
>
> as "syntactic sugar" for the lmer formula and it is a reasonable
> request.

This is, indeed, the behaviour I was expecting.

> Unfortunately, implementing this is not high on my priority
> list at present. (We just made a massive change in the sparse matrix
> implementation in the Matrix package and shaking the bugs out of that
> will take a while.)

All your efforts in these areas are, I'm sure, much appreciated. I'm
certainly very interested in learning to use lmer, and welcome all the
improvements that are being made.

> In any case the general recommendation for nested grouping factors is
> first to ensure that they are stored as factors and then to create the
> sequence of interaction terms.

As a brief aside, I know that some people assume that lme treats random
effects as factors even if they are of a numeric type. It might be worth
doing a check in lmer (and even lme) that random effects are factors,
producing a warning if not. Again, this is a non-vital suggestion, and I
don't wish to take up any more of your time!

Thanks

Yan



From Rau at demogr.mpg.de  Tue Sep 20 12:22:28 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 20 Sep 2005 12:22:28 +0200
Subject: [R] Teaching R - In front of the computer?
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E69FDD51@HERMES.demogr.mpg.de>

Dear all,

thanks for telling me your experiences how you proceed teaching R. I am currently giving a five week course teaching R to 16 graduate-level students consisting of 12 sessions ?? 1.5 hours. Two weeks have passed during that course and I was just questioning myself whether my "style" of teaching in front of the students with a running R session (+editor) is the best way of teaching. [1]

But thanks to the replies I got here, I believe this is a better approach than teaching in front of the class like in a normal lecture without any "live display" of R.

I especially liked the idea on John Fox Homepage to give code and the students have to debug it. 

Thanks again for all your replies.

Best,
Roland



[1] What I usually do is to describe what we want to do. Occasionally I remind them of the statistical background on the blackboard. Then I show them how to translate it into R. They should enter it as well into R. Then, I am explaining to them all the details how the code works and what one has to keep in mind. I distribute a script with all the examples and explanations and additional exercises (which can be easily done thanks to Friedrich Leisch's Sweave/Stangle/... function).

+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From je_lemaitre at hotmail.com  Tue Sep 20 14:29:30 2005
From: je_lemaitre at hotmail.com (=?iso-8859-1?B?Suly9G1lIExlbWHudHJl?=)
Date: Tue, 20 Sep 2005 08:29:30 -0400
Subject: [R] Running glm in batch and exporting  results (AIC) to HTML
Message-ID: <BAY103-DAV14B3FD6800BA034D2222EB90950@phx.gbl>

Dear all,

I'm doing univariate Poisson regressions using the "glm" and "glm.fit"
functions. I have 5 independent datasets and each dataset, has one response
variable and more than 20 factors to test.
Currently, I run one regression at a time and manually take notes of the
results in excel to have a quick overview on what is going on in my data. My
poor method is very time-consuming and I was looking for a faster and more
reliable way to do all the regressions.
I'm quite sure that R could do all of this for me but I can't think of a way
to tell it...

What I want R to do is
1) running one regression at a time in a particular dataset.
2) saving results. Here, I'm interested in AIC, Beta coefficient of the
factor, the z value and the p value of the factor
3) formatting results in a table with column names as follow:
factor; beta coefficient; z value; p value; aic
4) exporting the table in a way that I could read it in excel. In that way,
I would repeat the operation for each of the 5 datasets rather than for the
100 regressions.

I hope you could help me with this.
Thanks a lot for your answers.

J??r??me Lema??tre
Ph.D. student
Dpt biologie
Universit?? Laval
Qu??bec, Canada



From vincent at 7d4.com  Tue Sep 20 14:56:17 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Tue, 20 Sep 2005 14:56:17 +0200
Subject: [R] Running glm in batch and exporting  results (AIC) to HTML
In-Reply-To: <BAY103-DAV14B3FD6800BA034D2222EB90950@phx.gbl>
References: <BAY103-DAV14B3FD6800BA034D2222EB90950@phx.gbl>
Message-ID: <433006F1.4070100@7d4.com>

see
?write.table
hih



From tom at maladmin.com  Tue Sep 20 11:13:32 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 20 Sep 2005 05:13:32 -0400
Subject: [R] waveform filtering
In-Reply-To: <1127140564.4521.17.camel@localhost.localdomain>
References: <1127140564.4521.17.camel@localhost.localdomain>
Message-ID: <1127207612.4521.20.camel@localhost.localdomain>

On Mon, 2005-19-09 at 10:36 -0400, tom wright wrote:
> I'm not an engineer so I hope I'm using the correct terminology here. I
> have a recorded waveform that I want to apply low and high pass filters
> too, are tehre already R functions existing to do this or am I going to
> have to program my own?
> thanks for any pointers
> tom
> 
Thanks for the answers to this, after a little reading I realised that
what sounded so simple wasnt quite. However chapters 15-18 of
http://www.dspguide.com has been very useful.



From chrysopa at insecta.ufv.br  Tue Sep 20 15:24:37 2005
From: chrysopa at insecta.ufv.br (Ronaldo Reis-Jr.)
Date: Tue, 20 Sep 2005 10:24:37 -0300
Subject: [R] Change the mirror
Message-ID: <200509201024.37741.chrysopa@insecta.ufv.br>

Hi,

Please, change the brazilian mirror http://www.termix.ufv.br/CRAN/ to 
http://www.insecta.ufv.br/CRAN/ in R homepage.

Thanks
ROnaldo
-- 

Se dois homens no mesmo trabalho concordam o tempo todo, um deles ?? demais. Se 
discordam sem parar, ent??o os dois s??o dispens??veis

--Darryl F. Zanuck
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From murdoch at stats.uwo.ca  Tue Sep 20 15:35:19 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 20 Sep 2005 09:35:19 -0400
Subject: [R] Change the mirror
In-Reply-To: <200509201024.37741.chrysopa@insecta.ufv.br>
References: <200509201024.37741.chrysopa@insecta.ufv.br>
Message-ID: <43301017.8080406@stats.uwo.ca>

On 9/20/2005 9:24 AM, Ronaldo Reis-Jr. <chrysopa at insecta.ufv.br> wrote:
> Hi,
> 
> Please, change the brazilian mirror http://www.termix.ufv.br/CRAN/ to 
> http://www.insecta.ufv.br/CRAN/ in R homepage.

Requests like this should go to CRAN (to whom I've cc'd this).  The 
admins there might not see it in R-Help.

Duncan Murdoch



From jmoreira at fe.up.pt  Tue Sep 20 15:35:12 2005
From: jmoreira at fe.up.pt (jmoreira@fe.up.pt)
Date: Tue, 20 Sep 2005 14:35:12 +0100
Subject: [R] Interpretation of csplit from rpart.object
Message-ID: <20050920143512.fp38iykirk0ko808@webmail.fe.up.pt>

Dear members of R-list,

I need to reproduce the rules of a decision tree. For that I need to use the
csplit information from the rpart.object. But I cannot uderstand the
information because from my example I get:
> rpart.tree$csplit
      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
 [1,]    1    3    3    1    3    3    3
 [2,]    2    3    3    1    2    2    2
 [3,]    1    3    3    1    3    3    3
 [4,]    2    3    3    1    2    2    2
 [5,]    2    3    3    1    2    2    2
 [6,]    2    1    3    2    3    1    1
 [7,]    2    3    3    2    3    3    1
 [8,]    2    3    3    1    2    2    2
 [9,]    2    1    3    2    3    1    1
[10,]    2    1    3    3    2    2    2
[11,]    2    1    1    2    1    1    3
[12,]    2    3    3    1    2    2    2
[13,]    2    1    1    2    3    1    1
[14,]    2    3    3    1    2    2    2
[15,]    2    1    3    2    1    1    1
[16,]    2    3    1    1    2    2    2
[17,]    2    3    3    1    2    2    2
[18,]    2    1    3    2    1    3    1
[19,]    2    3    3    1    2    2    2
[20,]    2    1    3    2    1    3    3
[21,]    2    3    1    2    2    2    2
[22,]    2    1    3    2    1    1    1

I don't understand why I have 22 rows (my tree has 21 nodes including the root
node) and 7 columns (I have four explanatory variables: two numerics and two
factors; plus the numeric target variable)

?rpart.object it says:

  csplit: this will be present only if one of the split variables is a
          factor. There is one row for each such split, and column 'i =
          -1' if this level of the factor goes to the left, '+1' if it
          goes to the right, and 0 if that level is not present at this
          node of the tree. For an ordered categorical variable all
          levels are marked as 'R/L',  including levels that are not
          present.

The values I got are quite different.

Can some one give me information on how to deal with that?

Thanks in advance?

Joao Moreira



From dieter.menne at menne-biomed.de  Tue Sep 20 15:48:46 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 20 Sep 2005 13:48:46 +0000 (UTC)
Subject: [R] predicting residual expected survival times
References: <80102e8805092000073a789e53@mail.gmail.com>
Message-ID: <loom.20050920T154710-697@post.gmane.org>

Anne <anne.piotet <at> gmail.com> writes:

> Is there an implemented method to predict residual expected survival
> times for parametric/Cox PH models ? (I have modelled my data using
> the survival library)
> I would like to predict for a given subject (with a given profile )
> having survived up to time Ts the expected residual surviving time (or
> the  residual survival time quantiles)

What's wrong with documented resid?

 fit <- coxph(Surv(start, stop, event) ~ (age + surgery)* transplant,
               data=heart)
 mresid <- resid(fit, collapse=heart$id)

Dieter



From stecalza at tiscali.it  Tue Sep 20 15:55:59 2005
From: stecalza at tiscali.it (Stefano Calza)
Date: Tue, 20 Sep 2005 15:55:59 +0200
Subject: [R] Running glm in batch and exporting  results (AIC) to HTML
In-Reply-To: <433006F1.4070100@7d4.com>
References: <BAY103-DAV14B3FD6800BA034D2222EB90950@phx.gbl>
	<433006F1.4070100@7d4.com>
Message-ID: <20050920135559.GB7161@med.unibs.it>

see

?html

in Hmisc package

HIH
Ste

On Tue, Sep 20, 2005 at 02:56:17PM +0200, vincent at 7d4.com wrote:
<vincent>see
<vincent>?write.table
<vincent>hih
<vincent>
<vincent>______________________________________________
<vincent>R-help at stat.math.ethz.ch mailing list
<vincent>https://stat.ethz.ch/mailman/listinfo/r-help
<vincent>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfontain at free.fr  Tue Sep 20 16:31:50 2005
From: jfontain at free.fr (jfontain@free.fr)
Date: Tue, 20 Sep 2005 16:31:50 +0200
Subject: [R] waveform filtering
In-Reply-To: <1127207612.4521.20.camel@localhost.localdomain>
References: <1127140564.4521.17.camel@localhost.localdomain>
	<1127207612.4521.20.camel@localhost.localdomain>
Message-ID: <1127226710.43301d565fa0a@imp1-g19.free.fr>

Quoting tom wright <tom at maladmin.com>:

> On Mon, 2005-19-09 at 10:36 -0400, tom wright wrote:
> > I'm not an engineer so I hope I'm using the correct terminology here. I
> > have a recorded waveform that I want to apply low and high pass filters
> > too, are tehre already R functions existing to do this or am I going to
> > have to program my own?
> > thanks for any pointers
> > tom
> >
> Thanks for the answers to this, after a little reading I realised that
> what sounded so simple wasnt quite. However chapters 15-18 of
> http://www.dspguide.com has been very useful.

Maybe you need a more specialized tool, such as FFTW?
I found it by searching on "fourier" at www.freshmeat.net. Just an idea...


--
Jean-Luc



From reid_huntsinger at merck.com  Tue Sep 20 16:42:28 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 20 Sep 2005 10:42:28 -0400
Subject: [R] waveform filtering
Message-ID: <355C35514FEAC9458F75947F5270974D076CDC@usctmx1103.merck.com>

fftw is a library to do FFTs (fast Fourier transform). It's excellent, but
probably not necessary unless you have lots of long series and you use FFT's
repeatedly (say in an iterative fitting procedure). R's fft() is essentially
instantaneous for most one-shot applications.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jfontain at free.fr
Sent: Tuesday, September 20, 2005 10:32 AM
To: R-help mailing list
Subject: Re: [R] waveform filtering


Quoting tom wright <tom at maladmin.com>:

> On Mon, 2005-19-09 at 10:36 -0400, tom wright wrote:
> > I'm not an engineer so I hope I'm using the correct terminology here. I
> > have a recorded waveform that I want to apply low and high pass filters
> > too, are tehre already R functions existing to do this or am I going to
> > have to program my own?
> > thanks for any pointers
> > tom
> >
> Thanks for the answers to this, after a little reading I realised that
> what sounded so simple wasnt quite. However chapters 15-18 of
> http://www.dspguide.com has been very useful.

Maybe you need a more specialized tool, such as FFTW?
I found it by searching on "fourier" at www.freshmeat.net. Just an idea...


--
Jean-Luc

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From keith.bannister at astrium.eads.net  Tue Sep 20 17:46:01 2005
From: keith.bannister at astrium.eads.net (BANNISTER, Keith)
Date: Tue, 20 Sep 2005 16:46:01 +0100
Subject: [R] Neat way of using R for pivoting?
Message-ID: <B3645EBD393F7E4DAB7B18EFAAA4E83B0858E0@auk52177.ukr.astrium.corp>


Hi,

I'd like to use R to do what excel pivot tables do, and plot results.

I've never used R before, and I've managed to do something, but it's quite a
lot of code to do something simple. I can't help but think I'm not "Doing it
the R way".

I could be using R for the wrong thing, in which case, please tell me off.

I was hoping something like plot(by(t, factor(t$snr), summary)) would do
something, but it doesn't.

Say my data is (for example)
SNR	timeError
4	1.3
4	2.1
4	1.2
6	2.1
6	2.2
6	2.1
8	3.2
8	3.7
8	3.1

I want to produce a plot of SNR vs mean(timeError) with error bars of
magnitude 3 sigma.

here's what I've got so far (without the error bars. I can't do that yet).

I'm sure it's the wrong way to go about this:

******* BEGIN SNIPPET *******

get_stats <- function(t) {
cnfac <- factor(t$cnset);
mu <- as.list(by(t$snr, cnfac, mean));
tvar <- as.list(by(t$snr, cnfac, var));
t <- list(mu=mu, var=tvar);
}

vn <- read.table('vn.csv', sep=',');
vn_stats <- get_stats(vn);

vsn <- read.table('vsn.csv', sep=',');
vsn_stats <- get_stats(vsn);

snrs <- as.numeric(names(vn_stats$mu))

matplot(snrs, cbind(vn_stats$mu, vsn_stats$mu));

windows();
matplot(snrs, cbind(vn_stats$var, vsn_stats$var));

******* END SNIPPET *******

Appreciate any helpful hints from the pros.

Cheers!

p.s. We've been having rather a good time around the office recently with
"International Talk Like a Pirate Day" (www.yarr.org.uk). R fits in very
well: "I be usin' Arrrgghhhh for my post processin'".


Keith Bannister

--
Electrical Engineer
Astrium Ltd


This email is for the intended addressee only.
If you have received it in error then you must not use, retain, disseminate or otherwise deal with it.
Please notify the sender by return email.
The views of the author may not necessarily constitute the views of EADS Astrium Limited.
Nothing in this email shall bind EADS Astrium Limited in any contract or obligation.

EADS Astrium Limited, Registered in England and Wales No. 2449259
Registered Office: Gunnels Wood Road, Stevenage, Hertfordshire, SG1 2AS, England



From Mark.Steer at bristol.ac.uk  Tue Sep 20 17:47:40 2005
From: Mark.Steer at bristol.ac.uk (Mark Steer)
Date: Tue, 20 Sep 2005 16:47:40 +0100
Subject: [R] Degrees of freedom in glmmPQL
Message-ID: <C7355784CD1FA314D311F4A5@bio-ahustonpost.bio.bris.ac.uk>


I apologise in advance if this is a daft question, but I've been battling 
GLMMs for a while in R 2.1.1 and keep getting bizarre degrees of freedom 
appearing.

For instance, using the example data attached and a simplified model from 
the one I need, I have: binary choice data, fixed factors = PositionIndex 
and Treatment and random factor = Name nested within treatment.  I use the 
model:

glmmPQL(FreeChoice ~ PositionIndex + Treatment, random = ~1 + 
Treatment|Name, family = binomial, data = Exemplar)

The output I receive gives the expected df for the treatment variables (df 
= 47), but the PositionIndex df come out at 3953 when I think 7 should be 
closer to the mark.  Have I got my model wrong?  The full output is shown 
below, I'd be very grateful for any advice.

Many thanks, Mark

PS - is there a way to specify a single p-value for a factor e.g. 
PositionIndex as well as the two-way contrasts?


Linear mixed-effects model fit by maximum likelihood
 Data: students
       AIC      BIC    logLik
  17254.33 17361.37 -8610.163

Random effects:
 Formula: ~1 + Treatment | Name
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev     Corr
(Intercept) 0.08118364 (Intr) Trtmnt
Treatmentvl 0.89891387 -0.059
Treatmentvs 0.54219504 -0.438  0.006
Residual    0.98920592

Variance function:
 Structure: fixed weights
 Formula: ~invwt
Fixed effects: FreeChoice ~ PositionIndex + Treatment
                    Value Std.Error   DF   t-value p-value
(Intercept)     0.2329314 0.1231124 3953  1.892022  0.0586
PositionIndexb -0.0758549 0.1385700 3953 -0.547412  0.5841
PositionIndexc -0.2560203 0.1372310 3953 -1.865616  0.0622
PositionIndexd -0.2770403 0.1646272 3953 -1.682835  0.0925
PositionIndexe -0.1087767 0.1573432 3953 -0.691334  0.4894
PositionIndexf -0.2561706 0.1368936 3953 -1.871312  0.0614
PositionIndexg -0.5037723 0.1390067 3953 -3.624086  0.0003
PositionIndexh -0.7039981 0.1595509 3953 -4.412373  0.0000
Treatmentvl     0.0800244 0.2552349   47  0.313533  0.7553
Treatmentvs    -0.2539611 0.1443501   47 -1.759341  0.0850
 Correlation:
               (Intr) PstnIndxb PstnIndxc PstnIndxd PostnIndx PstnIndxf 
PstnIndxg PstnIndxh Trtmntvl
PositionIndexb -0.765 

PositionIndexc -0.768  0.673 

PositionIndexd -0.642  0.562     0.567 

PositionIndexe -0.667  0.589     0.591     0.494 

PositionIndexf -0.773  0.676     0.682     0.569     0.595 

PositionIndexg -0.767  0.667     0.672     0.561     0.587     0.675 

PositionIndexh -0.655  0.578     0.585     0.486     0.509     0.586 
0.578
Treatmentvl    -0.105  0.012     0.008     0.010     0.001     0.009 
0.010     0.001
Treatmentvs    -0.189  0.018     0.013     0.009     0.013     0.020 
0.029     0.006     0.082

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max
-2.3171394 -0.9911849 -0.3750489  1.0130225  3.2386547

Number of Observations: 4010
Number of Groups: 50

------------------------------------------------------------------------

Mark Steer
School of Biological Sciences
University of Bristol
Woodland Road
Bristol, BS8 1UG

tel - ++44 (0)117 9545945 (int. 45945)
-------------- next part --------------
Name	Sex	Treatment	PositionIndex	TotalDelay	FreeChoice
ab1322	m	cc	d	20.0548	1
ab1322	m	cc	e	18.0371	0
ab1322	m	cc	d	20.055	0
ab1322	m	cc	f	22.0451	1
ab1322	m	cc	b	20.0566	1
ab1322	m	cc	b	18.0371	0
ab1322	m	cc	e	20.0549	1
ab1322	m	cc	b	18.0468	0
ab1322	m	cc	e	20.0549	0
ab1322	m	cc	d	22.0451	1
ab1322	m	cc	c	20.0487	0
ab1322	m	cc	f	22.0449	1
ab1322	m	cc	c	20.0549	0
ab1322	m	cc	g	22.045	0
ab1322	m	cc	f	24.063	1
ab1322	m	cc	g	22.0504	0
ab1322	m	cc	f	24.0632	1
ab1322	m	cc	b	22.0451	1
ab1322	m	cc	c	20.0548	1
ab1322	m	cc	e	18.0369	1
ab1322	m	cc	d	16.0467	1
ab1322	m	cc	h	14.0564	0
ab1322	m	cc	b	16.0465	1
ab1322	m	cc	a	14.0565	0
ab1322	m	cc	e	16.0466	1
ab1322	m	cc	h	14.0562	0
ab1322	m	cc	a	16.0467	1
ab1322	m	cc	a	14.0566	0
ab1322	m	cc	a	16.0481	0
ab1322	m	cc	c	18.037	1
ab1322	m	cc	f	16.0467	0
ab1322	m	cc	b	18.0369	1
ab1322	m	cc	d	16.0464	0
ab1322	m	cc	c	18.037	1
ab1322	m	cc	a	16.0464	1
ab1322	m	cc	a	14.0566	0
ab1322	m	cc	e	16.0468	1
ab1322	m	cc	g	14.0563	0
ab1322	m	cc	a	16.0466	1
ab1322	m	cc	d	14.0566	1
ab1322	m	cc	b	12.0383	0
ab1322	m	cc	e	14.0565	0
ab1322	m	cc	b	16.0466	1
ab1322	m	cc	c	14.0563	0
ab1322	m	cc	c	16.0467	1
ab1322	m	cc	a	14.0565	0
ab1322	m	cc	f	16.0466	1
ab1322	m	cc	c	14.0564	0
ab1322	m	cc	h	16.0468	1
ab1322	m	cc	f	14.0562	0
ab1322	m	cc	a	16.0468	0
ab1322	m	cc	f	18.0369	1
ab1322	m	cc	b	16.0466	0
ab1322	m	cc	b	18.0369	0
ab1322	m	cc	f	20.0548	0
ab1322	m	cc	f	22.0449	1
ab1322	m	cc	g	20.055	1
ab1322	m	cc	e	18.037	1
ab1322	m	cc	e	16.0468	1
ab1322	m	cc	c	14.0563	0
ab1322	m	cc	g	16.0467	1
ab1322	m	cc	e	14.0563	1
ab1322	m	cc	c	12.0386	0
ab1322	m	cc	d	14.0565	1
ab1322	m	cc	g	12.0384	0
ab1322	m	cc	b	14.0562	0
ab1322	m	cc	a	16.0464	0
ab1322	m	cc	f	18.0367	1
ab1322	m	cc	b	16.0465	0
ab1322	m	cc	e	18.0369	1
ab1322	m	cc	a	16.0466	1
ab1322	m	cc	a	14.0564	0
ab1322	m	cc	f	16.0467	0
ab1322	m	cc	a	18.049	0
ab1322	m	cc	c	20.0546	1
ab1322	m	cc	g	18.0369	0
ab1322	m	cc	f	20.055	0
ab1322	m	cc	b	22.0452	1
ab1322	m	cc	h	20.0549	0
ab1322	m	cc	f	22.0452	0
ab1322	m	cc	g	24.0629	1
ab1322	m	cc	f	22.0451	0
ab1322	m	cc	a	24.0505	1
ab1322	m	cc	d	22.0468	1
ab1322	m	cc	c	20.0547	1
ab1322	m	cc	g	18.0371	1
ab1322	m	cc	b	16.0467	1
ab1322	m	cc	g	14.0564	1
ab1322	m	cc	d	12.0384	0
ab1322	m	cc	f	14.0565	1
ab1322	m	cc	d	12.0386	0
ab1322	m	cc	g	14.0565	0
ab1322	m	cc	a	16.0465	0
ab1322	m	cc	h	18.0369	0
ab1322	m	cc	g	20.0548	1
ab1322	m	cc	e	18.0371	1
ab1322	m	cc	c	16.0467	1
ab1452	f	cc	c	20.0496	1
ab1452	f	cc	g	18.0321	0
ab1452	f	cc	f	20.0494	1
ab1452	f	cc	f	18.0321	0
ab1452	f	cc	g	20.0494	1
ab1452	f	cc	c	18.0321	0
ab1452	f	cc	g	20.0494	0
ab1452	f	cc	c	22.0393	1
ab1452	f	cc	b	20.0496	1
ab1452	f	cc	g	18.032	0
ab1452	f	cc	g	20.0495	1
ab1452	f	cc	b	18.0321	1
ab1452	f	cc	h	16.0423	0
ab1452	f	cc	g	18.0322	0
ab1452	f	cc	c	20.0496	0
ab1452	f	cc	f	22.0393	1
ab1452	f	cc	c	20.0495	1
ab1452	f	cc	d	18.032	0
ab1452	f	cc	d	20.0496	0
ab1452	f	cc	f	22.0393	0
ab1452	f	cc	h	24.0291	1
ab1452	f	cc	a	22.0393	1
ab1452	f	cc	a	20.0495	0
ab1452	f	cc	a	22.0393	1
ab1452	f	cc	f	20.0496	0
ab1452	f	cc	e	22.0393	1
ab1452	f	cc	g	20.0496	0
ab1452	f	cc	b	22.0392	1
ab1452	f	cc	b	20.0496	1
ab1452	f	cc	c	18.0322	0
ab1452	f	cc	g	20.0494	0
ab1452	f	cc	g	22.0393	1
ab1452	f	cc	c	20.0496	1
ab1452	f	cc	c	18.0322	1
ab1452	f	cc	d	16.0425	1
ab1452	f	cc	g	14.025	0
ab1452	f	cc	g	16.0422	0
ab1452	f	cc	a	18.032	0
ab1452	f	cc	g	20.0495	0
ab1452	f	cc	b	22.0393	1
ab1452	f	cc	b	20.0494	0
ab1452	f	cc	g	22.0392	0
ab1452	f	cc	b	24.0291	0
ab1452	f	cc	f	26.0464	1
ab1452	f	cc	b	24.029	0
ab1452	f	cc	f	26.0464	0
ab1452	f	cc	a	28.0363	1
ab1452	f	cc	c	26.0465	1
ab1452	f	cc	f	24.0291	1
ab1452	f	cc	f	22.0393	1
ab1452	f	cc	b	20.0495	0
ab1452	f	cc	c	22.0393	1
ab1452	f	cc	g	20.0495	0
ab1452	f	cc	g	22.0393	0
ab1452	f	cc	f	24.0291	1
ab1452	f	cc	g	22.0393	1
ab1452	f	cc	h	20.0496	0
ab1452	f	cc	c	22.0393	1
ab1452	f	cc	h	20.0495	1
ab1452	f	cc	f	18.0322	0
ab1452	f	cc	a	20.0494	1
ab1452	f	cc	b	18.0321	1
ab1452	f	cc	a	16.0423	1
ab1452	f	cc	g	14.025	0
ab1452	f	cc	g	16.0425	0
ab1452	f	cc	f	18.0322	0
ab1452	f	cc	f	20.0496	0
ab1452	f	cc	h	22.0394	0
ab1452	f	cc	f	24.029	0
ab1452	f	cc	b	26.0465	1
ab1452	f	cc	b	24.029	1
ab1452	f	cc	d	22.0393	0
ab1452	f	cc	f	24.0291	1
ab1452	f	cc	b	22.0392	0
ab1452	f	cc	g	24.0291	1
ab1452	f	cc	g	22.0392	0
ab1452	f	cc	a	24.0289	1
ab1452	f	cc	h	22.0393	0
ab1452	f	cc	b	24.029	1
ab1452	f	cc	f	22.0393	1
ab1452	f	cc	g	20.0495	0
ab1528	f	vs	b	20.051	1
ab1528	f	vs	h	21.0459	0
ab1528	f	vs	h	20.051	0
ab1528	f	vs	g	20.0508	1
ab1528	f	vs	h	21.0458	1
ab1528	f	vs	g	22.0546	1
ab1528	f	vs	d	23.0496	1
ab1528	f	vs	g	24.0584	1
ab1528	f	vs	c	25.0532	1
ab1528	f	vs	b	26.0482	1
ab1528	f	vs	a	27.057	0
ab1528	f	vs	e	26.0483	0
ab1528	f	vs	d	25.0532	1
ab1528	f	vs	a	26.0483	0
ab1528	f	vs	c	25.0532	0
ab1528	f	vs	c	24.0583	0
ab1528	f	vs	g	23.0495	1
ab1528	f	vs	f	24.0582	0
ab1528	f	vs	g	23.0494	0
ab1528	f	vs	f	22.0546	1
ab1528	f	vs	c	23.0495	1
ab1528	f	vs	f	24.0584	0
ab1528	f	vs	g	23.0494	1
ab1528	f	vs	f	24.0584	1
ab1528	f	vs	g	25.0532	0
ab1528	f	vs	c	24.0583	1
ab1528	f	vs	b	25.0533	1
ab1528	f	vs	b	26.0482	1
ab1528	f	vs	g	27.0571	0
ab1528	f	vs	f	26.0482	0
ab1528	f	vs	d	25.0533	1
ab1528	f	vs	g	26.0483	0
ab1528	f	vs	c	25.0533	0
ab1528	f	vs	e	24.0582	1
ab1528	f	vs	a	25.0532	1
ab1528	f	vs	f	26.0481	1
ab1528	f	vs	g	27.0569	1
ab1528	f	vs	d	28.0519	1
ab1528	f	vs	b	29.0468	1
ab1528	f	vs	f	30.0557	1
ab1528	f	vs	d	31.0505	0
ab1528	f	vs	b	30.0556	0
ab1528	f	vs	b	29.0469	0
ab1528	f	vs	b	28.0517	0
ab1528	f	vs	g	27.0569	0
ab1528	f	vs	h	26.0483	0
ab1528	f	vs	b	25.0533	1
ab1528	f	vs	g	26.0483	0
ab1528	f	vs	b	25.0532	0
ab1528	f	vs	g	24.0584	1
ab1528	f	vs	b	25.0532	1
ab1528	f	vs	e	26.0482	1
ab1528	f	vs	d	27.0569	1
ab1528	f	vs	f	28.0519	0
ab1528	f	vs	h	27.057	0
ab1528	f	vs	c	26.0482	0
ab1528	f	vs	c	25.0532	1
ab1528	f	vs	c	26.0481	0
ab1528	f	vs	e	25.0532	1
ab1528	f	vs	f	26.0482	1
ab1528	f	vs	c	27.057	1
ab1528	f	vs	c	28.0519	0
ab1528	f	vs	c	27.057	0
ab1528	f	vs	b	26.0481	0
ab1528	f	vs	f	25.0533	1
ab1528	f	vs	h	26.0481	1
ab1528	f	vs	c	27.0571	0
ab1528	f	vs	h	26.0482	1
ab1528	f	vs	e	27.0567	0
ab1528	f	vs	a	26.0482	0
ab1528	f	vs	c	25.0532	0
ab1528	f	vs	c	24.0584	0
ac1874	f	vl	c	20.0456	1
ac1874	f	vl	f	21.0452	0
ac1874	f	vl	d	20.0456	1
ac1874	f	vl	c	21.0451	0
ac1874	f	vl	b	20.0457	1
ac1874	f	vl	d	21.0451	1
ac1874	f	vl	g	22.0446	1
ac1874	f	vl	f	23.0442	1
ac1874	f	vl	g	24.0436	1
ac1874	f	vl	f	25.0433	1
ac1874	f	vl	f	26.0426	1
ac1874	f	vl	e	27.0422	1
ac1874	f	vl	h	28.0416	0
ac1874	f	vl	b	30.6186	1
ac1874	f	vl	c	28.0417	1
ac1874	f	vl	g	29.041	1
ac1874	f	vl	f	30.0413	1
ac1874	f	vl	f	31.0402	1
ac1874	f	vl	f	32.0535	1
ac1874	f	vl	h	33.053	1
ac1874	f	vl	f	34.0525	1
ac1874	f	vl	h	35.052	1
ac1874	f	vl	d	36.0515	1
ac1874	f	vl	a	37.0512	1
ac1874	f	vl	d	38.0504	1
ac1874	f	vl	e	39.05	1
ac1874	f	vl	h	40.0495	1
ac1874	f	vl	c	41.049	1
ac1874	f	vl	g	42.0485	1
ac1874	f	vl	g	43.048	1
ac1874	f	vl	g	44.0476	1
ac1874	f	vl	c	45.0471	1
ac1874	f	vl	f	46.0473	1
ac1874	f	vl	a	47.0461	1
ac1874	f	vl	c	48.0456	1
ac1874	f	vl	e	49.045	1
ac1874	f	vl	a	50.0446	1
ac1874	f	vl	b	51.0441	1
ac1874	f	vl	e	52.0436	1
ac1874	f	vl	g	53.043	1
ac1874	f	vl	f	54.0426	1
ac1874	f	vl	c	55.0421	0
ac1874	f	vl	a	54.0426	1
ac1874	f	vl	h	55.042	0
ac1874	f	vl	g	54.0514	1
ac1874	f	vl	g	55.0419	0
ac1874	f	vl	d	54.0425	1
ac1874	f	vl	b	55.042	1
ac1874	f	vl	e	56.0415	1
ad0239	m	vs	d	20.0491	1
ad0239	m	vs	c	21.044	1
ad0239	m	vs	a	22.0525	0
ad0239	m	vs	f	21.0438	1
ad0239	m	vs	g	22.0527	1
ad0239	m	vs	e	23.0475	0
ad0239	m	vs	f	22.0527	0
ad0239	m	vs	a	21.0438	1
ad0239	m	vs	f	22.0526	0
ad0239	m	vs	e	21.0439	0
ad0239	m	vs	c	20.049	1
ad0239	m	vs	c	21.0438	1
ad0239	m	vs	b	22.0526	1
ad0239	m	vs	c	23.0475	0
ad0239	m	vs	g	22.0525	1
ad0239	m	vs	b	23.0473	1
ad0239	m	vs	b	24.0562	0
ad0239	m	vs	c	23.0475	1
ad0239	m	vs	e	24.0561	0
ad0239	m	vs	b	23.0476	1
ad0239	m	vs	d	24.0561	0
ad0239	m	vs	a	23.0475	1
ad0239	m	vs	f	24.056	0
ad0239	m	vs	e	23.0475	0
ad0239	m	vs	g	22.0526	0
ad0239	m	vs	c	21.0439	1
ad0239	m	vs	a	22.0526	1
ad0239	m	vs	c	23.0476	1
ad0239	m	vs	h	24.0562	0
ad0239	m	vs	e	23.0474	0
ad0239	m	vs	e	22.0526	0
ad0239	m	vs	c	21.0441	1
ad0239	m	vs	h	22.0525	0
ad0239	m	vs	c	21.044	1
ad0239	m	vs	b	22.0526	1
ad0239	m	vs	f	23.0475	0
ad0239	m	vs	c	22.0525	1
ad0239	m	vs	c	23.0474	0
ad0239	m	vs	b	22.0524	1
ad0239	m	vs	d	23.0475	0
ad0239	m	vs	h	22.0527	0
ad0239	m	vs	c	21.0439	1
ad0239	m	vs	f	22.0525	0
ad0239	m	vs	d	21.0439	0
ad0239	m	vs	c	20.0491	0
ad0239	m	vs	h	20.049	1
ad0239	m	vs	a	21.0438	1
ad0239	m	vs	b	22.0526	1
ad0239	m	vs	f	23.0475	0
ad0239	m	vs	h	22.0527	1
ad0239	m	vs	a	23.0474	1
ad0239	m	vs	b	24.056	0
ad0239	m	vs	g	23.0474	0
ad0239	m	vs	c	22.0525	1
ad0239	m	vs	f	23.0475	0
ad0239	m	vs	c	22.0526	0
ad0239	m	vs	g	21.0439	1
ad0239	m	vs	b	22.0526	1
ad0239	m	vs	c	23.0474	0
ad0239	m	vs	c	22.0527	1
ad0239	m	vs	f	23.0475	0
ad0239	m	vs	b	22.0526	0
ad0239	m	vs	g	21.0439	1
ad0239	m	vs	g	22.0526	0
ad0239	m	vs	b	21.0439	0
ad0239	m	vs	d	20.0492	0
ad0239	m	vs	c	20.049	0
ad0239	m	vs	f	20.049	1
ad0239	m	vs	b	21.0441	1
ad0239	m	vs	a	22.0525	1
ad0239	m	vs	b	23.0474	0
ad0239	m	vs	b	22.0526	0
ad0239	m	vs	c	21.0438	1
ad0239	m	vs	a	22.0525	1
ad0239	m	vs	c	23.0473	0
ad0239	m	vs	a	22.0526	0
ad0239	m	vs	d	21.044	1
ad0239	m	vs	d	22.0526	1
ad0239	m	vs	c	23.0474	1
ad0239	m	vs	d	24.0561	1
ah1575	m	cc	f	20.0418	1
ah1575	m	cc	c	18.0195	1
ah1575	m	cc	b	16.0251	1
ah1575	m	cc	e	14.0306	0
ah1575	m	cc	e	16.0252	0
ah1575	m	cc	h	18.0195	0
ah1575	m	cc	f	20.0417	0
ah1575	m	cc	d	22.0362	1
ah1575	m	cc	e	20.0416	1
ah1575	m	cc	b	18.0194	0
ah1575	m	cc	b	20.0417	1
ah1575	m	cc	b	18.0195	0
ah1575	m	cc	b	20.0417	0
ah1575	m	cc	b	22.0361	1
ah1575	m	cc	f	20.0416	0
ah1575	m	cc	a	22.0362	1
ah1575	m	cc	b	20.0417	1
ah1575	m	cc	g	18.0195	1
ah1575	m	cc	g	16.025	0
ah1575	m	cc	g	18.0196	1
ah1575	m	cc	f	16.0249	0
ah1575	m	cc	h	18.0195	0
ah1575	m	cc	b	20.0417	1
ah1575	m	cc	a	18.0194	1
ah1575	m	cc	b	16.025	0
ah1575	m	cc	g	18.0195	0
ah1575	m	cc	g	20.0417	1
ah1575	m	cc	g	18.0194	0
ah1575	m	cc	c	20.0417	0
ah1575	m	cc	b	22.0361	1
ah1575	m	cc	d	20.0416	0
ah1575	m	cc	d	22.036	0
ah1575	m	cc	c	24.0306	1
ah1575	m	cc	c	22.0361	0
ah1575	m	cc	g	24.0307	0
ah1575	m	cc	e	26.025	1
ah1575	m	cc	c	24.0306	0
ah1575	m	cc	c	26.025	1
ah1575	m	cc	c	24.0305	1
ah1575	m	cc	f	22.0361	1
ah1575	m	cc	b	20.0416	0
ah1575	m	cc	e	22.0361	1
ah1575	m	cc	c	20.0416	0
ah1575	m	cc	f	22.0362	1
ah1575	m	cc	a	20.0417	1
ah1575	m	cc	f	18.0195	0
ah1575	m	cc	f	20.0416	0
ah1575	m	cc	b	22.0362	1
ah1575	m	cc	c	20.0417	1
ah1575	m	cc	h	18.0195	0
ah1575	m	cc	g	20.0417	0
ah1575	m	cc	d	22.0361	1
ah1575	m	cc	b	20.0416	0
ah1575	m	cc	b	22.0362	1
ah1575	m	cc	h	20.0417	0
ah1575	m	cc	f	22.0362	1
ah1575	m	cc	f	20.0417	0
ah1575	m	cc	c	22.0361	1
ah1575	m	cc	g	20.0416	1
ah1575	m	cc	h	18.0195	1
ah1575	m	cc	g	16.025	0
ah1575	m	cc	d	18.0195	1
ah1575	m	cc	f	16.025	0
ah1575	m	cc	b	18.0195	1
ah1575	m	cc	b	16.025	0
ah1575	m	cc	b	18.0195	1
ah1575	m	cc	h	16.0251	0
ah1575	m	cc	g	18.0195	0
ah1575	m	cc	h	20.0416	1
ah1575	m	cc	g	18.0195	1
ah1575	m	cc	b	16.0249	0
ah1575	m	cc	e	18.0195	1
ah1575	m	cc	a	16.0251	0
ah1575	m	cc	g	18.0195	1
ah1575	m	cc	d	16.025	0
ah1575	m	cc	d	18.0195	0
ah1575	m	cc	f	20.0416	1
ah1575	m	cc	b	18.0194	1
ah1575	m	cc	e	16.025	1
ah1575	m	cc	b	14.0305	0
ah1575	m	cc	e	16.0251	1
ah1575	m	cc	c	14.0305	0
ah1575	m	cc	c	16.0251	1
ah1575	m	cc	c	14.0306	1
ah1575	m	cc	e	12.0361	0
ah1575	m	cc	f	14.0305	0
ah1575	m	cc	a	16.025	1
ah1575	m	cc	c	14.0307	1
ah1575	m	cc	b	12.0361	0
ah1575	m	cc	e	14.0306	0
ah1575	m	cc	a	16.025	1
ak1774	f	vl	b	20.0415	0
ak1774	f	vl	b	19.0445	0
ak1774	f	vl	c	18.047	1
ak1774	f	vl	g	19.0444	0
ak1774	f	vl	g	18.0472	0
ak1774	f	vl	b	17.0499	1
ak1774	f	vl	b	18.0472	1
ak1774	f	vl	b	19.0445	1
ak1774	f	vl	h	20.0418	0
ak1774	f	vl	c	19.0445	1
ak1774	f	vl	c	20.0416	0
ak1774	f	vl	h	19.0445	0
ak1774	f	vl	c	18.0472	1
ak1774	f	vl	c	19.0444	0
ak1774	f	vl	a	18.0472	0
ak1774	f	vl	c	17.05	1
ak1774	f	vl	c	18.0471	0
ak1774	f	vl	f	17.0499	1
ak1774	f	vl	h	18.0471	1
ak1774	f	vl	e	19.0444	0
ak1774	f	vl	g	18.0472	0
ak1774	f	vl	g	17.0498	0
ak1774	f	vl	h	16.0526	1
ak1774	f	vl	d	17.05	0
ak1774	f	vl	c	16.0528	1
ak1774	f	vl	g	17.0499	0
ak1774	f	vl	a	16.0527	1
ak1774	f	vl	g	17.05	1
ak1774	f	vl	h	18.0472	0
ak1774	f	vl	b	17.0499	1
ak1774	f	vl	b	18.0471	1
ak1774	f	vl	g	19.0444	1
ak1774	f	vl	c	20.0416	0
ak1774	f	vl	d	19.0444	0
ak1774	f	vl	d	18.0471	1
ak1774	f	vl	f	19.0445	0
ak1774	f	vl	h	18.0472	0
ak1774	f	vl	g	17.05	1
ak1774	f	vl	g	18.0473	1
ak1774	f	vl	f	19.0443	1
ak1774	f	vl	g	20.0416	0
ak1774	f	vl	f	19.0444	1
ak1774	f	vl	h	20.0415	1
ak1774	f	vl	c	21.0528	0
ak1774	f	vl	d	20.0417	1
ak1774	f	vl	f	21.0527	0
ak1774	f	vl	g	20.0417	1
ak1774	f	vl	c	21.0527	0
ak1774	f	vl	c	20.0417	1
ak1774	f	vl	c	21.0527	0
ak1774	f	vl	c	20.0416	1
ak1774	f	vl	g	21.0528	0
ak1774	f	vl	f	20.0415	1
ak1774	f	vl	g	21.0527	0
ak1774	f	vl	g	20.0416	1
ak1774	f	vl	f	21.0528	0
ak1774	f	vl	c	20.0416	1
ak1774	f	vl	f	21.0527	1
ak1774	f	vl	g	22.05	0
ak1774	f	vl	c	21.0528	1
ak1774	f	vl	c	22.0501	1
ak1774	f	vl	g	23.0473	0
ak1774	f	vl	a	22.05	0
ak1774	f	vl	c	21.0528	1
ak1774	f	vl	b	22.0499	1
ak1774	f	vl	g	23.0473	0
ak1774	f	vl	a	22.0501	1
ak1774	f	vl	a	23.0473	1
ak1774	f	vl	g	24.0444	1
ak1774	f	vl	c	25.0418	0
ak1774	f	vl	a	24.0445	1
ak1774	f	vl	a	25.0418	0
ak1774	f	vl	f	24.0445	0
ak1774	f	vl	g	23.0472	1
ak1774	f	vl	g	24.0444	0
ak1774	f	vl	b	23.0473	1
ak1774	f	vl	b	24.0444	0
ak1774	f	vl	f	23.047	1
ak1774	f	vl	d	24.0445	0
ak1774	f	vl	e	23.0473	1
ak1774	f	vl	c	24.0444	0
ak1774	f	vl	f	23.0473	1
ak1774	f	vl	a	24.0445	1
ak1774	f	vl	b	25.0417	0
ak1774	f	vl	h	24.0444	1
ak1774	f	vl	d	25.0417	0
ak1774	f	vl	g	24.0445	1
ak1774	f	vl	b	25.0415	0
ak1774	f	vl	h	24.0444	1
ak1774	f	vl	c	25.0416	1
ak1774	f	vl	a	26.0529	1
am1272	m	vl	g	20.05	0
am1272	m	vl	c	19.0543	0
am1272	m	vl	f	18.0457	1
am1272	m	vl	f	19.0545	1
am1272	m	vl	h	20.0491	1
am1272	m	vl	h	21.0441	1
am1272	m	vl	d	22.0529	0
am1272	m	vl	f	21.044	0
am1272	m	vl	g	20.0492	1
am1272	m	vl	g	21.0441	0
am1272	m	vl	c	20.0492	0
am1272	m	vl	f	19.0544	1
am1272	m	vl	g	20.0493	0
am1272	m	vl	h	19.0544	0
am1272	m	vl	d	18.0458	1
am1272	m	vl	g	19.0543	0
am1272	m	vl	b	18.0458	0
am1272	m	vl	e	17.0509	1
am1272	m	vl	f	18.0456	0
am1272	m	vl	g	17.0508	0
am1272	m	vl	a	16.056	1
am1272	m	vl	c	17.051	0
am1272	m	vl	h	16.0559	0
am1272	m	vl	a	15.0473	1
am1272	m	vl	c	16.056	1
am1272	m	vl	g	17.0508	1
am1272	m	vl	a	18.0458	1
am1272	m	vl	g	19.0545	0
am1272	m	vl	c	18.0456	1
am1272	m	vl	e	19.0543	0
am1272	m	vl	g	18.0455	1
am1272	m	vl	e	19.0543	0
am1272	m	vl	g	18.0456	0
am1272	m	vl	a	17.0508	1
am1272	m	vl	h	18.0458	0
am1272	m	vl	h	17.0507	1
am1272	m	vl	a	18.0456	1
am1272	m	vl	g	19.0543	0
am1272	m	vl	c	18.0456	0
am1272	m	vl	e	17.0507	1
am1272	m	vl	b	18.0458	0
am1272	m	vl	g	17.0508	1
am1272	m	vl	h	18.0457	0
am1272	m	vl	c	17.0507	0
am1272	m	vl	c	16.0559	1
am1272	m	vl	e	17.0509	1
am1272	m	vl	b	18.0456	0
am1272	m	vl	g	17.0508	1
am1272	m	vl	d	18.0457	1
am1272	m	vl	f	19.0545	1
am1272	m	vl	h	20.0492	0
am1272	m	vl	d	19.0544	1
am1272	m	vl	b	20.0492	1
am1272	m	vl	e	21.0442	1
am1272	m	vl	b	22.0527	1
am1272	m	vl	c	23.0476	0
am1272	m	vl	c	22.0528	1
am1272	m	vl	c	23.0477	1
am1272	m	vl	c	24.0561	0
am1371	m	vs	c	20.051	1
am1371	m	vs	g	21.0459	0
am1371	m	vs	f	20.051	0
am1371	m	vs	b	20.0509	1
am1371	m	vs	d	21.0459	0
am1371	m	vs	d	20.051	1
am1371	m	vs	a	21.0459	1
am1371	m	vs	g	22.0546	0
am1371	m	vs	h	21.0457	0
am1371	m	vs	c	20.0509	1
am1371	m	vs	e	21.0459	1
am1371	m	vs	c	22.0545	0
am1371	m	vs	a	21.0458	1
am1371	m	vs	b	22.0547	1
am1371	m	vs	g	23.0496	0
am1371	m	vs	e	22.0547	0
am1371	m	vs	a	21.0459	0
am1371	m	vs	h	20.051	0
am1371	m	vs	c	20.0509	1
am1371	m	vs	c	21.0459	0
am1371	m	vs	d	20.0509	1
am1371	m	vs	h	21.0458	0
am1371	m	vs	b	20.0509	1
am1371	m	vs	c	21.0458	1
am1371	m	vs	c	22.0545	0
am1371	m	vs	h	21.0457	0
am1371	m	vs	b	20.0509	1
am1371	m	vs	f	21.0459	0
am1371	m	vs	b	20.051	1
am1371	m	vs	f	21.0459	0
am1371	m	vs	f	20.0507	0
am1371	m	vs	d	20.0508	0
am1371	m	vs	e	20.0507	0
am1371	m	vs	a	20.051	1
am1371	m	vs	d	21.0458	0
am1371	m	vs	g	20.0509	0
am1371	m	vs	f	20.0509	0
am1371	m	vs	c	20.0509	1
am1371	m	vs	b	21.0459	1
am1371	m	vs	f	22.0546	1
am1371	m	vs	f	23.0495	0
am1371	m	vs	h	22.0546	0
am1371	m	vs	f	21.0459	0
am1371	m	vs	e	20.0509	1
am1371	m	vs	d	21.0459	0
am1371	m	vs	f	20.051	0
am1371	m	vs	b	20.0509	1
am1371	m	vs	f	21.0459	1
am1371	m	vs	c	22.0547	0
am1371	m	vs	f	21.0459	1
am1371	m	vs	d	22.0546	0
am1371	m	vs	b	21.0459	0
am1371	m	vs	f	20.0507	1
am1371	m	vs	h	21.0459	0
am1371	m	vs	c	20.0509	0
am1371	m	vs	b	20.0507	1
am1371	m	vs	g	21.0459	0
am1371	m	vs	d	20.051	1
am1371	m	vs	f	21.0456	0
am1371	m	vs	c	20.051	0
am1371	m	vs	d	20.0508	0
am1371	m	vs	g	20.051	0
am1371	m	vs	b	20.0507	1
am1371	m	vs	a	21.0458	1
am1371	m	vs	f	22.0546	1
am1371	m	vs	c	23.0495	1
am1371	m	vs	e	24.0581	0
am1371	m	vs	d	23.0496	0
am1371	m	vs	e	22.0545	1
am1371	m	vs	c	23.0495	0
am1371	m	vs	f	22.0546	0
am1371	m	vs	f	21.0457	1
am1371	m	vs	f	22.0543	0
am1371	m	vs	f	21.0459	0
am1371	m	vs	e	20.0509	0
am1371	m	vs	c	20.0509	1
am1371	m	vs	g	21.0457	0
am1371	m	vs	b	20.0509	0
am1371	m	vs	a	20.0508	1
am1371	m	vs	h	21.0459	0
am1371	m	vs	g	20.0508	1
am1371	m	vs	b	21.0458	0
am1371	m	vs	g	20.051	1
am1371	m	vs	e	21.0458	1
am1371	m	vs	g	22.0547	0
am1371	m	vs	c	21.046	1
am1371	m	vs	c	22.0547	1
am1387	f	vs	b	20.0637	1
am1387	f	vs	c	21.0637	0
am1387	f	vs	b	20.0549	0
am1387	f	vs	g	20.0548	0
am1387	f	vs	h	20.0551	0
am1387	f	vs	c	20.055	0
am1387	f	vs	g	20.055	0
am1387	f	vs	b	20.0551	0
am1387	f	vs	g	20.0548	0
am1387	f	vs	b	20.0548	0
am1387	f	vs	f	20.0549	0
am1387	f	vs	e	20.0545	1
am1387	f	vs	e	21.0638	0
am1387	f	vs	g	20.0547	0
am1387	f	vs	e	20.0549	1
am1387	f	vs	c	21.0723	0
am1387	f	vs	c	20.0549	0
am1387	f	vs	f	20.0546	1
am1387	f	vs	g	21.0578	0
am1387	f	vs	c	20.0548	1
am1387	f	vs	b	21.0639	1
am1387	f	vs	b	22.0589	0
am1387	f	vs	c	21.064	0
am1387	f	vs	a	20.055	1
am1387	f	vs	e	21.0636	0
am1387	f	vs	a	20.055	1
am1387	f	vs	b	21.0639	1
am1387	f	vs	f	22.0589	0
am1387	f	vs	d	21.0639	1
am1387	f	vs	e	22.0588	0
am1387	f	vs	c	21.064	0
am1387	f	vs	c	20.0574	1
am1387	f	vs	g	21.0638	0
am1387	f	vs	d	20.055	1
am1387	f	vs	g	21.064	0
am1387	f	vs	d	20.0549	1
am1387	f	vs	b	21.0635	1
am1387	f	vs	g	22.0591	0
am1387	f	vs	b	21.0639	1
am1387	f	vs	a	22.0589	0
am1387	f	vs	g	21.0636	1
am1387	f	vs	f	22.059	0
am1387	f	vs	g	21.0637	0
am1387	f	vs	g	20.0549	0
am1387	f	vs	h	20.0548	0
am1387	f	vs	h	20.0548	0
am1387	f	vs	c	20.0549	0
am1387	f	vs	g	20.0552	1
am1387	f	vs	h	21.0639	0
am1387	f	vs	f	20.0548	0
am1387	f	vs	e	20.0548	1
am1387	f	vs	e	21.064	0
am1387	f	vs	g	20.0548	1
am1387	f	vs	c	21.0639	0
am1387	f	vs	c	20.0549	1
am1387	f	vs	c	21.0637	0
am1387	f	vs	e	20.0549	1
am1387	f	vs	b	21.0639	0
am1387	f	vs	e	20.0546	1
am1387	f	vs	a	21.0638	1
am1387	f	vs	c	22.0589	0
am1387	f	vs	f	21.0638	0
am1387	f	vs	h	20.0548	0
am1387	f	vs	b	20.0551	1
am1387	f	vs	g	21.0648	0
am1387	f	vs	b	20.0549	0
am1387	f	vs	d	20.0548	1
am1387	f	vs	g	21.0637	0
am1387	f	vs	f	20.0548	0
am1387	f	vs	b	20.0549	1
am1387	f	vs	h	21.0638	1
am1387	f	vs	b	22.0591	0
am1387	f	vs	d	21.0639	1
am1387	f	vs	b	22.0589	1
am1387	f	vs	c	23.0541	0
am1387	f	vs	b	22.0588	0
am1387	f	vs	c	21.0638	0
am1387	f	vs	h	20.0547	0
am1387	f	vs	f	20.0548	0
am1387	f	vs	b	20.0506	1
am1387	f	vs	c	21.0639	1
am1387	f	vs	c	22.0591	0
am1387	f	vs	g	21.0639	0
ao2428	f	cc	c	20.0824	1
ao2428	f	cc	g	18.0837	0
ao2428	f	cc	g	20.0827	1
ao2428	f	cc	c	18.0563	0
ao2428	f	cc	g	20.0826	1
ao2428	f	cc	b	18.0701	1
ao2428	f	cc	c	16.0577	1
ao2428	f	cc	b	14.0591	0
ao2428	f	cc	c	16.0715	0
ao2428	f	cc	e	18.0561	0
ao2428	f	cc	d	20.0827	1
ao2428	f	cc	c	18.0563	0
ao2428	f	cc	a	20.0827	1
ao2428	f	cc	b	18.0565	0
ao2428	f	cc	a	20.0825	1
ao2428	f	cc	g	18.0701	0
ao2428	f	cc	c	20.0826	0
ao2428	f	cc	d	22.0812	0
ao2428	f	cc	b	24.0797	1
ao2428	f	cc	f	22.0811	1
ao2428	f	cc	c	20.0827	1
ao2428	f	cc	e	18.0703	0
ao2428	f	cc	h	20.0826	0
ao2428	f	cc	f	22.0812	1
ao2428	f	cc	a	20.0828	1
ao2428	f	cc	f	18.0703	0
ao2428	f	cc	h	20.0824	1
ao2428	f	cc	f	18.07	0
ao2428	f	cc	g	20.0827	0
ao2428	f	cc	b	22.0809	1
ao2428	f	cc	d	20.0827	1
ao2428	f	cc	a	18.084	1
ao2428	f	cc	g	16.0578	0
ao2428	f	cc	c	18.07	0
ao2428	f	cc	f	20.0826	1
ao2428	f	cc	c	18.0702	1
ao2428	f	cc	c	16.0581	0
ao2428	f	cc	b	18.0565	1
ao2428	f	cc	g	16.0577	0
ao2428	f	cc	g	18.0566	0
ao2428	f	cc	e	20.0827	1
ao2428	f	cc	h	18.0701	0
ao2428	f	cc	e	20.0825	1
ao2428	f	cc	h	18.0701	0
ao2428	f	cc	b	20.0827	0
ao2428	f	cc	b	22.0814	1
ao2428	f	cc	g	20.0825	0
ao2428	f	cc	b	22.0811	0
ao2428	f	cc	g	24.0795	0
ao2428	f	cc	d	26.0785	0
ao2428	f	cc	f	28.077	1
ao2428	f	cc	g	26.0783	1
ao2428	f	cc	f	24.0797	1
ao2428	f	cc	g	22.0809	0
ao2428	f	cc	f	24.0799	1
ao2428	f	cc	c	22.0813	1
ao2428	f	cc	c	20.0824	1
ao2428	f	cc	a	18.0566	0
ao2428	f	cc	c	20.0827	0
ao2428	f	cc	c	22.0811	1
ao2428	f	cc	a	20.0828	1
ao2428	f	cc	f	18.0562	1
ao2428	f	cc	f	16.0579	0
ao2428	f	cc	g	18.0566	0
ao2428	f	cc	f	20.0826	0
ao2428	f	cc	g	22.0811	1
ao2428	f	cc	f	20.0828	1
ao2428	f	cc	c	18.0702	0
ao2428	f	cc	h	20.0827	1
ao2428	f	cc	b	18.0565	0
ao2428	f	cc	g	20.0824	1
ao2428	f	cc	h	18.0563	0
ao2428	f	cc	e	20.0827	0
ao2428	f	cc	h	22.0811	1
ao2428	f	cc	c	20.0826	0
ao2428	f	cc	c	22.0813	0
ao2428	f	cc	d	24.0797	1
ao2428	f	cc	d	22.0809	1
ao2428	f	cc	f	20.0825	0
ao2428	f	cc	a	22.081	0
ao2428	f	cc	a	24.0799	1
ao2428	f	cc	c	22.081	0
ao2428	f	cc	b	24.0796	1
ao2428	f	cc	h	22.0813	0
ao2428	f	cc	b	24.0797	0
ao2428	f	cc	g	26.078	1
ao2428	f	cc	c	24.0797	1
ao2428	f	cc	f	22.0813	1
ao2428	f	cc	c	20.0828	1
aw0927	m	vs	g	20.049	0
aw0927	m	vs	g	20.0489	0
aw0927	m	vs	a	20.049	1
aw0927	m	vs	c	21.0438	1
aw0927	m	vs	c	22.0525	1
aw0927	m	vs	h	23.0473	0
aw0927	m	vs	b	22.0525	0
aw0927	m	vs	b	21.0438	1
aw0927	m	vs	b	22.0524	0
aw0927	m	vs	g	21.0438	1
aw0927	m	vs	g	22.0525	0
aw0927	m	vs	d	21.0438	1
aw0927	m	vs	b	22.0524	0
aw0927	m	vs	f	21.0438	1
aw0927	m	vs	h	22.0524	0
aw0927	m	vs	c	21.0439	1
aw0927	m	vs	b	22.0525	0
aw0927	m	vs	a	21.0438	1
aw0927	m	vs	g	22.0525	0
aw0927	m	vs	g	21.0438	1
aw0927	m	vs	f	22.0524	1
aw0927	m	vs	c	23.0473	0
aw0927	m	vs	c	22.0525	1
aw0927	m	vs	c	23.0473	0
aw0927	m	vs	g	22.0525	0
aw0927	m	vs	g	21.0439	0
aw0927	m	vs	d	20.0489	1
aw0927	m	vs	e	21.0438	1
aw0927	m	vs	c	22.0525	1
aw0927	m	vs	f	23.0473	0
aw0927	m	vs	e	22.0524	0
aw0927	m	vs	e	21.0436	0
aw0927	m	vs	e	20.049	1
aw0927	m	vs	b	21.0438	0
aw0927	m	vs	h	20.049	0
aw0927	m	vs	c	20.049	1
aw0927	m	vs	d	21.0438	0
aw0927	m	vs	g	20.0489	1
aw0927	m	vs	f	21.0438	0
aw0927	m	vs	c	20.049	0
aw0927	m	vs	f	20.049	1
aw0927	m	vs	b	21.0437	0
aw0927	m	vs	d	20.049	0
aw0927	m	vs	a	20.049	1
aw0927	m	vs	f	21.0438	1
aw0927	m	vs	f	22.0524	0
aw0927	m	vs	h	21.0437	0
aw0927	m	vs	b	20.0489	1
aw0927	m	vs	f	21.0437	1
aw0927	m	vs	d	22.0524	0
aw0927	m	vs	b	21.0437	0
aw0927	m	vs	b	20.0489	1
aw0927	m	vs	g	21.0438	1
aw0927	m	vs	c	22.0524	1
aw0927	m	vs	f	23.0473	0
aw0927	m	vs	a	22.0525	0
aw0927	m	vs	h	21.0437	0
aw0927	m	vs	b	20.0489	1
aw0927	m	vs	f	21.0438	0
aw0927	m	vs	c	20.0489	0
aw0927	m	vs	f	20.049	0
aw0927	m	vs	a	20.049	1
aw0927	m	vs	h	21.0437	0
aw0927	m	vs	c	20.0489	0
aw0927	m	vs	b	20.0487	0
aw0927	m	vs	b	20.049	1
aw0927	m	vs	d	21.0437	1
aw0927	m	vs	g	22.0525	0
aw0927	m	vs	e	21.0438	0
aw0927	m	vs	g	20.0488	0
aw0927	m	vs	a	20.0489	0
aw0927	m	vs	f	20.0488	1
aw0927	m	vs	f	21.0438	1
aw0927	m	vs	h	22.0525	1
aw0927	m	vs	h	23.0474	0
aw0927	m	vs	c	22.0525	0
aw0927	m	vs	c	21.0436	1
aw0927	m	vs	f	22.0524	0
aw0927	m	vs	b	21.0438	0
aw0927	m	vs	c	20.0489	0
aw0927	m	vs	a	20.049	1
aw0927	m	vs	f	21.0438	0
aw0927	m	vs	b	20.049	1
aw0927	m	vs	a	21.0438	0
aw0927	m	vs	f	20.0489	0
aw0927	m	vs	a	20.0487	0
aw0927	m	vs	g	20.0488	0
aw0927	m	vs	f	20.0488	0
aw1939	f	vl	c	20.0547	0
aw1939	f	vl	g	19.0597	1
aw1939	f	vl	c	20.0549	0
aw1939	f	vl	b	19.0596	0
aw1939	f	vl	g	18.0647	1
aw1939	f	vl	f	19.0595	1
aw1939	f	vl	b	20.0549	0
aw1939	f	vl	f	19.0598	1
aw1939	f	vl	d	20.0549	0
aw1939	f	vl	a	19.0599	0
aw1939	f	vl	f	18.0644	1
aw1939	f	vl	g	19.0598	1
aw1939	f	vl	c	20.0549	0
aw1939	f	vl	a	19.0596	0
aw1939	f	vl	f	18.0645	1
aw1939	f	vl	c	19.0598	0
aw1939	f	vl	h	18.0646	1
aw1939	f	vl	f	19.0596	1
aw1939	f	vl	h	20.0546	1
aw1939	f	vl	f	21.0638	1
aw1939	f	vl	b	22.059	1
aw1939	f	vl	d	23.0542	1
aw1939	f	vl	e	24.0631	1
aw1939	f	vl	a	32.0381	1
aw1939	f	vl	h	26.0885	1
aw1939	f	vl	b	27.1217	1
aw1939	f	vl	f	28.0574	0
aw1939	f	vl	c	27.0623	1
aw1939	f	vl	e	28.0574	0
aw1939	f	vl	f	27.0622	0
aw1939	f	vl	e	26.0535	1
aw1939	f	vl	f	27.0623	1
aw1939	f	vl	d	28.0575	1
aw1939	f	vl	g	29.0525	1
aw1939	f	vl	c	30.0615	1
aw1939	f	vl	b	31.0567	1
aw1939	f	vl	a	32.0518	0
aw1939	f	vl	b	31.0566	1
aw1939	f	vl	g	32.0516	1
aw1939	f	vl	g	33.0607	1
aw1939	f	vl	f	34.0558	0
aw1939	f	vl	g	33.0606	1
aw1939	f	vl	f	34.0558	0
aw1939	f	vl	g	33.0608	0
aw1939	f	vl	h	32.052	1
aw1939	f	vl	g	33.0608	1
aw1939	f	vl	h	34.0557	1
aw1939	f	vl	c	35.0648	1
aw1939	f	vl	h	36.073	1
aw1939	f	vl	f	37.0551	1
aw1939	f	vl	c	38.0518	1
aw1939	f	vl	e	39.0592	1
aw1939	f	vl	e	40.0538	1
aw1939	f	vl	a	41.0631	1
aw1939	f	vl	c	42.0586	1
aw1939	f	vl	g	43.0538	1
aw1939	f	vl	c	44.0628	1
aw1939	f	vl	f	45.0578	1
aw1939	f	vl	f	46.0528	1
aw1939	f	vl	d	47.0617	1
bf0188	m	vs	c	20.0491	1
bf0188	m	vs	f	21.0439	0
bf0188	m	vs	g	20.0491	1
bf0188	m	vs	b	21.0438	1
bf0188	m	vs	h	22.0526	0
bf0188	m	vs	f	21.0439	1
bf0188	m	vs	f	22.0526	0
bf0188	m	vs	b	21.044	0
bf0188	m	vs	h	20.049	0
bf0188	m	vs	h	20.049	0
bf0188	m	vs	c	20.0491	0
bf0188	m	vs	c	20.0491	0
bf0188	m	vs	g	20.0491	0
bf0188	m	vs	g	20.0491	0
bf0188	m	vs	b	20.0491	0
bf0188	m	vs	h	20.0491	0
bf0188	m	vs	h	20.0491	1
bf0188	m	vs	f	21.0439	0
bf0188	m	vs	d	20.0491	1
bf0188	m	vs	c	21.0439	0
bf0188	m	vs	e	20.049	1
bf0188	m	vs	b	21.0439	1
bf0188	m	vs	b	22.0527	1
bf0188	m	vs	b	23.0474	1
bf0188	m	vs	h	24.0562	1
bf0188	m	vs	h	25.0509	1
bf0188	m	vs	g	26.0458	1
bf0188	m	vs	h	27.0546	0
bf0188	m	vs	g	26.0459	1
bf0188	m	vs	e	27.0545	1
bf0188	m	vs	f	28.0492	1
bf0188	m	vs	a	29.0442	1
bf0188	m	vs	e	30.053	1
bf0188	m	vs	f	31.0477	1
bf0188	m	vs	e	32.0565	1
bf0188	m	vs	f	33.0512	0
bf0188	m	vs	e	32.0565	1
bf0188	m	vs	f	33.0513	1
bf0188	m	vs	a	34.046	0
bf0188	m	vs	g	33.0513	1
bf0188	m	vs	f	34.0461	0
bf0188	m	vs	b	33.0511	1
bf0188	m	vs	h	34.046	0
bf0188	m	vs	b	33.0511	1
bf0188	m	vs	b	34.046	1
bf0188	m	vs	g	35.0547	0
bf0188	m	vs	h	34.0463	0
bf0188	m	vs	b	33.0513	1
bf0188	m	vs	g	34.0462	0
bf0188	m	vs	f	33.0511	1
bf0188	m	vs	a	34.0462	0
bf0188	m	vs	f	33.0515	1
bf0188	m	vs	b	34.046	0
bf0188	m	vs	e	33.0512	1
bf0188	m	vs	c	34.046	0
bf0188	m	vs	e	33.0512	1
bf0188	m	vs	g	34.0461	0
bf0188	m	vs	g	33.0513	0
bf0188	m	vs	f	32.0564	0
bf0188	m	vs	b	31.0478	1
bf0188	m	vs	e	32.0565	0
bf0188	m	vs	b	31.0477	0
bf0188	m	vs	g	30.0529	1
bf0188	m	vs	f	31.0478	1
bf0188	m	vs	f	32.0565	0
bf0188	m	vs	c	31.0478	0
bf0188	m	vs	g	30.0528	0
bf0188	m	vs	b	29.0441	0
bf0188	m	vs	c	28.0493	1
bf0188	m	vs	f	29.0443	1
bf0188	m	vs	a	30.0528	0
bf0188	m	vs	b	29.0442	0
bf0188	m	vs	c	28.0494	1
bm1103	m	vl	c	20.0549	0
bm1103	m	vl	g	19.0595	0
bm1103	m	vl	b	18.0633	1
bm1103	m	vl	g	19.0594	1
bm1103	m	vl	g	20.0549	1
bm1103	m	vl	c	21.0637	0
bm1103	m	vl	d	20.0548	0
bm1103	m	vl	g	19.0597	1
bm1103	m	vl	a	20.0549	1
bm1103	m	vl	a	21.0637	0
bm1103	m	vl	a	20.0547	1
bm1103	m	vl	f	21.064	0
bm1103	m	vl	g	20.0551	0
bm1103	m	vl	f	19.0595	0
bm1103	m	vl	f	18.0578	0
bm1103	m	vl	e	17.0558	1
bm1103	m	vl	e	18.0629	1
bm1103	m	vl	b	19.0598	1
bm1103	m	vl	c	20.0551	1
bm1103	m	vl	b	21.0637	0
bm1103	m	vl	c	20.0549	1
bm1103	m	vl	c	21.0627	1
bm1103	m	vl	g	22.059	0
bm1103	m	vl	f	21.0636	1
bm1103	m	vl	f	22.0588	1
bm1103	m	vl	h	23.054	0
bm1103	m	vl	a	22.0587	1
bm1103	m	vl	a	23.054	1
bm1103	m	vl	h	24.063	0
bm1103	m	vl	b	23.0542	1
bm1103	m	vl	g	24.063	1
bm1103	m	vl	e	25.0596	1
bm1103	m	vl	b	26.0532	1
bm1103	m	vl	a	27.0623	0
bm1103	m	vl	h	26.0533	0
bm1103	m	vl	e	25.0583	1
bm1103	m	vl	c	26.0533	1
bm1103	m	vl	b	27.0623	0
bm1103	m	vl	e	26.0533	1
bm1103	m	vl	g	27.0623	0
bm1103	m	vl	f	26.0534	1
bm1103	m	vl	b	27.0619	0
bm1103	m	vl	d	26.0535	1
bm1103	m	vl	f	27.0622	0
bm1103	m	vl	h	26.0534	1
bm1103	m	vl	b	27.0625	0
bm1103	m	vl	g	26.0534	1
bm1103	m	vl	b	27.0624	1
bm1103	m	vl	h	28.0573	0
bm1103	m	vl	c	27.0625	1
bm1103	m	vl	e	28.0575	0
bm1103	m	vl	g	27.0623	0
bm1103	m	vl	h	26.0533	0
bm1103	m	vl	e	25.0582	1
bm1103	m	vl	b	26.0532	0
bm1103	m	vl	h	25.0581	1
bm1103	m	vl	e	26.0534	1
bm1103	m	vl	c	27.0624	1
bm1103	m	vl	f	28.0572	1
bm1103	m	vl	c	29.0526	1
bm1103	m	vl	b	30.0615	1
bm1103	m	vl	e	31.0566	1
bm1103	m	vl	h	32.0518	1
bm1103	m	vl	c	33.0607	0
bm1103	m	vl	h	32.0517	1
bm1103	m	vl	a	33.0609	1
bm1103	m	vl	g	34.0559	1
bm1103	m	vl	c	35.0511	0
bm1103	m	vl	c	34.0558	0
bm1103	m	vl	f	33.0608	0
bm1103	m	vl	c	32.0519	0
bm1103	m	vl	e	31.0589	1
bs0274	f	cc	f	20.079	0
bs0274	f	cc	c	22.0768	0
bs0274	f	cc	g	24.075	1
bs0274	f	cc	c	22.0771	0
bs0274	f	cc	c	24.0754	1
bs0274	f	cc	f	22.0768	1
bs0274	f	cc	f	20.0787	0
bs0274	f	cc	b	22.0771	1
bs0274	f	cc	b	20.0788	1
bs0274	f	cc	b	18.0806	0
bs0274	f	cc	d	20.0788	1
bs0274	f	cc	c	18.0808	0
bs0274	f	cc	g	20.0787	1
bs0274	f	cc	c	18.0805	0
bs0274	f	cc	c	20.0787	1
bs0274	f	cc	b	18.0808	0
bs0274	f	cc	f	20.0788	0
bs0274	f	cc	e	22.0768	0
bs0274	f	cc	g	24.0752	1
bs0274	f	cc	g	22.0768	1
bs0274	f	cc	g	20.0787	1
bs0274	f	cc	d	18.0807	0
bs0274	f	cc	d	20.0789	1
bs0274	f	cc	a	18.0806	0
bs0274	f	cc	d	20.0789	1
bs0274	f	cc	h	18.0807	0
bs0274	f	cc	d	20.0786	1
bs0274	f	cc	b	18.0808	0
bs0274	f	cc	f	20.0787	1
bs0274	f	cc	g	18.0807	0
bs0274	f	cc	g	20.0788	1
bs0274	f	cc	c	18.0807	0
bs0274	f	cc	g	20.0789	1
bs0274	f	cc	c	18.0805	0
bs0274	f	cc	b	20.0788	1
bs0274	f	cc	e	18.0806	0
bs0274	f	cc	f	20.0788	0
bs0274	f	cc	d	22.0768	1
bs0274	f	cc	g	20.0789	0
bs0274	f	cc	g	22.0769	0
bs0274	f	cc	h	24.0752	0
bs0274	f	cc	f	26.0735	1
bs0274	f	cc	a	24.0753	1
bs0274	f	cc	b	22.0769	1
bs0274	f	cc	f	20.0789	0
bs0274	f	cc	a	22.0772	1
bs0274	f	cc	d	20.0786	0
bs0274	f	cc	c	22.0769	1
bs0274	f	cc	b	20.0788	1
bs0274	f	cc	c	18.0806	0
bs0274	f	cc	a	20.0789	0
bs0274	f	cc	b	22.0769	1
bs0274	f	cc	e	20.0789	0
bs0274	f	cc	e	22.077	0
bs0274	f	cc	b	24.0753	1
bs0274	f	cc	c	22.0771	0
bs0274	f	cc	f	24.0752	0
bs0274	f	cc	b	26.0734	1
bs0274	f	cc	b	24.0753	0
bs0274	f	cc	f	26.0736	1
bs0274	f	cc	b	24.0752	1
bs0274	f	cc	f	22.077	1
bs0274	f	cc	b	20.079	1
bs0274	f	cc	f	18.0806	1
bs0274	f	cc	g	16.0547	0
bs0274	f	cc	g	18.0806	0
bs0274	f	cc	d	20.079	1
bs0274	f	cc	e	18.0803	0
bs0274	f	cc	f	20.0788	0
bs0274	f	cc	c	22.077	1
bs0274	f	cc	b	20.0789	1
bs0274	f	cc	b	18.0806	1
bs0274	f	cc	f	16.0548	0
bs0274	f	cc	d	18.0805	0
bs0274	f	cc	e	20.0788	0
bs0274	f	cc	g	22.0771	0
bs0274	f	cc	c	24.0754	0
bs0274	f	cc	f	26.0732	1
bs0274	f	cc	g	24.0751	0
bs0274	f	cc	c	26.0734	1
bs0274	f	cc	c	24.0753	1
bs1694	f	vl	e	20.0417	1
bs1694	f	vl	e	21.0526	0
bs1694	f	vl	e	20.0417	1
bs1694	f	vl	a	21.0527	0
bs1694	f	vl	g	20.0416	1
bs1694	f	vl	c	21.0528	0
bs1694	f	vl	e	20.0417	1
bs1694	f	vl	c	21.0527	1
bs1694	f	vl	a	22.05	0
bs1694	f	vl	e	21.0528	0
bs1694	f	vl	f	20.0418	1
bs1694	f	vl	d	21.0527	1
bs1694	f	vl	b	22.05	1
bs1694	f	vl	g	23.0472	0
bs1694	f	vl	f	22.05	0
bs1694	f	vl	g	21.0528	1
bs1694	f	vl	b	22.0499	0
bs1694	f	vl	a	21.0528	1
bs1694	f	vl	d	22.0499	0
bs1694	f	vl	b	21.0528	0
bs1694	f	vl	f	20.0417	1
bs1694	f	vl	e	21.0529	0
bs1694	f	vl	g	20.0416	1
bs1694	f	vl	f	21.0528	1
bs1694	f	vl	f	22.0499	1
bs1694	f	vl	f	23.0472	1
bs1694	f	vl	d	24.0444	1
bs1694	f	vl	f	25.0416	0
bs1694	f	vl	g	24.0445	0
bs1694	f	vl	g	23.0472	1
bs1694	f	vl	b	24.0445	0
bs1694	f	vl	e	23.0472	1
bs1694	f	vl	g	24.0446	1
bs1694	f	vl	g	25.0416	0
bs1694	f	vl	b	24.0445	1
bs1694	f	vl	g	25.0417	0
bs1694	f	vl	b	24.0444	1
bs1694	f	vl	h	25.0417	1
bs1694	f	vl	e	26.0528	0
bs1694	f	vl	e	25.0416	1
bs1694	f	vl	b	26.0528	1
bs1694	f	vl	c	27.0499	1
bs1694	f	vl	e	28.0473	0
bs1694	f	vl	h	27.05	0
bs1694	f	vl	g	26.0527	1
bs1694	f	vl	c	27.05	1
bs1694	f	vl	g	28.0473	1
bs1694	f	vl	c	29.0445	1
bs1694	f	vl	c	30.0416	1
bs1694	f	vl	f	31.0527	1
bs1694	f	vl	g	32.05	1
bs1694	f	vl	e	33.0473	0
bs1694	f	vl	f	32.0499	0
bs1694	f	vl	g	31.0528	1
bs1694	f	vl	b	32.05	1
bs1694	f	vl	c	33.0473	1
bs1694	f	vl	f	34.0446	0
bs1694	f	vl	g	33.0474	0
bs1694	f	vl	c	32.0501	1
bs1694	f	vl	c	33.0472	1
bs1694	f	vl	b	34.0445	0
bs1694	f	vl	f	33.0472	0
bs1694	f	vl	a	32.05	0
bs1694	f	vl	f	31.0529	0
bs1694	f	vl	d	30.0418	1
bs1694	f	vl	e	31.0528	1
bs1694	f	vl	g	32.0501	0
bs1694	f	vl	b	31.0528	0
bs1694	f	vl	d	30.0418	0
bs1694	f	vl	g	29.0445	0
bs1694	f	vl	f	28.0473	0
bs1694	f	vl	c	27.0501	0
bs1694	f	vl	g	26.0528	0
bs1694	f	vl	f	25.0417	0
bs1694	f	vl	f	24.0445	0
bs1694	f	vl	e	23.0473	0
bs1694	f	vl	b	22.0501	0
bs1694	f	vl	a	21.0526	0
bs1694	f	vl	b	20.0416	0
bs1694	f	vl	b	19.0442	0
bs1694	f	vl	b	18.0472	0
bs1694	f	vl	e	17.05	0
bs1694	f	vl	a	16.0528	0
bs1694	f	vl	d	15.0417	0
bs1694	f	vl	a	14.0443	0
ca1789	m	cc	d	20.0446	1
ca1789	m	cc	f	18.0458	1
ca1789	m	cc	c	16.0468	0
ca1789	m	cc	b	18.0457	1
ca1789	m	cc	c	16.0469	0
ca1789	m	cc	c	18.0457	0
ca1789	m	cc	h	20.0447	0
ca1789	m	cc	b	22.0435	0
ca1789	m	cc	g	24.0422	0
ca1789	m	cc	g	26.0412	0
ca1789	m	cc	c	28.0403	0
ca1789	m	cc	g	30.0393	0
ca1789	m	cc	a	32.038	0
ca1789	m	cc	f	34.037	0
ca1789	m	cc	e	36.0359	0
ca1789	m	cc	c	38.0348	0
ca1789	m	cc	d	40.0335	0
ca1789	m	cc	e	42.0325	0
ca1789	m	cc	c	44.0314	0
ca1789	m	cc	f	46.0305	0
ca1789	m	cc	b	48.0571	0
ca1789	m	cc	c	50.056	0
ca1789	m	cc	g	52.0549	0
ca1789	m	cc	d	54.0538	0
ca1789	m	cc	h	56.0528	0
ca1789	m	cc	c	58.0515	0
ca1789	m	cc	f	60.0506	0
ca1789	m	cc	e	62.0494	0
ca1789	m	cc	h	64.0484	0
ca1789	m	cc	e	66.0472	0
ca1789	m	cc	a	68.0463	0
ca1789	m	cc	h	70.0451	0
ca1789	m	cc	h	72.0439	0
ca1789	m	cc	b	74.0429	0
ca1789	m	cc	a	76.042	0
ca1789	m	cc	g	78.0406	0
ca1789	m	cc	c	80.0396	0
ca1789	m	cc	c	82.0385	0
ca1789	m	cc	b	84.0373	0
ca1789	m	cc	b	86.0362	0
ca1789	m	cc	g	88.0353	0
cd1041	f	vs	g	20.0497	1
cd1041	f	vs	g	21.0446	1
cd1041	f	vs	c	22.0533	0
cd1041	f	vs	h	21.0446	1
cd1041	f	vs	b	22.0534	1
cd1041	f	vs	a	23.0482	0
cd1041	f	vs	e	22.0533	1
cd1041	f	vs	g	23.0482	0
cd1041	f	vs	h	22.0533	0
cd1041	f	vs	f	21.0447	1
cd1041	f	vs	f	22.0532	1
cd1041	f	vs	g	23.0482	0
cd1041	f	vs	c	22.0534	0
cd1041	f	vs	e	21.0446	0
cd1041	f	vs	h	20.0498	1
cd1041	f	vs	h	21.0447	1
cd1041	f	vs	f	22.0534	0
cd1041	f	vs	a	21.0447	1
cd1041	f	vs	b	22.0533	0
cd1041	f	vs	b	21.0447	1
cd1041	f	vs	h	22.0532	0
cd1041	f	vs	c	21.0445	1
cd1041	f	vs	e	22.0534	1
cd1041	f	vs	h	23.0481	0
cd1041	f	vs	f	22.0533	0
cd1041	f	vs	d	21.0447	1
cd1041	f	vs	f	22.0533	0
cd1041	f	vs	b	21.0447	1
cd1041	f	vs	b	22.0533	0
cd1041	f	vs	d	21.0445	1
cd1041	f	vs	c	22.0534	1
cd1041	f	vs	c	23.0481	1
cd1041	f	vs	g	24.057	1
cd1041	f	vs	a	25.0519	0
cd1041	f	vs	e	24.0571	1
cd1041	f	vs	g	25.0519	0
cd1041	f	vs	f	24.0567	0
cd1041	f	vs	b	23.0483	1
cd1041	f	vs	g	24.0569	1
cd1041	f	vs	h	25.0518	0
cd1041	f	vs	e	24.0569	0
cd1041	f	vs	e	23.0482	1
cd1041	f	vs	g	24.057	0
cd1041	f	vs	h	23.0482	0
cd1041	f	vs	b	22.0531	1
cd1041	f	vs	e	23.0482	1
cd1041	f	vs	d	24.0569	1
cd1041	f	vs	b	25.0517	0
cd1041	f	vs	g	24.0568	1
cd1041	f	vs	d	25.0517	1
cd1041	f	vs	d	26.0467	1
cd1041	f	vs	f	27.0553	1
cd1041	f	vs	f	28.0503	0
cd1041	f	vs	c	27.0553	0
cd1041	f	vs	f	26.0466	0
cd1041	f	vs	e	25.0519	1
cd1041	f	vs	g	26.0467	0
cd1041	f	vs	a	25.0517	1
cd1041	f	vs	b	26.0467	0
cd1041	f	vs	c	25.0517	1
cd1041	f	vs	c	26.0467	0
cd1041	f	vs	b	25.0519	0
cd1041	f	vs	c	24.0569	1
cd1041	f	vs	f	25.0518	0
cd1041	f	vs	g	24.0568	0
cd1041	f	vs	a	23.0481	1
cd1041	f	vs	e	24.0569	1
cd1041	f	vs	g	25.0519	1
cd1041	f	vs	f	26.0468	0
cd1041	f	vs	f	25.0517	1
cd1041	f	vs	b	26.0467	1
cd1041	f	vs	g	27.0552	1
cd1041	f	vs	f	28.0504	1
cd1041	f	vs	a	29.0451	1
cg9526	f	vl	c	20.0417	1
cg9526	f	vl	c	21.0528	1
cg9526	f	vl	b	22.0499	1
cg9526	f	vl	d	23.0473	0
cg9526	f	vl	a	22.0489	1
cg9526	f	vl	a	23.0472	1
cg9526	f	vl	h	24.0444	0
cg9526	f	vl	c	23.0471	1
cg9526	f	vl	h	24.0443	0
cg9526	f	vl	f	23.0471	1
cg9526	f	vl	b	24.0444	0
cg9526	f	vl	h	23.0473	0
cg9526	f	vl	g	22.05	1
cg9526	f	vl	b	23.0472	0
cg9526	f	vl	f	22.05	1
cg9526	f	vl	h	23.0472	1
cg9526	f	vl	g	24.0444	1
cg9526	f	vl	e	25.0415	1
cg9526	f	vl	g	26.0528	0
cg9526	f	vl	a	25.0418	1
cg9526	f	vl	c	26.0527	1
cg9526	f	vl	c	27.05	1
cg9526	f	vl	c	28.0472	1
cg9526	f	vl	a	29.0445	0
cg9526	f	vl	b	28.0472	1
cg9526	f	vl	f	29.0444	1
cg9526	f	vl	g	30.0417	0
cg9526	f	vl	c	29.0445	1
cg9526	f	vl	h	30.0416	1
cg9526	f	vl	g	31.0529	1
cg9526	f	vl	c	32.0501	1
cg9526	f	vl	c	33.0474	1
cg9526	f	vl	h	34.0444	0
cg9526	f	vl	f	33.0473	1
cg9526	f	vl	c	34.0446	0
cg9526	f	vl	b	33.0473	0
cg9526	f	vl	c	32.05	0
cg9526	f	vl	f	31.0529	0
cg9526	f	vl	b	30.0415	1
cg9526	f	vl	f	31.0528	1
cg9526	f	vl	g	32.05	1
cg9526	f	vl	b	33.0473	1
cg9526	f	vl	b	34.0445	0
cg9526	f	vl	g	33.0472	0
cg9526	f	vl	h	32.0501	0
cg9526	f	vl	e	31.0528	0
cg9526	f	vl	b	30.0418	0
cg9526	f	vl	a	29.0444	0
cg9526	f	vl	f	28.0472	0
cg9526	f	vl	f	27.05	0
cg9526	f	vl	a	26.0528	0
cg9526	f	vl	f	25.0417	0
cg9526	f	vl	a	24.0445	0
cg9526	f	vl	b	23.0472	0
cg9526	f	vl	g	22.05	0
cg9526	f	vl	b	21.0528	0
cg9526	f	vl	h	20.0417	0
cg9526	f	vl	b	19.0444	0
cg9526	f	vl	g	18.0472	0
cg9526	f	vl	f	17.05	0
cg9526	f	vl	h	16.0526	0
cg9526	f	vl	h	15.0417	0
cg9526	f	vl	a	14.0445	0
cg9526	f	vl	g	13.0471	0
cg9526	f	vl	b	12.05	0
cg9526	f	vl	f	12.05	0
cg9526	f	vl	a	12.05	0
cg9526	f	vl	g	12.05	0
cg9526	f	vl	f	12.0498	0
cg9526	f	vl	c	12.05	0
cg9526	f	vl	a	12.0499	0
cg9526	f	vl	c	12.0497	0
cg9526	f	vl	f	12.05	0
cg9526	f	vl	d	12.0499	0
cg9526	f	vl	d	12.0499	0
cg9526	f	vl	h	12.05	0
cg9526	f	vl	b	12.0499	0
cg9526	f	vl	h	12.05	1
cg9526	f	vl	e	13.0471	0
cg9526	f	vl	f	12.05	1
cg9526	f	vl	f	13.0472	0
cg9526	f	vl	e	12.0499	1
cg9526	f	vl	c	13.0471	0
cg9526	f	vl	b	12.05	1
ch1298	f	vl	d	20.0497	1
ch1298	f	vl	a	21.0447	0
ch1298	f	vl	b	20.0497	0
ch1298	f	vl	h	19.0548	1
ch1298	f	vl	e	20.0497	0
ch1298	f	vl	g	19.0548	1
ch1298	f	vl	c	20.0498	0
ch1298	f	vl	g	19.0548	1
ch1298	f	vl	b	20.0498	1
ch1298	f	vl	f	21.0446	1
ch1298	f	vl	h	22.0532	1
ch1298	f	vl	h	23.0481	0
ch1298	f	vl	b	22.0533	0
ch1298	f	vl	c	21.0445	1
ch1298	f	vl	f	22.0532	0
ch1298	f	vl	f	21.0446	0
ch1298	f	vl	h	20.0497	1
ch1298	f	vl	e	21.0447	0
ch1298	f	vl	e	20.0497	0
ch1298	f	vl	b	19.0549	0
ch1298	f	vl	b	18.0461	1
ch1298	f	vl	a	19.0549	1
ch1298	f	vl	f	20.0498	0
ch1298	f	vl	b	19.0549	0
ch1298	f	vl	g	18.0462	0
ch1298	f	vl	f	17.0512	1
ch1298	f	vl	b	18.0461	1
ch1298	f	vl	c	19.0549	0
ch1298	f	vl	h	18.046	1
ch1298	f	vl	f	19.0547	1
ch1298	f	vl	f	20.0497	0
ch1298	f	vl	e	19.0548	0
ch1298	f	vl	f	18.0461	1
ch1298	f	vl	h	19.0548	1
ch1298	f	vl	c	20.0497	0
ch1298	f	vl	b	19.0549	1
ch1298	f	vl	h	20.0496	0
ch1298	f	vl	f	19.0548	0
ch1298	f	vl	d	18.0461	1
ch1298	f	vl	e	19.0548	1
ch1298	f	vl	h	20.0497	0
ch1298	f	vl	b	19.0547	1
ch1298	f	vl	e	20.0497	0
ch1298	f	vl	e	19.0548	0
ch1298	f	vl	a	18.0461	0
ch1298	f	vl	c	17.0513	1
ch1298	f	vl	a	18.046	1
ch1298	f	vl	b	19.055	0
ch1298	f	vl	g	18.0461	1
ch1298	f	vl	a	19.0549	1
ch1298	f	vl	c	20.0497	0
ch1298	f	vl	f	19.0548	1
ch1298	f	vl	g	20.0496	0
ch1298	f	vl	c	19.0548	1
ch1298	f	vl	f	20.0496	1
ch1298	f	vl	g	21.0447	1
ch1298	f	vl	c	22.0533	1
ch1298	f	vl	c	23.0482	0
ch1298	f	vl	f	22.0533	0
ch1298	f	vl	f	21.0447	0
ch1298	f	vl	c	20.0497	1
ch1298	f	vl	a	21.0445	1
ch1298	f	vl	f	22.0532	0
ch1298	f	vl	d	21.0447	1
ch1298	f	vl	a	22.0532	1
ch1298	f	vl	b	23.0482	1
ch1298	f	vl	b	24.0569	0
ch1298	f	vl	b	23.0481	0
ch1298	f	vl	f	22.0533	1
ch1298	f	vl	a	23.0482	1
ch1298	f	vl	c	24.0569	1
ch1298	f	vl	f	25.0518	0
ch1298	f	vl	e	24.0568	0
ch1298	f	vl	c	23.048	1
ch1298	f	vl	d	24.0569	1
ch1298	f	vl	h	25.0518	1
ch1298	f	vl	e	26.0467	1
ch1298	f	vl	a	27.0553	1
ch1298	f	vl	d	28.0504	0
ch1298	f	vl	c	27.0554	0
ch1298	f	vl	a	26.0467	1
ch1298	f	vl	c	27.0553	0
ch1298	f	vl	b	26.0467	1
ch1298	f	vl	f	27.0552	0
ch1298	f	vl	d	26.0467	1
ch1298	f	vl	b	27.0553	1
cn1800	f	vl	f	20.0489	0
cn1800	f	vl	h	19.0542	0
cn1800	f	vl	d	18.0453	0
cn1800	f	vl	g	17.0506	1
cn1800	f	vl	g	18.0455	1
cn1800	f	vl	f	19.0542	0
cn1800	f	vl	c	18.0455	1
cn1800	f	vl	a	19.0541	0
cn1800	f	vl	c	18.0455	1
cn1800	f	vl	c	19.0542	0
cn1800	f	vl	e	18.0454	1
cn1800	f	vl	g	19.0541	1
cn1800	f	vl	h	20.049	0
cn1800	f	vl	b	19.0541	1
cn1800	f	vl	c	20.0489	0
cn1800	f	vl	g	19.054	0
cn1800	f	vl	b	18.0455	0
cn1800	f	vl	b	17.0507	1
cn1800	f	vl	f	18.0453	1
cn1800	f	vl	g	19.0542	0
cn1800	f	vl	f	18.0455	1
cn1800	f	vl	h	19.0542	1
cn1800	f	vl	a	20.0489	0
cn1800	f	vl	a	19.054	1
cn1800	f	vl	h	20.049	1
cn1800	f	vl	a	21.0438	1
cn1800	f	vl	b	22.0525	0
cn1800	f	vl	g	21.0439	0
cn1800	f	vl	a	20.0489	1
cn1800	f	vl	a	21.0439	0
cn1800	f	vl	f	20.049	0
cn1800	f	vl	e	19.0541	1
cn1800	f	vl	a	20.0489	1
cn1800	f	vl	e	21.0438	0
cn1800	f	vl	h	20.049	0
cn1800	f	vl	e	19.0541	0
cn1800	f	vl	g	18.0455	1
cn1800	f	vl	g	19.0542	0
cn1800	f	vl	a	18.0454	1
cn1800	f	vl	f	19.0542	1
cn1800	f	vl	c	20.0489	1
cn1800	f	vl	b	21.0439	1
cn1800	f	vl	e	22.0524	0
cn1800	f	vl	c	21.0438	0
cn1800	f	vl	e	20.0489	0
cn1800	f	vl	c	19.0541	0
cn1800	f	vl	f	18.0455	0
cn1800	f	vl	f	17.0507	1
cn1800	f	vl	c	18.0455	0
cn1800	f	vl	b	17.0505	1
cn1800	f	vl	h	18.0455	1
cn1800	f	vl	h	19.0541	1
cn1800	f	vl	e	20.049	0
cn1800	f	vl	g	19.0541	1
cn1800	f	vl	e	20.049	1
cn1800	f	vl	a	21.0439	1
cn1800	f	vl	h	22.0524	0
cn1800	f	vl	g	21.0437	1
cn1800	f	vl	g	22.0525	1
cn1800	f	vl	b	23.0472	0
cn1800	f	vl	b	22.0525	1
cn1800	f	vl	f	23.0473	0
cn1800	f	vl	f	22.0524	0
cn1800	f	vl	b	21.0439	0
cn1800	f	vl	e	20.0488	1
cn1800	f	vl	g	21.0438	0
cn1800	f	vl	g	20.049	1
cn1800	f	vl	g	21.0438	1
cn1800	f	vl	g	22.0525	1
cn1800	f	vl	g	23.0471	1
cn1800	f	vl	e	24.0559	0
cn1800	f	vl	c	23.0474	0
cn1800	f	vl	d	22.0525	1
cn1800	f	vl	g	23.0473	0
cn1800	f	vl	g	22.0525	0
cn1800	f	vl	e	21.0438	1
cn1800	f	vl	c	22.0525	0
cn1800	f	vl	f	21.0438	1
cn1800	f	vl	f	22.0525	1
cn1800	f	vl	c	23.0473	1
cn1800	f	vl	c	24.0559	0
cn1800	f	vl	g	23.0473	0
cn1800	f	vl	c	22.0525	0
cn1800	f	vl	d	21.0439	0
cn1800	f	vl	f	20.049	1
cn1800	f	vl	e	21.0438	1
cn1800	f	vl	h	22.0525	0
cn1800	f	vl	f	21.0438	1
cn1800	f	vl	c	22.0524	0
cn1800	f	vl	b	21.0438	0
cn1800	f	vl	a	20.0489	1
cn1800	f	vl	b	21.0438	0
cn1800	f	vl	g	20.049	0
co0662	f	vl	e	20.0492	1
co0662	f	vl	a	21.0439	1
co0662	f	vl	g	22.0528	0
co0662	f	vl	f	21.0441	0
co0662	f	vl	e	20.049	1
co0662	f	vl	d	21.0442	0
co0662	f	vl	g	20.0491	1
co0662	f	vl	f	21.0441	0
co0662	f	vl	f	20.0492	0
co0662	f	vl	c	19.0543	1
co0662	f	vl	g	20.0492	0
co0662	f	vl	d	19.0543	0
co0662	f	vl	g	18.0458	0
co0662	f	vl	a	17.0507	1
co0662	f	vl	b	18.0457	0
co0662	f	vl	g	17.0508	0
co0662	f	vl	d	16.0561	1
co0662	f	vl	d	17.0508	0
co0662	f	vl	g	16.0561	0
co0662	f	vl	g	15.0471	1
co0662	f	vl	c	16.056	1
co0662	f	vl	b	17.0507	1
co0662	f	vl	c	18.0457	0
co0662	f	vl	g	17.0509	0
co0662	f	vl	g	16.0558	1
co0662	f	vl	b	17.0508	1
co0662	f	vl	c	18.0456	1
co0662	f	vl	b	19.0543	1
co0662	f	vl	g	20.049	0
co0662	f	vl	d	19.0543	0
co0662	f	vl	f	18.0457	1
co0662	f	vl	g	19.0543	0
co0662	f	vl	g	18.0456	0
co0662	f	vl	e	17.0508	0
co0662	f	vl	f	16.056	1
co0662	f	vl	a	17.0505	0
co0662	f	vl	f	16.056	1
co0662	f	vl	b	17.0508	1
co0662	f	vl	e	18.0458	1
co0662	f	vl	c	19.0544	1
co0662	f	vl	c	20.0493	0
co0662	f	vl	b	19.0544	0
co0662	f	vl	g	18.0457	0
co0662	f	vl	b	17.0509	1
co0662	f	vl	a	18.0457	1
co0662	f	vl	d	19.0544	0
co0662	f	vl	b	18.0457	0
co0662	f	vl	e	17.0507	1
co0662	f	vl	h	18.0457	0
co0662	f	vl	e	17.051	0
co0662	f	vl	b	16.0559	1
co0662	f	vl	d	17.0508	1
co0662	f	vl	g	18.0457	0
co0662	f	vl	b	17.051	0
co0662	f	vl	h	16.056	1
co0662	f	vl	g	17.0509	1
co0662	f	vl	g	18.0457	0
co0662	f	vl	f	17.0508	1
co0662	f	vl	f	18.0456	0
co0662	f	vl	a	17.0509	1
co0662	f	vl	b	18.0456	0
co0662	f	vl	c	17.0508	1
co0662	f	vl	c	18.0457	1
co0662	f	vl	b	19.0543	1
co0662	f	vl	f	20.0493	0
co0662	f	vl	e	19.0544	0
co0662	f	vl	e	18.0458	0
co0662	f	vl	a	17.0509	1
co0662	f	vl	b	18.0455	0
co0662	f	vl	f	17.0508	1
co0662	f	vl	b	18.0457	1
co0662	f	vl	c	19.0545	0
co0662	f	vl	e	18.0458	1
co0662	f	vl	g	19.0544	1
co0662	f	vl	f	20.0494	1
co0662	f	vl	f	21.0439	0
co0662	f	vl	h	20.0493	0
co0662	f	vl	g	19.0545	1
co0662	f	vl	b	20.0491	0
co0662	f	vl	f	19.0543	0
co0662	f	vl	b	18.0458	1
co0662	f	vl	f	19.0543	0
co0662	f	vl	f	18.0457	1
co0662	f	vl	b	19.0544	0
co0662	f	vl	e	18.0456	0
co0662	f	vl	f	17.0508	1
co0662	f	vl	e	18.0457	1
co0662	f	vl	b	19.0543	0
co0662	f	vl	h	18.0456	1
co0662	f	vl	e	19.0544	1
co0662	f	vl	g	20.0493	1
co0662	f	vl	f	21.044	0
co0662	f	vl	e	20.0493	0
co0662	f	vl	c	19.0543	1
co0662	f	vl	f	20.0492	0
co0662	f	vl	f	19.0543	1
co0662	f	vl	h	20.0492	1
co0662	f	vl	g	21.0441	1
cp0204	m	cc	g	20.0491	0
cp0204	m	cc	c	22.0387	1
cp0204	m	cc	g	20.049	1
cp0204	m	cc	g	18.0315	1
cp0204	m	cc	a	16.0421	0
cp0204	m	cc	e	18.0318	0
cp0204	m	cc	b	20.049	1
cp0204	m	cc	b	18.0317	1
cp0204	m	cc	e	16.0419	1
cp0204	m	cc	d	14.0248	0
cp0204	m	cc	f	16.0419	0
cp0204	m	cc	c	18.0317	0
cp0204	m	cc	e	20.0491	0
cp0204	m	cc	f	22.0388	1
cp0204	m	cc	g	20.0491	1
cp0204	m	cc	b	18.0317	0
cp0204	m	cc	b	20.049	0
cp0204	m	cc	b	22.0387	1
cp0204	m	cc	c	20.0491	0
cp0204	m	cc	h	22.0386	1
cp0204	m	cc	a	20.0493	1
cp0204	m	cc	g	18.0319	0
cp0204	m	cc	g	20.0493	0
cp0204	m	cc	f	22.0387	1
cp0204	m	cc	f	20.0491	0
cp0204	m	cc	g	22.0388	1
cp0204	m	cc	e	20.049	0
cp0204	m	cc	h	22.0389	1
cp0204	m	cc	b	20.0489	0
cp0204	m	cc	g	22.0388	0
cp0204	m	cc	a	24.0285	1
cp0204	m	cc	e	22.0385	0
cp0204	m	cc	f	24.0285	1
cp0204	m	cc	f	22.0389	1
cp0204	m	cc	g	20.0491	1
cp0204	m	cc	b	18.0318	1
cp0204	m	cc	g	16.042	0
cp0204	m	cc	g	18.0318	0
cp0204	m	cc	f	20.0489	0
cp0204	m	cc	e	22.0387	1
cp0204	m	cc	f	20.0492	0
cp0204	m	cc	f	22.0389	1
cp0204	m	cc	b	20.0491	1
cp0204	m	cc	h	18.0315	0
cp0204	m	cc	c	20.0491	1
cp0204	m	cc	b	18.0317	0
cp0204	m	cc	d	20.049	0
cp0204	m	cc	e	22.0387	1
cp0204	m	cc	b	20.0491	1
cp0204	m	cc	g	18.0317	0
cp0204	m	cc	h	20.049	1
cp0204	m	cc	g	18.0317	0
cp0204	m	cc	g	20.0492	1
cp0204	m	cc	h	18.0316	0
cp0204	m	cc	b	20.0491	1
cp0204	m	cc	b	18.0317	1
cp0204	m	cc	h	16.0421	0
cp0204	m	cc	f	18.0317	0
cp0204	m	cc	g	20.0491	0
cp0204	m	cc	h	22.0388	0
cp0204	m	cc	b	24.0285	1
cp0204	m	cc	b	22.0389	1
cp0204	m	cc	h	20.0491	0
cp0204	m	cc	d	22.0387	1
cp0204	m	cc	g	20.049	0
cp0204	m	cc	f	22.0388	1
cp0204	m	cc	b	20.049	1
cp0204	m	cc	b	18.0316	0
cp0204	m	cc	g	20.0489	1
cp0204	m	cc	c	18.0315	1
cp0204	m	cc	b	16.042	0
cp0204	m	cc	g	18.0316	0
cp0204	m	cc	e	20.0492	1
cp0204	m	cc	h	18.0318	0
cp0204	m	cc	g	20.049	0
cp0204	m	cc	c	22.0388	1
cp0204	m	cc	c	20.0491	0
cp0204	m	cc	h	22.0388	1
cp0204	m	cc	f	20.0492	0
cp0204	m	cc	a	22.0388	1
cp0204	m	cc	b	20.049	1
cp0204	m	cc	h	18.0317	0
cp0204	m	cc	g	20.0492	1
cp0204	m	cc	c	18.0318	0
cp0204	m	cc	h	20.0491	0
cp0204	m	cc	b	22.0387	1
cp0204	m	cc	e	20.049	1
cp0204	m	cc	g	18.0319	1
cp0204	m	cc	g	16.042	0
cw1403	f	vs	e	20.0417	1
cw1403	f	vs	h	21.0389	0
cw1403	f	vs	g	20.0417	0
cw1403	f	vs	b	20.0416	0
cw1403	f	vs	e	20.0416	1
cw1403	f	vs	g	21.0388	0
cw1403	f	vs	b	20.0416	1
cw1403	f	vs	e	21.0389	1
cw1403	f	vs	e	22.0361	1
cw1403	f	vs	h	23.0333	0
cw1403	f	vs	e	22.0362	0
cw1403	f	vs	c	21.0389	1
cw1403	f	vs	f	22.0363	0
cw1403	f	vs	e	21.039	0
cw1403	f	vs	b	20.0416	1
cw1403	f	vs	f	21.0389	0
cw1403	f	vs	b	20.0417	1
cw1403	f	vs	f	21.039	0
cw1403	f	vs	c	20.0417	1
cw1403	f	vs	e	21.0389	1
cw1403	f	vs	f	22.0362	0
cw1403	f	vs	a	21.0389	0
cw1403	f	vs	g	20.0417	0
cw1403	f	vs	a	20.0417	1
cw1403	f	vs	e	21.0389	0
cw1403	f	vs	a	20.0417	1
cw1403	f	vs	b	21.039	1
cw1403	f	vs	b	22.0361	0
cw1403	f	vs	d	21.0389	0
cw1403	f	vs	e	20.0417	1
cw1403	f	vs	c	21.0388	0
cw1403	f	vs	b	20.0416	0
cw1403	f	vs	a	20.0417	1
cw1403	f	vs	c	21.0389	0
cw1403	f	vs	f	20.0417	1
cw1403	f	vs	b	21.0388	0
cw1403	f	vs	g	20.0417	0
cw1403	f	vs	f	20.0417	0
cw1403	f	vs	h	20.0416	1
cw1403	f	vs	g	21.039	0
cw1403	f	vs	g	20.0417	1
cw1403	f	vs	d	21.039	0
cw1403	f	vs	a	20.0417	0
cw1403	f	vs	b	20.0416	1
cw1403	f	vs	c	21.039	0
cw1403	f	vs	g	20.0417	1
cw1403	f	vs	d	21.0389	1
cw1403	f	vs	c	22.0361	1
cw1403	f	vs	a	23.0333	0
cw1403	f	vs	h	22.036	0
cw1403	f	vs	f	21.039	1
cw1403	f	vs	c	22.0362	0
cw1403	f	vs	g	21.0389	0
cw1403	f	vs	f	20.0416	1
cw1403	f	vs	b	21.0388	1
cw1403	f	vs	b	22.0361	1
cw1403	f	vs	b	23.0333	1
cw1403	f	vs	c	24.0444	0
cw1403	f	vs	b	23.0333	0
cw1403	f	vs	e	22.0362	0
cw1403	f	vs	c	21.0389	1
cw1403	f	vs	f	22.0361	0
cw1403	f	vs	e	21.0388	1
cw1403	f	vs	b	22.0361	1
cw1403	f	vs	a	23.0333	0
cw1403	f	vs	g	22.0361	0
cw1403	f	vs	e	21.0389	1
cw1403	f	vs	b	22.0362	1
cw1403	f	vs	e	23.0333	0
cw1403	f	vs	b	22.0362	0
cw1403	f	vs	b	21.0389	1
cw1403	f	vs	f	22.0361	0
cw1403	f	vs	f	21.0389	1
cw1403	f	vs	b	22.0361	1
cw1403	f	vs	f	23.0334	0
cw1403	f	vs	d	22.0361	1
cw1403	f	vs	a	23.0334	0
cw1403	f	vs	c	22.0362	0
cw1403	f	vs	b	21.039	1
cw1403	f	vs	c	22.0361	0
cw1403	f	vs	a	21.0389	1
cw1403	f	vs	g	22.0362	0
cw1403	f	vs	f	21.0391	0
cw1403	f	vs	h	20.0417	0
cw1403	f	vs	c	20.0417	1
cw1403	f	vs	b	21.039	0
cw1403	f	vs	d	20.0417	0
cw1403	f	vs	a	20.0416	1
cw1403	f	vs	a	21.0389	1
cw1403	f	vs	d	22.0362	0
db1186	m	vl	b	20.0786	1
db1186	m	vl	e	21.0778	0
db1186	m	vl	a	20.0787	0
db1186	m	vl	b	19.0797	1
db1186	m	vl	e	20.0788	0
db1186	m	vl	c	19.0798	0
db1186	m	vl	f	18.0806	0
db1186	m	vl	d	17.0815	0
db1186	m	vl	g	16.0826	0
db1186	m	vl	g	15.0834	0
db1186	m	vl	f	14.084	1
db1186	m	vl	b	15.0834	0
db1186	m	vl	h	14.0843	0
db1186	m	vl	c	13.0713	0
db1186	m	vl	b	12.0722	0
db1186	m	vl	c	12.0721	1
db1186	m	vl	b	13.0714	0
db1186	m	vl	c	12.0724	0
db1186	m	vl	g	12.0722	0
db1186	m	vl	a	12.0726	0
db1186	m	vl	b	12.0722	1
db1186	m	vl	a	13.0712	0
db1186	m	vl	c	12.0722	0
db1186	m	vl	c	12.0722	0
db1186	m	vl	c	12.0723	0
db1186	m	vl	d	12.0724	0
db1186	m	vl	g	12.0724	0
db1186	m	vl	g	12.0725	0
db1186	m	vl	g	12.0721	0
db1186	m	vl	d	12.0721	0
db1186	m	vl	b	12.0722	0
db1186	m	vl	c	12.0722	0
db1186	m	vl	f	12.0724	0
db1186	m	vl	h	12.0722	0
db1186	m	vl	g	12.0724	0
db1186	m	vl	a	12.0722	0
db1186	m	vl	f	12.0722	0
db1186	m	vl	e	12.0723	0
db1186	m	vl	d	12.0722	0
db1186	m	vl	h	12.0724	0
db1186	m	vl	f	12.0721	0
db1186	m	vl	b	12.0721	0
db1186	m	vl	h	12.0723	0
db1186	m	vl	g	12.0723	0
db1186	m	vl	g	12.0725	0
db1186	m	vl	d	12.0721	0
db1186	m	vl	g	12.0724	0
db1186	m	vl	e	12.072	0
db1186	m	vl	g	12.0723	0
db1186	m	vl	f	12.0724	0
db1186	m	vl	f	12.0724	0
db1186	m	vl	g	12.0721	0
db1186	m	vl	e	12.0724	0
db1186	m	vl	d	12.0726	0
db1186	m	vl	b	12.0722	0
db1186	m	vl	b	12.0723	0
db1186	m	vl	g	12.0723	0
db1186	m	vl	f	12.0723	0
db1186	m	vl	a	12.0722	0
db1186	m	vl	f	12.0722	0
db1186	m	vl	d	12.0722	0
db1186	m	vl	c	12.0723	0
db1186	m	vl	c	12.0725	0
db1186	m	vl	e	12.0725	0
db1186	m	vl	c	12.0723	0
db1186	m	vl	d	12.0721	0
db1186	m	vl	h	12.0725	0
db1186	m	vl	c	12.0721	0
db1186	m	vl	e	12.0721	0
db1186	m	vl	b	12.0725	0
db1186	m	vl	c	12.0723	0
db1186	m	vl	c	12.0723	0
db1186	m	vl	a	12.0723	0
db1186	m	vl	h	12.0721	0
db1186	m	vl	b	12.0722	0
db1186	m	vl	a	12.0721	0
db1186	m	vl	b	12.0723	0
db1186	m	vl	f	12.0723	0
db1186	m	vl	g	12.0721	0
db1186	m	vl	e	12.0721	0
db1186	m	vl	e	12.0721	0
db1186	m	vl	f	12.0724	0
db1186	m	vl	f	12.0724	0
db1186	m	vl	f	12.0722	0
db1186	m	vl	c	12.0722	0
db1186	m	vl	c	12.0724	0
db1186	m	vl	h	12.0724	0
db1186	m	vl	b	12.0722	0
db1186	m	vl	f	12.0725	0
db1186	m	vl	e	12.0724	0
db1186	m	vl	b	12.0723	0
db1186	m	vl	g	12.0724	0
db1186	m	vl	f	12.0723	0
db1186	m	vl	f	12.0722	0
db1186	m	vl	c	12.0723	0
db1186	m	vl	f	12.0722	0
db1186	m	vl	a	12.0722	0
db1186	m	vl	h	12.0721	0
db1186	m	vl	b	12.0723	0
db1186	m	vl	h	12.0722	0
db1186	m	vl	g	12.072	0
db1186	m	vl	h	12.0725	0
db1186	m	vl	b	12.0722	0
db1186	m	vl	h	12.0725	0
db1186	m	vl	b	12.0721	0
db1186	m	vl	c	12.0722	0
db1186	m	vl	c	12.0723	0
db1186	m	vl	b	12.0724	0
dj1461	f	vs	e	20.0417	1
dj1461	f	vs	b	21.0387	0
dj1461	f	vs	d	20.0417	0
dj1461	f	vs	b	20.0416	0
dj1461	f	vs	e	20.0417	1
dj1461	f	vs	f	21.0388	1
dj1461	f	vs	h	22.0361	1
dj1461	f	vs	f	23.0334	0
dj1461	f	vs	g	22.0362	1
dj1461	f	vs	c	23.0334	0
dj1461	f	vs	c	22.0362	1
dj1461	f	vs	c	23.0335	0
dj1461	f	vs	d	22.0361	1
dj1461	f	vs	f	23.0334	0
dj1461	f	vs	d	22.0361	0
dj1461	f	vs	b	21.0389	0
dj1461	f	vs	f	20.0415	1
dj1461	f	vs	h	21.0389	0
dj1461	f	vs	b	20.0416	1
dj1461	f	vs	c	21.039	1
dj1461	f	vs	c	22.036	0
dj1461	f	vs	g	21.039	1
dj1461	f	vs	b	22.0361	1
dj1461	f	vs	a	23.0333	1
dj1461	f	vs	b	24.0445	1
dj1461	f	vs	b	25.0417	0
dj1461	f	vs	e	24.0443	1
dj1461	f	vs	e	25.0416	1
dj1461	f	vs	d	26.0389	1
dj1461	f	vs	f	27.036	1
dj1461	f	vs	b	28.0333	1
dj1461	f	vs	f	29.0443	1
dj1461	f	vs	b	30.0416	0
dj1461	f	vs	b	29.0445	0
dj1461	f	vs	b	28.0334	0
dj1461	f	vs	a	27.0361	1
dj1461	f	vs	b	28.0333	0
dj1461	f	vs	d	27.0361	1
dj1461	f	vs	e	28.0334	1
dj1461	f	vs	d	29.0445	0
dj1461	f	vs	g	28.0334	0
dj1461	f	vs	a	27.0361	0
dj1461	f	vs	f	26.0389	0
dj1461	f	vs	c	25.0417	1
dj1461	f	vs	g	26.0389	0
dj1461	f	vs	b	25.0417	0
dj1461	f	vs	a	24.0444	0
dj1461	f	vs	b	23.0333	0
dj1461	f	vs	b	22.036	0
dj1461	f	vs	c	21.0389	1
dj1461	f	vs	g	22.036	1
dj1461	f	vs	g	23.0335	1
dj1461	f	vs	g	24.0444	1
dj1461	f	vs	d	25.0416	0
dj1461	f	vs	e	24.0444	1
dj1461	f	vs	c	25.0416	1
dj1461	f	vs	g	26.0388	1
dj1461	f	vs	g	27.0361	0
dj1461	f	vs	h	26.0389	0
dj1461	f	vs	b	25.0418	0
dj1461	f	vs	h	24.0445	0
dj1461	f	vs	b	23.0334	1
dj1461	f	vs	e	24.0444	1
dj1461	f	vs	f	25.0417	1
dj1461	f	vs	b	26.039	1
dj1461	f	vs	d	27.0362	1
dj1461	f	vs	f	28.0333	0
dj1461	f	vs	g	27.0361	0
dj1461	f	vs	e	26.0389	1
dj1461	f	vs	b	27.0362	0
dj1461	f	vs	a	26.0389	0
dj1461	f	vs	e	25.0417	0
dj1461	f	vs	g	24.0444	0
dj1461	f	vs	g	23.0334	0
dj1461	f	vs	h	22.0361	1
dj1461	f	vs	f	23.0334	1
dj1461	f	vs	c	24.0446	1
dm1138	f	vl	c	20.0414	0
dm1138	f	vl	d	19.0445	1
dm1138	f	vl	h	20.0417	0
dm1138	f	vl	d	19.0445	0
dm1138	f	vl	a	18.0471	1
dm1138	f	vl	h	19.0444	1
dm1138	f	vl	e	20.0416	1
dm1138	f	vl	c	21.0528	1
dm1138	f	vl	g	22.05	0
dm1138	f	vl	h	21.0528	1
dm1138	f	vl	b	22.0498	1
dm1138	f	vl	b	23.0472	0
dm1138	f	vl	c	22.0499	1
dm1138	f	vl	c	23.0472	0
dm1138	f	vl	e	22.0498	0
dm1138	f	vl	g	21.0528	0
dm1138	f	vl	c	20.0416	1
dm1138	f	vl	b	21.0528	1
dm1138	f	vl	g	22.05	0
dm1138	f	vl	a	21.0528	1
dm1138	f	vl	f	22.05	0
dm1138	f	vl	f	21.0527	0
dm1138	f	vl	c	20.0417	0
dm1138	f	vl	f	19.0445	0
dm1138	f	vl	g	18.0473	1
dm1138	f	vl	g	19.0445	0
dm1138	f	vl	f	18.0471	1
dm1138	f	vl	g	19.0443	0
dm1138	f	vl	f	18.0471	0
dm1138	f	vl	h	17.0499	1
dm1138	f	vl	a	18.0471	1
dm1138	f	vl	h	19.0445	1
dm1138	f	vl	a	20.0417	1
dm1138	f	vl	e	21.0527	0
dm1138	f	vl	g	20.0416	0
dm1138	f	vl	a	19.0445	1
dm1138	f	vl	f	20.0416	1
dm1138	f	vl	d	21.0527	0
dm1138	f	vl	h	20.0417	1
dm1138	f	vl	b	21.0528	0
dm1138	f	vl	a	20.0417	1
dm1138	f	vl	h	21.0528	0
dm1138	f	vl	c	20.0418	1
dm1138	f	vl	c	21.0527	0
dm1138	f	vl	f	20.0417	1
dm1138	f	vl	c	21.0528	1
dm1138	f	vl	c	22.0499	1
dm1138	f	vl	c	23.0472	0
dm1138	f	vl	c	22.0499	0
dm1138	f	vl	g	21.0527	0
dm1138	f	vl	c	20.0416	1
dm1138	f	vl	g	21.0528	0
dm1138	f	vl	b	20.0417	1
dm1138	f	vl	b	21.0528	1
dm1138	f	vl	d	22.05	0
dm1138	f	vl	a	21.0528	0
dm1138	f	vl	e	20.0416	0
dm1138	f	vl	d	19.0445	1
dm1138	f	vl	f	20.0416	0
dm1138	f	vl	d	19.0445	0
dm1138	f	vl	g	18.047	1
dm1138	f	vl	b	19.0443	1
dm1138	f	vl	c	20.0416	0
dm1138	f	vl	h	19.0445	0
dm1138	f	vl	f	18.0471	0
dm1138	f	vl	b	17.05	1
dm1138	f	vl	f	18.0472	1
dm1138	f	vl	f	19.0445	1
dm1138	f	vl	g	20.0415	1
dm1138	f	vl	h	21.0526	1
dm1138	f	vl	d	22.05	1
dm1138	f	vl	c	23.0472	0
dm1138	f	vl	c	22.05	0
dm1138	f	vl	g	21.0528	1
dm1138	f	vl	c	22.0499	1
dm1138	f	vl	f	23.0472	1
dm1138	f	vl	h	24.0445	1
dm1138	f	vl	f	25.0417	0
dm1138	f	vl	a	24.0445	0
dm1138	f	vl	d	23.0472	1
dm1138	f	vl	h	24.0444	1
dm1138	f	vl	e	25.0417	1
dm1138	f	vl	g	26.0527	1
dm1138	f	vl	c	27.0499	1
dm1138	f	vl	a	28.0473	1
dm1138	f	vl	f	29.0445	1
dm1138	f	vl	e	30.0417	1
dm1138	f	vl	f	31.0529	1
do0949	m	cc	f	20.0828	1
do0949	m	cc	h	18.0565	0
do0949	m	cc	f	20.0823	1
do0949	m	cc	f	18.0563	1
do0949	m	cc	c	16.0575	0
do0949	m	cc	c	18.0564	0
do0949	m	cc	c	20.0825	1
do0949	m	cc	a	18.0564	1
do0949	m	cc	f	16.0578	0
do0949	m	cc	a	18.0702	0
do0949	m	cc	c	20.0825	0
do0949	m	cc	h	22.0812	0
do0949	m	cc	f	24.0799	1
do0949	m	cc	b	22.0809	0
do0949	m	cc	g	24.0795	0
do0949	m	cc	e	26.0782	1
do0949	m	cc	g	24.0796	0
do0949	m	cc	f	26.078	1
do0949	m	cc	f	24.0797	0
do0949	m	cc	h	26.0783	0
do0949	m	cc	g	28.0769	1
do0949	m	cc	c	26.0781	0
do0949	m	cc	c	28.0768	1
do0949	m	cc	c	26.0782	1
do0949	m	cc	g	24.0798	0
do0949	m	cc	f	26.0783	1
do0949	m	cc	b	24.0799	1
do0949	m	cc	g	22.0811	0
do0949	m	cc	g	24.0797	0
do0949	m	cc	e	26.0783	0
do0949	m	cc	b	28.0767	1
do0949	m	cc	f	26.0784	1
do0949	m	cc	a	24.0799	0
do0949	m	cc	c	26.0779	1
do0949	m	cc	h	24.08	0
do0949	m	cc	g	26.0782	0
do0949	m	cc	f	28.0766	1
do0949	m	cc	h	26.0782	0
do0949	m	cc	b	28.0769	1
do0949	m	cc	b	26.0782	0
do0949	m	cc	f	28.0767	1
do0949	m	cc	e	26.0782	1
do0949	m	cc	c	24.0798	0
do0949	m	cc	f	26.0783	1
do0949	m	cc	c	24.0797	0
do0949	m	cc	b	26.078	0
do0949	m	cc	b	28.0768	1
do0949	m	cc	a	26.0782	1
do0949	m	cc	f	24.0797	0
do0949	m	cc	e	26.0782	0
do0949	m	cc	f	28.0764	1
do0949	m	cc	e	26.0784	0
do0949	m	cc	g	28.0769	1
do0949	m	cc	b	26.0782	1
do0949	m	cc	e	24.0796	0
do0949	m	cc	g	26.0783	0
do0949	m	cc	c	28.0765	0
do0949	m	cc	g	30.0754	1
do0949	m	cc	d	28.0769	1
do0949	m	cc	e	26.078	1
do0949	m	cc	c	24.0796	0
do0949	m	cc	b	26.0782	1
do0949	m	cc	d	24.0796	0
do0949	m	cc	c	26.0782	0
do0949	m	cc	c	28.0768	1
do0949	m	cc	f	26.0778	0
do0949	m	cc	b	28.0768	1
do0949	m	cc	e	26.0781	0
do0949	m	cc	h	28.0767	1
do0949	m	cc	h	26.0781	0
do0949	m	cc	d	28.0767	0
do0949	m	cc	c	30.0752	1
do0949	m	cc	g	28.077	0
do0949	m	cc	g	30.0753	1
do0949	m	cc	e	28.0778	0
do0949	m	cc	g	30.0753	1
do0949	m	cc	c	28.0766	1
do0949	m	cc	g	26.0783	0
do0949	m	cc	f	28.0768	1
do0949	m	cc	f	26.0783	0
do0949	m	cc	f	28.0768	0
do0949	m	cc	h	30.0755	1
do0949	m	cc	b	28.0768	1
dp1179	m	cc	a	20.0455	0
dp1179	m	cc	f	22.0444	0
dp1179	m	cc	b	24.0435	0
dp1179	m	cc	b	26.0424	1
dp1179	m	cc	c	24.0435	0
dp1179	m	cc	c	26.0426	0
dp1179	m	cc	g	28.0415	0
dp1179	m	cc	g	30.0404	0
dp1179	m	cc	b	32.0395	1
dp1179	m	cc	g	30.0406	1
dp1179	m	cc	g	28.0416	0
dp1179	m	cc	h	30.0403	1
dp1179	m	cc	c	28.0414	1
dp1179	m	cc	f	26.0424	1
dp1179	m	cc	b	24.0434	1
dp1179	m	cc	e	22.0443	1
dp1179	m	cc	f	20.0454	1
dp1179	m	cc	d	18.0465	0
dp1179	m	cc	b	20.0455	0
dp1179	m	cc	c	22.0445	0
dp1179	m	cc	f	24.0435	0
dp1179	m	cc	g	26.0424	1
dp1179	m	cc	f	24.0434	1
dp1179	m	cc	b	22.0445	1
dp1179	m	cc	g	20.0455	1
dp1179	m	cc	f	18.0467	0
dp1179	m	cc	f	20.0453	0
dp1179	m	cc	f	22.0446	1
dp1179	m	cc	g	20.0456	1
dp1179	m	cc	f	18.0466	1
dp1179	m	cc	f	16.0475	0
dp1179	m	cc	g	18.0465	0
dp1179	m	cc	d	20.0455	0
dp1179	m	cc	h	22.0444	0
dp1179	m	cc	d	24.0435	1
dp1179	m	cc	f	22.0445	1
dp1179	m	cc	d	20.0456	1
dp1179	m	cc	b	18.0465	0
dp1179	m	cc	a	20.0454	1
dp1179	m	cc	b	18.0464	1
dp1179	m	cc	d	16.0476	0
dp1179	m	cc	e	18.0465	0
dp1179	m	cc	d	20.0455	0
dp1179	m	cc	g	22.0445	1
dp1179	m	cc	b	20.0404	0
dp1179	m	cc	d	22.0444	0
dp1179	m	cc	a	24.0436	1
dp1179	m	cc	e	22.0445	1
dp1179	m	cc	b	20.0453	1
dp1179	m	cc	d	18.0465	1
dp1179	m	cc	f	16.0473	0
dp1179	m	cc	g	18.0465	1
dp1179	m	cc	c	16.0476	1
dp1179	m	cc	b	14.0486	1
dp1179	m	cc	f	12.0495	1
dp1179	m	cc	a	12.0496	1
dp1179	m	cc	h	12.0495	1
dp1179	m	cc	d	12.0496	0
dp1179	m	cc	b	14.0485	0
dp1179	m	cc	a	16.0476	1
dp1179	m	cc	e	14.0485	0
dp1179	m	cc	d	16.0475	0
dp1179	m	cc	h	18.0464	1
dp1179	m	cc	f	16.0475	0
dp1179	m	cc	h	20.7761	1
dp1179	m	cc	g	16.0476	1
dp1179	m	cc	g	14.0494	1
dp1179	m	cc	b	12.0495	1
dp1179	m	cc	c	12.0494	0
dp1179	m	cc	f	14.0486	0
dp1179	m	cc	d	16.0476	1
dp1179	m	cc	e	14.0486	0
dp1179	m	cc	g	16.0476	0
dp1179	m	cc	g	18.0465	0
dp1179	m	cc	g	20.0456	0
dp1179	m	cc	g	22.0443	0
dp1179	m	cc	b	24.0435	0
dp1179	m	cc	c	26.0424	1
dp1179	m	cc	g	24.0435	0
dp1179	m	cc	d	26.0424	1
dp1179	m	cc	b	24.0435	1
dp1179	m	cc	b	22.0444	1
dp1179	m	cc	h	20.0455	0
dp1179	m	cc	f	22.0445	0
dp1179	m	cc	h	24.0434	1
dp1179	m	cc	d	22.0446	1
dp1179	m	cc	e	20.0455	1
dp1179	m	cc	b	18.0464	1
dp1179	m	cc	b	16.0476	1
dp1179	m	cc	g	14.0485	1
dp1179	m	cc	e	12.0494	0
dp1179	m	cc	a	14.0485	1
eb1714	f	cc	c	20.0418	0
eb1714	f	cc	f	22.0362	1
eb1714	f	cc	c	20.0417	1
eb1714	f	cc	b	18.0196	1
eb1714	f	cc	b	16.0249	0
eb1714	f	cc	g	18.0195	0
eb1714	f	cc	b	20.0417	1
eb1714	f	cc	e	18.0195	0
eb1714	f	cc	g	20.0417	1
eb1714	f	cc	b	18.0195	1
eb1714	f	cc	b	16.0251	0
eb1714	f	cc	g	18.0195	1
eb1714	f	cc	e	16.025	0
eb1714	f	cc	h	18.0195	1
eb1714	f	cc	h	16.0249	0
eb1714	f	cc	b	18.0194	1
eb1714	f	cc	b	16.0249	1
eb1714	f	cc	f	14.0305	0
eb1714	f	cc	c	16.0251	1
eb1714	f	cc	e	14.0306	0
eb1714	f	cc	f	16.025	1
eb1714	f	cc	b	14.0307	0
eb1714	f	cc	g	16.025	1
eb1714	f	cc	g	14.0307	0
eb1714	f	cc	d	16.025	1
eb1714	f	cc	e	14.0305	0
eb1714	f	cc	b	16.0249	1
eb1714	f	cc	e	14.0306	0
eb1714	f	cc	h	16.0249	1
eb1714	f	cc	b	14.0306	0
eb1714	f	cc	e	16.025	1
eb1714	f	cc	b	14.0305	0
eb1714	f	cc	g	16.025	1
eb1714	f	cc	e	14.0306	0
eb1714	f	cc	e	16.025	1
eb1714	f	cc	g	14.0305	1
eb1714	f	cc	h	12.0361	0
eb1714	f	cc	b	14.0305	1
eb1714	f	cc	g	12.036	0
eb1714	f	cc	b	14.0306	0
eb1714	f	cc	c	16.025	1
eb1714	f	cc	a	14.0306	0
eb1714	f	cc	c	16.0251	1
eb1714	f	cc	d	14.0308	1
eb1714	f	cc	f	12.0361	0
eb1714	f	cc	f	14.0305	0
eb1714	f	cc	f	16.0251	1
eb1714	f	cc	h	14.0305	0
eb1714	f	cc	c	16.025	1
eb1714	f	cc	g	14.0305	0
eb1714	f	cc	g	16.0251	1
eb1714	f	cc	d	14.0305	0
eb1714	f	cc	b	16.0251	1
eb1714	f	cc	c	14.0305	0
eb1714	f	cc	c	16.025	0
eb1714	f	cc	c	18.0195	1
eb1714	f	cc	g	16.0249	1
eb1714	f	cc	c	14.0306	0
eb1714	f	cc	c	16.0249	1
eb1714	f	cc	f	14.0306	0
eb1714	f	cc	e	16.0252	1
eb1714	f	cc	c	14.0305	0
eb1714	f	cc	f	16.0251	1
eb1714	f	cc	f	14.0305	1
eb1714	f	cc	d	12.036	0
eb1714	f	cc	g	14.0306	1
eb1714	f	cc	b	12.0362	0
eb1714	f	cc	f	14.0304	1
eb1714	f	cc	c	12.036	0
eb1714	f	cc	c	14.0306	0
eb1714	f	cc	c	16.0249	1
eb1714	f	cc	g	14.0306	1
eb1714	f	cc	f	12.0361	0
eb1714	f	cc	c	14.0305	0
eb1714	f	cc	a	16.0251	1
eb1714	f	cc	g	14.0306	0
eb1714	f	cc	f	16.0251	1
eb1714	f	cc	c	14.0307	0
eb1714	f	cc	b	16.0251	1
eb1714	f	cc	e	14.0305	1
eb1714	f	cc	g	12.036	0
eb1714	f	cc	d	14.0306	0
eb1714	f	cc	f	16.025	1
eb1714	f	cc	b	14.0306	0
eb1714	f	cc	c	16.0249	1
eb1714	f	cc	d	14.0307	0
eb1714	f	cc	c	16.0249	1
eb1714	f	cc	c	14.0305	1
eb1714	f	cc	g	12.0361	0
eb1714	f	cc	h	14.0306	1
eb1714	f	cc	a	12.0361	0
eb1714	f	cc	g	14.0305	1
eb1714	f	cc	a	12.0361	0
eb1714	f	cc	c	14.0307	1
eb1714	f	cc	d	12.0362	0
eb1714	f	cc	f	14.0305	0
eb1714	f	cc	b	16.025	0
eb1714	f	cc	e	18.0193	1
ec1202	f	vs	b	20.0496	0
ec1202	f	vs	a	20.0496	1
ec1202	f	vs	b	21.0445	1
ec1202	f	vs	b	22.0532	0
ec1202	f	vs	g	21.0446	0
ec1202	f	vs	a	20.0496	1
ec1202	f	vs	c	21.0444	1
ec1202	f	vs	c	22.0532	1
ec1202	f	vs	a	23.048	0
ec1202	f	vs	f	22.0532	1
ec1202	f	vs	g	23.0481	0
ec1202	f	vs	c	22.0532	0
ec1202	f	vs	b	21.0447	0
ec1202	f	vs	f	20.0496	0
ec1202	f	vs	g	20.0496	0
ec1202	f	vs	a	20.0497	1
ec1202	f	vs	a	21.0445	0
ec1202	f	vs	e	20.0497	0
ec1202	f	vs	f	20.0497	1
ec1202	f	vs	c	21.0446	0
ec1202	f	vs	h	20.0497	0
ec1202	f	vs	h	20.0497	0
ec1202	f	vs	a	20.0496	1
ec1202	f	vs	a	21.0444	0
ec1202	f	vs	g	20.0497	0
ec1202	f	vs	f	20.0497	1
ec1202	f	vs	g	21.0445	0
ec1202	f	vs	d	20.0497	1
ec1202	f	vs	g	21.0446	0
ec1202	f	vs	g	20.0496	0
ec1202	f	vs	h	20.0498	0
ec1202	f	vs	f	20.0498	0
ec1202	f	vs	g	20.0496	1
ec1202	f	vs	e	21.0445	1
ec1202	f	vs	a	22.0532	0
ec1202	f	vs	g	21.0444	0
ec1202	f	vs	b	20.0497	0
ec1202	f	vs	f	20.0498	0
ec1202	f	vs	b	20.0496	0
ec1202	f	vs	f	20.0497	1
ec1202	f	vs	f	21.0445	0
ec1202	f	vs	h	20.0496	1
ec1202	f	vs	a	21.0447	0
ec1202	f	vs	g	20.0496	0
ec1202	f	vs	d	20.0496	1
ec1202	f	vs	e	21.0446	0
ec1202	f	vs	c	20.0497	0
ec1202	f	vs	e	20.0497	0
ec1202	f	vs	c	20.0498	0
ec1202	f	vs	e	20.0497	1
ec1202	f	vs	g	21.0445	0
ec1202	f	vs	f	20.0496	1
ec1202	f	vs	f	21.0444	0
ec1202	f	vs	b	20.0496	1
ec1202	f	vs	c	21.0445	0
ec1202	f	vs	c	20.0497	0
ec1202	f	vs	f	20.0497	0
ec1202	f	vs	a	20.0497	0
ec1202	f	vs	f	20.0496	0
ec1202	f	vs	h	20.0497	0
ec1202	f	vs	c	20.0498	1
ec1202	f	vs	b	21.0446	0
ec1202	f	vs	c	20.0496	1
ec1202	f	vs	a	21.0445	0
ec1202	f	vs	c	20.0496	0
ec1202	f	vs	g	20.0496	0
ec1202	f	vs	f	20.0496	0
ec1202	f	vs	f	20.0496	1
ec1202	f	vs	c	21.0446	0
ec1202	f	vs	c	20.0497	0
ec1202	f	vs	h	20.0497	0
ec1202	f	vs	h	20.0495	0
ec1202	f	vs	f	20.0497	0
ec1202	f	vs	c	20.0497	1
ec1202	f	vs	c	21.0446	0
ec1202	f	vs	c	20.0496	1
ec1202	f	vs	c	21.0446	0
ec1202	f	vs	c	20.0497	0
ec1202	f	vs	e	20.0497	1
ec1202	f	vs	b	21.0446	0
ec1202	f	vs	h	20.0498	1
ec1202	f	vs	a	21.0446	0
ec1202	f	vs	f	20.0497	0
ec1202	f	vs	f	20.0496	0
ec1202	f	vs	h	20.0498	1
ec1202	f	vs	e	21.0444	1
ej1415	f	vl	f	20.0417	0
ej1415	f	vl	b	19.0444	1
ej1415	f	vl	c	20.0416	0
ej1415	f	vl	c	19.0445	1
ej1415	f	vl	c	20.0416	1
ej1415	f	vl	f	21.0528	0
ej1415	f	vl	g	20.0418	1
ej1415	f	vl	b	21.0527	1
ej1415	f	vl	c	22.0499	1
ej1415	f	vl	f	23.0473	0
ej1415	f	vl	b	22.0501	1
ej1415	f	vl	d	23.0473	1
ej1415	f	vl	b	24.0445	0
ej1415	f	vl	a	23.0472	1
ej1415	f	vl	f	24.0444	0
ej1415	f	vl	e	23.0473	0
ej1415	f	vl	c	22.05	1
ej1415	f	vl	f	23.0473	1
ej1415	f	vl	b	24.0445	1
ej1415	f	vl	h	25.0417	0
ej1415	f	vl	g	24.0445	0
ej1415	f	vl	f	23.0472	1
ej1415	f	vl	f	24.0445	0
ej1415	f	vl	a	23.0473	1
ej1415	f	vl	b	24.0443	0
ej1415	f	vl	c	23.0472	0
ej1415	f	vl	b	22.05	1
ej1415	f	vl	b	23.0473	0
ej1415	f	vl	g	22.0498	0
ej1415	f	vl	g	21.0528	1
ej1415	f	vl	b	22.0499	0
ej1415	f	vl	g	21.0528	0
ej1415	f	vl	a	20.0417	1
ej1415	f	vl	g	21.0528	1
ej1415	f	vl	e	22.0499	0
ej1415	f	vl	g	21.0528	1
ej1415	f	vl	f	22.0499	0
ej1415	f	vl	d	21.0528	0
ej1415	f	vl	f	20.0416	0
ej1415	f	vl	a	19.0444	0
ej1415	f	vl	h	18.0472	0
ej1415	f	vl	b	17.0499	1
ej1415	f	vl	a	18.0472	0
ej1415	f	vl	b	17.05	1
ej1415	f	vl	f	18.0471	0
ej1415	f	vl	g	17.0499	1
ej1415	f	vl	b	18.0471	1
ej1415	f	vl	e	19.0445	0
ej1415	f	vl	b	18.047	1
ej1415	f	vl	g	19.0444	1
ej1415	f	vl	f	20.0417	0
ej1415	f	vl	e	19.0444	0
ej1415	f	vl	g	18.0473	1
ej1415	f	vl	c	19.0445	1
ej1415	f	vl	c	20.0416	0
ej1415	f	vl	g	19.0444	1
ej1415	f	vl	g	20.0417	1
ej1415	f	vl	e	21.0528	0
ej1415	f	vl	f	20.0418	1
ej1415	f	vl	b	21.0527	0
ej1415	f	vl	b	20.0418	1
ej1415	f	vl	g	21.0528	0
ej1415	f	vl	b	20.0417	1
ej1415	f	vl	d	21.0528	0
ej1415	f	vl	c	20.0418	1
ej1415	f	vl	a	21.0527	0
ej1415	f	vl	c	20.0415	1
ej1415	f	vl	c	21.0527	1
ej1415	f	vl	c	22.05	0
ej1415	f	vl	b	21.0528	1
ej1415	f	vl	d	22.05	0
ej1415	f	vl	b	21.0526	1
ej1415	f	vl	b	22.0499	1
ej1415	f	vl	g	23.0472	0
ej1415	f	vl	d	22.05	1
ej1415	f	vl	b	23.0472	0
ej1415	f	vl	e	22.0501	0
ej1415	f	vl	g	21.0526	0
ej1415	f	vl	e	20.0416	1
ej1415	f	vl	e	21.0526	0
ej1415	f	vl	f	20.0417	0
ej1415	f	vl	h	19.0445	0
ej1415	f	vl	e	18.0473	1
ej1415	f	vl	h	19.0445	1
ej1415	f	vl	b	20.0417	0
ek0393	f	cc	f	20.0446	1
ek0393	f	cc	b	18.0458	0
ek0393	f	cc	d	20.0446	0
ek0393	f	cc	c	22.0435	1
ek0393	f	cc	b	20.0447	0
ek0393	f	cc	g	22.0434	1
ek0393	f	cc	f	20.0448	1
ek0393	f	cc	b	18.0457	0
ek0393	f	cc	b	20.0447	0
ek0393	f	cc	h	22.0437	1
ek0393	f	cc	g	20.0446	1
ek0393	f	cc	c	18.0458	0
ek0393	f	cc	f	20.0447	1
ek0393	f	cc	g	18.0457	1
ek0393	f	cc	a	16.0467	0
ek0393	f	cc	c	18.0458	0
ek0393	f	cc	c	20.0446	0
ek0393	f	cc	e	22.0434	1
ek0393	f	cc	c	20.0446	1
ek0393	f	cc	c	18.0456	0
ek0393	f	cc	e	20.0446	1
ek0393	f	cc	g	18.0457	0
ek0393	f	cc	b	20.0447	0
ek0393	f	cc	b	22.0435	0
ek0393	f	cc	d	24.0423	0
ek0393	f	cc	g	26.0413	1
ek0393	f	cc	f	24.0425	1
ek0393	f	cc	b	22.0436	0
ek0393	f	cc	h	24.0425	0
ek0393	f	cc	f	26.0415	1
ek0393	f	cc	d	24.0423	0
ek0393	f	cc	d	26.0415	1
ek0393	f	cc	h	24.0424	1
ek0393	f	cc	c	22.0435	1
ek0393	f	cc	g	20.0447	1
ek0393	f	cc	f	18.0458	1
ek0393	f	cc	b	16.0467	0
ek0393	f	cc	a	18.0458	0
ek0393	f	cc	b	20.0482	0
ek0393	f	cc	b	22.0463	1
ek0393	f	cc	a	20.0451	1
ek0393	f	cc	e	18.0485	1
ek0393	f	cc	d	16.0529	0
ek0393	f	cc	c	18.0457	1
ek0393	f	cc	h	16.0501	0
ek0393	f	cc	b	18.0458	1
ek0393	f	cc	d	16.0525	1
ek0393	f	cc	h	14.0491	0
ek0393	f	cc	d	16.0489	0
ek0393	f	cc	f	18.0491	1
ek0393	f	cc	e	16.047	0
ek0393	f	cc	d	18.0457	0
ek0393	f	cc	c	20.0459	1
ek0393	f	cc	d	18.0457	0
ek0393	f	cc	d	20.0489	1
ek0393	f	cc	b	18.0458	1
ek0393	f	cc	d	16.051	0
ek0393	f	cc	g	18.0493	0
ek0393	f	cc	f	20.0445	1
ek0393	f	cc	g	18.0481	1
ek0393	f	cc	g	16.0476	1
ek0393	f	cc	f	14.0496	0
ek0393	f	cc	c	16.0492	1
ek0393	f	cc	a	14.0481	1
ek0393	f	cc	g	12.0488	1
ek0393	f	cc	d	12.0522	1
ek0393	f	cc	h	12.0554	0
ek0393	f	cc	b	14.048	1
ek0393	f	cc	f	12.0515	0
ek0393	f	cc	f	14.0533	0
ek0393	f	cc	g	16.0469	0
ek0393	f	cc	f	18.0457	1
ek0393	f	cc	f	16.0467	1
ek0393	f	cc	a	14.054	1
ek0393	f	cc	g	12.0497	0
ek0393	f	cc	b	14.0479	1
ek0393	f	cc	e	12.0498	0
ek0393	f	cc	h	14.0479	0
ek0393	f	cc	g	16.0506	0
ek0393	f	cc	b	18.0458	0
ek0393	f	cc	b	20.0487	1
ek0393	f	cc	f	18.0459	0
ek0393	f	cc	f	20.0446	0
ek0393	f	cc	f	22.0465	1
ek0393	f	cc	c	20.0481	1
ek0393	f	cc	g	18.0476	0
ek0393	f	cc	c	20.0447	0
ek0393	f	cc	b	22.0444	1
ek0393	f	cc	g	20.0445	0
ek0393	f	cc	f	22.0471	0
em1203	f	cc	h	20.0417	0
em1203	f	cc	c	22.0363	0
em1203	f	cc	g	24.0305	1
em1203	f	cc	c	22.036	1
em1203	f	cc	e	20.0415	0
em1203	f	cc	g	22.0362	0
em1203	f	cc	d	24.0306	1
em1203	f	cc	f	22.0363	1
em1203	f	cc	f	20.0417	0
em1203	f	cc	a	22.0361	0
em1203	f	cc	b	24.0305	0
em1203	f	cc	c	26.025	0
em1203	f	cc	g	28.0195	0
em1203	f	cc	f	30.0417	0
em1203	f	cc	c	32.036	1
em1203	f	cc	h	30.0418	1
em1203	f	cc	b	28.0195	0
em1203	f	cc	g	30.0417	0
em1203	f	cc	f	32.036	0
em1203	f	cc	f	34.0306	0
em1203	f	cc	h	36.025	1
em1203	f	cc	c	34.0306	1
em1203	f	cc	c	32.0361	1
em1203	f	cc	f	30.0418	1
em1203	f	cc	c	28.0196	0
em1203	f	cc	b	30.0417	1
em1203	f	cc	a	28.0195	0
em1203	f	cc	f	30.0416	1
em1203	f	cc	f	28.0195	1
em1203	f	cc	e	26.0251	1
em1203	f	cc	b	24.0308	1
em1203	f	cc	b	22.0363	0
em1203	f	cc	c	24.0308	0
em1203	f	cc	c	26.0251	1
em1203	f	cc	g	24.0306	0
em1203	f	cc	d	26.0249	1
em1203	f	cc	c	24.0308	0
em1203	f	cc	a	26.0251	1
em1203	f	cc	f	24.0306	0
em1203	f	cc	h	26.0251	1
em1203	f	cc	c	24.0307	0
em1203	f	cc	e	26.0251	0
em1203	f	cc	g	28.0195	0
em1203	f	cc	h	30.0417	0
em1203	f	cc	c	32.0362	0
em1203	f	cc	g	34.0307	1
em1203	f	cc	b	32.0362	0
em1203	f	cc	g	34.0307	0
em1203	f	cc	c	36.0252	1
em1203	f	cc	b	34.0306	0
em1203	f	cc	d	36.0252	0
em1203	f	cc	g	38.0196	1
em1203	f	cc	d	36.0251	0
em1203	f	cc	f	38.0196	0
em1203	f	cc	c	40.0418	1
em1203	f	cc	c	38.0196	0
em1973	f	cc	b	20.0455	0
em1973	f	cc	c	22.0446	0
em1973	f	cc	g	24.0436	1
em1973	f	cc	a	22.0445	1
em1973	f	cc	f	20.0455	1
em1973	f	cc	c	18.0466	0
em1973	f	cc	f	20.0456	0
em1973	f	cc	c	22.0446	1
em1973	f	cc	g	20.0454	1
em1973	f	cc	c	18.0463	0
em1973	f	cc	f	20.0456	1
em1973	f	cc	b	22.5253	1
em1973	f	cc	a	16.0475	0
em1973	f	cc	b	18.0462	1
em1973	f	cc	b	16.0474	0
em1973	f	cc	h	18.0466	0
em1973	f	cc	d	20.0455	0
em1973	f	cc	f	22.0444	1
em1973	f	cc	e	20.0455	1
em1973	f	cc	f	18.0466	1
em1973	f	cc	g	16.0476	0
em1973	f	cc	g	18.0464	1
em1973	f	cc	g	16.0476	0
em1973	f	cc	c	18.0464	1
em1973	f	cc	g	16.0476	0
em1973	f	cc	g	18.0466	0
em1973	f	cc	c	20.0456	1
em1973	f	cc	c	18.0475	1
em1973	f	cc	c	16.0475	0
em1973	f	cc	f	18.0465	0
em1973	f	cc	b	20.0456	1
em1973	f	cc	c	18.0465	1
em1973	f	cc	g	16.0475	1
em1973	f	cc	b	14.0486	1
em1973	f	cc	e	12.0502	0
em1973	f	cc	b	14.0485	0
em1973	f	cc	a	16.0476	1
em1973	f	cc	f	14.0486	0
em1973	f	cc	g	16.0476	0
em1973	f	cc	f	18.0465	1
em1973	f	cc	b	16.0475	1
em1973	f	cc	g	14.0484	0
em1973	f	cc	g	16.0475	0
em1973	f	cc	h	18.0465	0
em1973	f	cc	c	20.0454	1
em1973	f	cc	g	18.0466	0
em1973	f	cc	f	20.0454	1
em1973	f	cc	f	18.0465	0
em1973	f	cc	f	20.0454	1
em1973	f	cc	b	18.0463	0
em1973	f	cc	h	20.0455	1
em1973	f	cc	b	18.0466	1
em1973	f	cc	f	16.0476	1
em1973	f	cc	c	14.0484	0
em1973	f	cc	g	16.0474	0
em1973	f	cc	f	18.0466	1
em1973	f	cc	g	16.0474	0
em1973	f	cc	e	18.0465	0
em1973	f	cc	a	20.0456	1
em1973	f	cc	d	18.0465	1
em1973	f	cc	b	16.0475	1
em1973	f	cc	b	14.0485	1
em1973	f	cc	a	12.0494	0
em1973	f	cc	e	14.0486	0
em1973	f	cc	f	16.0474	0
em1973	f	cc	e	18.0466	0
em1973	f	cc	d	20.1098	1
em1973	f	cc	b	18.0464	0
em1973	f	cc	a	20.0454	1
em1973	f	cc	h	18.0466	1
em1973	f	cc	g	16.0475	0
em1973	f	cc	f	18.0465	1
em1973	f	cc	f	16.0475	0
em1973	f	cc	c	18.0464	0
em1973	f	cc	b	20.0468	1
em1973	f	cc	c	18.0465	1
em1973	f	cc	e	16.0475	0
em1973	f	cc	b	18.0465	1
em1973	f	cc	c	16.0476	0
em1973	f	cc	h	18.0466	0
em1973	f	cc	g	22.2337	1
em1973	f	cc	b	18.0466	1
em1973	f	cc	c	16.0475	1
em1973	f	cc	b	14.0485	1
em1973	f	cc	b	12.0496	0
em1973	f	cc	d	14.0486	1
em1973	f	cc	g	12.0495	0
em1973	f	cc	d	14.0485	0
em1973	f	cc	c	16.0475	0
em1973	f	cc	d	18.0464	0
em1973	f	cc	d	20.0456	1
em1973	f	cc	g	18.0466	0
em1973	f	cc	g	20.0455	0
em1973	f	cc	e	23.2055	1
em1973	f	cc	f	20.0454	0
em1973	f	cc	e	22.0438	1
em1973	f	cc	c	20.0454	0
em1973	f	cc	a	22.0445	1
em1973	f	cc	a	20.0455	1
ep0961	f	vl	c	20.0417	1
ep0961	f	vl	a	21.0527	0
ep0961	f	vl	a	20.0418	1
ep0961	f	vl	f	21.0528	0
ep0961	f	vl	b	20.0417	1
ep0961	f	vl	h	21.0528	0
ep0961	f	vl	f	20.0416	0
ep0961	f	vl	f	19.0444	1
ep0961	f	vl	f	20.0417	1
ep0961	f	vl	f	21.0528	0
ep0961	f	vl	f	20.0417	1
ep0961	f	vl	g	21.0528	0
ep0961	f	vl	f	20.0416	1
ep0961	f	vl	a	21.0527	0
ep0961	f	vl	g	20.0415	1
ep0961	f	vl	d	21.0527	0
ep0961	f	vl	c	20.0418	0
ep0961	f	vl	d	19.0443	0
ep0961	f	vl	b	18.0473	1
ep0961	f	vl	c	19.0444	0
ep0961	f	vl	c	18.0472	0
ep0961	f	vl	f	17.0497	1
ep0961	f	vl	f	18.0472	0
ep0961	f	vl	c	17.05	0
ep0961	f	vl	b	16.0527	0
ep0961	f	vl	f	15.0417	1
ep0961	f	vl	e	16.0528	1
ep0961	f	vl	f	17.0498	1
ep0961	f	vl	b	18.0473	0
ep0961	f	vl	e	17.0501	1
ep0961	f	vl	f	18.0472	1
ep0961	f	vl	a	19.0444	0
ep0961	f	vl	e	18.0473	0
ep0961	f	vl	b	17.0499	1
ep0961	f	vl	f	18.0472	1
ep0961	f	vl	d	19.0444	0
ep0961	f	vl	b	18.0471	1
ep0961	f	vl	h	19.0445	1
ep0961	f	vl	b	20.0417	0
ep0961	f	vl	e	19.0445	1
ep0961	f	vl	b	20.0417	0
ep0961	f	vl	g	19.0443	1
ep0961	f	vl	b	20.0417	0
ep0961	f	vl	g	19.0444	0
ep0961	f	vl	d	18.0472	0
ep0961	f	vl	c	17.0499	0
ep0961	f	vl	c	16.0525	0
ep0961	f	vl	f	15.0415	1
ep0961	f	vl	c	16.0526	0
ep0961	f	vl	c	15.0416	0
ep0961	f	vl	f	14.0445	1
ep0961	f	vl	c	15.0416	0
ep0961	f	vl	c	14.0445	0
ep0961	f	vl	h	13.047	0
ep0961	f	vl	f	12.0499	1
ep0961	f	vl	a	13.0472	1
ep0961	f	vl	c	14.0443	0
ep0961	f	vl	a	13.0472	0
ep0961	f	vl	e	12.0499	1
ep0961	f	vl	f	13.0471	0
ep0961	f	vl	f	12.05	1
ep0961	f	vl	e	13.0472	0
ep0961	f	vl	b	12.05	0
ep0961	f	vl	e	12.0499	1
ep0961	f	vl	f	13.0473	1
ep0961	f	vl	c	14.0445	1
ep0961	f	vl	f	15.0417	1
ep0961	f	vl	g	16.0528	1
ep0961	f	vl	c	17.05	1
ep0961	f	vl	f	18.0471	1
ep0961	f	vl	f	19.0444	1
ep0961	f	vl	g	20.0417	1
ep0961	f	vl	b	21.0528	1
ep0961	f	vl	d	22.05	1
er1806	f	cc	f	20.0446	1
er1806	f	cc	c	18.0457	0
er1806	f	cc	b	20.0447	0
er1806	f	cc	b	22.0436	1
er1806	f	cc	e	20.0448	1
er1806	f	cc	c	18.0457	0
er1806	f	cc	g	20.0446	1
er1806	f	cc	h	18.0456	0
er1806	f	cc	d	20.0447	1
er1806	f	cc	f	18.0457	0
er1806	f	cc	f	20.0455	1
er1806	f	cc	g	18.0458	1
er1806	f	cc	b	16.0467	1
er1806	f	cc	f	14.048	0
er1806	f	cc	f	16.0467	0
er1806	f	cc	e	18.0455	0
er1806	f	cc	e	20.0447	0
er1806	f	cc	d	22.0435	0
er1806	f	cc	f	24.0424	1
er1806	f	cc	c	22.0435	0
er1806	f	cc	d	24.0424	1
er1806	f	cc	e	22.0435	1
er1806	f	cc	g	20.0446	0
er1806	f	cc	c	22.0435	1
er1806	f	cc	f	20.0447	0
er1806	f	cc	a	22.0434	1
er1806	f	cc	b	20.0448	0
er1806	f	cc	d	22.0435	1
er1806	f	cc	b	20.0446	0
er1806	f	cc	c	22.0436	1
er1806	f	cc	c	20.0445	1
er1806	f	cc	c	18.0458	0
er1806	f	cc	b	20.0447	0
er1806	f	cc	a	22.0436	0
er1806	f	cc	f	24.0426	1
er1806	f	cc	b	22.0434	0
er1806	f	cc	h	24.0423	0
er1806	f	cc	a	26.0414	1
er1806	f	cc	b	24.0423	1
er1806	f	cc	b	22.0436	0
er1806	f	cc	e	24.0424	0
er1806	f	cc	c	26.0414	0
er1806	f	cc	h	28.0403	1
er1806	f	cc	a	26.0412	0
er1806	f	cc	f	28.0403	1
er1806	f	cc	f	26.0415	0
er1806	f	cc	d	28.0402	1
er1806	f	cc	g	26.0412	0
er1806	f	cc	g	28.0403	1
er1806	f	cc	f	26.0414	1
er1806	f	cc	f	24.0424	1
er1806	f	cc	g	22.0435	0
er1806	f	cc	f	24.0423	1
er1806	f	cc	b	22.0435	0
er1806	f	cc	c	24.0424	1
er1806	f	cc	c	22.0436	1
er1806	f	cc	f	20.0448	0
er1806	f	cc	e	22.0435	1
er1806	f	cc	h	20.045	0
er1806	f	cc	b	22.0435	0
er1806	f	cc	h	24.0425	0
er1806	f	cc	g	26.0415	0
er1806	f	cc	f	28.0404	0
er1806	f	cc	f	30.0392	0
er1806	f	cc	b	32.0381	0
er1806	f	cc	b	34.0371	0
er1806	f	cc	e	36.0359	0
er1806	f	cc	g	38.0348	0
er1806	f	cc	c	40.0336	0
er1806	f	cc	f	42.0326	0
er1806	f	cc	c	44.0315	0
er1806	f	cc	e	46.0305	1
er1806	f	cc	b	44.0316	1
er1806	f	cc	d	42.0325	1
er1806	f	cc	g	40.0337	1
er1806	f	cc	c	38.0347	1
er1806	f	cc	c	36.0359	1
er1806	f	cc	g	34.0369	1
er1806	f	cc	g	32.0379	1
er1806	f	cc	c	30.039	1
er1806	f	cc	b	28.0403	1
et1115	f	vs	g	20.0492	1
et1115	f	vs	d	21.0438	1
et1115	f	vs	f	22.0527	0
et1115	f	vs	f	21.0439	1
et1115	f	vs	c	22.0527	1
et1115	f	vs	g	23.0476	1
et1115	f	vs	b	24.0563	1
et1115	f	vs	d	25.051	0
et1115	f	vs	f	24.0562	0
et1115	f	vs	c	23.0477	1
et1115	f	vs	d	24.0562	0
et1115	f	vs	d	23.0474	1
et1115	f	vs	b	24.0563	1
et1115	f	vs	c	25.0511	0
et1115	f	vs	g	24.0562	0
et1115	f	vs	f	23.0474	1
et1115	f	vs	c	24.0561	0
et1115	f	vs	a	23.0475	1
et1115	f	vs	a	24.0562	0
et1115	f	vs	f	23.0476	0
et1115	f	vs	c	22.0528	0
et1115	f	vs	c	21.044	1
et1115	f	vs	g	22.0527	1
et1115	f	vs	f	23.0475	1
et1115	f	vs	g	24.0561	1
et1115	f	vs	g	25.051	0
et1115	f	vs	f	24.0561	0
et1115	f	vs	b	23.0474	1
et1115	f	vs	c	24.0561	0
et1115	f	vs	g	23.0474	1
et1115	f	vs	c	24.0563	1
et1115	f	vs	a	25.0509	1
et1115	f	vs	c	26.0458	0
et1115	f	vs	f	25.0512	1
et1115	f	vs	h	26.0459	1
et1115	f	vs	d	27.0545	0
et1115	f	vs	f	26.0458	0
et1115	f	vs	a	25.051	0
et1115	f	vs	g	24.0561	0
et1115	f	vs	c	23.0476	1
et1115	f	vs	g	24.0563	0
et1115	f	vs	h	23.0473	1
et1115	f	vs	c	24.0561	0
et1115	f	vs	g	23.0477	1
et1115	f	vs	c	24.056	0
et1115	f	vs	a	23.0476	0
et1115	f	vs	g	22.0525	0
et1115	f	vs	f	21.0441	1
et1115	f	vs	f	22.0527	0
et1115	f	vs	b	21.0439	1
et1115	f	vs	d	22.0527	0
et1115	f	vs	c	21.0439	1
et1115	f	vs	c	22.0526	0
et1115	f	vs	c	21.044	0
et1115	f	vs	d	20.0491	0
et1115	f	vs	d	20.0491	1
et1115	f	vs	a	21.044	0
et1115	f	vs	b	20.0491	0
et1115	f	vs	h	20.0492	1
et1115	f	vs	f	21.044	0
et1115	f	vs	h	20.049	0
ew0289	f	vs	e	20.0491	0
ew0289	f	vs	f	20.0491	0
ew0289	f	vs	h	20.049	1
ew0289	f	vs	b	21.0439	1
ew0289	f	vs	c	22.0526	0
ew0289	f	vs	f	21.044	1
ew0289	f	vs	b	22.0527	1
ew0289	f	vs	b	23.0475	0
ew0289	f	vs	f	22.0527	1
ew0289	f	vs	f	23.0476	0
ew0289	f	vs	c	22.0526	1
ew0289	f	vs	e	23.0474	0
ew0289	f	vs	g	22.0526	1
ew0289	f	vs	g	23.0475	0
ew0289	f	vs	b	22.0526	0
ew0289	f	vs	g	21.0438	0
ew0289	f	vs	f	20.0491	1
ew0289	f	vs	d	21.0438	1
ew0289	f	vs	h	22.0526	0
ew0289	f	vs	c	21.0434	1
ew0289	f	vs	f	22.0527	0
ew0289	f	vs	f	21.044	1
ew0289	f	vs	h	22.0526	1
ew0289	f	vs	c	23.0475	0
ew0289	f	vs	b	22.0528	0
ew0289	f	vs	b	21.0441	1
ew0289	f	vs	c	22.0526	0
ew0289	f	vs	h	21.0441	1
ew0289	f	vs	a	22.0525	1
ew0289	f	vs	b	23.0474	0
ew0289	f	vs	h	22.0526	1
ew0289	f	vs	f	23.0473	0
ew0289	f	vs	h	22.0525	1
ew0289	f	vs	g	23.0474	0
ew0289	f	vs	g	22.0526	1
ew0289	f	vs	b	23.0474	0
ew0289	f	vs	f	22.0527	0
ew0289	f	vs	c	21.044	1
ew0289	f	vs	h	22.0527	0
ew0289	f	vs	c	21.0441	0
ew0289	f	vs	f	20.0492	1
ew0289	f	vs	b	21.0439	0
ew0289	f	vs	a	20.049	1
ew0289	f	vs	g	21.0441	0
ew0289	f	vs	a	20.049	1
ew0289	f	vs	h	21.044	0
ew0289	f	vs	f	20.0492	1
ew0289	f	vs	f	21.0439	0
ew0289	f	vs	e	20.0491	1
ew0289	f	vs	a	21.0439	1
ew0289	f	vs	b	22.0526	0
ew0289	f	vs	b	21.044	1
ew0289	f	vs	c	22.0525	1
ew0289	f	vs	f	23.0473	1
ew0289	f	vs	a	24.0561	0
ew0289	f	vs	h	23.0476	0
ew0289	f	vs	c	22.0527	1
ew0289	f	vs	g	23.0476	1
ew0289	f	vs	h	24.0562	1
ew0289	f	vs	b	25.0509	0
ew0289	f	vs	c	24.0564	1
ew0289	f	vs	c	25.0509	0
ew0289	f	vs	h	24.056	1
ew0289	f	vs	b	25.051	0
ew0289	f	vs	c	24.0562	0
ew0289	f	vs	c	23.0475	0
ew0289	f	vs	c	22.0525	0
ew0289	f	vs	b	21.0441	0
ew0289	f	vs	b	20.0491	0
ew0289	f	vs	f	20.0491	1
ew0289	f	vs	h	21.0439	0
ew0289	f	vs	c	20.049	1
ew0289	f	vs	c	21.0441	0
ew0289	f	vs	c	20.0491	0
ew0289	f	vs	b	20.0491	1
ew0289	f	vs	c	21.044	0
ew0289	f	vs	b	20.0491	1
ew0289	f	vs	f	21.044	1
ew0289	f	vs	a	22.0526	0
ew0289	f	vs	c	21.0439	0
ew0289	f	vs	f	20.0492	1
ew0289	f	vs	g	21.0438	0
ew1456	f	vs	g	20.0417	0
ew1456	f	vs	g	20.0417	1
ew1456	f	vs	a	21.0388	1
ew1456	f	vs	c	22.0362	0
ew1456	f	vs	f	21.0389	0
ew1456	f	vs	a	20.0416	1
ew1456	f	vs	g	21.0388	1
ew1456	f	vs	c	22.0362	0
ew1456	f	vs	h	21.0388	0
ew1456	f	vs	b	20.0416	1
ew1456	f	vs	g	21.0388	0
ew1456	f	vs	h	20.0416	0
ew1456	f	vs	a	20.0417	1
ew1456	f	vs	d	21.0388	1
ew1456	f	vs	g	22.0363	0
ew1456	f	vs	f	21.0389	0
ew1456	f	vs	h	20.0418	1
ew1456	f	vs	b	21.0388	1
ew1456	f	vs	c	22.0361	1
ew1456	f	vs	f	23.0333	0
ew1456	f	vs	g	22.0362	0
ew1456	f	vs	e	21.0389	0
ew1456	f	vs	f	20.0416	1
ew1456	f	vs	f	21.0389	0
ew1456	f	vs	b	20.0417	1
ew1456	f	vs	e	21.0389	0
ew1456	f	vs	d	20.0417	0
ew1456	f	vs	c	20.0417	1
ew1456	f	vs	a	21.039	1
ew1456	f	vs	g	22.0361	0
ew1456	f	vs	a	21.0389	1
ew1456	f	vs	d	22.0361	1
ew1456	f	vs	c	23.0334	0
ew1456	f	vs	e	22.036	1
ew1456	f	vs	g	23.0334	0
ew1456	f	vs	e	22.036	1
ew1456	f	vs	f	23.0334	0
ew1456	f	vs	g	22.0362	0
ew1456	f	vs	b	21.0389	0
ew1456	f	vs	a	20.0417	0
ew1456	f	vs	c	20.0417	1
ew1456	f	vs	e	21.0389	1
ew1456	f	vs	f	22.0361	1
ew1456	f	vs	d	23.0333	0
ew1456	f	vs	g	22.0361	1
ew1456	f	vs	h	23.0334	1
ew1456	f	vs	b	24.0444	1
ew1456	f	vs	h	25.0417	0
ew1456	f	vs	a	24.0444	0
ew1456	f	vs	f	23.0333	0
ew1456	f	vs	e	22.0362	1
ew1456	f	vs	h	23.0335	0
ew1456	f	vs	b	22.036	0
ew1456	f	vs	g	21.039	1
ew1456	f	vs	d	22.0361	1
ew1456	f	vs	e	23.0334	1
ew1456	f	vs	e	24.0444	1
ew1456	f	vs	e	25.0416	1
ew1456	f	vs	b	26.0388	1
ew1456	f	vs	h	27.0361	0
ew1456	f	vs	c	26.0389	1
ew1456	f	vs	g	27.0362	1
ew1456	f	vs	g	28.0334	1
ew1456	f	vs	b	29.0447	0
ew1456	f	vs	g	28.0334	0
ew1456	f	vs	h	27.0363	0
ew1456	f	vs	h	26.0389	0
ew1456	f	vs	c	25.0418	1
ew1456	f	vs	f	26.0389	1
ew1456	f	vs	b	27.0361	0
ew1456	f	vs	c	26.039	1
ew1456	f	vs	g	27.0363	1
ew1456	f	vs	d	28.0334	1
ew1456	f	vs	f	29.0444	1
fb0733	f	cc	b	20.0511	0
fb0733	f	cc	g	22.0411	1
fb0733	f	cc	c	20.0511	0
fb0733	f	cc	f	22.0411	1
fb0733	f	cc	g	20.0514	0
fb0733	f	cc	c	22.0412	1
fb0733	f	cc	b	20.0511	1
fb0733	f	cc	e	18.0337	0
fb0733	f	cc	h	20.0514	0
fb0733	f	cc	a	22.0411	0
fb0733	f	cc	f	24.0309	0
fb0733	f	cc	c	26.0486	0
fb0733	f	cc	a	28.0385	1
fb0733	f	cc	a	26.0485	0
fb0733	f	cc	b	28.0385	1
fb0733	f	cc	g	26.0486	1
fb0733	f	cc	b	24.031	0
fb0733	f	cc	g	26.0487	1
fb0733	f	cc	g	24.0312	0
fb0733	f	cc	h	26.0487	1
fb0733	f	cc	b	24.0312	1
fb0733	f	cc	c	22.0411	1
fb0733	f	cc	f	20.0512	0
fb0733	f	cc	g	22.041	0
fb0733	f	cc	b	24.0311	1
fb0733	f	cc	e	22.0411	0
fb0733	f	cc	d	24.031	0
fb0733	f	cc	f	26.0485	0
fb0733	f	cc	f	28.0384	0
fb0733	f	cc	g	30.0285	0
fb0733	f	cc	f	32.0461	1
fb0733	f	cc	e	30.0284	1
fb0733	f	cc	a	28.0386	1
fb0733	f	cc	f	26.0486	1
fb0733	f	cc	b	24.0312	1
fb0733	f	cc	f	22.0411	1
fb0733	f	cc	a	20.0512	1
fb0733	f	cc	f	18.0336	0
fb0733	f	cc	e	20.0511	1
fb0733	f	cc	g	18.0336	1
fb0733	f	cc	a	16.0437	1
fb0733	f	cc	b	14.0262	1
fb0733	f	cc	e	12.0363	0
fb0733	f	cc	f	14.0261	1
fb0733	f	cc	c	12.0363	0
fb0733	f	cc	f	14.026	0
fb0733	f	cc	d	16.0436	0
fb0733	f	cc	f	18.0337	0
fb0733	f	cc	e	20.0515	0
fb0733	f	cc	g	22.0411	1
fb0733	f	cc	f	20.0511	0
fb0733	f	cc	g	22.0411	1
fb0733	f	cc	g	20.0512	1
fb0733	f	cc	g	18.0337	1
fb0733	f	cc	e	16.0438	0
fb0733	f	cc	f	18.0335	1
fb0733	f	cc	c	16.0436	1
fb0733	f	cc	c	14.0262	0
fb0733	f	cc	b	16.0437	0
fb0733	f	cc	b	18.0336	0
fb0733	f	cc	d	20.0513	0
fb0733	f	cc	d	22.0411	0
fb0733	f	cc	b	24.0312	1
fb0733	f	cc	c	22.0412	1
fb0733	f	cc	h	20.0513	0
fb0733	f	cc	b	22.0412	1
fb0733	f	cc	b	20.0513	1
fb0733	f	cc	b	18.0336	1
fb0733	f	cc	g	16.0436	0
fb0733	f	cc	c	18.0337	1
fb0733	f	cc	c	16.0438	0
fb0733	f	cc	f	18.0336	1
fb0733	f	cc	f	16.0437	0
fb0733	f	cc	c	18.0335	0
fb0733	f	cc	b	20.0513	0
fb0733	f	cc	f	22.0412	0
fb0733	f	cc	g	24.0308	1
fb0733	f	cc	a	22.0411	1
fb0733	f	cc	c	20.0513	0
fb0733	f	cc	d	22.0411	0
fb0733	f	cc	g	24.031	0
fb0733	f	cc	b	26.0487	0
fb0733	f	cc	g	28.0385	1
fb0733	f	cc	b	26.0486	1
fb0733	f	cc	b	24.0311	1
fb0733	f	cc	e	22.0413	1
fb0733	f	cc	b	20.0512	1
fd1672	f	cc	c	20.0456	0
fd1672	f	cc	f	22.0446	1
fd1672	f	cc	e	20.0456	0
fd1672	f	cc	b	22.0446	0
fd1672	f	cc	f	24.0436	1
fd1672	f	cc	d	22.0447	1
fd1672	f	cc	h	20.0456	0
fd1672	f	cc	f	22.0455	0
fd1672	f	cc	g	24.0436	0
fd1672	f	cc	g	26.0426	1
fd1672	f	cc	g	24.0437	0
fd1672	f	cc	f	26.0426	1
fd1672	f	cc	b	24.1545	0
fd1672	f	cc	b	26.0427	1
fd1672	f	cc	d	26.237	1
fd1672	f	cc	b	24.3323	1
fd1672	f	cc	c	20.0395	1
fd1672	f	cc	a	18.0512	0
fd1672	f	cc	f	20.0462	0
fd1672	f	cc	f	22.0446	0
fd1672	f	cc	e	24.0436	1
fd1672	f	cc	h	22.0446	0
fd1672	f	cc	g	24.0436	1
fd1672	f	cc	g	22.0446	1
fd1672	f	cc	c	20.0456	0
fd1672	f	cc	c	22.0446	0
fd1672	f	cc	g	24.0435	0
fd1672	f	cc	f	26.0427	1
fd1672	f	cc	f	24.0435	1
fd1672	f	cc	f	22.0447	0
fd1672	f	cc	b	24.0437	1
fd1672	f	cc	c	22.0445	1
fd1672	f	cc	f	20.0456	0
fd1672	f	cc	c	22.0445	1
fd1672	f	cc	f	20.0457	0
fd1672	f	cc	g	22.0446	1
fd1672	f	cc	d	20.0457	0
fd1672	f	cc	f	22.0446	0
fd1672	f	cc	c	24.0436	0
fd1672	f	cc	g	26.0427	1
fd1672	f	cc	f	24.0436	0
fd1672	f	cc	b	26.4539	0
fd1672	f	cc	g	28.0417	1
fd1672	f	cc	h	26.0425	1
fd1672	f	cc	c	24.0436	1
fd1672	f	cc	g	22.0445	0
fd1672	f	cc	f	24.0437	1
fd1672	f	cc	h	22.0445	0
fd1672	f	cc	b	24.0436	1
fd1672	f	cc	h	22.0446	0
fd1672	f	cc	e	24.0384	0
fd1672	f	cc	c	26.0437	1
fd1672	f	cc	b	24.0435	1
fd1672	f	cc	f	22.0445	1
fd1672	f	cc	a	20.0455	0
fd1672	f	cc	c	22.0446	1
fd1672	f	cc	g	20.0456	0
fd1672	f	cc	b	22.0446	1
fd1672	f	cc	f	20.0461	0
fd1672	f	cc	f	22.0447	1
fd1672	f	cc	a	20.0457	0
fd1672	f	cc	c	22.0445	0
fd1672	f	cc	c	24.0437	1
fd1672	f	cc	c	22.0446	0
fd1672	f	cc	f	24.0436	1
fd1672	f	cc	b	22.0446	1
fd1672	f	cc	c	20.0457	0
fd1672	f	cc	g	22.0446	1
fd1672	f	cc	f	20.0457	1
fd1672	f	cc	f	18.0466	0
fd1672	f	cc	g	20.0456	0
fd1672	f	cc	h	22.0445	0
fd1672	f	cc	d	24.0436	1
fd1672	f	cc	g	22.0446	0
fd1672	f	cc	g	24.0436	0
fd1672	f	cc	c	26.0426	1
fw1072	f	vs	c	20.0417	0
fw1072	f	vs	b	20.0417	0
fw1072	f	vs	a	20.0417	1
fw1072	f	vs	d	21.0388	0
fw1072	f	vs	h	20.0417	0
fw1072	f	vs	c	20.0415	1
fw1072	f	vs	d	21.039	0
fw1072	f	vs	h	20.0416	0
fw1072	f	vs	e	20.0417	0
fw1072	f	vs	a	20.0416	0
fw1072	f	vs	c	20.0417	0
fw1072	f	vs	b	20.0417	0
fw1072	f	vs	g	20.0417	0
fw1072	f	vs	b	20.0415	0
fw1072	f	vs	c	20.0416	0
fw1072	f	vs	f	20.0417	0
fw1072	f	vs	e	20.0416	0
fw1072	f	vs	d	20.0416	1
fw1072	f	vs	g	21.0389	0
fw1072	f	vs	f	20.0418	0
fw1072	f	vs	a	20.0416	0
fw1072	f	vs	h	20.0415	0
fw1072	f	vs	c	20.0416	0
fw1072	f	vs	d	20.0418	0
fw1072	f	vs	a	20.0417	0
fw1072	f	vs	b	20.0418	0
fw1072	f	vs	c	20.0416	0
fw1072	f	vs	a	20.0418	0
fw1072	f	vs	c	20.0417	0
fw1072	f	vs	a	20.0417	0
fw1072	f	vs	f	20.0417	0
fw1072	f	vs	c	20.0416	0
fw1072	f	vs	g	20.0416	0
fw1072	f	vs	a	20.0416	0
fw1072	f	vs	a	20.0416	0
fw1072	f	vs	d	20.0417	0
fw1072	f	vs	d	20.0416	0
fw1072	f	vs	c	20.0417	0
fw1072	f	vs	e	20.0416	1
fw1072	f	vs	b	21.0388	0
gh0928	f	vs	f	20.0549	0
gh0928	f	vs	b	20.055	0
gh0928	f	vs	c	20.0547	1
gh0928	f	vs	a	21.0637	1
gh0928	f	vs	c	22.059	1
gh0928	f	vs	h	23.0541	0
gh0928	f	vs	g	22.0587	0
gh0928	f	vs	c	21.064	1
gh0928	f	vs	h	22.0589	1
gh0928	f	vs	c	23.054	1
gh0928	f	vs	g	24.0631	0
gh0928	f	vs	c	23.054	0
gh0928	f	vs	c	22.0588	0
gh0928	f	vs	c	21.0638	1
gh0928	f	vs	f	22.0587	1
gh0928	f	vs	b	23.054	1
gh0928	f	vs	b	24.0565	0
gh0928	f	vs	f	23.0541	1
gh0928	f	vs	c	24.0631	0
gh0928	f	vs	a	23.0541	1
gh0928	f	vs	f	24.0631	1
gh0928	f	vs	g	25.0584	0
gh0928	f	vs	f	24.0632	1
gh0928	f	vs	b	25.058	1
gh0928	f	vs	c	26.0673	1
gh0928	f	vs	c	27.0621	0
gh0928	f	vs	h	26.0658	1
gh0928	f	vs	d	27.0623	0
gh0928	f	vs	c	26.0671	0
gh0928	f	vs	c	25.0583	1
gh0928	f	vs	d	26.0657	0
gh0928	f	vs	c	25.0581	1
gh0928	f	vs	f	26.0657	1
gh0928	f	vs	a	27.0621	1
gh0928	f	vs	f	28.0574	0
gh0928	f	vs	g	27.0622	0
gh0928	f	vs	c	26.0672	0
gh0928	f	vs	d	25.0581	1
gh0928	f	vs	g	26.0683	0
gh0928	f	vs	e	25.0581	1
gh0928	f	vs	c	26.0671	0
gh0928	f	vs	e	25.058	1
gh0928	f	vs	b	26.0671	0
gh0928	f	vs	c	25.0582	1
gh0928	f	vs	h	26.0673	0
gh0928	f	vs	h	25.0584	0
gh0928	f	vs	c	24.063	0
gh0928	f	vs	h	23.0539	0
gh0928	f	vs	f	22.0588	0
gh0928	f	vs	e	21.0637	0
gh0928	f	vs	d	20.0548	0
gh0928	f	vs	f	20.0547	1
gh0928	f	vs	f	21.0637	0
gh0928	f	vs	f	20.0549	0
gh0928	f	vs	c	20.0572	1
gh0928	f	vs	h	21.0639	1
gh0928	f	vs	d	22.0588	0
gh0928	f	vs	f	21.0638	1
gl1463	m	vs	e	20.0549	1
gl1463	m	vs	b	21.0636	1
gl1463	m	vs	d	22.0591	0
gl1463	m	vs	c	21.0639	1
gl1463	m	vs	d	22.0591	0
gl1463	m	vs	h	21.0637	0
gl1463	m	vs	g	20.055	0
gl1463	m	vs	c	20.0548	1
gl1463	m	vs	h	21.0636	1
gl1463	m	vs	b	22.0589	1
gl1463	m	vs	e	23.054	1
gl1463	m	vs	f	24.0631	1
gl1463	m	vs	a	25.0585	1
gl1463	m	vs	b	26.0548	0
gl1463	m	vs	a	25.0583	1
gl1463	m	vs	d	26.0671	0
gl1463	m	vs	e	25.0579	0
gl1463	m	vs	h	24.063	0
gl1463	m	vs	c	23.0556	0
gl1463	m	vs	b	22.0593	0
gl1463	m	vs	e	21.0638	0
gl1463	m	vs	f	20.055	1
gl1463	m	vs	f	21.0639	0
gl1463	m	vs	b	20.055	0
gl1463	m	vs	h	20.0548	0
gl1463	m	vs	e	20.0552	0
gl1463	m	vs	c	20.055	0
gl1463	m	vs	d	20.055	0
gl1463	m	vs	b	20.0548	0
gl1463	m	vs	a	20.055	0
gl1463	m	vs	b	20.0549	0
gl1463	m	vs	b	20.0549	0
gl1463	m	vs	h	20.0549	0
gl1463	m	vs	b	20.0548	0
gl1463	m	vs	d	20.0549	0
gl1463	m	vs	c	20.055	0
gl1463	m	vs	f	20.0548	0
gl1463	m	vs	e	20.0575	0
gl1463	m	vs	c	20.0549	0
gl1463	m	vs	h	20.0549	0
gl1463	m	vs	h	20.0576	1
gl1463	m	vs	c	21.0638	0
gl1463	m	vs	c	20.0549	0
gl1463	m	vs	f	20.0547	0
gl1463	m	vs	a	20.0565	0
gl1463	m	vs	e	20.0548	0
gl1463	m	vs	d	20.0551	0
gl1463	m	vs	c	20.0551	0
gl1463	m	vs	g	20.055	0
gl1463	m	vs	h	20.0549	0
gl1463	m	vs	f	20.0548	0
gl1463	m	vs	b	20.055	0
gl1463	m	vs	b	20.0549	0
gl1463	m	vs	d	20.0548	0
gl1463	m	vs	b	20.0548	0
gl1463	m	vs	b	20.0547	0
gl1463	m	vs	d	20.0549	0
gl1463	m	vs	g	20.0549	0
gl1463	m	vs	d	20.055	0
gl1463	m	vs	c	20.0574	0
gl1463	m	vs	h	20.0549	0
gl1463	m	vs	d	20.0549	0
gl1463	m	vs	c	20.0548	0
gl1463	m	vs	b	20.0549	0
gl1463	m	vs	a	20.0548	0
gl1463	m	vs	h	20.0549	0
gl1463	m	vs	g	20.055	0
gl1463	m	vs	c	20.055	0
gl1463	m	vs	d	20.0548	0
gl1463	m	vs	b	20.0549	0
gl1463	m	vs	d	20.0549	0
gl1463	m	vs	b	20.0549	0
gl1463	m	vs	g	20.0547	0
gl1463	m	vs	b	20.0549	0
gl1463	m	vs	c	20.0658	0
gl1463	m	vs	g	20.0549	0
gl1463	m	vs	b	20.0547	0
gl1463	m	vs	h	20.0547	0
gl1463	m	vs	c	20.0547	0
gl1463	m	vs	f	20.0548	0
gl1463	m	vs	f	20.0548	0
gl1463	m	vs	g	20.0552	0
gl1463	m	vs	f	20.055	0
gl1463	m	vs	f	20.0549	0
gl1463	m	vs	a	20.0549	0
gl1463	m	vs	e	20.0548	0
gl1463	m	vs	f	20.0552	0
gl1463	m	vs	c	20.0547	0
gl1463	m	vs	a	20.0548	0
gl1463	m	vs	h	20.0552	0
gl1463	m	vs	g	20.0548	0
gl1463	m	vs	b	20.0549	0
gl1463	m	vs	c	20.0549	0
gm1760	m	cc	g	20.0511	1
gm1760	m	cc	f	18.0337	0
gm1760	m	cc	b	20.0512	0
gm1760	m	cc	c	22.0411	1
gm1760	m	cc	a	20.0512	1
gm1760	m	cc	f	18.0337	0
gm1760	m	cc	f	20.0512	0
gm1760	m	cc	f	22.0412	0
gm1760	m	cc	e	24.031	1
gm1760	m	cc	e	22.041	0
gm1760	m	cc	c	24.031	0
gm1760	m	cc	g	26.0486	1
gm1760	m	cc	b	24.031	1
gm1760	m	cc	f	22.041	0
gm1760	m	cc	g	24.031	0
gm1760	m	cc	b	26.0486	0
gm1760	m	cc	f	28.0386	1
gm1760	m	cc	c	26.0486	0
gm1760	m	cc	c	28.0385	1
gm1760	m	cc	c	26.0487	1
gm1760	m	cc	c	24.0311	1
gm1760	m	cc	g	22.0412	1
gm1760	m	cc	d	20.051	0
gm1760	m	cc	b	22.0411	1
gm1760	m	cc	c	20.0513	1
gm1760	m	cc	b	18.0336	1
gm1760	m	cc	b	16.0437	1
gm1760	m	cc	g	14.0262	1
gm1760	m	cc	c	12.0363	1
gm1760	m	cc	f	12.0363	1
gm1760	m	cc	g	12.036	1
gm1760	m	cc	c	12.0361	1
gm1760	m	cc	f	12.0363	1
gm1760	m	cc	a	12.0362	0
gm1760	m	cc	b	14.0261	1
gm1760	m	cc	f	12.0363	0
gm1760	m	cc	c	14.0263	0
gm1760	m	cc	c	16.044	0
gm1760	m	cc	f	18.0337	1
gm1760	m	cc	b	16.0437	1
gm1760	m	cc	g	14.026	1
gm1760	m	cc	e	12.0363	1
gm1760	m	cc	a	12.0363	0
gm1760	m	cc	a	14.0262	0
gm1760	m	cc	c	16.0437	1
gm1760	m	cc	f	14.0262	1
gm1760	m	cc	a	12.0363	0
gm1760	m	cc	h	14.0261	0
gm1760	m	cc	f	16.0438	0
gm1760	m	cc	h	18.0337	1
gm1760	m	cc	b	16.0438	0
gm1760	m	cc	h	18.0336	1
gm1760	m	cc	b	16.0438	0
gm1760	m	cc	f	18.0337	0
gm1760	m	cc	f	20.0512	0
gm1760	m	cc	e	22.0411	1
gm1760	m	cc	h	20.0513	1
gm1760	m	cc	g	18.0337	0
gm1760	m	cc	g	20.0513	0
gm1760	m	cc	c	22.0411	1
gm1760	m	cc	c	20.0513	1
gm1760	m	cc	d	18.0336	0
gm1760	m	cc	g	20.0513	1
gm1760	m	cc	a	18.0337	1
gm1760	m	cc	c	16.0437	0
gm1760	m	cc	h	18.0338	0
gm1760	m	cc	f	20.0513	1
gm1760	m	cc	g	18.0338	0
gm1760	m	cc	c	20.0513	0
gs0326	m	vs	g	20.0492	1
gs0326	m	vs	b	21.044	0
gs0326	m	vs	g	20.049	0
gs0326	m	vs	c	20.0491	0
gs0326	m	vs	g	20.0491	0
gs0326	m	vs	b	20.0492	1
gs0326	m	vs	f	21.0438	1
gs0326	m	vs	d	22.0526	0
gs0326	m	vs	c	21.0439	1
gs0326	m	vs	g	22.0526	1
gs0326	m	vs	g	23.0473	0
gs0326	m	vs	g	22.0527	0
gs0326	m	vs	d	21.044	1
gs0326	m	vs	d	22.0526	0
gs0326	m	vs	h	21.0439	1
gs0326	m	vs	a	22.0526	1
gs0326	m	vs	b	23.0474	1
gs0326	m	vs	d	24.0561	0
gs0326	m	vs	f	23.0474	0
gs0326	m	vs	e	22.0526	1
gs0326	m	vs	b	23.0474	0
gs0326	m	vs	c	22.0527	1
gs0326	m	vs	f	23.0473	0
gs0326	m	vs	c	22.0526	0
gs0326	m	vs	f	21.0439	1
gs0326	m	vs	e	22.0528	0
gs0326	m	vs	f	21.0438	1
gs0326	m	vs	d	22.0526	1
gs0326	m	vs	e	23.0474	0
gs0326	m	vs	f	22.0526	0
gs0326	m	vs	b	21.0439	0
gs0326	m	vs	g	20.0489	1
gs0326	m	vs	b	21.0439	1
gs0326	m	vs	g	22.0525	0
gs0326	m	vs	b	21.0438	0
gs0326	m	vs	b	20.0491	1
gs0326	m	vs	d	21.0439	0
gs0326	m	vs	h	20.0491	0
gs0326	m	vs	a	20.049	1
gs0326	m	vs	a	21.044	1
gs0326	m	vs	f	22.0525	1
gs0326	m	vs	g	23.0475	1
gs0326	m	vs	b	24.056	1
gs0326	m	vs	f	25.0509	0
gs0326	m	vs	b	24.0561	0
gs0326	m	vs	e	23.0475	1
gs0326	m	vs	h	24.0561	0
gs0326	m	vs	f	23.0474	0
gs0326	m	vs	g	22.0526	0
gs0326	m	vs	b	21.0439	1
gs0326	m	vs	g	22.0526	1
gs0326	m	vs	g	23.0475	0
gs0326	m	vs	h	22.0526	1
gs0326	m	vs	f	23.0474	0
gs0326	m	vs	f	22.0525	1
gs0326	m	vs	g	23.0473	1
gs0326	m	vs	f	24.0563	1
gs0326	m	vs	b	25.051	1
gs0326	m	vs	a	26.0458	1
gs0326	m	vs	f	27.0546	1
gs0326	m	vs	f	28.0492	1
gs0326	m	vs	h	29.0442	0
gs0326	m	vs	f	28.0494	1
gs0326	m	vs	g	29.0442	0
gs0326	m	vs	c	28.0493	0
gs0326	m	vs	a	27.0544	0
gs0326	m	vs	g	26.0457	0
gs0326	m	vs	d	25.0509	0
gs0326	m	vs	c	24.0561	0
gs0326	m	vs	e	23.0476	1
gs0326	m	vs	f	24.056	0
gs0326	m	vs	g	23.0475	0
gs0326	m	vs	f	22.0526	1
gs0326	m	vs	g	23.0474	1
gs0326	m	vs	c	24.0562	1
gs0326	m	vs	e	25.0511	0
gs0326	m	vs	h	24.0562	1
gs0326	m	vs	a	25.0509	0
gs0326	m	vs	c	24.0561	0
gs0326	m	vs	h	23.0474	1
gs0326	m	vs	e	24.0561	0
gs0326	m	vs	b	23.0474	1
gs0326	m	vs	c	24.056	0
gs0326	m	vs	a	23.0473	0
gs0326	m	vs	f	22.0526	0
gs0326	m	vs	a	21.0439	0
gs0326	m	vs	f	20.049	1
gs0326	m	vs	c	21.0439	1
gs0326	m	vs	h	22.0525	0
gs0326	m	vs	h	21.044	0
gs0326	m	vs	b	20.0491	0
gs1003	m	vs	d	20.0416	1
gs1003	m	vs	d	21.0388	1
gs1003	m	vs	c	22.036	0
gs1003	m	vs	h	21.0388	0
gs1003	m	vs	e	20.0416	1
gs1003	m	vs	a	21.0389	0
gs1003	m	vs	c	20.0417	1
gs1003	m	vs	c	21.039	0
gs1003	m	vs	d	20.0418	0
gs1003	m	vs	d	20.0416	1
gs1003	m	vs	d	21.0389	1
gs1003	m	vs	a	22.0361	0
gs1003	m	vs	h	21.0389	0
gs1003	m	vs	f	20.0417	0
gs1003	m	vs	a	20.0417	0
gs1003	m	vs	e	20.0414	1
gs1003	m	vs	f	21.0389	0
gs1003	m	vs	f	20.0417	0
gs1003	m	vs	c	20.0416	0
gs1003	m	vs	b	20.0418	1
gs1003	m	vs	g	21.0389	1
gs1003	m	vs	c	22.0362	0
gs1003	m	vs	c	21.0389	1
gs1003	m	vs	b	22.0363	1
gs1003	m	vs	g	23.0334	1
gs1003	m	vs	h	24.0444	0
gs1003	m	vs	f	23.0334	1
gs1003	m	vs	h	24.0444	1
gs1003	m	vs	g	25.0417	1
gs1003	m	vs	g	26.039	0
gs1003	m	vs	a	25.0417	0
gs1003	m	vs	f	24.0443	0
gs1003	m	vs	b	23.0333	0
gs1003	m	vs	a	22.0361	1
gs1003	m	vs	f	23.0334	0
gs1003	m	vs	c	22.0361	1
gs1003	m	vs	d	23.0333	1
gs1003	m	vs	f	24.0444	1
gs1003	m	vs	b	25.0417	1
gs1003	m	vs	f	26.039	0
gs1003	m	vs	g	25.0417	1
gs1003	m	vs	f	26.0389	1
gs1003	m	vs	b	27.0362	0
gs1003	m	vs	b	26.039	1
gs1003	m	vs	g	27.0362	0
gs1003	m	vs	e	26.0389	1
gs1003	m	vs	f	27.0362	1
gs1003	m	vs	f	28.0334	0
gs1003	m	vs	a	27.0362	0
gs1003	m	vs	b	26.0389	1
gs1003	m	vs	h	27.0361	0
gs1003	m	vs	c	26.039	1
gs1003	m	vs	f	27.0361	1
gs1003	m	vs	f	28.0333	0
gs1003	m	vs	f	27.0362	1
gs1003	m	vs	d	28.0335	1
gs1003	m	vs	d	29.0444	0
gs1003	m	vs	g	28.0335	1
gs1003	m	vs	b	29.0444	0
gs1003	m	vs	g	28.0334	0
gs1003	m	vs	a	27.0361	0
gs1003	m	vs	c	26.039	0
gs1003	m	vs	e	25.0417	0
gs1003	m	vs	f	24.0444	0
gs1003	m	vs	h	23.0333	1
gs1003	m	vs	a	24.0444	0
gs1003	m	vs	f	23.0333	0
gs1003	m	vs	f	22.036	0
gs1003	m	vs	d	21.0389	1
gs1003	m	vs	g	22.0362	0
gs1003	m	vs	b	21.0388	1
gs1003	m	vs	c	22.0361	1
hb1881	f	cc	g	20.0416	0
hb1881	f	cc	e	22.0361	1
hb1881	f	cc	f	20.0417	0
hb1881	f	cc	c	22.0361	1
hb1881	f	cc	h	20.0416	0
hb1881	f	cc	g	22.0361	1
hb1881	f	cc	e	20.0417	1
hb1881	f	cc	f	18.0196	1
hb1881	f	cc	g	16.0249	0
hb1881	f	cc	c	18.0195	1
hb1881	f	cc	b	16.025	1
hb1881	f	cc	c	14.0305	0
hb1881	f	cc	b	16.0251	1
hb1881	f	cc	c	14.0305	0
hb1881	f	cc	f	16.025	1
hb1881	f	cc	f	14.0304	0
hb1881	f	cc	g	16.025	1
hb1881	f	cc	b	14.0305	1
hb1881	f	cc	d	12.0359	0
hb1881	f	cc	d	14.0305	0
hb1881	f	cc	c	16.025	1
hb1881	f	cc	b	14.0307	0
hb1881	f	cc	g	16.0249	0
hb1881	f	cc	d	18.0195	1
hb1881	f	cc	h	16.025	0
hb1881	f	cc	g	18.0195	0
hb1881	f	cc	h	20.0418	0
hb1881	f	cc	f	22.0361	1
hb1881	f	cc	c	20.0417	1
hb1881	f	cc	f	18.0195	1
hb1881	f	cc	h	16.0248	1
hb1881	f	cc	g	14.0305	1
hb1881	f	cc	c	12.0361	0
hb1881	f	cc	e	14.0305	1
hb1881	f	cc	h	12.0359	0
hb1881	f	cc	f	14.0306	0
hb1881	f	cc	e	16.025	1
hb1881	f	cc	f	14.0306	1
hb1881	f	cc	f	12.036	0
hb1881	f	cc	g	14.0304	0
hb1881	f	cc	h	16.025	1
hb1881	f	cc	f	14.0306	1
hb1881	f	cc	g	12.0361	0
hb1881	f	cc	b	14.0306	0
hb1881	f	cc	g	16.025	1
hb1881	f	cc	f	14.0306	1
hb1881	f	cc	g	12.0361	0
hb1881	f	cc	b	14.0306	0
hb1881	f	cc	c	16.0251	1
hb1881	f	cc	b	14.0305	0
hb1881	f	cc	g	16.0249	1
hb1881	f	cc	a	14.0305	0
hb1881	f	cc	h	16.025	1
hb1881	f	cc	f	14.0306	1
hb1881	f	cc	b	12.036	0
hb1881	f	cc	f	14.0305	1
hb1881	f	cc	c	12.036	0
hb1881	f	cc	c	14.0306	0
hb1881	f	cc	h	16.0251	1
hb1881	f	cc	d	14.0305	0
hb1881	f	cc	c	16.0249	1
hb1881	f	cc	h	14.0306	1
hb1881	f	cc	e	12.0362	0
hb1881	f	cc	d	14.0306	1
hb1881	f	cc	g	12.0361	0
hb1881	f	cc	e	14.0306	0
hb1881	f	cc	f	16.0251	1
hb1881	f	cc	h	14.0306	0
hb1881	f	cc	a	16.025	1
hb1881	f	cc	g	14.0305	0
hb1881	f	cc	f	16.0251	1
hb1881	f	cc	g	14.0305	1
hb1881	f	cc	b	12.0359	0
hb1881	f	cc	c	14.0306	1
hb1881	f	cc	c	12.0361	0
hb1881	f	cc	c	14.0306	1
hb1881	f	cc	h	12.036	0
hb1881	f	cc	h	14.0305	0
hb1881	f	cc	f	16.025	1
hb1881	f	cc	f	14.0305	1
hb1881	f	cc	f	12.0361	0
hb1881	f	cc	e	14.0306	0
hb1881	f	cc	d	16.0249	1
hb1881	f	cc	g	14.0305	0
hb1881	f	cc	b	16.0251	0
hb1881	f	cc	c	18.0195	1
hb1881	f	cc	c	16.025	1
hb1881	f	cc	a	14.0305	0
hb1881	f	cc	g	16.0249	1
hb1881	f	cc	g	14.0305	0
hb1881	f	cc	a	16.025	1
hb1881	f	cc	h	14.0306	0
hb1881	f	cc	e	16.025	0
hb1881	f	cc	c	18.0196	1
hb1881	f	cc	g	16.025	0
hb1881	f	cc	g	18.0195	1
hb1881	f	cc	c	16.025	0

From ym at climpact.com  Tue Sep 20 17:55:10 2005
From: ym at climpact.com (Yves Magliulo)
Date: 20 Sep 2005 17:55:10 +0200
Subject: [R] Estimate predictor contribution in GAM models
Message-ID: <1127231710.7013.122.camel@new-york.climpact.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050920/78ced7ce/attachment.pl

From Oliver.Duerr at genedata.com  Tue Sep 20 18:10:48 2005
From: Oliver.Duerr at genedata.com (Oliver Duerr)
Date: Tue, 20 Sep 2005 18:10:48 +0200
Subject: [R] Strange Result using weightedMedian
Message-ID: <op.sxeskabbfd7sop@smtp.genedata.com>

Dear all,
I found a strange result using R's weightedMedian function.
Consider the following:

> x <- c (0.2, 0.3, 0.5)
> w <- c (1,1,2)
> weightedMedian(x,w)
> 0.3666

In cases like above, when the weights are integers, one could argue that  
the weighted
median should be the same as the standard median with the elements  
repeated according to their weights. This is trivially true for the mean.
In the example above, we simply double the occurrence of the 0.5 entry

> x1 <- c(0.2, 0.3, 0.5, 0.5)
> median(x1)  0.4

Does anyone know the answer to that inconsistency?
It must have to do with the interpolated version.
If you switch of the interpolation you get:
> weightedMedian(x,w,interpolate=FALSE)
> 0.4

However, I prefer the interpolated version since it is continuous with  
respect to the weights. Is there a interpolated version of the  
weightedMedian which does not show this inconsistency?


All the best,
  Oliver



From elvis at xlsolutions-corp.com  Tue Sep 20 18:27:57 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 20 Sep 2005 09:27:57 -0700
Subject: [R] Course***R/Splus Programming *** 6 cities Nationwide / October
	2005
Message-ID: <20050920092756.a108dc04937c07ba67766dad37185406.b257eb7367.wbe@email.email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" in 6 cities.
www.xlsolutions-corp.com/Rfund.htm


**** Seattle ---------------------------------- October 24th-25th, 2005
**** San Francisco -----------------------  October 27th-28th, 2005

**** New York City -----------------------  October 20th-21st, 2005
**** Boston ---------------------------------- October 20th-21st, 2005 

**** Chicago --------------------------------- October 27th-28th, 2005
**** Washington ---------------------------- October 27th-28th, 2005


Ask for our next class in your city.

Reserve your seat now at the early bird rates!  Payment due AFTER the
class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava

Interested in R/Splus Advanced course? email us.

Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From Albatine at nova.edu  Tue Sep 20 18:27:55 2005
From: Albatine at nova.edu (Albatineh, Ahmed)
Date: Tue, 20 Sep 2005 12:27:55 -0400
Subject: [R] Extended Hypergeometric Distribution
Message-ID: <4644295A9C08474FB2B3CBFB4B7570DD0262125E@apollo.ncs.nova.edu>

Do you mean the generalized Hypergeometric distribution ??


*********************************************************************

-----Original Message-----
From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
Sent: Tuesday, September 20, 2005 5:31 AM
To: Narcyz Ghinea
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Extended Hypergeometric Distribution

>>>>> "Narcyz" == Narcyz Ghinea <Narcyz.Ghinea at swsahs.nsw.gov.au>
>>>>>     on Mon, 19 Sep 2005 12:38:27 +1000 writes:

    Narcyz> Dear R Users,

    Narcyz> There exists a non-central hypergeometric
    Narcyz> distribution function in the (MCMCpack) package, and
    Narcyz> a hypergeometric distribution function in the
    Narcyz> (stats) package.

    Narcyz> Is there a function for sampling from an extended
    Narcyz> hypergeometric distribution?

what is "extended" ?
Do you mean "extended to include non-central"?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From greg.snow at ihc.com  Tue Sep 20 18:46:56 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Tue, 20 Sep 2005 10:46:56 -0600
Subject: [R] Neat way of using R for pivoting?
Message-ID: <s32fe8b9.001@lp-msg1.co.ihc.com>

>>> "BANNISTER, Keith" <keith.bannister at astrium.eads.net> 09/20/05
09:46AM >>>
>> 
>> Hi,
>> 
>> I'd like to use R to do what excel pivot tables do, and plot
results.

R does not have pivot tables and I hope that it never does.

My experiance with pivot tables is that they encourage poor initial
design followed
by non-easily-reproducable post-hoc twiddling.

R encourages proper initial design followed by fixing the core design
in cases
where things don't turn out the way you intended. 
 
In R I prefer to work with script files and save the file.  If the
table or graph
does not turn out the way I intended, then I just edit the script file
and rerun it.
While this may be a little more work than clicking on a pivot table at
first, in the 
long run I find it saves more time.

Consider the situation where you create a table/graph, then a month
later your
boss/client/coworker finds some typos in the original data and needs
the table
and/or graph recreated with the corrected data (or maybe a new dataset
that
needs a similar graph/table).  With the pivot table you need to try and
remember
everything that you clicked on and click on it again.  With the R
script file you 
just fix the data (or load in the new data) and rerun the script and
your done.

OK, enough of my ranting, on to helping with your problem.


>> I've never used R before, and I've managed to do something, but it's
quite a
>> lot of code to do something simple. I can't help but think I'm not
"Doing it
>> the R way".
>> 
>> I could be using R for the wrong thing, in which case, please tell
me off.
[snip]

"by" is a bit of an overkill for this situation, tapply will probably
work better.

try this basic script as a starting place:

### start ###
my.df <- data.frame( SNR=rep( c(4,6,8), each=3), 
	timeError = c(1.3,2.1,1.2,2.1,2.2,2.1,3.2,3.7,3.1))

tmp.mean <- tapply( my.df$timeError, my.df$SNR, mean)
tmp.sd   <- tapply( my.df$timeError, my.df$SNR, sd)

tmp.x <- unique(my.df$SNR)

plot( tmp.x, tmp.mean,
ylim=range(tmp.mean+3*tmp.sd,tmp.mean-3*tmp.sd),
	xlab='SNR',ylab='timeError')

segments(tmp.x, tmp.mean-3*tmp.sd, tmp.x, tmp.mean+3*tmp.sd,
col='green')

### optional
points(tmp.x, tmp.mean+3*tmp.sd, pch='-',cex=3,col='green')
points(tmp.x, tmp.mean-3*tmp.sd, pch='-',cex=3,col='green')
points(tmp.x, tmp.mean)

### end script ###

This may be even simpler with a loaded package. a quick search shows
the following functions (package in parens) that may help:

plotCI(gplots)          Plot Error Bars and Confidence Intervals
errbar(Hmisc)           Plot Error Bars
xYplot(Hmisc)           xyplot and dotplot with Matrix Variables to
Plot Error Bars and Bands

plotCI(plotrix)         Plot confidence intervals/error bars

errbar(sfsmisc)         Scatter Plot with Error Bars
plotCI(sfsmisc)         Plot Confidence Intervals / Error Bars




>> Appreciate any helpful hints from the pros.
>> 

hope this helps,

>> Cheers!
>> 
>> p.s. We've been having rather a good time around the office recently
with
>> "International Talk Like a Pirate Day" (www.yarr.org.uk). R fits in
very
>> well: "I be usin' Arrrgghhhh for my post processin'".
>> 
>> 
>> Keith Bannister


Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111



From bartzk at yahoo-inc.com  Tue Sep 20 18:52:38 2005
From: bartzk at yahoo-inc.com (Kevin Bartz)
Date: Tue, 20 Sep 2005 09:52:38 -0700
Subject: [R] Transform variable number of rows per subject to column
	variables?
Message-ID: <E4EBBAD66D826C41BDED4CCCDCC6D0F101553D84@EXCHG01-BUR>

Hey Bing,

Reshape's the ticker -- ?reshape.

For example, reshape(myFrame, idvar = "ID", timevar = "TEST.A") should
do most of the trick.

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bing Ho
Sent: Monday, September 19, 2005 10:04 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Transform variable number of rows per subject to column
variables?

Hello,

I am very new to R, but I am having trouble with my dataset.

I have a data frame where a subject has a variable number of multiple 
observations for each row, which I wish the transform these 
observations to column variables.

An example of the data frame
ID 	TEST.A	TEST.B
1	10	1
1	13	2
1	11	1
2	15	2
2	17	3

And I wish to transform it to the following:
ID	TEST.A1	TEST.A2	TEST.A3	TEST.B1	TEST.B2	TEST.B3
1	10		13		11		1
2		1
2	15		17		NA		2
3		NA

In other words, for the variable number of repeated follow up 
studies, a new column variable for each subject, but they are grouped 
by the original test.

Thank you for any help - I'm realizing that I am a terrible programmer!

Bing Ho

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From qyang at bu.edu  Tue Sep 20 18:59:51 2005
From: qyang at bu.edu (Qiong Yang)
Date: Tue, 20 Sep 2005 12:59:51 -0400 (EDT)
Subject: [R] How to exclude a level from a factor
Message-ID: <Pine.A41.4.63.0509201248590.214750@acsrs3.bu.edu>


Hi,

I could not use 'exlcude=' option in factor()
to exclude a level from a existing factor.

x is a factor:

> x
[1] a b c
Levels: a b c

> factor(x,exclude="c")
[1] a b c
Levels: a b c
Warning message:
NAs introduced by coercion

However, "c" is not coded as NA.

The following does not work either:

> factor(x,exclude=factor("c",levels=c("a","b","c")))
[1] a b c
Levels: a b c


What's wrong with my codes?

Thanks for any help

Qiong Yang



From dieter.menne at menne-biomed.de  Tue Sep 20 18:58:40 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 20 Sep 2005 16:58:40 +0000 (UTC)
Subject: [R] Strange Result using weightedMedian
References: <op.sxeskabbfd7sop@smtp.genedata.com>
Message-ID: <loom.20050920T185440-195@post.gmane.org>

Oliver Duerr <Oliver.Duerr <at> genedata.com> writes:

> 
> Dear all,
> I found a strange result using R's weightedMedian function
....

I could not find weightedMedian on CRAN; I know there is such a function in 
some Java lib, googling gave a reference to limma (Linear Models for Microarray 
Data by Gordon Smyth and others).

Best contact the authors about the subject.

Dieter



From h.wickham at gmail.com  Tue Sep 20 19:14:12 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 20 Sep 2005 12:14:12 -0500
Subject: [R] Neat way of using R for pivoting?
In-Reply-To: <B3645EBD393F7E4DAB7B18EFAAA4E83B0858E0@auk52177.ukr.astrium.corp>
References: <B3645EBD393F7E4DAB7B18EFAAA4E83B0858E0@auk52177.ukr.astrium.corp>
Message-ID: <f8e6ff05050920101470ab5e59@mail.gmail.com>

Hi Keith,

You might want to check out my reshape package
(http://had.co.nz/reshape/) which is very much pivot table inspired. 
I doesn't produce graphics yet, but the output is very amenable to
being fed into existing R graphics function (especially lattice
graphics).

Hadley



From sundar.dorai-raj at pdf.com  Tue Sep 20 19:15:53 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 20 Sep 2005 12:15:53 -0500
Subject: [R] How to exclude a level from a factor
In-Reply-To: <Pine.A41.4.63.0509201248590.214750@acsrs3.bu.edu>
References: <Pine.A41.4.63.0509201248590.214750@acsrs3.bu.edu>
Message-ID: <433043C9.60506@pdf.com>



Qiong Yang wrote:
> Hi,
> 
> I could not use 'exlcude=' option in factor()
> to exclude a level from a existing factor.
> 
> x is a factor:
> 
> 
>>x
> 
> [1] a b c
> Levels: a b c
> 
> 
>>factor(x,exclude="c")
> 
> [1] a b c
> Levels: a b c
> Warning message:
> NAs introduced by coercion
> 
> However, "c" is not coded as NA.
> 
> The following does not work either:
> 
> 
>>factor(x,exclude=factor("c",levels=c("a","b","c")))
> 
> [1] a b c
> Levels: a b c
> 
> 
> What's wrong with my codes?
> 


I think you want:

x <- factor(letters[1:3])
factor(as.character(x), exclude = "c")

HTH,
--sundar



From ggrothendieck at gmail.com  Tue Sep 20 19:31:17 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Sep 2005 13:31:17 -0400
Subject: [R] Neat way of using R for pivoting?
In-Reply-To: <s32fe8b9.001@lp-msg1.co.ihc.com>
References: <s32fe8b9.001@lp-msg1.co.ihc.com>
Message-ID: <971536df0509201031fd54f08@mail.gmail.com>

On 9/20/05, Greg Snow <greg.snow at ihc.com> wrote:
> >>> "BANNISTER, Keith" <keith.bannister at astrium.eads.net> 09/20/05
> 09:46AM >>>
> >>
> >> Hi,
> >>
> >> I'd like to use R to do what excel pivot tables do, and plot
> results.
> 
> R does not have pivot tables and I hope that it never does.
> 
> My experiance with pivot tables is that they encourage poor initial
> design followed
> by non-easily-reproducable post-hoc twiddling.
> 
> R encourages proper initial design followed by fixing the core design
> in cases
> where things don't turn out the way you intended.
> 
> In R I prefer to work with script files and save the file.  If the
> table or graph
> does not turn out the way I intended, then I just edit the script file
> and rerun it.
> While this may be a little more work than clicking on a pivot table at
> first, in the
> long run I find it saves more time.
> 
> Consider the situation where you create a table/graph, then a month
> later your
> boss/client/coworker finds some typos in the original data and needs
> the table
> and/or graph recreated with the corrected data (or maybe a new dataset
> that
> needs a similar graph/table).  With the pivot table you need to try and
> remember
> everything that you clicked on and click on it again.  With the R
> script file you
> just fix the data (or load in the new data) and rerun the script and
> your done.
> 
> OK, enough of my ranting, on to helping with your problem.

Just one comment here lest we be arguing against a strawman.
While I agree that reproducibility can be a problem with pivot tables
if created interactively and this applies to just about anything you do
in Excel if done interactively, it should also be realized that Excel is 
completely programmable, like R, using VBA or any language (including R!)
via its COM object interface. 

The fact that Excel has both an interactive interface and a script-based
interface whereas R has only a script-based interface puts it ahead, not 
behind, R in at least some respects.



From ligges at statistik.uni-dortmund.de  Tue Sep 20 19:35:02 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 20 Sep 2005 19:35:02 +0200
Subject: [R] How to exclude a level from a factor
In-Reply-To: <Pine.A41.4.63.0509201248590.214750@acsrs3.bu.edu>
References: <Pine.A41.4.63.0509201248590.214750@acsrs3.bu.edu>
Message-ID: <43304846.7070700@statistik.uni-dortmund.de>

Qiong Yang wrote:

> Hi,
> 
> I could not use 'exlcude=' option in factor()
> to exclude a level from a existing factor.
> 
> x is a factor:
> 
> 
>>x
> 
> [1] a b c
> Levels: a b c
> 
> 
>>factor(x,exclude="c")
> 
> [1] a b c
> Levels: a b c
> Warning message:
> NAs introduced by coercion
> 
> However, "c" is not coded as NA.
> 
> The following does not work either:
> 
> 
>>factor(x,exclude=factor("c",levels=c("a","b","c")))
> 
> [1] a b c
> Levels: a b c
> 
> 
> What's wrong with my codes?


  factor(x, levels=letters[1:2])

Uwe Ligges

> Thanks for any help
> 
> Qiong Yang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Sep 20 19:41:48 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 20 Sep 2005 13:41:48 -0400
Subject: [R] Neat way of using R for pivoting?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED461@usctmx1106.merck.com>

> From: Greg Snow
> 
> >>> "BANNISTER, Keith" <keith.bannister at astrium.eads.net> 09/20/05
> 09:46AM >>>
> >> 
> >> Hi,
> >> 
> >> I'd like to use R to do what excel pivot tables do, and plot
> results.
> 
> R does not have pivot tables and I hope that it never does.
> 
> My experiance with pivot tables is that they encourage poor initial
> design followed
> by non-easily-reproducable post-hoc twiddling.
> 
> R encourages proper initial design followed by fixing the core design
> in cases
> where things don't turn out the way you intended. 
>  
> In R I prefer to work with script files and save the file.  If the
> table or graph
> does not turn out the way I intended, then I just edit the script file
> and rerun it.
> While this may be a little more work than clicking on a pivot table at
> first, in the 
> long run I find it saves more time.

Actually, it's even better to write functions for repetitive tasks.
This is one of the things Martin talked about at useR! 2004:
http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Maechler.pdf

For Keith's problem, here's one possibility (using plotCI() from gplots):

myErrorBarPlot <- function(SNR, timeError, ...) {
    stopifnot(require(gplots))
    m <- aggregate(timeError, list(SNR), mean)
    d <- aggregate(timeError, list(SNR), sd)
    dat <- cbind(m, d[, 2])
    names(dat) <- c("SNR", "mean", "sd")
    dat$SNR <- as.numeric(as.character(dat$SNR))
    with(dat, plotCI(SNR, mean, uiw=3*sd, ...))
    invisible(dat)
}

vn <- read.table("clipboard", header=TRUE)

myErrorBarPlot(vn$SNR, vn$timeError)

Andy
 
> Consider the situation where you create a table/graph, then a month
> later your
> boss/client/coworker finds some typos in the original data and needs
> the table
> and/or graph recreated with the corrected data (or maybe a new dataset
> that
> needs a similar graph/table).  With the pivot table you need 
> to try and
> remember
> everything that you clicked on and click on it again.  With the R
> script file you 
> just fix the data (or load in the new data) and rerun the script and
> your done.
> 
> OK, enough of my ranting, on to helping with your problem.
> 
> 
> >> I've never used R before, and I've managed to do 
> something, but it's
> quite a
> >> lot of code to do something simple. I can't help but think I'm not
> "Doing it
> >> the R way".
> >> 
> >> I could be using R for the wrong thing, in which case, please tell
> me off.
> [snip]
> 
> "by" is a bit of an overkill for this situation, tapply will probably
> work better.
> 
> try this basic script as a starting place:
> 
> ### start ###
> my.df <- data.frame( SNR=rep( c(4,6,8), each=3), 
> 	timeError = c(1.3,2.1,1.2,2.1,2.2,2.1,3.2,3.7,3.1))
> 
> tmp.mean <- tapply( my.df$timeError, my.df$SNR, mean)
> tmp.sd   <- tapply( my.df$timeError, my.df$SNR, sd)
> 
> tmp.x <- unique(my.df$SNR)
> 
> plot( tmp.x, tmp.mean,
> ylim=range(tmp.mean+3*tmp.sd,tmp.mean-3*tmp.sd),
> 	xlab='SNR',ylab='timeError')
> 
> segments(tmp.x, tmp.mean-3*tmp.sd, tmp.x, tmp.mean+3*tmp.sd,
> col='green')
> 
> ### optional
> points(tmp.x, tmp.mean+3*tmp.sd, pch='-',cex=3,col='green')
> points(tmp.x, tmp.mean-3*tmp.sd, pch='-',cex=3,col='green')
> points(tmp.x, tmp.mean)
> 
> ### end script ###
> 
> This may be even simpler with a loaded package. a quick search shows
> the following functions (package in parens) that may help:
> 
> plotCI(gplots)          Plot Error Bars and Confidence Intervals
> errbar(Hmisc)           Plot Error Bars
> xYplot(Hmisc)           xyplot and dotplot with Matrix Variables to
> Plot Error Bars and Bands
> 
> plotCI(plotrix)         Plot confidence intervals/error bars
> 
> errbar(sfsmisc)         Scatter Plot with Error Bars
> plotCI(sfsmisc)         Plot Confidence Intervals / Error Bars
> 
> 
> 
> 
> >> Appreciate any helpful hints from the pros.
> >> 
> 
> hope this helps,
> 
> >> Cheers!
> >> 
> >> p.s. We've been having rather a good time around the 
> office recently
> with
> >> "International Talk Like a Pirate Day" (www.yarr.org.uk). R fits in
> very
> >> well: "I be usin' Arrrgghhhh for my post processin'".
> >> 
> >> 
> >> Keith Bannister
> 
> 
> Greg Snow, Ph.D.
> Statistical Data Center, LDS Hospital
> Intermountain Health Care
> greg.snow at ihc.com
> (801) 408-8111
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From qyang at bu.edu  Tue Sep 20 19:43:49 2005
From: qyang at bu.edu (Qiong Yang)
Date: Tue, 20 Sep 2005 13:43:49 -0400 (EDT)
Subject: [R] How to exclude a level from a factor
In-Reply-To: <433043C9.60506@pdf.com>
References: <Pine.A41.4.63.0509201248590.214750@acsrs3.bu.edu>
	<433043C9.60506@pdf.com>
Message-ID: <Pine.A41.4.63.0509201337500.214750@acsrs3.bu.edu>


Problem solved. Thanks a lot for your replies!

> x
[1] a b c
Levels: a b c

> factor(as.character(x),exclude="c")
[1] a    b    <NA>
Levels: a b

"exclude=" option may not work on factors.
One has to convert the factor to character first.

Qiong

On Tue, 20 Sep 2005, Sundar Dorai-Raj wrote:

>
>
> Qiong Yang wrote:
>> Hi,
>> 
>> I could not use 'exlcude=' option in factor()
>> to exclude a level from a existing factor.
>> 
>> x is a factor:
>> 
>> 
>>> x
>> 
>> [1] a b c
>> Levels: a b c
>> 
>> 
>>> factor(x,exclude="c")
>> 
>> [1] a b c
>> Levels: a b c
>> Warning message:
>> NAs introduced by coercion
>> 
>> However, "c" is not coded as NA.
>> 
>> The following does not work either:
>> 
>> 
>>> factor(x,exclude=factor("c",levels=c("a","b","c")))
>> 
>> [1] a b c
>> Levels: a b c
>> 
>> 
>> What's wrong with my codes?
>> 
>
>
> I think you want:
>
> x <- factor(letters[1:3])
> factor(as.character(x), exclude = "c")
>
> HTH,
> --sundar
>



From oliver.duerr at gmail.com  Tue Sep 20 20:04:40 2005
From: oliver.duerr at gmail.com (=?ISO-8859-1?Q?Oliver_D=FCrr?=)
Date: Tue, 20 Sep 2005 20:04:40 +0200
Subject: [R]  Strange Result using weightedMedian
Message-ID: <51DC6F5C-649D-41D2-AE0A-CB97E59E15B0@gmail.com>

weightedMedian is in the R.basic packag.
To download see:
See http://www.maths.lth.se/help/R/R.classes/#1.%20Introduction

Best,
  Oliver



From f.harrell at vanderbilt.edu  Tue Sep 20 20:40:44 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 20 Sep 2005 13:40:44 -0500
Subject: [R] Strange Result using weightedMedian
In-Reply-To: <op.sxeskabbfd7sop@smtp.genedata.com>
References: <op.sxeskabbfd7sop@smtp.genedata.com>
Message-ID: <433057AC.7080408@vanderbilt.edu>

Oliver Duerr wrote:
> Dear all,
> I found a strange result using R's weightedMedian function.
> Consider the following:
> 
> 
>>x <- c (0.2, 0.3, 0.5)
>>w <- c (1,1,2)
>>weightedMedian(x,w)
>>0.3666
> 
> 
> In cases like above, when the weights are integers, one could argue that  
> the weighted
> median should be the same as the standard median with the elements  
> repeated according to their weights. This is trivially true for the mean.
> In the example above, we simply double the occurrence of the 0.5 entry
> 
> 
>>x1 <- c(0.2, 0.3, 0.5, 0.5)
>>median(x1)  0.4
> 
> 
> Does anyone know the answer to that inconsistency?
> It must have to do with the interpolated version.
> If you switch of the interpolation you get:
> 
>>weightedMedian(x,w,interpolate=FALSE)
>>0.4
> 
> 
> However, I prefer the interpolated version since it is continuous with  
> respect to the weights. Is there a interpolated version of the  
> weightedMedian which does not show this inconsistency?
> 
> 
> All the best,
>   Oliver

By the way:

 > library(Hmisc)
 > wtd.quantile(x,w)
    0%   25%   50%   75%  100%
0.200 0.275 0.400 0.500 0.500

Also see the type argument to wtd.quantile

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From uiskin at udesa.edu.ar  Tue Sep 20 20:44:07 2005
From: uiskin at udesa.edu.ar (Uri Iskin)
Date: Tue, 20 Sep 2005 15:44:07 -0300
Subject: [R] help with estimating parameters with nls
Message-ID: <BAY17-DAV86642D33E424707FAE62BD7950@phx.gbl>

Dear helpeRs,

I have a vector containing values of incomes and I would like to estimate
the three parameters of a Dagum distribution.
Dagum himself recommends to use nonlinear least-squares method.
I have read nls, optim (and the posting guide!) and still have not succeded.

sipcf is my sorted vector of incomes with 3065 obs.
fda is a vector of the empirical cumulative distribution probabilities:
> fda <- vector(length=length(sipcf))
> for (I in 1:length(sipcf)){
> fda[i] <- sum(sipcf <= sipcf[i])/length(sipcf)
> }
fdae is the cdf:
> fdae <- function(x){
> fda[sum(sipcf<=x)]
}
dagfda is the cdf of Dagum:
> dagfda <- function(x,a,b,p){
(1+(x^(-a)*b^a))^(-p)
}

start values I am using: a=1.9 b=0.34 p=1.3

I am trying to get the parameters estimates with nls and optim, but all I
get are errors.
Could you help me showing how to get them?
Thank you in advance!

Uri Iskin



From pburns at pburns.seanet.com  Tue Sep 20 20:46:36 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 20 Sep 2005 19:46:36 +0100
Subject: [R] Neat way of using R for pivoting?
In-Reply-To: <971536df0509201031fd54f08@mail.gmail.com>
References: <s32fe8b9.001@lp-msg1.co.ihc.com>
	<971536df0509201031fd54f08@mail.gmail.com>
Message-ID: <4330590C.3070600@pburns.seanet.com>

Gabor Grothendieck wrote:
...

>The fact that Excel has both an interactive interface and a script-based
>interface whereas R has only a script-based interface puts it ahead, not 
>behind, R in at least some respects.
>  
>

Sorry, but I can't resist:  That very much depends on if
you are doing something that is appropriate to be done
in a spreadsheet.  The set of tasks appropriate for R is
very much larger than the set appropriate for Excel.

http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From ggrothendieck at gmail.com  Tue Sep 20 21:35:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Sep 2005 15:35:42 -0400
Subject: [R] Neat way of using R for pivoting?
In-Reply-To: <4330590C.3070600@pburns.seanet.com>
References: <s32fe8b9.001@lp-msg1.co.ihc.com>
	<971536df0509201031fd54f08@mail.gmail.com>
	<4330590C.3070600@pburns.seanet.com>
Message-ID: <971536df0509201235511774cb@mail.gmail.com>

On 9/20/05, Patrick Burns <pburns at pburns.seanet.com> wrote:
> Gabor Grothendieck wrote:
> ...
> 
> >The fact that Excel has both an interactive interface and a script-based
> >interface whereas R has only a script-based interface puts it ahead, not
> >behind, R in at least some respects.
> >
> >
> 
> Sorry, but I can't resist:  That very much depends on if
> you are doing something that is appropriate to be done
> in a spreadsheet.  The set of tasks appropriate for R is
> very much larger than the set appropriate for Excel.
> 
> http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
> 

I certainly don't want to be an apologist for Excel but I would
not asses its domain of applicability to be a subset of that of
R.  I agree with most of the points made in the link you cited 
but its mainly concerned  with stretching the use of spreadsheets 
to situations where R would be better  

At the same time the domain where spreadsheets are appropriate 
and preferable is very large and probably exceeds the domain where R
is preferable to Excel due to the fact that financial, accounting
and budgetary work done by every organization is mostly in the domain
of applicabilty of Excel.  Also I think the link overstates the case,
at least in reference to Excel, since some of the criticisms can
be overcome using Excel's scripting capability.



From greg.snow at ihc.com  Tue Sep 20 21:45:32 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Tue, 20 Sep 2005 13:45:32 -0600
Subject: [R] Neat way of using R for pivoting?
Message-ID: <s3301286.028@lp-msg1.co.ihc.com>


>>> Gabor Grothendieck <ggrothendieck at gmail.com> 09/20/05 11:31AM >>>
>> Just one comment here lest we be arguing against a strawman.
>> While I agree that reproducibility can be a problem with pivot
tables
>> if created interactively and this applies to just about anything you
do
>> in Excel if done interactively, it should also be realized that
Excel is 
>> completely programmable, like R, using VBA or any language
(including R!)
>> via its COM object interface. 
>> 
>> The fact that Excel has both an interactive interface and a
script-based
>> interface whereas R has only a script-based interface puts it ahead,
not 
>> behind, R in at least some respects.

Just one comment here lest we be arguing against a strawman.  
R has both interactive and script-based interfaces available and has
for a long time (I remember working with an early port of S in the
1980's on VMS machines which if you used the old Tek10 graphics driver
(anyone else remember the days of printer()...show() and 
tektronics(sp?) dumb terminals?) allowed you to click on a point in 
your graph and have it labelled).

One of the big differences I see between R and Excel is that while
they both have script and gui based interfaces, the gui interfaces
for R (take Rcmdr for example) provide an aid to learning, while
still encouraging the use of command lines, scripts, and functions,
while Excel hides the script interface from all but experts and 
encourages non-reproducable clicking.

Just because a software package has a capability does not mean much if
the overall design promotes the use of a less desirable feature.  I
remember one job where before I came along they were using a
spreedsheet to compute a column of numbers, highlighting and printing
out those numbers, then hand entering these same numbers into a
different spreadsheet.  Dr. Burns has already posted the url that
contains another of my experiances with intelligent people getting
caught in one of Excel's traps (and yes Excel has a feature that would
have prevented the trap, but Excel convieniently hid the need to use
it).



Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111



From ggrothendieck at gmail.com  Tue Sep 20 22:05:56 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Sep 2005 16:05:56 -0400
Subject: [R] Neat way of using R for pivoting?
In-Reply-To: <s3301286.027@lp-msg1.co.ihc.com>
References: <s3301286.027@lp-msg1.co.ihc.com>
Message-ID: <971536df050920130575e154bc@mail.gmail.com>

On 9/20/05, Greg Snow <greg.snow at ihc.com> wrote:
> 
> >>> Gabor Grothendieck <ggrothendieck at gmail.com> 09/20/05 11:31AM >>>
> >> Just one comment here lest we be arguing against a strawman.
> >> While I agree that reproducibility can be a problem with pivot
> tables
> >> if created interactively and this applies to just about anything you
> do
> >> in Excel if done interactively, it should also be realized that
> Excel is
> >> completely programmable, like R, using VBA or any language
> (including R!)
> >> via its COM object interface.
> >>
> >> The fact that Excel has both an interactive interface and a
> script-based
> >> interface whereas R has only a script-based interface puts it ahead,
> not
> >> behind, R in at least some respects.
> 
> Just one comment here lest we be arguing against a strawman.
> R has both interactive and script-based interfaces available and has
> for a long time (I remember working with an early port of S in the
> 1980's on VMS machines which if you used the old Tek10 graphics driver
> (anyone else remember the days of printer()...show() and
> tektronics(sp?) dumb terminals?) allowed you to click on a point in
> your graph and have it labelled).

This hardly qualifies to be in any way comparable to Excel's pervasive
all encompassing interactive GUI interface.

> 
> One of the big differences I see between R and Excel is that while
> they both have script and gui based interfaces, the gui interfaces
> for R (take Rcmdr for example) provide an aid to learning, while
> still encouraging the use of command lines, scripts, and functions,
> while Excel hides the script interface from all but experts and
> encourages non-reproducable clicking.

Rcmdr is an excellent package but it is restricted to prewritten
sets of functionality.  On the other hand, Excel is completely
general and will allow you to automatically write scripts that
can be massaged based on virtually any interactive operation
using its macro recording facility. 

> 
> Just because a software package has a capability does not mean much if
> the overall design promotes the use of a less desirable feature.  I

Maybe you are not really familiar with Excel.  The scripting capability
is very powerful and easier to learn than R.  

> remember one job where before I came along they were using a
> spreedsheet to compute a column of numbers, highlighting and printing
> out those numbers, then hand entering these same numbers into a
> different spreadsheet.  

Excel can produce output in many ways and one can copy and paste
from it too.  This is not a valid criticism of Excel.  Excel is excellent
at interacting with other applications and the operating system.  

> Dr. Burns has already posted the url that
> contains another of my experiances with intelligent people getting
> caught in one of Excel's traps (and yes Excel has a feature that would
> have prevented the trap, but Excel convieniently hid the need to use
> it).

One can get caught in many traps with R too and, in fact, just
about any piece of complex software will have some items that
require experience before you figure out the workarounds.



From spluque at gmail.com  Tue Sep 20 22:41:38 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Tue, 20 Sep 2005 15:41:38 -0500
Subject: [R] annotating an axis in bwplot (lattice)
Message-ID: <87vf0vwjy5.fsf@gmail.com>

Hi,

I'd like to add, say, the sample size for every group in a bwplot as a
parenthetical annotation to the axis.  Here's a sketch:

--8<---------------cut here---------------start------------->8---
require(Hmisc)
age <- sample(1:100, 1000, replace = TRUE)
sex <- gl(2, 8, 1000, c("Male", "Female"))
grp <- gl(4, 6, 1000, letters[1:4])

bwplot(grp ~ age | sex, aspect = 0.5, box.ratio = 2,
       panel = function(x, y, ...) {
         panel.bpplot(x, y, nout = 0.01, probs = seq(0.05, 0.45, 0.05))
         nage <- tapply(age, grp, length)
         panel.text(rep(0, length(x)), seq(along = x), labels = nage)
       })
--8<---------------cut here---------------end--------------->8---

I have two problems here: 1. place the sample size as a note in
parenthesis next to axis annotation label for the group (e.g. a (252), b
(252), c (250), d (246)), and 2. handle more complex subsetting in the
call to bwplot, i.e. when using the 'data' and 'subset' arguments, so that
'nage' in the code above is more flexible.  I have a feeling the
'subscripts' argument may be useful for the second issue, but I'm not
discovering how.  For the first point, I'll have to play some more with
'scales' argument and its 'at' and 'labels' components.

Any suggestions?

Thanks in advance,


-- 
Sebastian P. Luque



From dirk.enzmann at jura.uni-hamburg.de  Tue Sep 20 23:44:00 2005
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Tue, 20 Sep 2005 23:44:00 +0200
Subject: [R] Problem with read.spss() and as.data.frame(),
 or: alternative to subset()?
Message-ID: <433082A0.6010302@jura.uni-hamburg.de>

Trying to select a subset of cases (rows of data) I encountered several 
problems:

Firstly, because I did not read the help to read.spss() thoroughly 
enough, I treated the data read as a data frame. For example,

dr2000 <- read.spss('myfile.sav')
d <- subset(dr2000,RBINZ99 > 0)

and thus received an error message (Object "RBINZ99" not found), because 
dr2000 is not a data.frame but a list (shown by class(dr2000)).

d <- subset(dr2000,dr2000$RBINZ99)

didn' help either, because now d is empty (dim = NULL).

Thus, I tried to use the option "to.data.frame=T" of read.spss():

dr2000 <- read.spss('myfile.sav',to.data.frame=T)

However, now R "crashes" ('R for Windows GUI front-end has found an 
error and must be closed') (the error message is in German).

Finally, I tried again using read.spss() without the option 
'to.data.frame=T' (as before) and tried to convert dr2000 to a data 
frame by using

d <- as.data.frame(dr2000)

However, R crashes again (with the same error message).

Of course, I could use SPSS first and save only the cases with RBINZ99 > 
0, but this is not always possible (all users of the data must have SPSS 
available and we have to use different selection criteria). Is there 
another possibility to solve the problem by using R? I want to select 
certain rows (cases) based on the values of one "variable" of dr2000, 
but keep all columns (variables) - although dr2000 is not a data frame?

And: R should not crash but rather give a warning.

------------------------
R version 2.1.1 Patched (2005-07-15)
Package Foreign Version 0.8-10

Operating system: Windows XP Professional (5.1 (Build 2600))
CPU: Pentium Model 2 Stepping 9
RAM: 512 MB

*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From dirk.enzmann at jura.uni-hamburg.de  Tue Sep 20 23:47:03 2005
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Tue, 20 Sep 2005 23:47:03 +0200
Subject: [R] Problem with read.spss() and as.data.frame(),
 or: alternative to subset()? (2)
Message-ID: <43308357.4030603@jura.uni-hamburg.de>

In my previous mail there is a typo. Instead of

d <- subset(dr2000,dr2000$RBINZ99)

I used

d <- subset(dr2000,dr2000$RBINZ99 > 0)


*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From WWei at mdanderson.org  Wed Sep 21 01:30:57 2005
From: WWei at mdanderson.org (WWei@mdanderson.org)
Date: Tue, 20 Sep 2005 18:30:57 -0500
Subject: [R] why this postscript didn't work?
Message-ID: <OF908F84D4.0BE611F1-ON86257082.0080E7F3-86257082.00812D4B@mdacc.tmc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050920/9d98f82f/attachment.pl

From MSchwartz at mn.rr.com  Wed Sep 21 02:20:24 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 20 Sep 2005 19:20:24 -0500
Subject: [R] why this postscript didn't work?
In-Reply-To: <OF908F84D4.0BE611F1-ON86257082.0080E7F3-86257082.00812D4B@mdacc.tmc.edu>
References: <OF908F84D4.0BE611F1-ON86257082.0080E7F3-86257082.00812D4B@mdacc.tmc.edu>
Message-ID: <1127262024.4111.18.camel@localhost.localdomain>

On Tue, 2005-09-20 at 18:30 -0500, WWei at mdanderson.org wrote:
> Hi, List,
> 
> I used the following codes to generate ps plots but foo.ps contains 
> nothing. Would someone please point out what is wrong with my codes? 
> Thanks a million!
> 
> postscript('foo.ps')
> par(mfrow=c(2,1))
> par(mfg=c(1,1))
> hist(rnorm(100),col='blue')
> par(mfrow=c(2,2))
> par(mfg=c(2,1))
> hist(rnorm(50),col='blue')
> par(mfg=c(2,2))
> hist(rnorm(60),col='blue')
> dev.off()
> 
> Best,
> 
> Auston

It sort of works here using:

Version 2.1.1 Patched (2005-09-14) on FC4

in that the graphic is drawn, but the output device dimensions are not
appropriate for the plot and the plot is rotated relative to the page.
The page is landscape orientation and the plot is cuttoff on the bottom
(actually right axis) as a result of the rotation.

Your e-mail headers suggest that you are on Windows and you do not
specify which version of R you are running, so there may be some
differences in what you see on your system resulting in problematic
output.

Try this. I am using layout() here and note that your code above can be
replaced by:

postscript("foo.ps", width = 6, height = 6)
layout(matrix(c(1, 1, 2, 3), 2, 2, byrow = TRUE))
hist(rnorm(100),col='blue')
hist(rnorm(50),col='blue')
hist(rnorm(60),col='blue')
dev.off()


Note that I am specifying height and width dimensions for the postscript
output. See if that changes anything on your system. You can of course
modify the dimension arguments as you may require.

Also note that if you want to create an EPS output file, you would need
something like:

postscript("foo.ps", width = 6, height = 6, 
           horizontal = FALSE, onefile = FALSE, paper = "special")

See the Details section of ?postscript and also ?layout.

HTH,

Marc Schwartz



From robert.king at newcastle.edu.au  Wed Sep 21 06:44:01 2005
From: robert.king at newcastle.edu.au (Robert King)
Date: Tue, 20 Sep 2005 21:44:01 -0700
Subject: [R] Searchable mailing list archives -- not reachable FIXED
In-Reply-To: <17194.26865.242120.778616@stat.math.ethz.ch>
References: <3028F4C4647C9043B870276E28C69FD6B56851@syd05.aimnsw.com.au>
	<17194.26865.242120.778616@stat.math.ethz.ch>
Message-ID: <4330E511.6080007@newcastle.edu.au>

http://tolstoy.newcastle.edu.au/~rking/R/
is now working again, following a power supply problem that kept it off 
the net.  Unfortunately, even when it is working it is firewalled away 
from pings.

Robert.

Martin Maechler wrote:
> AFAIK, the correct URL --- as also used from CRAN's search page ---
> is
>     http://tolstoy.newcastle.edu.au/~rking/R/
> 
> However you are both correct that it is not reachable anymore;
> It seems because it's been firewalled off the world :
> 
>  PING tolstoy.newcastle.edu.au (134.148.237.146) 56(84) bytes of data.
>  From newcastle-atm.nswrno.net.au (203.15.123.42) icmp_seq=1 Packet filtered
>  From newcastle-atm.nswrno.net.au (203.15.123.42) icmp_seq=14 Packet filtered
>  From newcastle-atm.nswrno.net.au (203.15.123.42) icmp_seq=17 Packet filtered
> 
> Martin
>                     
> 
>>>>>>"Murray" == Murray Pung <Murraypu at aimnsw.com.au>
>>>>>>    on Fri, 16 Sep 2005 15:43:13 +1000 writes:
> 
> 
>     Murray> Yes, I've had the same trouble.  Robert may be able
>     Murray> to sort this out.
> 
>     Murray> -----Original Message----- From:
>     Murray> Fiona.Evans at csiro.au [mailto:Fiona.Evans at csiro.au]
>     Murray> Sent: Friday, 16 September 2005 3:34 PM To:
>     Murray> r-help at stat.math.ethz.ch Subject: [R] Searchable
>     Murray> archives
> 
>     > I cannot access the searchable archives at
>     > www.tolstoy.newcastle.au/~rking/R.  Does anyone else
>     > have this problem?
> 
>     > -- Fiona H. Evans
>     > http://www.cmis.csiro.au/Fiona.Evans



From gerifalte28 at hotmail.com  Wed Sep 21 06:59:27 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 21 Sep 2005 04:59:27 +0000
Subject: [R] help with estimating parameters with nls
In-Reply-To: <BAY17-DAV86642D33E424707FAE62BD7950@phx.gbl>
Message-ID: <BAY103-F228665F71F96610557791FA6940@phx.gbl>

RSiteSearch("Dagum")

Francisco


>From: "Uri Iskin" <uiskin at udesa.edu.ar>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] help with estimating parameters with nls
>Date: Tue, 20 Sep 2005 15:44:07 -0300
>
>Dear helpeRs,
>
>I have a vector containing values of incomes and I would like to estimate
>the three parameters of a Dagum distribution.
>Dagum himself recommends to use nonlinear least-squares method.
>I have read nls, optim (and the posting guide!) and still have not 
>succeded.
>
>sipcf is my sorted vector of incomes with 3065 obs.
>fda is a vector of the empirical cumulative distribution probabilities:
> > fda <- vector(length=length(sipcf))
> > for (I in 1:length(sipcf)){
> > fda[i] <- sum(sipcf <= sipcf[i])/length(sipcf)
> > }
>fdae is the cdf:
> > fdae <- function(x){
> > fda[sum(sipcf<=x)]
>}
>dagfda is the cdf of Dagum:
> > dagfda <- function(x,a,b,p){
>(1+(x^(-a)*b^a))^(-p)
>}
>
>start values I am using: a=1.9 b=0.34 p=1.3
>
>I am trying to get the parameters estimates with nls and optim, but all I
>get are errors.
>Could you help me showing how to get them?
>Thank you in advance!
>
>Uri Iskin
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From WWei at mdanderson.org  Wed Sep 21 07:07:47 2005
From: WWei at mdanderson.org (WWei@mdanderson.org)
Date: Wed, 21 Sep 2005 00:07:47 -0500
Subject: [R] why this postscript didn't work?
Message-ID: <OF71AE8A15.087B4D8E-ON86257083.0019ADCC-86257083.001C2DD6@mdacc.tmc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050921/77eca653/attachment.pl

From security-check at symantec.com  Wed Sep 21 08:01:43 2005
From: security-check at symantec.com (security-check@symantec.com)
Date: Wed, 21 Sep 2005 08:01:43 +0200
Subject: [R] Seu computador est? infectado com o virus W32.Ruland.A@mm
Message-ID: <E1EHxfr-0006IE-It@nancy.wesowe-is.nl>



From Robert.Lundqvist at ltu.se  Tue Sep 20 15:53:25 2005
From: Robert.Lundqvist at ltu.se (Robert Lundqvist)
Date: Tue, 20 Sep 2005 13:53:25 +0000 (UTC)
Subject: [R] Add function to histogram?
Message-ID: <loom.20050920T155206-363@post.gmane.org>

Is there any neat way to add a curve (frequency function or the like) to a
histogram or other plots? I haven't found one yet...

Robert



From ggrothendieck at gmail.com  Wed Sep 21 08:21:03 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Sep 2005 02:21:03 -0400
Subject: [R] Add function to histogram?
In-Reply-To: <loom.20050920T155206-363@post.gmane.org>
References: <loom.20050920T155206-363@post.gmane.org>
Message-ID: <971536df050920232171adcb1@mail.gmail.com>

On 9/20/05, Robert Lundqvist <Robert.Lundqvist at ltu.se> wrote:
> Is there any neat way to add a curve (frequency function or the like) to a
> histogram or other plots? I haven't found one yet...

Try this:

set.seed(1)
z <- rnorm(100)
library(MASS)
truehist(z)
lines(density(z))
curve(dnorm, color = "red", add = TRUE)



From Matthias.Templ at statistik.gv.at  Wed Sep 21 08:22:52 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 21 Sep 2005 08:22:52 +0200
Subject: [R] Add function to histogram?
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAC0F@xchg1.statistik.local>

Hello,

E.g. with lines() or add a new plot to the current plot with par(new = TRUE)  (and set equal xlim and ylins??s in the plot function)

r <- rnorm(100)
hist(r,freq=FALSE)
lines(density(r))

Best,
Matthias

> 
> Is there any neat way to add a curve (frequency function or 
> the like) to a histogram or other plots? I haven't found one yet...
> 
> Robert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From Hummel at mpimp-golm.mpg.de  Wed Sep 21 08:43:41 2005
From: Hummel at mpimp-golm.mpg.de (Jan Hummel)
Date: Wed, 21 Sep 2005 08:43:41 +0200
Subject: [R] SAX Parser best practise
Message-ID: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E75F@EMAIL.mpimp-golm.mpg.de>

Dear All,

I have a question regarding best practise in setting up a XML parser
within R. 
Because I have files with more than 100 MB and I'm only interested in
some values I think a SAX-like parser using xmlEventParse() will be the
best solution.
Unfortunately the values I'm looking for, to construct some higher "mass
spectrum", are distributed over different lines: as <spectrum id="2">,
<mzArrayBinary>, <intenArrayBinary> <... name="MassToChargeRatio"
value="445.598999"/> (as one can see in the xml snip set)

I know the mechanism of using Event Handlers, as shown in the examples.
But what I'm looking for is, how can I use some "path information" as
mentioned in "addContext" parameter of xmlEventParse()? May somebody
share a example using "addContext = TRUE" and pointing me to the
variables I may use if I implement the "..." parameter within my
handlers.

Do I have to implement a "status machine" using some variables within my
handlers, or would one prefer to use the "state" parameter of
xmlEventParse()?

I would appreciate any assistance very much!
	Jan



From nlei at sfu.ca  Wed Sep 21 08:46:13 2005
From: nlei at sfu.ca (nlei@sfu.ca)
Date: Tue, 20 Sep 2005 23:46:13 -0700
Subject: [R] about set.seed
Message-ID: <200509210646.j8L6kDVU028810@rm-rstar.sfu.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050920/506e01c4/attachment.pl

From pwolf at wiwi.uni-bielefeld.de  Wed Sep 21 10:00:55 2005
From: pwolf at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Wed, 21 Sep 2005 10:00:55 +0200
Subject: [R] Add function to histogram?
In-Reply-To: <loom.20050920T155206-363@post.gmane.org>
References: <loom.20050920T155206-363@post.gmane.org>
Message-ID: <43311337.3030600@wiwi.uni-bielefeld.de>

Robert Lundqvist wrote:
>Is there any neat way to add a curve (frequency function or the like) to a
>histogram or other plots? I haven't found one yet...
>
>Robert
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
???

dat<-rnorm(100)
hist(dat,prob=TRUE)
x<-seq(-3.5,3.5,length=100)
y<-dnorm(x)
lines(x,y)

have a look at:  http://cran.at.r-project.org/doc/manuals/R-intro.pdf

Peter Wolf



From manojsw at gmail.com  Wed Sep 21 10:33:29 2005
From: manojsw at gmail.com (Manoj)
Date: Wed, 21 Sep 2005 17:33:29 +0900
Subject: [R] Help on optim
Message-ID: <829e6c8a05092101335b7bbfc8@mail.gmail.com>

Dear R-help,
       I am new to optim function and need some help with optimization.

       Problem description: I am trying to optimize a weights vector
such that it produce maximum value for a function maxVal. The
optimization is subjected to constraint. The constraints are a) Min
weight should be greater than or equal to Zero. b) Max weight should
be less than or equal to 1 c) Sum of the weights should be equal to 1.

Now after googling around a bit & trying my hands at RSiteSearch , I
found that I can use optim with L-BFGS-B method and could possible use
lower bound and upper bound for constraint, however I am clueless as
to how I can impose the sum(wgts) = 1 condition. In excel solver, this
problem is extremely simple since it's just a matter of setting an
additional constraint of setting the summation cell to 1.

Your kind help is greatly appreciated.

Best Regards

Manoj



From francoisromain at free.fr  Wed Sep 21 10:37:31 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 21 Sep 2005 10:37:31 +0200
Subject: [R] Add function to histogram?
In-Reply-To: <43311337.3030600@wiwi.uni-bielefeld.de>
References: <loom.20050920T155206-363@post.gmane.org>
	<43311337.3030600@wiwi.uni-bielefeld.de>
Message-ID: <43311BCB.3010404@free.fr>

Le 21.09.2005 10:00, Peter Wolf a ??crit :

>Robert Lundqvist wrote:
>  
>
>>Is there any neat way to add a curve (frequency function or the like) to a
>>histogram or other plots? I haven't found one yet...
>>
>>Robert
>>    
>>
>???
>
>dat<-rnorm(100)
>hist(dat,prob=TRUE)
>x<-seq(-3.5,3.5,length=100)
>y<-dnorm(x)
>lines(x,y)
>
>have a look at:  http://cran.at.r-project.org/doc/manuals/R-intro.pdf
>
>Peter Wolf
>  
>
For a frequency polygon, try to work around that piece of code, 
following Peter's notations :

dat <- rnorm(100)
h <- hist(dat,prob=TRUE, border="gray", col="gray90")
diffBreaks <- diff(h$breaks)[1]
xx <- c(h$mids[1]-diffBreaks, h$mids, tail(h$mids,1)+diffBreaks)
yy <- c(0, h$density, 0)
lines(xx, yy, lwd=2)

However, you might prefer (and that's probably wise) the kernel density 
estimator : density() that other people suggested.

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From I.Visser at uva.nl  Wed Sep 21 10:50:31 2005
From: I.Visser at uva.nl (Ingmar Visser)
Date: Wed, 21 Sep 2005 10:50:31 +0200
Subject: [R] Help on optim
In-Reply-To: <829e6c8a05092101335b7bbfc8@mail.gmail.com>
Message-ID: <BF56EB77.7EFB%I.Visser@uva.nl>

Hi Manoj,
AFAIK there is no way in R to optimize functions under general linear
equality constraints (for inequality constraints there is constrOptim). In
your case you could reparametrize such that you estimate weights w(1) ...
w(n-1) and simply compute w(n) from those.
Even so, it would be great to have a routine that does optimization under
general linear constraints. Maybe someone out there could port the OPT++
library to R ... OPT++ is an open source optimization library that does
general linear and non-linear and bound-constraint optimization.
hth, ingmar

> From: Manoj <manojsw at gmail.com>
> Reply-To: manojsw at gmail.com
> Date: Wed, 21 Sep 2005 17:33:29 +0900
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help on optim
> 
> Dear R-help,
>      I am new to optim function and need some help with optimization.
> 
>      Problem description: I am trying to optimize a weights vector
> such that it produce maximum value for a function maxVal. The
> optimization is subjected to constraint. The constraints are a) Min
> weight should be greater than or equal to Zero. b) Max weight should
> be less than or equal to 1 c) Sum of the weights should be equal to 1.
> 
> Now after googling around a bit & trying my hands at RSiteSearch , I
> found that I can use optim with L-BFGS-B method and could possible use
> lower bound and upper bound for constraint, however I am clueless as
> to how I can impose the sum(wgts) = 1 condition. In excel solver, this
> problem is extremely simple since it's just a matter of setting an
> additional constraint of setting the summation cell to 1.
> 
> Your kind help is greatly appreciated.
> 
> Best Regards
> 
> Manoj
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bernarduse1 at yahoo.fr  Wed Sep 21 10:54:30 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Wed, 21 Sep 2005 10:54:30 +0200 (CEST)
Subject: [R] Sort a data frame with respect to more than one variable
Message-ID: <20050921085431.75995.qmail@web25807.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050921/0a349cc5/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Wed Sep 21 11:01:12 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 21 Sep 2005 11:01:12 +0200
Subject: [R] Help on optim
References: <829e6c8a05092101335b7bbfc8@mail.gmail.com>
Message-ID: <00f801c5be8b$041713c0$0540210a@www.domain>

you don't need L-BFGS-B in this case since you can easily 
re-parameterize you problem, i.e., you can always write:

pi_i = exp(a_i) / sum(exp(a_i)), with e.g., a_1 = 0

and thus maximize with respect to a_i's.


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm




----- Original Message ----- 
From: "Manoj" <manojsw at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, September 21, 2005 10:33 AM
Subject: [R] Help on optim


> Dear R-help,
>       I am new to optim function and need some help with 
> optimization.
>
>       Problem description: I am trying to optimize a weights vector
> such that it produce maximum value for a function maxVal. The
> optimization is subjected to constraint. The constraints are a) Min
> weight should be greater than or equal to Zero. b) Max weight should
> be less than or equal to 1 c) Sum of the weights should be equal to 
> 1.
>
> Now after googling around a bit & trying my hands at RSiteSearch , I
> found that I can use optim with L-BFGS-B method and could possible 
> use
> lower bound and upper bound for constraint, however I am clueless as
> to how I can impose the sum(wgts) = 1 condition. In excel solver, 
> this
> problem is extremely simple since it's just a matter of setting an
> additional constraint of setting the summation cell to 1.
>
> Your kind help is greatly appreciated.
>
> Best Regards
>
> Manoj
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From francoisromain at free.fr  Wed Sep 21 11:06:49 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 21 Sep 2005 11:06:49 +0200
Subject: [R] Sort a data frame with respect to more than one variable
In-Reply-To: <20050921085431.75995.qmail@web25807.mail.ukl.yahoo.com>
References: <20050921085431.75995.qmail@web25807.mail.ukl.yahoo.com>
Message-ID: <433122A9.2030707@free.fr>

Le 21.09.2005 10:54, Marc Bernard a ??crit :

>Dear All,
> 
>How can I sort a data frame with respect to more than one  variable?
> 
>I know that for one variable X  one may use:   df[order(df$X), ] where df is  the data frame containing X.
> 
>Many thanks,
> 
>Bernard
>  
>
You already know the answer of that question. order() accepts more than 
one variable.

x <- rep(rbinom(5,size=4,prob=.5), 6)
y <- rnorm(30)
cbind(x,y)[ order(x,y) , ]

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From dimitris.rizopoulos at med.kuleuven.be  Wed Sep 21 11:06:51 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 21 Sep 2005 11:06:51 +0200
Subject: [R] Sort a data frame with respect to more than one variable
References: <20050921085431.75995.qmail@web25807.mail.ukl.yahoo.com>
Message-ID: <00fd01c5be8b$ce076590$0540210a@www.domain>

if you need nested sorting then you can still use order(), e.g., in a 
longitudinal setting you may want to sort your data.frame with respect 
to subject's id and time within subject then use,

df[order(df$id, df$time), ]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Marc Bernard" <bernarduse1 at yahoo.fr>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, September 21, 2005 10:54 AM
Subject: [R] Sort a data frame with respect to more than one variable


> Dear All,
>
> How can I sort a data frame with respect to more than one  variable?
>
> I know that for one variable X  one may use:   df[order(df$X), ] 
> where df is  the data frame containing X.
>
> Many thanks,
>
> Bernard
>
>
>
>
> ---------------------------------
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From Rob.Calver at informa.com  Wed Sep 21 11:13:13 2005
From: Rob.Calver at informa.com (Rob.Calver@informa.com)
Date: Wed, 21 Sep 2005 10:13:13 +0100
Subject: [R] ANNOUNCEMENT: 20% discount on Chapman & Hall/CRC books for
	users of R
Message-ID: <5C8D1755908F324C9EF0207518D99AB2165C64@UKEXBE01.UK.CorpLAN.net>

20% discount on Chapman & Hall/CRC books for users of R

Chapman and Hall/CRC is pleased to announce that users of R are now able to purchase our books at 20% discount through our website. To take advantage of this permanent offer, simply visit http://www.crcpress.com/, choose your titles, and insert the online discount code - 585HHXXXX - in the 'Promotion Code' field at checkout.

You can also take advantage of a limited-time offer of free standard shipping on all orders through our website at this time.

Featured Titles:

R Graphics
Paul Murrell

A description of the core graphics features of R including: a brief introduction to R; an introduction to general R graphics features. The base graphics system of R: traditional S graphics. The power and flexibility of grid graphics. Building on top of the base or grid graphics: Trellis graphics and developing new graphics functions. 

Discounted Price: $55.96/??31.99

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C486X&parent_id=&pc

***

Using R for Introductory Statistics
John Verzani  

This book fills a gap as a true introduction to statistics using R. With emphasis on data analysis and practical examples, it encourages understanding rather than focusing on learning the underlying theory. It includes a large collection of exercises and numerous practical examples from a broad range of scientific disciplines. It comes complete with an online resource containing datasets, R functions, selected solutions to exercises, and updates to the latest features. A full solutions manual is available from Chapman & Hall/CRC.
 
Discounted Price: $35.96/??19.99

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C4509&parent_id=&pc

***

Linear Models with R
Julian J. Faraway

Focussed on the practice of regression and analysis of variance, this book clearly demonstrates the different methods available and in which situations each one applies. It covers all of the standard topics, from the basics of estimation to missing data, factorial designs, and block designs, but it also includes discussion of topics, such as model uncertainty, rarely addressed in books of this type. The presentation incorporates an abundance of examples that clarify both the use of each technique and the conclusions one can draw from the results. 

Discounted Price: $55.96/??31.99

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C4258&parent_id=&pc

***

Correspondence Analysis and Data Coding with Java and R
Fionn Murtagh

Provides an introduction to methods and applications of correspondence analysis, with an emphasis on data coding - the first step in correspondence analysis. It features a practical presentation of the theory with a range of applications from data mining, financial engineering, and the biosciences. Implementation of the methods is presented using Java and R software.  

Discounted Price: $63.96/??35.99

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C5289&parent_id=&pc

***

Call for Authors

If you are working on a manuscript or have an idea for a book, and are interested in publishing with Chapman & Hall/CRC, we would be delighted to hear from you. Please do not hesitate to contact me if you would like to discuss publishing, or have any questions about any of our titles.

Rob Calver
Acquisitions Editor, Statistics
Chapman & Hall/CRC
Taylor & Francis Group
Informa
24 Blades Court
Deodar Road
London SW15 2NU, UK

Tel: +44 (0)20 7017 6334
Fax: +44 (0)20 7017 6747
Email: rob.calver at informa.com



********************************************************************************

If you have received this message in error, please notify us by return and delete the message and any attachments.  Further enquiries/returns can be sent to postmaster at tfinforma.com



From bitwrit at ozemail.com.au  Wed Sep 21 21:27:36 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 21 Sep 2005 19:27:36 +0000
Subject: [R] Add function to histogram?
In-Reply-To: <loom.20050920T155206-363@post.gmane.org>
References: <loom.20050920T155206-363@post.gmane.org>
Message-ID: <4331B428.8010607@ozemail.com.au>

Robert Lundqvist wrote:
> Is there any neat way to add a curve (frequency function or the like) to a
> histogram or other plots? I haven't found one yet...
> 
For a general solution, one usually has to scale the density function to 
fit the plot. The simple, but convenient, function rescale() in the 
plotrix package can be used like this:

add.density.curve<-function(y,density=NA) {
  xyrange<-par("usr")
  if(is.na(density)) y.density<-density(y)
  lines(seq(xyrange[1],xyrange[2],length=length(y.density$y)),
   rescale(y.density$y,c(0,xyrange[4])))
}

This only works for histograms, but I am working on a general function 
that will outsmart the habit of some functions of running the plot range 
into negative numbers when these are not possible (i.e. you can't have a 
negative count in a histogram - or a negative density except in the very 
latest physics).

Jim



From vincent at 7d4.com  Wed Sep 21 11:02:53 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Wed, 21 Sep 2005 11:02:53 +0200
Subject: [R] Sort a data frame with respect to more than one variable
In-Reply-To: <20050921085431.75995.qmail@web25807.mail.ukl.yahoo.com>
References: <20050921085431.75995.qmail@web25807.mail.ukl.yahoo.com>
Message-ID: <433121BD.1060603@7d4.com>

Marc Bernard a ??crit :

> I know that for one variable X  one may use:   
 > df[order(df$X), ] where df is  the data frame containing X.

probably not optimal, but simply why not ?
dfX  = df[order(df$X), ];
dfXY  = dfX[order(dfX$Y), ];
hih



From p.campbell at econ.bbk.ac.uk  Wed Sep 21 11:49:25 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Wed, 21 Sep 2005 10:49:25 +0100
Subject: [R] about set.seed
Message-ID: <s3313ada.050@markets.econ.bbk.ac.uk>

I'm not an expert on set.seed but I think it would be helpful if you
could post the relevant section of your code. 

Phineas Campbell


>>> <nlei at sfu.ca> 09/21/05 7:46 AM >>>
Hi there,

I have some question about set.seed these days which I may need your
help.

I'm working on some sampling project which need to generate random
numbers
by some distributions(like bivariate normal). I noticed that in
different
computers to run my code,I got very significant different results. Then
I
started to try set.seed command which I wish to see by fixing the same
random numbers what can happen. But it turned out when I run my codes in
two
computers I still can get different results. I tried different
computers.
Some of them I can get the same results but some not. A special case is
I
run in two computers which have R version 2.0.1 and 2.1.1 separetely.
And
the results are different. 

Now my question is what's the reason can make the different result
happen
when we run the same codes by setting the same seed?

This will help me a lot with my project.Thank you very much!

Linda

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jmoreira at fe.up.pt  Wed Sep 21 12:10:59 2005
From: jmoreira at fe.up.pt (jmoreira@fe.up.pt)
Date: Wed, 21 Sep 2005 11:10:59 +0100
Subject: [R] Interpretation of csplit from rpart.object
Message-ID: <20050921111059.4oyaohc2oko444c0@webmail.fe.up.pt>


I send again this help message once previously was detected a virus. So, I don't
know if the R-list receive it. The virus problem is solved. Sorry for that.

----- Forwarded message from jmoreira at fe.up.pt -----
    Date: Tue, 20 Sep 2005 14:35:12 +0100
    From: jmoreira at fe.up.pt
Reply-To: jmoreira at fe.up.pt
 Subject: Interpretation of csplit from rpart.object
      To: r-help at stat.math.ethz.ch

Dear members of R-list,

I need to reproduce the rules of a decision tree. For that I need to use the
csplit information from the rpart.object. But I cannot uderstand the
information because from my example I get:
> rpart.tree$csplit
      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
 [1,]    1    3    3    1    3    3    3
 [2,]    2    3    3    1    2    2    2
 [3,]    1    3    3    1    3    3    3
 [4,]    2    3    3    1    2    2    2
 [5,]    2    3    3    1    2    2    2
 [6,]    2    1    3    2    3    1    1
 [7,]    2    3    3    2    3    3    1
 [8,]    2    3    3    1    2    2    2
 [9,]    2    1    3    2    3    1    1
[10,]    2    1    3    3    2    2    2
[11,]    2    1    1    2    1    1    3
[12,]    2    3    3    1    2    2    2
[13,]    2    1    1    2    3    1    1
[14,]    2    3    3    1    2    2    2
[15,]    2    1    3    2    1    1    1
[16,]    2    3    1    1    2    2    2
[17,]    2    3    3    1    2    2    2
[18,]    2    1    3    2    1    3    1
[19,]    2    3    3    1    2    2    2
[20,]    2    1    3    2    1    3    3
[21,]    2    3    1    2    2    2    2
[22,]    2    1    3    2    1    1    1

I don't understand why I have 22 rows (my tree has 21 nodes including the root
node) and 7 columns (I have four explanatory variables: two numerics and two
factors; plus the numeric target variable)

?rpart.object says:

  csplit: this will be present only if one of the split variables is a
          factor. There is one row for each such split, and column 'i =
          -1' if this level of the factor goes to the left, '+1' if it
          goes to the right, and 0 if that level is not present at this
          node of the tree. For an ordered categorical variable all
          levels are marked as 'R/L',  including levels that are not
          present.

The values I got are quite different.

Can some one give me information on how to deal with that?

Thanks in advance?

Joao Moreira


----- End forwarded message -----



From francoisromain at free.fr  Wed Sep 21 12:11:40 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 21 Sep 2005 12:11:40 +0200
Subject: [R] Add function to histogram?
In-Reply-To: <43311BCB.3010404@free.fr>
References: <loom.20050920T155206-363@post.gmane.org>	<43311337.3030600@wiwi.uni-bielefeld.de>
	<43311BCB.3010404@free.fr>
Message-ID: <433131DC.7060502@free.fr>

Le 21.09.2005 10:37, Romain Francois a ??crit :

>Le 21.09.2005 10:00, Peter Wolf a ??crit :
>
>  
>
>>Robert Lundqvist wrote:
>> 
>>
>>    
>>
>>>Is there any neat way to add a curve (frequency function or the like) to a
>>>histogram or other plots? I haven't found one yet...
>>>
>>>Robert
>>>   
>>>
>>>      
>>>
>>???
>>
>>dat<-rnorm(100)
>>hist(dat,prob=TRUE)
>>x<-seq(-3.5,3.5,length=100)
>>y<-dnorm(x)
>>lines(x,y)
>>
>>have a look at:  http://cran.at.r-project.org/doc/manuals/R-intro.pdf
>>
>>Peter Wolf
>> 
>>
>>    
>>
>For a frequency polygon, try to work around that piece of code, 
>following Peter's notations :
>
>dat <- rnorm(100)
>h <- hist(dat,prob=TRUE, border="gray", col="gray90")
>diffBreaks <- diff(h$breaks)[1]
>xx <- c(h$mids[1]-diffBreaks, h$mids, tail(h$mids,1)+diffBreaks)
>yy <- c(0, h$density, 0)
>lines(xx, yy, lwd=2)
>
>However, you might prefer (and that's probably wise) the kernel density 
>estimator : density() that other people suggested.
>
>Romain
>
>  
>
I just added frequency polygon to the R Graph Gallery, see :
http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=101

Regards,

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From RKrug at sun.ac.za  Wed Sep 21 12:11:21 2005
From: RKrug at sun.ac.za (Rainer M. Krug)
Date: Wed, 21 Sep 2005 12:11:21 +0200
Subject: [R] ppp from SpatStat
Message-ID: <433131C9.4000606@sun.ac.za>

Hi

I want to extract the points from an object of type ppp.
How can I do this?

Rainer



From ripley at stats.ox.ac.uk  Wed Sep 21 12:36:41 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Sep 2005 11:36:41 +0100 (BST)
Subject: [R] Interpretation of csplit from rpart.object
In-Reply-To: <20050921111059.4oyaohc2oko444c0@webmail.fe.up.pt>
References: <20050921111059.4oyaohc2oko444c0@webmail.fe.up.pt>
Message-ID: <Pine.LNX.4.61.0509211125260.21792@gannet.stats>

Your message *was* received, and you can check the archives to see it at

https://stat.ethz.ch/pipermail/r-help/2005-September/077889.html

You need to read the code to answer the question for yourself.  There is 
lots of code interpreting csplit in the rpart package.  These lines might 
be a clue, for example

rpart.s:    if (ncat>0) ans$csplit <- catmat +2
pred.rpart.s:                        as.integer(fit$csplit -2),
summary.rpart.s:  paste(c("L", "-", "R")[x$csplit[x$splits[i,4], 1:temp[i]]],

The documentation is from the authors and may well be out of date: but you 
need to read much more carefully what it says (e.g. `this level').


On Wed, 21 Sep 2005 jmoreira at fe.up.pt wrote:

>
> I send again this help message once previously was detected a virus. So, I don't
> know if the R-list receive it. The virus problem is solved. Sorry for that.
>
> ----- Forwarded message from jmoreira at fe.up.pt -----
>    Date: Tue, 20 Sep 2005 14:35:12 +0100
>    From: jmoreira at fe.up.pt
> Reply-To: jmoreira at fe.up.pt
> Subject: Interpretation of csplit from rpart.object
>      To: r-help at stat.math.ethz.ch
>
> Dear members of R-list,
>
> I need to reproduce the rules of a decision tree. For that I need to use the
> csplit information from the rpart.object. But I cannot uderstand the
> information because from my example I get:
>> rpart.tree$csplit
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
> [1,]    1    3    3    1    3    3    3
> [2,]    2    3    3    1    2    2    2
> [3,]    1    3    3    1    3    3    3
> [4,]    2    3    3    1    2    2    2
> [5,]    2    3    3    1    2    2    2
> [6,]    2    1    3    2    3    1    1
> [7,]    2    3    3    2    3    3    1
> [8,]    2    3    3    1    2    2    2
> [9,]    2    1    3    2    3    1    1
> [10,]    2    1    3    3    2    2    2
> [11,]    2    1    1    2    1    1    3
> [12,]    2    3    3    1    2    2    2
> [13,]    2    1    1    2    3    1    1
> [14,]    2    3    3    1    2    2    2
> [15,]    2    1    3    2    1    1    1
> [16,]    2    3    1    1    2    2    2
> [17,]    2    3    3    1    2    2    2
> [18,]    2    1    3    2    1    3    1
> [19,]    2    3    3    1    2    2    2
> [20,]    2    1    3    2    1    3    3
> [21,]    2    3    1    2    2    2    2
> [22,]    2    1    3    2    1    1    1
>
> I don't understand why I have 22 rows (my tree has 21 nodes including the root
> node) and 7 columns (I have four explanatory variables: two numerics and two
> factors; plus the numeric target variable)
>
> ?rpart.object says:
>
>  csplit: this will be present only if one of the split variables is a
>          factor. There is one row for each such split, and column 'i =
>          -1' if this level of the factor goes to the left, '+1' if it
>          goes to the right, and 0 if that level is not present at this
>          node of the tree. For an ordered categorical variable all
>          levels are marked as 'R/L',  including levels that are not
>          present.
>
> The values I got are quite different.
>
> Can some one give me information on how to deal with that?
>
> Thanks in advance?
>
> Joao Moreira
>
>
> ----- End forwarded message -----
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dirk.enzmann at jura.uni-hamburg.de  Wed Sep 21 13:18:32 2005
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Wed, 21 Sep 2005 13:18:32 +0200
Subject: [R] Problem with read.spss() and as.data.frame(),
 or: alternative to subset()?
In-Reply-To: <433082A0.6010302@jura.uni-hamburg.de>
References: <433082A0.6010302@jura.uni-hamburg.de>
Message-ID: <43314188.1050209@jura.uni-hamburg.de>

The selection problem can be solved by

dr2000=read.spss('myfile')
d=lapply(dr2000,subset,dr2000$RBINZ99 > 0)

however, there is still the problem that R crashes when using

d = as.data.frame(dr2000)

or

dr2000=read.spss('myfile',to.data.frame=T)

Any suggestions why? I checked whether all components of dr2000 are of 
the same length and the sort of object of each component. This is not 
the problem: Each component has the same length (9232) and there are 66 
components of the class 'character', 981 of the class 'factor', and 479 
of the class 'numeric'.


> Trying to select a subset of cases (rows of data) I encountered several 
> problems:
> 
> Firstly, because I did not read the help to read.spss() thoroughly 
> enough, I treated the data read as a data frame. For example,
> 
> dr2000 <- read.spss('myfile.sav')
> d <- subset(dr2000,RBINZ99 > 0)
> 
> and thus received an error message (Object "RBINZ99" not found), because 
> dr2000 is not a data.frame but a list (shown by class(dr2000)).
> 
> d <- subset(dr2000,dr2000$RBINZ99 > 0)
> 
> didn' help either, because now d is empty (dim = NULL).
> 
> Thus, I tried to use the option "to.data.frame=T" of read.spss():
> 
> dr2000 <- read.spss('myfile.sav',to.data.frame=T)
> 
> However, now R "crashes" ('R for Windows GUI front-end has found an 
> error and must be closed') (the error message is in German).
> 
> Finally, I tried again using read.spss() without the option 
> 'to.data.frame=T' (as before) and tried to convert dr2000 to a data 
> frame by using
> 
> d <- as.data.frame(dr2000)
> 
> However, R crashes again (with the same error message).
> 
> Of course, I could use SPSS first and save only the cases with RBINZ99 > 
> 0, but this is not always possible (all users of the data must have SPSS 
> available and we have to use different selection criteria). Is there 
> another possibility to solve the problem by using R? I want to select 
> certain rows (cases) based on the values of one "variable" of dr2000, 
> but keep all columns (variables) - although dr2000 is not a data frame?
> 
> And: R should not crash but rather give a warning.
> 
> ------------------------
> R version 2.1.1 Patched (2005-07-15)
> Package Foreign Version 0.8-10
> 
> Operating system: Windows XP Professional (5.1 (Build 2600))
> CPU: Pentium Model 2 Stepping 9
> RAM: 512 MB

*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From 042045003 at fudan.edu.cn  Wed Sep 21 13:50:00 2005
From: 042045003 at fudan.edu.cn (ronggui.wong)
Date: Wed, 21 Sep 2005 19:50:00 +0800
Subject: [R] error when loading rwinedt
Message-ID: <0IN600GDJ0DHYT@mail.fudan.edu.cn>

the error msg is:

> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
Error in if (RWinEdtVersion) RWinEdtVersion <- scan(file.path(InstallRoot,  : 
        argument is of length zero
Error: .onAttach failed in 'attachNamespace'
Error: package/namespace load failed for 'RWinEdt'


> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status   alpha          
major    2              
minor    2.0            
year     2005           
month    09             
day      07             
svn rev  35513          
language R



From antonio.fabio at gmail.com  Wed Sep 21 13:59:26 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Wed, 21 Sep 2005 13:59:26 +0200
Subject: [R] ts.intersect bug?
Message-ID: <b0808fdc05092104595e80f242@mail.gmail.com>

This code gives an error:

 a <- ts(1:10, start=0, freq=10)
 b <- ts(1:10, start=1, freq=10)
 ts.intersect(a,b)

This one works normally (and correctly):
 a <- ts(1:10, start=0)
 b <- ts(1:10, start=1)
 ts.intersect(a,b)

Antonio, Fabio Di Narzo.

P.S. How to switch off italian error messages to post on r-help?

> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.0
year     2005
month    04
day      18
language R



From maechler at stat.math.ethz.ch  Wed Sep 21 14:10:48 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 21 Sep 2005 14:10:48 +0200
Subject: [R] Extended Hypergeometric Distribution
In-Reply-To: <588D8BDAAC0BEB4B82DA2CC5AEBA899C3F7A0F@isdex001.intra.swsahs.nsw.gov.au>
References: <588D8BDAAC0BEB4B82DA2CC5AEBA899C3F7A0F@isdex001.intra.swsahs.nsw.gov.au>
Message-ID: <17201.19912.858234.997210@stat.math.ethz.ch>

>>>>> "Narcyz" == Narcyz Ghinea <Narcyz.Ghinea at swsahs.nsw.gov.au>
>>>>>     on Wed, 21 Sep 2005 08:33:59 +1000 writes:

    Narcyz> By extended I mean multivariate. Technically it
    Narcyz> seems to refer to a very particular type of
    Narcyz> hypergeometric distribution i.e. "the multivariate
    Narcyz> Fisher's noncentral hypergeometric distribution"
    Narcyz> [ http://www.agner.org/random/theory/nchyp2.pdf ]

Thank you, Narcyz, for the clarification.

The paper you reference above mentions an implemenation in C++;
that should not be hard to interface to R, in theory at least

Regards,
Martin M??chler

 >>>>> "Narcyz" == Narcyz Ghinea <Narcyz.Ghinea at swsahs.nsw.gov.au>
 >>>>>     on Mon, 19 Sep 2005 12:38:27 +1000 writes:

    Narcyz> Dear R Users,

    Narcyz> There exists a non-central hypergeometric
    Narcyz> distribution function in the (MCMCpack) package, and
    Narcyz> a hypergeometric distribution function in the
    Narcyz> (stats) package.

    Narcyz> Is there a function for sampling from an extended
    Narcyz> hypergeometric distribution?

 MM> what is "extended" ?
 MM> Do you mean "extended to include non-central"?



From ligges at statistik.uni-dortmund.de  Wed Sep 21 14:23:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Sep 2005 14:23:20 +0200
Subject: [R] error when loading rwinedt
In-Reply-To: <0IN600GDJ0DHYT@mail.fudan.edu.cn>
References: <0IN600GDJ0DHYT@mail.fudan.edu.cn>
Message-ID: <433150B8.8090108@statistik.uni-dortmund.de>

ronggui.wong wrote:

> the error msg is:
> 
> 
>>local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> 
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> Error in if (RWinEdtVersion) RWinEdtVersion <- scan(file.path(InstallRoot,  : 
>         argument is of length zero
> Error: .onAttach failed in 'attachNamespace'
> Error: package/namespace load failed for 'RWinEdt'


Please re-install and load RWinEdt as follows:

install.packages("RWinEdt")
library(RWinEdt)

It works for me with a recent R-2.2.0 alpha.

Uwe Ligges


> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status   alpha          
> major    2              
> minor    2.0            
> year     2005           
> month    09             
> day      07             
> svn rev  35513          
> language R
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chrysopa at gmail.com  Wed Sep 21 14:35:30 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Wed, 21 Sep 2005 09:35:30 -0300
Subject: [R] rkward
Message-ID: <200509210935.30590.chrysopa@gmail.com>

Hi,

I'm testing rkward to use in my class. Now I'm using XEmacs, but for many 
people XEmacs is more complicated than R.

Anybody here use this program?

It use plugins, what are these plugins?

Thanks
Ronaldo
-- 
Cada escola que se abre ?? uma cadeia que se fecha.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From hchen at utmem.edu  Wed Sep 21 14:43:57 2005
From: hchen at utmem.edu (Hao Chen)
Date: Wed, 21 Sep 2005 07:43:57 -0500
Subject: [R] win.metafile on linux?
Message-ID: <20050921124357.GA6504@utmail.utmem.edu>

Dear R-help,

Is it possible to use win.metafile() on *nix versions of R?

I tried R 2.1.1 on FreeBSD and R 1.9.0 on redhat with no success. I need
to give some graphs generated in R to my boss so that he can modify them
in Powerpoint to fit he style of his presentation. Recommendations on
other methods are appreciated as well.

Hao 

-- 
: Hao Chen, Ph.D.

: Instructor 
: Department of Pharmacology
: University of Tennessee Health Science Center
: Memphis, TN 38163 USA
: Office: 901 448 3201
: Mobil:  901 826 1845 

: Mining PubMed: http://www.chilibot.net
: --



From roger.bos at gmail.com  Wed Sep 21 14:39:25 2005
From: roger.bos at gmail.com (roger bos)
Date: Wed, 21 Sep 2005 08:39:25 -0400
Subject: [R] error when loading rwinedt
In-Reply-To: <433150B8.8090108@statistik.uni-dortmund.de>
References: <0IN600GDJ0DHYT@mail.fudan.edu.cn>
	<433150B8.8090108@statistik.uni-dortmund.de>
Message-ID: <1db72680050921053910c77c31@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050921/e5c73d10/attachment.pl

From bernd.weiss at uni-koeln.de  Wed Sep 21 14:47:24 2005
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Wed, 21 Sep 2005 14:47:24 +0200
Subject: [R] Extending a data frame
Message-ID: <4331727C.26360.1978447@localhost>

Dear all,

all data I am talking about can be found at 
<http://www.metaanalyse.de/tmp/test.dat>. 

The R-code is located at <http://www.metaanalyse.de/tmp/test.R>

I'd like to plot the frequencies for status against year. 

Typing table(tmp$year) it can be simple seen that some years are 
missing, e.g. 1975-1978, 1981 etc. 

Using xyplot to display the data I do not like the fact that some of 
the years are missing (due to missing observations, of course...). 

Is there any sensible way to extend the data frame (or the table or 
xyplot?) with the years that are missing? In other words: in need a 
continuous x-axis ranging from 1974 to 2004. 

TIA,

Bernd



From 042045003 at fudan.edu.cn  Wed Sep 21 14:58:50 2005
From: 042045003 at fudan.edu.cn (ronggui.wong)
Date: Wed, 21 Sep 2005 20:58:50 +0800
Subject: [R] error when loading rwinedt
Message-ID: <0IN600JGZ3K5MN@mail.fudan.edu.cn>



>ronggui.wong wrote:
>
>> the error msg is:
>> 
>> 
>>>local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>> 
>> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>> Error in if (RWinEdtVersion) RWinEdtVersion <- scan(file.path(InstallRoot,  : 
>>         argument is of length zero
>> Error: .onAttach failed in 'attachNamespace'
>> Error: package/namespace load failed for 'RWinEdt'
>
>
>Please re-install and load RWinEdt as follows:
>
>install.packages("RWinEdt")
>library(RWinEdt)
>
>It works for me with a recent R-2.2.0 alpha.
It does Not for me.

> install.packages("RWinEdt")
--- Please select a CRAN mirror for use in this session ---
trying URL 'http://www.lmbe.seu.edu.cn/CRAN/bin/windows/contrib/2.2/RWinEdt_1.7-3.zip'
Content type 'application/x-zip-compressed' length 386098 bytes
opened URL
downloaded 377Kb

package 'RWinEdt' successfully unpacked and MD5 sums checked
Warning: cannot remove prior installation of package 'RWinEdt'

The downloaded packages are in
        C:\Documents and Settings\Bluewater\Local Settings\Temp\Rtmp23519\downloaded_packages
updating HTML package descriptions
> library(RWinEdt)
Error in library(RWinEdt) : there is no package called 'RWinEdt'

>>>version
>> 
>>          _              
>> platform i386-pc-mingw32
>> arch     i386           
>> os       mingw32        
>> system   i386, mingw32  
>> status   alpha          
>> major    2              
>> minor    2.0            
>> year     2005           
>> month    09             
>> day      07             
>> svn rev  35513          
>> language R
>> 
>> 
>> 
>> ------------------------------------------------------------------------
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

= = = = = = = = = = = = = = = = = = = =
			

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ÖÂ
Àñ£¡
 
				 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ronggui.wong
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡042045003 at fudan.edu.cn
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-09-21



From r.hankin at noc.soton.ac.uk  Wed Sep 21 15:02:52 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 21 Sep 2005 14:02:52 +0100
Subject: [R] size of subplots with par() / layout()
Message-ID: <3BF4868F-7FB3-451D-81A0-4A1DC3609EA8@soc.soton.ac.uk>

Hi

If I do this:

par(mfrow=c(2,2))
persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
persp(matrix(1:4,6,6),box=F,phi=33,theta=33)


(
or indeed

layout(matrix(1:4,2,2))
persp  . . . .
)


then the mesh plots look too small to me.  How do I make them larger?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From maechler at stat.math.ethz.ch  Wed Sep 21 15:04:03 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 21 Sep 2005 15:04:03 +0200
Subject: [R] Problem with read.spss() and as.data.frame(),
 or: alternative to subset()?
In-Reply-To: <43314188.1050209@jura.uni-hamburg.de>
References: <433082A0.6010302@jura.uni-hamburg.de>
	<43314188.1050209@jura.uni-hamburg.de>
Message-ID: <17201.23107.106266.248116@stat.math.ethz.ch>

>>>>> "Dirk" == Dirk Enzmann <dirk.enzmann at jura.uni-hamburg.de>
>>>>>     on Wed, 21 Sep 2005 13:18:32 +0200 writes:

    Dirk> The selection problem can be solved by
    Dirk> dr2000=read.spss('myfile')
    Dirk> d=lapply(dr2000,subset,dr2000$RBINZ99 > 0)

    Dirk> however, there is still the problem that R crashes when using

    Dirk> d = as.data.frame(dr2000)

which is bug in a R, or at least in your R installation.

However we can't do anything about it at the moment, because we
can't even try to do reproduce it...

So dr2000 is a list; what length() does it have?, what names() ?
what does str(dr2000) look like?

What does happen for  as.data.frame(dr2000[1:10]) ?
and '100' or '1000' instead of '10'?

Maybe try to find a small version of 'dr2000' which still has
the problem, and show us that one,
e.g. by making it available via http://... if it is still large,
otherwise (if it's small), maybe even posting the result of
dump(..).

Regards,
Martin



From chrysopa at gmail.com  Wed Sep 21 15:06:38 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Wed, 21 Sep 2005 10:06:38 -0300
Subject: [R] Doubt about Sweave
Message-ID: <200509211006.38299.chrysopa@gmail.com>

Hi,

At this moment, I make my R output using comments and latex tags in R.
Something like this:

### make a plot
plot(y~x)
dev.copy2eps(file="plot.eps")
# @\includegraphics[width=\linewidth]{plot}@
...

After I include the file saved with this code in a latex document using the 
listings package.

It work.

I reading about Sweave and it make a good output. But all example is made with 
R commands mades in a file. Is possible to make an output with sweave 
interactively in R and after the analysis end export the latex code to a 
file?

Thanks
Ronaldo
-- 
And on the seventh day, He exited from append mode.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From maechler at stat.math.ethz.ch  Wed Sep 21 15:10:16 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 21 Sep 2005 15:10:16 +0200
Subject: [R] ts.intersect bug?
In-Reply-To: <b0808fdc05092104595e80f242@mail.gmail.com>
References: <b0808fdc05092104595e80f242@mail.gmail.com>
Message-ID: <17201.23480.534481.260305@stat.math.ethz.ch>

>>>>> "AntonioFDN" == Antonio, Fabio Di Narzo <antonio.fabio at gmail.com>
>>>>>     on Wed, 21 Sep 2005 13:59:26 +0200 writes:

    AntonioFDN> This code gives an error:

    AntonioFDN> a <- ts(1:10, start=0, freq=10)
    AntonioFDN> b <- ts(1:10, start=1, freq=10)
    AntonioFDN> ts.intersect(a,b)

No, it gives a *warning* and returns NULL.

Why should it not?  a and b  have are not overlapping,
ie. *have* intersection NULL :

  plot(c(0,2), c(0,10), type ="n") ; lines(a); lines(b,col=2)


I think you misunderstand what 'freq' means in the current
context.

    AntonioFDN> This one works normally (and correctly):
    AntonioFDN> a <- ts(1:10, start=0)
    AntonioFDN> b <- ts(1:10, start=1)
    AntonioFDN> ts.intersect(a,b)

    AntonioFDN> Antonio, Fabio Di Narzo.

    AntonioFDN> P.S. How to switch off italian error messages to post on r-help?

    >> version
    AntonioFDN> _
    AntonioFDN> platform i386-pc-mingw32
    AntonioFDN> arch     i386
    AntonioFDN> os       mingw32
    AntonioFDN> system   i386, mingw32
    AntonioFDN> status
    AntonioFDN> major    2
    AntonioFDN> minor    1.0
    AntonioFDN> year     2005
    AntonioFDN> month    04
    AntonioFDN> day      18
    AntonioFDN> language R

    AntonioFDN> ______________________________________________
    AntonioFDN> R-help at stat.math.ethz.ch mailing list
    AntonioFDN> https://stat.ethz.ch/mailman/listinfo/r-help
    AntonioFDN> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Sep 21 15:11:47 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Sep 2005 09:11:47 -0400
Subject: [R] error when loading rwinedt
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED46D@usctmx1106.merck.com>

You cannot install over a package that is currently loaded.  Quit R and
restart without loading the RWinEdt package and try again.

Andy

> From: ronggui.wong
> 
> >ronggui.wong wrote:
> >
> >> the error msg is:
> >> 
> >> 
> >>>local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> >> 
> >> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> >> Error in if (RWinEdtVersion) RWinEdtVersion <- 
> scan(file.path(InstallRoot,  : 
> >>         argument is of length zero
> >> Error: .onAttach failed in 'attachNamespace'
> >> Error: package/namespace load failed for 'RWinEdt'
> >
> >
> >Please re-install and load RWinEdt as follows:
> >
> >install.packages("RWinEdt")
> >library(RWinEdt)
> >
> >It works for me with a recent R-2.2.0 alpha.
> It does Not for me.
> 
> > install.packages("RWinEdt")
> --- Please select a CRAN mirror for use in this session ---
> trying URL 
> 'http://www.lmbe.seu.edu.cn/CRAN/bin/windows/contrib/2.2/RWinE
dt_1.7-3.zip'
Content type 'application/x-zip-compressed' length 386098 bytes
opened URL
downloaded 377Kb

package 'RWinEdt' successfully unpacked and MD5 sums checked
Warning: cannot remove prior installation of package 'RWinEdt'

The downloaded packages are in
        C:\Documents and Settings\Bluewater\Local
Settings\Temp\Rtmp23519\downloaded_packages
updating HTML package descriptions
> library(RWinEdt)
Error in library(RWinEdt) : there is no package called 'RWinEdt'

>>>version
>> 
>>          _              
>> platform i386-pc-mingw32
>> arch     i386           
>> os       mingw32        
>> system   i386, mingw32  
>> status   alpha          
>> major    2              
>> minor    2.0            
>> year     2005           
>> month    09             
>> day      07             
>> svn rev  35513          
>> language R
>> 
>> 
>> 
>> ------------------------------------------------------------------------
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.
html
>

= = = = = = = = = = = = = = = = = = = =
			

?????????
??
 
				 
????????ronggui.wong
????????042045003 at fudan.edu.cn
??????????2005-09-21



From pgilbert at bank-banque-canada.ca  Wed Sep 21 15:14:17 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 21 Sep 2005 09:14:17 -0400
Subject: [R] about set.seed
In-Reply-To: <200509210646.j8L6kDVU028810@rm-rstar.sfu.ca>
References: <200509210646.j8L6kDVU028810@rm-rstar.sfu.ca>
Message-ID: <43315CA9.7010605@bank-banque-canada.ca>

Linda

It is possible to reproduce the same uniform and normal random numbers 
on different architecture, and with all versions of R, with some 
caveats. (In R-0.99 there were some fairly big changes, in R-1.7.0 the 
default generator changed, and in 1.7.1 a bug was fixed in the 
Kindermann-Ramage generator.) I regularly test this on Solaris and a few 
Linux variants myself, and my packages have tests, so it is checked on 
all platforms on which R packages are tested. I have not tested 
bivariate normal or other distributions, so it is possible there is a 
problem, but it seems unlikely to me.

In very long simulations there can be small differences in accumulated 
errors, but you have to try hard to find these. If the difference you 
see is easily noticeable then something else is different. To reproduce 
random numbers you need more information than the seed. You also need to 
be sure you have the same uniform generator (kind) and the same 
transformation to your distribution (e.g. normal.kind). I believe the 
distributed R defaults are the same on all architectures, but you or the 
person that installed R may have set something differently in the 
various things that are run when R starts. Also you do have to be 
careful to set the seed properly.

There are some utilities in the package setRNG which help save and set 
information you need to reproduce random numbers.  The function 
random.number.test() in that package will do some simple tests of 
uniform and normal random numbers. It invisibly returns TRUE if  the 
tests are ok, and stops with a "failed" message otherwise. You might 
look at the various examples in that function, and the documentation for 
the package, if you want to reproduce your random numbers.

Paul Gilbert

nlei at sfu.ca wrote:
> Hi there,
> 
> I have some question about set.seed these days which I may need your help.
> 
> I'm working on some sampling project which need to generate random numbers
> by some distributions(like bivariate normal). I noticed that in different
> computers to run my code,I got very significant different results. Then I
> started to try set.seed command which I wish to see by fixing the same
> random numbers what can happen. But it turned out when I run my codes in two
> computers I still can get different results. I tried different computers.
> Some of them I can get the same results but some not. A special case is I
> run in two computers which have R version 2.0.1 and 2.1.1 separetely. And
> the results are different. 
> 
> Now my question is what's the reason can make the different result happen
> when we run the same codes by setting the same seed?
> 
> This will help me a lot with my project.Thank you very much!
> 
> Linda
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bernd.weiss at uni-koeln.de  Wed Sep 21 15:24:22 2005
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Wed, 21 Sep 2005 15:24:22 +0200
Subject: [R] Extending a data frame
In-Reply-To: <43315DA2.9070704@optonline.net>
References: <4331727C.26360.1978447@localhost>
Message-ID: <43317B26.6839.1B95EC8@localhost>

On 21 Sep 2005 at 9:18, Chuck Cleland wrote:

> Bernd:
>    I think you will get what you want if you convert year from a
>    factor 
> to numeric:
> 
> library(lattice)
> tmp <- read.table("http://www.metaanalyse.de/tmp/test.dat")
> xyplot(Freq~as.numeric(as.character(year)),group=status,data=data.fram
> e(table(tmp)))


Dear Chuck,

yes, that's it! 

Thank's a lot, 

Bernd 


-- 
Bernd Weiss, M.A.
University of Cologne / Research Institute for Sociology
Greinstr. 2 / D-50939 Cologne / Germany
E-Mail: <bernd.weiss at uni-koeln.de>
URL: <http://www.uni-koeln.de/wiso-fak/fisoz/>



From ripley at stats.ox.ac.uk  Wed Sep 21 15:29:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Sep 2005 14:29:14 +0100 (BST)
Subject: [R] user ignored an informative warning message (was
 ts.intersect bug?)
In-Reply-To: <b0808fdc05092104595e80f242@mail.gmail.com>
References: <b0808fdc05092104595e80f242@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0509211417240.29410@gannet.stats>

On Wed, 21 Sep 2005, Antonio, Fabio Di Narzo claimed:

> This code gives an error:
>
> a <- ts(1:10, start=0, freq=10)
> b <- ts(1:10, start=1, freq=10)
> ts.intersect(a,b)

In reality, it gives a helpful warning:

> ts.intersect(a,b)
NULL
Warning message:
non-intersecting series in: .cbind.ts(list(...), .makeNamesTs(...), dframe 
= dframe, union = FALSE)

Your two series have no time points in common, see

> time(a)
Time Series:
Start = c(0, 1)
End = c(0, 10)
Frequency = 10
  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
> time(b)
Time Series:
Start = c(1, 1)
End = c(1, 10)
Frequency = 10
  [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9

or try

> ts.union(a,b)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Konrad.Bogner at lfu.bayern.de  Wed Sep 21 15:32:03 2005
From: Konrad.Bogner at lfu.bayern.de (Bogner, Konrad (LfU))
Date: Wed, 21 Sep 2005 15:32:03 +0200
Subject: [R] cdecl and stdcall
Message-ID: <C373BB6AB83DD74484F4BCD3DA60F90E190317@S-STMLU0006>

Hi,
I'm trying to load a dynamic link library and it seems to work (is.loaded  -> TRUE). When I run the function, which calls the .Fortran subroutine, R crashes!
I'v tried the same in S-Plus 2000 and it worked. Therefore I suppose that the dll has been compiled with the stdcall calling convention (and not cdecl). But the problem is that I don't have access to the source code, I've just the dll without any working import library. Maybe someone could be so kind and send me an example how to write a wrapper? I've found one example at the NAG's site (http://www.nag.com/numeric/RunderWindows.asp), but it didn't work. At http://www.cygwin.com/cygwin-ug-net/dll.html there is a description of linking against dlls and how to create a def and an import libraray file, but unfortunately my dll seems to be stripped, because I get the error message "no symbols" after running  the command "nm".

I'm using R 2.1.0 (windows xp) and CYGWIN (gnu compilers,..)

Konrad



From ripley at stats.ox.ac.uk  Wed Sep 21 15:35:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Sep 2005 14:35:43 +0100 (BST)
Subject: [R] size of subplots with par() / layout()
In-Reply-To: <3BF4868F-7FB3-451D-81A0-4A1DC3609EA8@soc.soton.ac.uk>
References: <3BF4868F-7FB3-451D-81A0-4A1DC3609EA8@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.61.0509211432500.29410@gannet.stats>

On Wed, 21 Sep 2005, Robin Hankin wrote:

> If I do this:
>
> par(mfrow=c(2,2))
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
>
>
> (
> or indeed
>
> layout(matrix(1:4,2,2))
> persp  . . . .
> )
>
>
> then the mesh plots look too small to me.  How do I make them larger?

By reducing the size of the plot margins, which are taking up a lot of 
the space and you are not using.  Use par's mai or mar, and see `An 
Introduction to R' to understand plot layouts.

The effect is seen with just a single version of your example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From antonio.fabio at gmail.com  Wed Sep 21 15:39:34 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Wed, 21 Sep 2005 15:39:34 +0200
Subject: [R] Fwd:  ts.intersect bug?
In-Reply-To: <b0808fdc05092106347136c873@mail.gmail.com>
References: <b0808fdc05092104595e80f242@mail.gmail.com>
	<17201.23480.534481.260305@stat.math.ethz.ch>
	<b0808fdc05092106347136c873@mail.gmail.com>
Message-ID: <b0808fdc0509210639113da0ce@mail.gmail.com>

---------- Forwarded message ----------
From: Antonio, Fabio Di Narzo <antonio.fabio at gmail.com>
Date: 21-set-2005 15.34
Subject: Re: [R] ts.intersect bug?
To: Martin Maechler <maechler at stat.math.ethz.ch>


2005/9/21, Martin Maechler <maechler at stat.math.ethz.ch>:
> >>>>> "AntonioFDN" == Antonio, Fabio Di Narzo <antonio.fabio at gmail.com>
> >>>>>     on Wed, 21 Sep 2005 13:59:26 +0200 writes:
>
>     AntonioFDN> This code gives an error:
>
>     AntonioFDN> a <- ts(1:10, start=0, freq=10)
>     AntonioFDN> b <- ts(1:10, start=1, freq=10)
>     AntonioFDN> ts.intersect(a,b)
>
> No, it gives a *warning* and returns NULL.

Ooops, bad example.
Try this instead:
a <- ts(runif(6500), start=0, freq=10)
b <- lag(a, 1)
c <- ts.intersect(a, b)

Gives an error from .cbind.ts



From rxg218 at psu.edu  Wed Sep 21 16:00:05 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Wed, 21 Sep 2005 10:00:05 -0400
Subject: [R] R , Java & DCOM
Message-ID: <1127311205.31014.23.camel@blue.chem.psu.edu>

Hi,
 I am working on a Java project that will run primarily on Windows. As
part of the project I would like to use the R engine.

However, from what I have seen on this list & the SJava list, installing
SJava on Windows can be troublesome.

As a result I was wondering if anybody had used DCOM to access R from a
Java program on Windows (if it is all possible).

Any pointers to documents and/or examples would be appreciated.

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Gravity brings me down.



From ligges at statistik.uni-dortmund.de  Wed Sep 21 16:03:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Sep 2005 16:03:24 +0200
Subject: [R] error when loading rwinedt
In-Reply-To: <1db72680050921053910c77c31@mail.gmail.com>
References: <0IN600GDJ0DHYT@mail.fudan.edu.cn>	
	<433150B8.8090108@statistik.uni-dortmund.de>
	<1db72680050921053910c77c31@mail.gmail.com>
Message-ID: <4331682C.8000808@statistik.uni-dortmund.de>

roger bos wrote:

> I too have had problems with getting RWinEdt to work, I tried the above
> command lines and got the following error:
> 
> 
>>install.packages("RWinEdt")
> 
> --- Please select a CRAN mirror for use in this session ---
> trying URL '
> http://cran.us.r-project.org/bin/windows/contrib/2.1/RWinEdt_1.7-3.zip'
> Content type 'application/zip' length 386098 bytes
> opened URL
> downloaded 377Kb
> 
> package 'RWinEdt' successfully unpacked and MD5 sums checked
> 
> The downloaded packages are in
> C:\Documents and Settings\bosr\Local
> Settings\Temp\Rtmp12772\downloaded_packages
> updating HTML package descriptions
> 
>>library(RWinEdt)
> 
> Error in getWinEdt() :
> WinEdt is not installed properly.

So, what version of WinEdt are you using, where is it installed and what 
are the corresponding registry entries?

Uwe Ligges

> Either reinstall WinEdt or install R-WinEdt manually as described in the
> ReadMe
> Error: .onAttach failed in 'attachNamespace'
> Error: package/namespace load failed for 'RWinEdt'
> Does anyone have any suggestions?
> 
>  On 9/21/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>ronggui.wong wrote:
>>
>>
>>>the error msg is:
>>>
>>>
>>>
>>>>local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>>>
>>>+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
>>>Error in if (RWinEdtVersion) RWinEdtVersion <- scan(file.path(InstallRoot,
>>
>>:
>>
>>>argument is of length zero
>>>Error: .onAttach failed in 'attachNamespace'
>>>Error: package/namespace load failed for 'RWinEdt'
>>
>>
>>Please re-install and load RWinEdt as follows:
>>
>>install.packages("RWinEdt")
>>library(RWinEdt)
>>
>>It works for me with a recent R-2.2.0 alpha.
>>
>>Uwe Ligges
>>
>>
>>
>>>
>>>>version
>>>
>>>_
>>>platform i386-pc-mingw32
>>>arch i386
>>>os mingw32
>>>system i386, mingw32
>>>status alpha
>>>major 2
>>>minor 2.0
>>>year 2005
>>>month 09
>>>day 07
>>>svn rev 35513
>>>language R
>>>
>>>
>>>
>>>------------------------------------------------------------------------
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
>



From dieter.menne at menne-biomed.de  Wed Sep 21 15:57:58 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 21 Sep 2005 13:57:58 +0000 (UTC)
Subject: [R] size of subplots with par() / layout()
References: <3BF4868F-7FB3-451D-81A0-4A1DC3609EA8@soc.soton.ac.uk>
Message-ID: <loom.20050921T155710-879@post.gmane.org>

Robin Hankin <r.hankin <at> noc.soton.ac.uk> writes:

> 
> par(mfrow=c(2,2))
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
... 
> then the mesh plots look too small to me.  How do I make them larger?

Try 

par(mfrow=c(2,2),mar=c(0,0,0,0)) 

Dieter



From 042045003 at fudan.edu.cn  Wed Sep 21 16:03:31 2005
From: 042045003 at fudan.edu.cn (ronggui.wong)
Date: Wed, 21 Sep 2005 22:03:31 +0800
Subject: [R] error when loading rwinedt
Message-ID: <0IN600ITI6JZCQ@mail.fudan.edu.cn>

> install.packages("RWinEdt")
--- Please select a CRAN mirror for use in this session ---
trying URL 'http://www.lmbe.seu.edu.cn/CRAN/bin/windows/contrib/2.2/RWinEdt_1.7-3.zip'
Content type 'application/x-zip-compressed' length 386098 bytes
opened URL
downloaded 377Kb

package 'RWinEdt' successfully unpacked and MD5 sums checked

The downloaded packages are in
        C:\Documents and Settings\Bluewater\Local Settings\Temp\Rtmp3392\downloaded_packages
updating HTML package descriptions
> library(RWinEdt)
Error in if (RWinEdtVersion) RWinEdtVersion <- scan(file.path(InstallRoot,  : 
        argument is of length zero
Error: .onAttach failed in 'attachNamespace'
Error: package/namespace load failed for 'RWinEdt'


	

======= 2005-09-21 21:11:47 ÄúÔÚÀ´ÐÅÖÐÐ´µÀ£º=======

>You cannot install over a package that is currently loaded.  Quit R and
>restart without loading the RWinEdt package and try again.


have done as you suggest,but still errors.

>Andy
>
>> From: ronggui.wong
>> 
>> >ronggui.wong wrote:
>> >
>> >> the error msg is:
>> >> 
>> >> 
>> >>>local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>> >> 
>> >> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>> >> Error in if (RWinEdtVersion) RWinEdtVersion <- 
>> scan(file.path(InstallRoot,  : 
>> >>         argument is of length zero
>> >> Error: .onAttach failed in 'attachNamespace'
>> >> Error: package/namespace load failed for 'RWinEdt'
>> >
>> >
>> >Please re-install and load RWinEdt as follows:
>> >
>> >install.packages("RWinEdt")
>> >library(RWinEdt)
>> >
>> >It works for me with a recent R-2.2.0 alpha.
>> It does Not for me.
>> 
>> > install.packages("RWinEdt")
>> --- Please select a CRAN mirror for use in this session ---
>> trying URL 
>> 'http://www.lmbe.seu.edu.cn/CRAN/bin/windows/contrib/2.2/RWinE
>dt_1.7-3.zip'
>Content type 'application/x-zip-compressed' length 386098 bytes
>opened URL
>downloaded 377Kb
>
>package 'RWinEdt' successfully unpacked and MD5 sums checked
>Warning: cannot remove prior installation of package 'RWinEdt'
>
>The downloaded packages are in
>        C:\Documents and Settings\Bluewater\Local
>Settings\Temp\Rtmp23519\downloaded_packages
>updating HTML package descriptions
>> library(RWinEdt)
>Error in library(RWinEdt) : there is no package called 'RWinEdt'
>
>>>>version
>>> 
>>>          _              
>>> platform i386-pc-mingw32
>>> arch     i386           
>>> os       mingw32        
>>> system   i386, mingw32  
>>> status   alpha          
>>> major    2              
>>> minor    2.0            
>>> year     2005           
>>> month    09             
>>> day      07             
>>> svn rev  35513          
>>> language R
>>> 
>>> 
>>> 
>>> ------------------------------------------------------------------------
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.
>html
>>
>
>= = = = = = = = = = = = = = = = = = = =
>			
>
>$B!!!!!!!!!!!!!!!!CW(J
>$BNi!*(J
> 
>				 
>$B!!!!!!!!!!!!!!!!(Jronggui.wong
>$B!!!!!!!!!!!!!!!!(J042045003 at fudan.edu.cn
>$B!!!!!!!!!!!!!!!!!!!!(J2005-09-21
>
>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments...{{dropped}}



From ligges at statistik.uni-dortmund.de  Wed Sep 21 16:04:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Sep 2005 16:04:46 +0200
Subject: [R] win.metafile on linux?
In-Reply-To: <20050921124357.GA6504@utmail.utmem.edu>
References: <20050921124357.GA6504@utmail.utmem.edu>
Message-ID: <4331687E.4070207@statistik.uni-dortmund.de>

Hao Chen wrote:

> Dear R-help,
> 
> Is it possible to use win.metafile() on *nix versions of R?

No.

> I tried R 2.1.1 on FreeBSD and R 1.9.0 on redhat with no success. I need
> to give some graphs generated in R to my boss so that he can modify them
> in Powerpoint to fit he style of his presentation. Recommendations on
> other methods are appreciated as well.

There were some discussions on this topic during the last year. Please 
check the mailing list archives.

Uwe Ligges


> Hao 
>



From ligges at statistik.uni-dortmund.de  Wed Sep 21 16:06:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Sep 2005 16:06:09 +0200
Subject: [R] size of subplots with par() / layout()
In-Reply-To: <3BF4868F-7FB3-451D-81A0-4A1DC3609EA8@soc.soton.ac.uk>
References: <3BF4868F-7FB3-451D-81A0-4A1DC3609EA8@soc.soton.ac.uk>
Message-ID: <433168D1.6080300@statistik.uni-dortmund.de>

Robin Hankin wrote:

> Hi
> 
> If I do this:
> 
> par(mfrow=c(2,2))

Insert:
  par(mar=rep(0,4))

Uwe Ligges

> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
> persp(matrix(1:4,6,6),box=F,phi=33,theta=33)
> 
> 
> (
> or indeed
> 
> layout(matrix(1:4,2,2))
> persp  . . . .
> )
> 
> 
> then the mesh plots look too small to me.  How do I make them larger?
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bernarduse1 at yahoo.fr  Wed Sep 21 16:07:13 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Wed, 21 Sep 2005 16:07:13 +0200 (CEST)
Subject: [R] Identical  data frames
Message-ID: <20050921140713.16909.qmail@web25805.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050921/32e77a90/attachment.pl

From sfalcon at fhcrc.org  Wed Sep 21 16:26:06 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 21 Sep 2005 07:26:06 -0700
Subject: [R] SAX Parser best practise
In-Reply-To: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E75F@EMAIL.mpimp-golm.mpg.de>
	(Jan Hummel's message of "Wed, 21 Sep 2005 08:43:41 +0200")
References: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E75F@EMAIL.mpimp-golm.mpg.de>
Message-ID: <m2psr2pke9.fsf@macaroni.local>

Hi Jan,

On 20 Sep 2005, Hummel at mpimp-golm.mpg.de wrote:
> I have a question regarding best practise in setting up a XML parser
> within R. [snip]
> value="445.598999"/> (as one can see in the xml snip set)

I missed the xml snip, but I think I get the gist of your question.

> I know the mechanism of using Event Handlers, as shown in the
> examples.  But what I'm looking for is, how can I use some "path
> information" as mentioned in "addContext" parameter of
> xmlEventParse()? May somebody share a example using "addContext =
> TRUE" and pointing me to the variables I may use if I implement the
> "..." parameter within my handlers.
>
> Do I have to implement a "status machine" using some variables
> within my handlers, or would one prefer to use the "state" parameter
> of xmlEventParse()?

I'm not familiar with the addContext arg and don't know whether or not
that provides another solution to your problem.

I do know that you can do what you want by writing "state machine"
code.  I played a little with using the state arg for this purpose,
but ran into some problems (sorry, no details in my memory banks).

There is an example of the state approach in Bioconductor's AnnBuilder
package.  See R/GO.R.  It isn't the prettiest or best example, but
maybe it will help get you going.

The general approach is to use '<<-' to reach up a level and set the
state variables from inside the tag handlers.

HTH,

+ seth



From spencer.graves at pdf.com  Wed Sep 21 16:27:06 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 21 Sep 2005 09:27:06 -0500
Subject: [R] FDR analyses: minimum number of features
In-Reply-To: <C6E7B149DFD87D4AAD1ADA90B5DE0279493ACE@mailbe11.mc.vanderbilt.edu>
References: <C6E7B149DFD87D4AAD1ADA90B5DE0279493ACE@mailbe11.mc.vanderbilt.edu>
Message-ID: <43316DBA.7060204@pdf.com>

	  Two thoughts on this:

	  1.  Your FDR (Not Franklin Delano Roosevelt) sounds like another name 
for Type I error rate.  The definition of "reasonably reliable FDRs" 
would seem to relate to the status of the literature on this issue among 
researchers in genotyping.  As more reports of FRDs in genotyping are 
published, I would expect that methodology for estimation and the 
standard for accuracy would similarly evolve.

	  2.  Have you tried the Bioconductor (www.bioconductor.org/) 
listserve?  They might be able to say something more useful than a 
general list like this.

	  spencer graves

Dupont, William wrote:

> Dear List,
> 
> We are planning a genotyping study to be analyzed using false discovery
> rates (FDRs) (See Storey and Tibshirani PNAS 2003; 100:9440-5).  I am
> interested in learning if there is any consensus as to how many
> features (ie. how many P values) need to be studied before reasonably
> reliable FDRs can be derived.  Does anyone know of a citation where
> this is discussed?
> 
> Bill Dupont 
> 
> William D. Dupont          phone: 615-343-4100          URL
> http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/WilliamDupont
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Wed Sep 21 16:31:22 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 21 Sep 2005 09:31:22 -0500
Subject: [R] factor as seq() in for loop
In-Reply-To: <1127151294.432ef6beb14d7@www2.helsinki.fi>
References: <1127151294.432ef6beb14d7@www2.helsinki.fi>
Message-ID: <43316EBA.3060206@pdf.com>

	  Does the follow help?

 > aFactor <- factor(rep(letters[1:3], 1:3))
 > aFactor
[1] a b b c c c
Levels: a b c
 > for(af in levels(aFactor)){
+   print(sum(aFactor==af))
+ }
[1] 1
[1] 2
[1] 3

	  spencer graves

Tord Snall wrote:

> Dear all,
> I would like to use the values in vegaggr.BLMCMR02$colony 
> 
> str(vegaggr.BLMCMR02)
> `data.frame':   1678 obs. of  3 variables:
> $ vegtype       : Factor w/ 27 levels "2010","2020",..: 3 4 5 19 4 5 19 5  
> $ colony        : Factor w/ 406 levels "0","1","10","100",..: 1 1 1 1 2 2 2 
> $ Totvegproparea: num  0.00055 0.03956 0.95705 0.00284 0.05215 ...
> 
> as the seq in a for loop (for(var in seq)), and therefore first picked them 
> out as 
> cols = aggregate(list(A = vegaggr.BLMCMR02$Totvegproparea), 
>                 list(colony = vegaggr.BLMCMR02$colony), sum)[,1]
> 
>>str(cols)
> 
>  Factor w/ 406 levels "0","1","10","100",..: 1 2 3 4 5 6 7 8 9 10 ...
> 
> I next planned to transform cols using as.integer(cols). However, this 
> transformation gives a vector corresponding to seq(from=1,to=length(cols)). 
> 
> Could someone please give advice on how to make use of a factor as the seq 
> in a for loop (which is strictly not allowed' according to ?Control).
> 
> Thanks!
> 
> Tord
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Wed Sep 21 16:48:31 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Sep 2005 10:48:31 -0400
Subject: [R] Fwd: ts.intersect bug?
In-Reply-To: <b0808fdc0509210639113da0ce@mail.gmail.com>
References: <b0808fdc05092104595e80f242@mail.gmail.com>
	<17201.23480.534481.260305@stat.math.ethz.ch>
	<b0808fdc05092106347136c873@mail.gmail.com>
	<b0808fdc0509210639113da0ce@mail.gmail.com>
Message-ID: <971536df050921074842fdc91b@mail.gmail.com>

On 9/21/05, Antonio, Fabio Di Narzo <antonio.fabio at gmail.com> wrote:
> ---------- Forwarded message ----------
> From: Antonio, Fabio Di Narzo <antonio.fabio at gmail.com>
> Date: 21-set-2005 15.34
> Subject: Re: [R] ts.intersect bug?
> To: Martin Maechler <maechler at stat.math.ethz.ch>
>
>
> 2005/9/21, Martin Maechler <maechler at stat.math.ethz.ch>:
> > >>>>> "AntonioFDN" == Antonio, Fabio Di Narzo <antonio.fabio at gmail.com>
> > >>>>>     on Wed, 21 Sep 2005 13:59:26 +0200 writes:
> >
> >     AntonioFDN> This code gives an error:
> >
> >     AntonioFDN> a <- ts(1:10, start=0, freq=10)
> >     AntonioFDN> b <- ts(1:10, start=1, freq=10)
> >     AntonioFDN> ts.intersect(a,b)
> >
> > No, it gives a *warning* and returns NULL.
>
> Ooops, bad example.
> Try this instead:
> a <- ts(runif(6500), start=0, freq=10)
> b <- lag(a, 1)
> c <- ts.intersect(a, b)
>
> Gives an error from .cbind.ts

A workaround would be to convert both a and b to zoo class,
intersect them using zoo's merge and convert back to ts:

library(zoo)
z <- as.ts(merge(as.zoo(a), as.zoo(b), all = FALSE))

Regarding your other question,

Sys.putenv(LANGUAGE="en")

will result in error messages in English.



From pwolf at wiwi.uni-bielefeld.de  Wed Sep 21 16:49:10 2005
From: pwolf at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Wed, 21 Sep 2005 16:49:10 +0200
Subject: [R]  bagplot - a rough R function
In-Reply-To: <16715.4933.392083.824916@gargle.gargle.HOWL>
References: <web-1865915@calmail-be1.berkeley.edu>
	<Pine.LNX.4.44.0409131645200.19598-100000@gannet.stats>
	<16715.4933.392083.824916@gargle.gargle.HOWL>
Message-ID: <433172E6.70901@wiwi.uni-bielefeld.de>

In the moment I am writing an R function for drawing
bagplots (two dimensional boxplots).
For some elements of the plot are found numerically
the resulting plots differs a little bit from the correct bagplots.

Here is the link to the actual version:

http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/bagplot/bagplot.R 
( R code )

http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/bagplot/bagplot.pdf 
(examples and description)

Now I am looking for examples to test / to improve my function.
Where can I get data sets and the corresponding correct bagplots?

Thanks

Peter Wolf

(pwolf at wiwi.uni-bielefeld.de)



From snvk4u at gmail.com  Wed Sep 21 16:53:41 2005
From: snvk4u at gmail.com (Krishna)
Date: Wed, 21 Sep 2005 20:23:41 +0530
Subject: [R] MGARCH estimation
Message-ID: <139ef1c20509210753d017b86@mail.gmail.com>

Hi R-users

Can the users let me know how to do MGARCH estimate (Bivariate GARCH)
and volatility forecast for 2 variables in R.

thanks and regards

snvk



From 042045003 at fudan.edu.cn  Wed Sep 21 16:54:19 2005
From: 042045003 at fudan.edu.cn (ronggui.wong)
Date: Wed, 21 Sep 2005 22:54:19 +0800
Subject: [R] error when loading rwinedt
Message-ID: <0IN600KKT8WNP7@mail.fudan.edu.cn>


>So, what version of WinEdt are you using, where is it installed and what 
>are the corresponding registry entries?

Thank you!

my winedt's location is not the standard one which cause the error.and i follow the install guide and make it works.    the following is the guide.


 b) Alternative: Manual installation procedure

    - Unzip the archive and copy the sub-directory PlugIn into directory 
      ....\winedt\plugins
      (e.g. "c:\program files\winedt team\winedt\plugins\PlugIn")

    - In your Windows-Explorer double-click on "install.bat".

    - If you have a personal profile for WinEdt, you might need to copy R.* into this
      personal profile folder, as well as send2R.edt.

    - If you are running RGui in single windows style (SDI), the default mode should be 
      unchanged. For RGui in multiple window style (MDI) or if you want to use R-WinEdt with 
      S-PLUS, you can change the R-WinEdt mode permanently(!) using the Menu, e.g.:  
      R -> Set R --mdi mode
      Note that MDI mode is ONLY supported for english versions of RGui!

    - To invoke R-WinEdt from startmenu or desktop create a shortcut to WinEdt as follows:
      "c:\program files\winedt team\winedt\winedt" -C="R-WinEdt" -e=r.ini
      There is also an icon "R-WinEdt.ico", designed by Erich Neuwirth, available in 
      the archive to prettify the shortcut.

    - In R use something like (for example in your .Rprofile):
       options(editor="\"c:/program files/winedt team/winedt/winedt\" -c=\"R-WinEdt-edit\" -e=r.ini -V") 
       options(pager="\"c:/program files/winedt team/winedt/winedt\" -C=\"R-WinEdt\" -e=r.ini -V")


    Used WinEdt parameters (Remark: There is a difference between -c and -C !):
        -c="name": New instance of WinEdt called "name" will be started.
        -C="name": If an instance "name" of WinEdt is already running, it will be used.
                   So you can run WinEdt as LaTeX and R editor at the same time.
        -e="name": Using "name" as initialization-file.
        -V       : Running in "virgin"-mode



From antonio.fabio at gmail.com  Wed Sep 21 16:55:29 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Wed, 21 Sep 2005 16:55:29 +0200
Subject: [R] Fwd: ts.intersect bug?
In-Reply-To: <971536df050921074842fdc91b@mail.gmail.com>
References: <b0808fdc05092104595e80f242@mail.gmail.com>
	<17201.23480.534481.260305@stat.math.ethz.ch>
	<b0808fdc05092106347136c873@mail.gmail.com>
	<b0808fdc0509210639113da0ce@mail.gmail.com>
	<971536df050921074842fdc91b@mail.gmail.com>
Message-ID: <b0808fdc05092107554aaeae20@mail.gmail.com>

2005/9/21, Gabor Grothendieck <ggrothendieck at gmail.com>:
> On 9/21/05, Antonio, Fabio Di Narzo <antonio.fabio at gmail.com> wrote:
> > ---------- Forwarded message ----------
> > From: Antonio, Fabio Di Narzo <antonio.fabio at gmail.com>
> > Date: 21-set-2005 15.34
> > Subject: Re: [R] ts.intersect bug?
> > To: Martin Maechler <maechler at stat.math.ethz.ch>
> >
> >
> > 2005/9/21, Martin Maechler <maechler at stat.math.ethz.ch>:
> > > >>>>> "AntonioFDN" == Antonio, Fabio Di Narzo <antonio.fabio at gmail.com>
> > > >>>>>     on Wed, 21 Sep 2005 13:59:26 +0200 writes:
> > >
> > >     AntonioFDN> This code gives an error:
> > >
> > >     AntonioFDN> a <- ts(1:10, start=0, freq=10)
> > >     AntonioFDN> b <- ts(1:10, start=1, freq=10)
> > >     AntonioFDN> ts.intersect(a,b)
> > >
> > > No, it gives a *warning* and returns NULL.
> >
> > Ooops, bad example.
> > Try this instead:
> > a <- ts(runif(6500), start=0, freq=10)
> > b <- lag(a, 1)
> > c <- ts.intersect(a, b)
> >
> > Gives an error from .cbind.ts
>
> A workaround would be to convert both a and b to zoo class,
> intersect them using zoo's merge and convert back to ts:
>
> library(zoo)
> z <- as.ts(merge(as.zoo(a), as.zoo(b), all = FALSE))
>
> Regarding your other question,
>
> Sys.putenv(LANGUAGE="en")
>
> will result in error messages in English.
>

Many tnx, and again sorry for the starting bad example.
Will the ts.intersect bug addressed in next R release?

Antonio, Fabio Di Narzo.

P.S. A little OT: shouldn't the "Sys.putenv(LANGUAGE="en")" tip be put
in the posting guide?



From matthew_wiener at merck.com  Wed Sep 21 16:57:29 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 21 Sep 2005 10:57:29 -0400
Subject: [R] R , Java & DCOM
Message-ID: <4E9A692D8755DF478B56A2892388EE1F06B792@usctmx1118.merck.com>

This is not  a direct answer to your question.  But Rserve
(http://stats.math.uni-augsburg.de/Rserve/) provides another possible
approach.  We've had good success with it.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rajarshi Guha
Sent: Wednesday, September 21, 2005 10:00 AM
To: R
Subject: [R] R , Java & DCOM


Hi,
 I am working on a Java project that will run primarily on Windows. As
part of the project I would like to use the R engine.

However, from what I have seen on this list & the SJava list, installing
SJava on Windows can be troublesome.

As a result I was wondering if anybody had used DCOM to access R from a
Java program on Windows (if it is all possible).

Any pointers to documents and/or examples would be appreciated.

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Gravity brings me down.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bolker at ufl.edu  Wed Sep 21 17:02:18 2005
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 21 Sep 2005 15:02:18 +0000 (UTC)
Subject: [R] win.metafile on linux?
References: <20050921124357.GA6504@utmail.utmem.edu>
Message-ID: <loom.20050921T165217-346@post.gmane.org>

Hao Chen <hchen <at> utmem.edu> writes:

> 
> Dear R-help,
> 
> Is it possible to use win.metafile() on *nix versions of R?
> 
> I tried R 2.1.1 on FreeBSD and R 1.9.0 on redhat with no success. I need
> to give some graphs generated in R to my boss so that he can modify them
> in Powerpoint to fit he style of his presentation. Recommendations on
> other methods are appreciated as well.
> 
> Hao 
> 

  this comes up every so often.
  if you have fig2dev (which may come bundled with xfig), you
can save your graphics as xfig and then use fig2dev -L emf
to save in extended metafile format.

  the other answer that has been suggested is to install R
for windows under Wine (assuming you're running on Intel
hardware).

   Ben Bolker



From jfontain at free.fr  Wed Sep 21 17:19:34 2005
From: jfontain at free.fr (jfontain@free.fr)
Date: Wed, 21 Sep 2005 17:19:34 +0200
Subject: [R] automatic ARIMA best-fitting
Message-ID: <1127315974.43317a0634f69@imp5-g19.free.fr>

Is there such a beast implemented in R, something like
http://www.spss.com/trends/?

Many thanks in advance,

--
Jean-Luc



From bolker at ufl.edu  Wed Sep 21 17:03:35 2005
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 21 Sep 2005 15:03:35 +0000 (UTC)
Subject: [R] Help on optim
References: <829e6c8a05092101335b7bbfc8@mail.gmail.com>
	<00f801c5be8b$041713c0$0540210a@www.domain>
Message-ID: <loom.20050921T170303-393@post.gmane.org>

Dimitris Rizopoulos <dimitris.rizopoulos <at> med.kuleuven.be> writes:

> 
> you don't need L-BFGS-B in this case since you can easily 
> re-parameterize you problem, i.e., you can always write:
> 
> pi_i = exp(a_i) / sum(exp(a_i)), with e.g., a_1 = 0
> 
> and thus maximize with respect to a_i's.
> 
> I hope it helps.
> 
> Best,
> Dimitris

  although this may be problematic if some
of the original parameters lie on the boundary ...



From rohitvk at vsnl.com  Wed Sep 21 17:30:12 2005
From: rohitvk at vsnl.com (Rohit Vishal Kumar)
Date: Wed, 21 Sep 2005 21:00:12 +0530
Subject: [R] Problem with SAGx Library
Message-ID: <43317C84.3000600@vsnl.com>

Dear All:

I am a  newbie to R and as such i am posting this request for help.

I am trying to use R to compute the "Calinski Harabasz (CH) Index ". The 
  CH Index is available in the library SAGx.

The version of R i am running is 2.2.1.

I have my data in a CSV format which i read into R using the
read.table() command.  After the data has been read i am loading in the
"SAGx" and "MASS" libraries.

According to the SAGx help files the command for running a CH
procedure is as follows:

caha(data, cluster)

where: "data" is the data matrix and "cluster" is a vector describing 
the cluster membership consecutive numbers.

The commands that i give are as follows:

| > cl <- myclus(mydata, k=3)
| > caha(mydata, cl$cluster)

I get the following output
Error in "[.data.frame"(x, , cl == (i + min(cl) - 1)) :
         undefined columns selected

I cannot make head or tail out of the error.

Strangely enough SAGx the procedure "gap" to calculate the GAP
statistic has the same format as CH. So when i give the command (shown
below) after several minutes of computation i get the value for the
statistic say 0.39765:

| > gap (mydata, cl$cluster)
[1] 0.39765

Am i doing something wrong when i am implementing caha?

As my knowledge of R is quite limited i shall be extremely grateful for
your help in this regard.

Thanking you once again.

Rohit Vishal Kumar

From ripley at stats.ox.ac.uk  Wed Sep 21 17:33:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Sep 2005 16:33:13 +0100 (BST)
Subject: [R] cdecl and stdcall
In-Reply-To: <C373BB6AB83DD74484F4BCD3DA60F90E190317@S-STMLU0006>
References: <C373BB6AB83DD74484F4BCD3DA60F90E190317@S-STMLU0006>
Message-ID: <Pine.LNX.4.61.0509211612530.30537@gannet.stats>

This really is the wrong list.  Please do read the posting guide as we do 
ask (before posting).  In so far as this is relevant to R at all, it is 
within the description of the R-devel list.

On Wed, 21 Sep 2005, Bogner, Konrad (LfU) wrote:

> I'm trying to load a dynamic link library and it seems to work 
> (is.loaded -> TRUE). When I run the function, which calls the .Fortran 
> subroutine, R crashes! I'v tried the same in S-Plus 2000 and it worked. 
> Therefore I suppose that the dll has been compiled with the stdcall 
> calling convention (and not cdecl).

Hmm.  Why do you suppose so?  In so far as I can recall S-PLUS 2000 (it is 
long obselete) it worked with cdecl (as it was built with Watcom 
compilers).  The rw-FAQ explains how to get debugging information on the 
crash, which could have many causes.

> But the problem is that I don't have access to the source code, I've 
> just the dll without any working import library. Maybe someone could be 
> so kind and send me an example how to write a wrapper? I've found one 
> example at the NAG's site 
> (http://www.nag.com/numeric/RunderWindows.asp), but it didn't work. At 
> http://www.cygwin.com/cygwin-ug-net/dll.html there is a description of 
> linking against dlls and how to create a def and an import libraray 
> file, but unfortunately my dll seems to be stripped, because I get the 
> error message "no symbols" after running the command "nm".

A DLL with no symbols would be of no use to anyone, but your confusion is 
that nm does not apply to DLLs.  You need to find out what the DLL is 
exporting, with what convention.  pedump and pexports are the usual Open 
Source tools to do so.  The convention is normally hidden in the real 
symbol name, e.g. foo at 8 is a stdcall entry point, but may be aliased to 
foo.

> I'm using R 2.1.0 (windows xp) and CYGWIN (gnu compilers,..)

Cygwin is not a platform we support for R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From duncan at wald.ucdavis.edu  Wed Sep 21 17:43:57 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Wed, 21 Sep 2005 08:43:57 -0700
Subject: [R] SAX Parser best practise
In-Reply-To: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E75F@EMAIL.mpimp-golm.mpg.de>
References: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E75F@EMAIL.mpimp-golm.mpg.de>
Message-ID: <43317FBD.1010106@wald.ucdavis.edu>



Jan Hummel wrote:
> Dear All,
> 
> I have a question regarding best practise in setting up a XML parser
> within R. 
> Because I have files with more than 100 MB and I'm only interested in
> some values I think a SAX-like parser using xmlEventParse() will be the
> best solution.
> Unfortunately the values I'm looking for, to construct some higher "mass
> spectrum", are distributed over different lines: as <spectrum id="2">,
> <mzArrayBinary>, <intenArrayBinary> <... name="MassToChargeRatio"
> value="445.598999"/> (as one can see in the xml snip set)
> 
> I know the mechanism of using Event Handlers, as shown in the examples.
> But what I'm looking for is, how can I use some "path information" as
> mentioned in "addContext" parameter of xmlEventParse()? May somebody
> share a example using "addContext = TRUE" and pointing me to the
> variables I may use if I implement the "..." parameter within my
> handlers.
> 

The addContext was an attempt to provide contextual information,
but it is not obvious how to do this efficiently. And of course
efficiency is the name of the game with the SAX model.
If we wanted to know path information for the node, we would have
to build this and that would slow things down. There are no nodes
in the SAX world as we don't build the tree in any way.
So the addContext currently doesn't do much. It is there
as a hook that we can use if we want in the future.
But you can do anything you need in the R code.


> Do I have to implement a "status machine" using some variables within my
> handlers, or would one prefer to use the "state" parameter of
> xmlEventParse()?
> 


As Seth mentioned in his reply, you can use state in your R handler 
functions to determine where you are.  You can maintain a "stack"
to determine the exact path of the current "node" in the
startElement() handler and pop the name in the endElement() handler.

The difference between maintaining state via environments/local 
persisten scope (using <<- in Seth's mail) and using the state argument
is more of a personal preference in R.  The state argument was added
for S-Plus since it does not support environments.  Using the state
argument might save an epsilon amount of time, but it is hopefully
neglible.

BTW, do you have a schema for the XML document you are working on?





> I would appreciate any assistance very much!
> 	Jan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Wed Sep 21 17:54:12 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 21 Sep 2005 16:54:12 +0100
Subject: [R] Fwd:  ts.intersect bug?
In-Reply-To: <b0808fdc0509210639113da0ce@mail.gmail.com>
References: <b0808fdc05092104595e80f242@mail.gmail.com>	<17201.23480.534481.260305@stat.math.ethz.ch>	<b0808fdc05092106347136c873@mail.gmail.com>
	<b0808fdc0509210639113da0ce@mail.gmail.com>
Message-ID: <43318224.7060505@lancaster.ac.uk>

Antonio, Fabio Di Narzo wrote:

> Ooops, bad example.
> Try this instead:
> a <- ts(runif(6500), start=0, freq=10)
> b <- lag(a, 1)
> c <- ts.intersect(a, b)
> 
> Gives an error from .cbind.ts

or even:

  > a=ts(numeric(2600),freq=10,start=0);b=lag(a,1);c=ts.intersect(a,b)
Error in "[<-"(`*tmp*`, , (1 + cs[i]):cs[i + 1], value = c(0, 0, 0, 0,  :
         number of items to replace is not a multiple of replacement length

  seems to be sensitive to the lengths of the time series:

  > a=ts(numeric(2601),freq=10,start=0);b=lag(a,1);c=ts.intersect(a,b)
  > (works fine)


  Digging into the code, the window() function is returning a different 
length time series for each one in these failing cases. I reckon its a 
floating-point precision situation, where the last time series point 
should be included but the arithmetic precision of a 2600-long series at 
separation of 1/10 is leaving it out.

  > st;en
  [1] 0
  [1] 259.8

  > a=ts(numeric(2600),freq=10,start=0);b=lag(a,1)
  > length(window(a,st,en))
  [1] 2599
  > length(window(b,st,en))
  [1] 2598

  - ts.intersect is trying to put these two time series together, and so 
fails. But:

  > a=ts(numeric(2601),freq=10,start=0);b=lag(a,1)
  > length(window(a,st,en))
  [1] 2599
  > length(window(b,st,en))
  [1] 2599

  - works.

  Note that en is not precisely 259.8:

  > en == 259.8
  [1] FALSE
  > en-259.8
  [1] -5.684342e-14

  I've computed 'en' as the .cbind.ts function does, and its not exactly 
259.8. If it were, then it would work... Perhaps .cbind.ts should round 
to the nearest true time point or something...

  Note that it fails in plenty of smaller cases too:

  > a=ts(numeric(13),freq=10,start=0);b=lag(a,1);c=ts.intersect(a,b)
Error in "[<-"(`*tmp*`, , (1 + cs[i]):cs[i + 1], value = c(0, 0, 0, 0,  :
         number of items to replace is not a multiple of replacement length

  Seems to not like 13s and 10s and integer products thereof (2600, 
6500). Are you superstitious?

Baz

PS without counting, how many letters are there in 'superstitious'?



From maechler at stat.math.ethz.ch  Wed Sep 21 17:55:56 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 21 Sep 2005 17:55:56 +0200
Subject: [R] bagplot - a rough R function
In-Reply-To: <433172E6.70901@wiwi.uni-bielefeld.de>
References: <web-1865915@calmail-be1.berkeley.edu>
	<Pine.LNX.4.44.0409131645200.19598-100000@gannet.stats>
	<16715.4933.392083.824916@gargle.gargle.HOWL>
	<433172E6.70901@wiwi.uni-bielefeld.de>
Message-ID: <17201.33420.153819.995140@stat.math.ethz.ch>

>>>>> "Peter" == Peter Wolf <pwolf at wiwi.uni-bielefeld.de>
>>>>>     on Wed, 21 Sep 2005 16:49:10 +0200 writes:

    Peter> In the moment I am writing an R function for drawing
    Peter> bagplots (two dimensional boxplots).
    Peter> For some elements of the plot are found numerically
    Peter> the resulting plots differs a little bit from the correct bagplots.

    Peter> Here is the link to the actual version:

    Peter> http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/bagplot/bagplot.R 
    Peter> ( R code )

    Peter> http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/bagplot/bagplot.pdf 
    Peter> (examples and description)

    Peter> Now I am looking for examples to test / to improve my function.
    Peter> Where can I get data sets and the corresponding correct bagplots?

I'll send you my (unpublished) version of my package 'bagplot'
{{the one I've never released because of segmentation fault bugs
  somewhere in Fortran}} in a private E-mail.

In my version I had started make a function
   bag() 
which *computes* all the information needed and then returns an
(S3) object "bag".  Then, the plotting happens via
the 
    plot.bag <- function(.....) {..........}
method.

So you can say

   plot(bag(.........), ........)

if you want both (computing and plotting) at once, but can also
separate the two --- following very much the modern "S" way
"whole object ..".


Martin



From emma_hartnett at yahoo.co.uk  Wed Sep 21 17:57:44 2005
From: emma_hartnett at yahoo.co.uk (emma hartnett)
Date: Wed, 21 Sep 2005 16:57:44 +0100 (BST)
Subject: [R] Multiple density plots on 1 graph
Message-ID: <20050921155745.22961.qmail@web25510.mail.ukl.yahoo.com>

I want to overlay 50 denisty plots on a single plot. 
For each plot there are 10,000 data points and i want
the empirical density of the data. I have not been
able to find an easy way to achieve this (I have
scoured the manula and website so sorry if I missed
it!), does anyone have any suggestions?

Thankyou.



From Tord.Snall at helsinki.fi  Wed Sep 21 18:22:44 2005
From: Tord.Snall at helsinki.fi (Tord Snall)
Date: Wed, 21 Sep 2005 19:22:44 +0300
Subject: [R] factor as seq() in for loop
Message-ID: <1127319764.433188d434bad@www2.helsinki.fi>

Dear Spencer,

> 	  Does the follow help?

Yes, as did the earlier replies. Thanks!

Tord


> 
>  > aFactor <- factor(rep(letters[1:3], 1:3))
>  > aFactor
> [1] a b b c c c
> Levels: a b c
>  > for(af in levels(aFactor)){
> +   print(sum(aFactor==af))
> + }
> [1] 1
> [1] 2
> [1] 3
> 
> 	  spencer graves
> 
> Tord Snall wrote:
> 
> > Dear all,
> > I would like to use the values in vegaggr.BLMCMR02$colony 
> > 
> > str(vegaggr.BLMCMR02)
> > `data.frame':   1678 obs. of  3 variables:
> > $ vegtype       : Factor w/ 27 levels "2010","2020",..: 3 4 5 19 4 5 19
> 5  
> > $ colony        : Factor w/ 406 levels "0","1","10","100",..: 1 1 1 1 2
> 2 2 
> > $ Totvegproparea: num  0.00055 0.03956 0.95705 0.00284 0.05215 ...
> > 
> > as the seq in a for loop (for(var in seq)), and therefore first picked
> them 
> > out as 
> > cols = aggregate(list(A = vegaggr.BLMCMR02$Totvegproparea), 
> >                 list(colony = vegaggr.BLMCMR02$colony), sum)[,1]
> > 
> >>str(cols)
> > 
> >  Factor w/ 406 levels "0","1","10","100",..: 1 2 3 4 5 6 7 8 9 10 ...
> > 
> > I next planned to transform cols using as.integer(cols). However, this
> 
> > transformation gives a vector corresponding to
> seq(from=1,to=length(cols)). 
> > 
> > Could someone please give advice on how to make use of a factor as the
> seq 
> > in a for loop (which is strictly not allowed' according to ?Control).
> > 
> > Thanks!
> > 
> > Tord
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> -- 
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>



From kkiely at insightful.com  Wed Sep 21 18:43:42 2005
From: kkiely at insightful.com (Kathy Kiely)
Date: Wed, 21 Sep 2005 17:43:42 +0100
Subject: [R] =?iso-8859-1?q?Jos=E9_C=2E_Pinheiro_Training_in_UK_-_Analyzin?=
	=?iso-8859-1?q?g_Mixed-Effects_Models_with_S-PLUS?=
Message-ID: <B796B8C05975394DA24E457D1985BDB466E5C1@uk2kexch01.insightful.com>

Jos?? C. Pinheiro Training in UK - Analyzing Mixed-Effects Models with S-PLUS
 
Mixed-effects models provide a powerful tool for analyzing grouped data. This
course will overview the application of linear and nonlinear mixed-effects
models in the analysis of grouped data, using the NLME software in S-PLUS to
illustrate the different stages of model fitting. A new element to the course
will cover the functionality for fitting generalized linear mixed models
(GLMM's) which is now available in S-PLUS 7.
 
Ideal attendees would be Statisticians and, more broadly scientists, who
utilize or would like to utilize the mixed-effects model capabilities
provided in the NLME library in S-PLUS, for the analysis of correlated data.
 
This three day course includes methodological aspects and practical
application of mixed-effects models and is presented by one of the subject
areas leading researchers.  Jos?? C. Pinheiro is the co-author of the NLME
software in S-PLUS and of the excellent text book "Mixed Effects Models in S
and S-PLUS", which is supplied as part of the course materials.
 
Full details on the training course are as follows:-
 
Title: Analyzing Mixed-Effects Models with S-PLUS 
 
Presenter: Jos?? C. Pinheiro
Date: Nov 7-9, 2005
Location: Brighton, England
 
Abstract:
A unified model-building strategy for both linear and nonlinear models will
be presented and applied to the analysis of real datasets from a variety of
areas, including pharmacokinetics, agriculture, and manufacturing. Strong
emphasis will be placed on the use of graphical displays at the various
phases of the model-building process, starting with exploratory plots of the
data and concluding with diagnostic plots to assess the adequacy of a fitted
model.
 
Full details and registration link can be found at:- 
 
http://www.insightful.com/services/training/pinheiro_7nov05.asp
 
Please contact me if you would like any more information or have any
questions.
 
Regards
 
Kathy Kiely
Sales & Marketing Administrator
Insightful Limited
Network House, Basing View Basingstoke, Hampshire, RG21 4HG
Tel : +44 (0)1256 339822
Fax : +44 (0)1256 339839
email : kkiely at insightful.com



From ericpante at hotmail.com  Wed Sep 21 18:49:34 2005
From: ericpante at hotmail.com (Eric Pante)
Date: Wed, 21 Sep 2005 12:49:34 -0400
Subject: [R] Clustering and bootstrap
Message-ID: <BAY106-DAV21B381E482BCAA11D7BDEABC940@phx.gbl>

Dear Listers,

I emailed the list a few days ago about how to bootstrap a community 
matrix (species by sites) and get a consensus tree with node support. A 
friend pointed out that a similar question remained unanswered in 2004.
I wish to re-word my question: is anyone aware of a package / method to 
obtain a majority-rule consensus tree from x distance matrices ? Is 
anyone using R to generate phylogenetic trees with node support ?

thanks for your time and consideration,
Eric P.


Eric Pante
---------------------------------------------------------------
Graduate Student in Marine Biology
Grice Marine Laboratory
205 Fort Johnson Road, Charleston SC 29412
---------------------------------------------------------------

	"On ne force pas la curiosite, on l'eveille ..."
	Daniel Pennac



From pburns at pburns.seanet.com  Wed Sep 21 18:57:23 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 21 Sep 2005 17:57:23 +0100
Subject: [R] MGARCH estimation
In-Reply-To: <139ef1c20509210753d017b86@mail.gmail.com>
References: <139ef1c20509210753d017b86@mail.gmail.com>
Message-ID: <433190F3.3070400@pburns.seanet.com>

There is a Burns Statistics working paper that describes
an approach to getting multivariate garch when only
univariate garch estimates are available.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Krishna wrote:

>Hi R-users
>
>Can the users let me know how to do MGARCH estimate (Bivariate GARCH)
>and volatility forecast for 2 variables in R.
>
>thanks and regards
>
>snvk
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From ripley at stats.ox.ac.uk  Wed Sep 21 19:10:41 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Sep 2005 18:10:41 +0100 (BST)
Subject: [R] Fwd:  ts.intersect bug?
In-Reply-To: <43318224.7060505@lancaster.ac.uk>
References: <b0808fdc05092104595e80f242@mail.gmail.com>
	<17201.23480.534481.260305@stat.math.ethz.ch>
	<b0808fdc05092106347136c873@mail.gmail.com>
	<b0808fdc0509210639113da0ce@mail.gmail.com>
	<43318224.7060505@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.61.0509211756520.31772@gannet.stats>

The problem was that a tolerance in window.default was abs(start)*ts.eps, 
which is no tolerance at all if start = 0.

So the special feature here was that the intersection of the series 
started at zero. I had already tested and committed a fix.

window.default does use a tolerance for en, just as Baz suggests it 
should.  The code is there to be read ....  (That tolerance was inadequate 
if end = 0, but has been changed.)

On Wed, 21 Sep 2005, Barry Rowlingson wrote:

> Antonio, Fabio Di Narzo wrote:
>
>> Ooops, bad example.
>> Try this instead:
>> a <- ts(runif(6500), start=0, freq=10)
>> b <- lag(a, 1)
>> c <- ts.intersect(a, b)
>>
>> Gives an error from .cbind.ts
>
> or even:
>
>  > a=ts(numeric(2600),freq=10,start=0);b=lag(a,1);c=ts.intersect(a,b)
> Error in "[<-"(`*tmp*`, , (1 + cs[i]):cs[i + 1], value = c(0, 0, 0, 0,  :
>         number of items to replace is not a multiple of replacement length
>
>  seems to be sensitive to the lengths of the time series:
>
>  > a=ts(numeric(2601),freq=10,start=0);b=lag(a,1);c=ts.intersect(a,b)
>  > (works fine)
>
>
>  Digging into the code, the window() function is returning a different
> length time series for each one in these failing cases. I reckon its a
> floating-point precision situation, where the last time series point
> should be included but the arithmetic precision of a 2600-long series at
> separation of 1/10 is leaving it out.
>
>  > st;en
>  [1] 0
>  [1] 259.8
>
>  > a=ts(numeric(2600),freq=10,start=0);b=lag(a,1)
>  > length(window(a,st,en))
>  [1] 2599
>  > length(window(b,st,en))
>  [1] 2598
>
>  - ts.intersect is trying to put these two time series together, and so
> fails. But:
>
>  > a=ts(numeric(2601),freq=10,start=0);b=lag(a,1)
>  > length(window(a,st,en))
>  [1] 2599
>  > length(window(b,st,en))
>  [1] 2599
>
>  - works.
>
>  Note that en is not precisely 259.8:
>
>  > en == 259.8
>  [1] FALSE
>  > en-259.8
>  [1] -5.684342e-14
>
>  I've computed 'en' as the .cbind.ts function does, and its not exactly
> 259.8. If it were, then it would work... Perhaps .cbind.ts should round
> to the nearest true time point or something...
>
>  Note that it fails in plenty of smaller cases too:
>
>  > a=ts(numeric(13),freq=10,start=0);b=lag(a,1);c=ts.intersect(a,b)
> Error in "[<-"(`*tmp*`, , (1 + cs[i]):cs[i + 1], value = c(0, 0, 0, 0,  :
>         number of items to replace is not a multiple of replacement length
>
>  Seems to not like 13s and 10s and integer products thereof (2600,
> 6500). Are you superstitious?
>
> Baz
>
> PS without counting, how many letters are there in 'superstitious'?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Max.Kuhn at pfizer.com  Wed Sep 21 19:11:15 2005
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Wed, 21 Sep 2005 13:11:15 -0400
Subject: [R]  Multiple density plots on 1 graph
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3028855EF@groamrexm03.amer.pfizer.com>

Emma,

One way is to use lattice:

test <- data.frame(y = rnorm(5 * 50),
   groupings = rep(letters[1:5], 50))
   
library(lattice)

densityplot( ~ y, group = groupings, data =test)

HTH,

Max
   
   
>emma hartnett emma_hartnett at yahoo.co.uk
>Wed Sep 21 17:57:44 CEST 2005
>
>    * Previous message: [R] Problem with SAGx Library
>    * Next message: [R] factor as seq() in for loop
>    * Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]
>
>I want to overlay 50 denisty plots on a single plot. 
>For each plot there are 10,000 data points and i want
>the empirical density of the data. I have not been
>able to find an easy way to achieve this (I have
>scoured the manula and website so sorry if I missed
>it!), does anyone have any suggestions?
>
>Thankyou.


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From chrish at stats.ucl.ac.uk  Wed Sep 21 19:22:03 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Wed, 21 Sep 2005 18:22:03 +0100 (BST)
Subject: [R] Clustering and bootstrap
In-Reply-To: <BAY106-DAV21B381E482BCAA11D7BDEABC940@phx.gbl>
References: <BAY106-DAV21B381E482BCAA11D7BDEABC940@phx.gbl>
Message-ID: <Pine.LNX.4.58.0509211814160.16689@egon.stats.ucl.ac.uk>

Hi Eric,

I work about bootstrap and clustering and I also write R-code about that,
which is not documented up to now though.
You may have a look at
http://www.homepages.ucl.ac.uk/~ucakche/papers/classbrd.ps
I also worked about species by site data and there
is a parametric
bootstrap test implemented as function prabtest in package prabclus. You
may have a look at that, too.
However, all of this does not really fit to your new question and I am not
sure whether I have anything of interest to you.
Feel free to contact me if you can somehow relate your problem to my
work mentioned above.

Best,
Christian

On Wed, 21 Sep 2005, Eric Pante wrote:

> Dear Listers,
>
> I emailed the list a few days ago about how to bootstrap a community
> matrix (species by sites) and get a consensus tree with node support. A
> friend pointed out that a similar question remained unanswered in 2004.
> I wish to re-word my question: is anyone aware of a package / method to
> obtain a majority-rule consensus tree from x distance matrices ? Is
> anyone using R to generate phylogenetic trees with node support ?
>
> thanks for your time and consideration,
> Eric P.
>
>
> Eric Pante
> ---------------------------------------------------------------
> Graduate Student in Marine Biology
> Grice Marine Laboratory
> 205 Fort Johnson Road, Charleston SC 29412
> ---------------------------------------------------------------
>
> 	"On ne force pas la curiosite, on l'eveille ..."
> 	Daniel Pennac
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From murdoch at stats.uwo.ca  Wed Sep 21 19:27:47 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 21 Sep 2005 13:27:47 -0400
Subject: [R] cdecl and stdcall
In-Reply-To: <C373BB6AB83DD74484F4BCD3DA60F90E190317@S-STMLU0006>
References: <C373BB6AB83DD74484F4BCD3DA60F90E190317@S-STMLU0006>
Message-ID: <43319813.2060409@stats.uwo.ca>

On 9/21/2005 9:32 AM, Bogner, Konrad (LfU) wrote:
> Hi,
> I'm trying to load a dynamic link library and it seems to work (is.loaded  -> TRUE). When I run the function, which calls the .Fortran subroutine, R crashes!
> I'v tried the same in S-Plus 2000 and it worked. Therefore I suppose that the dll has been compiled with the stdcall calling convention (and not cdecl). But the problem is that I don't have access to the source code, I've just the dll without any working import library. Maybe someone could be so kind and send me an example how to write a wrapper? I've found one example at the NAG's site (http://www.nag.com/numeric/RunderWindows.asp), but it didn't work. At http://www.cygwin.com/cygwin-ug-net/dll.html there is a description of linking against dlls and how to create a def and an import libraray file, but unfortunately my dll seems to be stripped, because I get the error message "no symbols" after running  the command "nm".
> 
> I'm using R 2.1.0 (windows xp) and CYGWIN (gnu compilers,..)

You are likely to run into problems mixing Cygwin code with R.  R uses 
MinGW, not Cygwin.  You can probably get away with writing a DLL using 
Cygwin tools and calling it from R, but you may well find that there are 
subtle problems.  I'd suggest switching to MinGW.

The MinGW wiki gives instructions for linking to foreign DLLs on this page:

http://www.mingw.org/MinGWiki/index.php/CreateImportLibraries

For example, I've got a DLL with these exports:

$ pexports teststdcall.dll
LIBRARY teststdcall.dll
EXPORTS
dostdcall at 8

(pexports is a MinGW utility; not part of our standard toolset, but you 
can get the MinGW utils at the same place as the rest of MinGW).

Put those 3 lines in a file teststdcall.def, then

$ dlltool -d teststdcall.def -l libteststdcall.a

creates the import library.  (If your exports have names in a different 
format, you may need some different options here; see the Wiki.)

Set up your C wrapper something like this:

$ less callstdcall.c
__stdcall double dostdcall(double x);

void callit(double *x)
{
     double y;
     y = dostdcall(*x);
     *x = y;
}

You then need to set an environment variable PKG_LIBS to tell Rcmd SHLIB 
to look at your import library:

$ export PKG_LIBS="-L. -lteststdcall"

and then create your DLL to be called by R:

$ Rcmd SHLIB callstdcall.c
making callstdcall.d from callstdcall.c
gcc   -If:/R/svn/r-devel/R/include -Wall -O2   -c callstdcall.c -o 
callstdcall.o

ar cr callstdcall.a callstdcall.o
ranlib callstdcall.a
gcc  --shared -s  -o callstdcall.dll callstdcall.def callstdcall.a 
-Lf:/R/svn/r
-devel/R/src/gnuwin32  -L. -lteststdcall -lg2c -lR

Then in R

 > dyn.load('callstdcall.dll')
 > .C('callit', x=as.double( <some value> ))

will call it for you.

Duncan Murdoch

P.S. There's supposed to be a way in MinGW to avoid creating the import 
library, i.e. to link directly against the .dll, but I don't know what 
it is, and it's less flexible than this approach.



From gunter.berton at gene.com  Wed Sep 21 19:48:45 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 21 Sep 2005 10:48:45 -0700
Subject: [R] Multiple density plots on 1 graph
In-Reply-To: <20050921155745.22961.qmail@web25510.mail.ukl.yahoo.com>
Message-ID: <200509211748.j8LHmjXu014558@meitner.gene.com>

Emma:

1. I don't know what you mean by "empirical density of the data." I'm pretty
sure that overlaying 50, say, kernel density estimators (with the same
kernels?) would produce a completely useless mess. Ditto, I would think, for
anything that looks something like a kernel density (histograms, frequency
polygons, etc.).

2. You might be able to see something using empirical cumulative
distribution functions (?ecdf), so I suggest you consider that instead.

3. Better yet, as was suggested by Max Kuhn, try lattice plots of ecdf's (or
possibly kernel density estimators) with the lattice arrangement determined
by factors of interest or ordered by some useful summary statistic (e.g.
median) of the distributions. Or perhaps use qqmath if the data come from
some known distributional form (Gaussian, gamma, ...). Ordering the lattice
plots in an informative way can reveal a great deal more.

4. Thinking more carefully about what you want to show and what you want to
learn from these plots should help you decide what to do. Then you can use R
to do it (usually easily).


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of emma hartnett
> Sent: Wednesday, September 21, 2005 8:58 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Multiple density plots on 1 graph
> 
> I want to overlay 50 denisty plots on a single plot. 
> For each plot there are 10,000 data points and i want
> the empirical density of the data. I have not been
> able to find an easy way to achieve this (I have
> scoured the manula and website so sorry if I missed
> it!), does anyone have any suggestions?
> 
> Thankyou.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gerifalte28 at hotmail.com  Wed Sep 21 20:36:38 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 21 Sep 2005 18:36:38 +0000
Subject: [R] Identical data frames
In-Reply-To: <20050921140713.16909.qmail@web25805.mail.ukl.yahoo.com>
Message-ID: <BAY103-F1573524C22A8D079F00E16A6940@phx.gbl>

you just said t.  identical() will compare *strict* equality
i.e.

d1=data.frame(x=,y=1:30)
d2=d1
identical(d1,d2)
[1] TRUE

d1=data.frame(x=letters,y=1:26)
d2=data.frame(x=c("aa",letters[2:26]), y=1:26)
identical(d1,d2)
[1] FALSE #because The first row of d2$x was "aa" and in d1 was "a"

Cheers

Francisco

>From: Marc Bernard <bernarduse1 at yahoo.fr>
>To: r-help at stat.math.ethz.ch
>Subject: [R] Identical  data frames
>Date: Wed, 21 Sep 2005 16:07:13 +0200 (CEST)
>
>Dear All,
>
>Is there any R  function to test if two  data frames, say df1 and df2,  
>having the same  row names  and the same column names  are identiques?
>
>Thanks in advance,
>
>Bernard
>
>
>
>
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Wed Sep 21 20:46:50 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 21 Sep 2005 14:46:50 -0400
Subject: [R] Clustering and bootstrap
Message-ID: <355C35514FEAC9458F75947F5270974D076CE8@usctmx1103.merck.com>

I suppose you could average the distance matrices, getting a distance
matrix, and then apply hierarchical clustering... or threshold the average
distance matrix at a sequence of values and compute connected components for
each... that sounds like it's in the spirit of a "majority-rule consensus
tree" though I don't know what that is.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eric Pante
Sent: Wednesday, September 21, 2005 12:50 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Clustering and bootstrap


Dear Listers,

I emailed the list a few days ago about how to bootstrap a community 
matrix (species by sites) and get a consensus tree with node support. A 
friend pointed out that a similar question remained unanswered in 2004.
I wish to re-word my question: is anyone aware of a package / method to 
obtain a majority-rule consensus tree from x distance matrices ? Is 
anyone using R to generate phylogenetic trees with node support ?

thanks for your time and consideration,
Eric P.


Eric Pante
---------------------------------------------------------------
Graduate Student in Marine Biology
Grice Marine Laboratory
205 Fort Johnson Road, Charleston SC 29412
---------------------------------------------------------------

	"On ne force pas la curiosite, on l'eveille ..."
	Daniel Pennac

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rnews at kernstat.com  Wed Sep 21 21:19:47 2005
From: rnews at kernstat.com (Remington, Richard)
Date: Wed, 21 Sep 2005 13:19:47 -0600
Subject: [R] Multiple density plots on 1 graph
In-Reply-To: <20050921155745.22961.qmail@web25510.mail.ukl.yahoo.com>
References: <20050921155745.22961.qmail@web25510.mail.ukl.yahoo.com>
Message-ID: <4331B253.7050801@kernstat.com>


plot( density( rnorm(n=20)), xlim=c(-5,5), ylim=c(0,1))
for( i in 2:50) lines( density( rnorm(n=20)))

emma hartnett wrote:
> I want to overlay 50 denisty plots on a single plot. 
> For each plot there are 10,000 data points and i want
> the empirical density of the data. I have not been
> able to find an easy way to achieve this (I have
> scoured the manula and website so sorry if I missed
> it!), does anyone have any suggestions?
> 
> Thankyou.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 

Richard E. Remington
Statistician
KERN Statistical Services, Inc.
PO Box 1046
Boise, ID 83701
Tel: 208.426.0113
KernStat.com



From ripley at stats.ox.ac.uk  Wed Sep 21 22:57:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Sep 2005 21:57:21 +0100 (BST)
Subject: [R] source(file) => file becomes readonly
In-Reply-To: <0D54094F69391A438650C6151B0A33E46FD335@post2.imr.no>
References: <0D54094F69391A438650C6151B0A33E46FD335@post2.imr.no>
Message-ID: <Pine.LNX.4.61.0509212151240.3511@gannet.stats>

This was not really accurate.

Being `read-only' is a Windows phenomenon on open files (even files open 
for reading).  In 2.0.1 and 2.1.1 the file was open whilst it was parsed 
and so `read-only' during that time. In 2.0.1 it was then closed, but in 
2.1.1 it was (potentially) opened several times to find an encoding, 
parsed and left open until source() terminated.  In 2.2.0 it will be 
closed once it has been parsed.

Changing a file after it has been parsed will have no effect, so there is 
no reason to prevent it (beyond that was the simplest way to implement 
choosing a suitable encoding).

On Thu, 1 Sep 2005, Hjellvik Vidar wrote:

> The OS is Windows XP.
> I use to work on the file while it's executed and save changes 
> continually. It doesn't seem to have any effect on the current 
> execution. I just find it annoying not to be able to do it. It's not 
> crucial, but if there is some easy way around it I would like to 
> know....
>
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: 1. september 2005 14:21
> To: Hjellvik Vidar
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] source(file) => file becomes readonly
>
>
> Hjellvik Vidar wrote:
>
>> Hello,
>> when I work in R, I write code in a text file that I run with the "source(filename)" command. In R2.1.1 the file is read-only while the source command is executed. This was not the case in R2.0.1. Is this a bug-fix or is it possible not to have the file read-only when executed?
>
> So you want to modify a file *while* it is executed?
> Sounds dangerous to me, you may want to explain further...
>
> BTW: Which OS are we talking about?
>
> Uwe Ligges
>
>
>> Best regards
>> Vidar
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vincent.goulet at act.ulaval.ca  Wed Sep 21 23:10:55 2005
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Wed, 21 Sep 2005 17:10:55 -0400
Subject: [R] Add function to histogram?
In-Reply-To: <loom.20050920T155206-363@post.gmane.org>
References: <loom.20050920T155206-363@post.gmane.org>
Message-ID: <200509211710.55667.vincent.goulet@act.ulaval.ca>

Le 20 Septembre 2005 09:53, Robert Lundqvist a ??crit??:
> Is there any neat way to add a curve (frequency function or the like) to a
> histogram or other plots? I haven't found one yet...

The one key thing here is not to forget "prob=TRUE" in the call to hist(), to 
ensure that the y-axis is scaled in proportions, not in frequencies.

-- 
  Vincent Goulet, Professeur agr??g??
  ??cole d'actuariat
  Universit?? Laval, Qu??bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca



From berwin at maths.uwa.edu.au  Wed Sep 21 23:29:16 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Thu, 22 Sep 2005 05:29:16 +0800
Subject: [R] Doubt about Sweave
In-Reply-To: <200509211006.38299.chrysopa@gmail.com>
References: <200509211006.38299.chrysopa@gmail.com>
Message-ID: <17201.53420.776626.941810@bossiaea.maths.uwa.edu.au>

G'day Ronaldo,

>>>>> "RRJ" == Ronaldo Reis-Jr <chrysopa at gmail.com> writes:

    RRJ> I reading about Sweave and it make a good output. But all
    RRJ> example is made with R commands mades in a file. Is possible
    RRJ> to make an output with sweave interactively in R and after
    RRJ> the analysis end export the latex code to a file?
As far as I know, no.  But you may want to look at Peter Wolf's
package 'relax'; see: 
     http://www.wiwi.uni-bielefeld.de/~wolf/software/relax/relax.html
That package may have the functionality that you are looking for.

Cheers,

        Berwin



From rolf at math.unb.ca  Wed Sep 21 23:35:24 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 21 Sep 2005 18:35:24 -0300 (ADT)
Subject: [R] (Off topic) Limit question.
Message-ID: <200509212135.j8LLZOIL008785@erdos.math.unb.ca>

Please reply to me directly (rolf at math.unb.ca) rather than to the
list, since the question is completely R-free and I'm simply asking
this list because there are so many clever and knowledgeable people
on it.

Suppose that n_i, i = 1, 2, 3, ... are positive integers, and that

                      k
	  lim        SUM n_i^j = nu_j < infinity
     k --> infinity  i=1

for j = 1, 2, 3.  Need it be the case that

                     k-1
	  lim        SUM n_i * n_{i+1}     exists?
     k --> infinity  i=1

I can neither prove this, nor come up with a counter-example.
Can anyone help me out?

				cheers,

					Rolf Turner



From pauljohn at ku.edu  Wed Sep 21 23:42:52 2005
From: pauljohn at ku.edu (Paul Johnson)
Date: Wed, 21 Sep 2005 16:42:52 -0500
Subject: [R] controlling usage of digits & scientific notation in R plots;
 postscript margins
Message-ID: <4331D3DC.2000204@ku.edu>

Dear R users:

I assigned students to make some graphs and I'm having trouble answering 
some questions that they have.  We are all working on R 2.1 on Fedora 
Core Linux 4 systems.

1. In the plot, the axis is not labeled by "numbers", but rather 
scientific notation like "-2e+08" or such.  We realize that means 
-200,000,000.  We want to beautify the plot. We would rather just print 
out the whole, big  number. But if we can't have that, we would like 
something more beautiful and  mathematical, such as
                  8
           -2.0x10

Once the numbers are big, R automagically switches to scientific 
notation, and turning the values horizontal does not help on the y axis.

Example:

x <- rnorm(100,mean=100000,sd=100000000)
y <- rnorm(100,mean=10000000000,sd=100000000000)
plot(x,y,axes=F)
axis(2,las=2)  #turns y axis numbers perpendicular to y axis
axis(1)  # x axis

I realize a person could just divide all numbers by 1 million and then 
have a graph with -200 and an annotation (in millions).

On the x axis, we'd even settle to just label the two outermost points 
with full numerical values, and then have only ticks between.  I was 
looking for some way to use axis() to draw an unlabeled axis and then 
put in text labels after that.  However, I can't understand how to get 
the values of the tick marks placed by axis from the figure in order to 
place text by some tick marks.

2. Some students make postscript figures that fit "just right" into 
LaTeX documents, while some make figures that have huge, 
inches-and-inches of margins around the outside.  I'm not watching how 
they make these figures all the time, but I think I'm figuring out the 
cause of big margins.

Is this right: the margins issue relates to the use of the onefile=F 
option in a dev.copy command?  I think the figures turn out properly 
with this:

dev.copy(postscript,file="myfig.eps",height=5,width=5,horizontal=F,onefile=F)
dev.off()

If you mistakenly omit the onefile=F option, the margins stretch out 
from the center where the figure is placed to the edges of an entire 
sheet of paper. In other words, the default settings for margins in 
inches that I see in output from

 > par()
...
$mai
[1] 1.0066821 0.8092935 0.8092935 0.4145162
...

have no effect if a person forgets the onefile=F option.  We fiddled a 
while, trying to force margins down, par(mai=c(0,0,0,0)), but no matter 
what we did, the figures still had the massive margins.

We don't have these margin problems with other devices. Margins in jpeg 
or png pictures are sized appropriately.


-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From greg.snow at ihc.com  Thu Sep 22 00:08:59 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Wed, 21 Sep 2005 16:08:59 -0600
Subject: [R] controlling usage of digits & scientific notation in R
 plots; postscript margins
Message-ID: <s33185ac.084@lp-msg1.co.ihc.com>

One option is to use the 'scipen' option.  see ?options and look for
scipen.

An example:  

options(scipen=3)
# now do your plot and see what happens (try bigger numbers if 3
doesn't change anything).


you can also specify different axis locations and labels to do it by
hand:

plot( (1:10)*1e+5, 1:10, xaxt='n')
axis(1, at=(1:10)*1e+5, labels=paste( 1:10, "*10^5", sep=''))

also look at ?plotmath if you want actual superscripts.

Hope this helps,

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> Paul Johnson <pauljohn at ku.edu> 09/21/05 03:42PM >>>
Dear R users:

I assigned students to make some graphs and I'm having trouble
answering 
some questions that they have.  We are all working on R 2.1 on Fedora 
Core Linux 4 systems.

1. In the plot, the axis is not labeled by "numbers", but rather 
scientific notation like "-2e+08" or such.  We realize that means 
-200,000,000.  We want to beautify the plot. We would rather just print

out the whole, big  number. But if we can't have that, we would like 
something more beautiful and  mathematical, such as
                  8
           -2.0x10

Once the numbers are big, R automagically switches to scientific 
notation, and turning the values horizontal does not help on the y
axis.

Example:

x <- rnorm(100,mean=100000,sd=100000000)
y <- rnorm(100,mean=10000000000,sd=100000000000)
plot(x,y,axes=F)
axis(2,las=2)  #turns y axis numbers perpendicular to y axis
axis(1)  # x axis

I realize a person could just divide all numbers by 1 million and then

have a graph with -200 and an annotation (in millions).

On the x axis, we'd even settle to just label the two outermost points

with full numerical values, and then have only ticks between.  I was 
looking for some way to use axis() to draw an unlabeled axis and then 
put in text labels after that.  However, I can't understand how to get

the values of the tick marks placed by axis from the figure in order to

place text by some tick marks.

2. Some students make postscript figures that fit "just right" into 
LaTeX documents, while some make figures that have huge, 
inches-and-inches of margins around the outside.  I'm not watching how

they make these figures all the time, but I think I'm figuring out the

cause of big margins.

Is this right: the margins issue relates to the use of the onefile=F 
option in a dev.copy command?  I think the figures turn out properly 
with this:

dev.copy(postscript,file="myfig.eps",height=5,width=5,horizontal=F,onefile=F)
dev.off()

If you mistakenly omit the onefile=F option, the margins stretch out 
from the center where the figure is placed to the edges of an entire 
sheet of paper. In other words, the default settings for margins in 
inches that I see in output from

 > par()
...
$mai
[1] 1.0066821 0.8092935 0.8092935 0.4145162
...

have no effect if a person forgets the onefile=F option.  We fiddled a

while, trying to force margins down, par(mai=c(0,0,0,0)), but no matter

what we did, the figures still had the massive margins.

We don't have these margin problems with other devices. Margins in jpeg

or png pictures are sized appropriately.


-- 
Paul E. Johnson                       email: pauljohn at ku.edu 
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn 
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gerifalte28 at hotmail.com  Thu Sep 22 00:11:18 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 21 Sep 2005 22:11:18 +0000
Subject: [R] Add function to histogram?
In-Reply-To: <200509211710.55667.vincent.goulet@act.ulaval.ca>
Message-ID: <BAY103-F3D43AB9272FD1C6721ACCA6940@phx.gbl>

To be more precise, when using hist(prob=T) the y axis shows the densities. 
If you want relative frequencies (proportions) you can use the histogram(x, 
type=) function in the package lattice or write your own function.

Cheers

Francisco


>From: Vincent Goulet <vincent.goulet at act.ulaval.ca>
>Reply-To: vincent.goulet at act.ulaval.ca
>To: r-help at stat.math.ethz.ch
>Subject: Re: [R] Add function to histogram?
>Date: Wed, 21 Sep 2005 17:10:55 -0400
>
>Le 20 Septembre 2005 09:53, Robert Lundqvist a écrit :
> > Is there any neat way to add a curve (frequency function or the like) to 
>a
> > histogram or other plots? I haven't found one yet...
>
>The one key thing here is not to forget "prob=TRUE" in the call to hist(), 
>to
>ensure that the y-axis is scaled in proportions, not in frequencies.
>
>--
>   Vincent Goulet, Professeur agrégé
>   École d'actuariat
>   Université Laval, Québec
>   Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From maximilianotto at gmx.at  Thu Sep 22 02:07:46 2005
From: maximilianotto at gmx.at (MK)
Date: Wed, 21 Sep 2005 20:07:46 -0400
Subject: [R] mark values as NA in matrix
Message-ID: <6.2.1.2.0.20050921195824.02022a78@email.med.yale.edu>



No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From murdoch at stats.uwo.ca  Thu Sep 22 02:12:10 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 21 Sep 2005 20:12:10 -0400
Subject: [R] source(file) => file becomes readonly
In-Reply-To: <Pine.LNX.4.61.0509212151240.3511@gannet.stats>
References: <0D54094F69391A438650C6151B0A33E46FD335@post2.imr.no>
	<Pine.LNX.4.61.0509212151240.3511@gannet.stats>
Message-ID: <4331F6DA.2040503@stats.uwo.ca>

Prof Brian Ripley wrote:
> This was not really accurate.
> 
> Being `read-only' is a Windows phenomenon on open files (even files open 
> for reading).  In 2.0.1 and 2.1.1 the file was open whilst it was parsed 
> and so `read-only' during that time. In 2.0.1 it was then closed, but in 
> 2.1.1 it was (potentially) opened several times to find an encoding, 
> parsed and left open until source() terminated.  In 2.2.0 it will be 
> closed once it has been parsed.

But R doesn't open files in an exclusive mode.  It opens them in "share 
deny none" mode (_SH_DENYNO; this is an undocumented feature of the MSVC 
run-time library that MinGW uses).  I suspect whatever editor Hjellvik 
was using attempted to open the file with exclusive access, and when 
that failed (because R had it open), it fell back to read-only access.

It's certainly possible to open a file for write access in another 
process while R has it open for read access.  You just can't get 
exclusive access to it.

Duncan Murdoch
> 
> Changing a file after it has been parsed will have no effect, so there is 
> no reason to prevent it (beyond that was the simplest way to implement 
> choosing a suitable encoding).
> 
> On Thu, 1 Sep 2005, Hjellvik Vidar wrote:
> 
> 
>>The OS is Windows XP.
>>I use to work on the file while it's executed and save changes 
>>continually. It doesn't seem to have any effect on the current 
>>execution. I just find it annoying not to be able to do it. It's not 
>>crucial, but if there is some easy way around it I would like to 
>>know....
>>
>>-----Original Message-----
>>From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
>>Sent: 1. september 2005 14:21
>>To: Hjellvik Vidar
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] source(file) => file becomes readonly
>>
>>
>>Hjellvik Vidar wrote:
>>
>>
>>>Hello,
>>>when I work in R, I write code in a text file that I run with the "source(filename)" command. In R2.1.1 the file is read-only while the source command is executed. This was not the case in R2.0.1. Is this a bug-fix or is it possible not to have the file read-only when executed?
>>
>>So you want to modify a file *while* it is executed?
>>Sounds dangerous to me, you may want to explain further...
>>
>>BTW: Which OS are we talking about?
>>
>>Uwe Ligges
>>
>>
>>
>>>Best regards
>>>Vidar
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
>



From ritesh at pdx.edu  Thu Sep 22 02:31:57 2005
From: ritesh at pdx.edu (ritesh@pdx.edu)
Date: Wed, 21 Sep 2005 17:31:57 -0700
Subject: [R] accessing source code in R packages
Message-ID: <1127349117.4331fb7d5e9d3@webmail.pdx.edu>

Hi,

I am new the R world and would like to know how can I access source codes of
standard functions in R?

Thanks,
Ritesh.



From andy_liaw at merck.com  Thu Sep 22 03:48:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Sep 2005 21:48:00 -0400
Subject: [R] accessing source code in R packages
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED481@usctmx1106.merck.com>

R is open source.  You can download the source code from CRAN.

If you mean at the R prompt, usually you see the code for a function by
typing the name of the function at the R prompt, without parentheses.
`Usually' because some methods are delibrately `hidden' from users, and
should only be accessed through their generics.  There are still ways to get
around that.

However, the code you get at the R prompt is not the _source_, as it does
not contain any original comments.  You need to go to the source I refer to
above.

Andy

> From: ritesh at pdx.edu
> 
> Hi,
> 
> I am new the R world and would like to know how can I access 
> source codes of
> standard functions in R?
> 
> Thanks,
> Ritesh.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From 042045003 at fudan.edu.cn  Thu Sep 22 04:03:45 2005
From: 042045003 at fudan.edu.cn (ronggui.wong)
Date: Thu, 22 Sep 2005 10:03:45 +0800
Subject: [R] accessing source code in R packages
Message-ID: <0IN7003MI3WC06@mail.fudan.edu.cn>



>R is open source.  You can download the source code from CRAN.
>
>If you mean at the R prompt, usually you see the code for a function by
>typing the name of the function at the R prompt, without parentheses.
>`Usually' because some methods are delibrately `hidden' from users, and
>should only be accessed through their generics.  There are still ways to get
>around that.
sometimes,the getS3method is helpfull.

>library(MASS)
> princomp.default
Error: object "princomp.default" not found
> getS3method("princomp","default")
function (x, cor = FALSE, scores = TRUE, covmat = NULL, subset = rep(TRUE, 
    nrow(as.matrix(x))), ...) 
{
----snip----


>
>However, the code you get at the R prompt is not the _source_, as it does
>not contain any original comments.  You need to go to the source I refer to
>above.
>
>Andy
>
>> From: ritesh at pdx.edu
>> 
>> Hi,
>> 
>> I am new the R world and would like to know how can I access 
>> source codes of
>> standard functions in R?
>> 
>> Thanks,
>> Ritesh.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>> 
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ÖÂ
Àñ£¡
 
				 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ronggui.wong
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡042045003 at fudan.edu.cn
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-09-22



From gerifalte28 at hotmail.com  Thu Sep 22 04:33:55 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 22 Sep 2005 02:33:55 +0000
Subject: [R] accessing source code in R packages
In-Reply-To: <0IN7003MI3WC06@mail.fudan.edu.cn>
Message-ID: <BAY103-F23DB78772C4AD21BC8A19BA6970@phx.gbl>

or getAnywhere()


>From: "ronggui.wong" <042045003 at fudan.edu.cn>
>Reply-To: 042045003 at fudan.edu.cn
>To: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
>Subject: Re: [R] accessing source code in R packages
>Date: Thu, 22 Sep 2005 10:03:45 +0800
>
>
>
> >R is open source.  You can download the source code from CRAN.
> >
> >If you mean at the R prompt, usually you see the code for a function by
> >typing the name of the function at the R prompt, without parentheses.
> >`Usually' because some methods are delibrately `hidden' from users, and
> >should only be accessed through their generics.  There are still ways to 
>get
> >around that.
>sometimes,the getS3method is helpfull.
>
> >library(MASS)
> > princomp.default
>Error: object "princomp.default" not found
> > getS3method("princomp","default")
>function (x, cor = FALSE, scores = TRUE, covmat = NULL, subset = rep(TRUE,
>     nrow(as.matrix(x))), ...)
>{
>----snip----
>
>
> >
> >However, the code you get at the R prompt is not the _source_, as it does
> >not contain any original comments.  You need to go to the source I refer 
>to
> >above.
> >
> >Andy
> >
> >> From: ritesh at pdx.edu
> >>
> >> Hi,
> >>
> >> I am new the R world and would like to know how can I access
> >> source codes of
> >> standard functions in R?
> >>
> >> Thanks,
> >> Ritesh.
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >>
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>
>= = = = = = = = = = = = = = = = = = = =
>
>
>¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ÖÂ
>Àñ£¡
>
>
>¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ronggui.wong
>¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡042045003 at fudan.edu.cn
>¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡2005-09-22
>


>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From zhengqi04 at mails.gucas.ac.cn  Thu Sep 22 08:25:10 2005
From: zhengqi04 at mails.gucas.ac.cn (=?gb2312?B?1qPkvw==?=)
Date: Thu, 22 Sep 2005 14:25:10 +0800
Subject: [R] Questions about R
Message-ID: <327370310.03170@gucas.ac.cn>

                                                                Sep, 22nd,2005
Dear Authors,
Thanks for reading this email. I'm a graduate student from China  (PRC) and learning the R at present. Now I have some questions to ask you as I have met some strange problems during installing and running the R environment, which can not be found in the "R FAQ" document (or just I can not find them). 

It might be useful that I gived you the information of my operation system and hardwares. I am using R on a Mandrake Linux 10.1 operation system on a PC platform parchased from IBM company. The mainboard of my computer is Intel 915PG, that means it has an integrated graphic accelerator.

The first question is about the error occurs during my installation. Following the "R Installation and Administration" manual, I successully downloaded the latest version of R, unziped the file, and also completed the progress "./configure" and "make" successully. By the way, I managed to install a developmental package of Linux named "r77", which may be a FORTRAN developmental package, before installing the R environment into my computer. However, an error occured when I type the commond "make check" or "make check FORCE=FORCE" or "make check-devel", and the warnings displayed are written below:( Somewords are translated from Chinese into English by myself)

"running code in 'graphics-Ex.R' ...make[4]: *** [graphics-Ex.Rout] error 1
make[4]: Leaving directory `/home/zhengqi/R_HOME/R-2.1.1/tests/Examples'
make[3]: *** [test-Examples-Base] error 2
make[3]: Leaving directory `/home/zhengqi/R_HOME/R-2.1.1/tests/Examples'
make[2]: *** [test-Examples] error 2
make[2]: Leaving directory `/home/zhengqi/R_HOME/R-2.1.1/tests'
make[1]: *** [test-all-basics] error 1
make[1]: Leaving directory `/home/zhengqi/R_HOME/R-2.1.1/tests'
make: *** [check] error 2"

What's the mean of the error messeage listed up? Why it happens in my computer? Which kind of problems is it, software's or hardware's? I just  ignored the mistakes, as it says in your installation manual that this kind of failures "is not  necessarilly problems as they might be caused by missing functionality", until I met with the following problem.

The second bug occured when I was doing some ordinary statistical work. Suppose v1 is a numerical vector of length(v1)==20; further suppose 
> v1 <- c(1:20).
When I type the command
> lines(density(v1))    
or
> rug(v1)
simillar errors take place, and it prompts the following messages below: ( some key words of the promption are translated into English by myself)
For the first command, it warns:
"error occurs at: plot.xy(xy.coords(x, y), type = type, col = col, lty = lty, ...) :
        'plot.new' hasn't been called yet"

and for the second command, it warns:
"error occurs at: axis(side, at = x, lab = FALSE, lwd = lwd, ...) :
        'plot.new' hasn't been called yet"
Besides: warnings:
some values will be clipped in: rug(v1)"

These are some description of the problems I met during the installing and running the R environment. And if you need more detail description of those problems, please let me know and I will gather any information about them as much as possible. 

Waiting for your reply!

Best wishes to everyone!
                      
                                                            Sincerely yours,

                                          Anonymous user from Beijing, China



From Vidar.Hjellvik at imr.no  Thu Sep 22 08:34:12 2005
From: Vidar.Hjellvik at imr.no (Hjellvik Vidar)
Date: Thu, 22 Sep 2005 08:34:12 +0200
Subject: [R] source(file) => file becomes readonly
Message-ID: <0D54094F69391A438650C6151B0A33E46FD358@post2.imr.no>

That sounds fine! It will be as before in version 2.2.* then? The editor I use is Vim, which says that the file "is read-only", but I can override the protection. Wordpad says that the file is "in use by another application". In notepad it is saved without protests. It's especially when I use the browser() I like to edit and save the file while R has it open. 

Regards,
Vidar

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
Sent: 22. september 2005 02:12
To: Prof Brian Ripley
Cc: Hjellvik Vidar; r-help at stat.math.ethz.ch; Uwe Ligges
Subject: Re: [R] source(file) => file becomes readonly


Prof Brian Ripley wrote:
> This was not really accurate.
> 
> Being `read-only' is a Windows phenomenon on open files (even files open 
> for reading).  In 2.0.1 and 2.1.1 the file was open whilst it was parsed 
> and so `read-only' during that time. In 2.0.1 it was then closed, but in 
> 2.1.1 it was (potentially) opened several times to find an encoding, 
> parsed and left open until source() terminated.  In 2.2.0 it will be 
> closed once it has been parsed.

But R doesn't open files in an exclusive mode.  It opens them in "share 
deny none" mode (_SH_DENYNO; this is an undocumented feature of the MSVC 
run-time library that MinGW uses).  I suspect whatever editor Hjellvik 
was using attempted to open the file with exclusive access, and when 
that failed (because R had it open), it fell back to read-only access.

It's certainly possible to open a file for write access in another 
process while R has it open for read access.  You just can't get 
exclusive access to it.

Duncan Murdoch
> 
> Changing a file after it has been parsed will have no effect, so there is 
> no reason to prevent it (beyond that was the simplest way to implement 
> choosing a suitable encoding).
> 
> On Thu, 1 Sep 2005, Hjellvik Vidar wrote:
> 
> 
>>The OS is Windows XP.
>>I use to work on the file while it's executed and save changes 
>>continually. It doesn't seem to have any effect on the current 
>>execution. I just find it annoying not to be able to do it. It's not 
>>crucial, but if there is some easy way around it I would like to 
>>know....
>>
>>-----Original Message-----
>>From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
>>Sent: 1. september 2005 14:21
>>To: Hjellvik Vidar
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] source(file) => file becomes readonly
>>
>>
>>Hjellvik Vidar wrote:
>>
>>
>>>Hello,
>>>when I work in R, I write code in a text file that I run with the "source(filename)" command. In R2.1.1 the file is read-only while the source command is executed. This was not the case in R2.0.1. Is this a bug-fix or is it possible not to have the file read-only when executed?
>>
>>So you want to modify a file *while* it is executed?
>>Sounds dangerous to me, you may want to explain further...
>>
>>BTW: Which OS are we talking about?
>>
>>Uwe Ligges
>>
>>
>>
>>>Best regards
>>>Vidar
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
>



From Roger.Bivand at nhh.no  Thu Sep 22 08:54:13 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 22 Sep 2005 08:54:13 +0200 (CEST)
Subject: [R] Questions about R
In-Reply-To: <327370310.03170@gucas.ac.cn>
Message-ID: <Pine.LNX.4.44.0509220848100.23765-100000@reclus.nhh.no>

Thank you for your detailed description. Please look at what:

> capabilities()
    jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo 
    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE 
  cledit  IEEE754    iconv 
    TRUE     TRUE     TRUE 

says on your installation. If X11 is FALSE, then you need to be sure that 
your system has X11 development libraries installed - with X11 FALSE, 
graphics to screen will fail. At the Linux shell prompt (on RedHat EL3, 
but also using rpm packages), I see 

$ rpm -qa X*devel*
XFree86-devel-4.3.0-95.EL

Your development package may be called something else, but I think that
the development package appropriate for your X11 installation is what you
are missing.


On Thu, 22 Sep 2005, [gb2312] ???????? wrote:

>                                                                 Sep, 22nd,2005
> Dear Authors,
> Thanks for reading this email. I'm a graduate student from China  (PRC) and learning the R at present. Now I have some questions to ask you as I have met some strange problems during installing and running the R environment, which can not be found in the "R FAQ" document (or just I can not find them). 
> 
> It might be useful that I gived you the information of my operation system and hardwares. I am using R on a Mandrake Linux 10.1 operation system on a PC platform parchased from IBM company. The mainboard of my computer is Intel 915PG, that means it has an integrated graphic accelerator.
> 
> The first question is about the error occurs during my installation. Following the "R Installation and Administration" manual, I successully downloaded the latest version of R, unziped the file, and also completed the progress "./configure" and "make" successully. By the way, I managed to install a developmental package of Linux named "r77", which may be a FORTRAN developmental package, before installing the R environment into my computer. However, an error occured when I type the commond "make check" or "make check FORCE=FORCE" or "make check-devel", and the warnings displayed are written below:( Somewords are translated from Chinese into English by myself)
> 
> "running code in 'graphics-Ex.R' ...make[4]: *** [graphics-Ex.Rout] error 1
> make[4]: Leaving directory `/home/zhengqi/R_HOME/R-2.1.1/tests/Examples'
> make[3]: *** [test-Examples-Base] error 2
> make[3]: Leaving directory `/home/zhengqi/R_HOME/R-2.1.1/tests/Examples'
> make[2]: *** [test-Examples] error 2
> make[2]: Leaving directory `/home/zhengqi/R_HOME/R-2.1.1/tests'
> make[1]: *** [test-all-basics] error 1
> make[1]: Leaving directory `/home/zhengqi/R_HOME/R-2.1.1/tests'
> make: *** [check] error 2"
> 
> What's the mean of the error messeage listed up? Why it happens in my computer? Which kind of problems is it, software's or hardware's? I just  ignored the mistakes, as it says in your installation manual that this kind of failures "is not  necessarilly problems as they might be caused by missing functionality", until I met with the following problem.
> 
> The second bug occured when I was doing some ordinary statistical work. Suppose v1 is a numerical vector of length(v1)==20; further suppose 
> > v1 <- c(1:20).
> When I type the command
> > lines(density(v1))    
> or
> > rug(v1)
> simillar errors take place, and it prompts the following messages below: ( some key words of the promption are translated into English by myself)
> For the first command, it warns:
> "error occurs at: plot.xy(xy.coords(x, y), type = type, col = col, lty = lty, ...) :
>         'plot.new' hasn't been called yet"
> 
> and for the second command, it warns:
> "error occurs at: axis(side, at = x, lab = FALSE, lwd = lwd, ...) :
>         'plot.new' hasn't been called yet"
> Besides: warnings:
> some values will be clipped in: rug(v1)"
> 
> These are some description of the problems I met during the installing and running the R environment. And if you need more detail description of those problems, please let me know and I will gather any information about them as much as possible. 
> 
> Waiting for your reply!
> 
> Best wishes to everyone!
>                       
>                                                             Sincerely yours,
> 
>                                           Anonymous user from Beijing, China
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From andrew.h.west at gmail.com  Thu Sep 22 06:09:52 2005
From: andrew.h.west at gmail.com (Andrew West)
Date: Thu, 22 Sep 2005 00:09:52 -0400
Subject: [R] automatic ARIMA best-fitting
Message-ID: <84bfab8705092121093b9f2e76@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050922/c19f71d8/attachment.pl

From ripley at stats.ox.ac.uk  Thu Sep 22 09:33:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Sep 2005 08:33:04 +0100 (BST)
Subject: [R] controlling usage of digits & scientific notation in R
 plots; postscript margins
In-Reply-To: <4331D3DC.2000204@ku.edu>
References: <4331D3DC.2000204@ku.edu>
Message-ID: <Pine.LNX.4.61.0509220810011.13324@gannet.stats>

Concise answers to verbose questions:

1) Use options(scipen) (and probably change the margin sizes).
Or something like

options(scipen=10)
par(mar=c(5,8,4,2)+0.1)
plot(x, y, axes=FALSE)
axis(2, las=2)
axis(1, labels=FALSE)
axis(1, at = c(-2e8, 2e8), labels = expression(-2 %*% 10^8, 2 %*% 10^8))

2) This is a function of the including application, in particular the 
macro package used in latex.  Some use the bounding box (rather than the 
paper size) for files marked with EPSF in the header, which is done if 
onefile=FALSE (sic).  For others you need paper="special" as well.
Hence the advice in ?postscript.  Just follow it!


On Wed, 21 Sep 2005, Paul Johnson wrote:

> Dear R users:
>
> I assigned students to make some graphs and I'm having trouble answering
> some questions that they have.  We are all working on R 2.1 on Fedora
> Core Linux 4 systems.

There is no `R 2.1': please do study the posting guide.

> 1. In the plot, the axis is not labeled by "numbers", but rather
> scientific notation like "-2e+08" or such.  We realize that means
> -200,000,000.  We want to beautify the plot. We would rather just print
> out the whole, big  number. But if we can't have that, we would like
> something more beautiful and  mathematical, such as
>                  8
>           -2.0x10
>
> Once the numbers are big, R automagically switches to scientific
> notation, and turning the values horizontal does not help on the y axis.
>
> Example:
>
> x <- rnorm(100,mean=100000,sd=100000000)
> y <- rnorm(100,mean=10000000000,sd=100000000000)
> plot(x,y,axes=F)
> axis(2,las=2)  #turns y axis numbers perpendicular to y axis
> axis(1)  # x axis
>
> I realize a person could just divide all numbers by 1 million and then
> have a graph with -200 and an annotation (in millions).
>
> On the x axis, we'd even settle to just label the two outermost points
> with full numerical values, and then have only ticks between.  I was
> looking for some way to use axis() to draw an unlabeled axis and then
> put in text labels after that.  However, I can't understand how to get
> the values of the tick marks placed by axis from the figure in order to
> place text by some tick marks.
>
> 2. Some students make postscript figures that fit "just right" into
> LaTeX documents, while some make figures that have huge,
> inches-and-inches of margins around the outside.  I'm not watching how
> they make these figures all the time, but I think I'm figuring out the
> cause of big margins.
>
> Is this right: the margins issue relates to the use of the onefile=F
> option in a dev.copy command?  I think the figures turn out properly
> with this:
>
> dev.copy(postscript,file="myfig.eps",height=5,width=5,horizontal=F,onefile=F)
> dev.off()
>
> If you mistakenly omit the onefile=F option, the margins stretch out
> from the center where the figure is placed to the edges of an entire
> sheet of paper. In other words, the default settings for margins in
> inches that I see in output from
>
> > par()
> ...
> $mai
> [1] 1.0066821 0.8092935 0.8092935 0.4145162
> ...
>
> have no effect if a person forgets the onefile=F option.  We fiddled a
> while, trying to force margins down, par(mai=c(0,0,0,0)), but no matter
> what we did, the figures still had the massive margins.
>
> We don't have these margin problems with other devices. Margins in jpeg
> or png pictures are sized appropriately.
>
>
> -- 
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66044-3177           FAX: (785) 864-5700
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vehbisinan at gmail.com  Thu Sep 22 10:08:39 2005
From: vehbisinan at gmail.com (Vehbi Sinan Tunalioglu)
Date: Thu, 22 Sep 2005 11:08:39 +0300
Subject: [R] MGARCH estimation
In-Reply-To: <433190F3.3070400@pburns.seanet.com>
References: <139ef1c20509210753d017b86@mail.gmail.com>
	<433190F3.3070400@pburns.seanet.com>
Message-ID: <43326687.3090501@gmail.com>

There is a package available at
http://www.vsthost.com/materials/wordpress/mgarchBEKK_0.07-6.tar.gz

Actually we were planning to make a later release, but this might help you.

Regards,
Vehbi Sinan

Patrick Burns wrote:
> There is a Burns Statistics working paper that describes
> an approach to getting multivariate garch when only
> univariate garch estimates are available.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Krishna wrote:
> 
> 
>>Hi R-users
>>
>>Can the users let me know how to do MGARCH estimate (Bivariate GARCH)
>>and volatility forecast for 2 variables in R.
>>
>>thanks and regards
>>
>>snvk
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>
>> 
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vinum at iinet.net.au  Thu Sep 22 10:10:48 2005
From: vinum at iinet.net.au (John Charles Considine)
Date: Thu, 22 Sep 2005 16:10:48 +0800
Subject: [R] xyplot and abline
Message-ID: <1127376648.7491.19.camel@Tardis.considine.local>

How should I pass abline to this function so that I get a reference line
at h=0 in each panel?

sps <- trellis.par.get("superpose.symbol")
sps$pch <- 1:10
trellis.par.set("superpose.symbol",sps)
xyplot(fcast$LOGDIFF~fcast$VINTAGE|fcast$REGION,
       groups=fcast$LAG,
       panel=panel.superpose,
       type="b",mfcol=c(3,5),
       key=list(columns=3, 
         text=list(paste(c("forecast:  ", "",""),
           unique(fcast$LAG), "years")),
       points=Rows(sps,1:3)))

Cheers,

JC



From spencer.graves at pdf.com  Thu Sep 22 10:23:46 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Sep 2005 03:23:46 -0500
Subject: [R] accessing source code in R packages
In-Reply-To: <BAY103-F23DB78772C4AD21BC8A19BA6970@phx.gbl>
References: <BAY103-F23DB78772C4AD21BC8A19BA6970@phx.gbl>
Message-ID: <43326A12.3080501@pdf.com>

	  Is there general documentation on a procedure to follow to:

	  (a) Find what methods are available for a particular class of objects?

	  (b) Find what classes of objects have methods defined for a partilar 
generic function?

	  (c) Get the code that's actually used?

	  For example, I recently needed to access numbers associated with an 
object of class "lmer".  Sundar suggested I use with 'getMethod("show", 
"summary.lmer")'.  However, this doesn't work with the example below.

	  Thanks,
	  spencer graves

Francisco J. Zagmutt wrote:

> or getAnywhere()
> 
> 
>> From: "ronggui.wong" <042045003 at fudan.edu.cn>
>> Reply-To: 042045003 at fudan.edu.cn
>> To: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
>> Subject: Re: [R] accessing source code in R packages
>> Date: Thu, 22 Sep 2005 10:03:45 +0800
>>
>>
>>
>> >R is open source.  You can download the source code from CRAN.
>> >
>> >If you mean at the R prompt, usually you see the code for a function by
>> >typing the name of the function at the R prompt, without parentheses.
>> >`Usually' because some methods are delibrately `hidden' from users, and
>> >should only be accessed through their generics.  There are still ways 
>> to get
>> >around that.
>> sometimes,the getS3method is helpfull.
>>
>> >library(MASS)
>> > princomp.default
>> Error: object "princomp.default" not found
>> > getS3method("princomp","default")
>> function (x, cor = FALSE, scores = TRUE, covmat = NULL, subset = 
>> rep(TRUE,
>>     nrow(as.matrix(x))), ...)
>> {
>> ----snip----
>>
>>
>> >
>> >However, the code you get at the R prompt is not the _source_, as it 
>> does
>> >not contain any original comments.  You need to go to the source I 
>> refer to
>> >above.
>> >
>> >Andy
>> >
>> >> From: ritesh at pdx.edu
>> >>
>> >> Hi,
>> >>
>> >> I am new the R world and would like to know how can I access
>> >> source codes of
>> >> standard functions in R?
>> >>
>> >> Thanks,
>> >> Ritesh.
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide!
>> >> http://www.R-project.org/posting-guide.html
>> >>
>> >>
>> >>
>> >
>> >______________________________________________
>> >R-help at stat.math.ethz.ch mailing list
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>> = = = = = = = = = = = = = = = = = = = =
>>
>>
>> ????????????????????????????????????
>> ????????
>>
>>
>> ????????????????????????????????ronggui.wong
>> ????????????????????????????????042045003 at fudan.edu.cn
>> ????????????????????????????????????????2005-09-22
>>
> 
> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From petr.pikal at precheza.cz  Thu Sep 22 10:42:43 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 22 Sep 2005 10:42:43 +0200
Subject: [R] mark values as NA in matrix
In-Reply-To: <6.2.1.2.0.20050921195824.02022a78@email.med.yale.edu>
Message-ID: <43328AA3.22692.822DDD@localhost>

Hi

there is not much to evaluate what you actually want 

mymatrix[1,2] <- NA

changes respective value in row1 and col2 to NA

or

which(is.na(mymatrix), arr.ind=T)

gives you positions of NA in mymatrix.

HTH
Petr

 

On 21 Sep 2005 at 20:07, MK wrote:

> 
> 
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Hummel at mpimp-golm.mpg.de  Thu Sep 22 11:04:24 2005
From: Hummel at mpimp-golm.mpg.de (Jan Hummel)
Date: Thu, 22 Sep 2005 11:04:24 +0200
Subject: [R] SAX Parser best practise
Message-ID: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E77A@EMAIL.mpimp-golm.mpg.de>

Thank you Seth  and Duncan for your input! 

> BTW, do you have a schema for the XML document you are working on?

Yes, a schema is available here
http://psidev.sourceforge.net/ms/xml/mzdata/mzdata.xsd
Informations around mzData xml format are available here
http://psidev.sourceforge.net/ms/#mzdata

Next question I want to come up with: is there a way to validate xml
again a schema or a dtd while parsing using xmlEventParse()?

cheers
	Jan



From EndrikatKirsten at aol.com  Thu Sep 22 11:08:15 2005
From: EndrikatKirsten at aol.com (EndrikatKirsten@aol.com)
Date: Thu, 22 Sep 2005 05:08:15 EDT
Subject: [R] Answer about Mr. Giltinan
Message-ID: <1ef.4496de27.3063ce7f@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050922/69aa1b43/attachment.pl

From ripley at stats.ox.ac.uk  Thu Sep 22 11:10:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Sep 2005 10:10:08 +0100 (BST)
Subject: [R] accessing source code in R packages
In-Reply-To: <43326A12.3080501@pdf.com>
References: <BAY103-F23DB78772C4AD21BC8A19BA6970@phx.gbl>
	<43326A12.3080501@pdf.com>
Message-ID: <Pine.LNX.4.61.0509220936420.14847@gannet.stats>

The original reply was deliberately (I guess) vague. (I've removed the 
history, as attributions had already been removed, in violation of 
copyright law. If you cite someone, you MUST credit the author.)

Sometimes a little knowledge is a dangerous thing, and we have had a 
number of partially true answers.

Spreading confusion between the S4 classes of the 'methods' package and 
the (sometimes called S3) classes of base R is also dangerous.  The R 
documentation refers to S3 methods and classes unless otherwise stated 
(and in the methods package documentation).  Please follow that lead.

On Thu, 22 Sep 2005, Spencer Graves wrote:

> 	  Is there general documentation on a procedure to follow to:
>
> 	  (a) Find what methods are available for a particular class of 
> objects?

?methods, unless you mean an S4 class.

Be careful here: methods `for a particular class' are not all that might 
be dispatched, as methods for classes the object inherits from may also be 
used.  Thus "lm" methods may be invoked for "glm" objects, and you may 
need to call methods() for all the classes the object inherits from.

> 	  (b) Find what classes of objects have methods defined for a partilar
> generic function?

?methods, unless you mean S4 classes (and that help page leads you to the
right place for those).

> 	  (c) Get the code that's actually used?

getAnywhere() on the asterisked results of (a) or (b).

For a specific generic and a specific class, getS3method().

[There is a potential gap here as the "bar" method for class "foo" need 
not be called foo.bar().  So guessing the name may not work, but 
getS3method("foo", "bar") will.  AFAIK there are no live examples of 
this.]

> 	  For example, I recently needed to access numbers associated with an
> object of class "lmer".  Sundar suggested I use with 'getMethod("show",
> "summary.lmer")'.  However, this doesn't work with the example below.

(I think that was intended to refer to the default method for princomp, 
which is not an S4 generic in base R.

> methods("princomp")
[1] princomp.default* princomp.formula*

    Non-visible functions are asterisked
> getAnywhere("princomp.default")    # works
> getS3Method("princomp", "default") # works
> showMethods("princomp")

Function "princomp":
  <not a generic function>
)

show() is an S4 generic, not an S3 generic.  ?methods points you to how to 
explore S4 generics.

> library(lme4)
... (and drink some coffee while you wait)
> methods(show)
no methods were found
Warning message:
function 'show' appears not to be generic in: methods(show)
> showMethods("show")

Function "show":
object = "ANY"
object = "traceable"
object = "ObjectsWithPackage"
object = "MethodDefinition"
object = "MethodWithNext"
object = "genericFunction"
object = "classRepresentation"
object = "ddenseMatrix"
object = "Matrix"
object = "lmer"
object = "summary.lmer"
object = "VarCorr"
object = "sparseMatrix"
object = "lmList"

> selectMethod("show", "summary.lmer")
Method Definition:

function (object) ...

Here getMethod() will also work, but selectMethod() is more likely to 
find `the code that's actually used'.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From frank.schmid at vwi.unibe.ch  Thu Sep 22 11:23:10 2005
From: frank.schmid at vwi.unibe.ch (Frank Schmid)
Date: Thu, 22 Sep 2005 11:23:10 +0200
Subject: [R] R: extracting elements in a matrix
Message-ID: <0IN700GAJOQ13N@ubecx01.unibe.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050922/d8e8af7b/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Thu Sep 22 11:42:55 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 22 Sep 2005 11:42:55 +0200
Subject: [R] R: extracting elements in a matrix
References: <0IN700GAJOQ13N@ubecx01.unibe.ch>
Message-ID: <00e001c5bf5a$0246e690$0540210a@www.domain>

you could try this:

mat <- matrix(rnorm(100 * 10), 100, 10)

ind <- apply(mat, 2, function(x){
    iqr <- diff(quantile(x, probs = c(0.25, 0.75)))
    x[x > 2 * iqr]
})


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Frank Schmid" <frank.schmid at vwi.unibe.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, September 22, 2005 11:23 AM
Subject: [R] R: extracting elements in a matrix


> Dear R-users
>
> For a given matrix of dimension, say (n,p), I'd like to extract for 
> every
> column those elements that are bigger than twice the interquartile 
> range of
> the corresponding column.
>
> Can I get these elements without using a loop?
>
>
> Thank you for your help
>
>
> Frank
>
>
>
>
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From pwolf at wiwi.uni-bielefeld.de  Thu Sep 22 11:47:15 2005
From: pwolf at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Thu, 22 Sep 2005 11:47:15 +0200
Subject: [R] R: extracting elements in a matrix
In-Reply-To: <0IN700GAJOQ13N@ubecx01.unibe.ch>
References: <0IN700GAJOQ13N@ubecx01.unibe.ch>
Message-ID: <43327DA3.8060203@wiwi.uni-bielefeld.de>

... you can, for example

set.seed(13)
x<-matrix(rnorm(100),20,5)
lapply(split(x,col(x)),
       function(x){
         limit<-diff(quantile(x,c(.25,.75)))
         x[x>limit]
       })
output:
$"1"
[1] 1.775163 1.142526 1.229507 1.105144 1.396432
$"2"
[1] 1.836163
$"3"
[1] 1.590034 1.614479 1.408354 1.483691
$"4"
[1] 1.745427
$"5"
[1] 1.602120

Peter Wolf

Frank Schmid wrote:
>Dear R-users
> 
>For a given matrix of dimension, say (n,p), I'd like to extract for every
>column those elements that are bigger than twice the interquartile range of
>the corresponding column. 
> 
>Can I get these elements without using a loop?
> 
> 
>Thank you for your help
> 
> 
>Frank
> 
> 
> 
>
>		
>	
> 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jfontain at free.fr  Thu Sep 22 12:01:24 2005
From: jfontain at free.fr (jfontain@free.fr)
Date: Thu, 22 Sep 2005 12:01:24 +0200
Subject: [R] automatic ARIMA best-fitting
In-Reply-To: <84bfab8705092121093b9f2e76@mail.gmail.com>
References: <84bfab8705092121093b9f2e76@mail.gmail.com>
Message-ID: <1127383284.433280f4221fe@imp5-g19.free.fr>

Quoting Andrew West <andrew.h.west at gmail.com>:

> Take a look at Hyndman's Forecast package, and dse1 & 2, and see if that
> will do what you need.

Looks extremely interesting, especially the Hyndman package.

Thank you very much indeed.

--
Jean-Luc



From pwolf at wiwi.uni-bielefeld.de  Thu Sep 22 12:07:02 2005
From: pwolf at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Thu, 22 Sep 2005 12:07:02 +0200
Subject: [R] bagplot - a rough R function
In-Reply-To: <17201.33420.153819.995140@stat.math.ethz.ch>
References: <web-1865915@calmail-be1.berkeley.edu>
	<Pine.LNX.4.44.0409131645200.19598-100000@gannet.stats>
	<16715.4933.392083.824916@gargle.gargle.HOWL>
	<433172E6.70901@wiwi.uni-bielefeld.de>
	<17201.33420.153819.995140@stat.math.ethz.ch>
Message-ID: <43328246.2030101@wiwi.uni-bielefeld.de>

Martin Maechler wrote:
>>>>>>"Peter" == Peter Wolf <pwolf at wiwi.uni-bielefeld.de>
>>>>>>    on Wed, 21 Sep 2005 16:49:10 +0200 writes:
>>>>>>            
>
>    Peter> In the moment I am writing an R function for drawing
>    Peter> bagplots (two dimensional boxplots).
>    Peter> For some elements of the plot are found numerically
>    Peter> the resulting plots differs a little bit from the correct bagplots.
>
>    Peter> Here is the link to the actual version:
>
>    Peter> http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/bagplot/bagplot.R 
>    Peter> ( R code )
>
>    Peter> http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/bagplot/bagplot.pdf 
>    Peter> (examples and description)
>
>    Peter> Now I am looking for examples to test / to improve my function.
>    Peter> Where can I get data sets and the corresponding correct bagplots?
>
>I'll send you my (unpublished) version of my package 'bagplot'
>{{the one I've never released because of segmentation fault bugs
>  somewhere in Fortran}} in a private E-mail.
>
>In my version I had started make a function
>   bag() 
>which *computes* all the information needed and then returns an
>(S3) object "bag".  Then, the plotting happens via
>the 
>    plot.bag <- function(.....) {..........}
>method.
>
>So you can say
>
>   plot(bag(.........), ........)
>
>if you want both (computing and plotting) at once, but can also
>separate the two --- following very much the modern "S" way
>"whole object ..".
>
>
>Martin
>
>  
Hello Martin,

you are right. Separating computing and plotting is a better way for
a public version and I think that I will split my function. But
during testing algorithmic problems it is nice to have only one function.

Peter



From Ted.Harding at nessie.mcc.ac.uk  Thu Sep 22 12:04:36 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 22 Sep 2005 11:04:36 +0100 (BST)
Subject: [R] Questions about R
In-Reply-To: <327370310.03170@gucas.ac.cn>
Message-ID: <XFMail.050922110436.Ted.Harding@nessie.mcc.ac.uk>

On 22-Sep-05 ???????? wrote:
>                                                                 Sep,
> 22nd,2005
> Dear Authors,
> Thanks for reading this email. I'm a graduate student from China  (PRC)
> and learning the R at present.

Welcome!

I can't respond about the error messages you got when you were
installing R -- other people know more about what goes on during
the 'make' phase.

However:

> [...]
> The second bug occured when I was doing some ordinary statistical work.
> Suppose v1 is a numerical vector of length(v1)==20; further suppose 
>> v1 <- c(1:20).
> When I type the command
>> lines(density(v1))    
> or
>> rug(v1)
> simillar errors take place, and it prompts the following messages
> below: ( some key words of the promption are translated into English
. by myself)
> For the first command, it warns:
> "error occurs at: plot.xy(xy.coords(x, y), type = type, col = col, lty
> = lty, ...) :
>         'plot.new' hasn't been called yet"
> 
> and for the second command, it warns:
> "error occurs at: axis(side, at = x, lab = FALSE, lwd = lwd, ...) :
>         'plot.new' hasn't been called yet"
> Besides: warnings:
> some values will be clipped in: rug(v1)"

Both of these represent the same type of error. The point is that
both the 'lines' and 'rug' commands *add* elements to an *existing*
plot, so neither will work unless there is already a plotted graph
in existence that they can add something to.

So (starting in a new R session):

  v1<-c(1:20)
  lines(v1)
  # Error in plot.xy(xy.coords(x, y), type = type, col = col, lty = lty,
  # ...) : 
  #         plot.new has not been called yet

However,

  plot(v1)
  lines(v1)

works by adding the lines to the existing plot of points.

Similarly (again starting from scratch):

  v1<-c(1:20)
  rug(v1)
  #Error in axis(side, at = x, lab = FALSE, lwd = lwd, ...) :
  #        plot.new has not been called yet
  #In addition: Warning message: 
  #some values will be clipped in: rug(v1)

However,

  plot(v1)
  rug(v1)

again works, adding the "rug" lines to the x-axis of the existing plot.

In each case the underlying reason can be seen if you read carefully
the opening lines of the "help" documentation for these functions,
which is shown if you enter

  ?lines

  --> "Add Connected Line Segments to a Plot"

and

  ?rug

  --> "Add a Rug to a Plot

       Description:

       Adds a _rug_ representation (1-d plot) of the data to the plot."

The critical word in these is "Add". And you can of course throw
them all in together with:

  plot(v1)
  lines(v1)
  rug(v1)

As a person beginning to learn R, part of what one needs to learn
is that the documentation, available directly with "?" and also
via "help.search", usually contains the information you need
though you may need to interpret the wording of the documentation
carefully, even with the mind of a detective!

For example, the response to "?lines" that it will "Add Connected
Line Segments to a Plot" does not specify that there needs to be
an existing plot. Nor will you find such a statement anywehre in
the response to "?lines". Especially if one's native language is not
English (though I'm sure native speakers have been caught by this
as well), it might be natural to interpret this as allowing that
if the plot does not already exist then it will be created in such
a way as to accommodate the lines which will be added (to this
imaginary "null" object").

The clue in this case is in the error message:

  plot.new has not been called yet

which does indicate that something which needed to be done has not
been done. In conjunction with the "Add" word, this could give you
the idea that maybe you should "plot(v1)" first!

Good luck, and best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Sep-05                                       Time: 11:04:30
------------------------------ XFMail ------------------------------



From kjetil at acelerate.com  Thu Sep 22 04:45:11 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 21 Sep 2005 22:45:11 -0400
Subject: [R] FDR analyses: minimum number of features
In-Reply-To: <43316DBA.7060204@pdf.com>
References: <C6E7B149DFD87D4AAD1ADA90B5DE0279493ACE@mailbe11.mc.vanderbilt.edu>
	<43316DBA.7060204@pdf.com>
Message-ID: <43321AB7.7030708@acelerate.com>

Spencer Graves wrote:

>	  Two thoughts on this:
>
>	  1.  Your FDR (Not Franklin Delano Roosevelt) sounds like another name 
>for Type I error rate. 
>
It is certainly not the same as type I error rate. Type I error rate is 
the proportion of true
nulls which are rejected, while the FDR is the proportion of rejected 
null hypothesis
which really are true nulls!

To me FDR seems like a more promising avenue to multiple testing than 
the old
"familywise error rate". Who knows what is a family?

Kjetil

> The definition of "reasonably reliable FDRs" 
>would seem to relate to the status of the literature on this issue among 
>researchers in genotyping.  As more reports of FRDs in genotyping are 
>published, I would expect that methodology for estimation and the 
>standard for accuracy would similarly evolve.
>
>	  2.  Have you tried the Bioconductor (www.bioconductor.org/) 
>listserve?  They might be able to say something more useful than a 
>general list like this.
>
>	  spencer graves
>
>Dupont, William wrote:
>
>  
>
>>Dear List,
>>
>>We are planning a genotyping study to be analyzed using false discovery
>>rates (FDRs) (See Storey and Tibshirani PNAS 2003; 100:9440-5).  I am
>>interested in learning if there is any consensus as to how many
>>features (ie. how many P values) need to be studied before reasonably
>>reliable FDRs can be derived.  Does anyone know of a citation where
>>this is discussed?
>>
>>Bill Dupont 
>>
>>William D. Dupont          phone: 615-343-4100          URL
>>http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/WilliamDupont
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>    
>>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From anne.piotet at gmail.com  Thu Sep 22 12:32:25 2005
From: anne.piotet at gmail.com (Anne)
Date: Thu, 22 Sep 2005 11:32:25 +0100
Subject: [R] (survexp:) compute the expected remaining survival time
Message-ID: <80102e8805092203321d7bf45a@mail.gmail.com>

DeaR list
I would like to know if there is a direct method  to compute  the
expected remaining survival time for a subject having survived up to
time t. survexp gives me the probabilty for subject S to survive up to
day D


Thanks
--
Anne



From vinum at iinet.net.au  Thu Sep 22 12:39:35 2005
From: vinum at iinet.net.au (John Charles Considine)
Date: Thu, 22 Sep 2005 18:39:35 +0800
Subject: [R] xyplot and abline
In-Reply-To: <5F5D04205B6C124CA7FF7A001CEE37E101E7FFFB@bibex05.eu.boehringer.com>
References: <5F5D04205B6C124CA7FF7A001CEE37E101E7FFFB@bibex05.eu.boehringer.com>
Message-ID: <1127385575.7098.1.camel@Tardis.considine.local>

On Thu, 2005-09-22 at 10:35 +0200,
Vincent.Duval at bc.boehringer-ingelheim.com wrote:
> Use panel.superpose into a panel function
> Here is some changes in your function that should do the work, 
> I haven't tried it but at least it should be closed to it, 
> HTH
> Vincent
> 
Perfect.  Thank you.

> Suggestions
> xyplot(fcast$LOGDIFF~fcast$VINTAGE|fcast$REGION,
>        groups=fcast$LAG,
>        type="b",mfcol=c(3,5),
>        panel=function(x,y,...){
> 		panel.superpose(x,y,...)
> 		panel.abline(h=0)
> 	},
>        key=list(columns=3, 
>          text=list(paste(c("forecast:  ", "",""),
>            unique(fcast$LAG), "years")),
>        points=Rows(sps,1:3)))
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Charles
> Considine
> Sent: Thursday, September 22, 2005 10:11 AM
> To: r-help
> Subject: [R] xyplot and abline
> 
> 
> "MMS - ING02 <ing.boehringer-ingelheim.com>" made the following annotations.
> ============================================================================
> ==
> "This e-mail was identified as a newsletter.  Please decide if this e-mail
> is of business relevance, if it is not, please delete it. Thank you".
> ============================================================================
> ==
> 
> How should I pass abline to this function so that I get a reference line
> at h=0 in each panel?
> 
> sps <- trellis.par.get("superpose.symbol")
> sps$pch <- 1:10
> trellis.par.set("superpose.symbol",sps)
> xyplot(fcast$LOGDIFF~fcast$VINTAGE|fcast$REGION,
>        groups=fcast$LAG,
>        panel=panel.superpose,
>        type="b",mfcol=c(3,5),
>        key=list(columns=3, 
>          text=list(paste(c("forecast:  ", "",""),
>            unique(fcast$LAG), "years")),
>        points=Rows(sps,1:3)))
> 
> Cheers,
> 
> JC
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From adrian at maths.uwa.edu.au  Thu Sep 22 12:44:11 2005
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Thu, 22 Sep 2005 18:44:11 +0800
Subject: [R]  ppp from SpatStat
In-Reply-To: <mailman.11.1127383200.22046.r-help@stat.math.ethz.ch>
References: <mailman.11.1127383200.22046.r-help@stat.math.ethz.ch>
Message-ID: <17202.35579.59294.892920@maths.uwa.edu.au>


 "Rainer M. Krug" writes:

> I want to extract the points from an object of type ppp.
> How can I do this?

If `p' is an object of class "ppp" in spatstat,
then p$x and p$y are vectors containing the x and y coordinates
of the points. See help(ppp.object).

Adrian Baddeley

----
PS: Enquiries about contributed packages should be sent 
to the package authors or maintainers. 
Use library(help=<packagename>) to find out who to ask.



From arv at ono.com  Thu Sep 22 13:02:51 2005
From: arv at ono.com (antonio =?iso-8859-1?q?rodr=EDguez?=)
Date: Thu, 22 Sep 2005 13:02:51 +0200
Subject: [R] chinasea.txt data for grid package
In-Reply-To: <432A990D.5000300@columbia.edu>
References: <200509160822.11737.Sebastian.Leuzinger@unibas.ch>
	<432A8402.7080404@statistik.uni-dortmund.de>
	<432A990D.5000300@columbia.edu>
Message-ID: <200509221302.52160.arv@ono.com>

Can't find the mentioned database in the URL given by Paul Murrell in Rnews. 
Could anybody send it to me. Thanks in advance,

Antonio Rodr??guez



From zhengqi04 at mails.gucas.ac.cn  Thu Sep 22 13:35:16 2005
From: zhengqi04 at mails.gucas.ac.cn (=?gb2312?B?1qPkvw==?=)
Date: Thu, 22 Sep 2005 19:35:16 +0800
Subject: [R] It seems that it is not the reason causing my problems
Message-ID: <327388916.14473@gucas.ac.cn>

Dear Authors:

Thanks for your advice on my question. However, it seems that it is not the reason that your told in your reply. I just opened a new R session and type the command "capabilities()", and it displayed indentical results to the "right" ones described in your letter, that is:
"> capabilities()
    jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
  cledit  IEEE754    iconv
    TRUE     TRUE     TRUE"
And I can not pass the "make check" all the same. Is there any other possible reasons which could cause this depressing problem?

Thanks! 
                                                         Sincerely yours,
                                                         Anonymous user



From Roger.Bivand at nhh.no  Thu Sep 22 13:38:16 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 22 Sep 2005 13:38:16 +0200 (CEST)
Subject: [R] It seems that it is not the reason causing my problems
In-Reply-To: <327388916.14473@gucas.ac.cn>
Message-ID: <Pine.LNX.4.44.0509221332280.23915-100000@reclus.nhh.no>

On Thu, 22 Sep 2005, [gb2312] ???????? wrote:

> Dear Authors:
> 
> Thanks for your advice on my question. However, it seems that it is not
> the reason that your told in your reply. I just opened a new R session
> and type the command "capabilities()", and it displayed indentical
> results to the "right" ones described in your letter, that is:

> "> capabilities()
>     jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
>     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
>   cledit  IEEE754    iconv
>     TRUE     TRUE     TRUE"

> And I can not pass the "make check" all the same. Is there any other
> possible reasons which could cause this depressing problem?

There are two problems, and it may be that the second one was already 
answered in Ted Harding's reply. 

To examine the error in make check, you need to review the end of the 
file: tests/Examples/graphics-Ex.Rout, the last section will report which 
commands caused the error.

What happens if you try demo(graphics) in your R session?


> 
> Thanks! 
>                                                          Sincerely yours,
>                                                          Anonymous user
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From rolf at math.unb.ca  Thu Sep 22 13:40:33 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 22 Sep 2005 08:40:33 -0300 (ADT)
Subject: [R] (Off topic) Limit question --- corrected!!!
Message-ID: <200509221140.j8MBeXZJ020338@erdos.math.unb.ca>

It was gently pointed out to me by Ted Harding that my question was a
load of dingos' kidneys.  What happened was that I left out a crucial
factor of 1/k.

Here's the question again, stated correctly this time.
(I think!!!)

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===

Please reply to me directly (rolf at math.unb.ca) rather than to the
list, since the question is completely R-free and I'm simply asking
this list because there are so many clever and knowledgeable people
on it.

Suppose that n_i, i = 1, 2, 3, ... are positive integers, and that

                    1   k
	  lim      --- SUM n_i^j = nu_j < infinity
     k --> infinity k  i=1

for j = 1, 2, 3.  Need it be the case that

                    1  k-1
	  lim      --- SUM n_i * n_{i+1}     exists?
     k --> infinity k  i=1

I can neither prove this, nor come up with a counter-example.
Can anyone help me out?

				cheers,

					Rolf Turner



From tlumley at u.washington.edu  Thu Sep 22 13:43:19 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 22 Sep 2005 04:43:19 -0700 (PDT)
Subject: [R] (survexp:) compute the expected remaining survival time
In-Reply-To: <80102e8805092203321d7bf45a@mail.gmail.com>
References: <80102e8805092203321d7bf45a@mail.gmail.com>
Message-ID: <Pine.LNX.4.63a.0509220441380.29793@homer21.u.washington.edu>

On Thu, 22 Sep 2005, Anne wrote:

> DeaR list
> I would like to know if there is a direct method  to compute  the
> expected remaining survival time for a subject having survived up to
> time t. survexp gives me the probabilty for subject S to survive up to
> day D

You can't estimate expected survival times.  You could estimate  the 
probability of surviving to day D given survival to day t: just divide the 
probability of surviving to day D by the probability of surviving to day 
t.

 	-thomas



From rolf at math.unb.ca  Thu Sep 22 14:12:26 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 22 Sep 2005 09:12:26 -0300 (ADT)
Subject: [R] (Off topic) Limit question solved.
Message-ID: <200509221212.j8MCCQw9021206@erdos.math.unb.ca>


A colleague here at U.N.B. has provided a counter example for me.  So
the conjecture (that the existance of the ``product limit'' is
implied by the existance of the limits involving the first three
powers) is false.

If anyone is interested in the details, they should contact me
off-list.

My apologies for confusing everyone with my initial mis-statement of
the problem.

			cheers,

				Rolf Turner
				rolf at math.unb.ca



From h.brunschwig at utoronto.ca  Thu Sep 22 14:52:05 2005
From: h.brunschwig at utoronto.ca (Hadassa Brunschwig)
Date: Thu, 22 Sep 2005 08:52:05 -0400
Subject: [R] R2WinBUGS: Data loading error
Message-ID: <1127393525.4332a8f56691c@webmail.utoronto.ca>

Hi R-Help!

I am trying to use R2WinBUGS but I get the following error message in WinBUGS
(and there must be something wrong with my R statement as I tried it directly in
WinBUGS and it worked):

display(log)
check(C:/Documents and Settings/Daikon/Roche/pop_model.txt)
model is syntactically correct
data(C:/Documents and Settings/Daikon/Roche/data.txt)
expected key word structure
compile(7)
...(and of course nothing works after that)

and when I close WinBUGS i get:
Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file 'codaIndex.txt' 

Does anyone know what this 'expected key word structure' means?
This is my R code (and I guess my model file is ok):

modelA     <- c("C:/Documents and Settings/Daikon/Roche/pop_model.txt")
n          <- length(unique(subsetA$subject))    #number of subjects
nt         <- 13                                 #number of days
Y          <- subsetA$concentr                   #concentration per day/subject
t          <- 1:13                               #days
dataA      <- list("n","nt","Y","t")
parameters <- c("tau","C0","st90","C0.pop","st90.pop","tau.cpop","tau.stpop")
inits      <-
list(tau=c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),C0=5,st90=4,C0.pop=5,st90.pop=4,tau.cpop=0.2,tau.stpop=1)
mcmcA      <-
bugs(dataA,inits,parameters,modelA,debug=T,n.chains=7,bugs.directory="c:/Program
Files/WinBUGS14",working.directory="C:/Documents and Settings/Daikon/Roche")


Thanks so much...

Hadassa


-- 

Hadassa Brunschwig
Birmannsgasse 10A
CH-4055 Basel
Switzerland
Phone: +41 78 797 6065
Email: h.brunschwig at utoronto.ca



From spencer.graves at pdf.com  Thu Sep 22 14:55:12 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Sep 2005 07:55:12 -0500
Subject: [R] accessing source code in R packages
In-Reply-To: <Pine.LNX.4.61.0509220936420.14847@gannet.stats>
References: <BAY103-F23DB78772C4AD21BC8A19BA6970@phx.gbl>
	<43326A12.3080501@pdf.com>
	<Pine.LNX.4.61.0509220936420.14847@gannet.stats>
Message-ID: <4332A9B0.7000706@pdf.com>

Dear Prof. Ripley:  Thanks.  This looks like a very useful summary. 
spencer graves

Prof Brian Ripley wrote:

> The original reply was deliberately (I guess) vague. (I've removed the 
> history, as attributions had already been removed, in violation of 
> copyright law. If you cite someone, you MUST credit the author.)
> 
> Sometimes a little knowledge is a dangerous thing, and we have had a 
> number of partially true answers.
> 
> Spreading confusion between the S4 classes of the 'methods' package and 
> the (sometimes called S3) classes of base R is also dangerous.  The R 
> documentation refers to S3 methods and classes unless otherwise stated 
> (and in the methods package documentation).  Please follow that lead.
> 
> On Thu, 22 Sep 2005, Spencer Graves wrote:
> 
>>       Is there general documentation on a procedure to follow to:
>>
>>       (a) Find what methods are available for a particular class of 
>> objects?
> 
> 
> ?methods, unless you mean an S4 class.
> 
> Be careful here: methods `for a particular class' are not all that might 
> be dispatched, as methods for classes the object inherits from may also 
> be used.  Thus "lm" methods may be invoked for "glm" objects, and you 
> may need to call methods() for all the classes the object inherits from.
> 
>>       (b) Find what classes of objects have methods defined for a 
>> partilar
>> generic function?
> 
> 
> ?methods, unless you mean S4 classes (and that help page leads you to the
> right place for those).
> 
>>       (c) Get the code that's actually used?
> 
> 
> getAnywhere() on the asterisked results of (a) or (b).
> 
> For a specific generic and a specific class, getS3method().
> 
> [There is a potential gap here as the "bar" method for class "foo" need 
> not be called foo.bar().  So guessing the name may not work, but 
> getS3method("foo", "bar") will.  AFAIK there are no live examples of this.]
> 
>>       For example, I recently needed to access numbers associated with an
>> object of class "lmer".  Sundar suggested I use with 'getMethod("show",
>> "summary.lmer")'.  However, this doesn't work with the example below.
> 
> 
> (I think that was intended to refer to the default method for princomp, 
> which is not an S4 generic in base R.
> 
>> methods("princomp")
> 
> [1] princomp.default* princomp.formula*
> 
>    Non-visible functions are asterisked
> 
>> getAnywhere("princomp.default")    # works
>> getS3Method("princomp", "default") # works
>> showMethods("princomp")
> 
> 
> Function "princomp":
>  <not a generic function>
> )
> 
> show() is an S4 generic, not an S3 generic.  ?methods points you to how 
> to explore S4 generics.
> 
>> library(lme4)
> 
> ... (and drink some coffee while you wait)
> 
>> methods(show)
> 
> no methods were found
> Warning message:
> function 'show' appears not to be generic in: methods(show)
> 
>> showMethods("show")
> 
> 
> Function "show":
> object = "ANY"
> object = "traceable"
> object = "ObjectsWithPackage"
> object = "MethodDefinition"
> object = "MethodWithNext"
> object = "genericFunction"
> object = "classRepresentation"
> object = "ddenseMatrix"
> object = "Matrix"
> object = "lmer"
> object = "summary.lmer"
> object = "VarCorr"
> object = "sparseMatrix"
> object = "lmList"
> 
>> selectMethod("show", "summary.lmer")
> 
> Method Definition:
> 
> function (object) ...
> 
> Here getMethod() will also work, but selectMethod() is more likely to 
> find `the code that's actually used'.
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From william.dupont at Vanderbilt.Edu  Thu Sep 22 15:01:01 2005
From: william.dupont at Vanderbilt.Edu (Dupont, William)
Date: Thu, 22 Sep 2005 08:01:01 -0500
Subject: [R] FW:  FDR analyses: minimum number of features
Message-ID: <C6E7B149DFD87D4AAD1ADA90B5DE0279493AED@mailbe11.mc.vanderbilt.edu>

Dear Dr.  Graves

Many thanks for your response.  FDRs and their associated q values do
differ from Type I error rates and P values (See Storey and Tibshirani
PNAS 2003;100:9440-5).  It is an approach that is rapidly gaining
popularity in the analysis of genomic data where we have massive numbers
of covariates measured on a comparatively modest number of subjects.  To
my mind it is a real advance in dealing with the extreme multiple
comparisons problems that afflict such data.  Unfortunately, it is still
a relatively new technique and I do not believe that a consensus as to
the number of needed response features has been reached.

Submitting my question to the R-help list was a long shot, although
Storey's software for this methodology is written in R.

With best wishes,

Bill Dupont 

-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com]
Sent: Wednesday, September 21, 2005 9:27 AM
To: Dupont, William
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] FDR analyses: minimum number of features

	  Two thoughts on this:

	  1.  Your FDR (Not Franklin Delano Roosevelt) sounds like
another name for Type I error rate.  The definition of "reasonably
reliable FDRs" 
would seem to relate to the status of the literature on this issue among
researchers in genotyping.  As more reports of FRDs in genotyping are
published, I would expect that methodology for estimation and the
standard for accuracy would similarly evolve.

	  2.  Have you tried the Bioconductor (www.bioconductor.org/)
listserve?  They might be able to say something more useful than a
general list like this.

	  spencer graves

Dupont, William wrote:

> Dear List,
> 
> We are planning a genotyping study to be analyzed using false 
> discovery rates (FDRs) (See Storey and Tibshirani PNAS 2003; 
> 100:9440-5).  I am interested in learning if there is any consensus as

> to how many features (ie. how many P values) need to be studied before

> reasonably reliable FDRs can be derived.  Does anyone know of a 
> citation where this is discussed?
> 
> Bill Dupont
> 
> William D. Dupont          phone: 615-343-4100          URL
> http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/WilliamDupont
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

--
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From william.dupont at Vanderbilt.Edu  Thu Sep 22 15:05:01 2005
From: william.dupont at Vanderbilt.Edu (Dupont, William)
Date: Thu, 22 Sep 2005 08:05:01 -0500
Subject: [R] FDR analyses: minimum number of features
Message-ID: <C6E7B149DFD87D4AAD1ADA90B5DE0279493AEE@mailbe11.mc.vanderbilt.edu>

I agree.  What is unclear to me is the optimal way of justifying sample
size and SNP selection in grant applications that use the FDR approach.

-----Original Message-----
From: Kjetil Brinchmann Halvorsen [mailto:kjetil at acelerate.com] 
Sent: Wednesday, September 21, 2005 9:45 PM
To: Spencer Graves
Cc: Dupont, William; r-help at stat.math.ethz.ch
Subject: Re: [R] FDR analyses: minimum number of features

Spencer Graves wrote:

>	  Two thoughts on this:
>
>	  1.  Your FDR (Not Franklin Delano Roosevelt) sounds like
another 
>name for Type I error rate.
>
It is certainly not the same as type I error rate. Type I error rate is
the proportion of true nulls which are rejected, while the FDR is the
proportion of rejected null hypothesis which really are true nulls!

To me FDR seems like a more promising avenue to multiple testing than
the old "familywise error rate". Who knows what is a family?

Kjetil

> The definition of "reasonably reliable FDRs" 
>would seem to relate to the status of the literature on this issue 
>among researchers in genotyping.  As more reports of FRDs in genotyping

>are published, I would expect that methodology for estimation and the 
>standard for accuracy would similarly evolve.
>
>	  2.  Have you tried the Bioconductor (www.bioconductor.org/) 
>listserve?  They might be able to say something more useful than a 
>general list like this.
>
>	  spencer graves
>
>Dupont, William wrote:
>
>  
>
>>Dear List,
>>
>>We are planning a genotyping study to be analyzed using false 
>>discovery rates (FDRs) (See Storey and Tibshirani PNAS 2003; 
>>100:9440-5).  I am interested in learning if there is any consensus as

>>to how many features (ie. how many P values) need to be studied before

>>reasonably reliable FDRs can be derived.  Does anyone know of a 
>>citation where this is discussed?
>>
>>Bill Dupont
>>
>>William D. Dupont          phone: 615-343-4100          URL
>>http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/WilliamDupont
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





--
No virus found in this outgoing message.
Checked by AVG Anti-Virus.

20/09/2005



From tpapp at Princeton.EDU  Thu Sep 22 15:32:35 2005
From: tpapp at Princeton.EDU (Tamas K Papp)
Date: Thu, 22 Sep 2005 09:32:35 -0400
Subject: [R] multidimensional (smoothing) splines
Message-ID: <20050922133234.GA14233@tpapp.student.princeton.edu>

Hi,

I am approximation a value function (in dynamic programming) V:
R^n->R.  In different versions of the problem, n is between 2 and 5.
I would like to use splines to use this, I checked the mailing list
and CRAN but couldn't find anything that would help me (Tps in fields
is too slow, cobs would be really nice but seems to be
one-dimensional).  Is there any package that would at least give me
the basis for multidimensional b-splines?  Or even one that would do
the fitting, perhaps even with constaints on derivatives at the edges
(which I can specify from theory)?

Thank you,

Tamas

-- 
Bayesian statistics is difficult in the sense that thinking is difficult.
--Donald A. Berry, American Statistician 51:242 (1997)



From reid_huntsinger at merck.com  Thu Sep 22 15:49:56 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 22 Sep 2005 09:49:56 -0400
Subject: [R] multidimensional (smoothing) splines
Message-ID: <355C35514FEAC9458F75947F5270974D076CEB@usctmx1103.merck.com>

Is the data you will use to fit the approximation located on a regular grid?
(If so you could use a 1D version repeatedly...) 

You might also have a look at package gss on CRAN. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tamas K Papp
Sent: Thursday, September 22, 2005 9:33 AM
To: R-help mailing list
Subject: [R] multidimensional (smoothing) splines


Hi,

I am approximation a value function (in dynamic programming) V:
R^n->R.  In different versions of the problem, n is between 2 and 5.
I would like to use splines to use this, I checked the mailing list
and CRAN but couldn't find anything that would help me (Tps in fields
is too slow, cobs would be really nice but seems to be
one-dimensional).  Is there any package that would at least give me
the basis for multidimensional b-splines?  Or even one that would do
the fitting, perhaps even with constaints on derivatives at the edges
(which I can specify from theory)?

Thank you,

Tamas

-- 
Bayesian statistics is difficult in the sense that thinking is difficult.
--Donald A. Berry, American Statistician 51:242 (1997)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Sep 22 16:08:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 22 Sep 2005 16:08:44 +0200
Subject: [R] R2WinBUGS: Data loading error
In-Reply-To: <1127393525.4332a8f56691c@webmail.utoronto.ca>
References: <1127393525.4332a8f56691c@webmail.utoronto.ca>
Message-ID: <4332BAEC.7000905@statistik.uni-dortmund.de>

Hadassa Brunschwig wrote:

> Hi R-Help!
> 
> I am trying to use R2WinBUGS but I get the following error message in WinBUGS
> (and there must be something wrong with my R statement as I tried it directly in
> WinBUGS and it worked):
> 
> display(log)
> check(C:/Documents and Settings/Daikon/Roche/pop_model.txt)
> model is syntactically correct
> data(C:/Documents and Settings/Daikon/Roche/data.txt)
> expected key word structure
> compile(7)
> ...(and of course nothing works after that)
> 
> and when I close WinBUGS i get:
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file 'codaIndex.txt' 
> 
> Does anyone know what this 'expected key word structure' means?
> This is my R code (and I guess my model file is ok):
> 
> modelA     <- c("C:/Documents and Settings/Daikon/Roche/pop_model.txt")
> n          <- length(unique(subsetA$subject))    #number of subjects
> nt         <- 13                                 #number of days
> Y          <- subsetA$concentr                   #concentration per day/subject
> t          <- 1:13                               #days
> dataA      <- list("n","nt","Y","t")
> parameters <- c("tau","C0","st90","C0.pop","st90.pop","tau.cpop","tau.stpop")
> inits      <-
> list(tau=c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),C0=5,st90=4,C0.pop=5,st90.pop=4,tau.cpop=0.2,tau.stpop=1)
> mcmcA      <-
> bugs(dataA,inits,parameters,modelA,debug=T,n.chains=7,bugs.directory="c:/Program
> Files/WinBUGS14",working.directory="C:/Documents and Settings/Daikon/Roche")



Which versions of R, WinBUGS and R2WinBUGS?
Your example is not reproducible for us.

I'd take a look whether dimensions are OK and whether subsetA$concentr 
is in appropriate object, but without data and model file I am unable to 
help for the data part.


For the inits part, please see ?bugs:

inits: a list with n.chains elements; each element of the list is itself 
a list of starting values for the WinBUGS model, or a function creating 
(possibly random) initial values. Alternatively, if inits = NULL, 
initial values are generated by WinBUGS

Looks like you want to have the same inits for each chain. In order not 
to repeat the inits 7 times, you might want to specify them simply as a 
function such as:

  inits <- function(){
    list(tau = rep(1, 17), C0 = 5, st90 = 4, C0.pop = 5, st90.pop = 4,
         tau.cpop = 0.2, tau.stpop = 1)
  }


Uwe Ligges


[Further correspondence on this particular topic, please respond to to 
the package maintainer (Sibylle) and me directly rather than to R-help.]


> 
> Thanks so much...
> 
> Hadassa
> 
>



From Walter.Leite at coe.ufl.edu  Thu Sep 22 16:21:06 2005
From: Walter.Leite at coe.ufl.edu (Leite,Walter)
Date: Thu, 22 Sep 2005 10:21:06 -0400
Subject: [R] Automatic creation of file names
Message-ID: <FCCB21C198A64341A78B24E06AE7C8D302F8DF5F@coe-exchange01.ad.ufl.edu>


Dear R-Help members,


I have a question about how to save to the hard drive the one thousand
datasets I generated in a simulation. The datasets are created in a
"for" loop that repeatedly creates normally distributed datasets, such
as the example below:
Library(MASS)
for (number in 1:1000) {

dataset = mvrnorm(n = 400, mu = c(0,0,0),
	Sigma = matrix(c(1,0.3,0.3,0.3,1,0.3,0.3,0.3,1),3,3))
}

I don't know how to write the datasets with different names to the hard
drive. I have solved the problem before by creating a list of names
(file1.txt, file2.txt,file3.txt, etc...) in Microsoft Excel, reading it
to R with read.table, and using write.table to write each file with the
name I created in Excel. 
I want to know if it is possible to create a list of file names
automatically within R to save the datasets created in my simulation.
Thank you for your help,

Walter L. Leite



From hchen at utmem.edu  Thu Sep 22 16:28:25 2005
From: hchen at utmem.edu (Hao Chen)
Date: Thu, 22 Sep 2005 09:28:25 -0500
Subject: [R] Automatic creation of file names
In-Reply-To: <FCCB21C198A64341A78B24E06AE7C8D302F8DF5F@coe-exchange01.ad.ufl.edu>
References: <FCCB21C198A64341A78B24E06AE7C8D302F8DF5F@coe-exchange01.ad.ufl.edu>
Message-ID: <20050922142825.GA15953@utmail.utmem.edu>


On Thu, Sep 22, 2005 at 10:21:06AM -0400, Leite,Walter wrote:
> 
> Dear R-Help members,
> 
> 
> I have a question about how to save to the hard drive the one thousand
> datasets I generated in a simulation. The datasets are created in a
> "for" loop that repeatedly creates normally distributed datasets, such
> as the example below:
> Library(MASS)
> for (number in 1:1000) {
> 
> dataset = mvrnorm(n = 400, mu = c(0,0,0),
> 	Sigma = matrix(c(1,0.3,0.3,0.3,1,0.3,0.3,0.3,1),3,3))
> }
> 
> I don't know how to write the datasets with different names to the hard
> drive. I have solved the problem before by creating a list of names
> (file1.txt, file2.txt,file3.txt, etc...) in Microsoft Excel, reading it
> to R with read.table, and using write.table to write each file with the
> name I created in Excel. 
> I want to know if it is possible to create a list of file names
> automatically within R to save the datasets created in my simulation.

paste("file", 1:1000, ".txt", sep="")

> Thank you for your help,
> 
> Walter L. Leite
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
-
: Hao Chen, Ph.D.

: Instructor 
: Department of Pharmacology
: University of Tennessee Health Science Center
: Memphis, TN 38163 USA
: Office: 901 448 3201
: Mobil:  901 826 1845 

Mining PubMed: http://www.chilibot.net
-



From vincent at 7d4.com  Thu Sep 22 16:30:10 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Thu, 22 Sep 2005 16:30:10 +0200
Subject: [R] Automatic creation of file names
In-Reply-To: <FCCB21C198A64341A78B24E06AE7C8D302F8DF5F@coe-exchange01.ad.ufl.edu>
References: <FCCB21C198A64341A78B24E06AE7C8D302F8DF5F@coe-exchange01.ad.ufl.edu>
Message-ID: <4332BFF2.5090904@7d4.com>

as a toy example :

for (i in 1:nbfiles)
  {
  fullname = paste("myfile",i, sep="");
  write.table(intable,fullname,col.names=F,row.names=F,quote=F);
  }

see
?paste
hih



From Mike.Prager at noaa.gov  Thu Sep 22 16:32:54 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Thu, 22 Sep 2005 10:32:54 -0400
Subject: [R] Automatic creation of file names
In-Reply-To: <FCCB21C198A64341A78B24E06AE7C8D302F8DF5F@coe-exchange01.ad.ufl.edu>
References: <FCCB21C198A64341A78B24E06AE7C8D302F8DF5F@coe-exchange01.ad.ufl.edu>
Message-ID: <4332C096.90301@noaa.gov>

You can use paste() with something like

 formatC(number,digits=0,wid=3,flag="0")

(where number is your loop index) to generate the filenames.


Equipment maintenance,

on 9/22/2005 10:21 AM Leite,Walter said the following:

>Dear R-Help members,
>
>
>I have a question about how to save to the hard drive the one thousand
>datasets I generated in a simulation. ://www.R-project.org/posting-guide.html
>  
>

-- 
Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/



From andy_liaw at merck.com  Thu Sep 22 16:38:07 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 22 Sep 2005 10:38:07 -0400
Subject: [R] Automatic creation of file names
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED483@usctmx1106.merck.com>

> From: Leite,Walter
> 
> Dear R-Help members,
> 
> 
> I have a question about how to save to the hard drive the one thousand
> datasets I generated in a simulation. The datasets are created in a
> "for" loop that repeatedly creates normally distributed datasets, such
> as the example below:
> Library(MASS)
> for (number in 1:1000) {
> 
> dataset = mvrnorm(n = 400, mu = c(0,0,0),
> 	Sigma = matrix(c(1,0.3,0.3,0.3,1,0.3,0.3,0.3,1),3,3))
> }
> 
> I don't know how to write the datasets with different names 
> to the hard
> drive. I have solved the problem before by creating a list of names
> (file1.txt, file2.txt,file3.txt, etc...) in Microsoft Excel, 
> reading it
> to R with read.table, and using write.table to write each 
> file with the
> name I created in Excel. 
> I want to know if it is possible to create a list of file names
> automatically within R to save the datasets created in my simulation.
> Thank you for your help,
> 
> Walter L. Leite

If the total size of all the data isn't too large, that you can load all of
them into memory without problem, I'd suggest that you put them into a list
and just save the list.

If you really only want to use one of the datasets at a time, just insert
something like the following into the loop:

save(dataset, file=paste("data", number, ".rda", sep=""), compress=TRUE)

HTH,
Andy

 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From maximilianotto at gmx.at  Thu Sep 22 16:42:06 2005
From: maximilianotto at gmx.at (MK)
Date: Thu, 22 Sep 2005 10:42:06 -0400
Subject: [R] mark values as NA in matrix
Message-ID: <6.2.1.2.0.20050922103737.021444c0@email.med.yale.edu>



No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From manduchi at pcbi.upenn.edu  Thu Sep 22 16:44:59 2005
From: manduchi at pcbi.upenn.edu (Elisabetta Manduchi)
Date: Thu, 22 Sep 2005 10:44:59 -0400 (EDT)
Subject: [R] Fligner-Policello robust rank test
Message-ID: <Pine.LNX.4.61.0509221043490.8268@hera.pcbi.upenn.edu>


Can anybody tell me if there is an R implementation of the 
Fligner-Policello robust rank test?
Thanks,
Elisabetta



From Mike.Prager at noaa.gov  Thu Sep 22 16:51:54 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Thu, 22 Sep 2005 10:51:54 -0400
Subject: [R] Automatic creation of file names
In-Reply-To: <FCCB21C198A64341A78B24E06AE7C8D302F8DF5F@coe-exchange01.ad.ufl.edu>
References: <FCCB21C198A64341A78B24E06AE7C8D302F8DF5F@coe-exchange01.ad.ufl.edu>
Message-ID: <4332C50A.1010006@noaa.gov>

Walter --

P.S.  The advantage of using formatC over pasting the digits (1:1000) 
directly is that when one uses leading zeroes, as in the formatC example 
shown, the resulting filenames will sort into proper order.

...MHP


You can use paste() with something like

  formatC(number,digits=0,wid=3,flag="0")

(where number is your loop index) to generate the filenames.


on 9/22/2005 10:21 AM Leite,Walter said the following:

>I have a question about how to save to the hard drive the one thousand
>datasets I generated in a simulation. ://www.R-project.org/posting-guide.html



From h.brunschwig at utoronto.ca  Thu Sep 22 16:57:26 2005
From: h.brunschwig at utoronto.ca (Hadassa Brunschwig)
Date: Thu, 22 Sep 2005 10:57:26 -0400
Subject: [R] R2WinBUGS: Data loading error
In-Reply-To: <4332BAEC.7000905@statistik.uni-dortmund.de>
References: <1127393525.4332a8f56691c@webmail.utoronto.ca>
	<4332BAEC.7000905@statistik.uni-dortmund.de>
Message-ID: <1127401046.4332c6563c15c@webmail.utoronto.ca>

Thank you for your answer. I found out the mistake and it finally works!!

Hadassa
-- 

Hadassa Brunschwig
Birmannsgasse 10A
CH-4055 Basel
Switzerland
Phone: +41 78 797 6065
Email: h.brunschwig at utoronto.ca



Quoting Uwe Ligges <ligges at statistik.uni-dortmund.de>:

> Hadassa Brunschwig wrote:
> 
> > Hi R-Help!
> > 
> > I am trying to use R2WinBUGS but I get the following error message in
> WinBUGS
> > (and there must be something wrong with my R statement as I tried it
> directly in
> > WinBUGS and it worked):
> > 
> > display(log)
> > check(C:/Documents and Settings/Daikon/Roche/pop_model.txt)
> > model is syntactically correct
> > data(C:/Documents and Settings/Daikon/Roche/data.txt)
> > expected key word structure
> > compile(7)
> > ...(and of course nothing works after that)
> > 
> > and when I close WinBUGS i get:
> > Error in file(file, "r") : unable to open connection
> > In addition: Warning message:
> > cannot open file 'codaIndex.txt' 
> > 
> > Does anyone know what this 'expected key word structure' means?
> > This is my R code (and I guess my model file is ok):
> > 
> > modelA     <- c("C:/Documents and Settings/Daikon/Roche/pop_model.txt")
> > n          <- length(unique(subsetA$subject))    #number of subjects
> > nt         <- 13                                 #number of days
> > Y          <- subsetA$concentr                   #concentration per
> day/subject
> > t          <- 1:13                               #days
> > dataA      <- list("n","nt","Y","t")
> > parameters <-
> c("tau","C0","st90","C0.pop","st90.pop","tau.cpop","tau.stpop")
> > inits      <-
> >
>
list(tau=c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),C0=5,st90=4,C0.pop=5,st90.pop=4,tau.cpop=0.2,tau.stpop=1)
> > mcmcA      <-
> >
> bugs(dataA,inits,parameters,modelA,debug=T,n.chains=7,bugs.directory="c:/Program
> > Files/WinBUGS14",working.directory="C:/Documents and
> Settings/Daikon/Roche")
> 
> 
> 
> Which versions of R, WinBUGS and R2WinBUGS?
> Your example is not reproducible for us.
> 
> I'd take a look whether dimensions are OK and whether subsetA$concentr 
> is in appropriate object, but without data and model file I am unable to 
> help for the data part.
> 
> 
> For the inits part, please see ?bugs:
> 
> inits: a list with n.chains elements; each element of the list is itself 
> a list of starting values for the WinBUGS model, or a function creating 
> (possibly random) initial values. Alternatively, if inits = NULL, 
> initial values are generated by WinBUGS
> 
> Looks like you want to have the same inits for each chain. In order not 
> to repeat the inits 7 times, you might want to specify them simply as a 
> function such as:
> 
>   inits <- function(){
>     list(tau = rep(1, 17), C0 = 5, st90 = 4, C0.pop = 5, st90.pop = 4,
>          tau.cpop = 0.2, tau.stpop = 1)
>   }
> 
> 
> Uwe Ligges
> 
> 
> [Further correspondence on this particular topic, please respond to to 
> the package maintainer (Sibylle) and me directly rather than to R-help.]
> 
> 
> > 
> > Thanks so much...
> > 
> > Hadassa
> > 
> > 
> 
>



From greg.snow at ihc.com  Thu Sep 22 17:17:11 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Thu, 22 Sep 2005 09:17:11 -0600
Subject: [R] Automatic creation of file names
Message-ID: <s33276a4.049@lp-msg1.co.ihc.com>

Others have shown how to create a vector of names, but here are a
couple of other things you might want to look at.

If you are only using these datasets within R, then you might consider
not saving the data, just the random seeds, then when you need
the data again you can just generate it.

Also look at the g.data package, it provides methods for automatically
saving and loading datasets.

Another option for creating filenames is the tempfile command.

hope this helps,

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> "Leite,Walter" <Walter.Leite at coe.ufl.edu> 09/22/05 08:21AM >>>

Dear R-Help members,


I have a question about how to save to the hard drive the one thousand
datasets I generated in a simulation. The datasets are created in a
"for" loop that repeatedly creates normally distributed datasets, such
as the example below:
Library(MASS)
for (number in 1:1000) {

dataset = mvrnorm(n = 400, mu = c(0,0,0),
	Sigma = matrix(c(1,0.3,0.3,0.3,1,0.3,0.3,0.3,1),3,3))
}

I don't know how to write the datasets with different names to the
hard
drive. I have solved the problem before by creating a list of names
(file1.txt, file2.txt,file3.txt, etc...) in Microsoft Excel, reading
it
to R with read.table, and using write.table to write each file with
the
name I created in Excel. 
I want to know if it is possible to create a list of file names
automatically within R to save the datasets created in my simulation.
Thank you for your help,

Walter L. Leite

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mschwartz at mn.rr.com  Thu Sep 22 17:53:50 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 22 Sep 2005 10:53:50 -0500
Subject: [R] Fligner-Policello robust rank test
In-Reply-To: <Pine.LNX.4.61.0509221043490.8268@hera.pcbi.upenn.edu>
References: <Pine.LNX.4.61.0509221043490.8268@hera.pcbi.upenn.edu>
Message-ID: <1127404430.3487.6.camel@localhost.localdomain>

On Thu, 2005-09-22 at 10:44 -0400, Elisabetta Manduchi wrote:
> Can anybody tell me if there is an R implementation of the 
> Fligner-Policello robust rank test?
> Thanks,
> Elisabetta


I don't know about the Fligner-Policello test, but the Fligner-Killeen
test has been implemented in fligner.test(). From ?fligner.test:


The Fligner-Killeen (median) test has been determined in a simulation
study as one of the many tests for homogeneity of variances which is
most robust against departures from normality, see Conover, Johnson &
Johnson (1981). It is a k-sample simple linear rank which uses the ranks
of the absolute values of the centered samples and weights a(i) =
qnorm((1 + i/(n+1))/2). The version implemented here uses median
centering in each of the samples (F-K:med X^2 in the reference).


FWIW, in a Google search, I located the following SAS macro by Paul von
Hippel, which includes a comment that does not lead to optimism
regarding software implementations:

http://www.sociology.ohio-state.edu/people/ptv/macros/fligner_policello.htm

The above should not be overly difficult to convert to R, presuming you
know SAS macros...


HTH,

Marc Schwartz



From opa04 at yahoo.com  Thu Sep 22 18:10:04 2005
From: opa04 at yahoo.com (DAVID CAMACHO)
Date: Thu, 22 Sep 2005 09:10:04 -0700 (PDT)
Subject: [R] Descriptive statistics for tables
Message-ID: <20050922161004.75124.qmail@web30512.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050922/2a04c19a/attachment.pl

From thomas.friedrichsmeier at ruhr-uni-bochum.de  Thu Sep 22 18:14:12 2005
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: Thu, 22 Sep 2005 18:14:12 +0200
Subject: [R] warning.expression?
Message-ID: <200509221814.12666.thomas.friedrichsmeier@ruhr-uni-bochum.de>

Hi!

I'm trying to catch all warning-messages for special handling. It seems 
options (warning.expression=myfunc ()) can be used for that. However, the 
question is: How can I get at the actual warning in myfunc ()? Apparently in 
S, you can use .C("get_last_message") for that. Is there a similar mechanism 
in R?

Thanks for your help!
Thomas



From gunter.berton at gene.com  Thu Sep 22 18:27:44 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 22 Sep 2005 09:27:44 -0700
Subject: [R] Descriptive statistics for tables
In-Reply-To: <20050922161004.75124.qmail@web30512.mail.mud.yahoo.com>
Message-ID: <200509221627.j8MGRiWk022864@meitner.gene.com>

I don't know what a "quadratic, same size" table is or what you mean. If you
do not get a satisfactory reply I suggest:

1. Read and follow the posting guide at the end of this message.

2. In particular, provide a simple, reproducible example to show what you
want to do and perhaps any error messages that you may have received.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of DAVID CAMACHO
> Sent: Thursday, September 22, 2005 9:10 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Descriptive statistics for tables
> 
> 
> I have a lot (more than one hundred) of files with tables of 
> the same kind (quadratic, same size) and I want to obtain 
> some statistics for every position on them. Therefore, as a 
> result I want another table. I import every table, and create 
> an object read.table for it, then I have try to create a 
> list or a data frame and directly utilize some functions 
> like sd( ) without success, because it calculate sd for the 
> columns . Which kind of object should I create in order to 
> utilize directly this kind of functions?  Should I program it 
> throw for loops?
> 
> David
> 
> 
> 
> 		
> ---------------------------------
> 
>  Click here to donate to the Hurricane Katrina relief effort. 
> 	[[alternative HTML version deleted]]
> 
>



From gunter.berton at gene.com  Thu Sep 22 18:35:29 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 22 Sep 2005 09:35:29 -0700
Subject: [R] warning.expression?
In-Reply-To: <200509221814.12666.thomas.friedrichsmeier@ruhr-uni-bochum.de>
Message-ID: <200509221635.j8MGZTK2026080@meitner.gene.com>

?warnings

Also see ?conditions

Note that both were suggested by help.search('warning') -- Please do make
full use of R's internal documentation before posting.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas 
> Friedrichsmeier
> Sent: Thursday, September 22, 2005 9:14 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] warning.expression?
> 
> Hi!
> 
> I'm trying to catch all warning-messages for special 
> handling. It seems 
> options (warning.expression=myfunc ()) can be used for that. 
> However, the 
> question is: How can I get at the actual warning in myfunc 
> ()? Apparently in 
> S, you can use .C("get_last_message") for that. Is there a 
> similar mechanism 
> in R?
> 
> Thanks for your help!
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From elvis at xlsolutions-corp.com  Thu Sep 22 18:44:14 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Thu, 22 Sep 2005 09:44:14 -0700
Subject: [R] Course*** S-PLUS / R: Complementing and Extending Statistical
	Computing for SAS Users*** October 2005
Message-ID: <20050922094414.a108dc04937c07ba67766dad37185406.44805c0a97.wbe@email.email.secureserver.net>

XSolutions Corp (www.xlsolutions-corp.com) is proud to announce
our   October 2005 "S-PLUS / R: Complementing and Extending Statistical 
Computing for SAS Users" in Raleigh and Washington, DC
http://xlsolutions-corp.com/Rsas.htm
 
********* Raleigh  ------------------------ October 24th-25th, 2005
********* Washington, DC ------------------ October 27th-28th, 2005
 
 
Ask for group discount and reserve your seat Now  (payment due after
the class)
Email Sue Turner:  sue at xlsolutions-corp.com
 
 
This course is designed for users who want to learn how to complement
and 
extend statistical computing of SAS with the S or R system. The course
will 
give SAS users a strong foundation for becoming a versatile R/Splus
programmer. 
 
Course Topics 


Day 1 
-An Overview of resources: installation and demonstration, the R
project/core; 
CRAN; the distribution for UNIX and windows systems; the package
concept; 
literature: Venables and Ripley, Chambers and Hastie, other
publications. 
-A Comparison of R and S-PLUS 
-Quick review of the SAS environment: the OS interface, the data step,
macros,
 procs, reports. Issues of data archiving with SSD format; interfaces to
DBMS 
-Data manipulations in S and R (data frame and matrix operations) 
and SAS (the data step) -- issues of importing, formatting,
transformation, 
cataloging, exporting 
-Functions vs macros in SAS for programming repetitive processes. 
-The iteration models of SAS vs whole-object modeling 
Day 2 
-Statistical modeling support in R/S vs SAS PROCS. 
-Integrated documentation and example processing in R/S. 
-Post-processing of function output in R/S vs OUTPUT datasets in SAS. 
-Specific comparisons: linear modeling, glms, gees, lmes 
-Report Writing 
-Extending the systems for new data structures and new algorithms 
 
Please let us know if you and your colleagues are interested in this
class  to take advantage of group discount. Register now to secure your 
seat in this course!
 
 
Cheers,

Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From elvis at xlsolutions-corp.com  Thu Sep 22 18:44:19 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Thu, 22 Sep 2005 09:44:19 -0700
Subject: [R] Course  *** R/Splus Advanced Programming ***  Seattle,
	November 10th-11th, 2005
Message-ID: <20050922094419.a108dc04937c07ba67766dad37185406.ee54191ead.wbe@email.email.secureserver.net>

XSolutions Corp (www.xlsolutions-corp.com) is proud to announce
our  November, 2005 "Advanced R/Splus programming" in Seattle

www.xlsolutions-corp.com/Radv.htm

********* Seattle ------------------------ November 10th-11th, 2005

Ask for group discount and reserve your seat Now  (payment due after
the class)

Email Sue Turner:  sue at xlsolutions-corp.com

Course Outline:

- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 (S-Plus 6)
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function

It'll also deal with lots of S-Plus efficiency issues and any special
topics
from participants is welcome.

Please let us know if you and your colleagues are interested in this
class
to take advantage of group discount.  Register now to secure your seat
in this
course!

Cheers,

Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From rpeng at jhsph.edu  Thu Sep 22 18:45:05 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 22 Sep 2005 12:45:05 -0400
Subject: [R] warning.expression?
In-Reply-To: <200509221814.12666.thomas.friedrichsmeier@ruhr-uni-bochum.de>
References: <200509221814.12666.thomas.friedrichsmeier@ruhr-uni-bochum.de>
Message-ID: <4332DF91.8040607@jhsph.edu>

You might be interested in 'tryCatch' to catch warnings.

-roger

Thomas Friedrichsmeier wrote:
> Hi!
> 
> I'm trying to catch all warning-messages for special handling. It seems 
> options (warning.expression=myfunc ()) can be used for that. However, the 
> question is: How can I get at the actual warning in myfunc ()? Apparently in 
> S, you can use .C("get_last_message") for that. Is there a similar mechanism 
> in R?
> 
> Thanks for your help!
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From thomas.friedrichsmeier at ruhr-uni-bochum.de  Thu Sep 22 18:45:51 2005
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: Thu, 22 Sep 2005 18:45:51 +0200
Subject: [R]  rkward
Message-ID: <200509221845.51571.thomas.friedrichsmeier@ruhr-uni-bochum.de>

> I'm testing rkward to use in my class. Now I'm using XEmacs, but for many 
> people XEmacs is more complicated than R.

> Anybody here use this program?

> It use plugins, what are these plugins?

Hi!

You're lucky, I'm not subscribed to this list, but just came across your 
message. I'm the lead developer of rkward, and so, yes, I also use it.

I don't know, what exactly you're planning to do with rkward. But of you 
should note, that many parts of rkward are still in early development. I 
suggest you ask about specific questions on rkward-devel 
(http://lists.sourceforge.net/lists/listinfo/rkward-devel).

About plugins: Only few plugins exist so far, and all those are included in 
the standard distribution of rkward. There is documentation available on how 
to write plugins, here:

http://rkward.sourceforge.net/devel/plugins.php

That documentation refers to the CVS-version of rkward, and will not work as 
described for rkward 0.3.2.

Feel free to ask all sorts of questions on rkward-devel (there is no dedicated 
users' list so far).

Thomas



From kerryrekky at yahoo.com  Thu Sep 22 18:52:52 2005
From: kerryrekky at yahoo.com (Cunningham Kerry)
Date: Thu, 22 Sep 2005 09:52:52 -0700 (PDT)
Subject: [R] how to keep very small or large number?
Message-ID: <20050922165252.82817.qmail@web51807.mail.yahoo.com>

When I was computing some joint probabilities, I found
that R reported most of the results to to -Inf and
thus didn't record the value. I guess it is b/c the
joint log(probability) can be extremely small. Is
there a way in R to keep the values even if they are
small?



From Guangchun.Song at STJUDE.ORG  Thu Sep 22 18:57:52 2005
From: Guangchun.Song at STJUDE.ORG (Song, Guangchun)
Date: Thu, 22 Sep 2005 11:57:52 -0500
Subject: [R] Run R script on Linux backend
Message-ID: <5CB39BCA5724F349BCB748675C6CA1A2030C7D63@SJMEMXMB02.stjude.sjcrh.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050922/0334a945/attachment.pl

From thomas.friedrichsmeier at ruhr-uni-bochum.de  Thu Sep 22 18:57:48 2005
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: Thu, 22 Sep 2005 18:57:48 +0200
Subject: [R] warning.expression?
In-Reply-To: <200509221635.j8MGZTK2026080@meitner.gene.com>
References: <200509221635.j8MGZTK2026080@meitner.gene.com>
Message-ID: <200509221857.48332.thomas.friedrichsmeier@ruhr-uni-bochum.de>

> ?warnings

No, sorry, this does not work. Try this, if you like:

myfunc <- function () {
	warnings ()
}
options (warning.expression=quote (myfunc ()))
warning ("test")

> Also see ?conditions

tryCatch ()
and
withCallingHandlers ()

are a whole different story, as far as I understand them. I actually want 
_all_ warnings in the whole session to go through my handler (actually I'm 
writing a GUI front-end, and would like to have a way to easily identify all 
warnings).

That's what I thought warning.expression was designed for. So am I wrong in 
thinking so? If not, how to use it correctly?

> Note that both were suggested by help.search('warning') -- Please do make
> full use of R's internal documentation before posting.

It's quite possible, I'm overlooking something rather obvious. Not that 
obvious, however.

Regards
Thomas



From drodriguezca at unal.edu.co  Thu Sep 22 18:58:28 2005
From: drodriguezca at unal.edu.co (Daniel Rodriguez Caicedo)
Date: Thu, 22 Sep 2005 18:58:28 +0200
Subject: [R] I have a problem with npmc
Message-ID: <d331d0ca292d.4332fed4@unal.edu.co>

Hi to all:

The reason for my mail is this:

I am tryng to make an analysis with non-parametric multiple comparisons with a data set from Pinciples and Procedures of Satistics (Steel and Torrie,1980). The data are of a one-way layouth:

 > DOKNOPAR
   class  var
1   DOK1 19.4
2   DOK1 32.6
3   DOK1 27.0
4   DOK1 32.1
5   DOK1 33.0
6   DOK5 17.7
7   DOK5 24.8
8   DOK5 27.9
9   DOK5 25.2
10  DOK5 24.3
11  DOK4 17.0
12  DOK4 19.4
13  DOK4  9.1
14  DOK4 11.9
15  DOK4 15.8
16  DOK7 20.7
17  DOK7 21.0
18  DOK7 20.5
19  DOK7 18.8
20  DOK7 18.6
21 DOK13 14.3
22 DOK13 14.4
23 DOK13 11.8
24 DOK13 11.6
25 DOK13 14.2
26   COM 17.3
27   COM 19.4
28   COM 19.1
29   COM 16.9
30   COM 20.8

But when i try making de npmc, 

> npmc(DOKNOPAR)

R displays this message:



Error in uniroot(f = function(arg) p - z.dist(arg, corr = corr, df = df,  : 
        f() values at end points not of opposite sign

What is the reason for this? How can i make this npmc analysis correctly?

Daniel Rodriguez

Universidad Nacional de Colombia



From Ted.Harding at nessie.mcc.ac.uk  Thu Sep 22 18:51:52 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 22 Sep 2005 17:51:52 +0100 (BST)
Subject: [R] Automatic creation of file names
In-Reply-To: <4332C50A.1010006@noaa.gov>
Message-ID: <XFMail.050922175152.Ted.Harding@nessie.mcc.ac.uk>

On 22-Sep-05 Mike Prager wrote:
> Walter --
> 
> P.S.  The advantage of using formatC over pasting the digits (1:1000) 
> directly is that when one uses leading zeroes, as in the formatC
> example 
> shown, the resulting filenames will sort into proper order.
> 
> ...MHP
> 
> 
> You can use paste() with something like
> 
>   formatC(number,digits=0,wid=3,flag="0")
> 
> (where number is your loop index) to generate the filenames.
> 
> 
> on 9/22/2005 10:21 AM Leite,Walter said the following:
> 
>>I have a question about how to save to the hard drive the one thousand
>>datasets I generated in a simulation.
>>://www.R-project.org/posting-guide.html

For this precise question, the replies for filename creation, though
useful, have been slightly off-target. Walter may presumably want the
ilenames to be in sortable order corresponding to the numerical order
of creation, i.e. like

  file0001 file0002 ... file1000

The precise formatC specification required for this would be

  formatC(n,digits=0,wid=4,format="d",flag="0")

so that

  formatC(1,digits=0,wid=4,format="d",flag="0")
# [1] "0001" -> "file0001"

  formatC(999,digits=0,wid=4,format="d",flag="0")
# [1] "0999" -> "file0999"

  formatC(1000,digits=0,wid=4,format="d",flag="0")
# [1] "1000" -> "file1000"

The suggestions with "wid=3" would give

  formatC(999,digits=0,wid=3,format="d",flag="0")
# [1] "999" -> "file999"
  formatC(1000,digits=0,wid=3,format="d",flag="0")
# [1] "1000" -> "file1000"

which are now in the wrong order (since "file1000" sorts
alphabetically prior to "file999".

Also, if "format=d" is not specified we get things like

  formatC(100,digits=0,wid=3,flag="0")
# [1] "1e+02" -> "file1e+02"

which, while a valid filename, is on its head for sorting
(since now the exponent sorts fastest!).

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Sep-05                                       Time: 17:51:36
------------------------------ XFMail ------------------------------



From thomas.friedrichsmeier at ruhr-uni-bochum.de  Thu Sep 22 19:00:56 2005
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: Thu, 22 Sep 2005 19:00:56 +0200
Subject: [R] warning.expression?
In-Reply-To: <4332DF91.8040607@jhsph.edu>
References: <200509221814.12666.thomas.friedrichsmeier@ruhr-uni-bochum.de>
	<4332DF91.8040607@jhsph.edu>
Message-ID: <200509221900.56739.thomas.friedrichsmeier@ruhr-uni-bochum.de>

> You might be interested in 'tryCatch' to catch warnings.

Yes, thanks for pointing it out. However, I'm actually looking for a way to 
catch all warnings in a whole (interactive) session. Can warning.expression 
be used for that?

Thomas



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Sep 22 19:41:18 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 22 Sep 2005 13:41:18 -0400
Subject: [R] how to keep very small or large number?
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD503B9313D@us-arlington-0668.mail.saic.com>

 R can hold very small numbers. It is the round-off error during addition
(and other basic operations) that causes the problem. For example:
> x=1e-300
> x
[1] 1e-300
> (x+1)-1
[1] 0

If you need a code that can be immune to round-off problems - you need to
write it yourself (please correct me if I am wrong). At some point I needed
'sum' and 'cumsum' that would not have that problem and I end up writing my
own versions of those functions. See examples of 'sum.exact' and
'cumsum.exact' in 'caTools' package. 

Jarek Tuszynski

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] 
Sent: Thursday, September 22, 2005 12:53 PM
To: r-help at stat.math.ethz.ch
Subject: [R] how to keep very small or large number?

When I was computing some joint probabilities, I found that R reported most
of the results to to -Inf and thus didn't record the value. I guess it is
b/c the joint log(probability) can be extremely small. Is there a way in R
to keep the values even if they are small?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jholtman at gmail.com  Thu Sep 22 19:48:31 2005
From: jholtman at gmail.com (jim holtman)
Date: Thu, 22 Sep 2005 13:48:31 -0400
Subject: [R] Automatic creation of file names
In-Reply-To: <XFMail.050922175152.Ted.Harding@nessie.mcc.ac.uk>
References: <4332C50A.1010006@noaa.gov>
	<XFMail.050922175152.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <644e1f32050922104850f962bc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050922/618af90c/attachment.pl

From murdoch at stats.uwo.ca  Thu Sep 22 19:53:26 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 22 Sep 2005 13:53:26 -0400
Subject: [R] how to keep very small or large number?
In-Reply-To: <20050922165252.82817.qmail@web51807.mail.yahoo.com>
References: <20050922165252.82817.qmail@web51807.mail.yahoo.com>
Message-ID: <4332EF96.9000102@stats.uwo.ca>

On 9/22/2005 12:52 PM, Cunningham Kerry wrote:
> When I was computing some joint probabilities, I found
> that R reported most of the results to to -Inf and
> thus didn't record the value. I guess it is b/c the
> joint log(probability) can be extremely small. Is
> there a way in R to keep the values even if they are
> small?

Leave them in log form, take exponentials only when you need to report them.

Duncan Murdoch



From spencer.graves at pdf.com  Thu Sep 22 19:56:26 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Sep 2005 12:56:26 -0500
Subject: [R] how to keep very small or large number?
In-Reply-To: <20050922165252.82817.qmail@web51807.mail.yahoo.com>
References: <20050922165252.82817.qmail@web51807.mail.yahoo.com>
Message-ID: <4332F04A.50600@pdf.com>

	  Your question is too vague for me.  Can you please provide a very 
simple example in a few lines of code?

	  For example, do you first compute probabilities and then take 
logarithms, or do you try to compute logarithms directly?  Where exactly 
do you see the problem?  (The posting guide 
www.R-project.org/posting-guide.html might help.)

	  spencer graves

Cunningham Kerry wrote:

> When I was computing some joint probabilities, I found
> that R reported most of the results to to -Inf and
> thus didn't record the value. I guess it is b/c the
> joint log(probability) can be extremely small. Is
> there a way in R to keep the values even if they are
> small?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From rschulz at sonic.net  Thu Sep 22 20:28:43 2005
From: rschulz at sonic.net (Randall R Schulz)
Date: Thu, 22 Sep 2005 11:28:43 -0700
Subject: [R] warning.expression?
In-Reply-To: <200509221900.56739.thomas.friedrichsmeier@ruhr-uni-bochum.de>
References: <200509221814.12666.thomas.friedrichsmeier@ruhr-uni-bochum.de>
	<4332DF91.8040607@jhsph.edu>
	<200509221900.56739.thomas.friedrichsmeier@ruhr-uni-bochum.de>
Message-ID: <200509221128.43872.rschulz@sonic.net>

Thomas,

On Thursday 22 September 2005 10:00, Thomas Friedrichsmeier wrote:
> > You might be interested in 'tryCatch' to catch warnings.
>
> Yes, thanks for pointing it out. However, I'm actually looking for a
> way to catch all warnings in a whole (interactive) session. Can
> warning.expression be used for that?

Please permit, if you will, some feedback from someone who is quite 
uneducated in R, at least so far, but who has a lot of general 
programming background.

I'm afraid this does not make sense to me. An "interactive session" is 
not some monolithic blob. It is a sequence of one or more (often very 
many) discreet interactions. Any one of these may produce an 
exceptional condition, and it seems to me that each individual 
interaction that elicits an exception should be handled independently 
and if it does produce an exceptional condition, that should be 
reported as soon as it occurs. Things should then be cleaned up, as 
necessary and feasible, and the interactive session should be allowed 
to continue.

Does this make sense? Or am I misunderstanding your use case?


> Thomas


Randall Schulz



From thomas.friedrichsmeier at ruhr-uni-bochum.de  Thu Sep 22 20:56:54 2005
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: Thu, 22 Sep 2005 20:56:54 +0200
Subject: [R]  warning.expression?
Message-ID: <200509222056.54342.thomas.friedrichsmeier@ruhr-uni-bochum.de>

> I'm afraid this does not make sense to me. An "interactive session" is 
> not some monolithic blob. It is a sequence of one or more (often very 
> many) discreet interactions. Any one of these may produce an 
> exceptional condition, and it seems to me that each individual 
> interaction that elicits an exception should be handled independently 
> and if it does produce an exceptional condition, that should be 
> reported as soon as it occurs. Things should then be cleaned up, as 
> necessary and feasible, and the interactive session should be allowed 
> to continue.

Sorry, indeed I was simplifying beyond recognition. Actually, I'm writing a 
GUI front-end to R, and what I called an "interactive session" is technically 
not an interactive session in R at all, but would only seem like one to the 
user. In the GUI, there are R expressions being run from some application 
code, but also there is a sort of console, where the user can interact with R 
more or less directly, like they would in a regular interactive session.
What I mean by "special handling", then, is not so much reacting to the 
warnings/errors. Rather, the thing I'm really trying to do, is just to figure 
out, which output is "regular" output, which is a warning, and which is an 
error. All output gets displayed right in time, for the user to react to. 
However, one use case would be to mark up all warnings in a certain color and 
all errors in another. Also, for commands running for GUI-actions, it would 
be nice to inform the user - if applicable - "there have been the following 
warnings" or "an error occurred, see below".
So basically, my programmer's dream would be to have three simple callbacks: 
one each for "regular output", warnings and errors. While it's not quite as 
simple, I have most things in place: I'm catching all that would be going to 
stdout via ptr_R_WriteConsole. I'm catching the errors using my own callback 
in options (error=...). Now all I need is a way to find out about warnings. 
It seemed to me, options (warning.expression=...) would be a neat way to do 
that, but I could not find out, how to use it.
Hope that clarifies what I need this for. BTW, the project in question is 
this: http://rkward.sf.net

Regards
Thomas



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Sep 22 20:59:52 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 22 Sep 2005 14:59:52 -0400
Subject: [R] Survey of ROC AUC / wilcoxon test functions
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD503B9325A@us-arlington-0668.mail.saic.com>

Hi,

I was lately debugging parts of my 'colAUC' function in caTools package, and
in a process looked into other packages for calculating Areas Under ROC
Curves (AUC).  To my surprise I found at least 6 other functions: 
*	  wilcox.test
*	  AUC from ROC package, 
*	  performance from ROCR package, 
*	  auROC from limma package, 
*	  ROC from Epi package, 
*	  roc.area from verification package
And I belive I found problems with 3 of them: 'ROC', 'auROC' & 'roc.area' 

I wrote short code to compare them all:

	# Compare calAUC with other functions designed for similar purpose
	library(caTools)
	library(verification)
	library(ROC)
	library(ROCR)
	library(Epi)
	library(limma)
	library(MASS) 
	data(cats)
	colAUC(cats[,2:3], cats[,1], plotROC=TRUE) 
	auc  = matrix(NA,9,3)
      pval = matrix(NA,4,3)
	rownames(auc)  = c("colAUC(alg='ROC')", "colAUC(alg='Wilcox')",
"wilcox.test",
	                   "rank", "roc.area", "AUC", "performance", "ROC",
"auROC")
	colnames(auc ) = c("AUC(x)", "AUC(-x)", "AUC(x+noise)")
	rownames(pval) = c("wilcox.test(exact=1)","wilcox.test(exact=0)", 
                         "roc.area()$p.adj","roc.area()$p")
      colnames(pval) = c("p(x)", "p(-x)", "p(x+noise)")
	X = cbind(cats[,2], -cats[,2], cats[,2]+rnorm(nrow(cats))/10 )
	y = ifelse(cats[,1]=='F',0,1)
	for (i in 1:3) {
	  x = X[,i]
	  x1 = x[y==1]; n1 = length(x1);            # prepare input data ...
	  x2 = x[y==0]; n2 = length(x2);            # ... into required
format
	  r = rank(c(x1,x2))  
	  auc[1,i] = colAUC(x, y, alg="ROC") 
	  auc[2,i] = colAUC(x, y, alg="Wilcox")       
	  auc[3,i] = wilcox.test(x1, x2, exact=0)$statistic / (n1*n2)
        auc[4,i] = (sum(r[1:n1]) - n1*(n1+1)/2) / (n1*n2) 
	  auc[5,i] = roc.area(y, x)$A.tilda           
	  auc[6,i] = AUC(rocdemo.sca(y, x, dxrule.sca))    
	  auc[7,i] = performance(prediction( x, y),"auc")@y.values[[1]]
	  auc[8,i] = ROC(x,y,grid=0)$AUC # get AUC by 'ROC'
	  auc[9,i] = auROC(y, x)   # get AUC by 'auROC'
	  pval[1,i] = wilcox.test(x1, x2, exact=0)$p.value
	  pval[2,i] = wilcox.test(x1, x2, exact=1)$p.value

        pval[3,i] = roc.area(y, x)$p.adj
        pval[4,i] = roc.area(y, x)$p	  	      	  	      
      }
	print(auc)
      print(pval)

    

Which gave the following results:

	> print(auc)
   	                     AUC(x)     AUC(-x)    AUC(x+noise)
	colAUC(alg='ROC')    0.8338451  0.8338451  0.8225488
	colAUC(alg='Wilcox') 0.8338451  0.8338451  0.8225488
	wilcox.test          0.8338451  0.1661549  0.8225488
	rank                 0.8338451  0.1661549  0.8225488
	roc.area             0.8338451  0.1661549  0.8225488
	AUC                  0.8338451  0.1661549  0.8225488
	performance          0.8338451  0.1661549  0.8225488
	ROC                  0.8338451  0.1654968  0.8225488
	auROC                0.8131169  0.1454266  0.8225488
	>       print(pval)
	                     p(x)       p(-x)      p(x+noise)
	wilcox.test(exact=1) 8.200e-11  8.200e-11  3.774e-10
	wilcox.test(exact=0) 8.200e-11  8.200e-11  3.817e-11
	roc.area()$p.adj     4.042e-11  1.000e+00  1.861e-10
	roc.area()$p         4.446e-11  1.000e+00  1.861e-10


Some thoughts about those results:
- Data used for the testing: 
    column 1 (x) - has ties, 
    column 2 (-x) - negative of the same data
    column 3 (x+noise) - similar data but with no ties
- All AUC functions gave the same results for data with no ties
- 'ROC' and 'auROC' gave different results for data with ties
- 'colAUC' (my function) returns 'max(auc, 1-auc)' that's why AUC(x) and
AUC(-x) are the same
- I assume that "roc.area()$p.adj" and "wilcox.test" p.values suppose to
return the same results, but they are different.
- At least 'roc.area(x)$p.adj' and 'roc.area(-x)$p.adj' should be the same.


Comments? Corrections?

Jarek 
====================================================\====                 
 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \ 
 Jaroslaw.W.Tuszynski at saic.com                    `     \



From sledepi at operamail.com  Thu Sep 22 21:05:55 2005
From: sledepi at operamail.com (sloan jones)
Date: Thu, 22 Sep 2005 20:05:55 +0100
Subject: [R] problem with dates
Message-ID: <20050922190555.7EFC5CA0A3@ws5-11.us4.outblaze.com>

I Have been trying to convert a vector of dates into julian dates using the following commands: as.date & as.numeric.  I can convert a date no problem by doing the following:

as.numeric (as.date ("9/21/2004"))

but as soon as I try to do an entire vector I am given the following:

 as.numeric (as.date(date1))
Error in as.date(date1) : Cannot coerce to date format

I have tried starting with a number of different date formats including dd/mm/yy, dd/mm/yyyy, month, day, year..

Following is a sample of the vector of dates that I am using:

> date1
  [1] 7/1/2005   4/27/2005  7/15/2004  11/30/2004 1/8/2005   2/7/2005  
  [7] 7/17/2004  4/1/2004   1/4/2005   12/27/2004 1/5/2005   5/17/2005 .... 

it is a factor and its mode is numeric.

What do I need to do?
 
Sloan

-- 
_______________________________________________
Surf the Web in a faster, safer and easier way:
Download Opera 8 at http://www.opera.com



From yuting at iastate.edu  Thu Sep 22 21:10:41 2005
From: yuting at iastate.edu (yuting@iastate.edu)
Date: Thu, 22 Sep 2005 14:10:41 -0500 (CDT)
Subject: [R] Rf_initEmbeddedR in Windows
Message-ID: <3133.129.186.194.98.1127416241.squirrel@mail.eng.iastate.edu>

Hi All

My C++/linux program uses Rf_initEmbeddedR to start R and then calls some
R functions. Now I try to port it to Windows. Give the fact that
Rf_initEmbeddedR is missing in Windows, I try to use the implmentation in
Rserve by Simon Urbanek. When I build in MS Visual Studio, I get the
following linking error

error LNK2019: unresolved external symbol __imp__putenv referenced in
function _Rf_initEmbeddedR
error LNK2019: unresolved external symbol __imp__UserBreak referenced in
function _my_onintr
error LNK2019: unresolved external symbol _getDLLVersion referenced in
function _Rf_initEmbeddedR
error LNK2019: unresolved external symbol _R_DefParams referenced in
function _Rf_initEmbeddedR
error LNK2019: unresolved external symbol _R_SetParams referenced in
function _Rf_initEmbeddedR
error LNK2019: unresolved external symbol _R_SizeFromEnv referenced in
function _Rf_initEmbeddedR
error LNK2019: unresolved external symbol _setup_Rmainloop referenced in
function _Rf_initEmbeddedR
error LNK2019: unresolved external symbol _setup_term_ui referenced in
function _Rf_initEmbeddedR

I think the error come from the following declaration of functions calling
into R.dll and the missing of R.lib corresponding to R.dll.
extern char	*getDLLVersion();
extern void	R_DefParams(Rstart);
extern void  R_SetParams(Rstart);
extern void  setup_term_ui(void);
extern void  ProcessEvents(void);
extern void  end_Rmainloop(void), R_ReplDLLinit(void);
extern int  R_ReplDLLdo1();
extern void  run_Rmainloop(void);

Then I google it, I find some solution of Borland C++ Builder, i.e. using
implib to generate R.lib. The only place where  I can find implib is at 
(http://www.geocities.com/SiliconValley/5806/download.htm#IMPLIB) and the
commond generates a error as below
C:\Program Files\R\rw2011\bin>IMPLIB32.EXE R.dll
ImpLib32 Version 1.02 - ImpLib for Win32
Copyright (c) 1996-97 by Markus Seger (mseger at kagi.com)
Creating R.lib...
'c:\Program' is not recognized as an internal or external command,
operable program or batch file.
'c:\Program' is not recognized as an internal or external command,
operable program or batch file.

My questios are
1) Is it a right way to generate R.lib to remove the linking error.
2) If yes, how can I generate the R.lib valid for MS Visual Studio
3) If not, how can I call the functions directly in R.dll


Any helps are appreciated!

YUting



From pocernic at rap.ucar.edu  Thu Sep 22 21:28:19 2005
From: pocernic at rap.ucar.edu (Matt Pocernich)
Date: Thu, 22 Sep 2005 13:28:19 -0600 (MDT)
Subject: [R] Survey of ROC AUC / wilcoxon test functions
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD503B9325A@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD503B9325A@us-arlington-0668.mail.saic.com>
Message-ID: <Pine.LNX.4.62.0509221324460.4129@albedo.rap.ucar.edu>

Hi Jarek,

I am getting read to leave on a trip till next week, so I can't address 
your observation today.  I also can't seem to find the ROC library on CRAN 
and the moment.  Do you know where I would locate it?

Thanks for pointing out the discrepancy.  I'll talk to you later.

Matt ( verification package)


Matt Pocernich
NCAR - Research Applications Program
303-497-8312



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Sep 22 21:32:55 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 22 Sep 2005 15:32:55 -0400
Subject: [R] Survey of ROC AUC / wilcoxon test functions
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD503B932E1@us-arlington-0668.mail.saic.com>

ROC package can be found in BioConductor repository.

Jarek

-----Original Message-----
From: pocernic at rap.ucar.edu [mailto:pocernic at rap.ucar.edu] 
Sent: Thursday, September 22, 2005 3:28 PM
To: Tuszynski, Jaroslaw W.
Cc: (r-help at stat.math.ethz.ch.); bioinf at wehi.edu.au; bxc at steno.dk
Subject: Re: Survey of ROC AUC / wilcoxon test functions

Hi Jarek,

I am getting read to leave on a trip till next week, so I can't address your
observation today.  I also can't seem to find the ROC library on CRAN and
the moment.  Do you know where I would locate it?

Thanks for pointing out the discrepancy.  I'll talk to you later.

Matt ( verification package)


Matt Pocernich
NCAR - Research Applications Program
303-497-8312



From bagchi.r at gmail.com  Thu Sep 22 21:51:45 2005
From: bagchi.r at gmail.com (Robert Bagchi)
Date: Thu, 22 Sep 2005 20:51:45 +0100
Subject: [R] anova on binomial LMER objects
Message-ID: <43330B51.3070509@shef.ac.uk>

Dear R users,

I have been having problems getting believable estimates from anova on a 
model fit from lmer. I get the impression that F is being greatly 
underestimated, as can be seen by running the example I have given below.

First an explanation of what I'm trying to do. I am trying to fit a glmm 
with binomial errors to some data. The experiment involves 10 
shadehouses, divided between 2 light treatments (high, low). Within each 
shadehouse there are 12 seedlings of each of 2 species (hn & sl). 3 
damage treatments (0, 0.1, 0.25 leaf area removal) were applied to the 
seedlings (at random) so that there are 4 seedlings of each 
species*damage treatment in each shadehouse.  There maybe a shadehouse 
effect, so I need to include it as a random effect. Light is applied to 
a shadehouse, so it is outer to shadehouse. The other 2 factors are 
inner to shadehouse.

We want to assess if light, damage and species affect survival of 
seedlings. To test this I fitted a binomial mixed effects model with 
lmer (actually with quasibinomial errors). THe summary function suggests 
a large effect of both light and species (which agrees with graphical 
analysis). However, anova produces F values close to 0 and p values 
close to 1 (see example below).

Is this a bug, or am I doing something fundamentally wrong? If anova 
doesn't work with lmer is there a way to perform hypothesis tests on 
fixed effects in an lmer model? I was going to just delete terms and 
then do liklihood ratio tests, but according to Pinheiro & Bates (p. 87) 
that's very untrustworthy. Any suggestions?

I'm using R 2.1.1 on windows XP and lme4 0.98-1

Any help will be much appreciated.

many thanks
Robert

###############################
The data are somewhat like this

#setting up the dataframe

bm.surv<-data.frame(
                    house=rep(1:10, each=6),
                    light=rep(c("h", "l"), each=6, 5),
                    species=rep(c("sl", "hn"), each=3, 10),
                    damage=rep(c(0,.1,.25), 20)
                    )

bm.surv$survival<-ifelse(bm.surv$light=="h", rbinom(60, 4, .9), 
rbinom(60, 4, .6))       # difference in probablility should ensure a 
light effect
bm.surv$death<-4-bm.surv$survival

# fitting the model
m1<-lmer(cbind(survival, death)~light+species+damage+(1|house), 
data=bm.surv, family="quasibinomial")

summary(m1)         # suggests that light is very significant
Generalized linear mixed model fit using PQL
Formula: cbind(survival, death) ~ light + species + damage + (1 | table)
   Data: bm.surv
 Family: quasibinomial(logit link)
      AIC      BIC    logLik deviance
 227.0558 239.6218 -107.5279 215.0558
Random effects:
 Groups   Name        Variance   Std.Dev. 
 table    (Intercept) 1.8158e-09 4.2613e-05
 Residual             3.6317e+00 1.9057e+00
# of obs: 60, groups: table, 10

Fixed effects:
            Estimate Std. Error DF t value  Pr(>|t|)   
(Intercept)  2.35140    0.36832 56  6.3841 3.581e-08 ***
lightl      -1.71517    0.33281 56 -5.1535 3.447e-06 ***
speciessl   -0.57418    0.30085 56 -1.9085   0.06145 . 
damage       1.49963    1.46596 56  1.0230   0.31072   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
          (Intr) lightl spcssl
lightl    -0.665             
speciessl -0.494  0.070      
damage    -0.407 -0.038 -0.017


anova(m1)             # very low F value for light, corresponding to p 
values approaching 1

Analysis of Variance Table
        Df Sum Sq Mean Sq  Denom F value Pr(>F)
light    1  0.014   0.014 56.000  0.0018 0.9661
species  1  0.002   0.002 56.000  0.0002 0.9887
damage   1  0.011   0.011 56.000  0.0014 0.9704


-- 
Robert Bagchi
Animal & Plant Science
Alfred Denny Building
University of Sheffield
Western Bank
Sheffield S10 2TN
UK

t: +44 (0)114 2220062
e: r.bagchi at sheffield.ac.uk
   bagchi.r at gmail.com

http://www.shef.ac.uk/aps/apsrtp/bagchi-r



From ggrothendieck at gmail.com  Thu Sep 22 21:49:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 22 Sep 2005 15:49:00 -0400
Subject: [R] problem with dates
In-Reply-To: <20050922190555.7EFC5CA0A3@ws5-11.us4.outblaze.com>
References: <20050922190555.7EFC5CA0A3@ws5-11.us4.outblaze.com>
Message-ID: <971536df050922124951d1547b@mail.gmail.com>

Try converting it from factor to character first:

as.numeric(as.date(paste(date1)))

By the way, if you don't need the date package for a particular reason
you could use the built in Date class which does have an as.Date.factor
method.

On 9/22/05, sloan jones <sledepi at operamail.com> wrote:
> I Have been trying to convert a vector of dates into julian dates using the following commands: as.date & as.numeric.  I can convert a date no problem by doing the following:
>
> as.numeric (as.date ("9/21/2004"))
>
> but as soon as I try to do an entire vector I am given the following:
>
>  as.numeric (as.date(date1))
> Error in as.date(date1) : Cannot coerce to date format
>
> I have tried starting with a number of different date formats including dd/mm/yy, dd/mm/yyyy, month, day, year..
>
> Following is a sample of the vector of dates that I am using:
>
> > date1
>  [1] 7/1/2005   4/27/2005  7/15/2004  11/30/2004 1/8/2005   2/7/2005
>  [7] 7/17/2004  4/1/2004   1/4/2005   12/27/2004 1/5/2005   5/17/2005 ....
>
> it is a factor and its mode is numeric.
>
> What do I need to do?
>
> Sloan
>
> --
> _______________________________________________
> Surf the Web in a faster, safer and easier way:
> Download Opera 8 at http://www.opera.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From deepayan.sarkar at gmail.com  Thu Sep 22 22:19:31 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 22 Sep 2005 15:19:31 -0500
Subject: [R] annotating an axis in bwplot (lattice)
In-Reply-To: <87vf0vwjy5.fsf@gmail.com>
References: <87vf0vwjy5.fsf@gmail.com>
Message-ID: <eb555e6605092213193881b213@mail.gmail.com>

On 9/20/05, Sebastian Luque <spluque at gmail.com> wrote:
> Hi,
>
> I'd like to add, say, the sample size for every group in a bwplot as a
> parenthetical annotation to the axis.  Here's a sketch:
>
> --8<---------------cut here---------------start------------->8---
> require(Hmisc)
> age <- sample(1:100, 1000, replace = TRUE)
> sex <- gl(2, 8, 1000, c("Male", "Female"))
> grp <- gl(4, 6, 1000, letters[1:4])
>
> bwplot(grp ~ age | sex, aspect = 0.5, box.ratio = 2,
>        panel = function(x, y, ...) {
>          panel.bpplot(x, y, nout = 0.01, probs = seq(0.05, 0.45, 0.05))
>          nage <- tapply(age, grp, length)
>          panel.text(rep(0, length(x)), seq(along = x), labels = nage)
>        })
> --8<---------------cut here---------------end--------------->8---
>
> I have two problems here: 1. place the sample size as a note in
> parenthesis next to axis annotation label for the group (e.g. a (252), b
> (252), c (250), d (246)), and 2. handle more complex subsetting in the
> call to bwplot, i.e. when using the 'data' and 'subset' arguments, so that
> 'nage' in the code above is more flexible.

It's not clear to me what you want to do. Do you want the sample size
for the data in that panel, or for all the data? Your current
implementation does the latter, and in a way that wouldn't work if you
actually had a 'data' argument. If the former, then you should have
done


bwplot(grp ~ age | sex, aspect = 0.5, box.ratio = 2,
      panel = function(x, y, ...) {
        panel.bwplot(x, y, nout = 0.01, probs = seq(0.05, 0.45, 0.05))
        nage <- tapply(x, y, length)
        panel.text(rep(0, length(x)), seq(along = x), labels = nage)
      })

in which case data and subset wouldn't be a problem. If you really
want the frequencies for the whole (subsetted) data, you might as well
use something like:

dd <- data.frame(age, sex, grp)

with(subset(dd, age > 20),
     bwplot(grp ~ age | sex, aspect = 0.5, box.ratio = 2,
	    ylim = {
	      tg <- table(grp)
	      paste(names(tg), "(", tg, ")")
            }))

Deepayan



From deepayan.sarkar at gmail.com  Thu Sep 22 22:36:28 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 22 Sep 2005 15:36:28 -0500
Subject: [R] Are least-squares means useful or appropriate?
In-Reply-To: <432FCC31.8000407@unileon.es>
References: <432AAB98.4040103@unileon.es> <432F961C.5060404@pdf.com>
	<432FCC31.8000407@unileon.es>
Message-ID: <eb555e66050922133667f38d99@mail.gmail.com>

On 9/20/05, Felipe <felipe at unileon.es> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi.
> My question was just theoric. I was wondering if someone who were using
> SAS and R could give me their opinion on the topic. I was trying to use
> least-squares means for comparison in R, but then I found some
> indications against them, and I wanted to know if they had good basis
> (as I told earlier, they were not much detailed).

As a non-'SAS user', I'm not a good person to respond to this, but
aren't you asking the wrong crowd? I have never come across this
concept in a proper statistics course, and in my very brief encounter
with it, it made absolutely no sense to me. But of course this does
not automatically mean that it's non-sense or anything. So rather than
asking R users who do not use it why they do not use something that
there is no obvious reason to use to begin with, why don't you ask
those who do (like whoever taught you to use them when you worked with
SAS) to explain why they do?

Deepayan



From spluque at gmail.com  Fri Sep 23 00:03:37 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Thu, 22 Sep 2005 17:03:37 -0500
Subject: [R] annotating an axis in bwplot (lattice)
References: <87vf0vwjy5.fsf@gmail.com>
	<eb555e6605092213193881b213@mail.gmail.com>
Message-ID: <873bnw2212.fsf@gmail.com>

Hi Deepayan,


Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:

[...]

> If you really want the frequencies for the whole (subsetted) data, you
> might as well use something like:

> dd <- data.frame(age, sex, grp)

> with(subset(dd, age > 20),
> bwplot(grp ~ age | sex, aspect = 0.5, box.ratio = 2,
> 	    ylim = {
> 	      tg <- table(grp)
> 	      paste(names(tg), "(", tg, ")")
> }))

Thanks, this is almost what I was looking for.  Because I needed the
frequency of each group in *each* panel, I modified your suggestion like
(omitting the panel function for briefness):

--8<---------------cut here---------------start------------->8---
with(subset(dd, age > 20),
     bwplot(grp ~ age | sex, aspect = 0.5, box.ratio = 2,
            scales = list( y = list(relation = "free")),
	    ylim = {
	      tgf <- table(grp[sex == "Female"])
              tgm <- table(grp[sex == "Male"])
              list(paste(names(tgf), "(", tgf, ")"),
                   paste(names(tgm), "(", tgm, ")"))
            }))
--8<---------------cut here---------------end--------------->8---

One has to be careful though with the order of panels.


Thank you,

-- 
Sebastian P. Luque



From deepayan.sarkar at gmail.com  Fri Sep 23 00:44:50 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 22 Sep 2005 17:44:50 -0500
Subject: [R] annotating an axis in bwplot (lattice)
In-Reply-To: <873bnw2212.fsf@gmail.com>
References: <87vf0vwjy5.fsf@gmail.com>
	<eb555e6605092213193881b213@mail.gmail.com> <873bnw2212.fsf@gmail.com>
Message-ID: <eb555e6605092215441d2a0d2d@mail.gmail.com>

On 9/22/05, Sebastian Luque <spluque at gmail.com> wrote:
> Hi Deepayan,
>
>
> Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
>
> [...]
>
> > If you really want the frequencies for the whole (subsetted) data, you
> > might as well use something like:
>
> > dd <- data.frame(age, sex, grp)
>
> > with(subset(dd, age > 20),
> > bwplot(grp ~ age | sex, aspect = 0.5, box.ratio = 2,
> > 	    ylim = {
> > 	      tg <- table(grp)
> > 	      paste(names(tg), "(", tg, ")")
> > }))
>
> Thanks, this is almost what I was looking for.  Because I needed the
> frequency of each group in *each* panel, I modified your suggestion like
> (omitting the panel function for briefness):

This might be a better approach then:

bwplot(grp ~ age | sex, aspect = 0.5, box.ratio = 2,
       scales = list(y = "free", rot = 0),
       prepanel = function(x, y, ...) {
           tg <- table(y);
           list(ylim = paste(names(tg), "(", tg, ")"))
} )

Deepayan



From kerryrekky at yahoo.com  Fri Sep 23 03:07:28 2005
From: kerryrekky at yahoo.com (Cunningham Kerry)
Date: Thu, 22 Sep 2005 18:07:28 -0700 (PDT)
Subject: [R] how to keep very small or large number?
Message-ID: <20050923010730.15360.qmail@web51802.mail.yahoo.com>

Suppose I have n=1000 cases. For each case, the
probability can go like 1e-10, then I have to multiply
these probabilities together to get joint probability.

I tried to do the following:

p=1

x1=runif(n,0,1)
x2=runif(n,0,1)
y=ifelse(runif(m) < plogis(-6+6*x1+6*x2), 1, 0);

pta0=-6
pta1=6
pta2=6

for(i in 1:n)
{
  p <-
p*exp((y[i]*log(plogis(pta0+pta1*x1[i]+pta2*x2[i]))+(1-y[i])*log(1-plogis(pta0+pta1*x1[i]+pta2*x2[i]))))
}

but the result is shown to be zero.

--- Cunningham Kerry <kerryrekky at yahoo.com> wrote:

> When I was computing some joint probabilities, I
> found
> that R reported most of the results to to -Inf and
> thus didn't record the value. I guess it is b/c the
> joint log(probability) can be extremely small. Is
> there a way in R to keep the values even if they are
> small?
> 
> 
> 
> 		
> __________________________________ 

> http://mail.yahoo.com
>



From spencer.graves at pdf.com  Fri Sep 23 03:59:03 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Sep 2005 20:59:03 -0500
Subject: [R] how to keep very small or large number?
In-Reply-To: <20050923010730.15360.qmail@web51802.mail.yahoo.com>
References: <20050923010730.15360.qmail@web51802.mail.yahoo.com>
Message-ID: <43336167.6060607@pdf.com>

       Your code would not run for me, because "m" was not defined. 
When I let m=n=1000 and start your script with set.seed(1), set.seed(2), 
set.seed(3), and set.seed(4), I got  1.911056e-169, 1.264587e-162, 
2.225186e-176, and 9.289588e-175, respectively.  However, I'm quibbling: 
  With m=n=10000 and set.seed(4), I got 0.

       This problem is easily solved in R using the "log.p" and 
"lower.tail" arguments of "plogis".  The vector arithmetic also helps 
make things much easier to code if you can think vectors, in my opinion.

       Consider the following:

z <- pta0+pta1*x1+pta2*x2
ln.p <- plogis(z, log.p=TRUE, lower.tail=as.logical(y))
ln.p. <- sum(ln.p)
log10.p. <- ln.p./log(10)

printSmall <- function(ln.x){
   l10x <- ln.x/log(10)
   paste(10^(l10x%%1), "e", l10x%/%1, sep="")
}

printSmall(ln.p.)
  "6.43113656279925e-5355"

       How's this?
       spencer graves

Cunningham Kerry wrote:

> Suppose I have n=1000 cases. For each case, the
> probability can go like 1e-10, then I have to multiply
> these probabilities together to get joint probability.
> 
> I tried to do the following:
> 
> p=1
> 
> x1=runif(n,0,1)
> x2=runif(n,0,1)
> y=ifelse(runif(m) < plogis(-6+6*x1+6*x2), 1, 0);
> 
> pta0=-6
> pta1=6
> pta2=6
> 
> for(i in 1:n)
> {
>   p <-
> p*exp((y[i]*log(plogis(pta0+pta1*x1[i]+pta2*x2[i]))+(1-y[i])*log(1-plogis(pta0+pta1*x1[i]+pta2*x2[i]))))
> }
> 
> but the result is shown to be zero.
> 
> --- Cunningham Kerry <kerryrekky at yahoo.com> wrote:
> 
> 
>>When I was computing some joint probabilities, I
>>found
>>that R reported most of the results to to -Inf and
>>thus didn't record the value. I guess it is b/c the
>>joint log(probability) can be extremely small. Is
>>there a way in R to keep the values even if they are
>>small?
>>
>>
>>
>>		
>>__________________________________ 
> 
> 
>>http://mail.yahoo.com
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Fri Sep 23 04:01:17 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Sep 2005 21:01:17 -0500
Subject: [R] FDR analyses: minimum number of features
In-Reply-To: <C6E7B149DFD87D4AAD1ADA90B5DE0279493AEE@mailbe11.mc.vanderbilt.edu>
References: <C6E7B149DFD87D4AAD1ADA90B5DE0279493AEE@mailbe11.mc.vanderbilt.edu>
Message-ID: <433361ED.8070903@pdf.com>

	  Have you considered Monte Carlo?  From previous work, you could 
estimate a distribution for the differences to be detected and use that 
as input to a Monte Carlo, computing thereby a distribution for FDR as a 
function of distribution of differences and the number of features. 
 From this, you could estimate probabilities for obtaining results that 
were bogus vs. marginal, barely useful vs. highly accurate and plot them 
vs. alternative budgets, etc.

	  I hope this comment makes more sense than my earlier nonsense.

	  spencer graves

Dupont, William wrote:
> I agree.  What is unclear to me is the optimal way of justifying sample
> size and SNP selection in grant applications that use the FDR approach.
> 
> -----Original Message-----
> From: Kjetil Brinchmann Halvorsen [mailto:kjetil at acelerate.com] 
> Sent: Wednesday, September 21, 2005 9:45 PM
> To: Spencer Graves
> Cc: Dupont, William; r-help at stat.math.ethz.ch
> Subject: Re: [R] FDR analyses: minimum number of features
> 
> Spencer Graves wrote:
> 
> 
>>	  Two thoughts on this:
>>
>>	  1.  Your FDR (Not Franklin Delano Roosevelt) sounds like
> 
> another 
> 
>>name for Type I error rate.
>>
> 
> It is certainly not the same as type I error rate. Type I error rate is
> the proportion of true nulls which are rejected, while the FDR is the
> proportion of rejected null hypothesis which really are true nulls!
> 
> To me FDR seems like a more promising avenue to multiple testing than
> the old "familywise error rate". Who knows what is a family?
> 
> Kjetil
> 
> 
>>The definition of "reasonably reliable FDRs" 
>>would seem to relate to the status of the literature on this issue 
>>among researchers in genotyping.  As more reports of FRDs in genotyping
> 
> 
>>are published, I would expect that methodology for estimation and the 
>>standard for accuracy would similarly evolve.
>>
>>	  2.  Have you tried the Bioconductor (www.bioconductor.org/) 
>>listserve?  They might be able to say something more useful than a 
>>general list like this.
>>
>>	  spencer graves
>>
>>Dupont, William wrote:
>>
>> 
>>
>>
>>>Dear List,
>>>
>>>We are planning a genotyping study to be analyzed using false 
>>>discovery rates (FDRs) (See Storey and Tibshirani PNAS 2003; 
>>>100:9440-5).  I am interested in learning if there is any consensus as
> 
> 
>>>to how many features (ie. how many P values) need to be studied before
> 
> 
>>>reasonably reliable FDRs can be derived.  Does anyone know of a 
>>>citation where this is discussed?
>>>
>>>Bill Dupont
>>>
>>>William D. Dupont          phone: 615-343-4100          URL
>>>http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/WilliamDupont
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>   
>>>
>>
>> 
>>
> 
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From mdavy at hortresearch.co.nz  Fri Sep 23 04:34:11 2005
From: mdavy at hortresearch.co.nz (Marcus Davy)
Date: Fri, 23 Sep 2005 14:34:11 +1200
Subject: [R] as.character on OS X tiger
Message-ID: <BF59C2E3.4774%mdavy@hortresearch.co.nz>


Hi,
When I run this example code on a G5 the last digit is dropped off on a
binary installation of R (R.app), whereas on a linux machine the digit is
not removed.

# Code:
a <- seq(0,1, length=1000)[12]
a

as.character(a)
as.character(as.numeric(as.character(a)))


# Results I get:
a <- seq(0,1, length=1000)[12]
> a
[1] 0.01101101
> 
> as.character(a)
[1] "0.0110110110110110"
> as.character(as.numeric(as.character(a)))
[1] "0.011011011011011"

Can anyone enlighten me on the subtle difference between OS's, its causing
issues for me when I write to list elements with these characters strings
and some occasionally have lost the last "0".

Marcus

> sessionInfo()
R version 2.1.1, 2005-06-20, powerpc-apple-darwin7.9.0

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"


______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From zhengqi04 at mails.gucas.ac.cn  Fri Sep 23 04:48:02 2005
From: zhengqi04 at mails.gucas.ac.cn (=?gb2312?B?1qPkvw==?=)
Date: Fri, 23 Sep 2005 10:48:02 +0800
Subject: [R] More detail discriptions about my problem
Message-ID: <327443682.17167@gucas.ac.cn>

Dear Mr Bivand and all the authors,
Glad to recieve your letter, and it seems that you catched the very bug this time! First of all, there isn't any file named "graphics.Ex-rout" in direction    "tests/Examples/" but there are "graphics.Ex.R" and "graphics.Ex-Rout.fail" files there, the latter of which looks like a good crue of my problem. So I tared this file and sent it as an attached file of this letter, wishing it won't be rejected by your server. 

I also try demo(graphics) in a new R session and some errors occured as expected. The graphics demo can be displayed correctly until the third one. An mistake happened and braked down the command. The failing message is writed down below:( also some key words was translated into English by myself)
"Press 'Enter' for the next graphic :

> title(main = "January Pie Sales", cex.main = 1.8,
    font.main = 1)
error occurs in: title(main = "January Pie Sales", cex.main = 1.8, font.main = 1) :
        cannot load X11 font size 22 "

So it seems that there are still some problems with my package "X11", doesn't it?

All good luck!
                                                            
                                                              Yours sincerely,
                                                              An Chinese





From David.Duffy at qimr.edu.au  Fri Sep 23 05:04:53 2005
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Fri, 23 Sep 2005 13:04:53 +1000 (EST)
Subject: [R]  Clustering and bootstrap
In-Reply-To: <mailman.11.1127383200.22046.r-help@stat.math.ethz.ch>
References: <mailman.11.1127383200.22046.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0509231254530.3961@orpheus.qimr.edu.au>

> From: Eric Pante <ericpante at hotmail.com>
>
> Dear Listers,
>
> I emailed the list a few days ago about how to bootstrap a community
> matrix (species by sites) and get a consensus tree with node support. A
> friend pointed out that a similar question remained unanswered in 2004.
> I wish to re-word my question: is anyone aware of a package / method to
> obtain a majority-rule consensus tree from x distance matrices ? Is
> anyone using R to generate phylogenetic trees with node support ?
>

This is not (currently ;)) an R solution nor bootstrapping, instead
using a pretty general reversible-jump type Gibbs sampler (see P 11 onward
and refs)

http://eprint.uq.edu.au/archive/00001138/



From Roger.Bivand at nhh.no  Fri Sep 23 08:26:36 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 23 Sep 2005 08:26:36 +0200 (CEST)
Subject: [R] (New subject) Mandrake X11 install problems
In-Reply-To: <327443682.17167@gucas.ac.cn>
Message-ID: <Pine.LNX.4.44.0509230817280.29885-100000@reclus.nhh.no>

Could Mandrake users please examine this thread of make check failures? 
(Mandrake Linux 10.1 - fails graphics, possibly missing one of 100dpi or 
75dpi font rpms)

On Fri, 23 Sep 2005, [gb2312] ???????? wrote:

> Dear Mr Bivand and all the authors, Glad to recieve your letter, and it
> seems that you catched the very bug this time! First of all, there isn't
> any file named "graphics.Ex-rout" in direction "tests/Examples/" but
> there are "graphics.Ex.R" and "graphics.Ex-Rout.fail" files there, the
> latter of which looks like a good crue of my problem. So I tared this
> file and sent it as an attached file of this letter, wishing it won't be
> rejected by your server.

The file seems to be empty, so the make check on graphics may have failed 
immediately.

> 
> I also try demo(graphics) in a new R session and some errors occured as
> expected. The graphics demo can be displayed correctly until the third
> one. An mistake happened and braked down the command. The failing
> message is writed down below:( also some key words was translated into
> English by myself) "Press 'Enter' for the next graphic :
> 
> > title(main = "January Pie Sales", cex.main = 1.8,
>     font.main = 1)
> error occurs in: title(main = "January Pie Sales", cex.main = 1.8, font.main = 1) :
>         cannot load X11 font size 22 "
> 
> So it seems that there are still some problems with my package "X11",
> doesn't it?

I see from postings on R-help that Mandrake 10.1 users are advised to 
install both 'xorg-x11-100dpi-fonts' and 'xorg-x11-75dpi-fonts':

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/54320.html

and:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46407.html

Could you please say whether you only have one of these font files
installed now, and whether the problem is resolved by installing the
missing one?

> 
> All good luck!
>                                                             
>                                                               Yours sincerely,
>                                                               An Chinese

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Fri Sep 23 08:54:20 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Sep 2005 07:54:20 +0100 (BST)
Subject: [R] Rf_initEmbeddedR in Windows
In-Reply-To: <3133.129.186.194.98.1127416241.squirrel@mail.eng.iastate.edu>
References: <3133.129.186.194.98.1127416241.squirrel@mail.eng.iastate.edu>
Message-ID: <Pine.LNX.4.61.0509230745590.5638@gannet.stats>

How to generate an import library for R.dll for Visual Studio is discussed 
in README.packages: please do read it.

Do note the recommendation not to install R in a path with spaces - your 
tool is one of many that does not work with spaces.  But you don't want a 
Borland import library (any more than the MinGW import library which we 
do provide).

Note too the posting guide, and use the appropriate list.  This is not an 
appropriate question for R-help (it is not even a question about R, but 
about an unsupported compiler system).

On Thu, 22 Sep 2005 yuting at iastate.edu wrote:

> Hi All
>
> My C++/linux program uses Rf_initEmbeddedR to start R and then calls some
> R functions. Now I try to port it to Windows. Give the fact that
> Rf_initEmbeddedR is missing in Windows, I try to use the implmentation in
> Rserve by Simon Urbanek. When I build in MS Visual Studio, I get the
> following linking error
>
> error LNK2019: unresolved external symbol __imp__putenv referenced in
> function _Rf_initEmbeddedR
> error LNK2019: unresolved external symbol __imp__UserBreak referenced in
> function _my_onintr
> error LNK2019: unresolved external symbol _getDLLVersion referenced in
> function _Rf_initEmbeddedR
> error LNK2019: unresolved external symbol _R_DefParams referenced in
> function _Rf_initEmbeddedR
> error LNK2019: unresolved external symbol _R_SetParams referenced in
> function _Rf_initEmbeddedR
> error LNK2019: unresolved external symbol _R_SizeFromEnv referenced in
> function _Rf_initEmbeddedR
> error LNK2019: unresolved external symbol _setup_Rmainloop referenced in
> function _Rf_initEmbeddedR
> error LNK2019: unresolved external symbol _setup_term_ui referenced in
> function _Rf_initEmbeddedR
>
> I think the error come from the following declaration of functions calling
> into R.dll and the missing of R.lib corresponding to R.dll.
> extern char	*getDLLVersion();
> extern void	R_DefParams(Rstart);
> extern void  R_SetParams(Rstart);
> extern void  setup_term_ui(void);
> extern void  ProcessEvents(void);
> extern void  end_Rmainloop(void), R_ReplDLLinit(void);
> extern int  R_ReplDLLdo1();
> extern void  run_Rmainloop(void);
>
> Then I google it, I find some solution of Borland C++ Builder, i.e. using
> implib to generate R.lib. The only place where  I can find implib is at
> (http://www.geocities.com/SiliconValley/5806/download.htm#IMPLIB) and the
> commond generates a error as below
> C:\Program Files\R\rw2011\bin>IMPLIB32.EXE R.dll
> ImpLib32 Version 1.02 - ImpLib for Win32
> Copyright (c) 1996-97 by Markus Seger (mseger at kagi.com)
> Creating R.lib...
> 'c:\Program' is not recognized as an internal or external command,
> operable program or batch file.
> 'c:\Program' is not recognized as an internal or external command,
> operable program or batch file.
>
> My questios are
> 1) Is it a right way to generate R.lib to remove the linking error.
> 2) If yes, how can I generate the R.lib valid for MS Visual Studio
> 3) If not, how can I call the functions directly in R.dll


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dieter.menne at menne-biomed.de  Fri Sep 23 09:26:20 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 23 Sep 2005 07:26:20 +0000 (UTC)
Subject: [R] I have a problem with npmc
References: <d331d0ca292d.4332fed4@unal.edu.co>
Message-ID: <loom.20050923T092337-567@post.gmane.org>

Daniel Rodriguez Caicedo <drodriguezca <at> unal.edu.co> writes:

> 
> 
> I am tryng to make an analysis with non-parametric multiple comparisons with 
a data set from Pinciples and
> Procedures of Satistics (Steel and Torrie,1980). The data are of a one-way 
layouth:
> 
>  > DOKNOPAR
>    class  var
> 1   DOK1 19.4
> 2   DOK1 32.6
> 3   DOK1 27.0
...
> 
> But when i try making de npmc, 
> 
> > npmc(DOKNOPAR)
> 
> R displays this message:
> 
> Error in uniroot(f = function(arg) p - z.dist(arg, corr = corr, df = df,  : 
>         f() values at end points not of opposite sign
> 
....

Works for me under Windows, current R and packages, but needs some time. Try a 
str(DOKNOPAR), maybe your data are not exactly what you expecte

   cmp gn effect      lower.cl     upper.cl ...
1  1-2 10   0.94  0.5811513117 1.2988486883 ...
2  1-3 10   0.00 -0.0001573656 0.0001573656 
3  1-4 10   0.18 -0.5714102497 0.9314102497 
4  1-5 10   0.88  0.2349931421 1.5250068579 
....

Dieter



From Jordi.Molins at drkw.com  Fri Sep 23 09:37:48 2005
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Fri, 23 Sep 2005 09:37:48 +0200
Subject: [R] books about MCMC to use MCMC R packages?
Message-ID: <C5A76BA0CA4D734CA725124C4D6397AC032191D1@ibfftce502.de.ad.drkw.net>


Dear list users,

I need to learn about MCMC methods, and since there are several packages in
R that deal with this subject, I want to use them. 

I want to buy a book (or more than one, if necessary) that satisfies the
following requirements:

- it teaches well MCMC methods;

- it is easy to implement numerically the ideas of the book, and notation
and concepts are similar to the corresponding R packages that deal with MCMC
methods.

I have done a search and 2 books seem to satisfy my requirements:

- Markov Chain Monte Carlo In Practice, by W.R. Gilks and others.

- Monte Carlo Statistical methods, Robert and Casella.

What do people think about these books? Is there a suggestion of some other
book that could satisfy better my requirements?

Thank you very much in advance.




--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From ripley at stats.ox.ac.uk  Fri Sep 23 09:39:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Sep 2005 08:39:52 +0100 (BST)
Subject: [R] as.character on OS X tiger
In-Reply-To: <BF59C2E3.4774%mdavy@hortresearch.co.nz>
References: <BF59C2E3.4774%mdavy@hortresearch.co.nz>
Message-ID: <Pine.LNX.4.61.0509230757310.5638@gannet.stats>

This is just a question of rounding error.  You are computing 11/999, 
which is not exactly representable.  Most computers use IEC60559 
arithmetic, which gives a precision of .Machine$double.eps ~ 2e-16.  So 
you can expect rounding error around 11/999 * .Machine$double.eps ~ 2e-18. 
However, as.character is documented to represent the number to 15 
significant digits.

On all of Linux (i686 and AMD64), Solaris and Windows boxes I get

> a <- 11/999
> b <- as.numeric(as.character(a))
> b
[1] 0.01101101
> a - b
[1] 1.040834e-17
> as.character(a)
[1] "0.0110110110110110"
> as.character(b)
[1] "0.011011011011011"

and these are different representations of different numbers.

So your claim to get different results on `linux' is not one I can 
reproduce.

The fix is not to rely on the fine details of numerical (or character) 
representations of numbers: perhaps convert them back to numbers and use 
all.equal()?


On Fri, 23 Sep 2005, Marcus Davy wrote:

>
> Hi,
> When I run this example code on a G5 the last digit is dropped off on a
> binary installation of R (R.app), whereas on a linux machine the digit is
> not removed.
>
> # Code:
> a <- seq(0,1, length=1000)[12]
> a
>
> as.character(a)
> as.character(as.numeric(as.character(a)))
>
>
> # Results I get:
> a <- seq(0,1, length=1000)[12]
>> a
> [1] 0.01101101
>>
>> as.character(a)
> [1] "0.0110110110110110"
>> as.character(as.numeric(as.character(a)))
> [1] "0.011011011011011"
>
> Can anyone enlighten me on the subtle difference between OS's, its causing
> issues for me when I write to list elements with these characters strings
> and some occasionally have lost the last "0".

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From karin.lagesen at medisin.uio.no  Fri Sep 23 10:45:12 2005
From: karin.lagesen at medisin.uio.no (Karin Lagesen)
Date: Fri, 23 Sep 2005 10:45:12 +0200
Subject: [R] multi-class histogram?
Message-ID: <ypx6zmq4kw9z.fsf@uracil.uio.no>


I am new to R, and I couldn't find the answers to my question in a faq. 
This could however be because I didn't know what to look for...:)

I have three classes of data, data for bacteria, archaea and eukaryotes. 
I wish to display these in a histogram where all of the values are used 
to calculate each column. But, I want each column split in three, where 
the size of each coloured area represents the proportions of the values 
in that column that comes from each of the three classes. Do you have 
any tips on how I do this? 

Karin
-- 
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://www.cmbn.no/rognes/



From petr.pikal at precheza.cz  Fri Sep 23 11:02:51 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 23 Sep 2005 11:02:51 +0200
Subject: [R] multi-class histogram?
In-Reply-To: <ypx6zmq4kw9z.fsf@uracil.uio.no>
Message-ID: <4333E0DB.15944.B84334@localhost>

Hi

Look at ?barplot and its parameter beside.

HTH
Petr


On 23 Sep 2005 at 10:45, Karin Lagesen wrote:

> 
> I am new to R, and I couldn't find the answers to my question in a
> faq. This could however be because I didn't know what to look for...:)
> 
> I have three classes of data, data for bacteria, archaea and
> eukaryotes. I wish to display these in a histogram where all of the
> values are used to calculate each column. But, I want each column
> split in three, where the size of each coloured area represents the
> proportions of the values in that column that comes from each of the
> three classes. Do you have any tips on how I do this? 
> 
> Karin
> -- 
> Karin Lagesen, PhD student
> karin.lagesen at medisin.uio.no
> http://www.cmbn.no/rognes/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From p.dalgaard at biostat.ku.dk  Fri Sep 23 11:04:24 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Sep 2005 11:04:24 +0200
Subject: [R] multi-class histogram?
In-Reply-To: <ypx6zmq4kw9z.fsf@uracil.uio.no>
References: <ypx6zmq4kw9z.fsf@uracil.uio.no>
Message-ID: <x28xxo40kn.fsf@turmalin.kubism.ku.dk>

Karin Lagesen <karin.lagesen at medisin.uio.no> writes:

> I am new to R, and I couldn't find the answers to my question in a faq. 
> This could however be because I didn't know what to look for...:)
> 
> I have three classes of data, data for bacteria, archaea and eukaryotes. 
> I wish to display these in a histogram where all of the values are used 
> to calculate each column. But, I want each column split in three, where 
> the size of each coloured area represents the proportions of the values 
> in that column that comes from each of the three classes. Do you have 
> any tips on how I do this? 

Well, the thing is that a histogram tries to represent a probability
distribution on the line (it is a density estimator), and the thing
that you want does not; barplot() is better at this sort of job.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From christophe.pouzat at univ-paris5.fr  Fri Sep 23 11:21:47 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Fri, 23 Sep 2005 11:21:47 +0200
Subject: [R] books about MCMC to use MCMC R packages?
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC032191D1@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC032191D1@ibfftce502.de.ad.drkw.net>
Message-ID: <4333C92B.7050102@univ-paris5.fr>

Hello,

I don't know yet of any book which presents MCMC methods with R examples 
so I can't answer to this part of your question. But I can suggest some 
general references (see the attached BibTeX file for details):

My favorite starting point is Radford Neal review from 1993, you can 
download it from his web-site.

Julian Besag's 2000 working paper is also a good starting point 
especially for statisticians (you can also download it).

If you're not scared at seeing the minus log likelihood referred to as 
the energy you can take a look at the Physics literature (Sokal,  1996; 
Berg 2004 and 2004b).  It's a good way to learn about tricks physicists 
use to get faster relaxation of their chains, like simulated annealing 
and the replica exchange method / parallel tempering method. These 
tricks were apparently first found by statisticians (Geyer, 1991; Geyer 
& Thompson, 1995; Ogata, 1995; review by Iba, 2001) but don't seem to 
attract much attention in this community. In my experience they work 
spectacularly well.

Robert and Casella, 2004 is a thorough reference with a bit too much on 
reversible jump techniques and not enough on physicians tricks (in my 
opinion of course).

Liu, 2001 is a spectacular overview. He knows very well both the 
statistical and physical literatures. But it's often frustrating because 
not enough details are given (for slow guys like me at least).

Fishman, 1996 is very comprehensive with much more than MCMC (that he 
calls "random tours").

Finally a note of caution about MCMC method can be useful, see Ripley, 1996.

I hope that helps,

Christophe.

Molins, Jordi wrote:

>Dear list users,
>
>I need to learn about MCMC methods, and since there are several packages in
>R that deal with this subject, I want to use them. 
>
>I want to buy a book (or more than one, if necessary) that satisfies the
>following requirements:
>
>- it teaches well MCMC methods;
>
>- it is easy to implement numerically the ideas of the book, and notation
>and concepts are similar to the corresponding R packages that deal with MCMC
>methods.
>
>I have done a search and 2 books seem to satisfy my requirements:
>
>- Markov Chain Monte Carlo In Practice, by W.R. Gilks and others.
>
>- Monte Carlo Statistical methods, Robert and Casella.
>
>What do people think about these books? Is there a suggestion of some other
>book that could satisfy better my requirements?
>
>Thank you very much in advance.
>
>
>
>
>--------------------------------------------------------------------------------
>The information contained herein is confidential and is inte...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html


From zhengqi04 at mails.gucas.ac.cn  Fri Sep 23 11:36:30 2005
From: zhengqi04 at mails.gucas.ac.cn (=?gb2312?B?1qPkvw==?=)
Date: Fri, 23 Sep 2005 17:36:30 +0800
Subject: [R] Good news about my problem
Message-ID: <327468190.18358@gucas.ac.cn>

Greetings, Mr Bivand and all the other authors,

I'm glad to say that there are some encouraging advances in my debugging progress. Thanks to Mr Brivand, I could check the packages installed on my oparation system. I have found that I have only one of the two font files installed-'xorg-x11-75dpi-fonts' installed and left the other one. So I have it ('xorg-x11-100dpi-fonts') installed and then do those checks again. Howerver, nothing is changed after installing. Inspired by your advice, I searched for any packages related to 'X11' and 'font', and have them all installed accompained with those that they related to. For details, I installed 'xorg-x11-cyrillic-fonts-6.7.0-3mdk','libxorg-x11-static-devel' whose name contains the word 'devel' and some other packages they related to.
 
After doing all of these installation, I re-check the problem. Although I cannot pass the check command 'make check' nor can I find the file 'graphics-Ex.Rout', I can complete the demo command 'demo(graphics)' successfully! How happy I am! 

To ensure my conclusion, I will paste the last few lines of messages displayed after running the command 'demo(graphics)' below: Could try check it for me please?
 "> usr <- par("usr")

> rect(usr[1], usr[3], usr[2], usr[4], col = "green3")

> contour(x, y, volcano, levels = l, col = "yellow",
    lty = "solid", add = TRUE)

> box()

> title("A Topographic Map of Maunga Whau", font = 4)

> title(xlab = "Meters North", ylab = "Meters West",
    font = 3)

> mtext("10 Meter Contour Spacing", side = 3, line = 0.35,
    outer = FALSE, at = mean(par("usr")[1:2]), cex = 0.7, font = 3)

> par(bg = "cornsilk")

> coplot(lat ~ long | depth, data = quakes, pch = 21,
    bg = "green3")
press<return>for next graphic:

> par(opar)"

Thanks to your advice, I can display graphics correctly. And I am waiting for your further advice or suggestion to dealing with my problems. 

Thank you very much! Best wishes to everone!
                                               Sincerely yours,
					       An Chinese



From ym at climpact.com  Fri Sep 23 11:33:33 2005
From: ym at climpact.com (Yves Magliulo)
Date: 23 Sep 2005 11:33:33 +0200
Subject: [R] Smooth terms significance in GAM models
Message-ID: <1127468010.4977.43.camel@new-york.climpact.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050923/2c91afca/attachment.pl

From christophe.pouzat at univ-paris5.fr  Fri Sep 23 11:36:15 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Fri, 23 Sep 2005 11:36:15 +0200
Subject: [R] books about MCMC to use MCMC R packages?
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC032191D1@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC032191D1@ibfftce502.de.ad.drkw.net>
Message-ID: <4333CC8F.7070104@univ-paris5.fr>

This is the same mail as the previous one with a visible bibliography 
this time (sorry)...

Hello,

I don't know yet of any book which presents MCMC methods with R examples 
so I can't answer to this part of your question. But I can suggest some 
general references (see the attached BibTeX file for details):

My favorite starting point is Radford Neal review from 1993, you can 
download it from his web-site.

Julian Besag's 2000 working paper is also a good starting point 
especially for statisticians (you can also download it).

If you're not scared at seeing the minus log likelihood referred to as 
the energy you can take a look at the Physics literature (Sokal,  1996; 
Berg 2004 and 2004b).  It's a good way to learn about tricks physicists 
use to get faster relaxation of their chains, like simulated annealing 
and the replica exchange method / parallel tempering method. These 
tricks were apparently first found by statisticians (Geyer, 1991; Geyer 
& Thompson, 1995; Ogata, 1995; review by Iba, 2001) but don't seem to 
attract much attention in this community. In my experience they work 
spectacularly well.

Robert and Casella, 2004 is a thorough reference with a bit too much on 
reversible jump techniques and not enough on physicians tricks (in my 
opinion of course).

Liu, 2001 is a spectacular overview. He knows very well both the 
statistical and physical literatures. But it's often frustrating because 
not enough details are given (for slow guys like me at least).

Fishman, 1996 is very comprehensive with much more than MCMC (that he 
calls "random tours").

Finally a note of caution about MCMC method can be useful, see Ripley, 
1996.

I hope that helps,

Christophe.

PS: the bibliography

@TechReport{Neal_1993,
  Author         = {Neal, Radford M},
  Title          = {Probabilistic {I}nference {U}sing {M}arkov {C}hain
                   {M}onte {C}arlo {M}ethods},
  Institution    = {Department of Computer Science. University of Toronto},
  Number         = {CRG-TR-91-1},
  web            = {http://www.cs.toronto.edu/~radford/papers-online.html},
  year           = 1993
}

@TechReport{Besag_2000,
  Author         = {Besag, Julian},
  Title          = {Markov {C}hain {M}onte {C}arlo for {S}tatistical
                   {I}nference},
  Type           = {Working Paper},
  Number         = {9},
  Abstract       = {These notes provide an introduction to Markov chain
                   Monte Carlo methods that are useful in both Bayesian
                   and frequentist statistical inference. Such methods
                   have revolutionized what can be achieved
                   computationally, primarily but not only in the Bayesian
                   paradigm. The account begins by describing ordinary
                   Monte Carlo methods, which, in principle, have exactly
                   the same goals as the Markov chain versions but can
                   rarely be implemented. Subsequent sections describe
                   basic Markov chain Monte Carlo, founded on the Hastings
                   algorithm and including both the Metropolis method and
                   the Gibbs sampler as special cases, and go on to
                   discuss more recent developments. These include Markov
                   chain Monte Carlo p-values, the Langevin-Hastings
                   algorithm, auxiliary variables techniques, perfect
                   Markov chain Monte Carlo via coupling from the past,
                   and reversible jumps methods for target spaces of
                   varying dimensions. Specimen applications, drawn from
                   several different disciplines, are described throughout
                   the notes. Several of these appear for the first time.
                   All computations use APL as the programming language,
                   though this is not necessarily a recommendation! The
                   author welcomes comments and criticisms.},
  eprint         = {http://www.csss.washington.edu/Papers/wp9.pdf},
  URL            = {http://www.csss.washington.edu/Papers/},
  month          = sep,
  year           = 2000
}

@Book{Liu_2001,
  Author         = {Liu, Jun S.},
  Title          = {Monte {C}arlo {S}trategies in {S}cientific {C}omputing},
  Publisher      = {Springer Verlag},
  Series         = {Springer Series in Statistics},
  Edition        = {First},
  year           = 2001
}

@Book{RobertCasella_2004,
  Author         = {Robert, Christian P. and Casella, George},
  Title          = {Monte {C}arlo statistical methods},
  Publisher      = {Springer-Verlag},
  Series         = {Springer Texts in Statistics},
  Address        = {New York},
  Edition        = {Second},
  isbn           = {0-387-21239-6},
  year           = 2004
}

@InCollection{Sokal_1996,
  Author         = {Sokal, A.},
  Title          = {Monte {C}arlo methods in statistical mechanics:
                   foundations and new algorithms},
  BookTitle      = {Functional integration (Carg\`ese, 1996)},
  Publisher      = {Plenum},
  Volume         = {361},
  Series         = {NATO Adv. Sci. Inst. Ser. B Phys.},
  Pages          = {131--192},
  Address        = {New York},
  mrclass        = {82B80 (82B05 82B26 82B27)},
  mrnumber       = {MR1477456 (98k:82101)},
  mrreviewer     = {Emilio N. M. Cirillo},
  url            = {http://citeseer.nj.nec.com/sokal96monte.html},
  year           = 1997
}

@Article{Berg_2004,
  Author         = "Berg, Bernd A.",
  Title          = "Introduction to Markov Chain Monte Carlo Simulations
                   and their Statistical Analysis",
  eprint         = "cond-mat/0410490",
  Abstract       = {This article is a tutorial on Markov chain Monte Carlo
                   simulations and their statistical analysis. The
                   theoretical concepts are illustrated through many
                   numerical assignments from the author's book on the
                   subject. Computer code (in Fortran) is available for
                   all subjects covered and can be downloaded from the
                   web.},
  url            = {http://fr.arxiv.org/abs/cond-mat/0410490},
  year           = 2004
}


@Book{Berg_2004b,
  Author         = {Berg, Bernd A.},
  Title          = {Markov {C}hain {M}onte {C}arlo {S}imulations and their
                   {S}tatistical {A}nalysis},
  Publisher      = {World Scientific Publishing Company},
  isbn           = {9812389350},
  url            = {http://www.worldscibooks.com/physics/5602.html},
  year           = 2004
}

@Article{Iba_2001,
  Author         = "Iba, Yukito",
  Title          = "Extended Ensemble Monte Carlo",
  Journal        = "Int. J. Mod. Phys.",
  Volume         = "C12",
  Pages          = "623-656",
  eprint         = "cond-mat/0012323",
  Abstract       = {``Extended Ensemble Monte Carlo''is a generic term
                   that indicates a set of algorithms which are now
                   popular in a variety of fields in physics and
                   statistical information processing. Exchange Monte
                   Carlo (Metropolis-Coupled Chain, Parallel Tempering),
                   Simulated Tempering (Expanded Ensemble Monte Carlo),
                   and Multicanonical Monte Carlo (Adaptive Umbrella
                   Sampling) are typical members of this family. Here we
                   give a cross-disciplinary survey of these algorithms
                   with special emphasis on the great flexibility of the
                   underlying idea. In Sec.2, we discuss the background of
                   Extended Ensemble Monte Carlo. In Sec.3, 4 and 5, three
                   types of the algorithms, i.e., Exchange Monte Carlo,
                   Simulated Tempering, Multicanonical Monte Carlo are
                   introduced. In Sec.6, we give an introduction to
                   Replica Monte Carlo algorithm by Swendsen and Wang.
                   Strategies for the construction of special-purpose
                   extended ensembles are discussed in Sec.7. We stress
                   that an extension is not necessary restricted to the
                   space of energy or temperature. Even unphysical
                   (unrealizable) configurations can be included in the
                   ensemble, if the resultant fast mixing of the Markov
                   chain offsets the increasing cost of the sampling
                   procedure. Multivariate (multi-component) extensions
                   are also useful in many examples. In Sec.8, we give a
                   survey on extended ensembles with a state space whose
                   dimensionality is dynamically varying. In the appendix,
                   we discuss advantages and disadvantages of three types
                   of extended ensemble algorithms.},
  url            = {http://fr.arxiv.org/abs/cond-mat/0012323},
  year           = 2001
}

@InProceedings{Geyer_1991b,
  Author         = {Geyer, C. J.},
  Title          = {Markov chain {M}onte {C}arlo maximum likelihood},
  BookTitle      = {Computing {S}cience and {S}tatistics: {P}roc. 23rd
                   {S}ymp. {I}nterface},
  Pages          = {156-163},
  year           = 1991
}

@Article{GeyerThompson_1995,
  Author         = {Geyer, Charles J. and Thompson, Elizabeth A.},
  Title          = {Annealing {M}arkov {C}hain {M}onte {C}arlo with
                   {A}pplications to {A}ncestral {I}nference},
  Journal        = {Journal of the American Statistical Association},
  Volume         = {90},
  Number         = {431},
  Pages          = {909--920},
  eprint         = 
{http://links.jstor.org/sici?sici=0162-1459%28199509%2990%3A431%3C909%3AAMCMCW%3E2.0.CO%3B2-R},
  Abstract       = {Markov chain Monte Carlo (MCMC; the
                   Metropolis-Hastings algorithm) has been used for many
                   statistical problems, including Bayesian inference,
                   likelihood inference, and tests of significance. Though
                   the method generally works well, doubts about
                   convergence often remain. Here we propose MCMC methods
                   distantly related to simulated annealing. Our samplers
                   mix rapidly enough to be usable for problems in which
                   other methods would require eons of computing time.
                   They simulate realizations from a sequence of
                   distributions, allowing the distribution being
                   simulated to vary randomly over time. If the sequence
                   of distributions is well chosen, then the sampler will
                   mix well and produce accurate answers for all the
                   distributions. Even when there is only one distribution
                   of interest, these annealing-like samplers may be the
                   only known way to get a rapidly mixing sampler. These
                   methods are essential for attacking very hard problems,
                   which arise in areas such as statistical genetics. We
                   illustrate the methods with an application that is much
                   harder than any problem previously done by MCMC,
                   involving ancestral inference on a very large genealogy
                   (7 generations, 2,024 individuals). The problem is to
                   find, conditional on data on living individuals, the
                   probabilities of each individual having been a carrier
                   of cystic fibrosis. Exact calculation of these
                   conditional probabilities is infeasible. Moreover, a
                   Gibbs sampler for the problem would not mix in a
                   reasonable time, even on the fastest imaginable
                   computers. Our annealing-like samplers have mixing
                   times of a few hours. We also give examples of samplers
                   for the "witch's hat" distribution and the conditional
                   Strauss process.},
  month          = sep,
  year           = 1995
}

@Article{Ogata_1995,
  Author         = {Ogata, Yosihiko},
  Title          = {Markov {C}hain {M}onte {C}arlo {I}ntegration {T}hrough
                   {S}imulated {A}nnealing and {I}ts {A}pplication to
                   {L}ikelihood {C}omputation of {B}ayesian {M}odels},
  Journal        = {Bull. Int. Stat. Inst.},
  Volume         = {56},
  Number         = {4},
  Pages          = {1873-1891},
  year           = 1995
}

@Book{Ripley_1996,
  Author         = {Ripley, B. D.},
  Title          = {Pattern {R}ecognition and {N}eural {N}etworks},
  Publisher      = {Cambridge University Press},
  url            = {http://www.stats.ox.ac.uk/~ripley/PRbook/},
  year           = 1996
}

@Book{Fishman_1996,
  Author         = {Fishman, George S},
  Title          = {Monte {C}arlo. {C}oncepts, {A}lgorithms, and
                   {A}pplications},
  Publisher      = {Springer Verlag},
  Series         = {Springer Series in Opreations Research},
  Edition        = {First},
  year           = 1996
}




Molins, Jordi wrote:

>Dear list users,
>
>I need to learn about MCMC methods, and since there are several packages in
>R that deal with this subject, I want to use them. 
>
>I want to buy a book (or more than one, if necessary) that satisfies the
>following requirements:
>
>- it teaches well MCMC methods;
>
>- it is easy to implement numerically the ideas of the book, and notation
>and concepts are similar to the corresponding R packages that deal with MCMC
>methods.
>
>I have done a search and 2 books seem to satisfy my requirements:
>
>- Markov Chain Monte Carlo In Practice, by W.R. Gilks and others.
>
>- Monte Carlo Statistical methods, Robert and Casella.
>
>What do people think about these books? Is there a suggestion of some other
>book that could satisfy better my requirements?
>
>Thank you very much in advance.
>
>
>
>
>--------------------------------------------------------------------------------
>The information contained herein is confidential and is inte...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From booopi at yahoo.com  Fri Sep 23 12:31:15 2005
From: booopi at yahoo.com (booop booop)
Date: Fri, 23 Sep 2005 03:31:15 -0700 (PDT)
Subject: [R] suggest some books or materials for matrices basics ,
	its partitions, iterations, clusturing
Message-ID: <20050923103115.29625.qmail@web61219.mail.yahoo.com>

Dear sir,
Could anybody kindly suggest me some good
e-books(which are available in the internet),or other
materials in the web....
sites to refer examples ...
in the following area

matrices & its basics....
partitioning of matrices...
iterations of matrices....
Clusturing in matrices...

thank you sir..

with kind regards,
boopathy.



From christophe.pouzat at univ-paris5.fr  Fri Sep 23 13:30:24 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Fri, 23 Sep 2005 13:30:24 +0200
Subject: [R] books about MCMC to use MCMC R packages?
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC032191D4@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC032191D4@ibfftce502.de.ad.drkw.net>
Message-ID: <4333E750.5040708@univ-paris5.fr>

Hi Jordi,

As far as implementions are concerned the book of Bernd Berg seems to be 
the closest to what you're looking for.
You can find a link to the Fortran codes implementing the methods he 
describes from his web site:

http://www.csit.fsu.edu/~berg/

There is also a nice reference for the analysis of the output of MCMC 
algorithm by Wolfhard Janke:
Janke W (2002) Statistical Analysis of Simulations: Data Correlations 
and Error Estimation. In, Quantum Simulations of Complex Many-Body 
Systems: From Theory to Algorithms, Lecture Notes, J. Grotendorst, D. 
Marx, A. Muramatsu (Eds.), John von Neumann Institute for Computing, 
J??lich, NIC Series, Vol. *10*, pp. 423-445.
http://www.fz-juelich.de/nic-series/volume10


Christophe.

Molins, Jordi wrote:

>Hi Christophe,
>
>thank you very much for your detailed answer!
>
>I am not scared about physics literature, because I am a physicist myself,
>working in finance. So your suggestions suit me very well.
>
>What I would like is to implement numerically these methods. Is there some
>that goes closer into implementation?
>
>Thanks!
>
>Jordi
>  
>


-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From B.Rowlingson at lancaster.ac.uk  Fri Sep 23 13:36:40 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 23 Sep 2005 12:36:40 +0100
Subject: [R] warning.expression?
In-Reply-To: <200509221900.56739.thomas.friedrichsmeier@ruhr-uni-bochum.de>
References: <200509221814.12666.thomas.friedrichsmeier@ruhr-uni-bochum.de>	<4332DF91.8040607@jhsph.edu>
	<200509221900.56739.thomas.friedrichsmeier@ruhr-uni-bochum.de>
Message-ID: <4333E8C8.7000509@lancaster.ac.uk>

Thomas Friedrichsmeier wrote:

> Yes, thanks for pointing it out. However, I'm actually looking for a way to 
> catch all warnings in a whole (interactive) session. Can warning.expression 
> be used for that?

  I've just been nosing around the source code in errors.c, and it 
doesn't look good.

In this function:

static void vwarningcall_dflt(SEXP call, const char *format, va_list ap)

the warning.expression 's' is called here:

        cptr = R_GlobalContext;
         while ( !(cptr->callflag & CTXT_FUNCTION) && cptr->callflag )
             cptr = cptr->nextcontext;
         eval(s, cptr->cloenv);
	return;

but when the expression is null/nil the code goes on to the default 
case, in which it gets the warning message from the 'call' parameter:


  dcall = CHAR(STRING_ELT(deparse1(call, 0, SIMPLEDEPARSE), 0));
  REprintf(_("Warning in %s : "), dcall);

  So I don't see how this parameter can be available to the 
warning.expression call. There may be a way, but I don't see it.

  It seems a bit dumb that warning.expression functions can only say 
"Hey, something a bit iffy may have ocurred, but I dont know what and I 
dont know where!". Maybe there's something in that cptr->cloenv that can 
tell you...

  Otherwise it requires patching.

Baz



From h.wickham at gmail.com  Fri Sep 23 01:36:04 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 22 Sep 2005 18:36:04 -0500
Subject: [R] [R-pkgs] Reshape package: new version 0.5
Message-ID: <f8e6ff05050922163639f6d954@mail.gmail.com>

Reshape version 0.5
===================

Reshape is an R package for flexibly restructuring and aggregating
data.  It's very much pivot table inspired, and it (hopefully) makes
it very easy to view your data the way you want.  You can find out
more at http://had.co.nz/reshape

The big news in this version of reshape is that I've renamed all the
functions so they no longer conflict with any built in functions -
deshape is now melt (think liquid data) and reshape is now cast (think
solidifying data into the form you want).

What else is new?

 * recast melts and casts your data in one step
 * column naming bugs fixed
 * easily display all margins with the argument margins=T in cast

Please let me know what you think.

Regards,

Hadley

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From karin.lagesen at medisin.uio.no  Fri Sep 23 14:14:04 2005
From: karin.lagesen at medisin.uio.no (Karin Lagesen)
Date: Fri, 23 Sep 2005 14:14:04 +0200
Subject: [R] multifigure question
Message-ID: <ypx6slvwkmlv.fsf@uracil.uio.no>


I would like to put three figures next to each other in
a figure. I have been reading the introduction to R, 
section 12 several times now, and I still can't make heads
or tails out of it.

Lets say that I have three dataframes a, b, c, and I want
to plot a$V1, b$V1 and c$V1 in separate plots simply using
plot(), how do I put them next to each other?

I am sorry if this is a FAQ, but I cannot understand what
it says in the guide and the wonderful google couldn't 
turn up anything helpful either...:)

Karin
-- 
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://www.cmbn.no/rognes/



From dimitris.rizopoulos at med.kuleuven.be  Fri Sep 23 14:27:06 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 23 Sep 2005 14:27:06 +0200
Subject: [R] multifigure question
References: <ypx6slvwkmlv.fsf@uracil.uio.no>
Message-ID: <008d01c5c03a$1c616bf0$0540210a@www.domain>

probably you need:

par(mfrow = c(1, 3))
plot(a$V1)
plot(b$V1)
plot(c$V1)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Karin Lagesen" <karin.lagesen at medisin.uio.no>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, September 23, 2005 2:14 PM
Subject: [R] multifigure question


>
> I would like to put three figures next to each other in
> a figure. I have been reading the introduction to R,
> section 12 several times now, and I still can't make heads
> or tails out of it.
>
> Lets say that I have three dataframes a, b, c, and I want
> to plot a$V1, b$V1 and c$V1 in separate plots simply using
> plot(), how do I put them next to each other?
>
> I am sorry if this is a FAQ, but I cannot understand what
> it says in the guide and the wonderful google couldn't
> turn up anything helpful either...:)
>
> Karin
> -- 
> Karin Lagesen, PhD student
> karin.lagesen at medisin.uio.no
> http://www.cmbn.no/rognes/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From hb at maths.lth.se  Fri Sep 23 14:38:43 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 23 Sep 2005 14:38:43 +0200
Subject: [R] warning.expression?
In-Reply-To: <4332DF91.8040607@jhsph.edu>
References: <200509221814.12666.thomas.friedrichsmeier@ruhr-uni-bochum.de>
	<4332DF91.8040607@jhsph.edu>
Message-ID: <4333F753.4090004@maths.lth.se>

Roger D. Peng wrote:
> You might be interested in 'tryCatch' to catch warnings.

A warning here: tryCatch(expr, warning=...) will catch warnings and 
interrupt 'expr' (as if an error occured), whereas 
withCallingHandlers(expr, warning=...) does not do this.

/Henrik

> -roger
> 
> Thomas Friedrichsmeier wrote:
> 
>>Hi!
>>
>>I'm trying to catch all warning-messages for special handling. It seems 
>>options (warning.expression=myfunc ()) can be used for that. However, the 
>>question is: How can I get at the actual warning in myfunc ()? Apparently in 
>>S, you can use .C("get_last_message") for that. Is there a similar mechanism 
>>in R?
>>
>>Thanks for your help!
>>Thomas
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
>



From ggrothendieck at gmail.com  Fri Sep 23 14:51:16 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Sep 2005 08:51:16 -0400
Subject: [R] multifigure question
In-Reply-To: <ypx6slvwkmlv.fsf@uracil.uio.no>
References: <ypx6slvwkmlv.fsf@uracil.uio.no>
Message-ID: <971536df05092305515efea084@mail.gmail.com>

There are actually several ways to do this in R.  You probably
want par with mfrow= or mfcol= as has already been mentioned
but here are some possibilities:

1. par(mfrow=...) has already been mentioned with an example.

2. par(mfcol=...) is similar.  See ?par

3. layout.  See ?layout

4. split.screen.  See ?split.screen

5. grid.layout using grid graphics.  See:
         http://tolstoy.newcastle.edu.au/R/devel/05/06/1169.html
for an example.

6. Some of graphics routines themselves can do this in one
call.  plot.ts will plot ts class time series in mulitiple plots (unless
you specify plot.type = "single".  e.g.
  plot(ts(matrix(1:10,5)))
plot.zoo from the zoo package is similar but also has an nc=
argument for specifying the arrangement:
  library(zoo)
  plot(zoo(matrix(1:15,5)), nc = 3)
Various lattice routines that support conditioning such as xyplot
will automatically plot multiple plots.  The layout= argument to
xyplot can control this.
  library(lattice)
  xyplot(Sepal.Length ~ Sepal.Width | Species, data = iris, layout = c(3,1))

Try help with the various commands above.


On 9/23/05, Karin Lagesen <karin.lagesen at medisin.uio.no> wrote:
>
> I would like to put three figures next to each other in
> a figure. I have been reading the introduction to R,
> section 12 several times now, and I still can't make heads
> or tails out of it.
>
> Lets say that I have three dataframes a, b, c, and I want
> to plot a$V1, b$V1 and c$V1 in separate plots simply using
> plot(), how do I put them next to each other?
>
> I am sorry if this is a FAQ, but I cannot understand what
> it says in the guide and the wonderful google couldn't
> turn up anything helpful either...:)
>
> Karin
> --
> Karin Lagesen, PhD student
> karin.lagesen at medisin.uio.no
> http://www.cmbn.no/rognes/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From vdemart1 at tin.it  Fri Sep 23 15:06:29 2005
From: vdemart1 at tin.it (Vittorio)
Date: Fri, 23 Sep 2005 14:06:29 +0100 (GMT+01:00)
Subject: [R] Strange behaviour of as.Date function
Message-ID: <31387430.1127480789721.JavaMail.root@pswm17.cp.tin.it>

Dear All,
I'm happily extracting data of temperature from an oracle db 
under R via RODBC. After manipulating the extracted data I put them 
into a data.frame 'dati' which is as follows:

> dati
         DATA tm.
UDINE/RIVOLTO tm.TORINO/CASELLE
1  2005-07-01            
22.35             23.80
2  2005-07-02            22.70             
22.85
3  2005-07-03            23.80             24.30
4  2005-07-
04            23.80             25.40
..........

and

> str(dati)
`data.frame':	11 obs. of  3 variables:
 $ DATA             :'POSIXct', 
format: chr  "2005-07-01" "2005-07-02" "2005-07-03" "2005-07-04" ...
 $ 
tm.UDINE/RIVOLTO : num  22.4 22.7 23.8 23.8 21.8 ...
 $ tm.
TORINO/CASELLE: num  23.8 22.9 24.3 25.4 21.8 ...
 - attr(*, 
"reshapeWide")=List of 5
  ..$ v.names: NULL
  ..$ timevar: chr "NOME"
  ..$ idvar  : chr "DATA"
  ..$ times  : Factor w/ 2 levels 
"TORINO/CASELLE",..: 2 1
  ..$ varying: chr [1, 1:2] "tm.UDINE/RIVOLTO" 
"tm.TORINO/CASELLE"
> 

You see that the first field DATA is POSIXct

Now
> dati[1,1]
[1] "2005-07-01 ora solare Europa occidentale"

BUT

> 
as.Date(dati[1,1],"%d%m%Y")
[1] "2005-06-30"

How come?
What is wrong 
with it (or better with me)?

Ciao
Vittorio



From ggrothendieck at gmail.com  Fri Sep 23 15:16:19 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Sep 2005 09:16:19 -0400
Subject: [R] Strange behaviour of as.Date function
In-Reply-To: <31387430.1127480789721.JavaMail.root@pswm17.cp.tin.it>
References: <31387430.1127480789721.JavaMail.root@pswm17.cp.tin.it>
Message-ID: <971536df05092306164266bb07@mail.gmail.com>

When it converts the time to Date it does it relative to the
GMT time zone, not your time zone.  When its July 1st
in your time zone it can be June 30th in the GMT time zone.

See the article in R News 4/1 Help Desk, and the table at the
end of the article in particular, on ways to handle this.

On 9/23/05, Vittorio <vdemart1 at tin.it> wrote:
> Dear All,
> I'm happily extracting data of temperature from an oracle db
> under R via RODBC. After manipulating the extracted data I put them
> into a data.frame 'dati' which is as follows:
>
> > dati
>         DATA tm.
> UDINE/RIVOLTO tm.TORINO/CASELLE
> 1  2005-07-01
> 22.35             23.80
> 2  2005-07-02            22.70
> 22.85
> 3  2005-07-03            23.80             24.30
> 4  2005-07-
> 04            23.80             25.40
> ..........
>
> and
>
> > str(dati)
> `data.frame':   11 obs. of  3 variables:
>  $ DATA             :'POSIXct',
> format: chr  "2005-07-01" "2005-07-02" "2005-07-03" "2005-07-04" ...
>  $
> tm.UDINE/RIVOLTO : num  22.4 22.7 23.8 23.8 21.8 ...
>  $ tm.
> TORINO/CASELLE: num  23.8 22.9 24.3 25.4 21.8 ...
>  - attr(*,
> "reshapeWide")=List of 5
>  ..$ v.names: NULL
>  ..$ timevar: chr "NOME"
>  ..$ idvar  : chr "DATA"
>  ..$ times  : Factor w/ 2 levels
> "TORINO/CASELLE",..: 2 1
>  ..$ varying: chr [1, 1:2] "tm.UDINE/RIVOLTO"
> "tm.TORINO/CASELLE"
> >
>
> You see that the first field DATA is POSIXct
>
> Now
> > dati[1,1]
> [1] "2005-07-01 ora solare Europa occidentale"
>
> BUT
>
> >
> as.Date(dati[1,1],"%d%m%Y")
> [1] "2005-06-30"
>
> How come?
> What is wrong
> with it (or better with me)?
>
> Ciao
> Vittorio



From william.dupont at Vanderbilt.Edu  Fri Sep 23 15:18:01 2005
From: william.dupont at Vanderbilt.Edu (Dupont, William)
Date: Fri, 23 Sep 2005 08:18:01 -0500
Subject: [R] FDR analyses: minimum number of features
Message-ID: <C6E7B149DFD87D4AAD1ADA90B5DE0279493AFC@mailbe11.mc.vanderbilt.edu>

 Thanks.  That is an excellent idea.  Bill

-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Thursday, September 22, 2005 9:01 PM
To: Dupont, William
Cc: Kjetil Brinchmann Halvorsen; r-help at stat.math.ethz.ch
Subject: Re: [R] FDR analyses: minimum number of features

	  Have you considered Monte Carlo?  From previous work, you
could estimate a distribution for the differences to be detected and use
that as input to a Monte Carlo, computing thereby a distribution for FDR
as a function of distribution of differences and the number of features.

 From this, you could estimate probabilities for obtaining results that
were bogus vs. marginal, barely useful vs. highly accurate and plot them
vs. alternative budgets, etc.

	  I hope this comment makes more sense than my earlier nonsense.

	  spencer graves



From rschulz at sonic.net  Fri Sep 23 15:43:04 2005
From: rschulz at sonic.net (Randall R Schulz)
Date: Fri, 23 Sep 2005 06:43:04 -0700
Subject: [R] [R-pkgs] Reshape package: new version 0.5
In-Reply-To: <f8e6ff05050922163639f6d954@mail.gmail.com>
References: <f8e6ff05050922163639f6d954@mail.gmail.com>
Message-ID: <200509230643.05068.rschulz@sonic.net>

Hadley,

On Thursday 22 September 2005 16:36, hadley wickham wrote:
> Reshape version 0.5
> ===================
>
> Reshape is an R package for flexibly restructuring and aggregating
> data.  It's very much pivot table inspired, and it (hopefully) makes
> it very easy to view your data the way you want.  You can find out
> more at http://had.co.nz/reshape
>
> ...
>
> Please let me know what you think.

Perhaps I'm dense, but I cannot find the software at the URL you 
mention. There are links to a paper and a PDF slide presentation, but 
no R package.

There is also a dangling link (<http://r-project.org/>) in the first 
sentence.


> Regards,
>
> Hadley


Randall Schulz



From sundar.dorai-raj at pdf.com  Fri Sep 23 15:48:00 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 23 Sep 2005 08:48:00 -0500
Subject: [R] [R-pkgs] Reshape package: new version 0.5
In-Reply-To: <200509230643.05068.rschulz@sonic.net>
References: <f8e6ff05050922163639f6d954@mail.gmail.com>
	<200509230643.05068.rschulz@sonic.net>
Message-ID: <43340790.7060104@pdf.com>



Randall R Schulz wrote:
> Hadley,
> 
> On Thursday 22 September 2005 16:36, hadley wickham wrote:
> 
>>Reshape version 0.5
>>===================
>>
>>Reshape is an R package for flexibly restructuring and aggregating
>>data.  It's very much pivot table inspired, and it (hopefully) makes
>>it very easy to view your data the way you want.  You can find out
>>more at http://had.co.nz/reshape
>>
>>...
>>
>>Please let me know what you think.
> 
> 
> Perhaps I'm dense, but I cannot find the software at the URL you 
> mention. There are links to a paper and a PDF slide presentation, but 
> no R package.
> 
> There is also a dangling link (<http://r-project.org/>) in the first 
> sentence.
> 

Hi, Randall,

The first section of the webpage says:

<quote>
How to install

install.packages("reshape")
</quote>

Try the latter call at the R prompt.

HTH,

--sundar



From rschulz at sonic.net  Fri Sep 23 15:57:08 2005
From: rschulz at sonic.net (Randall R Schulz)
Date: Fri, 23 Sep 2005 06:57:08 -0700
Subject: [R] [R-pkgs] Reshape package: new version 0.5
In-Reply-To: <43340790.7060104@pdf.com>
References: <f8e6ff05050922163639f6d954@mail.gmail.com>
	<200509230643.05068.rschulz@sonic.net> <43340790.7060104@pdf.com>
Message-ID: <200509230657.08958.rschulz@sonic.net>

Sundar,

On Friday 23 September 2005 06:48, Sundar Dorai-Raj wrote:
> Randall R Schulz wrote:
> > Hadley,
> >
> > ...
> >
> > Perhaps I'm dense, but I cannot find the software at the URL you
> > mention. There are links to a paper and a PDF slide presentation,
> > but no R package.
> >
> > There is also a dangling link (<http://r-project.org/>) in the
> > first sentence.
>
> Hi, Randall,
>
> The first section of the webpage says:
>
> <quote>
> How to install
>
> install.packages("reshape")
> </quote>

Yes, of course I saw that.


> Try the latter call at the R prompt.

Huh? All the other packages I've installed, and there have been several, 
I've downloaded from CRAN and used:

% R CMD INSTALL packageName.tar.gz


How am I to do that when I don't have the package. Where is going to 
come from?

But now I see that there is a "reshape" package in CRAN.

I figured (yes, assumed) that since no mention of CRAN was made at the 
author's Web page that this release was not distributed there.


> HTH,
>
> --sundar


Randall Schulz



From ripley at stats.ox.ac.uk  Fri Sep 23 16:00:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Sep 2005 15:00:12 +0100 (BST)
Subject: [R] Strange behaviour of as.Date function
In-Reply-To: <31387430.1127480789721.JavaMail.root@pswm17.cp.tin.it>
References: <31387430.1127480789721.JavaMail.root@pswm17.cp.tin.it>
Message-ID: <Pine.LNX.4.61.0509231450140.1319@gannet.stats>

datai[1,1] appears to be a 'POSIXct' object.  Which date that is depends 
on the locale, and as.Date uses UTC (see ?as.Date).  For me:

> d <- as.POSIXct("2005-07-01")
> d
[1] "2005-07-01 BST"
> format(d, tz="GMT")
[1] "2005-06-30 23:00:00"
> as.Date(d)
[1] "2005-06-30"

Use

as.Date(d + 23.99*3600)

to avoid this.


On Fri, 23 Sep 2005, Vittorio wrote:

> Dear All,
> I'm happily extracting data of temperature from an oracle db
> under R via RODBC. After manipulating the extracted data I put them
> into a data.frame 'dati' which is as follows:
>
>> dati
>         DATA tm.
> UDINE/RIVOLTO tm.TORINO/CASELLE
> 1  2005-07-01
> 22.35             23.80
> 2  2005-07-02            22.70
> 22.85
> 3  2005-07-03            23.80             24.30
> 4  2005-07-
> 04            23.80             25.40
> ..........
>
> and
>
>> str(dati)
> `data.frame':	11 obs. of  3 variables:
> $ DATA             :'POSIXct',
> format: chr  "2005-07-01" "2005-07-02" "2005-07-03" "2005-07-04" ...
> $
> tm.UDINE/RIVOLTO : num  22.4 22.7 23.8 23.8 21.8 ...
> $ tm.
> TORINO/CASELLE: num  23.8 22.9 24.3 25.4 21.8 ...
> - attr(*,
> "reshapeWide")=List of 5
>  ..$ v.names: NULL
>  ..$ timevar: chr "NOME"
>  ..$ idvar  : chr "DATA"
>  ..$ times  : Factor w/ 2 levels
> "TORINO/CASELLE",..: 2 1
>  ..$ varying: chr [1, 1:2] "tm.UDINE/RIVOLTO"
> "tm.TORINO/CASELLE"
>>
>
> You see that the first field DATA is POSIXct
>
> Now
>> dati[1,1]
> [1] "2005-07-01 ora solare Europa occidentale"
>
> BUT
>
>>
> as.Date(dati[1,1],"%d%m%Y")
> [1] "2005-06-30"
>
> How come?
> What is wrong
> with it (or better with me)?
>
> Ciao
> Vittorio
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmbates at gmail.com  Fri Sep 23 16:00:26 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 23 Sep 2005 09:00:26 -0500
Subject: [R] Are least-squares means useful or appropriate?
In-Reply-To: <432FCC31.8000407@unileon.es>
References: <432AAB98.4040103@unileon.es> <432F961C.5060404@pdf.com>
	<432FCC31.8000407@unileon.es>
Message-ID: <40e66e0b0509230700522c914c@mail.gmail.com>

On 9/20/05, Felipe <felipe at unileon.es> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi.
> My question was just theoric. I was wondering if someone who were using
> SAS and R could give me their opinion on the topic. I was trying to use
> least-squares means for comparison in R, but then I found some
> indications against them, and I wanted to know if they had good basis
> (as I told earlier, they were not much detailed).
> Greetings.
>
> Felipe

As Deepayan said in his reply, the concept of least squares means is
associated with SAS and is not generally part of the theory of linear
models in statistics.  My vague understanding of these (I too am not a
SAS user) is that they are an attempt to estimate the "mean" response
for a particular level of a factor in a model in which that factor has
a non-ignorable interaction with another factor.  There is no clearly
acceptable definition of such a thing.

To understand why there should be an attempt to answer a question that
doesn't make sense, remember the history of SAS, which was developed
in the era of punched cards and magnetic tape.  Beneath the surface of
SAS with its GUI, etc. is the fundamental assumption that your data
are on a reel of magnetic tape over in the "Computer Center" that
houses an IBM Sytem/360 computer and that the way you are going to use
this program is by keypunching a deck of punched cards, putting some
mysterious JCL (the IBM Job Control Language which no one understood
and you learned only by imitation) cards at the beginning and end, and
submitting them at the I/O Window.  The next day you will go to the
computer center to pick up your output only to discover that you had a
JCL error.  You will spend most of the morning tracking down the one
person on campus who can tell you that "ERROR IEH92345" was caused by
the blank between the "DD" and the "*" in the card that reads //SYSIN
DD * so you change that and submit again.  After two or three days of
this you get the JCL right but discover that you have a syntax error
in your SAS code.  Another two or three cycles finally gets you to the
point where you have a card deck that runs and produces output.  At
that point you don't really care if the output makes sense or not -
all you want is some numbers for the report that is now a week
overdue.  You also want all the numbers that you might possibly need,
which is why SAS PROCs always have the potential to produce tons of
output if you ask for it.

R is an interactive language where it is a simple matter to fit a
series of models and base your analysis on a model that is
appropriate.  An approach of "give me the answer to any possible
question about this model, whether or not it make sense" is
unnecessary.

In many ways statistical theory and practice has not caught up with
statistical computing.  There are concepts that are regarded as part
of established statistical theory when they are, in fact, 
approximations or compromises motivated by the fact that you can't
compute the answer you want - except now you can compute it.  However,
that won't stop people who were trained in the old system from
assuming that things *must* be done in that way.

In short, I agree with Deepayan - the best thing to do is to ask
someone who uses SAS and least squares means to explain to you what
they are.



From andy_liaw at merck.com  Fri Sep 23 16:05:23 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 23 Sep 2005 10:05:23 -0400
Subject: [R] [R-pkgs] Reshape package: new version 0.5
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED48E@usctmx1106.merck.com>

> From: Randall R Schulz
> 
> Sundar,
> 
> On Friday 23 September 2005 06:48, Sundar Dorai-Raj wrote:
> > Randall R Schulz wrote:
> > > Hadley,
> > >
> > > ...
> > >
> > > Perhaps I'm dense, but I cannot find the software at the URL you
> > > mention. There are links to a paper and a PDF slide presentation,
> > > but no R package.
> > >
> > > There is also a dangling link (<http://r-project.org/>) in the
> > > first sentence.
> >
> > Hi, Randall,
> >
> > The first section of the webpage says:
> >
> > <quote>
> > How to install
> >
> > install.packages("reshape")
> > </quote>
> 
> Yes, of course I saw that.
> 
> 
> > Try the latter call at the R prompt.
> 
> Huh? All the other packages I've installed, and there have 
> been several, 
> I've downloaded from CRAN and used:
> 
> % R CMD INSTALL packageName.tar.gz
> 
> 
> How am I to do that when I don't have the package. Where is going to 
> come from?

If you do not know about the R function that is shown to you in response to
your question, wouldn't it help to check out its help page on your own
first?  By default install.packages() does the download _and_ the rest for
you automagically so you don't need to do that in steps outside of R.

 
> But now I see that there is a "reshape" package in CRAN.
> 
> I figured (yes, assumed) that since no mention of CRAN was 
> made at the 
> author's Web page that this release was not distributed there.

There's a short lag between package upload and its showing up on CRAN, and
possibly longer lag for that to show up on various mirror sites.  If and
when I announcement, I'd wait til I see it show up at least on the master
site before posting, or ask the users to check for availability.

Andy

 
> 
> > HTH,
> >
> > --sundar
> 
> 
> Randall Schulz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From keith at okmis.com  Fri Sep 23 16:14:49 2005
From: keith at okmis.com (keith  stephenson)
Date: Fri, 23 Sep 2005 09:14:49 -0500
Subject: [R] no make file
Message-ID: <019401c5c049$28526040$6600a8c0@KEITHSTEPHENSON>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050923/b427e1f5/attachment.pl

From m.booman at path.umcg.nl  Fri Sep 23 16:24:51 2005
From: m.booman at path.umcg.nl (Booman, M)
Date: Fri, 23 Sep 2005 16:24:51 +0200
Subject: [R] Installing local packages in R for MacOSX 10.4.2
Message-ID: <7315C7E20C5FD41198D700508BAD4C080C3F7410@ntazgas01.azg.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050923/d68544be/attachment.pl

From ligges at statistik.uni-dortmund.de  Fri Sep 23 16:32:34 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 23 Sep 2005 16:32:34 +0200
Subject: [R] no make file
In-Reply-To: <019401c5c049$28526040$6600a8c0@KEITHSTEPHENSON>
References: <019401c5c049$28526040$6600a8c0@KEITHSTEPHENSON>
Message-ID: <43341202.30608@statistik.uni-dortmund.de>

keith stephenson wrote:

>  
> I have downloaded from cran.r-progject.org/src/base/R-2 and unzipped R-2.1.1 into the directory /keith/r  
> and run ./configure on my linux CentOS 3.5 platform and when I run make I get:
> 
> make: *** No targets specified and no makefile found.  Stop.
> 
> any ideas on where the makefile is or why it's not there?


Probably because of errors during ./configure ... and we do not know 
which errors came up ...

Uwe Ligges



> Keith
> .
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wegmann at biozentrum.uni-wuerzburg.de  Fri Sep 23 16:33:36 2005
From: wegmann at biozentrum.uni-wuerzburg.de (Martin Wegmann)
Date: Fri, 23 Sep 2005 16:33:36 +0200
Subject: [R] CART for 0/1 data
Message-ID: <200509231633.39512.wegmann@biozentrum.uni-wuerzburg.de>

Dear R-user, 

I tried to generate classification / regression tree with a absence/presence 
matrix of species (400) in different locations (50) to visualise species 
which are important for splitting up two locations. 
Rpart and tree did not work for more than 10 species which is logical due to 
the limited amount of locations (n=50). However the error prompt is a "+" and 
no specific message, but I am pretty sure that I did not enter a false sign 
by mistake. 
Is it allowed at all to use 0/1 data for this statistical technique and if yes 
is there a way or different method to use all 400 species entries?
Otherwise I would apply a PCA beforehand but I would prefer to have the raw 
species informations. 

using R 2.1.1-1 (debian repos.)

regards, Martin


-- 
Martin Wegmann

DLR - German Aerospace Center
German Remote Sensing Data Center
@
Dept.of Geography
Remote Sensing and Biodiversity Unit
&&
Dept. of Animal Ecology and Tropical Biology
University of Wuerzburg
Am Hubland
97074 W??rzburg

phone: +49-(0)931 - 888 4797
mobile: +49-(0)175 2091725
fax:   +49-(0)931 - 888 4961
http://www.biota-africa.org
http://www.biogis.de



From goedman at mac.com  Fri Sep 23 16:53:29 2005
From: goedman at mac.com (Rob J Goedman)
Date: Fri, 23 Sep 2005 07:53:29 -0700
Subject: [R] Installing local packages in R for MacOSX 10.4.2
In-Reply-To: <7315C7E20C5FD41198D700508BAD4C080C3F7410@ntazgas01.azg.nl>
References: <7315C7E20C5FD41198D700508BAD4C080C3F7410@ntazgas01.azg.nl>
Message-ID: <1CF161EF-14D2-4C27-AED9-5A5EAC4D0564@mac.com>

Hi Marije,

When you select the Package Installer, select Local Source Package and
press Install (which is now enabled). This will get you to a Finder  
window
where you can select the source file to install.

Hope this helps,
Rob

PS There is a R-SIG-Mac alias for questions specific to R on Mac OS.


On Sep 23, 2005, at 7:24 AM, Booman, M wrote:

> Dear all,
>
> I have been using R for Windows before but recently switched to  
> Mac. Now I
> am confused about how to install local packages using R 2.1.1. on  
> the Mac.
> I found the menu Packages and Data, and have used it succesfully to  
> install
> CRAN and Bioconductor packages (all binaries) by selecting these
> repositories from the pull-down menu and clicking the Get List button.
> However there is one package from OmegaHat I want to install  
> (Rcurl). Since
> this is not a listed repository in the Install Packages pull-down  
> menu, I
> tried the Other Repository function and entered the URL for the  
> OmegaHat
> page. This doesn't work as R tries to add /PACKAGES to the URL and  
> this is
> apparently not how the OmegaHat site is set up (I tried the same with
> bioconductor's URL and that did work). SO I downloaded the .tar.gz  
> file. I
> remember from my R for Windows there was an option Install Package  
> From
> Local File. But in this version of R for MAC I can't find this. In  
> the R FAQ
> for MAC OSX it says I should choose, in the Install Packages menu, the
> option Local Package from the pull-down menu. But then the Get List  
> button
> is greyed out, and no packages appear in the list. I have tried  
> putting the
> .tar.gz file in several places: in the working directory (/user/ 
> admin in
> this case), in the R-Framework/Resources/Library directory, but  
> never is the
> file listed in the Install Packages menu. I must be doing something  
> wrong
> but don't know what. This is the text from the R FAQ for OSX, which I
> followed:
> "You can also download any other package from the Internet yourself  
> and
> decide to install it from source. In such case select one of the local
> entries in the top left list." (The top left list being the pull- 
> down menu I
> described and the local entries being Local Binary package, Local  
> Source
> Package, and Local Package Directory).
>
>
> Could somebody please help me out?
>
> Best wishes,
> Marije
>
>
> De inhoud van dit bericht is vertrouwelijk en alleen bestemd voor  
> de geadresseerde(n). Anderen dan de geadresseerde mogen geen  
> gebruik maken van dit bericht, het openbaar maken of op enige wijze  
> verspreiden of vermenigvuldigen. Het UMCG kan niet aansprakelijk  
> gesteld worden voor een incomplete aankomst of vertraging van dit  
> verzonden bericht.
>
> The contents of this message are confidential and only intended for  
> the eyes of the addressee(s). Others than the addressee(s) are not  
> allowed to use this message, to make it public or to distribute or  
> multiply this message in any way. The UMCG cannot be held  
> responsible for incomplete reception or delay of this transferred  
> message.
>
>     [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From droberts at montana.edu  Fri Sep 23 17:08:38 2005
From: droberts at montana.edu (Dave Roberts)
Date: Fri, 23 Sep 2005 09:08:38 -0600
Subject: [R] CART for 0/1 data
In-Reply-To: <200509231633.39512.wegmann@biozentrum.uni-wuerzburg.de>
References: <200509231633.39512.wegmann@biozentrum.uni-wuerzburg.de>
Message-ID: <43341A76.3050807@montana.edu>

Martin,

     If the data are actually coded 0/1, the tree function would 
probably intepret them as integers and try a regression instead of a 
classification.  If the dependent variable is called "var", try

x <- tree(factor(var)~species)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460


Martin Wegmann wrote:
> Dear R-user, 
> 
> I tried to generate classification / regression tree with a absence/presence 
> matrix of species (400) in different locations (50) to visualise species 
> which are important for splitting up two locations. 
> Rpart and tree did not work for more than 10 species which is logical due to 
> the limited amount of locations (n=50). However the error prompt is a "+" and 
> no specific message, but I am pretty sure that I did not enter a false sign 
> by mistake. 
> Is it allowed at all to use 0/1 data for this statistical technique and if yes 
> is there a way or different method to use all 400 species entries?
> Otherwise I would apply a PCA beforehand but I would prefer to have the raw 
> species informations. 
> 
> using R 2.1.1-1 (debian repos.)
> 
> regards, Martin
> 
>



From p.dalgaard at biostat.ku.dk  Fri Sep 23 17:22:45 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Sep 2005 17:22:45 +0200
Subject: [R] Are least-squares means useful or appropriate?
In-Reply-To: <40e66e0b0509230700522c914c@mail.gmail.com>
References: <432AAB98.4040103@unileon.es> <432F961C.5060404@pdf.com>
	<432FCC31.8000407@unileon.es>
	<40e66e0b0509230700522c914c@mail.gmail.com>
Message-ID: <x2r7bf3j22.fsf@turmalin.kubism.ku.dk>

Douglas Bates <dmbates at gmail.com> writes:

> On 9/20/05, Felipe <felipe at unileon.es> wrote:
> > -----BEGIN PGP SIGNED MESSAGE-----
> > Hash: SHA1
> >
> > Hi.
> > My question was just theoric. I was wondering if someone who were using
> > SAS and R could give me their opinion on the topic. I was trying to use
> > least-squares means for comparison in R, but then I found some
> > indications against them, and I wanted to know if they had good basis
> > (as I told earlier, they were not much detailed).
> > Greetings.
> >
> > Felipe
> 
> As Deepayan said in his reply, the concept of least squares means is
> associated with SAS and is not generally part of the theory of linear
> models in statistics.  My vague understanding of these (I too am not a
> SAS user) is that they are an attempt to estimate the "mean" response
> for a particular level of a factor in a model in which that factor has
> a non-ignorable interaction with another factor.  There is no clearly
> acceptable definition of such a thing.

(PD goes and fetches the SAS manual....)

Well, yes. it'll do that too, although only if you ask for the lsmeans
of A when an interaction like A*B is present in the model. This is
related to the tests of main effects when an interaction is present
using type III sums of squares, which has been beaten to death
repeatedly on the list. In both cases, there seems to be an implicit
assumption that categorical variables by nature comes from an
underlying fully balanced design.

If the interaction is absent from the model, the lsmeans are somewhat
more sensible in that they at least reproduce the parameter estimates
as contrasts between different groups. All continuous variables in the
design will be set to their mean, but values for categorical design
variables are weighted inversely as the number of groups. So if you're
doing an lsmeans of lung function by smoking adjusted for age and sex
you get estimates for the mean of a population of which everyone has
the same age and half are male and half are female. This makes some
sense, but if you do it for sex adjusting for smoking and age, you are
not only forcing the sexes to smoke equally much, but actually
adjusting to  smoking rates of 50%, which could be quite far from
reality. 

The whole operation really seems to revolve around 2 things: 

(1) pairwise comparisons between factor levels. This can alternatively
    be done fairly easily using parameter estimates for the relevant
    variable and associated covariances. You don't really need all the
    mumbo-jumbo of adjusting to particular values of other variables.

(2) plotting effects of a factor with error bars as if they were
    simple group means. This has some merit since the standard
    parametrizations are misleading at times (e.g. if you choose the
    group with the least data as the reference level, std. err. for
    the other groups will seem high). However, it seems to me that
    concepts like floating variances (see float() in the Epi package)
    are more to the point.

> R is an interactive language where it is a simple matter to fit a
> series of models and base your analysis on a model that is
> appropriate.  An approach of "give me the answer to any possible
> question about this model, whether or not it make sense" is
> unnecessary.
> 
> In many ways statistical theory and practice has not caught up with
> statistical computing.  There are concepts that are regarded as part
> of established statistical theory when they are, in fact, 
> approximations or compromises motivated by the fact that you can't
> compute the answer you want - except now you can compute it.  However,
> that won't stop people who were trained in the old system from
> assuming that things *must* be done in that way.
> 
> In short, I agree with Deepayan - the best thing to do is to ask
> someone who uses SAS and least squares means to explain to you what
> they are.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From wegmann at biozentrum.uni-wuerzburg.de  Fri Sep 23 18:02:31 2005
From: wegmann at biozentrum.uni-wuerzburg.de (Martin Wegmann)
Date: Fri, 23 Sep 2005 18:02:31 +0200
Subject: [R] CART for 0/1 data
In-Reply-To: <43341A76.3050807@montana.edu>
References: <200509231633.39512.wegmann@biozentrum.uni-wuerzburg.de>
	<43341A76.3050807@montana.edu>
Message-ID: <200509231802.34345.wegmann@biozentrum.uni-wuerzburg.de>

On Friday 23 September 2005 17:08, Dave Roberts wrote:
> Martin,
>
>      If the data are actually coded 0/1, the tree function would
> probably intepret them as integers and try a regression instead of a
> classification.  If the dependent variable is called "var", try

thanks, but I think I provided too less informations. 
My dependent variable are the locations which are names (I could transform 
them to numbers from 1 - n). The independent variables consist of 0/1 data 
(species). 
If I do 
tree(locations~factor(species1)+factor(species2)+.....+factor(speciesn), 
sp_data) 
I receive the same results as without the factor() part. 
BTW just a subset of the locations are displayed what is pretty weird 
considering that I included all locations in the analysis.

Martin 


> x <- tree(factor(var)~species)
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> David W. Roberts                                     office 406-994-4548
> Professor and Head                                      FAX 406-994-3190
> Department of Ecology                         email droberts at montana.edu
> Montana State University
> Bozeman, MT 59717-3460
>
> Martin Wegmann wrote:
> > Dear R-user,
> >
> > I tried to generate classification / regression tree with a
> > absence/presence matrix of species (400) in different locations (50) to
> > visualise species which are important for splitting up two locations.
> > Rpart and tree did not work for more than 10 species which is logical due
> > to the limited amount of locations (n=50). However the error prompt is a
> > "+" and no specific message, but I am pretty sure that I did not enter a
> > false sign by mistake.
> > Is it allowed at all to use 0/1 data for this statistical technique and
> > if yes is there a way or different method to use all 400 species entries?
> > Otherwise I would apply a PCA beforehand but I would prefer to have the
> > raw species informations.
> >
> > using R 2.1.1-1 (debian repos.)
> >
> > regards, Martin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Martin Wegmann

DLR - German Aerospace Center
German Remote Sensing Data Center
@
Dept.of Geography
Remote Sensing and Biodiversity Unit
&&
Dept. of Animal Ecology and Tropical Biology
University of Wuerzburg
Am Hubland
97074 W??rzburg

phone: +49-(0)931 - 888 4797
mobile: +49-(0)175 2091725
fax:   +49-(0)931 - 888 4961
http://www.biota-africa.org
http://www.biogis.de



From lyris-noreply at gselist.org  Fri Sep 23 18:35:50 2005
From: lyris-noreply at gselist.org (Lyris ListManager)
Date: Fri, 23 Sep 2005 12:35:50 -0400
Subject: [R] your subscribe request
Message-ID: <LYRIS-0-125852-2005.09.23-12.35.50--lyris-noreply@gselist.org>

Re: your subscribe request
> subscribe

Sorry, this email address is not allowed to subscribe to 'rachel'.



From berr0179 at umn.edu  Fri Sep 23 19:00:08 2005
From: berr0179 at umn.edu (Erin Berryman)
Date: Fri, 23 Sep 2005 12:00:08 -0500
Subject: [R] panel.linejoin groups
Message-ID: <c7e725fbe8d8333202ec91062fe9b714@umn.edu>

Dear R community,

I am still new to R, but I am attempting to use it for (hopefully) all 
my plotting needs. I have been using lattice and Hmisc for most plots 
so far successfully, but I need help creating a plot I desire for some 
new data I have.
This data frame consists of the same type of measurement (Eh) for 27 
different locations. I have 8000+ measurements for each location, and 
my datalogger organizes the output like this:

Date					A1L			A2L			A3L	
2005-07-14 22:00		208.1		-178.5		196.8
2005-07-14 22:10		207.9		-184.3		200.0

Now I'm having trouble plotting A1L, A2L, A3L on the same plot as a 
function of Date. I thought I would try panel.linejoin, like so:

xyplot(data=redox2, A1L + A2L + A3L ~ Date, 
panel=function(x,y,...){panel.linejoin(x,y,horizontal=F,col=1,...)}, 
scales=list(x=list(tick.number=10)))	

But of course, what I get is the mean of the measurements from those 
three columns plotted as one line, when what I want is a separate line 
for each column of data. Do I need to define "groups" somehow to get 
separate lines? If so, how do I define groups based on membership in 
different columns? Or do I just need to reorganize my data to make it 
work, like this:

Date				Location		Eh
2005-07-14 22:00	A1L			208.1	
2005-07-14 22:00	A2L			-178.5

Then I can define groups=Location, and it would work.  But it would be 
good to know if I can make the data work as is - because I have many 
many more datasets that are structured this way.

Thank you,

Erin



Erin M. Berryman
Graduate Research Assistant
Department of Soil, Water, and Climate
University of Minnesota
439 Borlaug Hall
1991 Upper Buford Circle
St. Paul, MN 55104
612.625.9747
berr0179 at umn.edu



From droberts at montana.edu  Fri Sep 23 19:19:42 2005
From: droberts at montana.edu (Dave Roberts)
Date: Fri, 23 Sep 2005 11:19:42 -0600
Subject: [R] CART for 0/1 data
In-Reply-To: <200509231802.34345.wegmann@biozentrum.uni-wuerzburg.de>
References: <200509231633.39512.wegmann@biozentrum.uni-wuerzburg.de>	<43341A76.3050807@montana.edu>
	<200509231802.34345.wegmann@biozentrum.uni-wuerzburg.de>
Message-ID: <4334392E.8020001@montana.edu>

Martin,

     Sorry, I don't think I read your message carefully enough.

     When you say the error message is "+", that woudl seem to indicate 
that you still had an unclosed parenthesis and that the function was 
looking for more input.

     Using a smaller data set (160 samples, 169 rows, only 5 classes) it 
did work fine for me.  pa = presence/absence dataframe, opt.5$clustering 
= cluster IDs.

*********************************************************************

 > test <- tree(factor(opt.5$clustering)~pa)
 > test
node), split, n, deviance, yval, (yprob)
       * denotes terminal node

  1) root 160 371.000 3 ( 0.23750 0.08750 0.57500 0.07500 0.02500 )
    2) pa.symore < 0.5 79 216.500 1 ( 0.48101 0.17722 0.15190 0.13924 
0.05063 )
      4) pa.artarb < 0.5 42 123.600 2 ( 0.07143 0.33333 0.26190 0.23810 
0.09524 )
        8) pa.macgri < 0.5 31  75.280 2 ( 0.09677 0.45161 0.00000 
0.32258 0.12903 )
    .        .         .
    .        .         .
    .        .         .
    3) pa.symore > 0.5 81  10.780 3 ( 0.00000 0.00000 0.98765 0.01235 
0.00000 )
      6) pa.carrss < 0.5 11   6.702 3 ( 0.00000 0.00000 0.90909 0.09091 
0.00000 ) *
      7) pa.carrss > 0.5 70   0.000 3 ( 0.00000 0.00000 1.00000 0.00000 
0.00000 ) *

************************************************************************

I'll try agin with a larger dataset and see if it's a memory limitation.

Dave Roberts

Martin Wegmann wrote:
> On Friday 23 September 2005 17:08, Dave Roberts wrote:
> 
>>Martin,
>>
>>     If the data are actually coded 0/1, the tree function would
>>probably intepret them as integers and try a regression instead of a
>>classification.  If the dependent variable is called "var", try
> 
> 
> thanks, but I think I provided too less informations. 
> My dependent variable are the locations which are names (I could transform 
> them to numbers from 1 - n). The independent variables consist of 0/1 data 
> (species). 
> If I do 
> tree(locations~factor(species1)+factor(species2)+.....+factor(speciesn), 
> sp_data) 
> I receive the same results as without the factor() part. 
> BTW just a subset of the locations are displayed what is pretty weird 
> considering that I included all locations in the analysis.
> 
> Martin 
> 
> 
> 
>>x <- tree(factor(var)~species)
>>
>>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>David W. Roberts                                     office 406-994-4548
>>Professor and Head                                      FAX 406-994-3190
>>Department of Ecology                         email droberts at montana.edu
>>Montana State University
>>Bozeman, MT 59717-3460
>>
>>Martin Wegmann wrote:
>>
>>>Dear R-user,
>>>
>>>I tried to generate classification / regression tree with a
>>>absence/presence matrix of species (400) in different locations (50) to
>>>visualise species which are important for splitting up two locations.
>>>Rpart and tree did not work for more than 10 species which is logical due
>>>to the limited amount of locations (n=50). However the error prompt is a
>>>"+" and no specific message, but I am pretty sure that I did not enter a
>>>false sign by mistake.
>>>Is it allowed at all to use 0/1 data for this statistical technique and
>>>if yes is there a way or different method to use all 400 species entries?
>>>Otherwise I would apply a PCA beforehand but I would prefer to have the
>>>raw species informations.
>>>
>>>using R 2.1.1-1 (debian repos.)
>>>
>>>regards, Martin
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
> 
>



From droberts at montana.edu  Fri Sep 23 19:25:28 2005
From: droberts at montana.edu (Dave Roberts)
Date: Fri, 23 Sep 2005 11:25:28 -0600
Subject: [R] CART for 0/1 data
In-Reply-To: <200509231802.34345.wegmann@biozentrum.uni-wuerzburg.de>
References: <200509231633.39512.wegmann@biozentrum.uni-wuerzburg.de>	<43341A76.3050807@montana.edu>
	<200509231802.34345.wegmann@biozentrum.uni-wuerzburg.de>
Message-ID: <43343A88.9040900@montana.edu>

Martin,

     I should have tried before the last post to save postings, but on 
my machine I tried samples = 1224, species = 962, clusters = 10 with no 
problems at all.

 > summary(test)

Classification tree:
tree(formula = factor(opt.10$clustering) ~ pa)
Variables actually used in tree construction:
  [1] "pa.PICENG" "pa.ARTTSV" "pa.PSEMEN" "pa.AGRSPI" "pa.DESCES" 
"pa.ABILAS"
  [7] "pa.FESIDA" "pa.POLBIS" "pa.CAREXX" "pa.PINCON" "pa.GEUMAC"
Number of terminal nodes:  16
Residual mean deviance:  1.551 = 1873 / 1208
Misclassification error rate: 0.2435 = 298 / 1224

You may want to reclassify to fewer than 50 locations, but I think it 
should work.

Good luck, Dave Roberts

Martin Wegmann wrote:
> On Friday 23 September 2005 17:08, Dave Roberts wrote:
> 
>>Martin,
>>
>>     If the data are actually coded 0/1, the tree function would
>>probably intepret them as integers and try a regression instead of a
>>classification.  If the dependent variable is called "var", try
> 
> 
> thanks, but I think I provided too less informations. 
> My dependent variable are the locations which are names (I could transform 
> them to numbers from 1 - n). The independent variables consist of 0/1 data 
> (species). 
> If I do 
> tree(locations~factor(species1)+factor(species2)+.....+factor(speciesn), 
> sp_data) 
> I receive the same results as without the factor() part. 
> BTW just a subset of the locations are displayed what is pretty weird 
> considering that I included all locations in the analysis.
> 
> Martin 
> 
> 
> 
>>x <- tree(factor(var)~species)
>>
>>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>David W. Roberts                                     office 406-994-4548
>>Professor and Head                                      FAX 406-994-3190
>>Department of Ecology                         email droberts at montana.edu
>>Montana State University
>>Bozeman, MT 59717-3460
>>
>>Martin Wegmann wrote:
>>
>>>Dear R-user,
>>>
>>>I tried to generate classification / regression tree with a
>>>absence/presence matrix of species (400) in different locations (50) to
>>>visualise species which are important for splitting up two locations.
>>>Rpart and tree did not work for more than 10 species which is logical due
>>>to the limited amount of locations (n=50). However the error prompt is a
>>>"+" and no specific message, but I am pretty sure that I did not enter a
>>>false sign by mistake.
>>>Is it allowed at all to use 0/1 data for this statistical technique and
>>>if yes is there a way or different method to use all 400 species entries?
>>>Otherwise I would apply a PCA beforehand but I would prefer to have the
>>>raw species informations.
>>>
>>>using R 2.1.1-1 (debian repos.)
>>>
>>>regards, Martin
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
> 
> 


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From tplate at acm.org  Fri Sep 23 19:38:28 2005
From: tplate at acm.org (Tony Plate)
Date: Fri, 23 Sep 2005 11:38:28 -0600
Subject: [R] books about MCMC to use MCMC R packages?
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC032191D1@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC032191D1@ibfftce502.de.ad.drkw.net>
Message-ID: <43343D94.4050906@acm.org>

I've found "Bayesian Data Analysis" by Gelman, Carlin, Stern & Rubin 
(2nd ed) to be quite useful for understanding how MCMC can be used for 
Bayesian models.  It has a little bit of R code in it too.

-- Tony Plate

Molins, Jordi wrote:
> Dear list users,
> 
> I need to learn about MCMC methods, and since there are several packages in
> R that deal with this subject, I want to use them. 
> 
> I want to buy a book (or more than one, if necessary) that satisfies the
> following requirements:
> 
> - it teaches well MCMC methods;
> 
> - it is easy to implement numerically the ideas of the book, and notation
> and concepts are similar to the corresponding R packages that deal with MCMC
> methods.
> 
> I have done a search and 2 books seem to satisfy my requirements:
> 
> - Markov Chain Monte Carlo In Practice, by W.R. Gilks and others.
> 
> - Monte Carlo Statistical methods, Robert and Casella.
> 
> What do people think about these books? Is there a suggestion of some other
> book that could satisfy better my requirements?
> 
> Thank you very much in advance.
> 
> 
> 
> 
> --------------------------------------------------------------------------------
> The information contained herein is confidential and is inte...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From deepayan.sarkar at gmail.com  Fri Sep 23 20:01:07 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 23 Sep 2005 13:01:07 -0500
Subject: [R] panel.linejoin groups
In-Reply-To: <c7e725fbe8d8333202ec91062fe9b714@umn.edu>
References: <c7e725fbe8d8333202ec91062fe9b714@umn.edu>
Message-ID: <eb555e66050923110162d954bb@mail.gmail.com>

On 9/23/05, Erin Berryman <berr0179 at umn.edu> wrote:
> Dear R community,
>
> I am still new to R, but I am attempting to use it for (hopefully) all
> my plotting needs. I have been using lattice and Hmisc for most plots
> so far successfully, but I need help creating a plot I desire for some
> new data I have.
> This data frame consists of the same type of measurement (Eh) for 27
> different locations. I have 8000+ measurements for each location, and
> my datalogger organizes the output like this:
>
> Date                                    A1L                     A2L                     A3L
> 2005-07-14 22:00                208.1           -178.5          196.8
> 2005-07-14 22:10                207.9           -184.3          200.0
>
> Now I'm having trouble plotting A1L, A2L, A3L on the same plot as a
> function of Date. I thought I would try panel.linejoin, like so:
>
> xyplot(data=redox2, A1L + A2L + A3L ~ Date,
> panel=function(x,y,...){panel.linejoin(x,y,horizontal=F,col=1,...)},
> scales=list(x=list(tick.number=10)))
>
> But of course, what I get is the mean of the measurements from those
> three columns plotted as one line, when what I want is a separate line
> for each column of data. Do I need to define "groups" somehow to get
> separate lines? If so, how do I define groups based on membership in
> different columns? Or do I just need to reorganize my data to make it
> work, like this:
>
> Date                            Location                Eh
> 2005-07-14 22:00        A1L                     208.1
> 2005-07-14 22:00        A2L                     -178.5
>
> Then I can define groups=Location, and it would work.  But it would be
> good to know if I can make the data work as is - because I have many
> many more datasets that are structured this way.

Yes, simply supply 'panel.groups' instead of 'panel'

panel.groups = function(x,y,...) { panel.linejoin(x,y,horizontal=F,col=1,...) },

panel.linejoin doesn't know anything about groups, and has to be used
through panel.superpose (which does). However, the more obvious
approach seems to be

xyplot(A1L + A2L + A3L ~ Date, data=redox2,
       type = 'l',
       scales=list(x=list(tick.number=10)))

Does that not work? panel.linejoin is meant for cases where several
observations need to be summarized by a single number (mean, median,
etc).

BTW, it's not a good idea to have anything other than the formula as
the first argument to xyplot etc.

Deepayan



From berr0179 at umn.edu  Fri Sep 23 20:26:28 2005
From: berr0179 at umn.edu (Erin Berryman)
Date: Fri, 23 Sep 2005 13:26:28 -0500
Subject: [R] panel.linejoin groups
In-Reply-To: <eb555e66050923110162d954bb@mail.gmail.com>
References: <c7e725fbe8d8333202ec91062fe9b714@umn.edu>
	<eb555e66050923110162d954bb@mail.gmail.com>
Message-ID: <42f55017d4e4212722ac3ae4ac219e64@umn.edu>

Both methods you listed do work, but type='l' is what I want, thank you 
for pointing that out to me. Incidentally, xyplot(type='l') printed 
much faster on the display than the panel.groups=() method. I was 
captivated by linejoin before I ever had a case to use 
xyplot(type='l'), which would explain why I tried to use it for this 
type of plot!


On Sep 23, 2005, at 1:01 PM, Deepayan Sarkar wrote:

> On 9/23/05, Erin Berryman <berr0179 at umn.edu> wrote:
>> Dear R community,
>>
>> I am still new to R, but I am attempting to use it for (hopefully) all
>> my plotting needs. I have been using lattice and Hmisc for most plots
>> so far successfully, but I need help creating a plot I desire for some
>> new data I have.
>> This data frame consists of the same type of measurement (Eh) for 27
>> different locations. I have 8000+ measurements for each location, and
>> my datalogger organizes the output like this:
>>
>> Date                                    A1L                     A2L   
>>                   A3L
>> 2005-07-14 22:00                208.1           -178.5          196.8
>> 2005-07-14 22:10                207.9           -184.3          200.0
>>
>> Now I'm having trouble plotting A1L, A2L, A3L on the same plot as a
>> function of Date. I thought I would try panel.linejoin, like so:
>>
>> xyplot(data=redox2, A1L + A2L + A3L ~ Date,
>> panel=function(x,y,...){panel.linejoin(x,y,horizontal=F,col=1,...)},
>> scales=list(x=list(tick.number=10)))
>>
>> But of course, what I get is the mean of the measurements from those
>> three columns plotted as one line, when what I want is a separate line
>> for each column of data. Do I need to define "groups" somehow to get
>> separate lines? If so, how do I define groups based on membership in
>> different columns? Or do I just need to reorganize my data to make it
>> work, like this:
>>
>> Date                            Location                Eh
>> 2005-07-14 22:00        A1L                     208.1
>> 2005-07-14 22:00        A2L                     -178.5
>>
>> Then I can define groups=Location, and it would work.  But it would be
>> good to know if I can make the data work as is - because I have many
>> many more datasets that are structured this way.
>
> Yes, simply supply 'panel.groups' instead of 'panel'
>
> panel.groups = function(x,y,...) { 
> panel.linejoin(x,y,horizontal=F,col=1,...) },
>
> panel.linejoin doesn't know anything about groups, and has to be used
> through panel.superpose (which does). However, the more obvious
> approach seems to be
>
> xyplot(A1L + A2L + A3L ~ Date, data=redox2,
>        type = 'l',
>        scales=list(x=list(tick.number=10)))
>
> Does that not work? panel.linejoin is meant for cases where several
> observations need to be summarized by a single number (mean, median,
> etc).
>
> BTW, it's not a good idea to have anything other than the formula as
> the first argument to xyplot etc.
>
> Deepayan
>
Erin M. Berryman
Graduate Research Assistant
Department of Soil, Water, and Climate
University of Minnesota
612.625.9747



From deepayan.sarkar at gmail.com  Fri Sep 23 20:39:28 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 23 Sep 2005 13:39:28 -0500
Subject: [R] panel.linejoin groups
In-Reply-To: <42f55017d4e4212722ac3ae4ac219e64@umn.edu>
References: <c7e725fbe8d8333202ec91062fe9b714@umn.edu>
	<eb555e66050923110162d954bb@mail.gmail.com>
	<42f55017d4e4212722ac3ae4ac219e64@umn.edu>
Message-ID: <eb555e6605092311394ca22295@mail.gmail.com>

On 9/23/05, Erin Berryman <berr0179 at umn.edu> wrote:
> Both methods you listed do work, but type='l' is what I want, thank you
> for pointing that out to me. Incidentally, xyplot(type='l') printed
> much faster on the display than the panel.groups=() method.

Not surprising if you have many values of 'Date', since panel.linejoin
will try to compute the mean (of a length-1 vector) for each
combination of Date and groups.

Deepayan



From mwittmann at bren.ucsb.edu  Fri Sep 23 21:06:28 2005
From: mwittmann at bren.ucsb.edu (Marion Wittmann)
Date: Fri, 23 Sep 2005 12:06:28 -0700
Subject: [R] Alternative to nlm()
Message-ID: <200509231906.j8NJ6T909263@icess.ucsb.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050923/45c1f0ee/attachment.pl

From tlumley at u.washington.edu  Sat Sep 24 01:00:49 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 23 Sep 2005 16:00:49 -0700 (PDT)
Subject: [R] Problem with read.spss() and as.data.frame(),
 or: alternative to subset()?
In-Reply-To: <17201.23107.106266.248116@stat.math.ethz.ch>
References: <433082A0.6010302@jura.uni-hamburg.de>
	<43314188.1050209@jura.uni-hamburg.de>
	<17201.23107.106266.248116@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.63a.0509231557250.21102@homer23.u.washington.edu>

On Wed, 21 Sep 2005, Martin Maechler wrote:

>>>>>> "Dirk" == Dirk Enzmann <dirk.enzmann at jura.uni-hamburg.de>
>>>>>>     on Wed, 21 Sep 2005 13:18:32 +0200 writes:
>
>    Dirk> The selection problem can be solved by
>    Dirk> dr2000=read.spss('myfile')
>    Dirk> d=lapply(dr2000,subset,dr2000$RBINZ99 > 0)
>
>    Dirk> however, there is still the problem that R crashes when using
>
>    Dirk> d = as.data.frame(dr2000)
>
> which is bug in a R, or at least in your R installation.
>
> However we can't do anything about it at the moment, because we
> can't even try to do reproduce it...

I suspect this is the same stack overflow in coerce.c:substituteList that 
was reported in PR#8141

 	-thomas






>
> So dr2000 is a list; what length() does it have?, what names() ?
> what does str(dr2000) look like?
>
> What does happen for  as.data.frame(dr2000[1:10]) ?
> and '100' or '1000' instead of '10'?
>
> Maybe try to find a small version of 'dr2000' which still has
> the problem, and show us that one,
> e.g. by making it available via http://... if it is still large,
> otherwise (if it's small), maybe even posting the result of
> dump(..).
>
> Regards,
> Martin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From duncan at wald.ucdavis.edu  Sat Sep 24 04:09:14 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Fri, 23 Sep 2005 19:09:14 -0700
Subject: [R] SAX Parser best practise
In-Reply-To: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E77A@EMAIL.mpimp-golm.mpg.de>
References: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E77A@EMAIL.mpimp-golm.mpg.de>
Message-ID: <4334B54A.6010701@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



Jan Hummel wrote:
> Thank you Seth  and Duncan for your input! 
> 
> 
>>BTW, do you have a schema for the XML document you are working on?
> 
> 
> Yes, a schema is available here
> http://psidev.sourceforge.net/ms/xml/mzdata/mzdata.xsd
> Informations around mzData xml format are available here
> http://psidev.sourceforge.net/ms/#mzdata
> 

Thanks.


> Next question I want to come up with: is there a way to validate xml
> again a schema or a dtd while parsing using xmlEventParse()?
> 

I dug around in the libxml code and the Web to verify that validation
is indeed only possible in libxml when one uses DOM (i.e. xmlTreeParse()).
Do you really need to validate the input? Given the size of the source,
it must be created automatically and so I tend to think it is either
correct or not, but that errors will be found with the creation
mechanism.


BTW, there is a new version of the XML package on the Omegahat web site.
It has several new features, including a function to find nodes via
XPath expressions, SAX2 support, recursive support for
xmlElementsByTagName().


> cheers
> 	Jan

- --
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (Darwin)
Comment: Using GnuPG with Thunderbird - http://enigmail.mozdev.org

iD8DBQFDNLVK9p/Jzwa2QP4RAqNvAJ97+XW1B1AO6zl8ZN2qtVHCcPuu4ACfXnR9
572gL8pD2eMHj/tUSRomZwQ=
=SXBQ
-----END PGP SIGNATURE-----



From kthomps7 at gmu.edu  Sat Sep 24 04:11:44 2005
From: kthomps7 at gmu.edu (kthomps7@gmu.edu)
Date: Fri, 23 Sep 2005 22:11:44 -0400
Subject: [R] Cluster and Tree Conversion
Message-ID: <f776e65813ddc.43347da0@gmu.edu>

hi, 
  i'm looking for some help w/ Antoine Lucas's package for Cluster and Tree Conversion (from r to Xcluster).  the r2dct functions includes 2 parameters not discussed in the .pdf.  if i've deciphered them correctly the hr is the labels and the hc is the Hclust object.  

  i can properly run the function with larger data matrix and only receive a warning msg:
Warning message:
number of items to replace is not a multiple of replacement length 

However smaller matrices fail:
Error in "[.data.frame"(data[, 5:m], hc$order) : 
        undefined columns selected

  as anyone had to work around this?

thanks,
kt



From ripley at stats.ox.ac.uk  Sat Sep 24 10:06:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 24 Sep 2005 09:06:36 +0100 (BST)
Subject: [R] Problem with read.spss() and as.data.frame(),
 or: alternative to subset()?
In-Reply-To: <Pine.LNX.4.63a.0509231557250.21102@homer23.u.washington.edu>
References: <433082A0.6010302@jura.uni-hamburg.de>
	<43314188.1050209@jura.uni-hamburg.de>
	<17201.23107.106266.248116@stat.math.ethz.ch>
	<Pine.LNX.4.63a.0509231557250.21102@homer23.u.washington.edu>
Message-ID: <Pine.LNX.4.61.0509240859290.22088@gannet.stats>

On Fri, 23 Sep 2005, Thomas Lumley wrote:

> On Wed, 21 Sep 2005, Martin Maechler wrote:
>
>>>>>>> "Dirk" == Dirk Enzmann <dirk.enzmann at jura.uni-hamburg.de>
>>>>>>>     on Wed, 21 Sep 2005 13:18:32 +0200 writes:
>>
>>    Dirk> The selection problem can be solved by
>>    Dirk> dr2000=read.spss('myfile')
>>    Dirk> d=lapply(dr2000,subset,dr2000$RBINZ99 > 0)
>>
>>    Dirk> however, there is still the problem that R crashes when using
>>
>>    Dirk> d = as.data.frame(dr2000)
>>
>> which is bug in a R, or at least in your R installation.
>>
>> However we can't do anything about it at the moment, because we
>> can't even try to do reproduce it...
>
> I suspect this is the same stack overflow in coerce.c:substituteList that
> was reported in PR#8141

Apparently not (it had only about 1500 columns rather than 198000).  After 
taking it offline I was able to make it work on 1Gb machines under Windows 
and Linux, and Dirk succeeded using --max-mem-size=640M on Windows.  So it 
looks like it was a problem with total memory usage - I have yet to find 
out what exactly.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dieter.menne at menne-biomed.de  Sat Sep 24 11:35:57 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 24 Sep 2005 09:35:57 +0000 (UTC)
Subject: [R] Alternative to nlm()
References: <200509231906.j8NJ6T909263@icess.ucsb.edu>
Message-ID: <loom.20050924T113435-78@post.gmane.org>

Marion Wittmann <mwittmann <at> bren.ucsb.edu> writes:

> 
>  I am using the nlm function in a minimization exercise but have
> consistently received the code: "last global step failed to locate a point
> lower than 'estimate'...." 
.. 
> So it seems that this function is only finding a local minimum, not
> necessarily a global minimum. 
> 
...
 
Try ?optim which has a few methods with a somewhat broader scope.

Dieter



From jfox at mcmaster.ca  Sat Sep 24 15:04:14 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 24 Sep 2005 09:04:14 -0400
Subject: [R] Are least-squares means useful or appropriate?
In-Reply-To: <x2r7bf3j22.fsf@turmalin.kubism.ku.dk>
Message-ID: <20050924130412.DMRV26967.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter, Doug, and Felipe,

My effects package (on CRAN, also see the article at
http://www.jstatsoft.org/counter.php?id=75&url=v08/i15/effect-displays-revis
ed.pdf) will compute and graph adjusted effects of various kinds for linear
and generalized linear models -- generalizing so-called "least-squares
means" (or "population marginal means" or "adjusted means").

A couple of comments: 

By default, the all.effects() function in the effects package computes
effects for high-order terms in the model, absorbing terms marginal to them.
You can ask the effect() function to compute an effect for a term that's
marginal to a higher-order term, and it will do so with a warning, but this
is rarely sensible.

Peter's mention of floating variances (or quasi-variances) in this context
is interesting, but what would most like to see, I think, are the
quasi-variances for the adjusted effects, that is for terms merged with
their lower-order relatives. These, for example, are unaffected by contrast
coding. How to define reasonable quasi-variances in this context has been
puzzling me for a while.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Friday, September 23, 2005 10:23 AM
> To: Douglas Bates
> Cc: Felipe; R-help at stat.math.ethz.ch
> Subject: Re: [R] Are least-squares means useful or appropriate?
> 
> Douglas Bates <dmbates at gmail.com> writes:
> 
> > On 9/20/05, Felipe <felipe at unileon.es> wrote:
> > > -----BEGIN PGP SIGNED MESSAGE-----
> > > Hash: SHA1
> > >
> > > Hi.
> > > My question was just theoric. I was wondering if someone who were 
> > > using SAS and R could give me their opinion on the topic. I was 
> > > trying to use least-squares means for comparison in R, but then I 
> > > found some indications against them, and I wanted to know if they 
> > > had good basis (as I told earlier, they were not much detailed).
> > > Greetings.
> > >
> > > Felipe
> > 
> > As Deepayan said in his reply, the concept of least squares 
> means is 
> > associated with SAS and is not generally part of the theory 
> of linear 
> > models in statistics.  My vague understanding of these (I 
> too am not a 
> > SAS user) is that they are an attempt to estimate the 
> "mean" response 
> > for a particular level of a factor in a model in which that 
> factor has 
> > a non-ignorable interaction with another factor.  There is 
> no clearly 
> > acceptable definition of such a thing.
> 
> (PD goes and fetches the SAS manual....)
> 
> Well, yes. it'll do that too, although only if you ask for 
> the lsmeans of A when an interaction like A*B is present in 
> the model. This is related to the tests of main effects when 
> an interaction is present using type III sums of squares, 
> which has been beaten to death repeatedly on the list. In 
> both cases, there seems to be an implicit assumption that 
> categorical variables by nature comes from an underlying 
> fully balanced design.
> 
> If the interaction is absent from the model, the lsmeans are 
> somewhat more sensible in that they at least reproduce the 
> parameter estimates as contrasts between different groups. 
> All continuous variables in the design will be set to their 
> mean, but values for categorical design variables are 
> weighted inversely as the number of groups. So if you're 
> doing an lsmeans of lung function by smoking adjusted for age 
> and sex you get estimates for the mean of a population of 
> which everyone has the same age and half are male and half 
> are female. This makes some sense, but if you do it for sex 
> adjusting for smoking and age, you are not only forcing the 
> sexes to smoke equally much, but actually adjusting to  
> smoking rates of 50%, which could be quite far from reality. 
> 
> The whole operation really seems to revolve around 2 things: 
> 
> (1) pairwise comparisons between factor levels. This can alternatively
>     be done fairly easily using parameter estimates for the relevant
>     variable and associated covariances. You don't really need all the
>     mumbo-jumbo of adjusting to particular values of other variables.
> 
> (2) plotting effects of a factor with error bars as if they were
>     simple group means. This has some merit since the standard
>     parametrizations are misleading at times (e.g. if you choose the
>     group with the least data as the reference level, std. err. for
>     the other groups will seem high). However, it seems to me that
>     concepts like floating variances (see float() in the Epi package)
>     are more to the point.
> 
> > R is an interactive language where it is a simple matter to fit a 
> > series of models and base your analysis on a model that is 
> > appropriate.  An approach of "give me the answer to any possible 
> > question about this model, whether or not it make sense" is 
> > unnecessary.
> > 
> > In many ways statistical theory and practice has not caught up with 
> > statistical computing.  There are concepts that are 
> regarded as part 
> > of established statistical theory when they are, in fact, 
> > approximations or compromises motivated by the fact that you can't 
> > compute the answer you want - except now you can compute 
> it.  However, 
> > that won't stop people who were trained in the old system from 
> > assuming that things *must* be done in that way.
> > 
> > In short, I agree with Deepayan - the best thing to do is to ask 
> > someone who uses SAS and least squares means to explain to you what 
> > they are.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From mark.little at imperial.ac.uk  Sat Sep 24 18:47:24 2005
From: mark.little at imperial.ac.uk (Little, Mark P)
Date: Sat, 24 Sep 2005 17:47:24 +0100
Subject: [R] rpart Error in yval[, 1] : incorrect number of dimensions
Message-ID: <F266707397DFCF48A743D9668BD2072006018E@icex4.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050924/109b68bd/attachment.pl

From goran.brostrom at gmail.com  Sat Sep 24 22:02:54 2005
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Sat, 24 Sep 2005 22:02:54 +0200
Subject: [R] Indentation in R code
Message-ID: <148ed818050924130247520493@mail.gmail.com>

I am using emacs-21.3 when writing R functions on Linux debian, and I
am trying to follow the advice i R-exts.pdf (2.1.1) regarding
indentation. That is, I set 'c-default-style' to "bsd" and
'c-basic-offset' to 4. However, while this gives me the intended
indentation in C code, it doesn't change the behavior in R code; I
still get an indentation of size 2. This is my .emacs file after
customization:

 (require 'ess-site)
(custom-set-variables
  ;; custom-set-variables was added by Custom -- don't edit or cut/paste it!
  ;; Your init file should contain only one such instance.
 '(c-basic-offset 4)
 '(c-default-style "bsd"))
(custom-set-faces
  ;; custom-set-faces was added by Custom -- don't edit or cut/paste it!
  ;; Your init file should contain only one such instance.
 )

What is missing in the documentation in 'R-exts.pdf'? Or what have I misssed?

--
GÃ¶ran BrostrÃ¶m



From mark.little at imperial.ac.uk  Sat Sep 24 20:54:04 2005
From: mark.little at imperial.ac.uk (Little, Mark P)
Date: Sat, 24 Sep 2005 19:54:04 +0100
Subject: [R] rpart Error in yval[, 1] : incorrect number of dimensions
Message-ID: <F266707397DFCF48A743D9668BD2072006018F@icex4.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050924/a4aeddd7/attachment.pl

From michel.friesenhahn.b at bayer.com  Sat Sep 24 20:22:32 2005
From: michel.friesenhahn.b at bayer.com (Michel Friesenhahn)
Date: Sat, 24 Sep 2005 11:22:32 -0700
Subject: [R] Legend out of Plot Region
Message-ID: <OF5585D1F2.309C103D-ON88257086.00647964-88257086.0064F0F7@bayer.com>


Hi,

Could someone tell me how to place a legend outside the plot region?

Thanks,

Mike



From dushoff at eno.princeton.edu  Sat Sep 24 20:11:42 2005
From: dushoff at eno.princeton.edu (Jonathan Dushoff)
Date: Sat, 24 Sep 2005 14:11:42 -0400 (EDT)
Subject: [R] Are least-squares means useful or appropriate?
In-Reply-To: <mailman.9.1127469601.31308.r-help@stat.math.ethz.ch>
References: <mailman.9.1127469601.31308.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0509241409390.2700@tahawus.Princeton.EDU>

On 9/20/05, Felipe <felipe at unileon.es> wrote:

> My question was just theoric. I was wondering if someone who were using
> SAS and R could give me their opinion on the topic. I was trying to use
> least-squares means for comparison in R, but then I found some
> indications against them, and I wanted to know if they had good basis
> (as I told earlier, they were not much detailed).

You may find this page helpful: http://www.tufts.edu/~gdallal/LHSP.HTM.
The author claims that "least-squares means" is just a weird SAS word
for adjusted means, and discusses several advantages and disadvantages.

JD



From francoisromain at free.fr  Sat Sep 24 22:42:43 2005
From: francoisromain at free.fr (Romain Francois)
Date: Sat, 24 Sep 2005 22:42:43 +0200
Subject: [R] Legend out of Plot Region
In-Reply-To: <OF5585D1F2.309C103D-ON88257086.00647964-88257086.0064F0F7@bayer.com>
References: <OF5585D1F2.309C103D-ON88257086.00647964-88257086.0064F0F7@bayer.com>
Message-ID: <4335BA43.6090208@free.fr>

Le 24.09.2005 20:22, Michel Friesenhahn a ??crit :

>Hi,
>
>Could someone tell me how to place a legend outside the plot region?
>
>Thanks,
>
>Mike
>  
>
Hi Mike,

Take a look at :

R> par(xpd=NA)

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From c_naber at yahoo.com.br  Sat Sep 24 23:08:05 2005
From: c_naber at yahoo.com.br (Caio Lucidius Naberezny Azevedo)
Date: Sat, 24 Sep 2005 21:08:05 +0000 (GMT)
Subject: [R] Install and load packages
Message-ID: <20050924210805.72425.qmail@web34003.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050924/538150cd/attachment.pl

From c_naber at yahoo.com.br  Sat Sep 24 23:17:42 2005
From: c_naber at yahoo.com.br (Caio Lucidius Naberezny Azevedo)
Date: Sat, 24 Sep 2005 21:17:42 +0000 (GMT)
Subject: [R] Install and load packages
Message-ID: <20050924211742.13800.qmail@web34009.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050924/6973b2b9/attachment.pl

From helprhelp at gmail.com  Sat Sep 24 23:22:07 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Sat, 24 Sep 2005 16:22:07 -0500
Subject: [R] a question on proximity measurement in randomForest
Message-ID: <cdf817830509241422293da471@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050924/9aeb4b84/attachment.pl

From murdoch at stats.uwo.ca  Sun Sep 25 05:15:42 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 24 Sep 2005 23:15:42 -0400
Subject: [R] Install and load packages
In-Reply-To: <20050924211742.13800.qmail@web34009.mail.mud.yahoo.com>
References: <20050924211742.13800.qmail@web34009.mail.mud.yahoo.com>
Message-ID: <4336165E.9000105@stats.uwo.ca>

Caio Lucidius Naberezny Azevedo wrote:
> Dear R-users,
>  
> I would like to know what are the commands to install (from a local zip file) a package and then to load it.
>  
On what system?  On Windows, the easiest way is to use the menu:

Packages | Install package(s) from local zip files

Duncan Murdoch



From murdoch at stats.uwo.ca  Sun Sep 25 05:17:11 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 24 Sep 2005 23:17:11 -0400
Subject: [R] Install and load packages
In-Reply-To: <20050924211742.13800.qmail@web34009.mail.mud.yahoo.com>
References: <20050924211742.13800.qmail@web34009.mail.mud.yahoo.com>
Message-ID: <433616B7.30608@stats.uwo.ca>

(Oops, wasn't quite done when I sent the last one.)

Caio Lucidius Naberezny Azevedo wrote:
> Dear R-users,
>  
> I would like to know what are the commands to install (from a local zip file) a package and then to load it.
>  
On what system?  On Windows, the easiest way is to use the menu:

Packages | Install package(s) from local zip files

then

Packages | Load package...

There are command line versions of both of those.

Duncan Murdoch



From A.Robinson at ms.unimelb.edu.au  Sun Sep 25 09:44:51 2005
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 25 Sep 2005 17:44:51 +1000
Subject: [R] R CMD build produces tar error under FreeBSD 5.4
Message-ID: <20050925074451.GF42964@ms.unimelb.edu.au>

Hi R-helpers,

I am trying to build a package under FreeBSD 5.4-RELEASE #0 using R
Version 2.1.1.

I have constructed a package using package.skeleton(), when I try

$ R CMD build foo
* checking for file 'foo/DESCRIPTION' ... OK
* preparing 'foo':
* checking DESCRIPTION meta-information ... OK
* cleaning src
* removing junk files
tar: Option -L is not permitted in mode -x
Error: cannot open file 'foo/DESCRIPTION' for reading

foo/DESCRIPTION exists and the permissions are correct. The same
command works under Linux Fedora 2.  The man pages on each OS imply
that tar differs across the two platforms.  Does anyone have any
thoughts on a work-around?

Thanks

Andrew
-- 
Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344-4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au



From Hanzz at gmx.com  Sun Sep 25 10:07:04 2005
From: Hanzz at gmx.com (Hanzz@gmx.com)
Date: Sun, 25 Sep 2005 10:07:04 +0200
Subject: [R] =?iso-8859-1?q?That=B4s_Business_-_so_verdient_man_heute=2E?=
	=?iso-8859-1?q?=2E=2E?=
Message-ID: <0cd201c5c1a8$1d4c8df0$1e00a8c0@seifenkiste>

Achtung: Wenn Sie ein Skeptiker und für neue innovative Möglichkeiten nicht aufgeschlossen sind, dann sollten Sie diese Webseite verlassen!
Anderenfalls bewahren Sie sich einfach Ihr gesundes Maß an Misstrauen und starten Sie.


===============

That´s Business

===============

450.000 Euro
in 7 Monaten möglich!
Durch Network-Marketing ging mein Traum in Erfüllung!
Hallo, schön, dass Sie auf meiner Homepage vorbeischauen. Ich bin genauso wie Sie, im Internet auf dieses Programm aufmerksam geworden. Was mir hier sofort sympathisch war: ALLES wird sofort beschrieben, wie Sie leicht Geld verdienen können und Sie können SOFORT loslegen. (Anders wie bei verschiedenen Nebenjob-Anbietern im Internet, wo Sie erst einmal für viel Geld Informationen anfordern müssen, um nur zu erfahren worum es überhaupt geht...) 
Nehmen Sie sich ein paar Minuten Zeit und lesen Sie sich den Text unten in Ruhe einmal durch. Und Sie werden sehen WIE einfach man Geld verdienen kann. Und was mir persönlich auch sehr wichtig war: Es macht Spass!!! 
Die Personen weiter unten auf der Liste sind ganz normale Personen wie Sie und ich, die sich auch entschieden haben an dem Programm teilzunehmen. Es gibt also keine Person "dazwischen" die noch Kohle einkassiert. Sie sind Ihr eigener Chef! 
Übrigens für alle Skeptiker: ich habe beim Gewerbeaufsichtsamt nochmals nachgefragt, es gibt definitiv rechtlich KEINE Einwände gegen dieses Programm!
Ich wünsche Ihnen genauso viel Erfolg und Spass wie ich bis jetzt damit habe!!!!!
>>> Wie aus 35 Euro 450.000 Euro und mehr werden - in 7 Monaten möglich !!!
Sehr geehrte Damen und Herren,
Wir distanzieren uns von allen bis jetzt da gewesenen, ähnlichen Programmen. Überzeugen Sie sich selbst. Den Grundgedanken haben wir gelassen, weil das Programm GOLD wert ist. Die Änderungen sind jetzt auf deutschsprachige Länder zugeschnitten (Gesetze, Verordnungen etc.).
Guten Tag!
Im Internet mit Ihrem PC von zu Hause Geld verdienen! Sie können innerhalb der nächsten 7 Monaten mehr als 450.000 ? erhalten, indem Sie kostenlos Werbung machen, erfahren Sie in einer Schritt für Schrittanweisung, die in den 7 E-Büchern stehen, die Sie per e-mail erhalten werden. Sie sind für jeden leicht verständlich geschrieben.
Bis 450.000 ? in nur 7 Monaten! Erscheint Ihnen das unmöglich? Lesen Sie weiter und erfahren Sie detailliert, wie das funktioniert.
Nein, - es gibt dabei keinen "Haken" !
Vielen Dank für Ihre Zeit und Ihr Interesse!
Wegen der Popularität dieses Briefes im Internet widmete ein bekanntes deutsches Nachrichtenmagazin eine komplette Sendung der Untersuchung des unten beschriebenen Programms, um herauszufinden ob es wirklich Geld bringt.

Diese Sendung prüfte auch, ob das Programm legal ist oder nicht. Dabei wurde herausgefunden, dass es keine Gesetze gibt, dass die Teilnahme an dem Programm verbietet. Dies hat dazu beigetragen zu zeigen, dass dies ein einfacher, harmloser Weg ist, zusätzlich Geld von zu Hause aus zu verdienen und bemerkenswerte Resultate gebracht..

Es nehmen so viele Menschen an diesem Programm teil, dass es für diejenigen, die schon dabei sind, noch besser läuft, als zuvor. Da jeder mehr verdient, je mehr Menschen es ausprobieren, war es in letzter Zeit sehr aufregend dabei zu sein. Das werden Sie verstehen, sobald Sie Erfahrungen sammeln.
************************************************************************************
Sie können sich das Folgende jetzt ausdrucken, um jederzeit darauf zurückzugreifen, in jedem Fall aber sicher aufbewahren, denn Sie werden das unglaubliche Konzept noch öfters lesen, d.h. wenn Sie gerne 450.000 ? in weniger als 7 Monaten verdienen möchten, dann lesen Sie das folgende Programm ... und dann lesen Sie es noch einmal!
Das Programm bietet eine legale Möglichkeit, Geld im Internet zu verdienen.
Dafür müssen Sie keinem etwas persönlich verkaufen, hart arbeiten und das Beste daran ist: Sie müssen nicht einmal das Haus verlassen. Sie werden eines Tages einen Kontostand erreichen, von dem Sie schon lange geträumt haben, ob Sie es wollen oder nicht!
Rezession: (Einer von vielen Anwendern dieser Geschäftsmöglichkeit)
Ich heiße Markus Weber. Vor zwei Jahren hat die Firma, für die ich die letzten 12 Jahre arbeitete, rationalisiert und ich wurde entlassen. Nach unergiebigen Vorstellungsgesprächen entschloss ich mich mein eigenes Geschäft aufzumachen.
In den vergangenen Jahren erlebte ich einige unvorhergesehene finanzielle Probleme. Ich schuldete meiner Familie, meinen Freunden und meinen Geldgebern mehr als 18.000 ?. Die Wirtschaftslage forderte ihren Tribut von meinem Geschäft und es gelang mir nicht, ein ausreichendes Auskommen zu finden. Ich musste refinanzieren und eine Hypothek aufnehmen, um meine Familie und mein Geschäft zu erhalten. In diesem Moment passierte etwas Entscheidendes in meinem Leben. Ich schreibe Ihnen dies, um meine Erfahrung zu teilen und bin sicher, dass es auch Ihr Leben finanziell für immer verändern wird!
Mitte März erhielt ich dieses Programm per E-Mail.
Noch sechs Monate vorher sah ich mich nach Informationen über verschiedene Geschäftsideen um. Alles, was ich erhielt, war in meinen Augen nicht profitabel. Entweder war es zu kompliziert für mich oder die Startsumme war zu hoch um das Risiko einzugehen, um nur zu sehen ob es läuft oder nicht. Eine Geschäftsidee behauptete, dass ich in einem Jahr eine Million verdienen würde... nur hatten sie mir nicht gesagt, dass ich dazu ein Buch schreiben müsste.
Aber, wie gesagt, im März 2003 erhielt ich dieses Programm. Ich hatte es nicht angefordert oder danach gefragt. Mein Name stand einfach auf einer Mailingliste. Gott sei Dank! Nachdem ich es mir mehrmals durchgelesen hatte, um sicherzugehen dass ich richtig verstanden hatte, traute ich meinen Augen nicht. Hier war ein Geld erzeugendes Phänomen. Um anzufangen konnte ich soviel wie ich wollte investieren, ohne mich weiter zu verschulden. Nachdem ich etwas zu schreiben holte und es durchrechnete, würde ich zumindest wesentlich mehr als meinen Einsatz wieder herausholen.
Ich begann mit der Werbemöglichkeit Nr. 23 (Kleinanzeigen) und inserierte in nur 80 kostenlosen Annoncenblätter. Ich verschickte dann auf die Anfragen diesen Werbetext per E-Mail.
Das wunderbare an E-Mail ist, dass es keine Druckkosten und keine Versandkosten gibt und da alle Bestellungen durch E-Mail-Versand erfüllt werden, ist der einzige Kostenfaktor meine eingesetzte Zeit. Ich sage Ihnen wie es ist, ich hoffe das schreckt Sie nicht ab, aber ich habe mir selbst versprochen, dass ich keinen über den Tisch ziehen würde, egal was es mich kosten würde.
In weniger als einer Woche begannen Bestellungen des E-Buches Nr.1 bei mir einzutreffen. Bis 13. April hatte ich 26 Bestellungen für E-Buch Nr.1 erhalten. Ihr Ziel ist es, innerhalb von zwei Wochen mindestens 20 Bestellungen für E-Buch Nr.1 zu erhalten. Falls Sie das nicht erreicht haben, verschicken Sie mehr Programme, solange bis Sie es erreicht haben. Dies war mein erster Schritt auf dem Weg zu den 450.000 ? in 7 Monaten.
Bis zum 5. Mai hatte ich 196 Bestellungen für E-Buch Nr.2. Ihr Ziel ist es, innerhalb von 4 Wochen mindestens 100 Bestellungen für E-Buch Nr.2 zu erhalten. Falls Sie das nicht erreicht haben, machen Sie solange weiter, bis Sie es erreicht haben. Wenn Sie die 100 Bestellungen haben, ist der Rest ganz leicht. Entspannen Sie sich, Sie werden mindestens 450.000 ? - Ziel erreichen.
Nun, ich hatte 196 Bestellungen für E-Buch Nr.2. - 96 mehr als ich brauchte. Ich lehnte mich also zurück und entspannte mich. Bis 25. Mai erhielt ich durch meine 1 x 80 kostenlos aufgegebenen Annoncen 70.000 ? und es kommt täglich mehr Geld dazu.
Ihre ganze Arbeit besteht darin, für dieses Programm zu werben.
Wo Sie Werbung machen, steht genauestens in den 7 E-Büchern (insgesamt 35 unterschiedliche kostenlose Werbemöglichkeiten) beschrieben.
Das ist wirklich alles!
Bitte nehmen Sie sich die Zeit, das nachfolgende Programm zu lesen, es wird Ihr Leben für immer verändern.
Denken Sie daran: Es wird nicht funktionieren, wenn Sie es nicht ausprobieren. Dieses Programm funktioniert, aber Sie müssen es g e n a u befolgen! Insbesondere die Vorschrift, Ihren Namen nicht auf einen anderen Platz in der Liste als angegeben zu setzen. Das wird nicht funktionieren und Sie verlieren enorm Geld! Damit dieses Programm funktionieren kann, müssen Sie Ihr Ziel von 20 Bestellungen für E-Buch Nr.1 und mindestens 100 Bestellungen für E-Buch Nr.2 erreichen, dann können Sie 450.000 ? oder mehr verdienen. Ich bin der lebende Beweis dafür, dass es funktioniert.
KONZENTRIEREN SIE SICH NUR AUF DAS
! ! ! ! ! ! ! ! ! EINE ZIEL ! ! ! ! ! ! ! ! !
Die meisten Menschen haben keine Erfolge, weil sie sich verzetteln!
Falls Sie sich gegen die Teilnahme an diesem Programm entscheiden, tut es mir leid. Es ist für Sie wirklich eine einmalige Chance mit wenig Risiko und wenigen Kosten. Falls Sie sich für die Teilnahme an dem Programm entscheiden, sind Sie auf dem Weg zu finanzieller Sicherheit. Wenn Sie auch ein Geschäftsmann sind und in finanziellen Schwierigkeiten, so wie ich es war oder Sie ein neues Geschäft anfangen, betrachten Sie das als ein Zeichen. Ich habe es so gemacht.
Mit freundlichen Grüßen
Markus Weber
************************************************************************************
Eine persönliche Bemerkung vom Gründer dieses Programms:
Nachdem Sie das Programm und die beigefügten Texte gelesen haben, wird Ihnen klar geworden sein, dass ein solches Programm, noch dazu eines, das legal ist, nicht von einem Amateur geschaffen werden konnte. Sie haben gerade Informationen erhalten, die Ihnen für den Rest Ihres Lebens finanzielle Sicherheit geben können, ohne Risiko und mit einem Minimum an Einsatz. In den nächsten Monaten können Sie mehr Geld verdienen, als Sie sich bisher vorgestellt haben. Ich möchte herausstellen, dass ich keinen Pfennig dieses Geldes sehen werde, noch irgend jemand, der in dieser Information Autor ist. Ich habe schon über 2 Millionen ? verdient! Ich habe mich von dem Programm zurückgezogen. Nun besitze ich mehrere Büros, welche andere Programme hier und in Übersee leiten.
Folgen Sie den Anweisungen dieses Programms genauso wie vorgeschrieben. Verändern Sie es auf keinen Fall! Es arbeitet perfekt in der Form wie es Ihnen vorliegt. Schicken Sie jeder Person, die durch Ihre Werbemaßnahme nähere Informationen möchten diesen Werbebrief. Eine dieser Person wird vielleicht mehr als 80 Inserate aufgeben oder die Werbemöglichkeit Nr. 5 oder 28 nehmen ... und Ihr Name steht dann in allen Werbetexten.
Denken Sie auch daran: Je mehr Sie Werbung schalten, desto mehr potentielle Kunden erreichen Sie. In den 7 E-Büchern sind 35 unterschiedliche kostenlose Werbemöglichkeiten beschrieben, die für viele Millionen Programmteilnehmer ausreichen.
So, ich habe Ihnen nun die Idee, die Informationen, das Material und die Gelegenheit gegeben finanziell unabhängig zu werden - Jetzt sind Sie dran! "Denken Sie darüber nach!" Bevor Sie diesen Text löschen, nehmen Sie sich doch ein wenig Zeit und denken Sie wirklich einmal darüber nach. Nehmen Sie einen Stift und rechnen Sie sich aus, was passieren kann, wenn Sie teilnehmen. Rechnen Sie mit der geringsten Rücklaufquote und Sie werden sehen, dass Sie immer noch eine Menge Geld verdienen werden! Auf alle Fälle werden Sie das, was Sie investieren wieder zurückbekommen. Alle Zweifel, die Sie noch haben, werden verschwinden, wenn die ersten Bestellungen eintreffen. Es funktioniert !!!
Wolfgang Müller / München /BRD
************************************************************************************
HIER IST DIE ANLEITUNG; WIE DIESES ERSTAUNLICHE PROGRAMM IHNEN TAUSENDE VERSCHAFFT !
Diese Methode Geld zu verdienen funktioniert jedes mal 100%-ig. Ich bin sicher, dass Sie 450.000 Euro oder mehr in den nächsten Monaten gut gebrauchen können. Bevor Sie jetzt sagen: "So ein Sch...!" lesen Sie bitte dieses Programm gründlich durch. Dies ist eine völlig legale Methode um Geld zu verdienen. Folgendes sollten Sie tun:
Wie bei allen Network-Marketing-Unternehmen bauen wir unser Geschäft auf, indem wir neue Partner gewinnen, die unser Produkt verkaufen. Für das Geld, was Sie einnehmen, verkaufen Sie eine Ware. Sie verkaufen E-Bücher. Das ist legal, denn es ist völlig egal, ob Sie Schuhe, Versicherungen oder E-Bücher verkaufen. Die Hauptsache ist, dass Sie überhaupt etwas verkaufen, für das Geld, was Sie erhalten.
Wegen der globalen Natur des Internet, werden Sie neue Network Geschäftspartner aus der ganzen Welt gewinnen können. Ihre Bestellungen kommen per Post und Sie erfüllen Sie per E-Mail, deshalb brauchen Sie keine persönlichen Verkaufsgespräche zu führen. Sie machen das zu Hause, in Ihren vier Wänden, im Büro oder im Geschäft. Das ist das größte Network-Marketing, dass es gibt.
Und so geht es:
1. Bestellen Sie alle 7 E-Bücher die unten auf der Liste stehen. Sie können die E-Bücher nicht verkaufen, wenn Sie diese nicht besitzen.
a.) Senden Sie für jedes E-Buch einen 5 Euro-Schein an die entsprechende Adresse. Sie verschicken also 7 Briefe an 7 unterschiedliche Adressen. Den 5 ? - Schein bitte in Papier einwickeln so ist er für die Post nicht sichtbar.
b.) Schreiben Sie auf Ihren 1. Bestellschein: Bitte senden sie mir das E-Buch Nr.1. Dann schreiben Sie bitte ganz deutlich Ihre E-Mailadresse, denn Sie erhalten die E-Bücher alle per E-Mail zugeschickt. Ihre E-Mailadresse muss funktionieren. Schreiben Sie nun noch Ihre genaue Postanschrift mit auf dem Bestellschein, falls es doch einmal mit Ihrer E-Mailadresse Schwierigkeiten geben sollte. Bei der 2., 3., 4., 5., 6. und 7. Bestellung verfahren Sie genauso wie oben beschrieben.
c.) nach einigen Tagen werden Sie per E-Mail alle E-Bücher erhalten. Speichern Sie diese auf Ihrem PC, damit Sie darauf zurückgreifen und diese an die Tausende Kunden verschicken können, welche bei Ihnen bestellen werden.
2. Wichtig:
Verändern Sie nicht die Namen oder Reihenfolge der Namen auf der Liste auf andere Art als nachfolgend unter 2.a bis 2.i beschrieben!!!!! Zuwiderhandlungen lässt Sie nur eine Menge Geld verlieren. Wenn Sie einmal verstanden haben, wie das Programm arbeitet, werden Sie auch verstehen, warum es nicht funktioniert, wenn Sie es verändern. Denken Sie daran, dieses Programm ist getestet und wenn Sie es verändern, wird es nicht funktionieren.
Konzentrieren Sie sich nur auf das EINE Ziel ! ! ! ! ! ! !
a.) Nehmen Sie die Liste der erhältlichen E-Bücher.
b.) Nachdem Sie die 7 E-Bücher bestellt haben, nehmen Sie diesen vorliegenden Text mit der Liste und entfernen Sie den Namen und die Adresse unter E-Buch Nr.7. Diese Person hat den Zyklus durchlaufen und zählt wahrscheinlich gerade die 450 000 ?.
c.) Verschieben Sie Name und Adresse unter E-Buch Nr. 6 nach unten zu E-Buch Nr. 7
d.) Verschieben Sie Name und Adresse unter E-Buch Nr. 5 nach unten zu E-Buch Nr. 6
e.) Verschieben Sie Name und Adresse unter E-Buch Nr. 4 nach unten zu E-Buch Nr. 5
f.) Verschieben Sie Name und Adresse unter E-Buch Nr. 3 nach unten zu E-Buch Nr. 4
g.) Verschieben Sie Name und Adresse unter E-Buch Nr. 2 nach unten zu E-Buch Nr. 3
h.) Verschieben Sie Name und Adresse unter E-Buch Nr. 1 nach unten zu E-Buch Nr. 2
i.) Fügen Sie jetzt Ihren Namen und Adresse an die Stelle unter E-Buch Nr.1 ein
Bitte achten Sie darauf Namen und Adresse korrekt zu übertragen!
3. Nehmen Sie den ganzen Werbetext von dieser Seite einschließlich der von Ihnen geänderten Namensliste und speichern Sie diesen auf Ihren PC.
Verändern Sie die Anleitung auf keinen Fall !!!
Sollten Sie Schwierigkeiten mit dem verschieben haben, dann drucken Sie sich alles aus, damit Sie es in gedruckter Form vor sich liegen haben. Öffnen Sie jetzt das Dokument, wo Sie diesen Werbetext gespeichert haben und löschen Sie bei jedem E-Buch die Adressen der jeweiligen Person. Jetzt schreiben Sie per Hand in der richtigen Reihenfolge, wie bei 2. beschrieben, die Adressen rein.
Ihre Startkosten sind minimal. Sie brauchen lediglich 35 ? als Startkapital. Offensichtlich besitzen Sie einen PC mit Internetanschluss und E-Mailadresse. Um Sie bei der Vermarktung Ihres Geschäfts im Internet zu unterstützen, haben wir 7 E-Bücher von unschätzbaren Wert zusammengestellt.
Nehmen Sie das, was in den E-Bücher steht ernst, dann erreichen Sie auch Ihr Ziel.
Außer Ihren Onlinegebühren entstehen Ihnen keine weitere Kosten.
Nehmen wir an, Sie entscheiden sich klein anzufangen, um zu sehen wie gut es läuft. Wir gehen davon aus, dass Ihr Ziel darin besteht, nur 5 Personen für Ihre erste Ebene zu werben. (Wenn Sie das anwenden, was in den 7 E-Bücher steht, werden Sie leicht viel mehr erreichen). Wir nehmen nun an, dass jeder in Ihrem Programm nur 5 weitere Teilnehmer wirbt. Folgen Sie der Beispielrechnung um den enormen Erfolg zu sehen:
1. Ebene - 5 Teilnehmer .......................... 25 ?
2. Ebene - 25 Teilnehmer ....................... 125 ?
3. Ebene - 125 Teilnehmer ........................ 625 ?
4. Ebene - 625 Teilnehmer ...................... 3125 ?
5. Ebene - 3125 Teilnehmer .................... 15625 ?
6. Ebene - 15625 Teilnehmer .....................78125 ?
7. Ebene - 78125 Teilnehmer ................... 390625 ?
Summe .................................................... 488275 ?
Wir haben angenommen, dass jeder Teilnehmer nur 5 weitere Teilnehmer anwirbt. Denken Sie einen Moment daran, was passiert, wenn Sie 20 Teilnehmer gewinnen. Manche werden 100 Teilnehmer und mehr gewinnen! Denken Sie darüber nach!
Für jeden 5 ? - Schein den Sie erhalten, müssen Sie nur das bestellte E-Buch per E-Mail versenden. Das ist alles! Versenden Sie immer am Tag der Bestellung! Sorgen Sie dafür, die E-Bücher als bald wie möglich zu verschicken, denn der Kunde kann erst mit dem Geschäft beginnen, wenn er Ihre E-Mail erhalten hat.
*********************************************************************************
** Bestellen Sie jedes E-Buch mit dem Titel und der Ordnungszahl ! (z.B.: E-Buch Nr. 1 --usw.) **
Senden Sie immer einen 5 ? - Schein. Senden Sie keinen Scheck! Senden Sie Ihre Bestellung mit normaler Briefpost.
Achten Sie darauf, dass der Geldschein eingepackt ist. Auf Ihrem Bestellschein schreiben Sie:
a.) Ordnungszahl und Titel des gewünschten E-Buches
b.) Ihre E-Mailadresse (bitte deutlich schreiben)
c.) Ihren Namen und Ihre Postanschrift.
Wie schon oben erwähnt, reichen die Werbemöglichkeiten, die in E-Buch Nr. 2 - 7 stehen, für viele Millionen Programmteilnehmer aus.
Bestellen Sie jetzt:
E-Buch Nr. 1:
Wie Sie eine Homepage erstellen (für Anfänger leicht verständlich geschrieben)
Sie bestellen dieses E-Buch für 5 ? bei:
Mandy Reinhardt
Moorkamp 51
D 30165 Hannover

************************************************************************************ 
E-Buch Nr. 2:
kostenlose Werbemöglichkeiten Nr.1 - 6
Sie bestellen dieses E-Buch für 5 ? bei
Sandro Richter
Omptedastr. 27 E
D 30165 Hannover

************************************************************************************
E-Buch Nr. 3:
kostenlose Werbemöglichkeiten Nr.7 - 12
Sie bestellen dieses E-Buch für 5 ? bei
Ronald Hentschel
Stieberstr. 36
D 02625 Bautzen

************************************************************************************
E-Buch Nr. 4:
kostenlose Werbemöglichkeiten Nr.13 - 18
Sie bestellen dieses E-Buch für 5 ? bei:
Ilona Brytsche
Zittauer Str. 29
D 02681 Wilthen

************************************************************************************
E-Buch Nr. 5:
kostenlose Werbemöglichkeiten Nr.19 - 24
Sie bestellen dieses E-Buch für 5 ? bei:
Andreas Trommler
Wilhelm-Busch Str. 16
D 74076 Heilbronn

************************************************************************************
E-Buch Nr. 6:
kostenlose Werbemöglichkeiten Nr.25 - 30
Sie bestellen dieses E-Buch für 5 ? bei:
Mathias Hertwig 
Im Bruch. 14 
D - 77736 Zell a.H

************************************************************************************ 
E-Buch Nr. 7:
kostenlose Werbemöglichkeiten Nr.31 - 36
Sie bestellen dieses E-Buch für 5 ? bei:
Manuela Richter
Bootsweg. 44
D - 23970 Wismar

************************************************************************************ 
TIPS Für Ihren Erfolg
* Betrachten Sie dieses Programm als Ihr Geschäft!
* Seien Sie prompt professionell und folgen Sie den Anweisungen.
* Bestellen Sie die 7 E-Bücher JETZT GLEICH, damit Sie diese zur Verfügung haben, wenn die Bestellungen kommen. Sie müssen das Produkt sofort ausliefern bzw. versenden.
* Versenden Sie immer gleich am Tag der Bestellung.
* Seien Sie geduldig und ausdauernd bei der Anwendung dieses Programms.
Wenn Sie den Anweisungen genau folgen, wird Ihr Ergebnis ERFOLG sein!
Vor allem: Glauben Sie an sich selbst und seien Sie von Ihrem Erfolg überzeugt !
************************************************************************************
ÜBRIGENS: >>>Momentan gibt es mehr als 175.000.000 Internet-Teilnehmer ! ! ! <<<
************************************************************************************
>>> Die Leitlinien, die Ihnen Ihren Erfolg garantieren <<<
Folgen Sie diesen Leitlinien, die Ihnen Ihren Erfolg garantieren:
Falls Sie innerhalb der ersten zwei Wochen keine Bestellungen für E-Buch Nr.1 erhalten - was völlig außergewöhnlich wäre - , fahren Sie fort Werbung zu machen, solange, bis Sie 20 Bestellungen haben. Dann sollten Sie innerhalb von 4 Wochen mindestens 100 Bestellungen für E-Buch Nr.2 erhalten. Wenn nicht, dann fahren Sie fort, Werbung zu schalten bis Sie 100 Bestellungen haben. Wenn Sie 100 Bestellungen für E-Buch Nr.2 erreicht haben, können Sie sich zurücklehnen und entspannen, denn das Programm arbeitet nun bereits für sie und das Geld wird Ihnen zuströmen.
WICHTIGE ERINNERUNG: Jedes Mal, wenn Sie in der Liste einen Platz nach unten vorrücken, steht Ihr Name für ein anderes E-Buch. Sie können Ihren Fortschritt dadurch beobachten, indem Sie die Ordnungszahlen der bestellten E-Bücher beachten. Falls Sie mehr Einkommen erzeugen wollen, fahren Sie fort mit Werbung und starten so den ganzen Prozess von vorne! Es gibt keine Grenze für Ihr Einkommen, dass Sie durch dieses Programm erzeugen können! Bevor Sie sich entscheiden, ob Sie nun an diesem Programm teilnehmen oder nicht, beantworten Sie eine Frage: Wollen Sie Ihr Leben verändern? Wenn Sie "ja" antworten, schauen Sie sich bitte folgende Fakten über dieses Programm an:
1. Sie versenden ein Produkt, das keine Produktkosten hat.
2. Sie versenden ein Produkt das keine Transportkosten hat.
3. Sie versenden ein Produkt, das keine Werbekosten hat.
4. Sie versenden ein Produkt, welches Anderen hilfreich ist, um Geld zu verdienen.
5. Sie benutzen die Power des Internet und des Network-Marketings.
6. Die einzige Investition, außer Ihren 35 ? ist Ihre Zeit und die Onlinegebühren.
7. Fast alles Einkommen, dass Sie erzeugen ist reiner Profit.
Wir verdienen nur, wenn Sie verdienen
- und wir wollen verdienen ! ! !
Zeugnisse:
Dieses Programm funktioniert, aber Sie müssen es genau befolgen !
Insbesondere die Vorschrift nicht zu versuchen Ihren Namen an eine andere Stelle zu schreiben, denn das wird nicht funktionieren und Sie werden eine Menge an potentiellen Einkommen verlieren. Ich bin der lebende Beweis, dass es funktioniert. Es ist wirklich eine großartige Möglichkeit relativ leicht Geld zu machen ohne große Investitionen. Wenn Sie sich entscheiden teilzunehmen, folgen Sie dem Programm genau und Sie betreten den Weg zur finanziellen Sicherheit.
Michael Schiffer, Ostende, Germany
************************************************************************************
Ich heiße Bernd. Meine Frau Judith und ich leben in Bayern. Ich arbeite im Rechnungswesen einer großen deutschen Firma und ich verdiene ganz gut. Als meine Frau das Programm erhielt, brummte ich einiges negatives gegenüber Network - Marketing. Ich machte mich über das ganze Ding lustig, indem ich mein Wissen über die Bevölkerung und die angegebenen Prozentzahlen ausbreitete. Ich "wusste", dass es nicht funktionieren würde. Judith ignorierte meine angebliche Intelligenz vollkommen und hat sofort losgelegt. Ich habe mich gnadenlos über Sie lustig gemacht und war bereit für das alte "Ich hab's dir ja gleich gesagt...", dass das Ding nicht laufen wird. Nun, sie hat zuletzt gelacht. Innerhalb von vier Wochen hatte sie über 120 Bestellungen. Innerhalb von 60 Tagen hatte sie bereits 20.000 ? in 5 ? -Scheinen erhalten! Ich war schockiert! Ich war sicher gewesen, alles genau berechnet zu haben und dass es nicht funktionieren würde. Ich wurde eines Besseren belehrt. Ich nehme nun an dem "Hobby" von Judith teil. Wir verdanken alles Network - Marketing.
In tiefer Dankbarkeit Euer Bernd Hoffmann, Bayern
************************************************************************************
Der Grund für diesen Brief ist, Sie zu überzeugen, dass das Programm ehrlich, legal und extrem profitabel ist, sowie ein Weg eine Menge Geldes in kurzer Zeit zu bekommen. Es wurde mehrere male an mich herangetragen bevor ich es ausprobierte. Ich habe teilgenommen, nur um zu sehen, was man für den minimalen Einsatz und das minimale Geld, das erforderlich ist, zurück bekommt. Zu meiner Verblüffung erhielt ich 36.000 ? in den ersten 10 Wochen und es kommt von Tag zu Tag mehr Geld.
Mit freundlichen Grüßen, Petra Koch, Hamburg
************************************************************************************
Da ich ein Spielertyp bin, brauche ich mehrere Wochen um mich zu entscheiden in diesem Programm mitzumachen. Konservativ, wie ich bin, entschied ich, dass die anfängliche Investition so gering war, dass es unmöglich wäre - zu wenige Bestellungen zu bekommen und wenigstens mein Geld zurückzubekommen. Ich war erstaunt als mein mittelgroßer Briefkasten gerammelt voll mit Bestellungen war. Zeitweise war er so überladen, dass ich die Post vom Briefträger holte. Ich werde dieses Jahr mehr verdienen als in den 10 Jahren zuvor. Das Schöne an der Sache ist, dass es egal ist, wo die Leute wohnen. Es gibt einfach keine bessere Investition mit besserem Gewinn.
Birgit Haas, Rheinland-Pfalz
************************************************************************************
Ich hatte dieses Programm schon einmal erhalten. Ich habe es gelöscht, aber dann habe ich mir gedacht, ob ich es nicht besser ausprobiert hätte. Ich hatte natürlich keine Idee, wen ich kontaktieren sollte, um ein anderes Exemplar zu erhalten, so musste ich darauf warten, dass mir ein anderes Programm zugeschickt wurde.... 4 Monate vergingen, dann kam es... Dieses Mal habe ich es nicht gelöscht!... Ich habe mehr als 1.500.000 Franken beim ersten Versuch eingenommen.
Franz Heberle, Schweiz
************************************************************************************
Das ist das dritte mal, dass ich an diesem Programm teilnehme. Wir haben unsere Jobs aufgegeben und werden uns bald ein Haus am Strand in der Südsee kaufen und von den Zinsen unseres Kapitals leben. Die einzige Art und Weise, wie dieses Programm für Sie arbeiten wird ist, dass Sie teilnehmen. Versäumen Sie diese goldene Gelegenheit nicht, Ihnen zuliebe und Ihrer Familie zuliebe. Viel Glück und fröhliches Geldausgeben!
Wolfgang Junger, Osnabrück
************************************************************************************
Bestellen Sie die 7 E-Bücher noch heute und machen Sie die ersten Schritte auf den Weg zur finanziellen Freiheit. Jetzt sind SIE dran!
Entschiedene Handlung bringt kraftvolle Resultate ! ! !
************************************************************************************
Hinweis:
Wenn Sie beim Beginn dieses Geschäfts Hilfe brauchen, bei der Einkommensteuer etc. rufen Sie einfach bei Ihrem Finanzamt an, welches Ihre Fragen beantworten wird oder besser: wenden Sie sich an Ihren Steuerberater.
Natürlich stehe auch ich Ihnen gerne zur Verfügung:
Ihre Einkünfte und Erfolge sind von Ihrer Aktivität abhängig. Dieser Text enthält keine ausdrückliche oder indirekte Garantien. Im Falle, dass festgestellt werden sollte, dass dieser Text in irgendeiner Art und Weise eine Garantie enthält, wird diese hiermit für ungültig erklärt. Einige Zeugnisse oder Einkommen, die hier in diesem Text aufgeführt werden, können Fakten sein oder unbeweisbar.
************************************************************************************


========================
http://www.thatsbusiness.de
========================

Ständige Aktualisierung
Stand 24.09.2005



From antonio.fabio at gmail.com  Sun Sep 25 11:37:03 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Sun, 25 Sep 2005 11:37:03 +0200
Subject: [R] getting variable length numerical gradient
Message-ID: <b0808fdc050925023723026856@mail.gmail.com>

Hi all.
I have a numerical function f(x), with x being a vector of generic
size (say k=4), and I wanna take the numerically computed gradient,
using deriv or numericDeriv (or something else).

My difficulties here are that in deriv and numericDeric the function
is passed as an expression, and one have to pass the list of variables
involved as a char vector... So, it's a pure R programming question.


Have a nice sunday,
Antonio, Fabio Di Narzo.



From u9370004 at cc.kmu.edu.tw  Sun Sep 25 12:49:26 2005
From: u9370004 at cc.kmu.edu.tw (Chun-Ying Lee)
Date: Sun, 25 Sep 2005 18:49:26 +0800
Subject: [R] summary nls output
Message-ID: <20050925102613.M74173@cc.kmu.edu.tw>

Dear R user:
    I bulid a package, and in the package I use the function "nls"
to solve some questions. If I have two sets of data, and I want to 
summary these two data's nls output, I write the command in the 
package source code like:
{
..........
summary(fm1)
summary(fm2)
}
then i compiler the package and use "Install package(s) from 
local zip files" to install my package.
But when I run the package, it just show the summary(fm2), the
part of summary(fm1) miss, why is it so?
Any suggestions would be most helpful! Thanks!!



From dimitris.rizopoulos at med.kuleuven.be  Sun Sep 25 13:07:16 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Sun, 25 Sep 2005 13:07:16 +0200
Subject: [R] getting variable length numerical gradient
References: <b0808fdc050925023723026856@mail.gmail.com>
Message-ID: <00b901c5c1c1$4a16bc20$0540210a@www.domain>

maybe you can find the following function useful (any comments are 
greatly appreciated):

fd <- function(x, f, scalar = TRUE, ..., eps = 
sqrt(.Machine$double.neg.eps)){
    f <- match.fun(f)
    out <- if(scalar){
        if(length(f0 <- f(x, ...)) != length(x))
            stop("'f' must be vectorized")
        x. <- x + eps * pmax(abs(x), 1)
        c(f(x., ...) - f0) / (x. - x)
    } else{
        n <- length(x)
        res <- array(0, c(n, n))
        f0 <- f(x, ...)
        ex <- pmax(abs(x), 1)
        for(i in 1:n){
            x. <- x
            x.[i] <- x[i] + eps * ex[i]
            res[, i] <- c(f(x., ...) - f0) / (x.[i] - x[i])
        }
        res
    }
    out
}


## Examples

x <- seq(-3.3, 3.3, 0.1)
all.equal(fd(x, pnorm, mean = 0.5), dnorm(x, mean = 0.5))


# Approximate the Hessian matrix for a logistic regression

# the score vector function
gn <- function(b, y, X){
    p <- as.vector(plogis(X %*% b))
    -colSums(X * (y - p))
}

# We simulate some data and fit the logistic regression
n <- 800
x1 <- runif(n,-3, 3); x2 <- runif(n, -3, 3)
pr <- plogis(0.8 + 0.4 * x1 - 0.3 * x2)
y <- rbinom(n, 1, pr)
fm <- glm(y ~ x1 + x2, binomial)

## The Hessian using forward difference approximation
fd(fm$coef, gn, scalar = FALSE, y = y, X = cbind(1, x1, x2))

## The true Hessian
solve(summary(fm)$cov.unscaled)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Antonio, Fabio Di Narzo" <antonio.fabio at gmail.com>
To: <R-help at stat.math.ethz.ch>
Sent: Sunday, September 25, 2005 11:37 AM
Subject: [R] getting variable length numerical gradient


> Hi all.
> I have a numerical function f(x), with x being a vector of generic
> size (say k=4), and I wanna take the numerically computed gradient,
> using deriv or numericDeriv (or something else).
>
> My difficulties here are that in deriv and numericDeric the function
> is passed as an expression, and one have to pass the list of 
> variables
> involved as a char vector... So, it's a pure R programming question.
>
>
> Have a nice sunday,
> Antonio, Fabio Di Narzo.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From ripley at stats.ox.ac.uk  Sun Sep 25 13:10:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Sep 2005 12:10:10 +0100 (BST)
Subject: [R] summary nls output
In-Reply-To: <20050925102613.M74173@cc.kmu.edu.tw>
Message-ID: <Pine.GSO.4.31.0509251207450.18680-100000@toucan.stats>

On Sun, 25 Sep 2005, Chun-Ying Lee wrote:

> Dear R user:
>     I bulid a package, and in the package I use the function "nls"
> to solve some questions. If I have two sets of data, and I want to
> summary these two data's nls output, I write the command in the
> package source code like:
> {
> ..........
> summary(fm1)
> summary(fm2)
> }
> then i compiler the package and use "Install package(s) from
> local zip files" to install my package.
> But when I run the package, it just show the summary(fm2), the
> part of summary(fm1) miss, why is it so?

You forgot to print it.  See the FAQ 7.16 and 7.22.

> Any suggestions would be most helpful! Thanks!!

Did you read the posting guide?  That does ask you to read the FAQ before
posting.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From A.Robinson at ms.unimelb.edu.au  Sun Sep 25 13:16:37 2005
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 25 Sep 2005 21:16:37 +1000
Subject: [R] summary nls output
In-Reply-To: <20050925102613.M74173@cc.kmu.edu.tw>
References: <20050925102613.M74173@cc.kmu.edu.tw>
Message-ID: <20050925111637.GH42964@ms.unimelb.edu.au>

This is because R only permits one object to be output.  If you weould
like two objects, you might try something like

{
..........
list(fm1 = summary(fm1), fm2 = summary(fm2))
}

Good luck,

Andrew

On Sun, Sep 25, 2005 at 06:49:26PM +0800, Chun-Ying Lee wrote:
> Dear R user:
>     I bulid a package, and in the package I use the function "nls"
> to solve some questions. If I have two sets of data, and I want to 
> summary these two data's nls output, I write the command in the 
> package source code like:
> {
> ..........
> summary(fm1)
> summary(fm2)
> }
> then i compiler the package and use "Install package(s) from 
> local zip files" to install my package.
> But when I run the package, it just show the summary(fm2), the
> part of summary(fm1) miss, why is it so?
> Any suggestions would be most helpful! Thanks!!
> 

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344-4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au



From ripley at stats.ox.ac.uk  Sun Sep 25 13:07:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Sep 2005 12:07:02 +0100 (BST)
Subject: [R] R CMD build produces tar error under FreeBSD 5.4
In-Reply-To: <20050925074451.GF42964@ms.unimelb.edu.au>
Message-ID: <Pine.GSO.4.31.0509251202000.18680-100000@toucan.stats>

On Sun, 25 Sep 2005, Andrew Robinson wrote:

> Hi R-helpers,
>
> I am trying to build a package under FreeBSD 5.4-RELEASE #0 using R
> Version 2.1.1.
>
> I have constructed a package using package.skeleton(), when I try
>
> $ R CMD build foo
> * checking for file 'foo/DESCRIPTION' ... OK
> * preparing 'foo':
> * checking DESCRIPTION meta-information ... OK
> * cleaning src
> * removing junk files
> tar: Option -L is not permitted in mode -x
> Error: cannot open file 'foo/DESCRIPTION' for reading
>
> foo/DESCRIPTION exists and the permissions are correct. The same
> command works under Linux Fedora 2.  The man pages on each OS imply
> that tar differs across the two platforms.  Does anyone have any
> thoughts on a work-around?

No, because R does not use tar -L (which is to do with tape lengths on GNU
tar).

It does use tar chf and tar xhf.  The h modifier would appear to be
applicable only to dumps, so at a wild guess the error message means -h is
not permitted.  Try replacing xhf by xf.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From A.Robinson at ms.unimelb.edu.au  Sun Sep 25 13:58:32 2005
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 25 Sep 2005 21:58:32 +1000
Subject: [R] R CMD build produces tar error under FreeBSD 5.4
In-Reply-To: <Pine.GSO.4.31.0509251202000.18680-100000@toucan.stats>
References: <20050925074451.GF42964@ms.unimelb.edu.au>
	<Pine.GSO.4.31.0509251202000.18680-100000@toucan.stats>
Message-ID: <20050925115832.GI42964@ms.unimelb.edu.au>

On Sun, Sep 25, 2005 at 12:07:02PM +0100, Prof Brian Ripley wrote:
> On Sun, 25 Sep 2005, Andrew Robinson wrote:
> 
> > I have constructed a package using package.skeleton(), when I try
> >
> > $ R CMD build foo
> > * checking for file 'foo/DESCRIPTION' ... OK
> > * preparing 'foo':
> > * checking DESCRIPTION meta-information ... OK
> > * cleaning src
> > * removing junk files
> > tar: Option -L is not permitted in mode -x
> > Error: cannot open file 'foo/DESCRIPTION' for reading
> >
> > foo/DESCRIPTION exists and the permissions are correct. The same
> > command works under Linux Fedora 2.  The man pages on each OS imply
> > that tar differs across the two platforms.  Does anyone have any
> > thoughts on a work-around?
> 
> No, because R does not use tar -L (which is to do with tape lengths on GNU
> tar).
> 
> It does use tar chf and tar xhf.  The h modifier would appear to be
> applicable only to dumps, so at a wild guess the error message means -h is
> not permitted.  Try replacing xhf by xf.

Yes, that works.  For the record: I removed the 'h' options from all
tar calls in 

/usr/local/lib/R/bin/build

Thanks for your speedy and accurate response.

Andrew
-- 
Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344-4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au



From ripley at stats.ox.ac.uk  Sun Sep 25 15:05:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Sun, 25 Sep 2005 14:05:08 +0100 (GMT Standard Time)
Subject: [R] R CMD build produces tar error under FreeBSD 5.4
In-Reply-To: <20050925115832.GI42964@ms.unimelb.edu.au>
References: <20050925074451.GF42964@ms.unimelb.edu.au>
	<Pine.GSO.4.31.0509251202000.18680-100000@toucan.stats>
	<20050925115832.GI42964@ms.unimelb.edu.au>
Message-ID: <Pine.WNT.4.58.0509251403470.4032@Petrel>

On Sun, 25 Sep 2005, Andrew Robinson wrote:

> On Sun, Sep 25, 2005 at 12:07:02PM +0100, Prof Brian Ripley wrote:
> > On Sun, 25 Sep 2005, Andrew Robinson wrote:
> >
> > > I have constructed a package using package.skeleton(), when I try
> > >
> > > $ R CMD build foo
> > > * checking for file 'foo/DESCRIPTION' ... OK
> > > * preparing 'foo':
> > > * checking DESCRIPTION meta-information ... OK
> > > * cleaning src
> > > * removing junk files
> > > tar: Option -L is not permitted in mode -x
> > > Error: cannot open file 'foo/DESCRIPTION' for reading
> > >
> > > foo/DESCRIPTION exists and the permissions are correct. The same
> > > command works under Linux Fedora 2.  The man pages on each OS imply
> > > that tar differs across the two platforms.  Does anyone have any
> > > thoughts on a work-around?
> >
> > No, because R does not use tar -L (which is to do with tape lengths on GNU
> > tar).
> >
> > It does use tar chf and tar xhf.  The h modifier would appear to be
> > applicable only to dumps, so at a wild guess the error message means -h is
> > not permitted.  Try replacing xhf by xf.
>
> Yes, that works.  For the record: I removed the 'h' options from all
> tar calls in

You do in principle want them for the chf calls.

>
> /usr/local/lib/R/bin/build
>
> Thanks for your speedy and accurate response.
>
> Andrew

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ajayshah at mayin.org  Sun Sep 25 16:41:09 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sun, 25 Sep 2005 20:11:09 +0530
Subject: [R] Question on lm(): When does R-squared come out as NA?
Message-ID: <20050925144109.GT437@lubyanka.local>

I have a situation with a large dataset (3000+ observations), where
I'm doing lags as regressors, where I get:

Call:
lm(formula = rj ~ rM + rM.1 + rM.2 + rM.3 + rM.4)

Residuals:
1990-06-04 1994-11-14 1998-08-21 2002-03-13 2005-09-15 
  -5.64672   -0.59596   -0.04143    0.55412    8.18229 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.003297   0.017603  -0.187    0.851    
rM           0.845169   0.010522  80.322   <2e-16 ***
rM.1         0.116330   0.010692  10.880   <2e-16 ***
rM.2         0.002044   0.010686   0.191    0.848    
rM.3         0.013181   0.010692   1.233    0.218    
rM.4         0.009587   0.010525   0.911    0.362    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 1.044 on 3532 degrees of freedom
Multiple R-Squared:    NA,	Adjusted R-squared:    NA 
F-statistic:    NA on 5 and 3532 DF,  p-value: NA 


rM.1, rM.2, etc. are lagged values of rM. The OLS seems fine in every
respect, except that there is an NA as the multiple R-squared. I will
be happy to give sample data to someone curious about what is going
on. I wondered if this was a well-known pathology. The way I know it,
if the data allows computation of (X'X)^{-1}, one can compute the R2.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From spencer.graves at pdf.com  Sun Sep 25 17:03:32 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 25 Sep 2005 10:03:32 -0500
Subject: [R] anova on binomial LMER objects
In-Reply-To: <43330B51.3070509@shef.ac.uk>
References: <43330B51.3070509@shef.ac.uk>
Message-ID: <4336BC44.8070404@pdf.com>

	  I agree:  Something looks strange to me in this example also;  I have 
therefore copied Douglas Bates and  Deepayan Sarkar.  You've provided a 
nice simulation.  If either of them have time to look at this, I think 
they could tell us what is happening here.

	  If you need an answer to your particular problem, you could run that 
simulation 1000 or 1,000 times.  That would tell you whether to believe 
the summary or the anova, or neither.  If you want to understand the 
algorithm, you could walk through the code.  However, "lmer" is a 
generic, and I don't have time now to figure out how to find the source. 
  A response from Brian Ripley to a question from me a couple of days 
ago provides a nice summary of how to do that, but I don't have time to 
check that now.

	  Sorry I couldn't help more.
	  spencer graves

Robert Bagchi wrote:

> Dear R users,
> 
> I have been having problems getting believable estimates from anova on a 
> model fit from lmer. I get the impression that F is being greatly 
> underestimated, as can be seen by running the example I have given below.
> 
> First an explanation of what I'm trying to do. I am trying to fit a glmm 
> with binomial errors to some data. The experiment involves 10 
> shadehouses, divided between 2 light treatments (high, low). Within each 
> shadehouse there are 12 seedlings of each of 2 species (hn & sl). 3 
> damage treatments (0, 0.1, 0.25 leaf area removal) were applied to the 
> seedlings (at random) so that there are 4 seedlings of each 
> species*damage treatment in each shadehouse.  There maybe a shadehouse 
> effect, so I need to include it as a random effect. Light is applied to 
> a shadehouse, so it is outer to shadehouse. The other 2 factors are 
> inner to shadehouse.
> 
> We want to assess if light, damage and species affect survival of 
> seedlings. To test this I fitted a binomial mixed effects model with 
> lmer (actually with quasibinomial errors). THe summary function suggests 
> a large effect of both light and species (which agrees with graphical 
> analysis). However, anova produces F values close to 0 and p values 
> close to 1 (see example below).
> 
> Is this a bug, or am I doing something fundamentally wrong? If anova 
> doesn't work with lmer is there a way to perform hypothesis tests on 
> fixed effects in an lmer model? I was going to just delete terms and 
> then do liklihood ratio tests, but according to Pinheiro & Bates (p. 87) 
> that's very untrustworthy. Any suggestions?
> 
> I'm using R 2.1.1 on windows XP and lme4 0.98-1
> 
> Any help will be much appreciated.
> 
> many thanks
> Robert
> 
> ###############################
> The data are somewhat like this
> 
> #setting up the dataframe
> 
> bm.surv<-data.frame(
>                     house=rep(1:10, each=6),
>                     light=rep(c("h", "l"), each=6, 5),
>                     species=rep(c("sl", "hn"), each=3, 10),
>                     damage=rep(c(0,.1,.25), 20)
>                     )
> 
> bm.surv$survival<-ifelse(bm.surv$light=="h", rbinom(60, 4, .9), 
> rbinom(60, 4, .6))       # difference in probablility should ensure a 
> light effect
> bm.surv$death<-4-bm.surv$survival
> 
> # fitting the model
> m1<-lmer(cbind(survival, death)~light+species+damage+(1|house), 
> data=bm.surv, family="quasibinomial")
> 
> summary(m1)         # suggests that light is very significant
> Generalized linear mixed model fit using PQL
> Formula: cbind(survival, death) ~ light + species + damage + (1 | table)
>    Data: bm.surv
>  Family: quasibinomial(logit link)
>       AIC      BIC    logLik deviance
>  227.0558 239.6218 -107.5279 215.0558
> Random effects:
>  Groups   Name        Variance   Std.Dev. 
>  table    (Intercept) 1.8158e-09 4.2613e-05
>  Residual             3.6317e+00 1.9057e+00
> # of obs: 60, groups: table, 10
> 
> Fixed effects:
>             Estimate Std. Error DF t value  Pr(>|t|)   
> (Intercept)  2.35140    0.36832 56  6.3841 3.581e-08 ***
> lightl      -1.71517    0.33281 56 -5.1535 3.447e-06 ***
> speciessl   -0.57418    0.30085 56 -1.9085   0.06145 . 
> damage       1.49963    1.46596 56  1.0230   0.31072   
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Correlation of Fixed Effects:
>           (Intr) lightl spcssl
> lightl    -0.665             
> speciessl -0.494  0.070      
> damage    -0.407 -0.038 -0.017
> 
> 
> anova(m1)             # very low F value for light, corresponding to p 
> values approaching 1
> 
> Analysis of Variance Table
>         Df Sum Sq Mean Sq  Denom F value Pr(>F)
> light    1  0.014   0.014 56.000  0.0018 0.9661
> species  1  0.002   0.002 56.000  0.0002 0.9887
> damage   1  0.011   0.011 56.000  0.0014 0.9704
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From sfalcon at fhcrc.org  Sun Sep 25 19:19:02 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Sun, 25 Sep 2005 10:19:02 -0700
Subject: [R] Indentation in R code
In-Reply-To: <148ed818050924130247520493@mail.gmail.com> (=?iso-8859-1?Q?G?=
	=?iso-8859-1?Q?=F6ran_Brostr=F6m's?=
	message of "Sat, 24 Sep 2005 22:02:54 +0200")
References: <148ed818050924130247520493@mail.gmail.com>
Message-ID: <m2u0g93w1l.fsf@macaroni.local>

On 24 Sep 2005, goran.brostrom at gmail.com wrote:

> I am using emacs-21.3 when writing R functions on Linux debian, and
> I am trying to follow the advice i R-exts.pdf (2.1.1) regarding
> indentation. That is, I set 'c-default-style' to "bsd" and
> 'c-basic-offset' to 4. However, while this gives me the intended
> indentation in C code, it doesn't change the behavior in R code; I
> still get an indentation of size 2. This is my .emacs file after
> customization:
>
> (require 'ess-site)
> (custom-set-variables
> ;; custom-set-variables was added by Custom -- don't edit or
> ;; cut/paste it!  Your init file should contain only one such
> ;; instance.
> '(c-basic-offset 4)
> '(c-default-style "bsd"))
> (custom-set-faces
> ;; custom-set-faces was added by Custom -- don't edit or cut/paste it!
> ;; Your init file should contain only one such instance.
> )

Not sure if this is the best way, but I have the following after
loading ess-site:

(setq ess-indent-level 4)


+ seth



From p.dalgaard at biostat.ku.dk  Sun Sep 25 19:40:45 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Sep 2005 19:40:45 +0200
Subject: [R] Indentation in R code
In-Reply-To: <m2u0g93w1l.fsf@macaroni.local>
References: <148ed818050924130247520493@mail.gmail.com>
	<m2u0g93w1l.fsf@macaroni.local>
Message-ID: <x2u0g9nizm.fsf@turmalin.kubism.ku.dk>

Seth Falcon <sfalcon at fhcrc.org> writes:

> On 24 Sep 2005, goran.brostrom at gmail.com wrote:
> 
> > I am using emacs-21.3 when writing R functions on Linux debian, and
> > I am trying to follow the advice i R-exts.pdf (2.1.1) regarding
> > indentation. That is, I set 'c-default-style' to "bsd" and
> > 'c-basic-offset' to 4. However, while this gives me the intended
> > indentation in C code, it doesn't change the behavior in R code; I
> > still get an indentation of size 2. This is my .emacs file after
> > customization:
> >
> > (require 'ess-site)
> > (custom-set-variables
> > ;; custom-set-variables was added by Custom -- don't edit or
> > ;; cut/paste it!  Your init file should contain only one such
> > ;; instance.
> > '(c-basic-offset 4)
> > '(c-default-style "bsd"))
> > (custom-set-faces
> > ;; custom-set-faces was added by Custom -- don't edit or cut/paste it!
> > ;; Your init file should contain only one such instance.
> > )
> 
> Not sure if this is the best way, but I have the following after
> loading ess-site:
> 
> (setq ess-indent-level 4)
> 

I have (I believe it stems from Martin M. originally):

(add-hook 'c-mode-hook '(lambda()
                          (c-set-style "stroustrup")))
(add-hook 'ess-mode-hook
      '(lambda()
         (if (or (string< ess-version "5.0")
                 (string= ess-version "5.0"))
             (ess-set-style 'C++)
           (ess-set-style 'C++ 'quiet))

         (add-hook 'local-write-file-hooks
                   '(lambda()
                      (delete-trailing-whitespace)
                      ))
         ))


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From wpogoda at coba.usf.edu  Sun Sep 25 20:02:09 2005
From: wpogoda at coba.usf.edu (Pogoda, Wendy)
Date: Sun, 25 Sep 2005 14:02:09 -0400
Subject: [R] xyplot main title question
Message-ID: <AC6230A5F7852248946CFC481739A95E37AD07@tiki.fastmail.usf.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050925/1c0ba227/attachment.pl

From deepayan.sarkar at gmail.com  Sun Sep 25 20:34:19 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sun, 25 Sep 2005 13:34:19 -0500
Subject: [R] xyplot main title question
In-Reply-To: <AC6230A5F7852248946CFC481739A95E37AD07@tiki.fastmail.usf.edu>
References: <AC6230A5F7852248946CFC481739A95E37AD07@tiki.fastmail.usf.edu>
Message-ID: <eb555e660509251134508ad54b@mail.gmail.com>

On 9/25/05, Pogoda, Wendy <wpogoda at coba.usf.edu> wrote:
> When I use the xyplot function (lattice library) the titles are too long
> (i.e. part of the title are cut off).  This fixes if I maximize the plot,
> but I would like the user of the program to not have to manually maximize
> the plotting window.  I have tried to shrink the main title by using the
> "par(cex.main=.7)" before the xyplot function, but it seems any prior par
> options are ignored by xyplot.  There is an option inside xyplot called
> "par.settings", but I have been unable to figure out how to make it change
> the main title text size.
>
> So, there are 2 possible solutions to my problem: 1) if there a text command
> to maximize the graphing window, what is it? or

None that I know of (though you can specify the width and height when
starting a device).

> 2) how can I change the main title text size in the xyplot function?

xyplot( ... ,
       par.settings = list(par.main.text = list(cex = 0.7)))

Deepayan



From dieter.menne at menne-biomed.de  Sun Sep 25 20:32:55 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 25 Sep 2005 18:32:55 +0000 (UTC)
Subject: [R] xyplot main title question
References: <AC6230A5F7852248946CFC481739A95E37AD07@tiki.fastmail.usf.edu>
Message-ID: <loom.20050925T202635-130@post.gmane.org>

Pogoda, Wendy <wpogoda <at> coba.usf.edu> writes:

> 
> When I use the xyplot function (lattice library) the titles are too long 
....
How can I change the main title text size in the xyplot function? . 

This IS confusing. General rule: whenever you use a function from 
lattice/trellis, par(...) is the wrong way to go. Use trellis.par.get and 
trellis.par set instead.

library(lattice)

mt = trellis.par.get("par.main.text")
mt$cex = 0.7
# may also set other options, such as font.
trellis.par.set("par.main.text",mt)

xyplot(lat ~ long , data = quakes,
  main="This is a very long text This is a very long text ")

----
My way if lost in lattice:

trellis.par.get()

Copy the output you to a text file, and keep it on your disk. Whenever you want 
to set a graphics options, search that file for something that comes close, and 
use the above get/set. 
Don't forget to update the file on lattice updates, these settings change.
Use show.settings() helps sometimes, but it's not complete.

Dieter



From ggrothendieck at gmail.com  Sun Sep 25 21:27:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 25 Sep 2005 15:27:41 -0400
Subject: [R] grid.locator
Message-ID: <971536df050925122745383d02@mail.gmail.com>

If I do:

library(lattice)
x <- 1:10
xyplot(x ~ x)
grid.locator()

and click on 2,2 say the I get this:

> grid.locator()
$x
[1] 201native

$y
[1] 476native

How do I get the user coordinates of the graph?

(If I did not use lattice/grid I could have just done:

plot(x ~ x)
locator()

and it would have worked as expected.)



From p.murrell at auckland.ac.nz  Sun Sep 25 22:21:26 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 26 Sep 2005 08:21:26 +1200
Subject: [R] grid.locator
References: <971536df050925122745383d02@mail.gmail.com>
Message-ID: <433706C6.2050304@stat.auckland.ac.nz>

Hi


Gabor Grothendieck wrote:
> If I do:
> 
> library(lattice)
> x <- 1:10
> xyplot(x ~ x)


After the lattice plot, you are back at the top-level grid viewport.


> grid.locator()
> 
> and click on 2,2 say the I get this:
> 
> 
>>grid.locator()
> 
> $x
> [1] 201native
> 
> $y
> [1] 476native


grid.locator() by default gives you the click in "native" coordinates. 
The "native" coordinates for the top-level viewport depend on the 
device;  on-screen it is pixel-based.


> How do I get the user coordinates of the graph?


Look up the lattice function trellis.focus().

Paul


> (If I did not use lattice/grid I could have just done:
> 
> plot(x ~ x)
> locator()
> 
> and it would have worked as expected.)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From nepossiver at yahoo.com  Sun Sep 25 23:05:33 2005
From: nepossiver at yahoo.com (Horacio Montenegro)
Date: Sun, 25 Sep 2005 14:05:33 -0700 (PDT)
Subject: [R] anova on binomial LMER objects
In-Reply-To: <4336BC44.8070404@pdf.com>
Message-ID: <20050925210533.44547.qmail@web50610.mail.yahoo.com>


    Hi Spencer and Robert,

    I have found the same behaviour, but only for lme4
and Matrix after the 0.96 release. lme4 0.95-10 and
Matrix 0.95-13 releases gave "sensible" results. This
could be an introduced bug, or a solved bug - you
should ask Prof. Bates.

    hope this helps, cheers,

    Horacio Montenegro

--- Spencer Graves <spencer.graves at pdf.com> wrote:
> 	  I agree:  Something looks strange to me in this
> example also;  I have 
> therefore copied Douglas Bates and  Deepayan Sarkar.
>  You've provided a 
> nice simulation.  If either of them have time to
> look at this, I think 
> they could tell us what is happening here.
> 
> 	  If you need an answer to your particular problem,
> you could run that 
> simulation 1000 or 1,000 times.  That would tell you
> whether to believe 
> the summary or the anova, or neither.  If you want
> to understand the 
> algorithm, you could walk through the code. 
> However, "lmer" is a 
> generic, and I don't have time now to figure out how
> to find the source. 
>   A response from Brian Ripley to a question from me
> a couple of days 
> ago provides a nice summary of how to do that, but I
> don't have time to 
> check that now.
> 
> 	  Sorry I couldn't help more.
> 	  spencer graves
> 
> Robert Bagchi wrote:
> 
> > Dear R users,
> > 
> > I have been having problems getting believable
> estimates from anova on a 
> > model fit from lmer. I get the impression that F
> is being greatly 
> > underestimated, as can be seen by running the
> example I have given below.
> > 
> > First an explanation of what I'm trying to do. I
> am trying to fit a glmm 
> > with binomial errors to some data. The experiment
> involves 10 
> > shadehouses, divided between 2 light treatments
> (high, low). Within each 
> > shadehouse there are 12 seedlings of each of 2
> species (hn & sl). 3 
> > damage treatments (0, 0.1, 0.25 leaf area removal)
> were applied to the 
> > seedlings (at random) so that there are 4
> seedlings of each 
> > species*damage treatment in each shadehouse. 
> There maybe a shadehouse 
> > effect, so I need to include it as a random
> effect. Light is applied to 
> > a shadehouse, so it is outer to shadehouse. The
> other 2 factors are 
> > inner to shadehouse.
> > 
> > We want to assess if light, damage and species
> affect survival of 
> > seedlings. To test this I fitted a binomial mixed
> effects model with 
> > lmer (actually with quasibinomial errors). THe
> summary function suggests 
> > a large effect of both light and species (which
> agrees with graphical 
> > analysis). However, anova produces F values close
> to 0 and p values 
> > close to 1 (see example below).
> > 
> > Is this a bug, or am I doing something
> fundamentally wrong? If anova 
> > doesn't work with lmer is there a way to perform
> hypothesis tests on 
> > fixed effects in an lmer model? I was going to
> just delete terms and 
> > then do liklihood ratio tests, but according to
> Pinheiro & Bates (p. 87) 
> > that's very untrustworthy. Any suggestions?
> > 
> > I'm using R 2.1.1 on windows XP and lme4 0.98-1
> > 
> > Any help will be much appreciated.
> > 
> > many thanks
> > Robert
> > 
> >



From ggrothendieck at gmail.com  Mon Sep 26 00:23:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 25 Sep 2005 18:23:30 -0400
Subject: [R] grid.locator
In-Reply-To: <433706C6.2050304@stat.auckland.ac.nz>
References: <971536df050925122745383d02@mail.gmail.com>
	<433706C6.2050304@stat.auckland.ac.nz>
Message-ID: <971536df05092515237fa1a887@mail.gmail.com>

Thanks.  Following the example there worked.

I suggest adding trellis.focus to the See Also in ?grid.locator


On 9/25/05, Paul Murrell <p.murrell at auckland.ac.nz> wrote:
> Hi
>
>
> Gabor Grothendieck wrote:
> > If I do:
> >
> > library(lattice)
> > x <- 1:10
> > xyplot(x ~ x)
>
>
> After the lattice plot, you are back at the top-level grid viewport.
>
>
> > grid.locator()
> >
> > and click on 2,2 say the I get this:
> >
> >
> >>grid.locator()
> >
> > $x
> > [1] 201native
> >
> > $y
> > [1] 476native
>
>
> grid.locator() by default gives you the click in "native" coordinates.
> The "native" coordinates for the top-level viewport depend on the
> device;  on-screen it is pixel-based.
>
>
> > How do I get the user coordinates of the graph?
>
>
> Look up the lattice function trellis.focus().
>
> Paul
>
>
> > (If I did not use lattice/grid I could have just done:
> >
> > plot(x ~ x)
> > locator()
> >
> > and it would have worked as expected.)



From Mike.Prager at noaa.gov  Mon Sep 26 01:12:54 2005
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Sun, 25 Sep 2005 19:12:54 -0400
Subject: [R] Tip: Working directory in titlebar
Message-ID: <43372EF6.6030206@noaa.gov>

I hope others may find this useful.  I use R in many different 
directories.  By adding the following line to the .Rprofile file:

utils:::setWindowTitle(paste("-",getwd()))

the titlebar of each R instance shows the directory from which it was 
started. I have tested this with rterm.exe and rgui.exe under Windows 
and with RECENT versions of R only.

-----

For changing directories during a session, I use the following function, 
a slight modification of Andy Liaw's cd() function that also changes the 
titlebar:

cd <- function(dir = tclvalue(tkchooseDirectory()), saveOld=NA, 
loadNew=TRUE) {
    # From Andy Liaw  ... additions by MHP
    # Change the working directory during an R session
    #
    # First check that the tcl-tk package is available:
    stopifnot(require(tcltk))
    # Print any error message from trying to load:
    if (.Platform$OS.type=="windows") flush.console()  ## MHP
    # Warn user if saving existing workspace was not chosen: (MHP)
    if (is.na(saveOld)) {
      tmp = tclvalue(tkmessageBox(message="Save current workspace image?",
         type="yesnocancel",title="Changing directories",icon="question"))
         saveOld = switch(tmp, "yes"=TRUE, "no"=FALSE, 
"cancel"=return(invisible(NULL)))
         }
    # Save old workspace:
    if (saveOld) save.image(compress=TRUE)
    # Change directory:
    # Note: the need to evaluate "dir" on the next line causes the 
"tclvalue" call
    #    in the function definition (first line) to take place now:
    if (dir=="") return(invisible(NULL))  # Fail gracefully if "CANCEL" 
chosen from TK dialog (MHP)
    setwd(dir)
    # Remove all the data from the old working directory:
    rm(list=ls(all=TRUE, envir=.GlobalEnv), envir=.GlobalEnv)
    # Change the window title:
    setWindowTitle(paste("-",dir))
    # Load the new data:
    if (loadNew) {
         if (file.exists(".RData")) {
            loaded <- load(".RData", envir=.GlobalEnv)
            return(invisible(loaded))
         }
         else {
            cat("Warning: .RData not found in new directory\n")
            return(invisible(FALSE))
         }
    }
}

-----

Although both these bits of code have been used, they may have errors.  
Comments and corrections welcome.

-- 
Michael H. Prager, Ph.D.
Population Dynamics Team
NOAA Center for Coastal Habitat and Fisheries Research
NMFS Southeast Fisheries Science Center
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/



From hastie at stanford.edu  Mon Sep 26 04:17:37 2005
From: hastie at stanford.edu (Trevor Hastie)
Date: Sun, 25 Sep 2005 19:17:37 -0700
Subject: [R] Data Modeling Short Course
Message-ID: <B48EB431-3C5E-48D1-80E7-B067F9468893@stanford.edu>

Short course: Statistical Learning and Data Mining II:
                 tools for tall and wide data

Trevor Hastie and Robert Tibshirani, Stanford University

The Conference Center at Harvard Medical School
Boston, MA,
Oct 31-Nov 1, 2005

This is a *new* two-day course on statistical models for data mining,
inference and prediction. It is the third in a series, and follows our
past offerings "Modern Regression and Classification", and
"Statistical Learning and Data Mining".

In this course we emphasize the tools useful for tackling modern-day
data analysis problems. We focus on both "tall" data ( N>p where
N=#cases, p=#features) and "wide" data (p>N). The tools include
gradient boosting, SVMs and kernel methods, random forests, lasso and
LARS, ridge regression and GAMs, supervised principal components, and
cross-validation.  We also present some interesting case studies in a
variety of application areas. All our examples are developed using the
S language, and most of the procedures we discuss are implemented in
publically available R packages.

Please visit the site
http://www-stat.stanford.edu/~hastie/sldm.html
for more information and registration details.

-------------------------------------------------------------------
   Trevor Hastie                                   hastie at stanford.edu
   Professor, Department of Statistics, Stanford University
   Phone: (650) 725-2231 (Statistics)          Fax: (650) 725-8977
   (650) 498-5233 (Biostatistics)   Fax: (650) 725-6951
   URL: http://www-stat.stanford.edu/~hastie
    address: room 104, Department of Statistics, Sequoia Hall
            390 Serra Mall, Stanford University, CA 94305-4065
  --------------------------------------------------------------------



From tony.meissner at bigpond.com  Mon Sep 26 07:14:24 2005
From: tony.meissner at bigpond.com (Tony Meissner)
Date: Mon, 26 Sep 2005 15:14:24 +1000
Subject: [R] nls and na/Nan/Inf error
Message-ID: <433783B0.3030109@bigpond.com>

I am trying to it a particular nonlinear model common in Soil Science to 
moisture release data from soil.  I have written the function as shown 
below according to the logist example in Ch8 of Pinheiro & Bates.  I am 
getting the following error (R version 2.1.1)

*Error in qr(attr(rhs, "gradient")) : NA/NaN/Inf in foreign function 
call (arg 1)*

Below is the function and data.

/# the van genuchten moisture release function
vanGen <- function(x, Vr, Vm, alpha, lamda) {
  if (Vr < 0) Vr <- 0
  Vr + (Vm - Vr)/((1+(alpha*x)^lamda)^(1-1/lamda))
  }
vanGen <- deriv(~Vr + (Vm - Vr)/((1+(alpha*x)^lamda)^(1-1/lamda)),
  c("Vr", "Vm", "alpha", "lamda"), function(x, Vr, Vm, alpha, lamda) {} )/


the call in R

/> fm1fld.nls <- nls(Moisture ~ vanGen(Suction, Vr,Vm,alpha,lamda),
+ data=fldgd, start=c(Vr=0.229, Vm=0.433, alpha=0.2, lamda=1.5))

/and the data:/

/*   Suction Moisture
1        0    0.433
2        1    0.421
3        4    0.400
4       10    0.379
5       20    0.366
6       30    0.362
7       40    0.358
8       50    0.353
9       60    0.351
10      70    0.349
*/
/can anyone offer any suggestions.  The parameters Vr, Vm >= 0,  alpha > 
0, and
lamda > 1



From ggrothendieck at gmail.com  Mon Sep 26 09:04:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Sep 2005 03:04:26 -0400
Subject: [R] nls and na/Nan/Inf error
In-Reply-To: <433783B0.3030109@bigpond.com>
References: <433783B0.3030109@bigpond.com>
Message-ID: <971536df05092600046bb9c096@mail.gmail.com>

Use a grid search to get the starting values in which case you
will likely be close enough that you won't run into problems
even without derivatives:

attach(fldgd)
grid <- expand.grid(Vr = seq(0,.3,.1), Vm = seq(.45, 1, .05),
	alpha = seq(1,2,.25), lamda = seq(1,2,.25))
ss <- function(p) sum((Moisture - vanGen(Suction, p[1], p[2], p[3], p[4]))^2)
idx  <- which.min(apply(grid, 1, ss))
startval <- grid[idx,]
nls(Moisture ~ vanGen(Suction, Vr, Vm, alpha, lamda), start = startval)


On 9/26/05, Tony Meissner <tony.meissner at bigpond.com> wrote:
> I am trying to it a particular nonlinear model common in Soil Science to
> moisture release data from soil.  I have written the function as shown
> below according to the logist example in Ch8 of Pinheiro & Bates.  I am
> getting the following error (R version 2.1.1)
>
> *Error in qr(attr(rhs, "gradient")) : NA/NaN/Inf in foreign function
> call (arg 1)*
>
> Below is the function and data.
>
> /# the van genuchten moisture release function
> vanGen <- function(x, Vr, Vm, alpha, lamda) {
>  if (Vr < 0) Vr <- 0
>  Vr + (Vm - Vr)/((1+(alpha*x)^lamda)^(1-1/lamda))
>  }
> vanGen <- deriv(~Vr + (Vm - Vr)/((1+(alpha*x)^lamda)^(1-1/lamda)),
>  c("Vr", "Vm", "alpha", "lamda"), function(x, Vr, Vm, alpha, lamda) {} )/
>
>
> the call in R
>
> /> fm1fld.nls <- nls(Moisture ~ vanGen(Suction, Vr,Vm,alpha,lamda),
> + data=fldgd, start=c(Vr=0.229, Vm=0.433, alpha=0.2, lamda=1.5))
>
> /and the data:/
>
> /*   Suction Moisture
> 1        0    0.433
> 2        1    0.421
> 3        4    0.400
> 4       10    0.379
> 5       20    0.366
> 6       30    0.362
> 7       40    0.358
> 8       50    0.353
> 9       60    0.351
> 10      70    0.349
> */
> /can anyone offer any suggestions.  The parameters Vr, Vm >= 0,  alpha >
> 0, and
> lamda > 1
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Mon Sep 26 09:27:56 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 26 Sep 2005 09:27:56 +0200
Subject: [R] Indentation in R code
In-Reply-To: <x2u0g9nizm.fsf@turmalin.kubism.ku.dk>
References: <148ed818050924130247520493@mail.gmail.com>
	<m2u0g93w1l.fsf@macaroni.local>
	<x2u0g9nizm.fsf@turmalin.kubism.ku.dk>
Message-ID: <17207.41724.23980.747435@stat.math.ethz.ch>

I'm crossposting to the ESS-help mailing list which is slightly
more appropriate here.  [This may be a rare case where
crossposting seems to make much sense.]

>>>>> "PD" == Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>>>>>     on 25 Sep 2005 19:40:45 +0200 writes:

    PD> Seth Falcon <sfalcon at fhcrc.org> writes:

    >> On 24 Sep 2005, goran.brostrom at gmail.com wrote:
    >> 
    >> > I am using emacs-21.3 when writing R functions on Linux debian, and
    >> > I am trying to follow the advice i R-exts.pdf (2.1.1) regarding
    >> > indentation. That is, I set 'c-default-style' to "bsd" and
    >> > 'c-basic-offset' to 4. However, while this gives me the intended
    >> > indentation in C code, it doesn't change the behavior in R code; I
    >> > still get an indentation of size 2. This is my .emacs file after
    >> > customization:
    >> >
    >> > (require 'ess-site)
    >> > (custom-set-variables
    >> >  ;; custom-set-variables was added by Custom -- don't edit or
    >> >  ;; cut/paste it!  Your init file should contain only one such
    >> >  ;; instance.
    >> >  '(c-basic-offset 4)
    >> >  '(c-default-style "bsd"))
    >> >  (custom-set-faces
    >> >  ;; custom-set-faces was added by Custom -- don't edit or cut/paste it!
    >> >  ;; Your init file should contain only one such instance.
    >> > )
    >> 
    >> Not sure if this is the best way, but I have the following after
    >> loading ess-site:
    >> 
    >> (setq ess-indent-level 4)



    PD> I have (I believe it stems from Martin M. originally):

yes, most probably  {IIRC, Kurt Hornik was involved too}.

    PD>      (add-hook 'c-mode-hook '(lambda()
    PD> 			       (c-set-style "stroustrup")))

the above is not quite what I have or did recommend, 
which is rather  "bsd" + "offset 4"  as G??ran has above

In fact, G??ran mentions the "R-exts" manual and that has
the following  *before* giving the emacs-lisp statements:

>> (For GNU Emacs 20: for GNU Emacs 21 use
>> customization to set the `c-default-style' to `"bsd"' and
>> `c-basic-offset' to `4'.)

and indeed, that's what G??ran did and you should do with a
current emacs, either customize via GUI or,
in your ~/.emacs file, find the section '(custom-set-variables ...)' and add 
 '(c-basic-offset 4)
 '(c-default-style "bsd")

to the lines already there, or if there's no such section, add

(custom-set-variables
  ;; custom-set-variables was added by Custom -- don't edit or cut/paste it!
  ;; Your init file should contain only one such instance.
 '(c-basic-offset 4)
 '(c-default-style "bsd")
)
to the end of your ~/.emacs file

    PD>      (add-hook 'ess-mode-hook
    PD> 	   '(lambda()
    PD> 	      (if (or (string< ess-version "5.0")
    PD> 		      (string= ess-version "5.0"))
    PD> 		  (ess-set-style 'C++)
    PD> 		(ess-set-style 'C++ 'quiet))
    PD> 
    PD> 	      (add-hook 'local-write-file-hooks
    PD> 			'(lambda()
    PD> 			   (delete-trailing-whitespace)
    PD> 			   ))
    PD> 	      ))

yes; using  (add-hook .......) is really more clean than first
requiring ess-site.
Also, since nowadays I assume everyone has an ess-version >= 5, 
the above becomes simply

    (add-hook 'ess-mode-hook
              '(lambda()
		  (ess-set-style 'C++ 'quiet)
		  (add-hook 'local-write-file-hooks
			    '(lambda()
			       (delete-trailing-whitespace)))))

Note that this has the standard e-lisp function
'delete-trailing-whitespace' which is simpler but also less
flexible than the 'ess-nuke-trailing-whitespace' which we've had
there.

Martin Maechler, ETH Zurich



From ripley at stats.ox.ac.uk  Mon Sep 26 09:28:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Sep 2005 08:28:24 +0100 (BST)
Subject: [R] nls and na/Nan/Inf error
In-Reply-To: <433783B0.3030109@bigpond.com>
Message-ID: <Pine.GSO.4.31.0509260815500.19625-100000@toucan.stats>

This works if you omit the deriv() step.

Use R's options(error=dump.frames) and debugger().  This gives

Browse[1]> rhs
 [1] 0.4330000 0.4272571 0.3994105 0.3594037 0.3270730 0.3104752 0.3000927
 [8] 0.2928445 0.2874249 0.2831787
attr(,"gradient")
              Vr        Vm       alpha        lamda
 [1,] 0.00000000 1.0000000  0.00000000          NaN
 [2,] 0.02815158 0.9718484 -0.04069202  0.001183749
 [3,] 0.16465431 0.8353457 -0.17769291 -0.035591190
 [4,] 0.36076599 0.6392340 -0.24085444 -0.100064577
 [5,] 0.51925014 0.4807499 -0.21793994 -0.136056450
 [6,] 0.60061200 0.3993880 -0.19071160 -0.145267481
 [7,] 0.65150658 0.3484934 -0.17020938 -0.147113828
 [8,] 0.68703698 0.3129630 -0.15471851 -0.146388612
 [9,] 0.71360324 0.2863968 -0.14263118 -0.144660967
[10,] 0.73441811 0.2655819 -0.13290951 -0.142543261

and note the NaN.  Now think about your formula for x = 0: it does not
actually depend on lamda.  The analytical derivative ends up with a
calculation as 0/0.

On Mon, 26 Sep 2005, Tony Meissner wrote:

> I am trying to it a particular nonlinear model common in Soil Science to
> moisture release data from soil.  I have written the function as shown
> below according to the logist example in Ch8 of Pinheiro & Bates.  I am
> getting the following error (R version 2.1.1)
>
> *Error in qr(attr(rhs, "gradient")) : NA/NaN/Inf in foreign function
> call (arg 1)*
>
> Below is the function and data.
>
> /# the van genuchten moisture release function
> vanGen <- function(x, Vr, Vm, alpha, lamda) {
>   if (Vr < 0) Vr <- 0
>   Vr + (Vm - Vr)/((1+(alpha*x)^lamda)^(1-1/lamda))
>   }
> vanGen <- deriv(~Vr + (Vm - Vr)/((1+(alpha*x)^lamda)^(1-1/lamda)),
>   c("Vr", "Vm", "alpha", "lamda"), function(x, Vr, Vm, alpha, lamda) {} )/
>
>
> the call in R
>
> /> fm1fld.nls <- nls(Moisture ~ vanGen(Suction, Vr,Vm,alpha,lamda),
> + data=fldgd, start=c(Vr=0.229, Vm=0.433, alpha=0.2, lamda=1.5))
>
> /and the data:/
>
> /*   Suction Moisture
> 1        0    0.433
> 2        1    0.421
> 3        4    0.400
> 4       10    0.379
> 5       20    0.366
> 6       30    0.362
> 7       40    0.358
> 8       50    0.353
> 9       60    0.351
> 10      70    0.349
> */
> /can anyone offer any suggestions.  The parameters Vr, Vm >= 0,  alpha >
> 0, and
> lamda > 1

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Hummel at mpimp-golm.mpg.de  Mon Sep 26 10:13:53 2005
From: Hummel at mpimp-golm.mpg.de (Jan Hummel)
Date: Mon, 26 Sep 2005 10:13:53 +0200
Subject: [R] SAX Parser best practise
Message-ID: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E7A4@EMAIL.mpimp-golm.mpg.de>

Hi Duncan,

thanks again for your comments.

> I dug around in the libxml code and the Web to verify that 
> validation is indeed only possible in libxml when one uses 
> DOM (i.e. xmlTreeParse()).
Using DOM is not an option for me, so I need to "validate" the xml parts
I'm interested in within my creation mechanism. It's OK, but not the
best solution in questions of design.

> BTW, there is a new version of the XML package on the 
> Omegahat web site.
I'll use it extensive in this days and unfortunately I have already a
question/problem pending:

Taking the following R function:

test<-function(){
	sep=""
	xmlText <-""
	xmlText <-paste(xmlText,"<spectrum id=\"3257\">",sep=sep)
	xmlText <-paste(xmlText,"<mzArrayBinary>",sep=sep)
	xmlText <-paste(xmlText,"<data>Monday</data>",sep=sep)
	xmlText <-paste(xmlText,"</mzArrayBinary>",sep=sep)
	xmlText <-paste(xmlText,"<intenArrayBinary>",sep=sep)
	xmlText <-paste(xmlText,"<data>Tuesday</data>",sep=sep)
	xmlText <-paste(xmlText,"</intenArrayBinary>",sep=sep)
#	xmlText <-paste(xmlText,"</spectrum>",sep=sep)
#	xmlText <-paste(xmlText,"<spectrum id=\"3259\">",sep=sep)
	xmlText <-paste(xmlText,"<mzArrayBinary>",sep=sep)
	xmlText <-paste(xmlText,"<data>Wednesday</data>",sep=sep)
	xmlText <-paste(xmlText,"</mzArrayBinary>",sep=sep)
	xmlText <-paste(xmlText,"<intenArrayBinary>",sep=sep)
	xmlText <-paste(xmlText,"<data>Thursday</data>",sep=sep)
	xmlText <-paste(xmlText,"</intenArrayBinary>",sep=sep)
	xmlText <-paste(xmlText,"</spectrum>",sep=sep)

	xmlEventParse(xmlText, asText=TRUE, handlers = list(text =
function(x, ...) {cat(nchar(x),x, "\n")}))
	return(invisible(NULL))
}

Using this function in the given form works fine. xmlEventParse() with
the simplest handler I can imagine finds all 4 text-nodes within the
<spectrum> tag and prints them out. But if one uncomment both lines in
the middle, introducing 2 <spectrum> tags with different id's
xmlEventParse() returns with an exception. Of course the weekdays within
<data> are arbitrary values used here. Further, using an other input
file I could see, that for one and the same <data> node the handler for
"text"-nodes was invoked two times, one time for a first part of the
content and one time for the rest of the content. Both invocations
together gave me exactly the content from the <data> node. 

So, am I on the wrong way? Or is this some buggy behaviour? 

I appreciat any help and assistance!

Jan



From tmlammail at yahoo.com  Mon Sep 26 10:37:29 2005
From: tmlammail at yahoo.com (Martin Lam)
Date: Mon, 26 Sep 2005 01:37:29 -0700 (PDT)
Subject: [R]  How to get the rowindices without using which?
Message-ID: <20050926083729.94774.qmail@web40506.mail.yahoo.com>

Hi,

I was wondering if it is possible to get the
rowindices without using the function "which" because
I don't have a restriction criteria. Here's an example
of what I mean:
# take 10 randomly selected instances
iris[sample(1:nrow(iris), 10),]

# output
    Sepal.Length Sepal.Width Petal.Length Petal.Width 
  Species
76           6.6         3.0          4.4         1.4
versicolor
105          6.5         3.0          5.8         2.2 
virginica
131          7.4         2.8          6.1         1.9 
virginica
79           6.0         2.9          4.5         1.5
versicolor
69           6.2         2.2          4.5         1.5
versicolor
42           4.5         2.3          1.3         0.3 
   setosa
25           4.8         3.4          1.9         0.2 
   setosa
129          6.4         2.8          5.6         2.1 
virginica
60           5.2         2.7          3.9         1.4
versicolor
80           5.7         2.6          3.5         1.0
versicolor

What I want to get are their rownumbers: 76, 105, 131,
79, 69, 42, 25, 129, 60, 80.

Thanks in advance,

Martin



From dimitris.rizopoulos at med.kuleuven.be  Mon Sep 26 10:51:41 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 26 Sep 2005 10:51:41 +0200
Subject: [R] How to get the rowindices without using which?
References: <20050926083729.94774.qmail@web40506.mail.yahoo.com>
Message-ID: <00a101c5c277$83672940$0540210a@www.domain>

try this:

dat <- iris[sample(1:nrow(iris), 10), ]
dat
match(rownames(dat), rownames(iris))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Martin Lam" <tmlammail at yahoo.com>
To: "R" <r-help at stat.math.ethz.ch>
Sent: Monday, September 26, 2005 10:37 AM
Subject: [R] How to get the rowindices without using which?


> Hi,
>
> I was wondering if it is possible to get the
> rowindices without using the function "which" because
> I don't have a restriction criteria. Here's an example
> of what I mean:
> # take 10 randomly selected instances
> iris[sample(1:nrow(iris), 10),]
>
> # output
>    Sepal.Length Sepal.Width Petal.Length Petal.Width
>  Species
> 76           6.6         3.0          4.4         1.4
> versicolor
> 105          6.5         3.0          5.8         2.2
> virginica
> 131          7.4         2.8          6.1         1.9
> virginica
> 79           6.0         2.9          4.5         1.5
> versicolor
> 69           6.2         2.2          4.5         1.5
> versicolor
> 42           4.5         2.3          1.3         0.3
>   setosa
> 25           4.8         3.4          1.9         0.2
>   setosa
> 129          6.4         2.8          5.6         2.1
> virginica
> 60           5.2         2.7          3.9         1.4
> versicolor
> 80           5.7         2.6          3.5         1.0
> versicolor
>
> What I want to get are their rownumbers: 76, 105, 131,
> 79, 69, 42, 25, 129, 60, 80.
>
> Thanks in advance,
>
> Martin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From antonio.fabio at gmail.com  Mon Sep 26 10:58:09 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Mon, 26 Sep 2005 10:58:09 +0200
Subject: [R] getting variable length numerical gradient
In-Reply-To: <00b901c5c1c1$4a16bc20$0540210a@www.domain>
References: <b0808fdc050925023723026856@mail.gmail.com>
	<00b901c5c1c1$4a16bc20$0540210a@www.domain>
Message-ID: <b0808fdc05092601581a1e59d5@mail.gmail.com>

Tnx very much Dimitris,
your code does what I need. I've just adapted it to my needs (e.g., I
don't deal with scalar functions), and so solved my problem.

Given this, is there a way to use the deriv function in the base
package, within this context (variable length vector of indipendent
variables)?

Best,
Antonio, Fabio Di Narzo.

On 9/25/05, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be> wrote:
> maybe you can find the following function useful (any comments are
> greatly appreciated):
>
> fd <- function(x, f, scalar = TRUE, ..., eps =
> sqrt(.Machine$double.neg.eps)){
>     f <- match.fun(f)
>     out <- if(scalar){
>         if(length(f0 <- f(x, ...)) != length(x))
>             stop("'f' must be vectorized")
>         x. <- x + eps * pmax(abs(x), 1)
>         c(f(x., ...) - f0) / (x. - x)
>     } else{
>         n <- length(x)
>         res <- array(0, c(n, n))
>         f0 <- f(x, ...)
>         ex <- pmax(abs(x), 1)
>         for(i in 1:n){
>             x. <- x
>             x.[i] <- x[i] + eps * ex[i]
>             res[, i] <- c(f(x., ...) - f0) / (x.[i] - x[i])
>         }
>         res
>     }
>     out
> }
>
>
> ## Examples
>
> x <- seq(-3.3, 3.3, 0.1)
> all.equal(fd(x, pnorm, mean = 0.5), dnorm(x, mean = 0.5))
>
>
> # Approximate the Hessian matrix for a logistic regression
>
> # the score vector function
> gn <- function(b, y, X){
>     p <- as.vector(plogis(X %*% b))
>     -colSums(X * (y - p))
> }
>
> # We simulate some data and fit the logistic regression
> n <- 800
> x1 <- runif(n,-3, 3); x2 <- runif(n, -3, 3)
> pr <- plogis(0.8 + 0.4 * x1 - 0.3 * x2)
> y <- rbinom(n, 1, pr)
> fm <- glm(y ~ x1 + x2, binomial)
>
> ## The Hessian using forward difference approximation
> fd(fm$coef, gn, scalar = FALSE, y = y, X = cbind(1, x1, x2))
>
> ## The true Hessian
> solve(summary(fm)$cov.unscaled)
>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://www.med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
> ----- Original Message -----
> From: "Antonio, Fabio Di Narzo" <antonio.fabio at gmail.com>
> To: <R-help at stat.math.ethz.ch>
> Sent: Sunday, September 25, 2005 11:37 AM
> Subject: [R] getting variable length numerical gradient
>
>
> > Hi all.
> > I have a numerical function f(x), with x being a vector of generic
> > size (say k=4), and I wanna take the numerically computed gradient,
> > using deriv or numericDeriv (or something else).
> >
> > My difficulties here are that in deriv and numericDeric the function
> > is passed as an expression, and one have to pass the list of
> > variables
> > involved as a char vector... So, it's a pure R programming question.
> >
> >
> > Have a nice sunday,
> > Antonio, Fabio Di Narzo.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>



From p.dalgaard at biostat.ku.dk  Mon Sep 26 11:01:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Sep 2005 11:01:07 +0200
Subject: [R] How to get the rowindices without using which?
In-Reply-To: <20050926083729.94774.qmail@web40506.mail.yahoo.com>
References: <20050926083729.94774.qmail@web40506.mail.yahoo.com>
Message-ID: <x2vf0odwz0.fsf@turmalin.kubism.ku.dk>

Martin Lam <tmlammail at yahoo.com> writes:

> Hi,
> 
> I was wondering if it is possible to get the
> rowindices without using the function "which" because
> I don't have a restriction criteria. Here's an example
> of what I mean:
> # take 10 randomly selected instances
> iris[sample(1:nrow(iris), 10),]
> 
> # output
>     Sepal.Length Sepal.Width Petal.Length Petal.Width 
>   Species
> 76           6.6         3.0          4.4         1.4
> versicolor
> 105          6.5         3.0          5.8         2.2 
> virginica
> 131          7.4         2.8          6.1         1.9 
> virginica
> 79           6.0         2.9          4.5         1.5
> versicolor
> 69           6.2         2.2          4.5         1.5
> versicolor
> 42           4.5         2.3          1.3         0.3 
>    setosa
> 25           4.8         3.4          1.9         0.2 
>    setosa
> 129          6.4         2.8          5.6         2.1 
> virginica
> 60           5.2         2.7          3.9         1.4
> versicolor
> 80           5.7         2.6          3.5         1.0
> versicolor
> 
> What I want to get are their rownumbers: 76, 105, 131,
> 79, 69, 42, 25, 129, 60, 80.

Er, they are either the rownames of the result or the value that you
used to index with:

iris[nums <- sample(1:nrow(iris), 10),]

leaves the numbers sitting in nums. 

(If the original data frame has rownames that are not 1:nrow(yourframe),
then you have to decide whether it is names or numbers that you want.)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From felipe at unileon.es  Mon Sep 26 11:37:43 2005
From: felipe at unileon.es (Felipe)
Date: Mon, 26 Sep 2005 11:37:43 +0200
Subject: [R] Are least-squares means useful or appropriate?
In-Reply-To: <Pine.LNX.4.61.0509241409390.2700@tahawus.Princeton.EDU>
References: <mailman.9.1127469601.31308.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.61.0509241409390.2700@tahawus.Princeton.EDU>
Message-ID: <4337C167.5050904@unileon.es>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Thanks to everybody. I was a bit confused about this issue, and now I
have learned many useful things.
Greetings.
Felipe

Jonathan Dushoff wrote:
| On 9/20/05, Felipe <felipe at unileon.es> wrote:
|
|
|>My question was just theoric. I was wondering if someone who were using
|>SAS and R could give me their opinion on the topic. I was trying to use
|>least-squares means for comparison in R, but then I found some
|>indications against them, and I wanted to know if they had good basis
|>(as I told earlier, they were not much detailed).
|
|
| You may find this page helpful: http://www.tufts.edu/~gdallal/LHSP.HTM.
| The author claims that "least-squares means" is just a weird SAS word
| for adjusted means, and discusses several advantages and disadvantages.
|
| JD
|
| ______________________________________________
| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
|
|
-----BEGIN PGP SIGNATURE-----

iEYEARECAAYFAkM3wWcACgkQWtdQtNzjBl4IIACfeb1oRBapuL+UlrmJj65ggZN0
WTkAn1QAkK3u5LsfEEAIKKnf8NgFLr5F
=F3ab
-----END PGP SIGNATURE-----



From wegmann at biozentrum.uni-wuerzburg.de  Mon Sep 26 12:09:36 2005
From: wegmann at biozentrum.uni-wuerzburg.de (Martin Wegmann)
Date: Mon, 26 Sep 2005 12:09:36 +0200
Subject: [R] CART for 0/1 data
In-Reply-To: <4334392E.8020001@montana.edu>
References: <200509231633.39512.wegmann@biozentrum.uni-wuerzburg.de>
	<200509231802.34345.wegmann@biozentrum.uni-wuerzburg.de>
	<4334392E.8020001@montana.edu>
Message-ID: <200509261209.39271.wegmann@biozentrum.uni-wuerzburg.de>

Hello Robert, 

I tried over the week-end and managed, thanks to your help, to get my whole 
0/1 matrix into the model. However when I call summary(tree1) I get:

> summary(tree1)
Classification tree:
tree(formula = factor(dat$loc) ~ sp0.m)
Variables actually used in tree construction:
[1] "sp0.m.Pni"         "sp0.m.Bbe"
[3] "sp0.m.Arid"         "sp0.m.Pca"
Number of terminal nodes:  5
Residual mean deviance:  4.714 = 150.8 / 32
Misclassification error rate: 0.8649 = 32 / 37

The plot() output looks reasonable considering that only 5 locations are 
plotted.

However I have 400 variabels (in the 0/1 matrix sp0.m) and 50 locations 
(data$loc) and not just 5 terminal nodes. Is there a way to force tree() to 
plot all locations and their respective variables in the tree construction?

regards, Martin


On Friday 23 September 2005 19:19, Dave Roberts wrote:
> Martin,
>
>      Sorry, I don't think I read your message carefully enough.
>
>      When you say the error message is "+", that woudl seem to indicate
> that you still had an unclosed parenthesis and that the function was
> looking for more input.
>
>      Using a smaller data set (160 samples, 169 rows, only 5 classes) it
> did work fine for me.  pa = presence/absence dataframe, opt.5$clustering
> = cluster IDs.
>
> *********************************************************************
>
>  > test <- tree(factor(opt.5$clustering)~pa)
>  > test
>
> node), split, n, deviance, yval, (yprob)
>        * denotes terminal node
>
>   1) root 160 371.000 3 ( 0.23750 0.08750 0.57500 0.07500 0.02500 )
>     2) pa.symore < 0.5 79 216.500 1 ( 0.48101 0.17722 0.15190 0.13924
> 0.05063 )
>       4) pa.artarb < 0.5 42 123.600 2 ( 0.07143 0.33333 0.26190 0.23810
> 0.09524 )
>         8) pa.macgri < 0.5 31  75.280 2 ( 0.09677 0.45161 0.00000
> 0.32258 0.12903 )
>     .        .         .
>     .        .         .
>     .        .         .
>     3) pa.symore > 0.5 81  10.780 3 ( 0.00000 0.00000 0.98765 0.01235
> 0.00000 )
>       6) pa.carrss < 0.5 11   6.702 3 ( 0.00000 0.00000 0.90909 0.09091
> 0.00000 ) *
>       7) pa.carrss > 0.5 70   0.000 3 ( 0.00000 0.00000 1.00000 0.00000
> 0.00000 ) *
>
> ************************************************************************
>
> I'll try agin with a larger dataset and see if it's a memory limitation.
>
> Dave Roberts
>
> Martin Wegmann wrote:
> > On Friday 23 September 2005 17:08, Dave Roberts wrote:
> >>Martin,
> >>
> >>     If the data are actually coded 0/1, the tree function would
> >>probably intepret them as integers and try a regression instead of a
> >>classification.  If the dependent variable is called "var", try
> >
> > thanks, but I think I provided too less informations.
> > My dependent variable are the locations which are names (I could
> > transform them to numbers from 1 - n). The independent variables consist
> > of 0/1 data (species).
> > If I do
> > tree(locations~factor(species1)+factor(species2)+.....+factor(speciesn),
> > sp_data)
> > I receive the same results as without the factor() part.
> > BTW just a subset of the locations are displayed what is pretty weird
> > considering that I included all locations in the analysis.
> >
> > Martin
> >
> >>x <- tree(factor(var)~species)
> >>
> >>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> >>David W. Roberts                                     office 406-994-4548
> >>Professor and Head                                      FAX 406-994-3190
> >>Department of Ecology                         email droberts at montana.edu
> >>Montana State University
> >>Bozeman, MT 59717-3460
> >>
> >>Martin Wegmann wrote:
> >>>Dear R-user,
> >>>
> >>>I tried to generate classification / regression tree with a
> >>>absence/presence matrix of species (400) in different locations (50) to
> >>>visualise species which are important for splitting up two locations.
> >>>Rpart and tree did not work for more than 10 species which is logical
> >>> due to the limited amount of locations (n=50). However the error prompt
> >>> is a "+" and no specific message, but I am pretty sure that I did not
> >>> enter a false sign by mistake.
> >>>Is it allowed at all to use 0/1 data for this statistical technique and
> >>>if yes is there a way or different method to use all 400 species
> >>> entries? Otherwise I would apply a PCA beforehand but I would prefer to
> >>> have the raw species informations.
> >>>
> >>>using R 2.1.1-1 (debian repos.)
> >>>
> >>>regards, Martin
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Martin Wegmann

DLR - German Aerospace Center
German Remote Sensing Data Center
@
Dept.of Geography
Remote Sensing and Biodiversity Unit
&&
Dept. of Animal Ecology and Tropical Biology
University of Wuerzburg
Am Hubland
97074 W??rzburg

phone: +49-(0)931 - 888 4797
mobile: +49-(0)175 2091725
fax:   +49-(0)931 - 888 4961
http://www.biota-africa.org
http://www.biogis.de



From viudez_ant at gva.es  Mon Sep 26 12:18:58 2005
From: viudez_ant at gva.es (Toni =?iso-8859-1?q?Vi=FAdez?=)
Date: Mon, 26 Sep 2005 12:18:58 +0200
Subject: [R] merge maps from shapefile to lattice
Message-ID: <200509261218.58778.viudez_ant@gva.es>

Hi everybody:
Could anybody help how I solve the next problem?.
I'm doing interpolation maps of tropospheric ozone of my region, and after 
create it using IDW, and kriging methods, I want from shapefiles (*.shx, 
*.shp, *.dbf, *.sbx & *.sbn ) create contour over the interpolation maps.
Could anybody tell me how do it?.
Thanks in advance.

######################################
Antonio Viudez Mora
Departamento de Din??mica
 de Contaminantes
Fundaci??n CEAM
Paterna (Valencia)
tel: 961318190. ext: 216
e-mail:toni at ceam.es
######################################



From nikonia at duckmail.nl  Mon Sep 26 12:15:55 2005
From: nikonia at duckmail.nl (nikonia)
Date: Mon, 26 Sep 2005 12:15:55 +0200
Subject: [R] calculating distances using Gower's coefficient on mixed
	variables.
Message-ID: <4337CA5B.3010400@duckmail.nl>

I want to compute the distances in a mixed variable matrix using the
Gower coefficient. I understand it is possible to calculate distances in
a matrix with mixed variables using the dudi.pco command. How would this
work?

Jorine



From Knut.Krueger at usa.com  Sun Sep 25 14:34:10 2005
From: Knut.Krueger at usa.com (Knut Krueger)
Date: Sun, 25 Sep 2005 14:34:10 +0200
Subject: [R] hist(x, ...)  with  normal distribution  curve
Message-ID: <43369942.6070604@usa.com>

.
I am looking for a histogram or box plot with the adding  normal 
distribution  curve
I think that must be possible, but I am not able to find out how to do.



Regards Knut



From vinum at iinet.net.au  Mon Sep 26 14:34:14 2005
From: vinum at iinet.net.au (John Charles Considine)
Date: Mon, 26 Sep 2005 20:34:14 +0800
Subject: [R] figure widths in sweave
Message-ID: <1127738054.7532.42.camel@Tardis.considine.local>

gRoovers,

Can the size of figures be controlled from within a noweb document
without resorting to editing the \includegraphics sections in the .tex
file?

Can the figure widths be set in the environmental declarations at the
start?

Can they be set within the \begin{figure} environment?

JC



From petr.pikal at precheza.cz  Mon Sep 26 14:52:36 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 26 Sep 2005 14:52:36 +0200
Subject: [R] hist(x, ...)  with  normal distribution  curve
In-Reply-To: <43369942.6070604@usa.com>
Message-ID: <43380B34.20824.18B6494@localhost>

Hi

answered hundered times.

> Dear R people: 
> 
> I would like to superimpose a normal curve on a histogram. 
x<-rnorm(150) 
h<-hist(x,breaks=15) 
xhist<-c(min(h$breaks),h$breaks) 
yhist<-c(0,h$density,0) 
xfit<-seq(min(x),max(x),length=40) 
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
plot(xhist,yhist,type="s",ylim=c(0,max(yhist,yfit))) 
lines(xfit,yfit) 


Bill 

above is e.g. Bill Simpson's answer from 2001. Found from R-site 
search  ***histogram density normal***.

HTH
Petr


On 25 Sep 2005 at 14:34, Knut Krueger wrote:

> .
> I am looking for a histogram or box plot with the adding  normal
> distribution  curve I think that must be possible, but I am not able
> to find out how to do.
> 
> 
> 
> Regards Knut
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From francoisromain at free.fr  Mon Sep 26 14:59:34 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 26 Sep 2005 14:59:34 +0200
Subject: [R] hist(x, ...)  with  normal distribution  curve
In-Reply-To: <43369942.6070604@usa.com>
References: <43369942.6070604@usa.com>
Message-ID: <4337F0B6.9060502@free.fr>

Le 25.09.2005 14:34, Knut Krueger a ??crit :

>.
>I am looking for a histogram or box plot with the adding  normal 
>distribution  curve
>I think that must be possible, but I am not able to find out how to do.
>
>
>
>Regards Knut
>  
>
Hi Knut,

There are a lot of ways to do that, let x be your data (assume x ~ 
N(mu=2,sd=.4))
R> x <- rnorm(200, mean=2, sd=.4)

** With the traditionnal graphics system, do :
R> hist(x, prob=T)
R> curve(dnorm, col=2, mean=mean(x), sd=sd(x))

** With lattice :
R> histogram(~x,
  panel = function(x,...){
    panel.histogram(x,...)
    panel.mathdensity(dmath = dnorm, col = "red",
                                args = list(mean=mean(x),sd=sd(x)))
   },
  type="density")

Then, have a look at :
http://addictedtor.free.fr/graphiques/search.php?q=hist

And also have a nice day....

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From dmbates at gmail.com  Mon Sep 26 15:05:26 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 26 Sep 2005 08:05:26 -0500
Subject: [R] anova on binomial LMER objects
In-Reply-To: <20050925210533.44547.qmail@web50610.mail.yahoo.com>
References: <4336BC44.8070404@pdf.com>
	<20050925210533.44547.qmail@web50610.mail.yahoo.com>
Message-ID: <40e66e0b050926060532bee7e0@mail.gmail.com>

On 9/25/05, Horacio Montenegro <nepossiver at yahoo.com> wrote:
>
>     Hi Spencer and Robert,
>
>     I have found the same behaviour, but only for lme4
> and Matrix after the 0.96 release. lme4 0.95-10 and
> Matrix 0.95-13 releases gave "sensible" results. This
> could be an introduced bug, or a solved bug - you
> should ask Prof. Bates.
>
>     hope this helps, cheers,
>
>     Horacio Montenegro

I have run into a couple of other things that the "improvements" from
the 0.95 series to the 0.96 series has made worse.  This may take a
while to sort out.  Thanks to Robert Bagchi for the very thorough
error report.


>
> --- Spencer Graves <spencer.graves at pdf.com> wrote:
> >         I agree:  Something looks strange to me in this
> > example also;  I have
> > therefore copied Douglas Bates and  Deepayan Sarkar.
> >  You've provided a
> > nice simulation.  If either of them have time to
> > look at this, I think
> > they could tell us what is happening here.
> >
> >         If you need an answer to your particular problem,
> > you could run that
> > simulation 1000 or 1,000 times.  That would tell you
> > whether to believe
> > the summary or the anova, or neither.  If you want
> > to understand the
> > algorithm, you could walk through the code.
> > However, "lmer" is a
> > generic, and I don't have time now to figure out how
> > to find the source.
> >   A response from Brian Ripley to a question from me
> > a couple of days
> > ago provides a nice summary of how to do that, but I
> > don't have time to
> > check that now.
> >
> >         Sorry I couldn't help more.
> >         spencer graves
> >
> > Robert Bagchi wrote:
> >
> > > Dear R users,
> > >
> > > I have been having problems getting believable
> > estimates from anova on a
> > > model fit from lmer. I get the impression that F
> > is being greatly
> > > underestimated, as can be seen by running the
> > example I have given below.
> > >
> > > First an explanation of what I'm trying to do. I
> > am trying to fit a glmm
> > > with binomial errors to some data. The experiment
> > involves 10
> > > shadehouses, divided between 2 light treatments
> > (high, low). Within each
> > > shadehouse there are 12 seedlings of each of 2
> > species (hn & sl). 3
> > > damage treatments (0, 0.1, 0.25 leaf area removal)
> > were applied to the
> > > seedlings (at random) so that there are 4
> > seedlings of each
> > > species*damage treatment in each shadehouse.
> > There maybe a shadehouse
> > > effect, so I need to include it as a random
> > effect. Light is applied to
> > > a shadehouse, so it is outer to shadehouse. The
> > other 2 factors are
> > > inner to shadehouse.
> > >
> > > We want to assess if light, damage and species
> > affect survival of
> > > seedlings. To test this I fitted a binomial mixed
> > effects model with
> > > lmer (actually with quasibinomial errors). THe
> > summary function suggests
> > > a large effect of both light and species (which
> > agrees with graphical
> > > analysis). However, anova produces F values close
> > to 0 and p values
> > > close to 1 (see example below).
> > >
> > > Is this a bug, or am I doing something
> > fundamentally wrong? If anova
> > > doesn't work with lmer is there a way to perform
> > hypothesis tests on
> > > fixed effects in an lmer model? I was going to
> > just delete terms and
> > > then do liklihood ratio tests, but according to
> > Pinheiro & Bates (p. 87)
> > > that's very untrustworthy. Any suggestions?
> > >
> > > I'm using R 2.1.1 on windows XP and lme4 0.98-1
> > >
> > > Any help will be much appreciated.
> > >
> > > many thanks
> > > Robert
> > >
> > >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Mon Sep 26 15:06:44 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 26 Sep 2005 15:06:44 +0200 (CEST)
Subject: [R] merge maps from shapefile to lattice
In-Reply-To: <200509261218.58778.viudez_ant@gva.es>
Message-ID: <Pine.LNX.4.44.0509261435160.4157-100000@reclus.nhh.no>

On Mon, 26 Sep 2005, Toni Vi??dez wrote:

> Hi everybody:
> Could anybody help how I solve the next problem?.
> I'm doing interpolation maps of tropospheric ozone of my region, and after 
> create it using IDW, and kriging methods, I want from shapefiles (*.shx, 
> *.shp, *.dbf, *.sbx & *.sbn ) create contour over the interpolation maps.
> Could anybody tell me how do it?.

In replies off-list, I've suggested that using base graphics is simpler 
because you can build things up bit by bit with add=TRUE. However, using 
the maptools and sp packages, you can look at the example in ?meuse.riv:

     spplot(meuse.grid, col.regions=bpy.colors(), main = "meuse.grid",
       sp.layout=list(
             list("sp.polygons", meuse.sr),
             list("sp.points", meuse, pch="+", col="black")
       )
     )

which plots a grid of data (your interpolations would need to be converted
to this format) over polygons but under points, where "meuse.sr"  is a
SpatialPolygons object. You would then do:

my_polys <- readShapePoly("my_polys.shp")
spplot(meuse.grid, sp.layout=list("sp.polygons", my_polys))

to get something similar. Note that the polygons seem to be plotted under 
the image in the example.


> Thanks in advance.
> 
> ######################################
> Antonio Viudez Mora
> Departamento de Din??mica
>  de Contaminantes
> Fundaci??n CEAM
> Paterna (Valencia)
> tel: 961318190. ext: 216
> e-mail:toni at ceam.es
> ######################################
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From W.E.Wolski at newcastle.ac.uk  Mon Sep 26 15:41:42 2005
From: W.E.Wolski at newcastle.ac.uk (nwew)
Date: Mon, 26 Sep 2005 14:41:42 +0100
Subject: [R] regression methods for circular(?) data.
Message-ID: <4337FD8E@webmail.ncl.ac.uk>

Dear R-users,

I have the following data

x <- runif(300,min=1,max=230)

y <- x*0.005 + 0.2
y <- y+rnorm(100,mean=0,sd=0.1)
y <- y%%1 #  <------- modulo operation
plot(x,y)

and would like to recapture the slope (0.005) and intercept(0.2). I wonder if 
there are any clever algorithms to do this. I was looking at the function 
lm.cirucalar. Is this the method to use? If, which of the references is best 
too look at?

Eryk



From anderson_carl at hotmail.com  Mon Sep 26 15:42:09 2005
From: anderson_carl at hotmail.com (Carl Anderson)
Date: Mon, 26 Sep 2005 23:42:09 +1000
Subject: [R] Error Message - Error: symbol print-name too long
Message-ID: <BAY101-F143604833C234BBA754EC5828B0@phx.gbl>

Dear All,

I write to ask for information regarding the error message:

Error: symbol print-name too long.

I am afraid that I can't include any code to help with any further diagnosis 
of the problem as the code is far too long to be of any use, and I have not 
been able to re-create the problem in shorter example codes.

I have searched the R manual and help pages and found no mention of this 
error message.

All help appreciated,

Carl A Anderson.



From rschulz at sonic.net  Mon Sep 26 15:53:50 2005
From: rschulz at sonic.net (Randall R Schulz)
Date: Mon, 26 Sep 2005 06:53:50 -0700
Subject: [R] getting variable length numerical gradient
In-Reply-To: <00b901c5c1c1$4a16bc20$0540210a@www.domain>
References: <b0808fdc050925023723026856@mail.gmail.com>
	<00b901c5c1c1$4a16bc20$0540210a@www.domain>
Message-ID: <200509260653.50941.rschulz@sonic.net>

Dimitris,

I'm new to R programming, and I'm trying to learn the proper way to do 
certain things. E.g., I had a piece of code with explicit iteration to 
apply some computations to a vector. It was pretty slow. I found a way 
to utilize R's built-in vectorization and it was sped up considerably.

So I want to ask about the code you supplied. Please see below.

(By the way, this message is best viewed using a mono-spaced font.)


On Sunday 25 September 2005 04:07, Dimitris Rizopoulos wrote:
> maybe you can find the following function useful (any comments are
> greatly appreciated):
>
> fd <- function(x, f, scalar = TRUE, ..., eps =
> sqrt(.Machine$double.neg.eps)){
>     f <- match.fun(f)
>     out <- if(scalar){
>         ...
>     } else{
>         n <- length(x)
>         res <- array(0, c(n, n))
>         f0 <- f(x, ...)
>         ex <- pmax(abs(x), 1)
>         for(i in 1:n){

This (following) statement will create a copy of the entire "x" vector 
on each iteration. It doesn't look like that's what you would want to 
do:

>             x. <- x

The computation described by this statement could be vectorized outside 
the loop:

>             x.[i] <- x[i] + eps * ex[i]

>             res[, i] <- c(f(x., ...) - f0) / (x.[i] - x[i])
>         }
>         res
>     }
>     out
> }

Offhand, I cannot tell for sure if the last line of that loop is 
vectorizable, but I have a hunch it is.

So at a minimum, it seems this fragment of your code:

?? ?? ?? ?? for(i in 1:n){
?? ?? ?? ?? ?? ?? x. <- x
?? ?? ?? ?? ?? ?? x.[i] <- x[i] + eps * ex[i]
?? ?? ?? ?? ?? ?? res[, i] <- c(f(x., ...) - f0) / (x.[i] - x[i])
?? ?? ?? ?? }

Could be more efficiently and succinctly replaced with this:

        x. <- x + eps * ex
        for (in in 1:n)
            res[, i] <- c(f(x., ...) - f0) / (x.[i] - x[i])


Could your someone else with R programming experience comment?


Thanks.


Randall Schulz



From gmbegnis at yahoo.it  Mon Sep 26 15:55:50 2005
From: gmbegnis at yahoo.it (giacomo moro)
Date: Mon, 26 Sep 2005 15:55:50 +0200 (CEST)
Subject: [R] create trend variable in a regression using R
Message-ID: <20050926135550.60129.qmail@web25702.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050926/ad700e28/attachment.pl

From florian.defregger at ku-eichstaett.de  Mon Sep 26 15:36:21 2005
From: florian.defregger at ku-eichstaett.de (Florian Defregger)
Date: Mon, 26 Sep 2005 15:36:21 +0200
Subject: [R] histogram - one bin for all values larger than a certain value
Message-ID: <000b01c5c29f$48232aa0$0ca54e8d@asnb22301>

Dear all,
I wonder if I can put together a histogram where one bin contains all the 
values that are larger than a certain specified value.

Example:
I have values ranging from 0 to 40 and I want 10 bins from 0 to 10, i.e. for 
the intervals [0,1), [1,2) , ..., [9,10). And then I want one last bin which 
contains all the values larger than 10, i.e. for the interval [10, 40).

Thanks,
Florian



From duncan at wald.ucdavis.edu  Mon Sep 26 16:13:28 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Mon, 26 Sep 2005 07:13:28 -0700
Subject: [R] SAX Parser best practise
In-Reply-To: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E7A4@EMAIL.mpimp-golm.mpg.de>
References: <1F5B2A5E6712B94DA65EA9EBF0A1A38230E7A4@EMAIL.mpimp-golm.mpg.de>
Message-ID: <43380208.4090600@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


When you uncomment the two lines, your document
becomes two nodes
 <spectrum>
    ...
 <spectrum>
 <spectrum>
   ...
 </spectrum>

XML requires that there be a single top-level node.
And so the parser throws an error saying
  Extra content at the end of the document

And it is the second <spectrum> .. </spectrum>
node that it is complaining about.
You can wrap the entire thing in a top node, e.g.
<spectra> <spectrum>...</spectrum><spectrum>...</spectrum></spectra>

How did I find this?  I looked at the error message from
libxml. Now that we have exceptions in R and we are using
libxml2, etc. I can make this material available at the
R level. So I'll do that.


Jan Hummel wrote:
> Hi Duncan,
> 
> 
>>BTW, there is a new version of the XML package on the 
>>Omegahat web site.
> 
> I'll use it extensive in this days and unfortunately I have already a
> question/problem pending:
> 
> Taking the following R function:
> 
> test<-function(){
> 	sep=""
> 	xmlText <-""
> 	xmlText <-paste(xmlText,"<spectrum id=\"3257\">",sep=sep)
> 	xmlText <-paste(xmlText,"<mzArrayBinary>",sep=sep)
> 	xmlText <-paste(xmlText,"<data>Monday</data>",sep=sep)
> 	xmlText <-paste(xmlText,"</mzArrayBinary>",sep=sep)
> 	xmlText <-paste(xmlText,"<intenArrayBinary>",sep=sep)
> 	xmlText <-paste(xmlText,"<data>Tuesday</data>",sep=sep)
> 	xmlText <-paste(xmlText,"</intenArrayBinary>",sep=sep)
> #	xmlText <-paste(xmlText,"</spectrum>",sep=sep)
> #	xmlText <-paste(xmlText,"<spectrum id=\"3259\">",sep=sep)
> 	xmlText <-paste(xmlText,"<mzArrayBinary>",sep=sep)
> 	xmlText <-paste(xmlText,"<data>Wednesday</data>",sep=sep)
> 	xmlText <-paste(xmlText,"</mzArrayBinary>",sep=sep)
> 	xmlText <-paste(xmlText,"<intenArrayBinary>",sep=sep)
> 	xmlText <-paste(xmlText,"<data>Thursday</data>",sep=sep)
> 	xmlText <-paste(xmlText,"</intenArrayBinary>",sep=sep)
> 	xmlText <-paste(xmlText,"</spectrum>",sep=sep)
> 
> 	xmlEventParse(xmlText, asText=TRUE, handlers = list(text =
> function(x, ...) {cat(nchar(x),x, "\n")}))
> 	return(invisible(NULL))
> }
> 
> Using this function in the given form works fine. xmlEventParse() with
> the simplest handler I can imagine finds all 4 text-nodes within the
> <spectrum> tag and prints them out. But if one uncomment both lines in
> the middle, introducing 2 <spectrum> tags with different id's
> xmlEventParse() returns with an exception. Of course the weekdays within
> <data> are arbitrary values used here. Further, using an other input
> file I could see, that for one and the same <data> node the handler for
> "text"-nodes was invoked two times, one time for a first part of the
> content and one time for the rest of the content. Both invocations
> together gave me exactly the content from the <data> node. 
> 
> So, am I on the wrong way? Or is this some buggy behaviour? 
> 
> I appreciat any help and assistance!
> 
> Jan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

- --
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (Darwin)
Comment: Using GnuPG with Thunderbird - http://enigmail.mozdev.org

iD4DBQFDOAII9p/Jzwa2QP4RAg+9AKCCkYAwTjlMQ9R9dsLbeWQxuf63uQCYkR3g
nEZl4wFXtkYSmsQ8/JyMDA==
=wXfS
-----END PGP SIGNATURE-----



From dimitris.rizopoulos at med.kuleuven.be  Mon Sep 26 16:16:51 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 26 Sep 2005 16:16:51 +0200
Subject: [R] getting variable length numerical gradient
References: <b0808fdc050925023723026856@mail.gmail.com><00b901c5c1c1$4a16bc20$0540210a@www.domain>
	<200509260653.50941.rschulz@sonic.net>
Message-ID: <00d101c5c2a4$f05f2700$0540210a@www.domain>

Randall,

thanks for your comments; however, you have to take into account what 
is the purpose of the function here! The goal is to approximate 
*partial* derivatives numerically, using in fact the definition of the 
partial derivatives. If you recall this definition I hope that you can 
see why I change the ith element of the x vector and not the whole 
one. You could also test your approach with the original one in the 
logistic regression example and see the difference.

I hope it is more clear now.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Randall R Schulz" <rschulz at sonic.net>
To: "R Help" <R-Help at stat.math.ethz.ch>
Sent: Monday, September 26, 2005 3:53 PM
Subject: Re: [R] getting variable length numerical gradient


Dimitris,

I'm new to R programming, and I'm trying to learn the proper way to do
certain things. E.g., I had a piece of code with explicit iteration to
apply some computations to a vector. It was pretty slow. I found a way
to utilize R's built-in vectorization and it was sped up considerably.

So I want to ask about the code you supplied. Please see below.

(By the way, this message is best viewed using a mono-spaced font.)


On Sunday 25 September 2005 04:07, Dimitris Rizopoulos wrote:
> maybe you can find the following function useful (any comments are
> greatly appreciated):
>
> fd <- function(x, f, scalar = TRUE, ..., eps =
> sqrt(.Machine$double.neg.eps)){
>     f <- match.fun(f)
>     out <- if(scalar){
>         ...
>     } else{
>         n <- length(x)
>         res <- array(0, c(n, n))
>         f0 <- f(x, ...)
>         ex <- pmax(abs(x), 1)
>         for(i in 1:n){

This (following) statement will create a copy of the entire "x" vector
on each iteration. It doesn't look like that's what you would want to
do:

>             x. <- x

The computation described by this statement could be vectorized 
outside
the loop:

>             x.[i] <- x[i] + eps * ex[i]

>             res[, i] <- c(f(x., ...) - f0) / (x.[i] - x[i])
>         }
>         res
>     }
>     out
> }

Offhand, I cannot tell for sure if the last line of that loop is
vectorizable, but I have a hunch it is.

So at a minimum, it seems this fragment of your code:

for(i in 1:n){
x. <- x
x.[i] <- x[i] + eps * ex[i]
res[, i] <- c(f(x., ...) - f0) / (x.[i] - x[i])
}

Could be more efficiently and succinctly replaced with this:

        x. <- x + eps * ex
        for (in in 1:n)
            res[, i] <- c(f(x., ...) - f0) / (x.[i] - x[i])


Could your someone else with R programming experience comment?


Thanks.


Randall Schulz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From sundar.dorai-raj at pdf.com  Mon Sep 26 16:15:20 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 26 Sep 2005 09:15:20 -0500
Subject: [R] histogram - one bin for all values larger than a certain
 value
In-Reply-To: <000b01c5c29f$48232aa0$0ca54e8d@asnb22301>
References: <000b01c5c29f$48232aa0$0ca54e8d@asnb22301>
Message-ID: <43380278.4050000@pdf.com>



Florian Defregger wrote:
> Dear all,
> I wonder if I can put together a histogram where one bin contains all the 
> values that are larger than a certain specified value.
> 
> Example:
> I have values ranging from 0 to 40 and I want 10 bins from 0 to 10, i.e. for 
> the intervals [0,1), [1,2) , ..., [9,10). And then I want one last bin which 
> contains all the values larger than 10, i.e. for the interval [10, 40).
> 
> Thanks,
> Florian
> 

Hi, Florian,

See the "breaks" argument in ?hist.

x <- sample(1:40, 1000, replace = TRUE)
hist(x, breaks = c(0:10, 40))

Is this what you intended?

--sundar



From Carlisle.Thacker at noaa.gov  Mon Sep 26 16:20:40 2005
From: Carlisle.Thacker at noaa.gov (Carlisle Thacker)
Date: Mon, 26 Sep 2005 10:20:40 -0400
Subject: [R] k-d tree for loess
Message-ID: <433803B8.5020707@noaa.gov>

I am exploring the use of loess for oceanographic applications and would
like to plot the locations (longitude and latitude) points where the models
(salinity~temperature*longitude*latitude,parametric="temperature") are
fitted.  Chambers and Hastie(1993) explains the locations are nodes of a
k-d tree. but I cannot find anything about accessing this information.  It
would be useful to superimpose on such plots contours of the weights for at
least one point.  Sample code of drawing such plots would be greatly
appreciated.

Thanks,

Carlisle Thacker



From rschulz at sonic.net  Mon Sep 26 16:25:15 2005
From: rschulz at sonic.net (Randall R Schulz)
Date: Mon, 26 Sep 2005 07:25:15 -0700
Subject: [R] getting variable length numerical gradient
In-Reply-To: <00d101c5c2a4$f05f2700$0540210a@www.domain>
References: <b0808fdc050925023723026856@mail.gmail.com>
	<200509260653.50941.rschulz@sonic.net>
	<00d101c5c2a4$f05f2700$0540210a@www.domain>
Message-ID: <200509260725.15238.rschulz@sonic.net>

Dimitris,

On Monday 26 September 2005 07:16, Dimitris Rizopoulos wrote:
> Randall,
>
> thanks for your comments; however, you have to take into account what
> is the purpose of the function here! The goal is to approximate
> *partial* derivatives numerically, ...
>
> I hope it is more clear now.

Yes. Thanks for clearing up my misunderstanding.


> Best,
> Dimitris


Randall Schulz



From duncan at wald.ucdavis.edu  Mon Sep 26 16:31:08 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Mon, 26 Sep 2005 07:31:08 -0700
Subject: [R] Error Message - Error: symbol print-name too long
In-Reply-To: <BAY101-F143604833C234BBA754EC5828B0@phx.gbl>
References: <BAY101-F143604833C234BBA754EC5828B0@phx.gbl>
Message-ID: <4338062C.2000506@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



You aren't giving us much to go on, so I can only
make a very wild guess.  Check that your file
doesn't have a stray ` character in it.
R will start reading from that point on and try
to make this an internal symbol.  If it doesn't
find the closing ` for too many characters,
it gives the error message you see.

I have seen this once before and that is what the cause
was - a stray ` put in by hitting the ` key rather than Esc.



Carl Anderson wrote:
> Dear All,
> 
> I write to ask for information regarding the error message:
> 
> Error: symbol print-name too long.
> 
> I am afraid that I can't include any code to help with any further diagnosis 
> of the problem as the code is far too long to be of any use, and I have not 
> been able to re-create the problem in shorter example codes.
> 
> I have searched the R manual and help pages and found no mention of this 
> error message.
> 
> All help appreciated,
> 
> Carl A Anderson.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

- --
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (Darwin)
Comment: Using GnuPG with Thunderbird - http://enigmail.mozdev.org

iD8DBQFDOAYs9p/Jzwa2QP4RAhtAAJ97sqWdUTOPjtZ2RMJR0qcfyjIAZgCfZLF1
IXTk0bY5RQUD2e+8VlzLpw4=
=+pim
-----END PGP SIGNATURE-----



From francoisromain at free.fr  Mon Sep 26 16:35:55 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 26 Sep 2005 16:35:55 +0200
Subject: [R] histogram - one bin for all values larger than a certain
 value
In-Reply-To: <43380278.4050000@pdf.com>
References: <000b01c5c29f$48232aa0$0ca54e8d@asnb22301>
	<43380278.4050000@pdf.com>
Message-ID: <4338074B.8020006@free.fr>

Le 26.09.2005 16:15, Sundar Dorai-Raj a ??crit :

>Florian Defregger wrote:
>  
>
>>Dear all,
>>I wonder if I can put together a histogram where one bin contains all the 
>>values that are larger than a certain specified value.
>>
>>Example:
>>I have values ranging from 0 to 40 and I want 10 bins from 0 to 10, i.e. for 
>>the intervals [0,1), [1,2) , ..., [9,10). And then I want one last bin which 
>>contains all the values larger than 10, i.e. for the interval [10, 40).
>>
>>Thanks,
>>Florian
>>
>>    
>>
>
>Hi, Florian,
>
>See the "breaks" argument in ?hist.
>
>x <- sample(1:40, 1000, replace = TRUE)
>hist(x, breaks = c(0:10, 40))
>
>Is this what you intended?
>
>--sundar
>  
>
Maybe also take a look at the right argument.
I think this is closer to what Florian wanted in the first place :

R> hist(x, breaks = c(0:10, 40), right=FALSE)


Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From wang at math.s.chiba-u.ac.jp  Mon Sep 26 16:52:34 2005
From: wang at math.s.chiba-u.ac.jp (Jinfang Wang)
Date: Mon, 26 Sep 2005 23:52:34 +0900
Subject: [R] quasi-random vector according to an independent graph
Message-ID: <003701c5c2a9$ee20b670$030ba8c0@IBM4D6040982F0>

Dear R-users,

Is anyone aware of any function/package for generating a random vector from 
a joint distribution defined by an independent graph? Or I have to work it 
out myself?

Thanks.

Jinfang

------------------------------
Jinfang Wang, Associate Professor
Chiba University, Japan



From ripley at stats.ox.ac.uk  Mon Sep 26 17:09:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Sep 2005 16:09:17 +0100 (BST)
Subject: [R] k-d tree for loess
In-Reply-To: <433803B8.5020707@noaa.gov>
References: <433803B8.5020707@noaa.gov>
Message-ID: <Pine.LNX.4.61.0509261606270.23025@gannet.stats>

First a warning: loess in R is only loosely related to loess in S, being 
derived from a C implementation (by the same authors).

In R I don't think you can do this.  Those details are never exposed, and 
are hidden in an undocumented C/Fortran workspace.

On Mon, 26 Sep 2005, Carlisle Thacker wrote:

> I am exploring the use of loess for oceanographic applications and would
> like to plot the locations (longitude and latitude) points where the models
> (salinity~temperature*longitude*latitude,parametric="temperature") are
> fitted.  Chambers and Hastie(1993) explains the locations are nodes of a
> k-d tree. but I cannot find anything about accessing this information.  It
> would be useful to superimpose on such plots contours of the weights for at
> least one point.  Sample code of drawing such plots would be greatly
> appreciated.
>
> Thanks,
>
> Carlisle Thacker

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jose.pinheiro at novartis.com  Mon Sep 26 17:16:03 2005
From: jose.pinheiro at novartis.com (jose.pinheiro@novartis.com)
Date: Mon, 26 Sep 2005 11:16:03 -0400
Subject: [R] ASA Stat. Computing and Stat. Graphics 2006 Student Paper
	competition
Message-ID: <OFAA22785B.324501E6-ON85257088.0053BD5C-85257088.0053DF02@EU.novartis.net>

The Statistical Computing and Statistical Graphics Sections of the ASA
are co-sponsoring a student paper competition on the topics of
Statistical Computing and Statistical Graphics.  Students are
encouraged to submit a paper in one of these areas, which might be
original methodological research, some novel computing or graphical
application in statistics, or any other suitable contribution (for
example, a software-related project).  The selected winners will
present their papers in a topic-contributed session at the 2006 Joint
Statistical Meetings.  The Sections will pay registration fees for the
winners as well as a substantial allowance for transportation to the
meetings and lodging. Enclosed below is the full text of the award 
announcement.
More details can be found at the Stat. Computing Section website at
http://www.statcomputing.org. 



Best Regards,

--Jos? Pinheiro

Awards Chair
ASA Statistical Computing Section
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: StudentPaper2006.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050926/2da3b7ca/StudentPaper2006.txt

From Knut.Krueger at usa.com  Sun Sep 25 17:30:06 2005
From: Knut.Krueger at usa.com (Knut Krueger)
Date: Sun, 25 Sep 2005 17:30:06 +0200
Subject: [R] hist(x, ...)  with  normal distribution  curve
In-Reply-To: <43380B34.20824.18B6494@localhost>
References: <43380B34.20824.18B6494@localhost>
Message-ID: <4336C27E.7080708@usa.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050925/e827f644/attachment.pl

From r.shengzhe at gmail.com  Mon Sep 26 17:45:03 2005
From: r.shengzhe at gmail.com (Shengzhe Wu)
Date: Mon, 26 Sep 2005 17:45:03 +0200
Subject: [R] Help: x11 position in the Unix environment
Message-ID: <ea57975b05092608453c7ff6f4@mail.gmail.com>

Hello,

In the Unix environment, I open a window by x11(). May I specify the
position of this window by specifying the position of the top left of
the window as in Windows environment? Or some other parameters can be
used to do that?

Thank you,
Shengzhe



From Ted.Harding at nessie.mcc.ac.uk  Mon Sep 26 17:46:56 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 26 Sep 2005 16:46:56 +0100 (BST)
Subject: [R] regression methods for circular(?) data.
In-Reply-To: <4337FD8E@webmail.ncl.ac.uk>
Message-ID: <XFMail.050926164656.Ted.Harding@nessie.mcc.ac.uk>

On 26-Sep-05 nwew wrote:
> Dear R-users,
> 
> I have the following data
> 
> x <- runif(300,min=1,max=230)
> 
> y <- x*0.005 + 0.2
> y <- y+rnorm(100,mean=0,sd=0.1)
> y <- y%%1 #  <------- modulo operation
> plot(x,y)
> 
> and would like to recapture the slope (0.005) and intercept(0.2).
> I wonder if there are any clever algorithms to do this. I was
> looking at the function lm.cirucalar. Is this the method to use?
> If, which of the references is best too look at?
> 
> Eryk

Hi Eryk,

If you know the modulus (in your case 1.0) and you get data that
look like the result of your "plot(x,y)", then I wouldn't mess
about.

I would simply do something like

y1<-y
ix <- ix<-(y < 0.9*(x-50)/200)
y1[ix] <- y1[ix]+1.0
lm(y1~x)

(the constants 0.9/200, -50 being chosen to give a good separation
on the graph).

On the other hand, if there are good reasons why this very simple
approach is not suitable, then if we knew what they were a more
helpful reply would be easier to formulate!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Sep-05                                       Time: 15:56:48
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Mon Sep 26 18:08:32 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Sep 2005 18:08:32 +0200
Subject: [R] hist(x, ...)  with  normal distribution  curve
In-Reply-To: <4336C27E.7080708@usa.com>
References: <43380B34.20824.18B6494@localhost> <4336C27E.7080708@usa.com>
Message-ID: <x2r7bb3j7j.fsf@turmalin.kubism.ku.dk>

Knut Krueger <Knut.Krueger at usa.com> writes:

> Petr Pikal schrieb:
> 
> >Hi
> >
> >answered hundered times.
> >
> >  
> >
> >>Dear R people: 
> >>
> >>I would like to superimpose a normal curve on a histogram. 
> >>    
> >>
> >x<-rnorm(150) 
> >h<-hist(x,breaks=15) 
> >xhist<-c(min(h$breaks),h$breaks) 
> >yhist<-c(0,h$density,0) 
> >xfit<-seq(min(x),max(x),length=40) 
> >yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
> >plot(xhist,yhist,type="s",ylim=c(0,max(yhist,yfit))) 
> >lines(xfit,yfit) 
> >
> >
> >Bill 
> >
> >above is e.g. Bill Simpson's answer from 2001. Found from R-site 
> >search  ***histogram density normal***.
> >  
> >
> Ok If I merge both of your answers I get the graph like in SPSS but
> the ylab is density instead frequency:

Many people consider that a feature, since histograms are supposed to
be density estimates....

> and I do not have the clicks in SPSS to redo the same graph :-(
> I hav only the Data file and the SPSS plot.
> 
> 
> x<-5+rnorm(150) 
> 
> h<-hist(x,breaks=10,freq = TRUE) 
> #I need this histogramm  with...
> 
> xfit<-seq(min(x),max(x),length=40) 
> yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
> 
> lines(xfit,yfit) 
> 
> 
> 
> h<-hist(x,breaks=10,prob=T) 
> 
> xfit<-seq(min(x),max(x),length=40) 
> yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
> # ... this line 
> lines(xfit,yfit)


Er, something got duplicated in there? 

Anyways, if you want the normal curve blown up to the scale of
counts/bin, just multiply yfit by the number of observations times the
bin width.

To find the bin width take, e.g. diff(h$mids[1:2]). If they're not all
equal, then you're in deeper trouble.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From francoisromain at free.fr  Mon Sep 26 18:11:21 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 26 Sep 2005 18:11:21 +0200
Subject: [R] hist(x, ...)  with  normal distribution  curve
In-Reply-To: <4336C27E.7080708@usa.com>
References: <43380B34.20824.18B6494@localhost> <4336C27E.7080708@usa.com>
Message-ID: <43381DA9.2050002@free.fr>

Le 25.09.2005 17:30, Knut Krueger a ??crit :

>Petr Pikal schrieb:
>
>  
>
>>Hi
>>
>>answered hundered times.
>>
>> 
>>
>>    
>>
>>>Dear R people: 
>>>
>>>I would like to superimpose a normal curve on a histogram. 
>>>   
>>>
>>>      
>>>
>>x<-rnorm(150) 
>>h<-hist(x,breaks=15) 
>>xhist<-c(min(h$breaks),h$breaks) 
>>yhist<-c(0,h$density,0) 
>>xfit<-seq(min(x),max(x),length=40) 
>>yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
>>plot(xhist,yhist,type="s",ylim=c(0,max(yhist,yfit))) 
>>lines(xfit,yfit) 
>>
>>
>>Bill 
>>
>>above is e.g. Bill Simpson's answer from 2001. Found from R-site 
>>search  ***histogram density normal***.
>> 
>>
>>    
>>
>Ok If I merge both of your answers I get the graph like in SPSS but the ylab is density instead frequency:
>and I do not have the clicks in SPSS to redo the same graph :-(
>I hav only the Data file and the SPSS plot.
>
>
>x<-5+rnorm(150) 
>
>h<-hist(x,breaks=10,freq = TRUE) 
>#I need this histogramm  with...
>
>xfit<-seq(min(x),max(x),length=40) 
>yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
>
>lines(xfit,yfit) 
>
>
>
>h<-hist(x,breaks=10,prob=T) 
>
>xfit<-seq(min(x),max(x),length=40) 
>yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
># ... this line 
>lines(xfit,yfit)
>
>
>Thanks Knut
>  
>
Ok then.
what you are trying to do doesn't make any sense to me.
(( That does not make much sense (for me) to have the density curve on 
the same scale than frequencies ))
Do you want that :

x<-5+rnorm(150) 
h<-hist(x,breaks=10,freq = TRUE) 
xfit<-seq(min(x),max(x),length=40) 
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 

lines(xfit,yfit * 150 * (h$breaks[2]-h$breaks[1]))


?


-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From W.E.Wolski at ncl.ac.uk  Mon Sep 26 18:20:32 2005
From: W.E.Wolski at ncl.ac.uk (Witold Eryk Wolski)
Date: Mon, 26 Sep 2005 18:20:32 +0200
Subject: [R] regression methods for circular(?) data.
In-Reply-To: <XFMail.050926164656.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050926164656.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <43381FD0.1080902@ncl.ac.uk>

Hi,

I do not know the intercept and slope.
And you have to know them in order to do something like:
ix<-(y < 0.9*(x-50)/200

I am right?

cheers


(Ted Harding) wrote:
> On 26-Sep-05 nwew wrote:
> 
>>Dear R-users,
>>
>>I have the following data
>>
>>x <- runif(300,min=1,max=230)
>>
>>y <- x*0.005 + 0.2
>>y <- y+rnorm(100,mean=0,sd=0.1)
>>y <- y%%1 #  <------- modulo operation
>>plot(x,y)
>>
>>and would like to recapture the slope (0.005) and intercept(0.2).
>>I wonder if there are any clever algorithms to do this. I was
>>looking at the function lm.cirucalar. Is this the method to use?
>>If, which of the references is best too look at?
>>
>>Eryk
> 
> 
> Hi Eryk,
> 
> If you know the modulus (in your case 1.0) and you get data that
> look like the result of your "plot(x,y)", then I wouldn't mess
> about.
> 
> I would simply do something like
> 
> y1<-y
> ix <- ix<-(y < 0.9*(x-50)/200)
> y1[ix] <- y1[ix]+1.0
> lm(y1~x)
> 
> (the constants 0.9/200, -50 being chosen to give a good separation
> on the graph).
> 
> On the other hand, if there are good reasons why this very simple
> approach is not suitable, then if we knew what they were a more
> helpful reply would be easier to formulate!
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 26-Sep-05                                       Time: 15:56:48
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

From mschwartz at mn.rr.com  Mon Sep 26 18:23:36 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 26 Sep 2005 11:23:36 -0500
Subject: [R] Help: x11 position in the Unix environment
In-Reply-To: <ea57975b05092608453c7ff6f4@mail.gmail.com>
References: <ea57975b05092608453c7ff6f4@mail.gmail.com>
Message-ID: <1127751816.4288.34.camel@localhost.localdomain>

On Mon, 2005-09-26 at 17:45 +0200, Shengzhe Wu wrote:
> Hello,
> 
> In the Unix environment, I open a window by x11(). May I specify the
> position of this window by specifying the position of the top left of
> the window as in Windows environment? Or some other parameters can be
> used to do that?
> 
> Thank you,
> Shengzhe


I don't believe so. In general, under Unix/Linux, the Window Manager
determines window positioning upon startup unless the application
overrides this behavior. Some applications let you specify application
window positioning via command line 'geometry' arguments or via the use
of an .Xresources file. 

Some WM's provide more or less functionality for this behavior relative
to user customization. For example, Sawfish provides quite a bit,
whereas Metacity hides much of it.

You may want to check the documentation for the WM that you are using.

There is also an application called Devil's Pie:

  http://www.burtonini.com/blog/computers/devilspie

which provides additional Sawfish-like customization for window
positioning, etc. However, this is global for a given window, not on a
per instance basis.

HTH,

Marc Schwartz



From chabotd at globetrotter.net  Mon Sep 26 18:25:04 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Mon, 26 Sep 2005 12:25:04 -0400
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <mailman.8.1127728801.16099.r-help@stat.math.ethz.ch>
References: <mailman.8.1127728801.16099.r-help@stat.math.ethz.ch>
Message-ID: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>

Hi,

I am fairly new to GAM and started using package mgcv. I like the  
fact that optimal smoothing is automatically used (i.e. df are not  
determined a priori but calculated by the gam procedure).

But the mgcv manual warns that p-level for the smooth can be  
underestimated when df are estimated by the model. Most of the time  
my p-levels are so small that even doubling them would not result in  
a value close to the P=0.05 threshold, but I have one case with P=0.033.

I thought, probably naively, that running a second model with fixed  
df, using the value of df found in the first model. I could not  
achieve this with mgcv: its gam function does not seem to accept  
fractional values of df (in my case 8.377).

So I used the gam package and fixed df to 8.377. The P-value I  
obtained was slightly larger than with mgcv (0.03655 instead of  
0.03328), but it is still < 0.05.

Was this a correct way to get around the "underestimated P-level"?

Furthermore, although the gam.check function of the mgcv package  
suggests to me that the gaussian family (and identity link) are  
adequate for my data, I must say the instructions in R help for  
"family" and in Hastie, T. and Tibshirani, R. (1990) Generalized  
Additive Models are too technical for me. If someone knows a  
reference that explains how to choose model and link, i.e. what tests  
to run on your data before choosing, I would really appreciate it.

Thanks in advance,

Denis Chabot



From ealaca at ucdavis.edu  Mon Sep 26 18:29:18 2005
From: ealaca at ucdavis.edu (Dr. Emilio A. Laca)
Date: Mon, 26 Sep 2005 09:29:18 -0700
Subject: [R] hidden markov models
Message-ID: <86AB6669-65EA-4716-8C6B-5129F477169F@ucdavis.edu>

Dear R community,

I am looking for an R package or other software to study hidden  
Markov models. I need to be able to incorporate multivariate  
emissions and covariates for the transition probabilities. The msm  
package seems almost perfect for my purpose, but I do not think it  
allows multivariate emissions.

I will be grateful for your suggestions.

All the best,

-- 
Emilio A. Laca
One Shields Avenue, 2306 PES Bldg.
Plant Sciences                                               
ealaca at ucdavis.edu
University of California                                fax: (530)  
752-4361
Davis, California  95616                         voice: (530) 754-4083



From tlumley at u.washington.edu  Mon Sep 26 18:54:43 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 26 Sep 2005 09:54:43 -0700 (PDT)
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>
References: <mailman.8.1127728801.16099.r-help@stat.math.ethz.ch>
	<25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>
Message-ID: <Pine.LNX.4.63a.0509260952090.3432@homer22.u.washington.edu>

On Mon, 26 Sep 2005, Denis Chabot wrote:
>
> But the mgcv manual warns that p-level for the smooth can be
> underestimated when df are estimated by the model. Most of the time
> my p-levels are so small that even doubling them would not result in
> a value close to the P=0.05 threshold, but I have one case with P=0.033.
>
> I thought, probably naively, that running a second model with fixed
> df, using the value of df found in the first model. I could not
> achieve this with mgcv: its gam function does not seem to accept
> fractional values of df (in my case 8.377).

No, this won't work.  The problem is the usual one with model selection: 
the p-value is calculated as if the df had been fixed, when really it was 
estimated.

It is likely to be quite hard to get an honest p-value out of something 
that does adaptive smoothing.

 	-thomas



From dsonneborn at ucdavis.edu  Mon Sep 26 19:00:49 2005
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Mon, 26 Sep 2005 10:00:49 -0700
Subject: [R] reading SAS data files
Message-ID: <43382941.50607@yellow.ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050926/f80e1741/attachment.pl

From B.Rowlingson at lancaster.ac.uk  Mon Sep 26 19:01:24 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 26 Sep 2005 18:01:24 +0100
Subject: [R] Help: x11 position in the Unix environment
In-Reply-To: <1127751816.4288.34.camel@localhost.localdomain>
References: <ea57975b05092608453c7ff6f4@mail.gmail.com>
	<1127751816.4288.34.camel@localhost.localdomain>
Message-ID: <43382964.3010206@lancaster.ac.uk>

Marc Schwartz (via MN) wrote:

> 
> I don't believe so. In general, under Unix/Linux, the Window Manager
> determines window positioning upon startup unless the application
> overrides this behavior. Some applications let you specify application
> window positioning via command line 'geometry' arguments or via the use
> of an .Xresources file. 
> 

  Splus used to have extensive control of the graphics window via X 
resources - here's a chunk from an old Xresources file of mine:

splus*background:       blue
splus*Canvas.height:    632
splus*Canvas.width:     800
splus*Command*background:       red
splus*Command*foreground:       cyan
splus*Label*background: cyan
splus*Label*foreground: red
splus*Text*background:  #a0f
splus*Text*foreground:  yellow
splus*colors:   white black white 54 black

  - and so on and so forth. Sadly I dont think such customisation is 
coded into R's X11 graphics window. Looking at the code there is no sign 
of it playing with X11's resource database. Shame.

Baz



From gb at stat.umu.se  Mon Sep 26 19:05:33 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Mon, 26 Sep 2005 19:05:33 +0200
Subject: [R] [ESS]  Indentation in R code
In-Reply-To: <17207.41724.23980.747435@stat.math.ethz.ch>
References: <148ed818050924130247520493@mail.gmail.com>
	<m2u0g93w1l.fsf@macaroni.local>
	<x2u0g9nizm.fsf@turmalin.kubism.ku.dk>
	<17207.41724.23980.747435@stat.math.ethz.ch>
Message-ID: <20050926170533.GA15945@stat.umu.se>

On Mon, Sep 26, 2005 at 09:27:56AM +0200, Martin Maechler wrote:
> I'm crossposting to the ESS-help mailing list which is slightly
> more appropriate here.  [This may be a rare case where
> crossposting seems to make much sense.]
> 
> >>>>> "PD" == Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> >>>>>     on 25 Sep 2005 19:40:45 +0200 writes:
> 
>     PD> Seth Falcon <sfalcon at fhcrc.org> writes:
> 
>     >> On 24 Sep 2005, goran.brostrom at gmail.com wrote:
>     >> 
>     >> > I am using emacs-21.3 when writing R functions on Linux debian, and
>     >> > I am trying to follow the advice i R-exts.pdf (2.1.1) regarding
>     >> > indentation. That is, I set 'c-default-style' to "bsd" and
>     >> > 'c-basic-offset' to 4. However, while this gives me the intended
>     >> > indentation in C code, it doesn't change the behavior in R code; I
>     >> > still get an indentation of size 2. This is my .emacs file after
>     >> > customization:
>     >> >
>     >> > (require 'ess-site)
>     >> > (custom-set-variables
>     >> >  ;; custom-set-variables was added by Custom -- don't edit or
>     >> >  ;; cut/paste it!  Your init file should contain only one such
>     >> >  ;; instance.
>     >> >  '(c-basic-offset 4)
>     >> >  '(c-default-style "bsd"))
>     >> >  (custom-set-faces
>     >> >  ;; custom-set-faces was added by Custom -- don't edit or cut/paste it!
>     >> >  ;; Your init file should contain only one such instance.
>     >> > )
>     >> 
>     >> Not sure if this is the best way, but I have the following after
>     >> loading ess-site:
>     >> 
>     >> (setq ess-indent-level 4)
> 
> 
> 
>     PD> I have (I believe it stems from Martin M. originally):
> 
> yes, most probably  {IIRC, Kurt Hornik was involved too}.
> 
>     PD>      (add-hook 'c-mode-hook '(lambda()
>     PD> 			       (c-set-style "stroustrup")))
> 
> the above is not quite what I have or did recommend, 
> which is rather  "bsd" + "offset 4"  as G??ran has above
> 
> In fact, G??ran mentions the "R-exts" manual and that has
> the following  *before* giving the emacs-lisp statements:
> 
> >> (For GNU Emacs 20: for GNU Emacs 21 use
> >> customization to set the `c-default-style' to `"bsd"' and
> >> `c-basic-offset' to `4'.)
> 
> and indeed, that's what G??ran did and you should do with a
> current emacs, either customize via GUI or,
> in your ~/.emacs file, find the section '(custom-set-variables ...)' and add 
>  '(c-basic-offset 4)
>  '(c-default-style "bsd")
> 
> to the lines already there, or if there's no such section, add
> 
> (custom-set-variables
>   ;; custom-set-variables was added by Custom -- don't edit or cut/paste it!
>   ;; Your init file should contain only one such instance.
>  '(c-basic-offset 4)
>  '(c-default-style "bsd")
> )
> to the end of your ~/.emacs file
[...]

but this is not sufficient to get correct (4) indentation (ess) in R 
functions. We need some reference to ess as well, right? Maybe another
reference to the ESS manual is in order in 'R-exts'?

Thanks for all the help. I got it working now.

G??ran



From HStevens at MUOhio.edu  Mon Sep 26 19:12:31 2005
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Mon, 26 Sep 2005 13:12:31 -0400
Subject: [R] anova on binomial LMER objects
In-Reply-To: <40e66e0b050926060532bee7e0@mail.gmail.com>
References: <4336BC44.8070404@pdf.com>
	<20050925210533.44547.qmail@web50610.mail.yahoo.com>
	<40e66e0b050926060532bee7e0@mail.gmail.com>
Message-ID: <A4B6367C-C734-4E5B-8F3A-74EF4695D097@MUOhio.edu>

Hello all,
1. Does Matrix 0.98-7 fix any of this?
2. Assuming "no", how does one acquire Matrix 0.95-13?
Cheers, and thank you kindly in advance,
Hank

On Sep 26, 2005, at 9:05 AM, Douglas Bates wrote:

> On 9/25/05, Horacio Montenegro <nepossiver at yahoo.com> wrote:
>
>>
>>     Hi Spencer and Robert,
>>
>>     I have found the same behaviour, but only for lme4
>> and Matrix after the 0.96 release. lme4 0.95-10 and
>> Matrix 0.95-13 releases gave "sensible" results. This
>> could be an introduced bug, or a solved bug - you
>> should ask Prof. Bates.
>>
>>     hope this helps, cheers,
>>
>>     Horacio Montenegro
>>
>
> I have run into a couple of other things that the "improvements" from
> the 0.95 series to the 0.96 series has made worse.  This may take a
> while to sort out.  Thanks to Robert Bagchi for the very thorough
> error report.
>
>
>
>>
>> --- Spencer Graves <spencer.graves at pdf.com> wrote:
>>
>>>         I agree:  Something looks strange to me in this
>>> example also;  I have
>>> therefore copied Douglas Bates and  Deepayan Sarkar.
>>>  You've provided a
>>> nice simulation.  If either of them have time to
>>> look at this, I think
>>> they could tell us what is happening here.
>>>
>>>         If you need an answer to your particular problem,
>>> you could run that
>>> simulation 1000 or 1,000 times.  That would tell you
>>> whether to believe
>>> the summary or the anova, or neither.  If you want
>>> to understand the
>>> algorithm, you could walk through the code.
>>> However, "lmer" is a
>>> generic, and I don't have time now to figure out how
>>> to find the source.
>>>   A response from Brian Ripley to a question from me
>>> a couple of days
>>> ago provides a nice summary of how to do that, but I
>>> don't have time to
>>> check that now.
>>>
>>>         Sorry I couldn't help more.
>>>         spencer graves
>>>
>>> Robert Bagchi wrote:
>>>
>>>
>>>> Dear R users,
>>>>
>>>> I have been having problems getting believable
>>>>
>>> estimates from anova on a
>>>
>>>> model fit from lmer. I get the impression that F
>>>>
>>> is being greatly
>>>
>>>> underestimated, as can be seen by running the
>>>>
>>> example I have given below.
>>>
>>>>
>>>> First an explanation of what I'm trying to do. I
>>>>
>>> am trying to fit a glmm
>>>
>>>> with binomial errors to some data. The experiment
>>>>
>>> involves 10
>>>
>>>> shadehouses, divided between 2 light treatments
>>>>
>>> (high, low). Within each
>>>
>>>> shadehouse there are 12 seedlings of each of 2
>>>>
>>> species (hn & sl). 3
>>>
>>>> damage treatments (0, 0.1, 0.25 leaf area removal)
>>>>
>>> were applied to the
>>>
>>>> seedlings (at random) so that there are 4
>>>>
>>> seedlings of each
>>>
>>>> species*damage treatment in each shadehouse.
>>>>
>>> There maybe a shadehouse
>>>
>>>> effect, so I need to include it as a random
>>>>
>>> effect. Light is applied to
>>>
>>>> a shadehouse, so it is outer to shadehouse. The
>>>>
>>> other 2 factors are
>>>
>>>> inner to shadehouse.
>>>>
>>>> We want to assess if light, damage and species
>>>>
>>> affect survival of
>>>
>>>> seedlings. To test this I fitted a binomial mixed
>>>>
>>> effects model with
>>>
>>>> lmer (actually with quasibinomial errors). THe
>>>>
>>> summary function suggests
>>>
>>>> a large effect of both light and species (which
>>>>
>>> agrees with graphical
>>>
>>>> analysis). However, anova produces F values close
>>>>
>>> to 0 and p values
>>>
>>>> close to 1 (see example below).
>>>>
>>>> Is this a bug, or am I doing something
>>>>
>>> fundamentally wrong? If anova
>>>
>>>> doesn't work with lmer is there a way to perform
>>>>
>>> hypothesis tests on
>>>
>>>> fixed effects in an lmer model? I was going to
>>>>
>>> just delete terms and
>>>
>>>> then do liklihood ratio tests, but according to
>>>>
>>> Pinheiro & Bates (p. 87)
>>>
>>>> that's very untrustworthy. Any suggestions?
>>>>
>>>> I'm using R 2.1.1 on windows XP and lme4 0.98-1
>>>>
>>>> Any help will be much appreciated.
>>>>
>>>> many thanks
>>>> Robert
>>>>
>>>>
>>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From Knut.Krueger at usa.com  Sun Sep 25 19:22:23 2005
From: Knut.Krueger at usa.com (Knut Krueger)
Date: Sun, 25 Sep 2005 19:22:23 +0200
Subject: [R] hist(x, ...)  with  normal distribution  curve
In-Reply-To: <43381DA9.2050002@free.fr>
References: <43380B34.20824.18B6494@localhost> <4336C27E.7080708@usa.com>
	<43381DA9.2050002@free.fr>
Message-ID: <4336DCCF.3020808@usa.com>



Romain Francois schrieb:

>Do you want that :
>
>h<-hist(x,breaks=10,freq = TRUE) 
>xfit<-seq(min(x),max(x),length=40) 
>yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
>
>lines(xfit,yfit * 150 * (h$breaks[2]-h$breaks[1]))
>
>
>  
>
Right thats what I want ... but does it make sense to fit the line with 
a try and error multipier (150)

Is there no way to compute the frequency and the distribution line with 
standardised function?
I used SPSS with the data from x<-5+rnorm(150)
Hit the graph-histogramm menue choosed: display normal curve and got the 
result:
http://biostatistic.de/temp/spss1.jpg - just as easy as possible

So, they have any standardised function.
maybe it does not make sence, but i am not able to see why there is such 
a esay to use function in SPSS but not in R
Maybe anybody is able to explane whiy


and my intention:
As a previous computer scientist I am trying to find a way to eleminate 
SPSS an use R
Sure there is a big lack of my statistic knowledge - but the often the 
SPSS user have similar lack  but it is easy to click and view instead to 
try the similar steps in R
But if I am not able to find the steps in R for the common .. 
"SPSS-clicks" , I will never be able to suggest R in the institute to 
the people with mor statistical knowledge but no knowledge about 
computer science ... and command line interpreter

Regards Knut




-- 
Viele Gr????e
Knut Kr??ger
-- 

          Reitpark Einthal

    Leitung: 1 Tierarzt, 1 Berufsreiter 
     Homepage http://www.einthal.de

Eine fachgerechte Betreuung rund um die Uhr.



From HStevens at muohio.edu  Mon Sep 26 19:12:31 2005
From: HStevens at muohio.edu (Martin Henry H. Stevens)
Date: Mon, 26 Sep 2005 13:12:31 -0400
Subject: [R] anova on binomial LMER objects
In-Reply-To: <40e66e0b050926060532bee7e0@mail.gmail.com>
References: <4336BC44.8070404@pdf.com>
	<20050925210533.44547.qmail@web50610.mail.yahoo.com>
	<40e66e0b050926060532bee7e0@mail.gmail.com>
Message-ID: <A4B6367C-C734-4E5B-8F3A-74EF4695D097@MUOhio.edu>

Hello all,
1. Does Matrix 0.98-7 fix any of this?
2. Assuming "no", how does one acquire Matrix 0.95-13?
Cheers, and thank you kindly in advance,
Hank

On Sep 26, 2005, at 9:05 AM, Douglas Bates wrote:

> On 9/25/05, Horacio Montenegro <nepossiver at yahoo.com> wrote:
>
>>
>>     Hi Spencer and Robert,
>>
>>     I have found the same behaviour, but only for lme4
>> and Matrix after the 0.96 release. lme4 0.95-10 and
>> Matrix 0.95-13 releases gave "sensible" results. This
>> could be an introduced bug, or a solved bug - you
>> should ask Prof. Bates.
>>
>>     hope this helps, cheers,
>>
>>     Horacio Montenegro
>>
>
> I have run into a couple of other things that the "improvements" from
> the 0.95 series to the 0.96 series has made worse.  This may take a
> while to sort out.  Thanks to Robert Bagchi for the very thorough
> error report.
>
>
>
>>
>> --- Spencer Graves <spencer.graves at pdf.com> wrote:
>>
>>>         I agree:  Something looks strange to me in this
>>> example also;  I have
>>> therefore copied Douglas Bates and  Deepayan Sarkar.
>>>  You've provided a
>>> nice simulation.  If either of them have time to
>>> look at this, I think
>>> they could tell us what is happening here.
>>>
>>>         If you need an answer to your particular problem,
>>> you could run that
>>> simulation 1000 or 1,000 times.  That would tell you
>>> whether to believe
>>> the summary or the anova, or neither.  If you want
>>> to understand the
>>> algorithm, you could walk through the code.
>>> However, "lmer" is a
>>> generic, and I don't have time now to figure out how
>>> to find the source.
>>>   A response from Brian Ripley to a question from me
>>> a couple of days
>>> ago provides a nice summary of how to do that, but I
>>> don't have time to
>>> check that now.
>>>
>>>         Sorry I couldn't help more.
>>>         spencer graves
>>>
>>> Robert Bagchi wrote:
>>>
>>>
>>>> Dear R users,
>>>>
>>>> I have been having problems getting believable
>>>>
>>> estimates from anova on a
>>>
>>>> model fit from lmer. I get the impression that F
>>>>
>>> is being greatly
>>>
>>>> underestimated, as can be seen by running the
>>>>
>>> example I have given below.
>>>
>>>>
>>>> First an explanation of what I'm trying to do. I
>>>>
>>> am trying to fit a glmm
>>>
>>>> with binomial errors to some data. The experiment
>>>>
>>> involves 10
>>>
>>>> shadehouses, divided between 2 light treatments
>>>>
>>> (high, low). Within each
>>>
>>>> shadehouse there are 12 seedlings of each of 2
>>>>
>>> species (hn & sl). 3
>>>
>>>> damage treatments (0, 0.1, 0.25 leaf area removal)
>>>>
>>> were applied to the
>>>
>>>> seedlings (at random) so that there are 4
>>>>
>>> seedlings of each
>>>
>>>> species*damage treatment in each shadehouse.
>>>>
>>> There maybe a shadehouse
>>>
>>>> effect, so I need to include it as a random
>>>>
>>> effect. Light is applied to
>>>
>>>> a shadehouse, so it is outer to shadehouse. The
>>>>
>>> other 2 factors are
>>>
>>>> inner to shadehouse.
>>>>
>>>> We want to assess if light, damage and species
>>>>
>>> affect survival of
>>>
>>>> seedlings. To test this I fitted a binomial mixed
>>>>
>>> effects model with
>>>
>>>> lmer (actually with quasibinomial errors). THe
>>>>
>>> summary function suggests
>>>
>>>> a large effect of both light and species (which
>>>>
>>> agrees with graphical
>>>
>>>> analysis). However, anova produces F values close
>>>>
>>> to 0 and p values
>>>
>>>> close to 1 (see example below).
>>>>
>>>> Is this a bug, or am I doing something
>>>>
>>> fundamentally wrong? If anova
>>>
>>>> doesn't work with lmer is there a way to perform
>>>>
>>> hypothesis tests on
>>>
>>>> fixed effects in an lmer model? I was going to
>>>>
>>> just delete terms and
>>>
>>>> then do liklihood ratio tests, but according to
>>>>
>>> Pinheiro & Bates (p. 87)
>>>
>>>> that's very untrustworthy. Any suggestions?
>>>>
>>>> I'm using R 2.1.1 on windows XP and lme4 0.98-1
>>>>
>>>> Any help will be much appreciated.
>>>>
>>>> many thanks
>>>> Robert
>>>>
>>>>
>>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Mon Sep 26 19:11:35 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 26 Sep 2005 18:11:35 +0100 (BST)
Subject: [R] regression methods for circular(?) data.
In-Reply-To: <43381FD0.1080902@ncl.ac.uk>
Message-ID: <XFMail.050926181135.Ted.Harding@nessie.mcc.ac.uk>

On 26-Sep-05 Witold Eryk Wolski wrote:
> Hi,
> 
> I do not know the intercept and slope.
> And you have to know them in order to do something like:
> ix<-(y < 0.9*(x-50)/200
> 
> I am right?
> 
> cheers

Although I really knew them from the way you generated the data,
I "pretended" I did not know them.

Read below: "If you know the modulus (in your case 1.0)" -- I did
assume that this was known, i.e. that the data "wrap round" to 0
above 1.0. Also: "the constants 0.9/200, -50 being chosen to give
a good separation on the graph" -- I plotted the data, and saw that
the "wrapped" data were well separated, and that 0.9*(x-50)/200
was an adequate discriminant function. This was estimated purely by
eye, by looking at the graph, to find some line that went between
the two groups of data; no attempt was made to calculate anything
precisely. Apart from assuming that the modulus was 1.0, and that
the well-separated data at the bottom right of the graph were
"wrapped round" data, no other information was used by me!

So the question remains: If you can assume that the modulus is 1.0,
and that the wrapped-round data will be well separated, then all
is simple. All you need to do is to "unwrap" the "wrapped" data
by adding 1.0, having first identified them by virtue of their
obvious separation. Then you can estimate the slope by using 'lm'.

But:-- if you, Witold, can not assume these two things for your
real data, what can we assume in considering your question?
Is the modulus unknown, for instance? Is the scatter so large that
the groups are not well separated? Might we have "twice-wrapped"
data (i.e. original y > 2)? 

In short, do your real data look like the data you sent us, and
are they wrapped at 1.0? or what?

With thanks, and best wishes,
Ted.

> (Ted Harding) wrote:
>> On 26-Sep-05 nwew wrote:
>> 
>>>Dear R-users,
>>>
>>>I have the following data
>>>
>>>x <- runif(300,min=1,max=230)
>>>
>>>y <- x*0.005 + 0.2
>>>y <- y+rnorm(100,mean=0,sd=0.1)
>>>y <- y%%1 #  <------- modulo operation
>>>plot(x,y)
>>>
>>>and would like to recapture the slope (0.005) and intercept(0.2).
>>>I wonder if there are any clever algorithms to do this. I was
>>>looking at the function lm.cirucalar. Is this the method to use?
>>>If, which of the references is best too look at?
>>>
>>>Eryk
>> 
>> 
>> Hi Eryk,
>> 
>> If you know the modulus (in your case 1.0) and you get data that
>> look like the result of your "plot(x,y)", then I wouldn't mess
>> about.
>> 
>> I would simply do something like
>> 
>> y1<-y
>> ix <- ix<-(y < 0.9*(x-50)/200)
>> y1[ix] <- y1[ix]+1.0
>> lm(y1~x)
>> 
>> (the constants 0.9/200, -50 being chosen to give a good separation
>> on the graph).
>> 
>> On the other hand, if there are good reasons why this very simple
>> approach is not suitable, then if we knew what they were a more
>> helpful reply would be easier to formulate!
>> 
>> Best wishes,
>> Ted.
>> 
>> 
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861
>> Date: 26-Sep-05                                       Time: 15:56:48
>> ------------------------------ XFMail ------------------------------
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> 
>> 

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Sep-05                                       Time: 18:08:28
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Mon Sep 26 19:57:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Sep 2005 18:57:39 +0100 (BST)
Subject: [R] anova on binomial LMER objects
In-Reply-To: <A4B6367C-C734-4E5B-8F3A-74EF4695D097@MUOhio.edu>
References: <4336BC44.8070404@pdf.com>
	<20050925210533.44547.qmail@web50610.mail.yahoo.com>
	<40e66e0b050926060532bee7e0@mail.gmail.com>
	<A4B6367C-C734-4E5B-8F3A-74EF4695D097@MUOhio.edu>
Message-ID: <Pine.LNX.4.61.0509261856001.17272@gannet.stats>

On Mon, 26 Sep 2005, Martin Henry H. Stevens wrote:

> Hello all,
> 1. Does Matrix 0.98-7 fix any of this?
> 2. Assuming "no", how does one acquire Matrix 0.95-13?

It is in the Archive on CRAN, e.g.

http://cran.r-project.org/src/contrib/Archive/M/Matrix_0.95-13.tar.gz

> Cheers, and thank you kindly in advance,
> Hank
>
> On Sep 26, 2005, at 9:05 AM, Douglas Bates wrote:
>
>> On 9/25/05, Horacio Montenegro <nepossiver at yahoo.com> wrote:
>>
>>>
>>>     Hi Spencer and Robert,
>>>
>>>     I have found the same behaviour, but only for lme4
>>> and Matrix after the 0.96 release. lme4 0.95-10 and
>>> Matrix 0.95-13 releases gave "sensible" results. This
>>> could be an introduced bug, or a solved bug - you
>>> should ask Prof. Bates.
>>>
>>>     hope this helps, cheers,
>>>
>>>     Horacio Montenegro
>>>
>>
>> I have run into a couple of other things that the "improvements" from
>> the 0.95 series to the 0.96 series has made worse.  This may take a
>> while to sort out.  Thanks to Robert Bagchi for the very thorough
>> error report.
>>
>>
>>
>>>
>>> --- Spencer Graves <spencer.graves at pdf.com> wrote:
>>>
>>>>         I agree:  Something looks strange to me in this
>>>> example also;  I have
>>>> therefore copied Douglas Bates and  Deepayan Sarkar.
>>>>  You've provided a
>>>> nice simulation.  If either of them have time to
>>>> look at this, I think
>>>> they could tell us what is happening here.
>>>>
>>>>         If you need an answer to your particular problem,
>>>> you could run that
>>>> simulation 1000 or 1,000 times.  That would tell you
>>>> whether to believe
>>>> the summary or the anova, or neither.  If you want
>>>> to understand the
>>>> algorithm, you could walk through the code.
>>>> However, "lmer" is a
>>>> generic, and I don't have time now to figure out how
>>>> to find the source.
>>>>   A response from Brian Ripley to a question from me
>>>> a couple of days
>>>> ago provides a nice summary of how to do that, but I
>>>> don't have time to
>>>> check that now.
>>>>
>>>>         Sorry I couldn't help more.
>>>>         spencer graves
>>>>
>>>> Robert Bagchi wrote:
>>>>
>>>>
>>>>> Dear R users,
>>>>>
>>>>> I have been having problems getting believable
>>>>>
>>>> estimates from anova on a
>>>>
>>>>> model fit from lmer. I get the impression that F
>>>>>
>>>> is being greatly
>>>>
>>>>> underestimated, as can be seen by running the
>>>>>
>>>> example I have given below.
>>>>
>>>>>
>>>>> First an explanation of what I'm trying to do. I
>>>>>
>>>> am trying to fit a glmm
>>>>
>>>>> with binomial errors to some data. The experiment
>>>>>
>>>> involves 10
>>>>
>>>>> shadehouses, divided between 2 light treatments
>>>>>
>>>> (high, low). Within each
>>>>
>>>>> shadehouse there are 12 seedlings of each of 2
>>>>>
>>>> species (hn & sl). 3
>>>>
>>>>> damage treatments (0, 0.1, 0.25 leaf area removal)
>>>>>
>>>> were applied to the
>>>>
>>>>> seedlings (at random) so that there are 4
>>>>>
>>>> seedlings of each
>>>>
>>>>> species*damage treatment in each shadehouse.
>>>>>
>>>> There maybe a shadehouse
>>>>
>>>>> effect, so I need to include it as a random
>>>>>
>>>> effect. Light is applied to
>>>>
>>>>> a shadehouse, so it is outer to shadehouse. The
>>>>>
>>>> other 2 factors are
>>>>
>>>>> inner to shadehouse.
>>>>>
>>>>> We want to assess if light, damage and species
>>>>>
>>>> affect survival of
>>>>
>>>>> seedlings. To test this I fitted a binomial mixed
>>>>>
>>>> effects model with
>>>>
>>>>> lmer (actually with quasibinomial errors). THe
>>>>>
>>>> summary function suggests
>>>>
>>>>> a large effect of both light and species (which
>>>>>
>>>> agrees with graphical
>>>>
>>>>> analysis). However, anova produces F values close
>>>>>
>>>> to 0 and p values
>>>>
>>>>> close to 1 (see example below).
>>>>>
>>>>> Is this a bug, or am I doing something
>>>>>
>>>> fundamentally wrong? If anova
>>>>
>>>>> doesn't work with lmer is there a way to perform
>>>>>
>>>> hypothesis tests on
>>>>
>>>>> fixed effects in an lmer model? I was going to
>>>>>
>>>> just delete terms and
>>>>
>>>>> then do liklihood ratio tests, but according to
>>>>>
>>>> Pinheiro & Bates (p. 87)
>>>>
>>>>> that's very untrustworthy. Any suggestions?
>>>>>
>>>>> I'm using R 2.1.1 on windows XP and lme4 0.98-1
>>>>>
>>>>> Any help will be much appreciated.
>>>>>
>>>>> many thanks
>>>>> Robert
>>>>>
>>>>>
>>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-
>>> guide.html
>>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-
>> guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From W.E.Wolski at ncl.ac.uk  Mon Sep 26 20:01:48 2005
From: W.E.Wolski at ncl.ac.uk (Witold Eryk Wolski)
Date: Mon, 26 Sep 2005 20:01:48 +0200
Subject: [R] regression methods for circular(?) data.
In-Reply-To: <XFMail.050926181135.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050926181135.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <4338378C.7080407@ncl.ac.uk>

Ted,

I agree with you that if you unwrap the data you can use lm.
And you can separate the data in the way you describe. However, if you 
have thousands of such datasets I do not want to do it by "looking at 
the graph".

Yes the scatter may be larger as in the example and range(y) may be 
larger than 2.

And as you said in order to unwrap the data you have to separate them 
first. It would be easy to do it using for example single linkage 
clustering if they were no overlaps (but they do sometimes). So I were 
just wondering if there are no more fancy methods to do this.

Thanks,

cheers



(Ted Harding) wrote:
> On 26-Sep-05 Witold Eryk Wolski wrote:
> 
>>Hi,
>>
>>I do not know the intercept and slope.
>>And you have to know them in order to do something like:
>>ix<-(y < 0.9*(x-50)/200
>>
>>I am right?
>>
>>cheers
> 
> 
> Although I really knew them from the way you generated the data,
> I "pretended" I did not know them.
> 
> Read below: "If you know the modulus (in your case 1.0)" -- I did
> assume that this was known, i.e. that the data "wrap round" to 0
> above 1.0. Also: "the constants 0.9/200, -50 being chosen to give
> a good separation on the graph" -- I plotted the data, and saw that
> the "wrapped" data were well separated, and that 0.9*(x-50)/200
> was an adequate discriminant function. This was estimated purely by
> eye, by looking at the graph, to find some line that went between
> the two groups of data; no attempt was made to calculate anything
> precisely. Apart from assuming that the modulus was 1.0, and that
> the well-separated data at the bottom right of the graph were
> "wrapped round" data, no other information was used by me!
> 
> So the question remains: If you can assume that the modulus is 1.0,
> and that the wrapped-round data will be well separated, then all
> is simple. All you need to do is to "unwrap" the "wrapped" data
> by adding 1.0, having first identified them by virtue of their
> obvious separation. Then you can estimate the slope by using 'lm'.
> 
> But:-- if you, Witold, can not assume these two things for your
> real data, what can we assume in considering your question?
> Is the modulus unknown, for instance? Is the scatter so large that
> the groups are not well separated? Might we have "twice-wrapped"
> data (i.e. original y > 2)? 
> 
> In short, do your real data look like the data you sent us, and
> are they wrapped at 1.0? or what?
> 
> With thanks, and best wishes,
> Ted.
> 
> 
>>(Ted Harding) wrote:
>>
>>>On 26-Sep-05 nwew wrote:
>>>
>>>
>>>>Dear R-users,
>>>>
>>>>I have the following data
>>>>
>>>>x <- runif(300,min=1,max=230)
>>>>
>>>>y <- x*0.005 + 0.2
>>>>y <- y+rnorm(100,mean=0,sd=0.1)
>>>>y <- y%%1 #  <------- modulo operation
>>>>plot(x,y)
>>>>
>>>>and would like to recapture the slope (0.005) and intercept(0.2).
>>>>I wonder if there are any clever algorithms to do this. I was
>>>>looking at the function lm.cirucalar. Is this the method to use?
>>>>If, which of the references is best too look at?
>>>>
>>>>Eryk
>>>
>>>
>>>Hi Eryk,
>>>
>>>If you know the modulus (in your case 1.0) and you get data that
>>>look like the result of your "plot(x,y)", then I wouldn't mess
>>>about.
>>>
>>>I would simply do something like
>>>
>>>y1<-y
>>>ix <- ix<-(y < 0.9*(x-50)/200)
>>>y1[ix] <- y1[ix]+1.0
>>>lm(y1~x)
>>>
>>>(the constants 0.9/200, -50 being chosen to give a good separation
>>>on the graph).
>>>
>>>On the other hand, if there are good reasons why this very simple
>>>approach is not suitable, then if we knew what they were a more
>>>helpful reply would be easier to formulate!
>>>
>>>Best wishes,
>>>Ted.
>>>
>>>
>>>--------------------------------------------------------------------
>>>E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>>>Fax-to-email: +44 (0)870 094 0861
>>>Date: 26-Sep-05                                       Time: 15:56:48
>>>------------------------------ XFMail ------------------------------
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 26-Sep-05                                       Time: 18:08:28
> ------------------------------ XFMail ------------------------------
> 
> 

From ripley at stats.ox.ac.uk  Mon Sep 26 20:54:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Sep 2005 19:54:04 +0100 (BST)
Subject: [R] reading SAS data files
In-Reply-To: <43382941.50607@yellow.ucdavis.edu>
References: <43382941.50607@yellow.ucdavis.edu>
Message-ID: <Pine.LNX.4.61.0509261950230.14085@gannet.stats>

On Mon, 26 Sep 2005, Dean Sonneborn wrote:

> I am attempting to read in a SAS 9.1 data file. After starting R I
> change to the directory containing the sas data file and use the "dir"
> command to confirm that it is there. Then I run the following R-code:
>
> library(foreign)
>
> sashome <- "/Program Files/SAS/SAS 9.1"
>
> test<-read.ssd(file.path(sashome), "pcb",
>
>         sascmd = file.path(sashome, "sas.exe"))
>
>
> 	but R responds with:
>
> SAS failed.  SAS program at C:\DOCUME~1\DSONNE~1\LOCALS~1\Temp\Rtmp3540\file16169.sas
> The log file will be file16169.log in the current directory
> Warning message:
> SAS return code was 2 in: read.ssd(file.path(sashome), "pcb", sascmd = file.path(sashome,
>
>
> 	the SAS log file contain this:
>
> NOTE: Copyright (c) 2002-2003 by SAS Institute Inc., Cary, NC, USA.
> NOTE: SAS (r) 9.1 (TS1M3)
>      Licensed to UNIV OF CA/DAVIS, Site 0029107010.
> NOTE: This session is executing on the XP_PRO  platform.
>
>
>
> NOTE: SAS initialization used:
>      real time           0.13 seconds
>      cpu time            0.18 seconds
>
> 1          libname src2rd '/Program Files/SAS/SAS 9.1';
> NOTE: Libref SRC2RD was successfully assigned as follows:
>      Engine:        V9
>      Physical Name: C:\Program Files\SAS\SAS 9.1
> 2          libname rd xport 'C:\DOCUME~1\DSONNE~1\LOCALS~1\Temp\Rtmp3540\file26090';
> NOTE: Libref RD was successfully assigned as follows:
>      Engine:        XPORT
>      Physical Name: C:\DOCUME~1\DSONNE~1\LOCALS~1\Temp\Rtmp3540\file26090
> 3          proc copy in=src2rd out=rd;
> 4          select pcb ;
> ERROR: The file SRC2RD.PCB (memtype=ALL) was not found, but appears on a SELECT statement.
> ERROR: The file SRC2RD.PCB (memtype=ALL) was not found, but appears on a SELECT statement.
> ERROR: The file SRC2RD.PCB (memtype=ALL) was not found, but appears on a SELECT statement.
> WARNING: Input library SRC2RD is empty.
> NOTE: Statements not processed because of errors noted above.
> NOTE: The SAS System stopped processing this step because of errors.
> NOTE: PROCEDURE COPY used (Total process time):
>      real time           0.00 seconds
>      cpu time            0.01 seconds
>
>
> ERROR: Errors printed on page 1.
> ERROR: Errors printed on page 1.
> ERROR: Errors printed on page 1.
>
> NOTE: SAS Institute Inc., SAS Campus Drive, Cary, NC USA 27513-2414
> NOTE: The SAS System used:
>      real time           0.16 seconds
>      cpu time            0.20 seconds
>
> Does anyone see what I am doing incorrectly and can they offer any 
> suggestions about getting this to run correctly? I'm not sure where SAS 
> is expecting to find the data file. I have it in the default R 
> directory. Is this where SAS is looking for it or does it need to be 
> somewhere else?

I suggest you read the help file for read.ssd.  You are not telling it 
where the SAS files are correctly.  See the extracts below and the 
examples.

Usage:

      read.ssd(libname, sectionnames,
         tmpXport=tempfile(), tmpProgLoc=tempfile(), sascmd="sas")

Arguments:

  libname: character string defining the SAS library (usually a
           directory reference)

sectionnames: character vector giving member names. These are files in
           the 'libname' directory. They will usually have a '.ssd0x' or
           '.sas7bdat' extension, which should be omitted.

The files are not in the SAS homne directory, but the `default R 
directory' (perhaps you mean the working directory?).


> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Mon Sep 26 22:02:37 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 26 Sep 2005 21:02:37 +0100 (BST)
Subject: [R] regression methods for circular(?) data.
In-Reply-To: <4338378C.7080407@ncl.ac.uk>
Message-ID: <XFMail.050926210237.Ted.Harding@nessie.mcc.ac.uk>

On 26-Sep-05 Witold Eryk Wolski wrote:
> Ted,
> 
> I agree with you that if you unwrap the data you can use lm.
> And you can separate the data in the way you describe. However, if you 
> have thousands of such datasets I do not want to do it by "looking at 
> the graph".
> 
> Yes the scatter may be larger as in the example and range(y) may be 
> larger than 2.
> 
> And as you said in order to unwrap the data you have to separate them 
> first. It would be easy to do it using for example single linkage 
> clustering if they were no overlaps (but they do sometimes). So I were 
> just wondering if there are no more fancy methods to do this.

OK, the problems are now clearer! So we cannot rely on separation
(though there would be ways to detect this automatically if it could
be relied on).

This is where real experts on unwrapping circular data should step in,
but my immediate suggestion would be that developing something out
of the following should be useful.

First, generate the data so that we have something to work with:

  x <- runif(300,min=1,max=230)
  y <- x*0.005 + 0.2
  y <- y+rnorm(100,mean=0,sd=0.1)
  y0 <- y%%1 #  <------- modulo operation

(I've called the wrapped data "y0").

Now, assume

A. That we know the modulus is 1.0

B. That we are looking for a model y0 = (a*x + b)%%1.0

C. The we do have some idea about a range of values for a and b,
   say 0 < a < 0.01 and 0 < b < 1.0

Now try the following and inspect what you get:

  M<-numeric(101)

  for(i in (0:100)){v<-(i*0.01/100);
    M[i+1]<-max(Mod(fft((y0-v*x-0.0)%%1)*2*pi))
  }
  plot(0.01*(0:100)/100,M,ylim=c(0,1000))

  for(j in 0.5*(0:10)/10){
    for(i in (0:100)){
      v<-(i*0.01/100);
      M[i+1]<-max(Mod(fft((y0-v*x-j)%%1)*2*pi))
    }
    points(0.01*(0:100)/100,M)
  }

This gives an indication that a good value for 'a' ('i' in the
plots) is about 0.5 (or slightly larger) for some value of 'b'
('j' in the plots), from which, conditioning on this, a value
for b could be obtained similarly. The plots from the above do
not distinguish between the curves for different values of 'b';
a method of indicating this would be useful.

Just a suggestion. There may be, in some R package, a function
which implements this approach in a better way.

Over to the gurus at this point!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Sep-05                                       Time: 20:54:50
------------------------------ XFMail ------------------------------



From i.visser at uva.nl  Mon Sep 26 22:24:56 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Mon, 26 Sep 2005 22:24:56 +0200
Subject: [R] hidden markov models
In-Reply-To: <86AB6669-65EA-4716-8C6B-5129F477169F@ucdavis.edu>
Message-ID: <BF5E25B8.8E41%i.visser@uva.nl>

Emilio,
The depmix package on cran has multivariate distributions and the
possibility of (linear) covariates on the parameters.
If you have questions about its use feel free to contact me,
best, ingmar visser


On 9/26/05 6:29 PM, "Dr. Emilio A. Laca" <ealaca at ucdavis.edu> wrote:

> Dear R community,
> 
> I am looking for an R package or other software to study hidden
> Markov models. I need to be able to incorporate multivariate
> emissions and covariates for the transition probabilities. The msm
> package seems almost perfect for my purpose, but I do not think it
> allows multivariate emissions.
> 
> I will be grateful for your suggestions.
> 
> All the best,

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From gerifalte28 at hotmail.com  Mon Sep 26 22:35:35 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 26 Sep 2005 20:35:35 +0000
Subject: [R] histogram - one bin for all values larger than a certain
	value
In-Reply-To: <000b01c5c29f$48232aa0$0ca54e8d@asnb22301>
Message-ID: <BAY103-F26769F97630A9483CECFE3A68B0@phx.gbl>

x=runif(100,0,40)
hist(x, breaks=c(0,1,2,3,4,5,6,7,8,9,10,40))

Is this what you had in mind?

Francisco

>From: "Florian Defregger" <florian.defregger at ku-eichstaett.de>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] histogram - one bin for all values larger than a certain value
>Date: Mon, 26 Sep 2005 15:36:21 +0200
>
>Dear all,
>I wonder if I can put together a histogram where one bin contains all the
>values that are larger than a certain specified value.
>
>Example:
>I have values ranging from 0 to 40 and I want 10 bins from 0 to 10, i.e. 
>for
>the intervals [0,1), [1,2) , ..., [9,10). And then I want one last bin 
>which
>contains all the values larger than 10, i.e. for the interval [10, 40).
>
>Thanks,
>Florian
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From jross at openvistas.net  Mon Sep 26 23:57:27 2005
From: jross at openvistas.net (Jeff Ross)
Date: Mon, 26 Sep 2005 15:57:27 -0600
Subject: [R] Make check fails on d-p-q-r-tests.R...
Message-ID: <330a15840cdb7ac1f5a590cec1396780@openvistas.net>

Hi,

R-2.1.1
OS:  OpenBSD-current (3.8) on i386
Compiler:gcc version 3.3.5 (propolice)
Thread model: single

configure \
  --with-readline \
  --with-tcltk \
  --with-tcl-config=/usr/local/lib/tcl8.4/tclConfig.sh \
  --with-tk-config=/usr/local/lib/tk8.4/tkConfig.sh \
  --with-libpng \
  --with-jpeglib \
  --with-zlib \
  --with-bzlib \
  --with-pcre \
  --with-libiconv-prefix=/usr/local/


I'm brand new to R so I don't know how critical this error is.

Here's the last bit what make check FORCE=FORCE outputs:

running code in 'grDevices-Ex.R' ... OK
comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.prev' ... OK
running code in 'graphics-Ex.R' ... OK
comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.prev' ... OK
running code in 'stats-Ex.R' ... OK
comparing 'stats-Ex.Rout' to 'stats-Ex.Rout.prev' ... OK
running code in 'datasets-Ex.R' ... OK
comparing 'datasets-Ex.Rout' to 'datasets-Ex.Rout.prev' ... OK
running code in 'methods-Ex.R' ... OK
comparing 'methods-Ex.Rout' to 'methods-Ex.Rout.prev' ... OK
running code in 'grid-Ex.R' ... OK
comparing 'grid-Ex.Rout' to 'grid-Ex.Rout.prev' ... OK
running code in 'splines-Ex.R' ... OK
comparing 'splines-Ex.Rout' to 'splines-Ex.Rout.prev' ... OK
running code in 'stats4-Ex.R' ... OK
comparing 'stats4-Ex.Rout' to 'stats4-Ex.Rout.prev' ... OK
running code in 'tcltk-Ex.R' ... OK
comparing 'tcltk-Ex.Rout' to 'tcltk-Ex.Rout.prev' ... OK
updating test dependencies
`Makedeps' is up to date.
running strict specific tests
running code in 'eval-etc.R' ... OK
comparing 'eval-etc.Rout' to './eval-etc.Rout.save' ... OK
running code in 'simple-true.R' ... OK
comparing 'simple-true.Rout' to './simple-true.Rout.save' ... OK
running code in 'arith-true.R' ... OK
comparing 'arith-true.Rout' to './arith-true.Rout.save' ... OK
running code in 'arith.R' ... OK
comparing 'arith.Rout' to './arith.Rout.save' ... OK
running code in 'lm-tests.R' ... OK
comparing 'lm-tests.Rout' to './lm-tests.Rout.save' ... OK
running code in 'primitive-funs.R' ... OK
comparing 'primitive-funs.Rout' to './primitive-funs.Rout.save' ... OK
running code in 'ok-errors.R' ... OK
comparing 'ok-errors.Rout' to './ok-errors.Rout.save' ... OK
running code in 'method-dispatch.R' ... OK
comparing 'method-dispatch.Rout' to './method-dispatch.Rout.save' ... OK
running code in 'd-p-q-r-tests.R' ...*** Error code 1
Stop in /usr/local/src/R-2.1.1/tests.
*** Error code 1

Stop in /usr/local/src/R-2.1.1/tests (line 206 of Makefile).
*** Error code 1

Stop in /usr/local/src/R-2.1.1/tests (line 191 of Makefile).


Tried finding this error in the archives to no avail.  If it is okay to
ignore this error, how can I skip this test?

Thanks for any input!

Jeff Ross



From mathdoc2be at hotmail.com  Tue Sep 27 00:08:08 2005
From: mathdoc2be at hotmail.com (C Tate)
Date: Mon, 26 Sep 2005 18:08:08 -0400
Subject: [R] plotting multiple plots on a single graph
Message-ID: <BAY102-F14CB02A7C09A1E29CD7FD6828B0@phx.gbl>



From prm at runbox.us  Tue Sep 27 01:12:38 2005
From: prm at runbox.us (Paul MacManus)
Date: Mon, 26 Sep 2005 19:12:38 -0400 (EDT)
Subject: [R] less precision, please!
In-Reply-To: <17187.2280.225482.62964@stat.math.ethz.ch>
References: <E1EDsUn-0003Nb-BN@garm.runbox.com> <432220EB.5030204@stats.uwo.ca>
	<17187.2280.225482.62964@stat.math.ethz.ch>
Message-ID: <E1EK29G-0007OD-O9@garm.runbox.com>

> 
>     Duncan> On 9/9/2005 7:41 PM, Paul MacManus wrote:
>     >> I need to run qbeta on a set of 500K different parameter
>     >> pairs (with a fixed quantile). For most pairs qbeta finds
>     >> the solution very quickly but for a substantial minority
>     >> of the cases qbeta is very slow. This occurs when the
>     >> solution is very close to zero. qbeta is getting answers
>     >> to a precision of about 16 decimal places. I don't need
>     >> that accuracy. Is there any way to set the precision of
>     >> R's calculations to, say, 9 decimal places and so speed
>     >> up the whole process?
>     >> 
>     >> I could, of course, avoid this problem by not running
>     >> qbeta when I know the solution is going to be
>     >> sufficiently small but I'm more interested in ways to
>     >> adjust the precision of calculations in R.
> 
>     Duncan> There's no general way to do this.  The function
>     Duncan> that implements qbeta may have some tuning
>     Duncan> parameters (I haven't looked), but they aren't
>     Duncan> usually needed, and aren't exposed in R.
> 
> Yes.
> 
> However, I've had thoughts in the past on possibly providing such
> a possibility from both R and C level.  One problem is that
> ``for symmetry reasons'' you would want to have this ``for all functions'' 
> which would need a lot of work, for something that's really not
> of too high a need.   
> I agree that qbeta() can be particularly "nasty".  I'm open to
> more in-depth discussion on this -- after R 2.2.0 is out
> 
>     Duncan> If you want a quick approximation, I'd suggest doing
>     Duncan> your calculation on a grid of values and using
>     Duncan> approx() to interpolate.
> 
> yes, or approxfun() {which prefer for its UI},
> or even more smoothly  using  spline() or splinefun() {again
> preferably the latter}.
> 
> One problem may be that these are only for 1-D interpolation and
> qbeta() depends on three principal arguments.
> Package 'akima' provides somewhat smooth 2-D interpolation.
> 


Hi again,

      Thank you both for your feedback and my apologies for not replying sooner. 

Tunable parameters would be nice to have but they are probably not really necessary. Workarounds of one type or another, such as the ones you suggested, always seem to be available. 

The qbeta issue, specifically, is more interesting. The surfaces associated with beta and inverse beta functions are notoriously badly behaved and developing functions that deal in a good way with the full range of parameters is very tricky. qbeta() does a very good job in general but it is not surprising that it has trouble at times. The underlying algorithms that qbeta() uses seems to be good. I would be interested in talking more about the qbeta issue  when you have more time.

                  Best, Paul



From yuleih at umich.edu  Tue Sep 27 01:56:13 2005
From: yuleih at umich.edu (Yulei He)
Date: Mon, 26 Sep 2005 19:56:13 -0400 (EDT)
Subject: [R] questions about boxplots
Message-ID: <Pine.LNX.4.63.0509261952340.3276@xevious.gpcc.itd.umich.edu>

Hi, there.

I have two questions about using R to create boxplots.

1. The function boxplot() plots the outliers. How can I label the exact 
values arount these outlier points? Does R have an option allow me to 
do that?

2. How can I put two boxplots in one x-y axis?

Thanks.

Yulei


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Yulei He
276 Grove St. Apt 3
Newton, MA 02466
617-796-7834(H)
617-432-3428(O)
617-432-3435(fax)
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$



From chrisb at fcdarwin.org.ec  Mon Sep 26 18:09:38 2005
From: chrisb at fcdarwin.org.ec (Chris Buddenhagen)
Date: Mon, 26 Sep 2005 10:09:38 -0600
Subject: [R] dates are shown as X15.Feb.03
Message-ID: <004401c5c2b4$b23cad70$4c01a8c0@Chris>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050926/330a587c/attachment.pl

From carl.anderson at qimr.edu.au  Tue Sep 27 02:23:36 2005
From: carl.anderson at qimr.edu.au (Carl Anderson)
Date: Tue, 27 Sep 2005 10:23:36 +1000
Subject: [R] Error Message - Error: symbol print-name too long
References: <BAY101-F143604833C234BBA754EC5828B0@phx.gbl>
	<4338062C.2000506@wald.ucdavis.edu>
Message-ID: <001001c5c2f9$b44c9450$37a06298@kylie>

Duncan,

Thank you for your help. I am pleased to say your 'very wild guess' was 
exactly correct. Can't believe I missed it!

Carl
----- Original Message ----- 
From: "Duncan Temple Lang" <duncan at wald.ucdavis.edu>
To: "Carl Anderson" <anderson_carl at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>; <carl.anderson at qimr.edu.au>
Sent: Tuesday, September 27, 2005 12:31 AM
Subject: Re: [R] Error Message - Error: symbol print-name too long


> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
>
> You aren't giving us much to go on, so I can only
> make a very wild guess.  Check that your file
> doesn't have a stray ` character in it.
> R will start reading from that point on and try
> to make this an internal symbol.  If it doesn't
> find the closing ` for too many characters,
> it gives the error message you see.
>
> I have seen this once before and that is what the cause
> was - a stray ` put in by hitting the ` key rather than Esc.
>
>
>
> Carl Anderson wrote:
>> Dear All,
>>
>> I write to ask for information regarding the error message:
>>
>> Error: symbol print-name too long.
>>
>> I am afraid that I can't include any code to help with any further 
>> diagnosis
>> of the problem as the code is far too long to be of any use, and I have 
>> not
>> been able to re-create the problem in shorter example codes.
>>
>> I have searched the R manual and help pages and found no mention of this
>> error message.
>>
>> All help appreciated,
>>
>> Carl A Anderson.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> - --
> Duncan Temple Lang                duncan at wald.ucdavis.edu
> Department of Statistics          work:  (530) 752-4782
> 371 Kerr Hall                     fax:   (530) 752-7099
> One Shields Ave.
> University of California at Davis
> Davis, CA 95616, USA
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.2 (Darwin)
> Comment: Using GnuPG with Thunderbird - http://enigmail.mozdev.org
>
> iD8DBQFDOAYs9p/Jzwa2QP4RAhtAAJ97sqWdUTOPjtZ2RMJR0qcfyjIAZgCfZLF1
> IXTk0bY5RQUD2e+8VlzLpw4=
> =+pim
> -----END PGP SIGNATURE-----
>



From murdoch at stats.uwo.ca  Tue Sep 27 05:04:21 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 26 Sep 2005 23:04:21 -0400
Subject: [R] dates are shown as X15.Feb.03
In-Reply-To: <004401c5c2b4$b23cad70$4c01a8c0@Chris>
References: <004401c5c2b4$b23cad70$4c01a8c0@Chris>
Message-ID: <4338B6B5.8020909@stats.uwo.ca>

Chris Buddenhagen wrote:
> Why is R recognizing dates like this?

Recognizing and showing are different things.  Which are you complaining 
about?  What are you doing to cause these to be recognized/shown?

Duncan Murdoch



From weigand.stephen at charter.net  Tue Sep 27 05:16:50 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Mon, 26 Sep 2005 22:16:50 -0500
Subject: [R] questions about boxplots
In-Reply-To: <Pine.LNX.4.63.0509261952340.3276@xevious.gpcc.itd.umich.edu>
References: <Pine.LNX.4.63.0509261952340.3276@xevious.gpcc.itd.umich.edu>
Message-ID: <808c61478ca173549be932c3ba71b6d5@charter.net>

Dear Yulei,

On Sep 26, 2005, at 6:56 PM, Yulei He wrote:

> Hi, there.
>
> I have two questions about using R to create boxplots.
>
> 1. The function boxplot() plots the outliers. How can I label the exact
> values arount these outlier points? Does R have an option allow me to
> do that?

You can use identify(). Here's a toy example

set.seed(1)
y <- rt(30, 3)
boxplot(y)
identify(x = rep(1,30), y = y, label = format(y, digits = 2))
### now click on the outlier in the plot and you should see "-7.398"
### beside the outlier

> 2. How can I put two boxplots in one x-y axis?

x <- rnorm(10)
y <- rnorm(20, 3, 5)
z <- runif(30)

boxplot(x, y, z, labels = c("x", "y", "z"))
### - or -
boxplot(list(x = x, y = y, z = z))


> Thanks.
>
> Yulei
>

Stephen Weigand
Rochester, Minnesota, USA



From vinum at iinet.net.au  Tue Sep 27 05:40:51 2005
From: vinum at iinet.net.au (John Charles Considine)
Date: Tue, 27 Sep 2005 11:40:51 +0800
Subject: [R] figure widths in sweave
In-Reply-To: <1127738054.7532.42.camel@Tardis.considine.local>
References: <1127738054.7532.42.camel@Tardis.considine.local>
Message-ID: <1127792451.7523.8.camel@Tardis.considine.local>

On Mon, 2005-09-26 at 20:34 +0800, John Charles Considine wrote:
> gRoovers,
> 
> Can the size of figures be controlled from within a noweb document
> without resorting to editing the \includegraphics sections in the .tex
> file?
> 
yes,
Sweave sets graphics widths to 0.8\textwidth by default.  To change it
for a document, to say 1.0\textwidth, include the line 

\setkeys{Gin}{width=1\textwidth}

after \begin{document} in the noweb file.

see p.12 of Sweave manual at...
http://www.ci.tuwien.ac.at/~leisch/Sweave/Sweave-manual-20050914.pdf
 

> Can the figure widths be set in the environmental declarations at the
> start?
> 
> Can they be set within the \begin{figure} environment?
> 
> JC
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Tue Sep 27 06:18:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Sep 2005 00:18:22 -0400
Subject: [R] questions about boxplots
In-Reply-To: <Pine.LNX.4.63.0509261952340.3276@xevious.gpcc.itd.umich.edu>
References: <Pine.LNX.4.63.0509261952340.3276@xevious.gpcc.itd.umich.edu>
Message-ID: <971536df05092621181c183d77@mail.gmail.com>

Try this:

result <- boxplot(Petal.Length ~ Species, iris)
if (length(result$out))
  text(result$group, result$out, result$out, pos = 4, col = "red")

On 9/26/05, Yulei He <yuleih at umich.edu> wrote:
> Hi, there.
>
> I have two questions about using R to create boxplots.
>
> 1. The function boxplot() plots the outliers. How can I label the exact
> values arount these outlier points? Does R have an option allow me to
> do that?
>
> 2. How can I put two boxplots in one x-y axis?
>
> Thanks.
>
> Yulei
>
>
> $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
> Yulei He
> 276 Grove St. Apt 3
> Newton, MA 02466
> 617-796-7834(H)
> 617-432-3428(O)
> 617-432-3435(fax)
> $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From paul.bliese at us.army.mil  Tue Sep 27 08:57:44 2005
From: paul.bliese at us.army.mil (Bliese, Paul D LTC USAMH)
Date: Tue, 27 Sep 2005 08:57:44 +0200
Subject: [R] Simulate phi-coefficient (correlation between dichotomous
	vars)
Message-ID: <FADCFAA8BA80C748890C1D3893C198D993C97F@amedmlmhah01.eur.amed.ds.army.mil>

Newsgroup members, 

I appreciate the help on this topic.

David Duffy provided a solution (below) that was quite helpful, and came
close to what I needed.  It did a great job creating two vectors of
dichotomous variables with a known correlation (what I referred to as a
phi-coefficient).

My situation is a bit more complicated and I'm not sure it is easily
solved.  The problem is that I must assume one of the vectors is
constant and generate one or more vectors that covary with the constant
vector.

In a continuous example I could use the following code that I got from
the S-PLUS newsgroup year ago:

sample.cor<-function (x, rho) 
{
    y <- (rho * (x - mean(x)))/sqrt(var(x)) + sqrt(1 - rho^2) * 
        rnorm(length(x))
    cat("Sample corr = ", cor(x, y), "\n")
    return(y)
}

X<-rnorm(100)  #a constant vector
Y1<-sample.cor(X,.30) # a new vector that correlates with X .30
Y2<-sample.cor(X,.45) # a second vector that correlates with X .45

I can, of course, have X be a vector of zeros and ones, and I can
dichotomize Y1 and Y2, but the program always returns a phi-coefficient
correlation lower than the continuous correlation.  Mathematically, I
guess this is expected because the phi-coefficient is partially a
function of the percentage of positive responses.  This, in turn,
explains Pearson's (1900) interest in the whole area of tetrachoric
correlations -- a tetrachoric correlation being the Pearson product
moment correlation that would have been observed had two dichotomously
scored variables been measured on a continuous scale (Pearson, 1900).

Appreciate any additional input or possible solutions.

Paul



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Duffy
Sent: Monday, September 12, 2005 1:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Simulate phi-coefficient

> From: "Bliese, Paul D LTC USAMH" <paul.bliese at us.army.mil>
>
> Given a sample of zeros and ones, for example:
> > VECTOR1<-rep(c(1,0),c(15,10))
> How would I create a new sample (VECTOR2) also containing zeros and
> ones, in which the phi-coefficient between the two sample vectors was
> drawn from a population with a known phi-coefficient value?
>
> I know there are ways to do this with normally distributed numbers
(for
> example the mvrnorm function in MASS), but am stumped when dealing
with
> dichotomous variables.
>
> Paul

One way is to sample from the 2x2 table with the specified means and
pearson
correlation (phi):

for a fourfold table, a b
                      c d
with marginal proportions p1 and p2
cov <- phi * sqrt(p1*(1-p1)*p2*(1-p2))
a <- p1*p2 + cov
b <- p1*(1-p2) - cov
c <- (1-p1)*p2 - cov
d <- (1-p1)*(1-p2) + cov
expand.grid(0:1,0:1)[sample(1:4, size=25, replace=TRUE,
prob=c(a,b,c,d)),]

David.

| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Sep 27 09:03:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Sep 2005 08:03:39 +0100 (BST)
Subject: [R] Make check fails on d-p-q-r-tests.R...
In-Reply-To: <330a15840cdb7ac1f5a590cec1396780@openvistas.net>
References: <330a15840cdb7ac1f5a590cec1396780@openvistas.net>
Message-ID: <Pine.LNX.4.61.0509270757200.5508@gannet.stats>

You will have to show us the error!   It will be shown in 
d-p-q-r-tests.Rout.fail (unless this was a segfault or similar).

It is not OK to skip the test, but note that this test is random and does 
fail about 1 in 50 times, so you could just try rerunning it.

On Mon, 26 Sep 2005, Jeff Ross wrote:

> Hi,
>
> R-2.1.1
> OS:  OpenBSD-current (3.8) on i386
> Compiler:gcc version 3.3.5 (propolice)
> Thread model: single
>
> configure \
>  --with-readline \
>  --with-tcltk \
>  --with-tcl-config=/usr/local/lib/tcl8.4/tclConfig.sh \
>  --with-tk-config=/usr/local/lib/tk8.4/tkConfig.sh \
>  --with-libpng \
>  --with-jpeglib \
>  --with-zlib \
>  --with-bzlib \
>  --with-pcre \
>  --with-libiconv-prefix=/usr/local/

Please do NOT use unrecommended flags like --with-zlib, especially not on 
your first build.  None of those flags should have been needed.

>
> I'm brand new to R so I don't know how critical this error is.
>
> Here's the last bit what make check FORCE=FORCE outputs:
>
> running code in 'grDevices-Ex.R' ... OK
> comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.prev' ... OK
> running code in 'graphics-Ex.R' ... OK
> comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.prev' ... OK
> running code in 'stats-Ex.R' ... OK
> comparing 'stats-Ex.Rout' to 'stats-Ex.Rout.prev' ... OK
> running code in 'datasets-Ex.R' ... OK
> comparing 'datasets-Ex.Rout' to 'datasets-Ex.Rout.prev' ... OK
> running code in 'methods-Ex.R' ... OK
> comparing 'methods-Ex.Rout' to 'methods-Ex.Rout.prev' ... OK
> running code in 'grid-Ex.R' ... OK
> comparing 'grid-Ex.Rout' to 'grid-Ex.Rout.prev' ... OK
> running code in 'splines-Ex.R' ... OK
> comparing 'splines-Ex.Rout' to 'splines-Ex.Rout.prev' ... OK
> running code in 'stats4-Ex.R' ... OK
> comparing 'stats4-Ex.Rout' to 'stats4-Ex.Rout.prev' ... OK
> running code in 'tcltk-Ex.R' ... OK
> comparing 'tcltk-Ex.Rout' to 'tcltk-Ex.Rout.prev' ... OK
> updating test dependencies
> `Makedeps' is up to date.
> running strict specific tests
> running code in 'eval-etc.R' ... OK
> comparing 'eval-etc.Rout' to './eval-etc.Rout.save' ... OK
> running code in 'simple-true.R' ... OK
> comparing 'simple-true.Rout' to './simple-true.Rout.save' ... OK
> running code in 'arith-true.R' ... OK
> comparing 'arith-true.Rout' to './arith-true.Rout.save' ... OK
> running code in 'arith.R' ... OK
> comparing 'arith.Rout' to './arith.Rout.save' ... OK
> running code in 'lm-tests.R' ... OK
> comparing 'lm-tests.Rout' to './lm-tests.Rout.save' ... OK
> running code in 'primitive-funs.R' ... OK
> comparing 'primitive-funs.Rout' to './primitive-funs.Rout.save' ... OK
> running code in 'ok-errors.R' ... OK
> comparing 'ok-errors.Rout' to './ok-errors.Rout.save' ... OK
> running code in 'method-dispatch.R' ... OK
> comparing 'method-dispatch.Rout' to './method-dispatch.Rout.save' ... OK
> running code in 'd-p-q-r-tests.R' ...*** Error code 1
> Stop in /usr/local/src/R-2.1.1/tests.
> *** Error code 1
>
> Stop in /usr/local/src/R-2.1.1/tests (line 206 of Makefile).
> *** Error code 1
>
> Stop in /usr/local/src/R-2.1.1/tests (line 191 of Makefile).
>
>
> Tried finding this error in the archives to no avail.  If it is okay to
> ignore this error, how can I skip this test?
>
> Thanks for any input!
>
> Jeff Ross
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vito_ricci at yahoo.com  Tue Sep 27 09:08:14 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 27 Sep 2005 09:08:14 +0200 (CEST)
Subject: [R] create trend variable in a regression using R
Message-ID: <20050927070814.95849.qmail@web36112.mail.mud.yahoo.com>

Dear Giacomo,

what you mean precisely saying "to create a Trend
variable in a regression"? If it concerns about time
series analysis you could give a look to my
contribute:

http://cran.r-project.org/doc/contrib/Ricci-ts-italian.pdf

Best Regards

Vito 


Hi,
my name is Giacomo. 
I would like to know how to create a Trend variable in
a regression using R.
Thank you for your help.
My best regards,   
Giacomo


Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From maechler at stat.math.ethz.ch  Tue Sep 27 09:10:05 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 27 Sep 2005 09:10:05 +0200
Subject: [R] calculating distances using Gower's coefficient on
	mixed	variables.
In-Reply-To: <4337CA5B.3010400@duckmail.nl>
References: <4337CA5B.3010400@duckmail.nl>
Message-ID: <17208.61517.739681.881439@stat.math.ethz.ch>

>>>>> "nikonia" == nikonia  <nikonia at duckmail.nl>
>>>>>     on Mon, 26 Sep 2005 12:15:55 +0200 writes:

    nikonia> I want to compute the distances in a mixed variable
    nikonia> matrix using the Gower coefficient. I understand it
    nikonia> is possible to calculate distances in a matrix with
    nikonia> mixed variables using the dudi.pco command. How
    nikonia> would this work?

No need for esoteric functions.  The recommended(*) package
'cluster' has  daisy() for computing dissimilarities.
One of its major features has always been the ability to work with
mixed variables (continuous, nominal, ordinal, .. (a)symmetric binary,...)
implementing a slight generalization of Gower's proposal.

Even though the help for daisy starts its 'Details' section with

 >> 'daisy' is fully described in chapter 1 of Kaufman and Rousseeuw (1990).

I think I should add some more details to the help page when I
get time..

Martin Maechler, ETH Zurich

(*) recommended packages are always available by library(<pkg>) 
    in a normal R installation.



From pwolf at wiwi.uni-bielefeld.de  Tue Sep 27 10:32:20 2005
From: pwolf at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Tue, 27 Sep 2005 10:32:20 +0200
Subject: [R] hist(x, ...)  with  normal distribution  curve
In-Reply-To: <43369942.6070604@usa.com>
References: <43369942.6070604@usa.com>
Message-ID: <43390394.20102@wiwi.uni-bielefeld.de>

Knut Krueger wrote:
>.
>I am looking for a histogram or box plot with the adding  normal 
>distribution  curve
>I think that must be possible, but I am not able to find out how to do.
>
>
>
>Regards Knut
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
*There are a lot of answers to add a histogram.
Here is a simple way to add a tiny boxplot to a plot / histogram

x<-rexp(100)
hist(x)
boxplot(x,axes=F,add=T,horizontal=T,
     at=par()$usr[3]+diff(par()$usr[3:4])*.017,
     boxwex=0.02*diff(par()$usr[3:4]),pch=8)

Peter Wolf


*



From pwolf at wiwi.uni-bielefeld.de  Tue Sep 27 10:40:50 2005
From: pwolf at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Tue, 27 Sep 2005 10:40:50 +0200
Subject: [R] questions about boxplots
In-Reply-To: <808c61478ca173549be932c3ba71b6d5@charter.net>
References: <Pine.LNX.4.63.0509261952340.3276@xevious.gpcc.itd.umich.edu>
	<808c61478ca173549be932c3ba71b6d5@charter.net>
Message-ID: <43390592.6040708@wiwi.uni-bielefeld.de>

concerning question 1:  -- labeling of outliers --

you can get the outliers by boxplot(...)$out

try:

set.seed(17)
x<-rexp(99)
names(x)<-paste("x",1:99)
out<-boxplot(x)$out
text(rep(1.1,length(out)), out, names(out))

Peter Wolf

Stephen D. Weigand wrote:
>Dear Yulei,
>
>On Sep 26, 2005, at 6:56 PM, Yulei He wrote:
>
>  
>>Hi, there.
>>
>>I have two questions about using R to create boxplots.
>>
>>1. The function boxplot() plots the outliers. How can I label the exact
>>values arount these outlier points? Does R have an option allow me to
>>do that?
>>    
>
>You can use identify(). Here's a toy example
>
>set.seed(1)
>y <- rt(30, 3)
>boxplot(y)
>identify(x = rep(1,30), y = y, label = format(y, digits = 2))
>### now click on the outlier in the plot and you should see "-7.398"
>### beside the outlier
>
>  
>>2. How can I put two boxplots in one x-y axis?
>>    
>
>x <- rnorm(10)
>y <- rnorm(20, 3, 5)
>z <- runif(30)
>
>boxplot(x, y, z, labels = c("x", "y", "z"))
>### - or -
>boxplot(list(x = x, y = y, z = z))
>
>
>  
>>Thanks.
>>
>>Yulei
>>
>>    
>
>Stephen Weigand
>Rochester, Minnesota, USA
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From francoisromain at free.fr  Tue Sep 27 11:04:00 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 27 Sep 2005 11:04:00 +0200
Subject: [R] hist(x, ...)  with  normal distribution  curve
In-Reply-To: <43390394.20102@wiwi.uni-bielefeld.de>
References: <43369942.6070604@usa.com> <43390394.20102@wiwi.uni-bielefeld.de>
Message-ID: <43390B00.8010509@free.fr>

Le 27.09.2005 10:32, Peter Wolf a ??crit :

>Knut Krueger wrote:
>  
>
>>I am looking for a histogram or box plot with the adding  normal 
>>distribution  curve
>>I think that must be possible, but I am not able to find out how to do.
>>
>>Regards Knut
>> 
>>    
>>
>*There are a lot of answers to add a histogram.
>Here is a simple way to add a tiny boxplot to a plot / histogram
>
>x<-rexp(100)
>hist(x)
>boxplot(x,axes=F,add=T,horizontal=T,
>     at=par()$usr[3]+diff(par()$usr[3:4])*.017,
>     boxwex=0.02*diff(par()$usr[3:4]),pch=8)
>
>Peter Wolf
>
>  
>
The tufte axes, described there : 
http://www.cl.cam.ac.uk/users/sjm217/projects/graphics/
may be of interrest here.

What about making it a possibility for all plots ? For example using :
R> par(xaxt='tufte')
R> plot(rnorm(200), rnorm(200))
would produce a scatterplot with the tufte axes

Romain.

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From Knut.Krueger at usa.com  Mon Sep 26 11:07:21 2005
From: Knut.Krueger at usa.com (Knut Krueger)
Date: Mon, 26 Sep 2005 11:07:21 +0200
Subject: [R] Wrong signature
In-Reply-To: <4336DCCF.3020808@usa.com>
References: <43380B34.20824.18B6494@localhost>
	<4336C27E.7080708@usa.com>	<43381DA9.2050002@free.fr>
	<4336DCCF.3020808@usa.com>
Message-ID: <4337BA49.9070302@usa.com>

I've got a comlaint about the signature in the last post.
I answered form another computer and there is the automatic signature of 
our second business.
On this place is also a part  of the equine resarch program in progress.
I did not realize that there was the siganture below.
Sorry for that.

Regards Knut



From Ted.Harding at nessie.mcc.ac.uk  Tue Sep 27 10:59:42 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 27 Sep 2005 09:59:42 +0100 (BST)
Subject: [R] regression methods for circular(?) data.
In-Reply-To: <XFMail.050926210237.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050927095942.Ted.Harding@nessie.mcc.ac.uk>

I retract the siggestion I proposed last night -- it was based
on a bad hunch! Sorry for wasting time.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 27-Sep-05                                       Time: 09:59:29
------------------------------ XFMail ------------------------------



From juansan at dca.upv.es  Tue Sep 27 11:11:20 2005
From: juansan at dca.upv.es (Juan Pablo Sanchez Serrano)
Date: Tue, 27 Sep 2005 11:11:20 +0200
Subject: [R] About Coda Package
Message-ID: <000701c5c343$6d3963f0$10662a9e@upvnet.upv.es>

Dear R users:
I am using the package coda (the last verison in CRAN) to analyse the output from a  MCMC Bayesian analysis. And I get unconsitented results. I have export the chain using the read.table function and after I have transformed this data frame to an mcmc object using the mcmc function. I am interested in three variables, when I use the function effectiveSize I have these figures:  403.3730    1854.4534     369.8643. But when I run the function codamenu to perform same convergence test I get this warning mensage:

Checking effective sample size ...
*******************************************
WARNING !!!                              
Some variables in your chain have an     
effective sample size of less than 200   
This is too small, and may cause errors  
in the diagnostic tests                  
HINT:                                    
Look at plots first to identify variables
with slow mixing.  (Choose menu Output   
Analysis then Plots)                     
Re-run your chain with a larger sample   
size and thinning interval. If possible, 
reparameterize your model to improve mixing
*******************************************
Some thing is wrong. Could someone explain to me what is happening?
Thanks, 
Juan Pablo.

===========================
Juan Pablo S??nchez Serrano
Dep. Ciencia Animal, UPV.
C/ Camino de Vera s/n 46022
Valencia (Spain)
Telf. 963877007 Ext.74382



From petr.pikal at precheza.cz  Tue Sep 27 11:16:10 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 27 Sep 2005 11:16:10 +0200
Subject: [R] plotting multiple plots on a single graph
In-Reply-To: <BAY102-F14CB02A7C09A1E29CD7FD6828B0@phx.gbl>
Message-ID: <433929FA.9054.B9B15A@localhost>

Hi

***PLEASE do read the posting guide!***

see
?par
?points
?lines
?split.screen
?layout
?matplot
?lattice
....

HTH
Petr




On 26 Sep 2005 at 18:08, C Tate wrote:

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From plummer at iarc.fr  Tue Sep 27 11:35:29 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Tue, 27 Sep 2005 11:35:29 +0200
Subject: [R] About Coda Package
In-Reply-To: <000701c5c343$6d3963f0$10662a9e@upvnet.upv.es>
References: <000701c5c343$6d3963f0$10662a9e@upvnet.upv.es>
Message-ID: <1127813729.3544.9.camel@seurat>

Dear Juan Pablo,

It is best to send package-specific queries the package maintainer (me
in this case) which you can find out by typeing "library(help=coda)"

The inconsistency comes from two different ways of estimating the
spectral density at frequency 0: spectrum0() adapted from Heidelberger &
Welch, fits a generalized linear model to the periodogram. This function
is called by the effectiveSize() function.  Unfortunately it crashes
when there is very high autocorrelation.  The alternative function
spectrum0.ar does the same thing by fitting an autoregressive model to
the data, and won't crash even on highly autocorrelated data. The latter
function is called by codamenu().

The two functions are giving different results here, but to be honest,
an effective sample size of 370 isn't sufficient for serious inference
from an MCMC sample either.   I can only reiterate the advice given by
the warning message.

Martyn

On Tue, 2005-09-27 at 11:11 +0200, Juan Pablo Sanchez Serrano wrote:
> Dear R users:
> I am using the package coda (the last verison in CRAN) to analyse the
> output from a  MCMC Bayesian analysis. And I get unconsitented
> results. I have export the chain using the read.table function and
> after I have transformed this data frame to an mcmc object using the
> mcmc function. I am interested in three variables, when I use the
> function effectiveSize I have these figures:  403.3730    1854.4534
> 369.8643. But when I run the function codamenu to perform same
> convergence test I get this warning mensage:
> 
> Checking effective sample size ...
> *******************************************
> WARNING !!!                              
> Some variables in your chain have an     
> effective sample size of less than 200   
> This is too small, and may cause errors  
> in the diagnostic tests                  
> HINT:                                    
> Look at plots first to identify variables
> with slow mixing.  (Choose menu Output   
> Analysis then Plots)                     
> Re-run your chain with a larger sample   
> size and thinning interval. If possible, 
> reparameterize your model to improve mixing
> *******************************************
> Some thing is wrong. Could someone explain to me what is happening?
> Thanks, 
> Juan Pablo.

-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}



From roger.bos at gmail.com  Tue Sep 27 14:25:24 2005
From: roger.bos at gmail.com (roger bos)
Date: Tue, 27 Sep 2005 08:25:24 -0400
Subject: [R] dates are shown as X15.Feb.03
In-Reply-To: <004401c5c2b4$b23cad70$4c01a8c0@Chris>
References: <004401c5c2b4$b23cad70$4c01a8c0@Chris>
Message-ID: <1db7268005092705252f6a2194@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050927/771a539a/attachment.pl

From oarabile at stams.strath.ac.uk  Tue Sep 27 14:35:38 2005
From: oarabile at stams.strath.ac.uk (Oarabile Molaodi)
Date: Tue, 27 Sep 2005 13:35:38 +0100
Subject: [R] Producing empirical bayes estimates in disease mapping for
 lognormal model
Message-ID: <43393C9A.7090106@stams.strath.ac.uk>

I'm trying to produce empirical bayes estimates based on the lognormal 
model  in disease mapping
Is there a way this can be done in R?

thanks
Oarabile



From samuel.bertrand at paris.ensam.fr  Tue Sep 27 15:17:46 2005
From: samuel.bertrand at paris.ensam.fr (Samuel Bertrand)
Date: Tue, 27 Sep 2005 15:17:46 +0200
Subject: [R]  regsubsets selection criterion
Message-ID: <5.0.2.1.2.20050927150918.031edea0@mailhost.paris.ensam.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050927/73d890c7/attachment.pl

From I.Visser at uva.nl  Tue Sep 27 15:20:18 2005
From: I.Visser at uva.nl (Ingmar Visser)
Date: Tue, 27 Sep 2005 15:20:18 +0200
Subject: [R] regsubsets selection criterion
In-Reply-To: <5.0.2.1.2.20050927150918.031edea0@mailhost.paris.ensam.fr>
Message-ID: <BF5F13B2.84D0%I.Visser@uva.nl>

> Is there anyone who can tell me
> on which criterion is based
> the 'regsubsets' function ?

the leaps help page says:

cp or adjr2 or r2 is the value of the chosen model selection statistic for
each model

so each of these can be chosen as the selection statistic using the method
argument

hth, ingmar



From antonio.fabio at gmail.com  Tue Sep 27 15:27:38 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Tue, 27 Sep 2005 15:27:38 +0200
Subject: [R] getting variable length numerical gradient
In-Reply-To: <00c301c5c279$3cc43490$0540210a@www.domain>
References: <b0808fdc050925023723026856@mail.gmail.com>
	<00b901c5c1c1$4a16bc20$0540210a@www.domain>
	<b0808fdc05092601581a1e59d5@mail.gmail.com>
	<00c301c5c279$3cc43490$0540210a@www.domain>
Message-ID: <b0808fdc05092706275ee1242b@mail.gmail.com>

2005/9/26, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be>:
> AFAIK the deriv() is for symbolic derivatives, so I don't know if it
> will work in your case.

numericDeriv surely computes numerical gradient. The problem here is
its interface, which requires the vector of symbols involved in the
expression to differenziate.


>
> Best,
> Dimitris
>
>
> ----- Original Message -----
> From: "Antonio, Fabio Di Narzo" <antonio.fabio at gmail.com>
> To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
> Cc: <R-help at stat.math.ethz.ch>
> Sent: Monday, September 26, 2005 10:58 AM
> Subject: Re: getting variable length numerical gradient
>
>
> Tnx very much Dimitris,
> your code does what I need. I've just adapted it to my needs (e.g., I
> don't deal with scalar functions), and so solved my problem.
>
> Given this, is there a way to use the deriv function in the base
> package, within this context (variable length vector of indipendent
> variables)?
>
> Best,
> Antonio, Fabio Di Narzo.
>
> On 9/25/05, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be>
> wrote:
> > maybe you can find the following function useful (any comments are
> > greatly appreciated):
> >
> > fd <- function(x, f, scalar = TRUE, ..., eps =
> > sqrt(.Machine$double.neg.eps)){
> >     f <- match.fun(f)
> >     out <- if(scalar){
> >         if(length(f0 <- f(x, ...)) != length(x))
> >             stop("'f' must be vectorized")
> >         x. <- x + eps * pmax(abs(x), 1)
> >         c(f(x., ...) - f0) / (x. - x)
> >     } else{
> >         n <- length(x)
> >         res <- array(0, c(n, n))
> >         f0 <- f(x, ...)
> >         ex <- pmax(abs(x), 1)
> >         for(i in 1:n){
> >             x. <- x
> >             x.[i] <- x[i] + eps * ex[i]
> >             res[, i] <- c(f(x., ...) - f0) / (x.[i] - x[i])
> >         }
> >         res
> >     }
> >     out
> > }
> >
> >
> > ## Examples
> >
> > x <- seq(-3.3, 3.3, 0.1)
> > all.equal(fd(x, pnorm, mean = 0.5), dnorm(x, mean = 0.5))
> >
> >
> > # Approximate the Hessian matrix for a logistic regression
> >
> > # the score vector function
> > gn <- function(b, y, X){
> >     p <- as.vector(plogis(X %*% b))
> >     -colSums(X * (y - p))
> > }
> >
> > # We simulate some data and fit the logistic regression
> > n <- 800
> > x1 <- runif(n,-3, 3); x2 <- runif(n, -3, 3)
> > pr <- plogis(0.8 + 0.4 * x1 - 0.3 * x2)
> > y <- rbinom(n, 1, pr)
> > fm <- glm(y ~ x1 + x2, binomial)
> >
> > ## The Hessian using forward difference approximation
> > fd(fm$coef, gn, scalar = FALSE, y = y, X = cbind(1, x1, x2))
> >
> > ## The true Hessian
> > solve(summary(fm)$cov.unscaled)
> >
> >
> > I hope it helps.
> >
> > Best,
> > Dimitris
> >
> > ----
> > Dimitris Rizopoulos
> > Ph.D. Student
> > Biostatistical Centre
> > School of Public Health
> > Catholic University of Leuven
> >
> > Address: Kapucijnenvoer 35, Leuven, Belgium
> > Tel: +32/(0)16/336899
> > Fax: +32/(0)16/337015
> > Web: http://www.med.kuleuven.be/biostat/
> >      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> >
> >
> > ----- Original Message -----
> > From: "Antonio, Fabio Di Narzo" <antonio.fabio at gmail.com>
> > To: <R-help at stat.math.ethz.ch>
> > Sent: Sunday, September 25, 2005 11:37 AM
> > Subject: [R] getting variable length numerical gradient
> >
> >
> > > Hi all.
> > > I have a numerical function f(x), with x being a vector of generic
> > > size (say k=4), and I wanna take the numerically computed
> > > gradient,
> > > using deriv or numericDeriv (or something else).
> > >
> > > My difficulties here are that in deriv and numericDeric the
> > > function
> > > is passed as an expression, and one have to pass the list of
> > > variables
> > > involved as a char vector... So, it's a pure R programming
> > > question.
> > >
> > >
> > > Have a nice sunday,
> > > Antonio, Fabio Di Narzo.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> > Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> >
> >
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>



From karin.lagesen at medisin.uio.no  Tue Sep 27 15:27:29 2005
From: karin.lagesen at medisin.uio.no (Karin Lagesen)
Date: Tue, 27 Sep 2005 15:27:29 +0200
Subject: [R] graphics guide?
Message-ID: <ypx6achyljy6.fsf@uracil.uio.no>


I am trying to create some graphs with R and it seems to be able to do
what I need. However, I have so far not been able to find any sort of
explanation of how the graphics system works. I am for instance trying
to create a multiple figure, and I seem to have to call plot.new()
before every new plot command, I have however not found any
explanation of what this actually does. ?plot.new does give an
explanation, but it is explained in R, so to speak. Where do I find
out what for instance a graphics frame is?

Thanks,

Karin
-- 
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://www.cmbn.no/rognes/



From Kevin.Wang at maths.anu.edu.au  Tue Sep 27 15:38:08 2005
From: Kevin.Wang at maths.anu.edu.au (Ko-Kang Kevin Wang)
Date: Tue, 27 Sep 2005 23:38:08 +1000
Subject: [R] graphics guide?
In-Reply-To: <ypx6achyljy6.fsf@uracil.uio.no>
References: <ypx6achyljy6.fsf@uracil.uio.no>
Message-ID: <43394B40.7020709@maths.anu.edu.au>

Hi,

Karin Lagesen wrote:
> I am trying to create some graphs with R and it seems to be able to do
> what I need. However, I have so far not been able to find any sort of
> explanation of how the graphics system works. I am for instance trying
> to create a multiple figure, and I seem to have to call plot.new()
> before every new plot command, I have however not found any

Do you mean several graphs in the same window?

If so, you want something like, e.g.:
   par(mfrow = c(2, 2))

Take a look at ?par and the "mfrow" or "mfcol" options.

Cheers and HTH,

Kev

-- 
Ko-Kang Kevin Wang
PhD Student
Centre for Bioinformation Science
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 2601
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7488
Ph (M): +61-40-451-8301



From flom at ndri.org  Tue Sep 27 15:50:49 2005
From: flom at ndri.org (Peter Flom)
Date: Tue, 27 Sep 2005 09:50:49 -0400
Subject: [R] graphics guide?
Message-ID: <s3391610.000@MAIL.NDRI.ORG>

>>> Karin Lagesen <karin.lagesen at medisin.uio.no> 9/27/2005 9:27:29 AM
>>>
<<<
I am trying to create some graphs with R and it seems to be able to do
what I need. However, I have so far not been able to find any sort of
explanation of how the graphics system works. I am for instance trying
to create a multiple figure, and I seem to have to call plot.new()
before every new plot command, I have however not found any
explanation of what this actually does. ?plot.new does give an
explanation, but it is explained in R, so to speak. Where do I find
out what for instance a graphics frame is?
>>>
If you'd like a more comprehensive reference in book form, I recommend
Paul Murrell's book R Graphics.

HTH

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
http://cduhr.ndri.org
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From Rau at demogr.mpg.de  Tue Sep 27 15:54:51 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 27 Sep 2005 15:54:51 +0200
Subject: [R] hist(x, ...)  with  normal distribution  curve
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E69FDD88@HERMES.demogr.mpg.de>

 
> > >Knut Krueger wrote:
> >  
> >
> >>I am looking for a histogram or box plot with the adding  normal 
> >>distribution  curve
> >>I think that must be possible, but I am not able to find 
> out how to do.
> >>
Since there are various solutions, here is mine:

hist(rnorm(10000), freq=FALSE)
xvals <- seq(-5,5,length=100)
lines(x=xvals, y=dnorm(xvals))


Is this what you are looking for?
Best,
Roland

+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From anthony at stat.sdu.dk  Tue Sep 27 15:54:28 2005
From: anthony at stat.sdu.dk (Gichangi, Anthony)
Date: Tue, 27 Sep 2005 15:54:28 +0200
Subject: [R] graphics guide?
References: <ypx6achyljy6.fsf@uracil.uio.no>
Message-ID: <000d01c5c36a$fa4dce80$cb83e182@yatesvmware>

you may want to look at some introduction notes of R graphics
at http://lib.stat.cmu.edu/R/CRAN/other-docs.html

Regards
Anthony

----- Original Message ----- 
From: "Karin Lagesen" <karin.lagesen at medisin.uio.no>
To: <r-help at r-project.org>
Sent: Tuesday, September 27, 2005 3:27 PM
Subject: [R] graphics guide?


>
> I am trying to create some graphs with R and it seems to be able to do
> what I need. However, I have so far not been able to find any sort of
> explanation of how the graphics system works. I am for instance trying
> to create a multiple figure, and I seem to have to call plot.new()
> before every new plot command, I have however not found any
> explanation of what this actually does. ?plot.new does give an
> explanation, but it is explained in R, so to speak. Where do I find
> out what for instance a graphics frame is?
>
> Thanks,
>
> Karin
> -- 
> Karin Lagesen, PhD student
> karin.lagesen at medisin.uio.no
> http://www.cmbn.no/rognes/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From JAROSLAW.W.TUSZYNSKI at saic.com  Tue Sep 27 15:59:45 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Tue, 27 Sep 2005 09:59:45 -0400
Subject: [R] graphics guide?
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD503C3BE87@us-arlington-0668.mail.saic.com>

If you like to learn from examples than you will find plenty of them in the
following 3 web-sites

http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery/dock/svGalle
ry.html
http://addictedtor.free.fr/graphiques/ 

 Jarek 
====================================================\==== 
 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">  \ 
 Jaroslaw.W.Tuszynski at saic.com                     `   \ 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] 
Sent: Tuesday, September 27, 2005 9:27 AM
To: r-help at r-project.org
Subject: [R] graphics guide?


I am trying to create some graphs with R and it seems to be able to do what
I need. However, I have so far not been able to find any sort of explanation
of how the graphics system works. I am for instance trying to create a
multiple figure, and I seem to have to call plot.new() before every new plot
command, I have however not found any explanation of what this actually
does. ?plot.new does give an explanation, but it is explained in R, so to
speak. Where do I find out what for instance a graphics frame is?

Thanks,

Karin
--
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://www.cmbn.no/rognes/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From samuel.bertrand at paris.ensam.fr  Tue Sep 27 16:09:56 2005
From: samuel.bertrand at paris.ensam.fr (Samuel Bertrand)
Date: Tue, 27 Sep 2005 16:09:56 +0200
Subject: [R] regsubsets selection criterion
In-Reply-To: <BF5F13B2.84D0%I.Visser@uva.nl>
References: <5.0.2.1.2.20050927150918.031edea0@mailhost.paris.ensam.fr>
Message-ID: <5.0.2.1.2.20050927155759.031edea0@mailhost.paris.ensam.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050927/c7dd7e24/attachment.pl

From tlumley at u.washington.edu  Tue Sep 27 16:15:24 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 27 Sep 2005 07:15:24 -0700 (PDT)
Subject: [R] regsubsets selection criterion
In-Reply-To: <5.0.2.1.2.20050927150918.031edea0@mailhost.paris.ensam.fr>
References: <5.0.2.1.2.20050927150918.031edea0@mailhost.paris.ensam.fr>
Message-ID: <Pine.LNX.4.63a.0509270713550.15579@homer24.u.washington.edu>

On Tue, 27 Sep 2005, Samuel Bertrand wrote:

> Hello,
>
> I am using the 'regsubsets' function
> (from leaps package)
> to get the best linear models
> to explain 1 variable
> from 1 to 5 explanatory variables
> (exhaustive search).
>
> Is there anyone who can tell me
> on which criterion is based
> the 'regsubsets' function ?
>

As you get a best model for each size, all the commonly used criteria are 
equivalent. RSS, AIC, BIC, Mallows Cp, adjusted R^2, etc, all give the 
same ordering for models with the same number of predictors.


 	-thomas



From j.hadfield at sheffield.ac.uk  Tue Sep 27 16:27:23 2005
From: j.hadfield at sheffield.ac.uk (Jarrod Hadfield)
Date: Tue, 27 Sep 2005 15:27:23 +0100
Subject: [R] outer function and boolean operators
Message-ID: <f60b7fc9a8f74590255c209713f309a6@shef.ac.uk>

Hi,

Could anyone tell me the name (if there is one) of the class of 
matrices (z) formed by:

a)  z<-outer(x,y, "==")

and

b) z<-outer(x,y, "!=")

Thanks in advance,

Jarrod.



From ligges at statistik.uni-dortmund.de  Tue Sep 27 16:26:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Sep 2005 16:26:06 +0200
Subject: [R] outer function and boolean operators
In-Reply-To: <f60b7fc9a8f74590255c209713f309a6@shef.ac.uk>
References: <f60b7fc9a8f74590255c209713f309a6@shef.ac.uk>
Message-ID: <4339567E.5020700@statistik.uni-dortmund.de>

Jarrod Hadfield wrote:

> Hi,
> 
> Could anyone tell me the name (if there is one) of the class of 
> matrices (z) formed by:
> 
> a)  z<-outer(x,y, "==")
> 
> and
> 
> b) z<-outer(x,y, "!=")

Yes, it is "matrix":

class(outer(x,y, "=="))
[1] "matrix"

in this case of "logical" values:

 > mode(outer(x,y, "=="))
[1] "logical"


Uwe Ligges


> Thanks in advance,
> 
> Jarrod.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From iaingallagher at btopenworld.com  Tue Sep 27 16:34:53 2005
From: iaingallagher at btopenworld.com (IAIN GALLAGHER)
Date: Tue, 27 Sep 2005 15:34:53 +0100 (BST)
Subject: [R] multiple plots on same x axis
Message-ID: <20050927143453.13395.qmail@web86709.mail.ukl.yahoo.com>

Hi.

I have two vectors of gene expression for each of
several days. I want to plot both vectors on the same
plot for a visual representation of up versus down
regulation. I've tried using add=T but that doesn't
work. 

eg

>plot(Day, gene1)
>plot(Day, gene2, add=T)

Any help would be appreciated.

Iain



From jross at openvistas.net  Tue Sep 27 16:46:59 2005
From: jross at openvistas.net (Jeff Ross)
Date: Tue, 27 Sep 2005 08:46:59 -0600
Subject: [R] Make check fails on d-p-q-r-tests.R...
In-Reply-To: <Pine.LNX.4.61.0509270757200.5508@gannet.stats>
References: <330a15840cdb7ac1f5a590cec1396780@openvistas.net>
	<Pine.LNX.4.61.0509270757200.5508@gannet.stats>
Message-ID: <a37dd5e4dc23b1a54eb750b1913022a1@openvistas.net>

On 1:03:39 am 09/27/05 Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> You will have to show us the error!   It will be shown in
> d-p-q-r-tests.Rout.fail (unless this was a segfault or similar).
>
> It is not OK to skip the test, but note that this test is random and
> does fail about 1 in 50 times, so you could just try rerunning it.

I am so sorry.  I didn't even think to look for a .fail file.

Here is the last bit of it:

>> ## for PR#7902:
> ex <- -c(rev(1/x), ex)
> All.eq(-x, qcauchy(pcauchy(-x)))
[1] TRUE
> All.eq(+x, qcauchy(pcauchy(+x, log=TRUE), log=TRUE))
[1] TRUE
> All.eq(1/x, pcauchy(qcauchy(1/x)))
[1] TRUE
> All.eq(ex,  pcauchy(qcauchy(ex, log=TRUE), log=TRUE))
[1] "`is.NA' value mismatches: 1 in current, 0  in target"
Warning message:
NaNs produced in: qcauchy(p, location, scale, lower.tail, log.p)
> II <- c(-Inf,Inf)
> stopifnot(pcauchy(II) == 0:1, ## qcauchy(0:1) == II,
+           pcauchy(II, log=TRUE) == c(-Inf,0),
+           qcauchy(c(-Inf,0), log=TRUE) == II)
Error in if (!(is.logical(r <- eval(ll[[i]])) && all(r)))
stop(paste(deparse(mc[[i +  :
  missing value where TRUE/FALSE needed
In addition: Warning message:
NaNs produced in: qcauchy(p, location, scale, lower.tail, log.p)
Execution halted

This is from the latest R-patched source tar ball, and it is the identical
error as R-2.1.1.  I've consistently gotten the same error, even with
configuration file tweaks and making clean between runs.

Thanks!

Jeff Ross



From richard.nixon at mrc-bsu.cam.ac.uk  Tue Sep 27 16:57:42 2005
From: richard.nixon at mrc-bsu.cam.ac.uk (Richard Nixon)
Date: Tue, 27 Sep 2005 15:57:42 +0100
Subject: [R] quick "points" question
Message-ID: <43395DE6.8070908@mrc-bsu.cam.ac.uk>

Hi
Just one of those niggles.......

I've just been trying to plot a filled circle.
I thought that this would do it

plot(1,1,type="n")
points(1,1,pch=1,bg="blue",cex=5)
#bg: background ("fill") color for open plot symbols

But I need to do this instead
points(1.2,1,pch=19,col="blue",cex=5)

Am I misunderstanding the "bg" option in the points function?

Richard

-- 
Dr. Richard Nixon, MRC Biostatistics Unit, Cambridge, UK
http://www.mrc-bsu.cam.ac.uk/personal/richard



From ligges at statistik.uni-dortmund.de  Tue Sep 27 17:12:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Sep 2005 17:12:37 +0200
Subject: [R] quick "points" question
In-Reply-To: <43395DE6.8070908@mrc-bsu.cam.ac.uk>
References: <43395DE6.8070908@mrc-bsu.cam.ac.uk>
Message-ID: <43396165.60700@statistik.uni-dortmund.de>

Richard Nixon wrote:

> Hi
> Just one of those niggles.......
> 
> I've just been trying to plot a filled circle.
> I thought that this would do it
> 
> plot(1,1,type="n")
> points(1,1,pch=1,bg="blue",cex=5)
> #bg: background ("fill") color for open plot symbols
> 
> But I need to do this instead
> points(1.2,1,pch=19,col="blue",cex=5)
> 
> Am I misunderstanding the "bg" option in the points function?
> 
> Richard
> 

Please read ?points and it's paragraph on te argument "pch" with quite 
some information regarding filled symbols...

Uwe Ligges



From p.dalgaard at biostat.ku.dk  Tue Sep 27 17:17:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Sep 2005 17:17:00 +0200
Subject: [R] quick "points" question
In-Reply-To: <43395DE6.8070908@mrc-bsu.cam.ac.uk>
References: <43395DE6.8070908@mrc-bsu.cam.ac.uk>
Message-ID: <x2hdc68rrn.fsf@viggo.kubism.ku.dk>

Richard Nixon <richard.nixon at mrc-bsu.cam.ac.uk> writes:

> Hi
> Just one of those niggles.......
> 
> I've just been trying to plot a filled circle.
> I thought that this would do it
> 
> plot(1,1,type="n")
> points(1,1,pch=1,bg="blue",cex=5)
> #bg: background ("fill") color for open plot symbols
> 
> But I need to do this instead
> points(1.2,1,pch=19,col="blue",cex=5)
> 
> Am I misunderstanding the "bg" option in the points function?

Sort of. The quick answer is: example(points). I think you'll get the
point.
 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Simon.Bond at mrc-bsu.cam.ac.uk  Tue Sep 27 17:23:22 2005
From: Simon.Bond at mrc-bsu.cam.ac.uk (Simon.Bond)
Date: Tue, 27 Sep 2005 16:23:22 +0100 (BST)
Subject: [R] negative binomial in GEE
Message-ID: <Pine.GSO.4.58.0509271614290.16234@bononcini>

Dear R-help,


I was recently wanting to use GEE with the negative binomial "family". It
seems that this is lacking in the otherwise excellent implementations of
the GEE methodology ( packages: gee, yags, geepack).

I would have thought it a simple step to allow the creation of a family,
i.e providing the link function (log mu) and the variance function (mu +
mu^2/theta) , assuming theta is specified; and then to let the gee code
do the business in a similar fashion to glm.

However the gee codes all seem to rely a set of C routines with either the
normal, poisson, or binomial, hardwired in.

Does anyone have any suggestions for a way round this problem for the
future (I had to resort to using Stata), or maybe more realistically, how
much work it would take to build an extendible version of the gee
"algorithm"?


thanks Simon Bond.



From jfontain at free.fr  Tue Sep 27 17:29:50 2005
From: jfontain at free.fr (jfontain@free.fr)
Date: Tue, 27 Sep 2005 17:29:50 +0200
Subject: [R] time series: smooth and aggregate?
Message-ID: <1127834990.4339656e5adf0@imp3-g19.free.fr>

I am working on automatic optimization of ARIMA parameters.
That takes a lot of computing power, which I would like to reduce by aggregating
and smoothing.

Any thoughts on the subject?
Suggested algorithms?
What is the best order? aggregate then smooth or smooth then aggregate?

Many thanks in advance,

--
Jean-Luc



From mayerfer at yahoo.com.br  Tue Sep 27 17:42:50 2005
From: mayerfer at yahoo.com.br (Fernando Mayer)
Date: Tue, 27 Sep 2005 12:42:50 -0300
Subject: [R] Error in "make check-all"
Message-ID: <4339687A.6040301@yahoo.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050927/914d53a3/attachment.pl

From ripley at stats.ox.ac.uk  Tue Sep 27 17:55:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Sep 2005 16:55:08 +0100 (BST)
Subject: [R] Error in "make check-all"
In-Reply-To: <4339687A.6040301@yahoo.com.br>
References: <4339687A.6040301@yahoo.com.br>
Message-ID: <Pine.LNX.4.61.0509271653360.10946@gannet.stats>

This is what happens if you don't have a usable X11 display.  Did you 
perhaps use a root account on a console owned by a normal user?

On Tue, 27 Sep 2005, Fernando Mayer wrote:

> Dear R-users,
>
> i'm a very newbie in linux, but decided to build R from source.
> Following the "R Installation and Administration" manual, i did this:
>
> ./configure --enable-R-shlib # this option is here because i intend to
> build the GNOME console after...
> make
> make check
>
> no problems in make check, but:
>
> make check-devel #and also
> make check-all
>
> indicated some WARNINGs in the log file:
>
> /usr/local/R-2.1.1/tests/tcltk.Rcheck/00check.Rcheck
>
> Right below is the content of this log file (I translated some words):
>
> [start log file]
>
> * using log directory '/usr/local/R-2.1.1/tests/tcltk.Rcheck'
> * using R version 2.1.1, 2005-06-20
> * looks like 'tcltk' is a base package
> * skipping installation test
> * checking package directory ... OK
> * checking for portable file names ... OK
> * checking for sufficient/correct file permissions ... OK
> * checking DESCRIPTION meta-information ... OK
> * checking package dependencies ... OK
> * checking index information ... OK
> * checking package subdirectories ... OK
> * checking S3 generic/method consistency ... WARNING
> Erro: .First.lib failed for 'tcltk'
> Call sequence:
> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>       domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> FALSE)
> Interrupted execution
> See section 'Generic functions and methods' of the 'Writing R Extensions'
> manual.
> * checking replacement functions ... WARNING
> Error: .First.lib failed for 'tcltk'
> Call sequence:
> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>       domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> FALSE)
> Interrupted execution
> In R, the argument of a replacement function which corresponds to the right
> hand side must be named 'value'.
> * checking foreign function calls ... WARNING
> Error: .First.lib failed for 'tcltk'
> Call sequence:
> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>       domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> FALSE)
> Interrupted execution
> See section 'System and foreign language interfaces' of the 'Writing R
> Extensions' manual.
> * checking Rd files ... OK
> * checking for missing documentation entries ... WARNING
> Error: .First.lib failed for 'tcltk'
> Call sequence:
> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>       domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> FALSE)
> Interrupted execution
> All user-level objects in a package should have documentation entries.
> See chapter 'Writing R documentation files' in manual 'Writing R
> Extensions'.
> * checking for code/documentation mismatches ... WARNING
> Error: .First.lib failed for 'tcltk'
> Call sequence:
> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>       domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> FALSE)
> Interrupted execution
> Error: .First.lib failed for 'tcltk'
> Call sequence:
> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>       domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> FALSE)
> Interrupted execution
> Error: .First.lib failed for 'tcltk'
> Call sequence:
> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>       domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> FALSE)
> Interrupted execution
> * checking Rd \usage sections ... OK
> * checking DVI version of manual ... OK
>
> [end log file]
>
> The problem is in the tcltk package. I have searched in the archives,
> but didin't find anything similar to this problem (maybe there is, but i
> was not able to identify it). I really don't know what all the warnings
> means. What i need to know is what am i missing, and if there is
> something i can fix it. Also, are there any problems if i continue the
> installation without fix something?
>
> P.S.: actually R is running already from where it was buit, but i want
> to install it. So i want to avoid future problems, if this warnings
> really make some.
>
> Any help is appreciated. Thanks!
>
> Fernando Mayer.
>
> version
>         _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu [SuSE 9.3]
> system   i686, linux-gnu
> status
> major    2
> minor    1.1
> year     2005
> month    06
> day      20
> language R
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Sep 27 18:01:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Sep 2005 18:01:22 +0200
Subject: [R] Error in "make check-all"
In-Reply-To: <4339687A.6040301@yahoo.com.br>
References: <4339687A.6040301@yahoo.com.br>
Message-ID: <x28xxifqjx.fsf@viggo.kubism.ku.dk>

Fernando Mayer <mayerfer at yahoo.com.br> writes:

> Dear R-users,
..
> Erro: .First.lib failed for 'tcltk'
...
> The problem is in the tcltk package. I have searched in the archives, 
> but didin't find anything similar to this problem (maybe there is, but i 
> was not able to identify it). I really don't know what all the warnings 
> means. What i need to know is what am i missing, and if there is 
> something i can fix it. Also, are there any problems if i continue the 
> installation without fix something?

Well, tcl/tk isn't working...
 
> P.S.: actually R is running already from where it was buit, but i want 
> to install it. So i want to avoid future problems, if this warnings 
> really make some.
> 
> Any help is appreciated. Thanks!
> 
> Fernando Mayer.
> 
> version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu [SuSE 9.3]

Installing the tk-devel and tcl-devel packages should help.

> system   i686, linux-gnu
> status
> major    2
> minor    1.1
> year     2005
> month    06
> day      20
> language R
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Tue Sep 27 18:02:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Sep 2005 17:02:32 +0100 (BST)
Subject: [R] quick "points" question
In-Reply-To: <43395DE6.8070908@mrc-bsu.cam.ac.uk>
References: <43395DE6.8070908@mrc-bsu.cam.ac.uk>
Message-ID: <Pine.LNX.4.61.0509271657570.10946@gannet.stats>

On Tue, 27 Sep 2005, Richard Nixon wrote:

> Hi
> Just one of those niggles.......
>
> I've just been trying to plot a filled circle.
> I thought that this would do it
>
> plot(1,1,type="n")
> points(1,1,pch=1,bg="blue",cex=5)
> #bg: background ("fill") color for open plot symbols
>
> But I need to do this instead
> points(1.2,1,pch=19,col="blue",cex=5)
>
> Am I misunderstanding the "bg" option in the points function?

Yes. See the examples.   pch=1 is not an `open plot symbol': 21-25 are.
So in your case

points(1.3, 1, pch=21, col="blue", bg="red", cex=5)

may be illuminating.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ccthomas at joimail.com  Tue Sep 27 19:16:11 2005
From: ccthomas at joimail.com (Courtney Thomas)
Date: Tue, 27 Sep 2005 12:16:11 -0500
Subject: [R] make check   in ~/fem   outputs only 1 test, huh  ?
Message-ID: <1127841371.538.137.camel@bsdbox.SAMBA>

fem-4.0.1 [and all the other modules, except post] compiles OK, AFAIK,
but when I do a 

make check 

only 1 test is reported, as follows:


making check	src
making check	view3d
making check	viewaxis
making check	tests

make check-TESTS
$ELMER_HOME undefined, setting it to ../src
CC: no input files specified
test: unexpected operator
Tests completed, passed: 0 out of total 0 tests 
PASS: runtests
===========================
All 1 tests passed
===========================
 

As I understand it, about 50 tests should have been run. What's wrong
and how do I fix it, please ?

Gratefully,
Courtney



From p.dalgaard at biostat.ku.dk  Tue Sep 27 18:10:28 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Sep 2005 18:10:28 +0200
Subject: [R] Error in "make check-all"
In-Reply-To: <Pine.LNX.4.61.0509271653360.10946@gannet.stats>
References: <4339687A.6040301@yahoo.com.br>
	<Pine.LNX.4.61.0509271653360.10946@gannet.stats>
Message-ID: <x24q86fq4r.fsf@viggo.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> This is what happens if you don't have a usable X11 display.  Did you 
> perhaps use a root account on a console owned by a normal user?

More likely on SUSE is to get done in by a lack of -devel packages.
I've already mentioned tcl-devel and tk-devel but you also need 
xorg-x11-devel, readline-devel, and libjpeg-devel. The latter two are
not going to cause that particular error though. 
 
> On Tue, 27 Sep 2005, Fernando Mayer wrote:
> 
> > Dear R-users,
> >
> > i'm a very newbie in linux, but decided to build R from source.
> > Following the "R Installation and Administration" manual, i did this:
> >
> > ./configure --enable-R-shlib # this option is here because i intend to
> > build the GNOME console after...
> > make
> > make check
> >
> > no problems in make check, but:
> >
> > make check-devel #and also
> > make check-all
> >
> > indicated some WARNINGs in the log file:
> >
> > /usr/local/R-2.1.1/tests/tcltk.Rcheck/00check.Rcheck
> >
> > Right below is the content of this log file (I translated some words):
> >
> > [start log file]
> >
> > * using log directory '/usr/local/R-2.1.1/tests/tcltk.Rcheck'
> > * using R version 2.1.1, 2005-06-20
> > * looks like 'tcltk' is a base package
> > * skipping installation test
> > * checking package directory ... OK
> > * checking for portable file names ... OK
> > * checking for sufficient/correct file permissions ... OK
> > * checking DESCRIPTION meta-information ... OK
> > * checking package dependencies ... OK
> > * checking index information ... OK
> > * checking package subdirectories ... OK
> > * checking S3 generic/method consistency ... WARNING
> > Erro: .First.lib failed for 'tcltk'
> > Call sequence:
> > 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >       domain = NA)
> > 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> > FALSE)
> > Interrupted execution
> > See section 'Generic functions and methods' of the 'Writing R Extensions'
> > manual.
> > * checking replacement functions ... WARNING
> > Error: .First.lib failed for 'tcltk'
> > Call sequence:
> > 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >       domain = NA)
> > 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> > FALSE)
> > Interrupted execution
> > In R, the argument of a replacement function which corresponds to the right
> > hand side must be named 'value'.
> > * checking foreign function calls ... WARNING
> > Error: .First.lib failed for 'tcltk'
> > Call sequence:
> > 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >       domain = NA)
> > 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> > FALSE)
> > Interrupted execution
> > See section 'System and foreign language interfaces' of the 'Writing R
> > Extensions' manual.
> > * checking Rd files ... OK
> > * checking for missing documentation entries ... WARNING
> > Error: .First.lib failed for 'tcltk'
> > Call sequence:
> > 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >       domain = NA)
> > 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> > FALSE)
> > Interrupted execution
> > All user-level objects in a package should have documentation entries.
> > See chapter 'Writing R documentation files' in manual 'Writing R
> > Extensions'.
> > * checking for code/documentation mismatches ... WARNING
> > Error: .First.lib failed for 'tcltk'
> > Call sequence:
> > 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >       domain = NA)
> > 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> > FALSE)
> > Interrupted execution
> > Error: .First.lib failed for 'tcltk'
> > Call sequence:
> > 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >       domain = NA)
> > 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> > FALSE)
> > Interrupted execution
> > Error: .First.lib failed for 'tcltk'
> > Call sequence:
> > 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >       domain = NA)
> > 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> > FALSE)
> > Interrupted execution
> > * checking Rd \usage sections ... OK
> > * checking DVI version of manual ... OK
> >
> > [end log file]
> >
> > The problem is in the tcltk package. I have searched in the archives,
> > but didin't find anything similar to this problem (maybe there is, but i
> > was not able to identify it). I really don't know what all the warnings
> > means. What i need to know is what am i missing, and if there is
> > something i can fix it. Also, are there any problems if i continue the
> > installation without fix something?
> >
> > P.S.: actually R is running already from where it was buit, but i want
> > to install it. So i want to avoid future problems, if this warnings
> > really make some.
> >
> > Any help is appreciated. Thanks!
> >
> > Fernando Mayer.
> >
> > version
> >         _
> > platform i686-pc-linux-gnu
> > arch     i686
> > os       linux-gnu [SuSE 9.3]
> > system   i686, linux-gnu
> > status
> > major    2
> > minor    1.1
> > year     2005
> > month    06
> > day      20
> > language R
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From fernandomayer at gmail.com  Tue Sep 27 18:22:45 2005
From: fernandomayer at gmail.com (Fernando Mayer)
Date: Tue, 27 Sep 2005 13:22:45 -0300
Subject: [R] Error in "make check-all"
In-Reply-To: <Pine.LNX.4.61.0509271653360.10946@gannet.stats>
References: <4339687A.6040301@yahoo.com.br>
	<Pine.LNX.4.61.0509271653360.10946@gannet.stats>
Message-ID: <433971D5.8050005@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050927/6bb2e2b3/attachment.pl

From ripley at stats.ox.ac.uk  Tue Sep 27 18:28:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Sep 2005 17:28:49 +0100 (BST)
Subject: [R] Error in "make check-all"
In-Reply-To: <433971D5.8050005@gmail.com>
References: <4339687A.6040301@yahoo.com.br>
	<Pine.LNX.4.61.0509271653360.10946@gannet.stats>
	<433971D5.8050005@gmail.com>
Message-ID: <Pine.LNX.4.61.0509271726400.11177@gannet.stats>

On Tue, 27 Sep 2005, Fernando Mayer wrote:

> Dear Prof. Ripley, that was exactly what i did. Should i login on the system 
> as root, instead of login as user and became root via console? What should i 
> do to have X11 usable?

Well, I don't worry about this, but it you want to do a complete test you 
need the console running the user that is going to be running R.

(I see this when running tests remotely, as then I have no X11 display set 
to save bandwidth.  I just ignore it.)

>
> Thanks,
> Fernando Mayer.
>
>
> Prof Brian Ripley escreveu:
>
>> This is what happens if you don't have a usable X11 display.  Did you 
>> perhaps use a root account on a console owned by a normal user?
>> 
>> On Tue, 27 Sep 2005, Fernando Mayer wrote:
>> 
>> 
>>> Dear R-users,
>>> 
>>> i'm a very newbie in linux, but decided to build R from source.
>>> Following the "R Installation and Administration" manual, i did this:
>>> 
>>> ./configure --enable-R-shlib # this option is here because i intend to
>>> build the GNOME console after...
>>> make
>>> make check
>>> 
>>> no problems in make check, but:
>>> 
>>> make check-devel #and also
>>> make check-all
>>> 
>>> indicated some WARNINGs in the log file:
>>> 
>>> /usr/local/R-2.1.1/tests/tcltk.Rcheck/00check.Rcheck
>>> 
>>> Right below is the content of this log file (I translated some words):
>>> 
>>> [start log file]
>>> 
>>> * using log directory '/usr/local/R-2.1.1/tests/tcltk.Rcheck'
>>> * using R version 2.1.1, 2005-06-20
>>> * looks like 'tcltk' is a base package
>>> * skipping installation test
>>> * checking package directory ... OK
>>> * checking for portable file names ... OK
>>> * checking for sufficient/correct file permissions ... OK
>>> * checking DESCRIPTION meta-information ... OK
>>> * checking package dependencies ... OK
>>> * checking index information ... OK
>>> * checking package subdirectories ... OK
>>> * checking S3 generic/method consistency ... WARNING
>>> Erro: .First.lib failed for 'tcltk'
>>> Call sequence:
>>> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>>>      domain = NA)
>>> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
>>> FALSE)
>>> Interrupted execution
>>> See section 'Generic functions and methods' of the 'Writing R Extensions'
>>> manual.
>>> * checking replacement functions ... WARNING
>>> Error: .First.lib failed for 'tcltk'
>>> Call sequence:
>>> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>>>      domain = NA)
>>> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
>>> FALSE)
>>> Interrupted execution
>>> In R, the argument of a replacement function which corresponds to the 
>>> right
>>> hand side must be named 'value'.
>>> * checking foreign function calls ... WARNING
>>> Error: .First.lib failed for 'tcltk'
>>> Call sequence:
>>> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>>>      domain = NA)
>>> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
>>> FALSE)
>>> Interrupted execution
>>> See section 'System and foreign language interfaces' of the 'Writing R
>>> Extensions' manual.
>>> * checking Rd files ... OK
>>> * checking for missing documentation entries ... WARNING
>>> Error: .First.lib failed for 'tcltk'
>>> Call sequence:
>>> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>>>      domain = NA)
>>> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
>>> FALSE)
>>> Interrupted execution
>>> All user-level objects in a package should have documentation entries.
>>> See chapter 'Writing R documentation files' in manual 'Writing R
>>> Extensions'.
>>> * checking for code/documentation mismatches ... WARNING
>>> Error: .First.lib failed for 'tcltk'
>>> Call sequence:
>>> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>>>      domain = NA)
>>> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
>>> FALSE)
>>> Interrupted execution
>>> Error: .First.lib failed for 'tcltk'
>>> Call sequence:
>>> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>>>      domain = NA)
>>> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
>>> FALSE)
>>> Interrupted execution
>>> Error: .First.lib failed for 'tcltk'
>>> Call sequence:
>>> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>>>      domain = NA)
>>> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
>>> FALSE)
>>> Interrupted execution
>>> * checking Rd \usage sections ... OK
>>> * checking DVI version of manual ... OK
>>> 
>>> [end log file]
>>> 
>>> The problem is in the tcltk package. I have searched in the archives,
>>> but didin't find anything similar to this problem (maybe there is, but i
>>> was not able to identify it). I really don't know what all the warnings
>>> means. What i need to know is what am i missing, and if there is
>>> something i can fix it. Also, are there any problems if i continue the
>>> installation without fix something?
>>> 
>>> P.S.: actually R is running already from where it was buit, but i want
>>> to install it. So i want to avoid future problems, if this warnings
>>> really make some.
>>> 
>>> Any help is appreciated. Thanks!
>>> 
>>> Fernando Mayer.
>>> 
>>> version
>>>        _
>>> platform i686-pc-linux-gnu
>>> arch     i686
>>> os       linux-gnu [SuSE 9.3]
>>> system   i686, linux-gnu
>>> status
>>> major    2
>>> minor    1.1
>>> year     2005
>>> month    06
>>> day      20
>>> language R
>>> 
>>> 
>>> 	[[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>> 
>>> 
>> 
>> 
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ferran.carrascosa at gmail.com  Tue Sep 27 18:31:13 2005
From: ferran.carrascosa at gmail.com (Ferran Carrascosa)
Date: Tue, 27 Sep 2005 18:31:13 +0200
Subject: [R] Weighting factor
Message-ID: <fcd289fd05092709314d71b299@mail.gmail.com>

Hi everyone,

I would like some package of R or any help to solve the next problem
with a weighting fatcors:

Giving a data matrix with dichotomous (2 or more) variables in columns
and individuals in rows, and also a theorical distribution of the
dichotomous variables I would like a row weight (one per individual)
that transform the real distribution of variables to the theorical
distribution.

Thanks in advance and best regards,

--
Ferran Carrascosa



From tlumley at u.washington.edu  Tue Sep 27 18:33:09 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 27 Sep 2005 09:33:09 -0700 (PDT)
Subject: [R] negative binomial in GEE
In-Reply-To: <Pine.GSO.4.58.0509271614290.16234@bononcini>
References: <Pine.GSO.4.58.0509271614290.16234@bononcini>
Message-ID: <Pine.LNX.4.63a.0509270925220.4400@homer22.u.washington.edu>

On Tue, 27 Sep 2005, Simon.Bond wrote:

>
> Does anyone have any suggestions for a way round this problem for the
> future (I had to resort to using Stata), or maybe more realistically, how
> much work it would take to build an extendible version of the gee
> "algorithm"?
>

I don't think it would take that much work [and I've done it a couple of 
times, once in XLISP-Stat and once in SPIDA].  An entirely interpreted 
implementation will be relatively slow, though it might be possible to 
improve that quite a bit with sparse matrix techniques.

 	-thomas



From murdoch at stats.uwo.ca  Tue Sep 27 18:36:32 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 27 Sep 2005 12:36:32 -0400
Subject: [R] dates are shown as X15.Feb.03
In-Reply-To: <001101c5c37e$72315b20$4c01a8c0@Chris>
References: <001101c5c37e$72315b20$4c01a8c0@Chris>
Message-ID: <43397510.1080000@stats.uwo.ca>

On 9/27/2005 12:13 PM, Chris Buddenhagen wrote:
> Dear Duncan Murdoch
> 
> Thanks for responding. I will provide more complete info with future
> questions. The data for a small pilot study are below. 
> 
> The values for each month are the percentage cover of an invasive plant here
> in the Galapagos (TT=treatment) type = immediately herbicide applied or
> applied to regrowth later. I called the data glyphosate.txt.
> 
> 
> The code is
> 
> glyp<-read.delim("C:\\Chris\\active projects\\Aristolochia\\glyphosate.txt",
> header=T)
> list(names(glyp))
> 
> This is the output that mystified me:
> 
> [1] "TT"         "type"       "CUADRANTES" "X15dec02"   "X15jan03"  
>  [6] "X15feb03"   "X15apr03"   "X15jun03"   "X15aug03"   "X15oct03"

It looks as though you have column headings like "15dec02".  That's not 
a legal R variable name, so it prepends the X to make it into one.  That 
way you can do things like glyp$X15dec02 to extract that column. 
glyp$15dec02 would give a syntax error.


> 
> While I have an expert on hand I have another question, how do I produce a
> graph of time against percentage cover with error bars. I wanted to only
> present months on the x axis. I start by reorganizing the data using this:

Various functions implement error bars, but I haven't used them in a 
long  time, so I'll leave this question for someone else to answer.

Duncan Murdoch

> 
> glyp2<-reshape(glyp, direction="long", idvar=c("TT", "type",
> "CUADRANTES"),varying=list(names(glyp)[4:10]),v.names="cover") but couldn't
> get month names to appear in the column time.
> 
> 
> 
> TT	type	CUADRANTES	"15dec02"	"15jan03"	"15feb03"
> "15apr03"	"15jun03"	"15aug03"	"15oct03"
> T1	immediate	1	63	59	60	55	4.5	1
> 2
> T1	immediate	2	75	75	75	75	6.5	1
> 2
> T1	immediate	22	72	70	70	65	35	2
> 3.5
> T1	immediate	26	90	80	80	75	17.5	2
> 4
> T1	immediate	33	75	80	85	85	40	1
> 2
> T2	immediate	18	85	80	80	22.5	2.5	2
> 5
> T2	immediate	21	90	85	85	60	16.5	0
> 1
> T2	immediate	23	85	80	80	75	17.5	1
> 2
> T2	immediate	28	90	85	85	27.5	27.5	0
> 1
> T2	immediate	31	90	85	85	9	12.5	1
> 2
> T3	immediate	3	50	58	60	65	35	1
> 2
> T3	immediate	10	63	60	60	15	2.5	0
> 10
> T3	immediate	12	90	85	85	17.5	15	0
> 5
> T3	immediate	16	85	82	80	78	9	1
> 1.5
> T3	immediate	34	75	80	85	85	35	1.5
> 2.5
> T4	wait	4	75	75	75	7.5	2.5	0	0
> T4	wait	11	63	60	60	12.5	2.5	1	2
> T4	wait	24	90	85	80	3.5	5.5	1	2
> T4	wait	29	85	80	80	1.5	4.5	1	2
> T4	wait	32	85	80	80	3.5	4.5	2	3.5
> T5	wait	6	50	50	50	1	1	0.5	2
> T5	wait	14	80	75	75	15	5.5	1	2
> T5	wait	15	90	80	80	1	5.5	1	3
> T5	wait	20	85	80	80	22.5	8.5	0	0
> T5	wait	25	85	80	80	22.5	8.5	1	1.5
> T6	wait	8	75	75	75	1.5	4.5	0	1.5
> T6	wait	9	90	85	85	1.5	3.5	1	2
> T6	wait	13	80	75	75	1	3.5	1	1.5
> T6	wait	19	90	85	80	1	2.5	1	2
> T6	wait	27	85	80	75	1	3.5	1	2
> Tc	control	5	50	45	45	40	23	20	25
> Tc	control	7	50	53	54	55	55	50	55
> Tc	control	17	90	80	70	22.5	9	15	20
> Tc	control	30	75	75	70	75	45	60	70
> Tc	control	35	90	85	80	85	65	60	70
> 
> Chris Buddenhagen, Botany Department, Charles Darwin Research Station, Santa
> Cruz,Galapagos. Mail: Charles Darwin Foundation, Casilla 17-01-3891 Avenida
> 6 de Diciembre N36-109 y Pasaje California Quito, ECUADOR
>  
> 
> 
> 
> 
> 
> ______________________________________________________________________
> EL CONTENIDO DE ESTE MENSAJE ES DE ABSOLUTA RESPONSABILIDAD DEL AUTOR.
> FUNDACION CHARLES DARWIN
> WWW.DARWINFOUNDATION.ORG



From Soren.Hojsgaard at agrsci.dk  Tue Sep 27 19:12:13 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Tue, 27 Sep 2005 19:12:13 +0200
Subject: [R] Using unsplit - unsplit does not seem to reverse the effect of
	split
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9B14@DJFPOST01.djf.agrsci.dk>

In data OME in MASS I would like to extract the first 5 observations per subject (=ID). So I do
 
library(MASS)
OMEsub <- split(OME, OME$ID)
OMEsub <- lapply(OMEsub,function(x)x[1:5,])
unsplit(OMEsub, OME$ID)

- which results in 
 
[[1]]
[1] 1 1 1 1 1
[[2]]
[1] 30 30 30 30 30
[[3]]
[1] low low low low low
Levels: N/A high low
[[4]]
[1] 35 35 40 40 45
[[5]]
[1] coherent   incoherent coherent   incoherent coherent  
Levels: coherent incoherent
[[6]]
[1] 1 4 0 1 2

............
 
[[1094]]
[1] 4 5 5 5 2
[[1095]]
[1] 100 100 100 100 100
[[1096]]
[1] 18 18 18 18 18
[[1097]]
[1] N/A N/A N/A N/A N/A
Levels: N/A high low
There were 50 or more warnings (use warnings() to see the first 50)

warnings()
Warning messages:
1: number of items to replace is not a multiple of replacement length
2: number of items to replace is not a multiple of replacement length
3: number of items to replace is not a multiple of replacement length
....
 
According to documentation unsplit is the reverse of split, but I must be missing a point somewhere... Can anyone help? Thanks in advance. S??ren



From Charles.Annis at StatisticalEngineering.com  Tue Sep 27 19:22:11 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Tue, 27 Sep 2005 13:22:11 -0400
Subject: [R] multiple plots on same x axis
In-Reply-To: <20050927143453.13395.qmail@web86709.mail.ukl.yahoo.com>
Message-ID: <200509271724.j8RHO35u007576@hypatia.math.ethz.ch>

Try 

plot(Day, gene1)
lines(Day, gene2)


see 

?lines
 
for more details.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of IAIN GALLAGHER
Sent: Tuesday, September 27, 2005 10:35 AM
To: r-help at stat.math.ethz.ch
Subject: [R] multiple plots on same x axis

Hi.

I have two vectors of gene expression for each of
several days. I want to plot both vectors on the same
plot for a visual representation of up versus down
regulation. I've tried using add=T but that doesn't
work. 

eg

>plot(Day, gene1)
>plot(Day, gene2, add=T)

Any help would be appreciated.

Iain

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Sep 27 19:49:33 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Sep 2005 19:49:33 +0200
Subject: [R] Using unsplit - unsplit does not seem to reverse the effect
	of split
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9B14@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9B14@DJFPOST01.djf.agrsci.dk>
Message-ID: <x2u0g64d02.fsf@turmalin.kubism.ku.dk>

S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> writes:

> In data OME in MASS I would like to extract the first 5 observations per subject (=ID). So I do
>  
> library(MASS)
> OMEsub <- split(OME, OME$ID)
> OMEsub <- lapply(OMEsub,function(x)x[1:5,])
> unsplit(OMEsub, OME$ID)
> 
> - which results in 
>  
> [[1]]
> [1] 1 1 1 1 1
> [[2]]
> [1] 30 30 30 30 30
> [[3]]
> [1] low low low low low
> Levels: N/A high low
> [[4]]
> [1] 35 35 40 40 45
> [[5]]
> [1] coherent   incoherent coherent   incoherent coherent  
> Levels: coherent incoherent
> [[6]]
> [1] 1 4 0 1 2
> 
> ............
>  
> [[1094]]
> [1] 4 5 5 5 2
> [[1095]]
> [1] 100 100 100 100 100
> [[1096]]
> [1] 18 18 18 18 18
> [[1097]]
> [1] N/A N/A N/A N/A N/A
> Levels: N/A high low
> There were 50 or more warnings (use warnings() to see the first 50)
> 
> warnings()
> Warning messages:
> 1: number of items to replace is not a multiple of replacement length
> 2: number of items to replace is not a multiple of replacement length
> 3: number of items to replace is not a multiple of replacement length
> ....
>  
> According to documentation unsplit is the reverse of split, but I must be missing a point somewhere... Can anyone help? Thanks in advance. S??ren

It only works if the first argument is or could have resulted from a
split on the second argument. That is clearly not the case when you
are creating subvectors. 

I have on occasion wanted an unsplit that worked without the 2nd
argument as in

unsplit(l, rep(seq(along=l), sapply(l,length)) )

but if you think about it, it's not really doing anything that
do.call("c",l) or do.call("rbind",l) won't do.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From mschwartz at mn.rr.com  Tue Sep 27 19:58:18 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 27 Sep 2005 12:58:18 -0500
Subject: [R] Using unsplit - unsplit does not seem to reverse the	effect
	of split
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9B14@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9B14@DJFPOST01.djf.agrsci.dk>
Message-ID: <1127843898.4253.50.camel@localhost.localdomain>

On Tue, 2005-09-27 at 19:12 +0200, SÃ¸ren HÃ¸jsgaard wrote:
> In data OME in MASS I would like to extract the first 5 observations
> per subject (=ID). So I do
>  
> library(MASS)
> OMEsub <- split(OME, OME$ID)
> OMEsub <- lapply(OMEsub,function(x)x[1:5,])
> unsplit(OMEsub, OME$ID)
> 
> - which results in 
>  
> [[1]]
> [1] 1 1 1 1 1
> [[2]]
> [1] 30 30 30 30 30
> [[3]]
> [1] low low low low low
> Levels: N/A high low
> [[4]]
> [1] 35 35 40 40 45
> [[5]]
> [1] coherent   incoherent coherent   incoherent coherent  
> Levels: coherent incoherent
> [[6]]
> [1] 1 4 0 1 2
> 
> ............
>  
> [[1094]]
> [1] 4 5 5 5 2
> [[1095]]
> [1] 100 100 100 100 100
> [[1096]]
> [1] 18 18 18 18 18
> [[1097]]
> [1] N/A N/A N/A N/A N/A
> Levels: N/A high low
> There were 50 or more warnings (use warnings() to see the first 50)
> 
> warnings()
> Warning messages:
> 1: number of items to replace is not a multiple of replacement length
> 2: number of items to replace is not a multiple of replacement length
> 3: number of items to replace is not a multiple of replacement length
> ....
>  
> According to documentation unsplit is the reverse of split, but I must
> be missing a point somewhere... Can anyone help? Thanks in advance.
> SÃ¸ren


If you read the documentation for split/unsplit, you will also note that
in the Details section it says:

 'unsplit' works only with lists of vectors

as opposed to lists of data frames, which is the result of your split()
operation.

Also note that in the Value section, it indicates:

'unsplit' returns a vector for which 'split(x, f)' equals 'value'

as opposed to unsplit returning a data frame.


Thus, use:

  OME1 <- do.call("rbind", OMEsub)

where OME1 will be the result of rbind()'ing the data frames in the
OMEsub list.

See ?do.call for more information.

HTH,

Marc Schwartz



From tlumley at u.washington.edu  Tue Sep 27 20:00:38 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 27 Sep 2005 11:00:38 -0700 (PDT)
Subject: [R] Weighting factor
In-Reply-To: <fcd289fd05092709314d71b299@mail.gmail.com>
References: <fcd289fd05092709314d71b299@mail.gmail.com>
Message-ID: <Pine.LNX.4.63a.0509271059160.4400@homer22.u.washington.edu>

On Tue, 27 Sep 2005, Ferran Carrascosa wrote:

> Hi everyone,
>
> I would like some package of R or any help to solve the next problem
> with a weighting fatcors:
>
> Giving a data matrix with dichotomous (2 or more) variables in columns
> and individuals in rows, and also a theorical distribution of the
> dichotomous variables I would like a row weight (one per individual)
> that transform the real distribution of variables to the theorical
> distribution.
>

You should be able to do this with postStratify() in the survey package 
(if you have joint distributions for the variables) or calibrate() if you 
have only marginal distributions.

You would need to create a survey design object, use postStratify() or 
calibrate() on it, then use weights() to extract the weights.

 	-thomas



From helprhelp at gmail.com  Tue Sep 27 20:17:06 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 27 Sep 2005 13:17:06 -0500
Subject: [R] missing handling
Message-ID: <cdf81783050927111745d66647@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050927/da350083/attachment.pl

From tom at maladmin.com  Tue Sep 27 16:39:55 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 27 Sep 2005 10:39:55 -0400
Subject: [R] dynamic lists (data frames?)
Message-ID: <1127831995.4523.12.camel@localhost.localdomain>

Can someone please show me what I need to get something like this to
work

for(a in c(1:5)){
	data$a<-c(a:10)
}

so that i end up with a structure
data$1<-[1,2,3,4,5,6,7,8,9,10]
data$2<-[2,3,4,5,67,8,9,10]
data$3<-[3,4,5,67,8,9,10]
data$4<-[4,5,67,8,9,10]
data$5<-[5,67,8,9,10]

thanks loads
Tom



From leif at reflectivity.com  Tue Sep 27 20:36:00 2005
From: leif at reflectivity.com (Leif Kirschenbaum)
Date: Tue, 27 Sep 2005 11:36:00 -0700
Subject: [R] Listing object sizes without RODBC objects: contributed function
Message-ID: <200509271836.j8RIa8qg023908@hypatia.math.ethz.ch>

I include my workaround for object.size failing on RODBC objects.
Code adapted from Petr Pikal's code (http://tolstoy.newcastle.edu.au/R/help/04/06/1228.html).

ls.objects<-function (pos = 1, order.by,...)
{
    napply <- function(names, fn) sapply(names, function(x) fn(get(x,pos=pos)))
    names <- ls(pos = pos,...)
    obj.class <- napply(names, function(x) as.character(class(x))[1])
    obj.drop <- match("RODBC",obj.class)
    obj.class <- obj.class[-obj.drop]
    names <- names[-obj.drop]
    obj.mode <- napply(names, mode)
    obj.type <- ifelse(is.na(obj.class), obj.mode, obj.class)
    obj.size <- napply(names, object.size)
    obj.dim <- t(napply(names, function(x) as.numeric(dim(x))[1:2]))
    vec <- is.na(obj.dim)[, 1] & (obj.type != "function")
    obj.dim[vec, 1] <- napply(names, length)[vec]
    out <- data.frame(obj.type, obj.size, obj.dim)
    names(out) <- c("Type", "Size", "Rows", "Columns")
    if (!missing(order.by)) out <- out[order(out[[order.by]]), ]
    out
} 


Leif S. Kirschenbaum, Ph.D.
Senior Yield Engineer
Reflectivity, Inc.



From vincent.goulet at act.ulaval.ca  Tue Sep 27 20:44:29 2005
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Tue, 27 Sep 2005 14:44:29 -0400
Subject: [R] dynamic lists (data frames?)
In-Reply-To: <1127831995.4523.12.camel@localhost.localdomain>
References: <1127831995.4523.12.camel@localhost.localdomain>
Message-ID: <200509271444.29323.vincent.goulet@act.ulaval.ca>

Le 27 Septembre 2005 10:39, tom wright a ??crit??:
> Can someone please show me what I need to get something like this to
> work
>
> for(a in c(1:5)){
> 	data$a<-c(a:10)
> }
>
> so that i end up with a structure
> data$1<-[1,2,3,4,5,6,7,8,9,10]
> data$2<-[2,3,4,5,67,8,9,10]
> data$3<-[3,4,5,67,8,9,10]
> data$4<-[4,5,67,8,9,10]
> data$5<-[5,67,8,9,10]
>
> thanks loads
> Tom

That's an exercise I give to my students! This will create the sequences:

> data <- lapply(1:5, seq, 10)

You can then assign the names with

> names(data) <- as.character(1:5)

but 'data$1' will not work. You will need either 'data$"1"' or 'data[["1"]]'. 
I'd use different names...

HTH.

-- 
  Vincent Goulet, Associate Professor
  ??cole d'actuariat
  Universit?? Laval, Qu??bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca



From jholtman at gmail.com  Tue Sep 27 21:02:33 2005
From: jholtman at gmail.com (jim holtman)
Date: Tue, 27 Sep 2005 15:02:33 -0400
Subject: [R] missing handling
In-Reply-To: <cdf81783050927111745d66647@mail.gmail.com>
References: <cdf81783050927111745d66647@mail.gmail.com>
Message-ID: <644e1f3205092712027fc34712@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050927/963e7cc9/attachment.pl

From ggrothendieck at gmail.com  Tue Sep 27 21:04:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Sep 2005 15:04:40 -0400
Subject: [R] dynamic lists (data frames?)
In-Reply-To: <1127831995.4523.12.camel@localhost.localdomain>
References: <1127831995.4523.12.camel@localhost.localdomain>
Message-ID: <971536df05092712047261b8e3@mail.gmail.com>

When sapply is used on a character vector it will use those
as the names so:

data <- sapply(as.character(1:5), function(x) seq(as.numeric(x),10))

will give data with the required names.

On 9/27/05, tom wright <tom at maladmin.com> wrote:
> Can someone please show me what I need to get something like this to
> work
>
> for(a in c(1:5)){
>        data$a<-c(a:10)
> }
>
> so that i end up with a structure
> data$1<-[1,2,3,4,5,6,7,8,9,10]
> data$2<-[2,3,4,5,67,8,9,10]
> data$3<-[3,4,5,67,8,9,10]
> data$4<-[4,5,67,8,9,10]
> data$5<-[5,67,8,9,10]
>
> thanks loads
> Tom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From je_lemaitre at hotmail.com  Tue Sep 27 21:27:41 2005
From: je_lemaitre at hotmail.com (=?iso-8859-1?B?Suly9G1lIExlbWHudHJl?=)
Date: Tue, 27 Sep 2005 15:27:41 -0400
Subject: [R] Binding dataframe with different length in rows and columns
Message-ID: <BAY103-DAV1023B5F87708A47F03F0A3908A0@phx.gbl>

Dear all,

I'm trying to combine in R the two following datasets. 
I'm would be very grateful if someone could help me with this problem.

Dataset 1:
X	Y	Z
1	27	48
2	25	50
3	27	40
4	28	56

Where X is a unique number and Y and Z some variables 

Dataset 2:
X	I	J
1	10	12
1	12	12
1	23	30
1	40	46
2	7	8
2	8	9
2	9	8
3	2	0
4	98	87
4	78	89
Where X IS NOT this time a unique number. I and J are some variables.

Thank you by advance


J??r??me Lema??tre
??tudiant au doctorat
Universit?? Laval
Qu??bec, QC



From nlemeur at fhcrc.org  Tue Sep 27 21:45:16 2005
From: nlemeur at fhcrc.org (Nolwenn LeMeur)
Date: Tue, 27 Sep 2005 12:45:16 -0700 (PDT)
Subject: [R] Binding dataframe with different length in rows and columns
In-Reply-To: <BAY103-DAV1023B5F87708A47F03F0A3908A0@phx.gbl>,
	<000a01c5c399$876c9520$26f0cb84@yourd93e6doqk7>
References: <BAY103-DAV1023B5F87708A47F03F0A3908A0@phx.gbl>,
	<000a01c5c399$876c9520$26f0cb84@yourd93e6doqk7>
Message-ID: <Pine.LNX.4.61.0509271244360.15870@vole.fhcrc.org>

Hi Jerome,

look at the merge() function

Nolwenn

On Tue, 27 Sep 2005, J?r?me Lema?tre wrote:

> Dear all,
> 
> I'm trying to combine in R the two following datasets. 
> I'm would be very grateful if someone could help me with this problem.
> 
> Dataset 1:
> X	Y	Z
> 1	27	48
> 2	25	50
> 3	27	40
> 4	28	56
> 
> Where X is a unique number and Y and Z some variables 
> 
> Dataset 2:
> X	I	J
> 1	10	12
> 1	12	12
> 1	23	30
> 1	40	46
> 2	7	8
> 2	8	9
> 2	9	8
> 3	2	0
> 4	98	87
> 4	78	89
> Where X IS NOT this time a unique number. I and J are some variables.
> 
> Thank you by advance
> 
> 
> J?r?me Lema?tre
> ?tudiant au doctorat
> Universit? Laval
> Qu?bec, QC
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

From tom at maladmin.com  Tue Sep 27 17:57:26 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 27 Sep 2005 11:57:26 -0400
Subject: [R] dynamic lists (data frames?)
In-Reply-To: <1127831995.4523.12.camel@localhost.localdomain>
References: <1127831995.4523.12.camel@localhost.localdomain>
Message-ID: <1127836646.4523.18.camel@localhost.localdomain>

Hmm my bad,
Thanks for your replies but I think my example was a little to simple
the actual code I'm using is:


f_haardisolve<-function(v_dataset){
    #pad data to make length a power of 2
    v_dataset<-f_paddata(v_dataset)

    l_cooef<-list() #holder for cooefficents
    i_count<-1      #identity counter

    while(length(v_dataset)>0){
        #seperate odd and even points
        v_dataset<-f_splitpoints(v_dataset)

        v_dataset<-f_haarpredict(v_dataset)
        v_dataset<-f_haaruplift(v_dataset)
        
        str_idx<-paste('c',i_count,sep='')
	l_cooef
$str_idx<-v_dataset[c((length(v_dataset)/2)+1:(length(v_dataset)/2))]
#should be no wrap on the previous line 

       v_dataset<-v_dataset[c(1:length(v_dataset)/2)]

        i_count<-i_count+1
    }
    l_cooef
}

In this particular case I could probably calculate the number of
iterations and use l_cooef<-names() but more is there a more generic
method for adding another column of data?



On Tue, 2005-27-09 at 10:39 -0400, tom wright wrote:
> Can someone please show me what I need to get something like this to
> work
> 
> for(a in c(1:5)){
> 	data$a<-c(a:10)
> }
> 
> so that i end up with a structure
> data$1<-[1,2,3,4,5,6,7,8,9,10]
> data$2<-[2,3,4,5,67,8,9,10]
> data$3<-[3,4,5,67,8,9,10]
> data$4<-[4,5,67,8,9,10]
> data$5<-[5,67,8,9,10]
> 
> thanks loads
> Tom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From gb at stat.umu.se  Tue Sep 27 21:58:45 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Tue, 27 Sep 2005 21:58:45 +0200
Subject: [R] [ESS]  Indentation in R code
In-Reply-To: <20050926170533.GA15945@stat.umu.se>
References: <148ed818050924130247520493@mail.gmail.com>
	<m2u0g93w1l.fsf@macaroni.local>
	<x2u0g9nizm.fsf@turmalin.kubism.ku.dk>
	<17207.41724.23980.747435@stat.math.ethz.ch>
	<20050926170533.GA15945@stat.umu.se>
Message-ID: <20050927195845.GA26067@stat.umu.se>

On Mon, Sep 26, 2005 at 07:05:33PM +0200, G??ran Brostr??m wrote:
> On Mon, Sep 26, 2005 at 09:27:56AM +0200, Martin Maechler wrote:
> > I'm crossposting to the ESS-help mailing list which is slightly
> > more appropriate here.  [This may be a rare case where
> > crossposting seems to make much sense.]
> > 
> > >>>>> "PD" == Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> > >>>>>     on 25 Sep 2005 19:40:45 +0200 writes:
> > 
> >     PD> Seth Falcon <sfalcon at fhcrc.org> writes:
> > 
> >     >> On 24 Sep 2005, goran.brostrom at gmail.com wrote:
> >     >> 
> >     >> > I am using emacs-21.3 when writing R functions on Linux debian, and
> >     >> > I am trying to follow the advice i R-exts.pdf (2.1.1) regarding
> >     >> > indentation. That is, I set 'c-default-style' to "bsd" and
> >     >> > 'c-basic-offset' to 4. However, while this gives me the intended
> >     >> > indentation in C code, it doesn't change the behavior in R code; I
> >     >> > still get an indentation of size 2. This is my .emacs file after
> >     >> > customization:
> >     >> >
> >     >> > (require 'ess-site)
> >     >> > (custom-set-variables
> >     >> >  ;; custom-set-variables was added by Custom -- don't edit or
> >     >> >  ;; cut/paste it!  Your init file should contain only one such
> >     >> >  ;; instance.
> >     >> >  '(c-basic-offset 4)
> >     >> >  '(c-default-style "bsd"))
> >     >> >  (custom-set-faces
> >     >> >  ;; custom-set-faces was added by Custom -- don't edit or cut/paste it!
> >     >> >  ;; Your init file should contain only one such instance.
> >     >> > )
> >     >> 
> >     >> Not sure if this is the best way, but I have the following after
> >     >> loading ess-site:
> >     >> 
> >     >> (setq ess-indent-level 4)
> > 
> > 
> > 
> >     PD> I have (I believe it stems from Martin M. originally):
> > 
> > yes, most probably  {IIRC, Kurt Hornik was involved too}.
> > 
> >     PD>      (add-hook 'c-mode-hook '(lambda()
> >     PD> 			       (c-set-style "stroustrup")))
> > 
> > the above is not quite what I have or did recommend, 
> > which is rather  "bsd" + "offset 4"  as G??ran has above
> > 
> > In fact, G??ran mentions the "R-exts" manual and that has
> > the following  *before* giving the emacs-lisp statements:
> > 
> > >> (For GNU Emacs 20: for GNU Emacs 21 use
> > >> customization to set the `c-default-style' to `"bsd"' and
> > >> `c-basic-offset' to `4'.)
> > 
> > and indeed, that's what G??ran did and you should do with a
> > current emacs, either customize via GUI or,
> > in your ~/.emacs file, find the section '(custom-set-variables ...)' and add 
> >  '(c-basic-offset 4)
> >  '(c-default-style "bsd")
> > 
> > to the lines already there, or if there's no such section, add
> > 
> > (custom-set-variables
> >   ;; custom-set-variables was added by Custom -- don't edit or cut/paste it!
> >   ;; Your init file should contain only one such instance.
> >  '(c-basic-offset 4)
> >  '(c-default-style "bsd")


> > to the end of your ~/.emacs file
> [...]
> 
> but this is not sufficient to get correct (4) indentation (ess) in R 
> functions. We need some reference to ess as well, right? Maybe another
> reference to the ESS manual is in order in 'R-exts'?

Martin, I realize that it was stupid of me to cut here; I only refer to C
indentation, and forget that the lines of lisp code referring to ESS of 
course is necessary to get indentation working in R functions.

Once again, thanks for your good help.

G??ran



From tom at maladmin.com  Tue Sep 27 18:10:43 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 27 Sep 2005 12:10:43 -0400
Subject: [R] dynamic lists (data frames?)
In-Reply-To: <1127831995.4523.12.camel@localhost.localdomain>
References: <1127831995.4523.12.camel@localhost.localdomain>
Message-ID: <1127837443.4523.21.camel@localhost.localdomain>

Got it thanks
the trick was to create a variable with the new name
str_idx<-paste('c',i,sep='')
then use double square brackets
l_cooef[str_idx]]<-whatever()


f_haardisolve<-function(v_dataset){
    #pad data to make length a power of 2
    v_dataset<-f_paddata(v_dataset)

    l_cooef<-list() #holder for cooefficents
    i_count<-1      #identity counter

    while(length(v_dataset)>0){
        #seperate odd and even points
        v_dataset<-f_splitpoints(v_dataset)

        v_dataset<-f_haarpredict(v_dataset)
        v_dataset<-f_haaruplift(v_dataset)
        
        str_idx<-paste('c',i_count,sep='')
        l_cooef[[str_idx]]<-list()

l_cooef[[str_idx]]<-v_dataset[c((length(v_dataset)/2)+1:(length(v_dataset)/2))]
        v_dataset<-v_dataset[c(1:length(v_dataset)/2)]

        i_count<-i_count+1
    }
    l_cooef
}


On Tue, 2005-27-09 at 10:39 -0400, tom wright wrote:
> Can someone please show me what I need to get something like this to
> work
> 
> for(a in c(1:5)){
> 	data$a<-c(a:10)
> }
> 
> so that i end up with a structure
> data$1<-[1,2,3,4,5,6,7,8,9,10]
> data$2<-[2,3,4,5,67,8,9,10]
> data$3<-[3,4,5,67,8,9,10]
> data$4<-[4,5,67,8,9,10]
> data$5<-[5,67,8,9,10]
> 
> thanks loads
> Tom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From venomousanimal at web.de  Tue Sep 27 22:07:36 2005
From: venomousanimal at web.de (venomousanimal)
Date: Tue, 27 Sep 2005 22:07:36 +0200
Subject: [R] Binding dataframe with different length in rows and columns
In-Reply-To: <BAY103-DAV1023B5F87708A47F03F0A3908A0@phx.gbl>
References: <BAY103-DAV1023B5F87708A47F03F0A3908A0@phx.gbl>
Message-ID: <4339A688.9040203@web.de>

Hello ,
How about cbind or rbind?
You could create a new data frame and push yours into it.
Greetz

J??r??me Lema??tre schrieb:

>Dear all,
>
>I'm trying to combine in R the two following datasets. 
>I'm would be very grateful if someone could help me with this problem.
>
>Dataset 1:
>X	Y	Z
>1	27	48
>2	25	50
>3	27	40
>4	28	56
>
>Where X is a unique number and Y and Z some variables 
>
>Dataset 2:
>X	I	J
>1	10	12
>1	12	12
>1	23	30
>1	40	46
>2	7	8
>2	8	9
>2	9	8
>3	2	0
>4	98	87
>4	78	89
>Where X IS NOT this time a unique number. I and J are some variables.
>
>Thank you by advance
>
>
>J??r??me Lema??tre
>??tudiant au doctorat
>Universit?? Laval
>Qu??bec, QC
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Tue Sep 27 22:25:57 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 27 Sep 2005 15:25:57 -0500
Subject: [R] suggest some books or materials for matrices basics ,
 its partitions, iterations, clusturing
In-Reply-To: <20050923103115.29625.qmail@web61219.mail.yahoo.com>
References: <20050923103115.29625.qmail@web61219.mail.yahoo.com>
Message-ID: <4339AAD5.50800@pdf.com>

	  Your question seems almost too broad, but I will attempt a brief 
response.

	  Have you reviewed the material on matrices in "An Introduction to R", 
avaliable via, e.g., "help.start()"?  Also, have you reviewed LAPACK 
(http://www.netlib.org/lapack/)?

	  If the answer to both is "yes", submit another post (after reading 
the posting guide, www.R-project.org/posting-guide.html, which should 
increase the chances that you will get a quick, useful reply).

	  spencer graves

booop booop wrote:

> Dear sir,
> Could anybody kindly suggest me some good
> e-books(which are available in the internet),or other
> materials in the web....
> sites to refer examples ...
> in the following area
> 
> matrices & its basics....
> partitioning of matrices...
> iterations of matrices....
> Clusturing in matrices...
> 
> thank you sir..
> 
> with kind regards,
> boopathy.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ron.ophir at weizmann.ac.il  Tue Sep 27 22:27:19 2005
From: ron.ophir at weizmann.ac.il (Ron Ophir)
Date: Tue, 27 Sep 2005 23:27:19 +0300
Subject: [R] Dummy quesion about environment
Message-ID: <s339d562.082@wisemail.weizmann.ac.il>

Hi,
I'm trying to understand environment object in R.
I used the example:
  f <- function(x) {
         y <- 10
         g <- function(x) x + y
         return(g)
     }
     h <- f()
     h(3)
then i saw that f return an environment
> h
function(x) x + y
<environment: 01B28570>
but I coudn't access to x and y object in that environment:
I tried 
get("x",env=h)
I tried
h$y
can I access y and x?
how can I see an environment tree? oes search does it?
Thanks,
Ron



From rpeng at jhsph.edu  Tue Sep 27 22:47:57 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 27 Sep 2005 16:47:57 -0400
Subject: [R] Dummy quesion about environment
In-Reply-To: <s339d562.082@wisemail.weizmann.ac.il>
References: <s339d562.082@wisemail.weizmann.ac.il>
Message-ID: <4339AFFD.9060008@jhsph.edu>

Try 'get("x", env = environment(h))'

-roger

Ron Ophir wrote:
> Hi,
> I'm trying to understand environment object in R.
> I used the example:
>   f <- function(x) {
>          y <- 10
>          g <- function(x) x + y
>          return(g)
>      }
>      h <- f()
>      h(3)
> then i saw that f return an environment
> 
>>h
> 
> function(x) x + y
> <environment: 01B28570>
> but I coudn't access to x and y object in that environment:
> I tried 
> get("x",env=h)
> I tried
> h$y
> can I access y and x?
> how can I see an environment tree? oes search does it?
> Thanks,
> Ron
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From p.dalgaard at biostat.ku.dk  Tue Sep 27 22:49:48 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Sep 2005 22:49:48 +0200
Subject: [R] Dummy quesion about environment
In-Reply-To: <s339d562.082@wisemail.weizmann.ac.il>
References: <s339d562.082@wisemail.weizmann.ac.il>
Message-ID: <x2oe6e44nn.fsf@turmalin.kubism.ku.dk>

"Ron Ophir" <ron.ophir at weizmann.ac.il> writes:

> Hi,
> I'm trying to understand environment object in R.
> I used the example:
>   f <- function(x) {
>          y <- 10
>          g <- function(x) x + y
>          return(g)
>      }
>      h <- f()
>      h(3)
> then i saw that f return an environment
> > h
> function(x) x + y
> <environment: 01B28570>
> but I coudn't access to x and y object in that environment:
> I tried 
> get("x",env=h)
> I tried
> h$y
> can I access y and x?

Well, there are special issues with x above, but the basic thing is to
take environment(h). Notice that h _is_ a function that _has_ an
associated environment. 

> get("y",env=environment(h))
[1] 10

As I said, x is stranger, which is because you used f() in the call:

> get("x",env=environment(h))

> str(get("x",env=environment(h)))
 symbol
> a <- get("x",env=environment(h))
> missing(a)
[1] TRUE
> evalq(x,environment(h))
Error in eval(expr, envir, enclos) : argument "x" is missing, with no
 default
> evalq(missing(x),environment(h))
[1] TRUE

You'll get the point if you look long and hard enough...

> how can I see an environment tree? 

You can't. You can see the parent of an environment, the grandparent,
etc., but there is no way to see which children a given environment
has. 

> oes search does it?

Huh?

> Thanks,
> Ron
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From venomousanimal at web.de  Tue Sep 27 22:55:41 2005
From: venomousanimal at web.de (venomousanimal)
Date: Tue, 27 Sep 2005 22:55:41 +0200
Subject: [R] Dummy quesion about environment
In-Reply-To: <4339AFFD.9060008@jhsph.edu>
References: <s339d562.082@wisemail.weizmann.ac.il> <4339AFFD.9060008@jhsph.edu>
Message-ID: <4339B1CD.1060600@web.de>

Hej,

but in your function you add x and y to 10 and 3, so your values are 
merged to one value available in g variable.
And now you want to see what was your y and your x?
I guess I do not get the idea of your question.
Well, then you could return y and x as well as g.

Greetz n god luck.


Roger D. Peng schrieb:

>Try 'get("x", env = environment(h))'
>
>-roger
>
>Ron Ophir wrote:
>  
>
>>Hi,
>>I'm trying to understand environment object in R.
>>I used the example:
>>  f <- function(x) {
>>         y <- 10
>>         g <- function(x) x + y
>>         return(g)
>>     }
>>     h <- f()
>>     h(3)
>>then i saw that f return an environment
>>
>>    
>>
>>>h
>>>      
>>>
>>function(x) x + y
>><environment: 01B28570>
>>but I coudn't access to x and y object in that environment:
>>I tried 
>>get("x",env=h)
>>I tried
>>h$y
>>can I access y and x?
>>how can I see an environment tree? oes search does it?
>>Thanks,
>>Ron
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>  
>



From ferran.carrascosa at gmail.com  Tue Sep 27 22:58:44 2005
From: ferran.carrascosa at gmail.com (Ferran Carrascosa)
Date: Tue, 27 Sep 2005 22:58:44 +0200
Subject: [R] Weighting factor
In-Reply-To: <Pine.LNX.4.63a.0509271059160.4400@homer22.u.washington.edu>
References: <fcd289fd05092709314d71b299@mail.gmail.com>
	<Pine.LNX.4.63a.0509271059160.4400@homer22.u.washington.edu>
Message-ID: <fcd289fd05092713581c6b3b51@mail.gmail.com>

Uau!
This package is very interesting, very thank you Thomas.

Thanks to all!

2005/9/27, Thomas Lumley <tlumley at u.washington.edu>:
> On Tue, 27 Sep 2005, Ferran Carrascosa wrote:
>
> > Hi everyone,
> >
> > I would like some package of R or any help to solve the next problem
> > with a weighting fatcors:
> >
> > Giving a data matrix with dichotomous (2 or more) variables in columns
> > and individuals in rows, and also a theorical distribution of the
> > dichotomous variables I would like a row weight (one per individual)
> > that transform the real distribution of variables to the theorical
> > distribution.
> >
>
> You should be able to do this with postStratify() in the survey package
> (if you have joint distributions for the variables) or calibrate() if you
> have only marginal distributions.
>
> You would need to create a survey design object, use postStratify() or
> calibrate() on it, then use weights() to extract the weights.
>
>        -thomas
>


--
Ferran Carrascosa



From ripley at stats.ox.ac.uk  Tue Sep 27 23:38:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Sep 2005 22:38:24 +0100 (BST)
Subject: [R] Make check fails on d-p-q-r-tests.R...
In-Reply-To: <a37dd5e4dc23b1a54eb750b1913022a1@openvistas.net>
References: <330a15840cdb7ac1f5a590cec1396780@openvistas.net>
	<Pine.LNX.4.61.0509270757200.5508@gannet.stats>
	<a37dd5e4dc23b1a54eb750b1913022a1@openvistas.net>
Message-ID: <Pine.LNX.4.61.0509272221250.18397@gannet.stats>

I am afraid this does look like a real problem, if a minor one. We have 
for the first problem

x <- 10^(ex <- c(1,2,5*(1:5),50,100,200,300,Inf))
ex <- -c(rev(1/x), ex)
qcauchy(ex, log=TRUE)

The first entry of the result should be Inf and the last -Inf.  From the 
output you have shown us I would guess the last is NaN.

If this is what is going on, it is a problem but a somewhat esoteric one 
that probably reflects a lack of IEC60559 (aka IEEE754) conformance.  It 
needs more hands-on debugging than we can provide on a help list.


On Tue, 27 Sep 2005, Jeff Ross wrote:

> On 1:03:39 am 09/27/05 Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>
>> You will have to show us the error!   It will be shown in
>> d-p-q-r-tests.Rout.fail (unless this was a segfault or similar).
>>
>> It is not OK to skip the test, but note that this test is random and
>> does fail about 1 in 50 times, so you could just try rerunning it.
>
> I am so sorry.  I didn't even think to look for a .fail file.
>
> Here is the last bit of it:
>
>>> ## for PR#7902:
>> ex <- -c(rev(1/x), ex)
>> All.eq(-x, qcauchy(pcauchy(-x)))
> [1] TRUE
>> All.eq(+x, qcauchy(pcauchy(+x, log=TRUE), log=TRUE))
> [1] TRUE
>> All.eq(1/x, pcauchy(qcauchy(1/x)))
> [1] TRUE
>> All.eq(ex,  pcauchy(qcauchy(ex, log=TRUE), log=TRUE))
> [1] "`is.NA' value mismatches: 1 in current, 0  in target"
> Warning message:
> NaNs produced in: qcauchy(p, location, scale, lower.tail, log.p)
>> II <- c(-Inf,Inf)
>> stopifnot(pcauchy(II) == 0:1, ## qcauchy(0:1) == II,
> +           pcauchy(II, log=TRUE) == c(-Inf,0),
> +           qcauchy(c(-Inf,0), log=TRUE) == II)
> Error in if (!(is.logical(r <- eval(ll[[i]])) && all(r)))
> stop(paste(deparse(mc[[i +  :
>  missing value where TRUE/FALSE needed
> In addition: Warning message:
> NaNs produced in: qcauchy(p, location, scale, lower.tail, log.p)
> Execution halted
>
> This is from the latest R-patched source tar ball, and it is the identical
> error as R-2.1.1.  I've consistently gotten the same error, even with
> configuration file tweaks and making clean between runs.
>
> Thanks!
>
> Jeff Ross

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From David.Duffy at qimr.edu.au  Tue Sep 27 23:38:49 2005
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 28 Sep 2005 07:38:49 +1000 (EST)
Subject: [R] Simulate phi-coefficient (correlation between dichotomous
 vars)
In-Reply-To: <FADCFAA8BA80C748890C1D3893C198D993C97F@amedmlmhah01.eur.amed.ds.army.mil>
References: <FADCFAA8BA80C748890C1D3893C198D993C97F@amedmlmhah01.eur.amed.ds.army.mil>
Message-ID: <Pine.LNX.4.58.0509280612150.6084@orpheus.qimr.edu.au>

On Tue, 27 Sep 2005, Bliese, Paul D LTC USAMH wrote:
> My situation is a bit more complicated and I'm not sure it is easily
> solved.  The problem is that I must assume one of the vectors is
> constant and generate one or more vectors that covary with the constant
> vector.
>
>> One way is to sample from the 2x2 table with the specified means and
>> pearson correlation (phi):
>>
>> for a fourfold table, a b
>>                       c d
>> with marginal proportions p1 and p2
>> cov <- phi * sqrt(p1*(1-p1)*p2*(1-p2))
>> a <- p1*p2 + cov
>> b <- p1*(1-p2) - cov
>> c <- (1-p1)*p2 - cov
>> d <- (1-p1)*(1-p2) + cov
>

Calculate the conditional probabilities from the above

P(X2=1|X1=1)= a/(a+b) = p2 + cov/p1
P(X2=1|X1=0)= c/(c+d) = p2 - cov/(1-p1)


condsim <- function(X, phi, p2, p1=NULL) {
  if (!all(X %in% c(0,1))) stop("expecting 1's and 0's")
  if (is.null(p1)) p1 <- mean(X)
  covar <- phi  * sqrt(p1*(1-p1)*p2*(1-p2))
  if (covar>0 && covar>(min(p1,p2)-p1*p2)) {
    warning("Specified correlation too large for given marginal proportions")
    covar <- min(p1,p2)-p1*p2
  }else if (covar<0 && covar < -min(p1*p2,(1-p1)*(1-p2))) {
    warning("Specified correlation too small for given marginal proportions")
    covar <- -min(p1*p2,(1-p1)*(1-p2))
  }
  Y <- X
  i1 <- X==1
  i0 <- X==0
  Y[i1] <- rbinom(sum(i1),1, p2 + covar/p1)
  Y[i0] <- rbinom(sum(i0),1, p2 - covar/(1-p1))
  data.frame(X,Y)
}

David Duffy.



From vangyzen at stat.duke.edu  Wed Sep 28 00:03:55 2005
From: vangyzen at stat.duke.edu (Eric van Gyzen)
Date: Tue, 27 Sep 2005 18:03:55 -0400
Subject: [R] R CMD build produces tar error under FreeBSD 5.4
In-Reply-To: <Pine.GSO.4.31.0509251202000.18680-100000@toucan.stats>
References: <20050925074451.GF42964@ms.unimelb.edu.au>
	<Pine.GSO.4.31.0509251202000.18680-100000@toucan.stats>
Message-ID: <20050927220355.GA3002@xavier.isds.duke.edu>

On Sun, Sep 25, 2005 at 12:07:02PM +0100, Prof Brian Ripley wrote:
> On Sun, 25 Sep 2005, Andrew Robinson wrote:
> 
> > Hi R-helpers,
> >
> > I am trying to build a package under FreeBSD 5.4-RELEASE #0 using R
> > Version 2.1.1.
> >
> > I have constructed a package using package.skeleton(), when I try
> >
> > $ R CMD build foo
> > * checking for file 'foo/DESCRIPTION' ... OK
> > * preparing 'foo':
> > * checking DESCRIPTION meta-information ... OK
> > * cleaning src
> > * removing junk files
> > tar: Option -L is not permitted in mode -x
> > Error: cannot open file 'foo/DESCRIPTION' for reading
> >
> > foo/DESCRIPTION exists and the permissions are correct. The same
> > command works under Linux Fedora 2.  The man pages on each OS imply
> > that tar differs across the two platforms.  Does anyone have any
> > thoughts on a work-around?
> 
> No, because R does not use tar -L (which is to do with tape lengths on GNU
> tar).
> 
> It does use tar chf and tar xhf.  The h modifier would appear to be
> applicable only to dumps, so at a wild guess the error message means -h is
> not permitted.  Try replacing xhf by xf.

FreeBSD >= 5.3 uses bsdtar.  Previous versions used GNU tar.

In bsdtar, -h is a synonym for -L, and -L means:

     -L      (c and r mode only) All symbolic links will be followed.  Nor-
             mally, symbolic links are archived as such.  With this option,
             the target of the link will be archived instead.

The man page for bsdtar doesn't indicate an option to dereference
symlinks during extraction.  :(

Eric



From mail at bymouth.com  Wed Sep 28 00:17:21 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Wed, 28 Sep 2005 08:17:21 +1000
Subject: [R] different models
Message-ID: <000001c5c3b1$41c73850$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050928/20b051bd/attachment.pl

From David.Brahm at geodecapital.com  Wed Sep 28 00:26:40 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Tue, 27 Sep 2005 18:26:40 -0400
Subject: [R] Help: x11 position in the Unix environment
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BC6E@MSGBOSCLF2WIN.DMN1.FMR.COM>

Shengzhe Wu <shengzhe at gmail.com> wrote:

> In the Unix environment, I open a window by x11(). May I specify the
> position of this window by specifying the position of the top left of
> the window as in Windows environment?

I use "xwit" (version 3.4), a system command which manipulates existing
X windows.  My R code for initiating a graphics window contains:

  system(paste("xwit -move 2045 0 -names 'R Graphics: Device ",
               dev.cur(), "'", sep=""), ignore.stderr=TRUE)

See, e.g, <http://www.x.org/contrib/utilities/xwit-3.4.tar.gz>.
(I'm pretty sure I got the idea from another kind soul on R-help.)

-- David Brahm (brahm at alum.mit.edu)



From jross at openvistas.net  Wed Sep 28 00:32:25 2005
From: jross at openvistas.net (Jeff Ross)
Date: Tue, 27 Sep 2005 16:32:25 -0600 (MDT)
Subject: [R] Make check fails on d-p-q-r-tests.R...
In-Reply-To: <Pine.LNX.4.61.0509272221250.18397@gannet.stats>
References: <330a15840cdb7ac1f5a590cec1396780@openvistas.net>
	<Pine.LNX.4.61.0509270757200.5508@gannet.stats>
	<a37dd5e4dc23b1a54eb750b1913022a1@openvistas.net>
	<Pine.LNX.4.61.0509272221250.18397@gannet.stats>
Message-ID: <Pine.BSO.4.63.0509271631450.9423@heinlein.openvistas.net>



On Tue, 27 Sep 2005, Prof Brian Ripley wrote:

> I am afraid this does look like a real problem, if a minor one. We have for 
> the first problem
>
> x <- 10^(ex <- c(1,2,5*(1:5),50,100,200,300,Inf))
> ex <- -c(rev(1/x), ex)
> qcauchy(ex, log=TRUE)
>
> The first entry of the result should be Inf and the last -Inf.  From the 
> output you have shown us I would guess the last is NaN.
>
> If this is what is going on, it is a problem but a somewhat esoteric one that 
> probably reflects a lack of IEC60559 (aka IEEE754) conformance.  It needs 
> more hands-on debugging than we can provide on a help list.
>

Thanks for the analysis.  I'll move this over to r-devel.

Jeff



From aorchid at mac.com  Wed Sep 28 02:11:44 2005
From: aorchid at mac.com (Aric Gregson)
Date: Tue, 27 Sep 2005 20:11:44 -0400
Subject: [R] Plot Data Points in boxplots
Message-ID: <272357639150304F937C975E@G4Desktop.local>

Hello,

I would like to plot my data in a fashion similar to a boxplot, but 
plot the true data points without a box, just overlay lines of the 
summary generated by the boxplot. I have less than 10 observations 
within each group, so I think showing the actual data would be more 
effective than the box of the boxplot. I have been unable to find a way 
to do this.

Here is example data:
> d168teni
  d168dh10i d168hb10i d168icc10i d168rcs10i d168t410i d168tb410i
1        72        52         29         80        39         68
2        76        47         28         68        49         21
3       123        85         87         71       164        137
4        58        47         50         70        18          1

> boxplot(d168teni)

works to describe the data (each column a column in the plot). However, 
instead of the boxes, I want the data plotted (in a column) with the 5 
summary lines drawn over the points.

I have tried using functions from Design and have been unable to find a 
solution. I think I am missing the point.

Any suggestions on where to look or how to approach this differently?

thanks,

aric



From ggrothendieck at gmail.com  Wed Sep 28 02:55:05 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Sep 2005 20:55:05 -0400
Subject: [R] Plot Data Points in boxplots
In-Reply-To: <272357639150304F937C975E@G4Desktop.local>
References: <272357639150304F937C975E@G4Desktop.local>
Message-ID: <971536df0509271755308e5a33@mail.gmail.com>

Try this:

boxplot(Sepal.Length ~ Species, iris)
with(iris, stripchart(Sepal.Length ~ Species, vertical = TRUE, add = TRUE))

On 9/27/05, Aric Gregson <aorchid at mac.com> wrote:
> Hello,
>
> I would like to plot my data in a fashion similar to a boxplot, but
> plot the true data points without a box, just overlay lines of the
> summary generated by the boxplot. I have less than 10 observations
> within each group, so I think showing the actual data would be more
> effective than the box of the boxplot. I have been unable to find a way
> to do this.
>
> Here is example data:
> > d168teni
>  d168dh10i d168hb10i d168icc10i d168rcs10i d168t410i d168tb410i
> 1        72        52         29         80        39         68
> 2        76        47         28         68        49         21
> 3       123        85         87         71       164        137
> 4        58        47         50         70        18          1
>
> > boxplot(d168teni)
>
> works to describe the data (each column a column in the plot). However,
> instead of the boxes, I want the data plotted (in a column) with the 5
> summary lines drawn over the points.
>
> I have tried using functions from Design and have been unable to find a
> solution. I think I am missing the point.
>
> Any suggestions on where to look or how to approach this differently?
>
> thanks,
>
> aric
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From klebyn at yahoo.com.br  Wed Sep 28 04:14:29 2005
From: klebyn at yahoo.com.br (klebyn)
Date: Wed, 28 Sep 2005 00:14:29 -0200
Subject: [R] scatterplot3d + density() + polygon(color)
Message-ID: <4339FC85.7090409@yahoo.com.br>


Hi R Users,

How to use the function polygon()
together with the package scatterplot3d?

I am trying to color below of the curves
defined for the function density().

I tried to use the site: R GRAPH GALLERY
as tutorial.

I tried to adapt the example of this page:
[figure]:
http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=30

[code]:
http://addictedtor.free.fr/graphiques/sources/source_30.R

to my case but I do not obtain success.

Somebody could give a tip to me, please?

I am thankful anticipatedly.

Cleber Borges



#My code test
##############
library(scatterplot3d)
x=c(0.4, -1.2, .8, -.7, 0)

d1 = density(x[1],bw=1.2, from=-3.0,  to=3.0  )
d2 = density(x[2],bw=0.8, from=-3.0,  to=3.0  )
d3 = density(x[3],bw=0.6, from=-2.5,  to=2.5  )
d4 = density(x[4],bw=0.5, from=-2.0,  to=2.0  )
d5 = density(x[5],bw=0.3, from=-1.5,  to=1.5  )

sx = c(d1$x,d2$x,d3$x,d4$x,d5$x)
sy = c(d1$y,d2$y,d3$y,d4$y,d5$y)
sz = c(rep(0.1,512),rep(0.2,512),rep(0.3,512),rep(0.4,512),rep(0.5,512))

scatterplot3d(x=sx,y=sz,z=sy,type='l')
##############



From qsb at nlbmol.ibp.ac.cn  Wed Sep 28 08:28:38 2005
From: qsb at nlbmol.ibp.ac.cn (Simple)
Date: Wed, 28 Sep 2005 14:28:38 +0800
Subject: [R] need suggestion about building formual
Message-ID: <200509281428.38761.qsb@nlbmol.ibp.ac.cn>

hi,
I'm an newbie for R,I want do some fitting in R.

I wander if it is possible to write a few of equations but only one formual 
when fitting 

Currently,My problem is,in R, is there methods combination a few equations 
into one formual?
For example, 
y=f1(k);
k=f2(t);
t=f3(x);
although it is certain that the can be equations turn into one formual as 
y~f(x),but write such a complexity string make me painful.

I have searched the web and found out there were only examples with one 
formual.any suggestion? 

I hope that I have omit something.



From joel3000 at gmail.com  Wed Sep 28 08:54:15 2005
From: joel3000 at gmail.com (Joel Bremson)
Date: Tue, 27 Sep 2005 23:54:15 -0700
Subject: [R] gfortran Makefile for cygwin
Message-ID: <1253d67a05092723546a00ec82@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050927/492a1055/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed Sep 28 08:58:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 28 Sep 2005 08:58:16 +0200
Subject: [R] scatterplot3d + density() + polygon(color)
In-Reply-To: <4339FC85.7090409@yahoo.com.br>
References: <4339FC85.7090409@yahoo.com.br>
Message-ID: <433A3F08.9090501@statistik.uni-dortmund.de>

klebyn wrote:
> Hi R Users,
> 
> How to use the function polygon()
> together with the package scatterplot3d?
> 
> I am trying to color below of the curves
> defined for the function density().
> 
> I tried to use the site: R GRAPH GALLERY
> as tutorial.
> 
> I tried to adapt the example of this page:
> [figure]:
> http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=30
> 
> [code]:
> http://addictedtor.free.fr/graphiques/sources/source_30.R
> 
> to my case but I do not obtain success.
> 
> Somebody could give a tip to me, please?
> 
> I am thankful anticipatedly.
> 
> Cleber Borges
> 
> 
> 
> #My code test
> ##############
> library(scatterplot3d)
> x=c(0.4, -1.2, .8, -.7, 0)
> 
> d1 = density(x[1],bw=1.2, from=-3.0,  to=3.0  )
> d2 = density(x[2],bw=0.8, from=-3.0,  to=3.0  )
> d3 = density(x[3],bw=0.6, from=-2.5,  to=2.5  )
> d4 = density(x[4],bw=0.5, from=-2.0,  to=2.0  )
> d5 = density(x[5],bw=0.3, from=-1.5,  to=1.5  )
> 
> sx = c(d1$x,d2$x,d3$x,d4$x,d5$x)
> sy = c(d1$y,d2$y,d3$y,d4$y,d5$y)
> sz = c(rep(0.1,512),rep(0.2,512),rep(0.3,512),rep(0.4,512),rep(0.5,512))
> 
> scatterplot3d(x=sx,y=sz,z=sy,type='l')
> ##############
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Example:

     library(scatterplot3d)

     x <- c(0.4, -1.2, .8, -.7, 0)
     d <- vector(length = length(x), mode = "list")
     d[[1]] <- density(x[1], bw = 1.2, from = -3.0, to = 3.0)
     d[[2]] <- density(x[2], bw = 0.8, from = -3.0, to = 3.0)
     d[[3]] <- density(x[3], bw = 0.6, from = -2.5, to = 2.5)
     d[[4]] <- density(x[4], bw = 0.5, from = -2.0, to = 2.0)
     d[[5]] <- density(x[5], bw = 0.3, from = -1.5, to = 1.5)

     x <- lapply(d, "[[", "x")
     y <- lapply(d, "[[", "y")
     z <- lapply(seq(0.1, 0.5, 0.1), rep, each = 512)

     sx <- unlist(x)
     sy <- unlist(y)
     sz <- unlist(z)

     s3d <- scatterplot3d(x = sx, y = sz, z = sy, type = "n")
     for(i in rev(seq(along=d))){
         s3d_coords <- s3d$xyz.convert(x[[i]], z[[i]], y[[i]])
         polygon(s3d_coords, col = i, border = "black")
     }


Uwe Ligges



From ligges at statistik.uni-dortmund.de  Wed Sep 28 09:01:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 28 Sep 2005 09:01:55 +0200
Subject: [R] gfortran Makefile for cygwin
In-Reply-To: <1253d67a05092723546a00ec82@mail.gmail.com>
References: <1253d67a05092723546a00ec82@mail.gmail.com>
Message-ID: <433A3FE3.3080203@statistik.uni-dortmund.de>

Joel Bremson wrote:

> Hi all,
> 
> I'm porting a package that I've worked on for OS X to Cygwin/Windows.

Cygwin is not supported.

Please use MinGW's compilers, the tools and import from MkRules, as the 
manuals indicate. See other packages' Makefile.win files as examples.

Messages regarding development seem to be more appropriate for the 
R-devel list.
[Please move it there for follow-ups.]

Uwe Ligges


> This package requires a Makefile. My question is, how can I find out
> (or what is), the link command?
> 
> Here is the OS X Makefile:
> 
> 
> RLIB_LOC=${R_HOME}
> 
> F90_FILES=\
> class_data_frame.f90 \
> class_old_dbest.f90 \
> class_cm_data.f90 \
> class_cm.f90 \
> class_bgw.f90 \
> class_cm_mle.f90 \
> cme.f90
> 
> 
> FORTRAN_FILES=\
> dgletc.f \
> dglfgb.f\
> dglfg.f\
> dmdc.f\
> mecdf.f
> 
> 
> %.o: %.f90
> gfortran -c -g $<
> 
> %.o: %.f
> gfortran -c -g $<
> 
> bpkg.so: $(F90_FILES:%.f90=%.o) $(FORTRAN_FILES:%.f=%.o)
> gcc -Wall -bundle -flat_namespace -undefined suppress -L/sw/lib
> -L/usr/local/lib -o $@ $^ \
> -L$(RLIB_LOC)/lib -lR
> 
> ###EOF####
> 
> The -L lib dirs are not correct. On a *nix platform I would do something
> like this
> 
> sh -x R CMD SHLIB ...
> 
> to get at the R internal link information but I can't get that to work on
> Cygwin.
> 
> Regards,
> 
> Joel
> 
> 
> --
> Joel Bremson
> Graduate Student
> Institute for Transportation Studies - UC Davis
> http://etrans.blogspot.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Sep 28 09:23:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Sep 2005 08:23:59 +0100 (BST)
Subject: [R] Question on lm(): When does R-squared come out as NA?
In-Reply-To: <20050925144109.GT437@lubyanka.local>
References: <20050925144109.GT437@lubyanka.local>
Message-ID: <Pine.LNX.4.61.0509280822440.27891@gannet.stats>

I've not seen a reply to this, nor ever seen it.
Please make a reproducible example available (do see the posting guide).

On Sun, 25 Sep 2005, Ajay Narottam Shah wrote:

> I have a situation with a large dataset (3000+ observations), where
> I'm doing lags as regressors, where I get:
>
> Call:
> lm(formula = rj ~ rM + rM.1 + rM.2 + rM.3 + rM.4)
>
> Residuals:
> 1990-06-04 1994-11-14 1998-08-21 2002-03-13 2005-09-15
>  -5.64672   -0.59596   -0.04143    0.55412    8.18229
>
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) -0.003297   0.017603  -0.187    0.851
> rM           0.845169   0.010522  80.322   <2e-16 ***
> rM.1         0.116330   0.010692  10.880   <2e-16 ***
> rM.2         0.002044   0.010686   0.191    0.848
> rM.3         0.013181   0.010692   1.233    0.218
> rM.4         0.009587   0.010525   0.911    0.362
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Residual standard error: 1.044 on 3532 degrees of freedom
> Multiple R-Squared:    NA,	Adjusted R-squared:    NA
> F-statistic:    NA on 5 and 3532 DF,  p-value: NA
>
>
> rM.1, rM.2, etc. are lagged values of rM. The OLS seems fine in every
> respect, except that there is an NA as the multiple R-squared. I will
> be happy to give sample data to someone curious about what is going
> on. I wondered if this was a well-known pathology. The way I know it,
> if the data allows computation of (X'X)^{-1}, one can compute the R2.
>
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Sep 28 09:15:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Sep 2005 08:15:55 +0100 (BST)
Subject: [R] R CMD build produces tar error under FreeBSD 5.4
In-Reply-To: <20050927220355.GA3002@xavier.isds.duke.edu>
References: <20050925074451.GF42964@ms.unimelb.edu.au>
	<Pine.GSO.4.31.0509251202000.18680-100000@toucan.stats>
	<20050927220355.GA3002@xavier.isds.duke.edu>
Message-ID: <Pine.LNX.4.61.0509280812320.27891@gannet.stats>

On Tue, 27 Sep 2005, Eric van Gyzen wrote:

> On Sun, Sep 25, 2005 at 12:07:02PM +0100, Prof Brian Ripley wrote:
>> On Sun, 25 Sep 2005, Andrew Robinson wrote:
>>
>>> Hi R-helpers,
>>>
>>> I am trying to build a package under FreeBSD 5.4-RELEASE #0 using R
>>> Version 2.1.1.
>>>
>>> I have constructed a package using package.skeleton(), when I try
>>>
>>> $ R CMD build foo
>>> * checking for file 'foo/DESCRIPTION' ... OK
>>> * preparing 'foo':
>>> * checking DESCRIPTION meta-information ... OK
>>> * cleaning src
>>> * removing junk files
>>> tar: Option -L is not permitted in mode -x
>>> Error: cannot open file 'foo/DESCRIPTION' for reading
>>>
>>> foo/DESCRIPTION exists and the permissions are correct. The same
>>> command works under Linux Fedora 2.  The man pages on each OS imply
>>> that tar differs across the two platforms.  Does anyone have any
>>> thoughts on a work-around?
>>
>> No, because R does not use tar -L (which is to do with tape lengths on GNU
>> tar).
>>
>> It does use tar chf and tar xhf.  The h modifier would appear to be
>> applicable only to dumps, so at a wild guess the error message means -h is
>> not permitted.  Try replacing xhf by xf.
>
> FreeBSD >= 5.3 uses bsdtar.  Previous versions used GNU tar.
>
> In bsdtar, -h is a synonym for -L, and -L means:
>
>     -L      (c and r mode only) All symbolic links will be followed.  Nor-
>             mally, symbolic links are archived as such.  With this option,
>             the target of the link will be archived instead.
>
> The man page for bsdtar doesn't indicate an option to dereference
> symlinks during extraction.  :(

Thanks for the confirmation (which I had managed to find from

http://www.freebsd.org/cgi/man.cgi?query=tar&apropos=0&sektion=0&manpath=FreeBSD+5.4-RELEASE+and+Ports&format=html

).  R does not need xh here, and I have changed it in 2.2.0-beta.

I do suggest you submit a bug report on the incorrect error message, 
though, which should refer to the option used, not one that is not used.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Wed Sep 28 09:45:09 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 28 Sep 2005 09:45:09 +0200
Subject: [R] scatterplot3d + density() + polygon(color)
In-Reply-To: <433A3F08.9090501@statistik.uni-dortmund.de>
References: <4339FC85.7090409@yahoo.com.br>
	<433A3F08.9090501@statistik.uni-dortmund.de>
Message-ID: <17210.18949.633974.419924@stat.math.ethz.ch>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Wed, 28 Sep 2005 08:58:16 +0200 writes:

    UweL> klebyn wrote:
    >> Hi R Users,
    >> 
    >> How to use the function polygon() together with the
    >> package scatterplot3d?
    >> 
    >> I am trying to color below of the curves defined for the
    >> function density().
    >> 
    >> I tried to use the site: R GRAPH GALLERY as tutorial.
    >> 
    >> I tried to adapt the example of this page: [figure]:
    >> http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=30
    >> 
    >> [code]:
    >> http://addictedtor.free.fr/graphiques/sources/source_30.R
    >> 
    >> to my case but I do not obtain success.
    >> 
    >> Somebody could give a tip to me, please?
    >> 
    >> I am thankful anticipatedly.
    >> 
    >> Cleber Borges
    >> 
    >> 
    >> 
    >> #My code test
    >> ##############
    >>    ...................

    UweL> Example:

>>      library(scatterplot3d)
>> 
>>      x <- c(0.4, -1.2, .8, -.7, 0)
>>      d <- vector(length = length(x), mode = "list")
>>      d[[1]] <- density(x[1], bw = 1.2, from = -3.0, to = 3.0)
>>      d[[2]] <- density(x[2], bw = 0.8, from = -3.0, to = 3.0)
>>      d[[3]] <- density(x[3], bw = 0.6, from = -2.5, to = 2.5)
>>      d[[4]] <- density(x[4], bw = 0.5, from = -2.0, to = 2.0)
>>      d[[5]] <- density(x[5], bw = 0.3, from = -1.5, to = 1.5)
>> 
>>      x <- lapply(d, "[[", "x")
>>      y <- lapply(d, "[[", "y")
>>      z <- lapply(seq(0.1, 0.5, 0.1), rep, each = 512)
>> 
>>      sx <- unlist(x)
>>      sy <- unlist(y)
>>      sz <- unlist(z)
>> 
>>      s3d <- scatterplot3d(x = sx, y = sz, z = sy, type = "n")
>>      for(i in rev(seq(along=d))){
>>          s3d_coords <- s3d$xyz.convert(x[[i]], z[[i]], y[[i]])
>>          polygon(s3d_coords, col = i, border = "black")
>>      }
>> 

Very nice, Uwe!

To make it "perfect", you'd have to add

     s3d$box3d()

at the end; otherwise some of the painted polygons hide lines of
the cube box which should not be hidden.

Martin Maechler



From h.brunschwig at utoronto.ca  Wed Sep 28 09:55:08 2005
From: h.brunschwig at utoronto.ca (Hadassa Brunschwig)
Date: Wed, 28 Sep 2005 03:55:08 -0400
Subject: [R] R2WinBUGS: Comparison to WinBUGS
Message-ID: <1127894108.433a4c5cde322@webmail.utoronto.ca>

Hi R-Help!


I used R2WinBUGS and WinBUGS directly on the same model just to compare. It
seems I am still making a mistake: after running the function bugs() I tried to
plot the posteriors of the parameters by using read.bugs() to convert the output
to an mcmc object and then plot.mcmc() to plot the densities. Using the same
model, the same number of iterations, the same initial values and the same data
I get completely different plots for the densities (e.g. the range of one
parameter in R2WinBUGS is from 0 to 8 but in WinBUGS only from 1.5 to 3)??? That
means my results are different, too.
Also, on the plot it says N=345 which is not what I specified in the bugs()
function (I specified 12000 iterations).
Below I put some of the code I used (if that helps):

parameters <-
c("tau","C0","st90","C0.pop","st90.pop","tau.cpop","tau.stpop","st90.pop80")
inits      <- inits <- function(){
    list(tau = rep(1, 17),tau.cpop = 0.2, tau.stpop = 1)
  }
  
mcmcA      <-
bugs(dataA,inits,parameters,modelA,n.chains=3,debug=T,n.iter=12000,n.burnin=2001,
                       bugs.directory="c:/Program
Files/WinBUGS14",working.directory="C:/Documents and
Settings/Daikon/Roche/R2WinBUGS Output",codaPkg=T)
 
codaA1      <- read.bugs(mcmcA[1])
plot(codaA1)


THANKS A LOT!!
-- 

Hadassa Brunschwig
Birmannsgasse 10A
CH-4055 Basel
Switzerland
Phone: +41 78 797 6065
Email: h.brunschwig at utoronto.ca



From sturtz at statistik.uni-dortmund.de  Wed Sep 28 10:14:46 2005
From: sturtz at statistik.uni-dortmund.de (Sibylle Sturtz)
Date: Wed, 28 Sep 2005 10:14:46 +0200
Subject: [R] [Fwd:  R2WinBUGS: Comparison to WinBUGS]
In-Reply-To: <433A4E53.6010409@statistik.uni-dortmund.de>
References: <433A4E53.6010409@statistik.uni-dortmund.de>
Message-ID: <433A50F6.1020800@statistik.uni-dortmund.de>

This is due to the following:

In bugs(), the default for thinning is

n.thin = max(1, floor(n.chains * (n.iter - n.burnin)/1000))

which is 29 for n.iter=12000 and n.burnin=2001 as in your example. 
Therefore, the number of iterations used for calculation of posterior 
values is

(12000-2001)/29 = 344.7931

which corresponds to the number of iterations given in your plot. If you 
specify the thinning parameter directly in bugs() it should be fine.

Sibylle

> -------- Original Message --------
> Subject: [R] R2WinBUGS: Comparison to WinBUGS
> Date: Wed, 28 Sep 2005 03:55:08 -0400
> From: Hadassa Brunschwig <h.brunschwig at utoronto.ca>
> To: r-help at stat.math.ethz.ch
> 
> Hi R-Help!
> 
> 
> I used R2WinBUGS and WinBUGS directly on the same model just to compare. It
> seems I am still making a mistake: after running the function bugs() I 
> tried to
> plot the posteriors of the parameters by using read.bugs() to convert 
> the output
> to an mcmc object and then plot.mcmc() to plot the densities. Using the 
> same
> model, the same number of iterations, the same initial values and the 
> same data
> I get completely different plots for the densities (e.g. the range of one
> parameter in R2WinBUGS is from 0 to 8 but in WinBUGS only from 1.5 to 
> 3)??? That
> means my results are different, too.
> Also, on the plot it says N=345 which is not what I specified in the bugs()
> function (I specified 12000 iterations).
> Below I put some of the code I used (if that helps):
> 
> parameters <-
> c("tau","C0","st90","C0.pop","st90.pop","tau.cpop","tau.stpop","st90.pop80") 
> 
> inits      <- inits <- function(){
>     list(tau = rep(1, 17),tau.cpop = 0.2, tau.stpop = 1)
>   }
> 
> mcmcA      <-
> bugs(dataA,inits,parameters,modelA,n.chains=3,debug=T,n.iter=12000,n.burnin=2001, 
> 
>                        bugs.directory="c:/Program
> Files/WinBUGS14",working.directory="C:/Documents and
> Settings/Daikon/Roche/R2WinBUGS Output",codaPkg=T)
> 
> codaA1      <- read.bugs(mcmcA[1])
> plot(codaA1)
> 
> 
> THANKS A LOT!!

-- 
Dipl.-Stat. Sibylle Sturtz
Mathematische Statistik und biometrische Anwendungen
Fachbereich Statistik
Universit??t Dortmund
44221 Dortmund
Tel.: 0231/755 4391
FAX : 0231/755 5303



From francoisromain at free.fr  Wed Sep 28 10:22:15 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 28 Sep 2005 10:22:15 +0200
Subject: [R] scatterplot3d + density() + polygon(color)
In-Reply-To: <17210.18949.633974.419924@stat.math.ethz.ch>
References: <4339FC85.7090409@yahoo.com.br>	<433A3F08.9090501@statistik.uni-dortmund.de>
	<17210.18949.633974.419924@stat.math.ethz.ch>
Message-ID: <433A52B7.6010907@free.fr>

Le 28.09.2005 09:45, Martin Maechler a ??crit :

>>>>>>"UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>>    on Wed, 28 Sep 2005 08:58:16 +0200 writes:
>>>>>>            
>>>>>>
>
>    UweL> klebyn wrote:
>    >> Hi R Users,
>    >> 
>    >> How to use the function polygon() together with the
>    >> package scatterplot3d?
>    >> 
>    >> I am trying to color below of the curves defined for the
>    >> function density().
>    >> 
>    >> I tried to use the site: R GRAPH GALLERY as tutorial.
>    >> 
>    >> I tried to adapt the example of this page: [figure]:
>    >> http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=30
>    >> 
>    >> [code]:
>    >> http://addictedtor.free.fr/graphiques/sources/source_30.R
>    >> 
>    >> to my case but I do not obtain success.
>    >> 
>    >> Somebody could give a tip to me, please?
>    >> 
>    >> I am thankful anticipatedly.
>    >> 
>    >> Cleber Borges
>    >> 
>    >> 
>    >> 
>    >> #My code test
>    >> ##############
>    >>    ...................
>
>    UweL> Example:
>
>  
>
>>>     library(scatterplot3d)
>>>
>>>     x <- c(0.4, -1.2, .8, -.7, 0)
>>>     d <- vector(length = length(x), mode = "list")
>>>     d[[1]] <- density(x[1], bw = 1.2, from = -3.0, to = 3.0)
>>>     d[[2]] <- density(x[2], bw = 0.8, from = -3.0, to = 3.0)
>>>     d[[3]] <- density(x[3], bw = 0.6, from = -2.5, to = 2.5)
>>>     d[[4]] <- density(x[4], bw = 0.5, from = -2.0, to = 2.0)
>>>     d[[5]] <- density(x[5], bw = 0.3, from = -1.5, to = 1.5)
>>>
>>>     x <- lapply(d, "[[", "x")
>>>     y <- lapply(d, "[[", "y")
>>>     z <- lapply(seq(0.1, 0.5, 0.1), rep, each = 512)
>>>
>>>     sx <- unlist(x)
>>>     sy <- unlist(y)
>>>     sz <- unlist(z)
>>>
>>>     s3d <- scatterplot3d(x = sx, y = sz, z = sy, type = "n")
>>>     for(i in rev(seq(along=d))){
>>>         s3d_coords <- s3d$xyz.convert(x[[i]], z[[i]], y[[i]])
>>>         polygon(s3d_coords, col = i, border = "black")
>>>     }
>>>
>>>      
>>>
>
>Very nice, Uwe!
>
>To make it "perfect", you'd have to add
>
>     s3d$box3d()
>
>at the end; otherwise some of the painted polygons hide lines of
>the cube box which should not be hidden.
>
>Martin Maechler
>  
>
Very nice indeed,

may i suggest some changes in the lapply calls :

     x <- lapply(d, function(dd){dd$x[c(1,1:512,512)]})
     y <- lapply(d, function(dd){c(0,dd$y,0)})
     z <- lapply(seq(0.1, 0.5, 0.1), rep, each = 514)

some densities weren't 0 at the end of the interval, so the curves 
seemed rotated. especially the red one.


Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ron.ophir at weizmann.ac.il  Wed Sep 28 11:01:30 2005
From: ron.ophir at weizmann.ac.il (Ron Ophir)
Date: Wed, 28 Sep 2005 12:01:30 +0300
Subject: [R] Dummy quesion about environment
Message-ID: <s33a8640.056@wisemail.weizmann.ac.il>

Thank you Peter,
for the comprehensive explanation. The reason I asked Does '"search" do it?' is that as I can run
ls(env=environment(h))
I can run 
ls(env=environment("package:methods"))
or ls("package:methods")
which I can see by search.
I thought maybe what I see by search is all the environments under .GobalEnv which I understan this is not what I see by search.
Thanks
Ron


>>> Peter Dalgaard <p.dalgaard at biostat.ku.dk> 09/27/05 11:49 PM >>>
"Ron Ophir" <ron.ophir at weizmann.ac.il> writes:

> Hi,
> I'm trying to understand environment object in R.
> I used the example:
>   f <- function(x) {
>          y <- 10
>          g <- function(x) x + y
>          return(g)
>      }
>      h <- f()
>      h(3)
> then i saw that f return an environment
> > h
> function(x) x + y
> <environment: 01B28570>
> but I coudn't access to x and y object in that environment:
> I tried 
> get("x",env=h)
> I tried
> h$y
> can I access y and x?

Well, there are special issues with x above, but the basic thing is to
take environment(h). Notice that h _is_ a function that _has_ an
associated environment. 

> get("y",env=environment(h))
[1] 10

As I said, x is stranger, which is because you used f() in the call:

> get("x",env=environment(h))

> str(get("x",env=environment(h)))
 symbol
> a <- get("x",env=environment(h))
> missing(a)
[1] TRUE
> evalq(x,environment(h))
Error in eval(expr, envir, enclos) : argument "x" is missing, with no
 default
> evalq(missing(x),environment(h))
[1] TRUE

You'll get the point if you look long and hard enough...

> how can I see an environment tree? 

You can't. You can see the parent of an environment, the grandparent,
etc., but there is no way to see which children a given environment
has. 

> oes search does it?

Huh?

> Thanks,
> Ron
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 
> 

-- 
   O__  ---- Peter Dalgaard             Ã˜ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From h.brunschwig at utoronto.ca  Wed Sep 28 11:13:23 2005
From: h.brunschwig at utoronto.ca (Hadassa Brunschwig)
Date: Wed, 28 Sep 2005 05:13:23 -0400
Subject: [R] [Fwd:  R2WinBUGS: Comparison to WinBUGS]
In-Reply-To: <433A50F6.1020800@statistik.uni-dortmund.de>
References: <433A4E53.6010409@statistik.uni-dortmund.de>
	<433A50F6.1020800@statistik.uni-dortmund.de>
Message-ID: <1127898803.433a5eb3111f9@webmail.utoronto.ca>

Thanks for the tip. That was already helpful. But I am still not satisfied with
the results. I now really changed n.thin to the same I had in WinBUGS. It looks
like the commands should now be the same. However, I still get differences of
0.6 in the means of interesting parameters which should not be. The plot as well
looks completely different. While in WinBUGS I get an approximately Gaussian
posterior, this is not the case in R2WinBUGS, it is rather skewed. Does anyone
know where the problem could be? As getting a Gaussian posterior is crucial for
my work, I dont really know which results i should rely on.

Thanks a lot.
-- 

Hadassa Brunschwig
Birmannsgasse 10A
CH-4055 Basel
Switzerland
Phone: +41 78 797 6065
Email: h.brunschwig at utoronto.ca



Quoting Sibylle Sturtz <sturtz at statistik.uni-dortmund.de>:

> This is due to the following:
> 
> In bugs(), the default for thinning is
> 
> n.thin = max(1, floor(n.chains * (n.iter - n.burnin)/1000))
> 
> which is 29 for n.iter=12000 and n.burnin=2001 as in your example. 
> Therefore, the number of iterations used for calculation of posterior 
> values is
> 
> (12000-2001)/29 = 344.7931
> 
> which corresponds to the number of iterations given in your plot. If you 
> specify the thinning parameter directly in bugs() it should be fine.
> 
> Sibylle
> 
> > -------- Original Message --------
> > Subject: [R] R2WinBUGS: Comparison to WinBUGS
> > Date: Wed, 28 Sep 2005 03:55:08 -0400
> > From: Hadassa Brunschwig <h.brunschwig at utoronto.ca>
> > To: r-help at stat.math.ethz.ch
> > 
> > Hi R-Help!
> > 
> > 
> > I used R2WinBUGS and WinBUGS directly on the same model just to compare.
> It
> > seems I am still making a mistake: after running the function bugs() I 
> > tried to
> > plot the posteriors of the parameters by using read.bugs() to convert 
> > the output
> > to an mcmc object and then plot.mcmc() to plot the densities. Using the 
> > same
> > model, the same number of iterations, the same initial values and the 
> > same data
> > I get completely different plots for the densities (e.g. the range of one
> > parameter in R2WinBUGS is from 0 to 8 but in WinBUGS only from 1.5 to 
> > 3)??? That
> > means my results are different, too.
> > Also, on the plot it says N=345 which is not what I specified in the
> bugs()
> > function (I specified 12000 iterations).
> > Below I put some of the code I used (if that helps):
> > 
> > parameters <-
> >
> c("tau","C0","st90","C0.pop","st90.pop","tau.cpop","tau.stpop","st90.pop80")
> 
> > 
> > inits      <- inits <- function(){
> >     list(tau = rep(1, 17),tau.cpop = 0.2, tau.stpop = 1)
> >   }
> > 
> > mcmcA      <-
> >
> bugs(dataA,inits,parameters,modelA,n.chains=3,debug=T,n.iter=12000,n.burnin=2001,
> 
> > 
> >                        bugs.directory="c:/Program
> > Files/WinBUGS14",working.directory="C:/Documents and
> > Settings/Daikon/Roche/R2WinBUGS Output",codaPkg=T)
> > 
> > codaA1      <- read.bugs(mcmcA[1])
> > plot(codaA1)
> > 
> > 
> > THANKS A LOT!!
> 
> -- 
> Dipl.-Stat. Sibylle Sturtz
> Mathematische Statistik und biometrische Anwendungen
> Fachbereich Statistik
> Universit??t Dortmund
> 44221 Dortmund
> Tel.: 0231/755 4391
> FAX : 0231/755 5303
>



From mendes150 at gmail.com  Wed Sep 28 11:30:40 2005
From: mendes150 at gmail.com (richard)
Date: Wed, 28 Sep 2005 11:30:40 +0200
Subject: [R] installation on mac os x
Message-ID: <0f80d22312c71dcf2a24ce388516d1eb@gmail.com>

hello everybody

I'm currently doing an internship where i need to build a pipeline 
between a database and R, programmed in python.
For the interface between python and R i installed the RPY package. but 
i still can't make a connection with R because on the mac os x system i 
can't install R with a shared library. does anybody know how to solve 
this problem ?

thanks in advance

richard



From p.dalgaard at biostat.ku.dk  Wed Sep 28 12:06:21 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Sep 2005 12:06:21 +0200
Subject: [R] Error in "make check-all"
In-Reply-To: <433971D5.8050005@gmail.com>
References: <4339687A.6040301@yahoo.com.br>
	<Pine.LNX.4.61.0509271653360.10946@gannet.stats>
	<433971D5.8050005@gmail.com>
Message-ID: <x2slvp4ici.fsf@viggo.kubism.ku.dk>

Fernando Mayer <fernandomayer at gmail.com> writes:

> Dear Prof. Ripley, that was exactly what i did. Should i login on the 
> system as root, instead of login as user and became root via console? 
> What should i do to have X11 usable?

I don't actually think that this is the problem. You shouldn't
generally trust me rather than Brian, but I'm sitting at a SUSE 9.3
system, and he's not... If I "su -" to root, I can happily run "xterm"
etc. from the root shell with display on the console, and also run
make check on the current r-devel. So go look for missing RPM
packages.

That said, it is not generally a good idea to build as root. I prefer
to build things as myself and only do the final install step as root
if necessary.

        -pd

(PS: I had a powercut when I was either 99.99% or 100.01% done with
writing this before. Apologies if it went out twice...) 
 
> Thanks,
> Fernando Mayer.
> 
> 
> Prof Brian Ripley escreveu:
> 
> >This is what happens if you don't have a usable X11 display.  Did you 
> >perhaps use a root account on a console owned by a normal user?
> >
> >On Tue, 27 Sep 2005, Fernando Mayer wrote:
> >
> >  
> >
> >>Dear R-users,
> >>
> >>i'm a very newbie in linux, but decided to build R from source.
> >>Following the "R Installation and Administration" manual, i did this:
> >>
> >>./configure --enable-R-shlib # this option is here because i intend to
> >>build the GNOME console after...
> >>make
> >>make check
> >>
> >>no problems in make check, but:
> >>
> >>make check-devel #and also
> >>make check-all
> >>
> >>indicated some WARNINGs in the log file:
> >>
> >>/usr/local/R-2.1.1/tests/tcltk.Rcheck/00check.Rcheck
> >>
> >>Right below is the content of this log file (I translated some words):
> >>
> >>[start log file]
> >>
> >>* using log directory '/usr/local/R-2.1.1/tests/tcltk.Rcheck'
> >>* using R version 2.1.1, 2005-06-20
> >>* looks like 'tcltk' is a base package
> >>* skipping installation test
> >>* checking package directory ... OK
> >>* checking for portable file names ... OK
> >>* checking for sufficient/correct file permissions ... OK
> >>* checking DESCRIPTION meta-information ... OK
> >>* checking package dependencies ... OK
> >>* checking index information ... OK
> >>* checking package subdirectories ... OK
> >>* checking S3 generic/method consistency ... WARNING
> >>Erro: .First.lib failed for 'tcltk'
> >>Call sequence:
> >>2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >>      domain = NA)
> >>1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> >>FALSE)
> >>Interrupted execution
> >>See section 'Generic functions and methods' of the 'Writing R Extensions'
> >>manual.
> >>* checking replacement functions ... WARNING
> >>Error: .First.lib failed for 'tcltk'
> >>Call sequence:
> >>2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >>      domain = NA)
> >>1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> >>FALSE)
> >>Interrupted execution
> >>In R, the argument of a replacement function which corresponds to the right
> >>hand side must be named 'value'.
> >>* checking foreign function calls ... WARNING
> >>Error: .First.lib failed for 'tcltk'
> >>Call sequence:
> >>2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >>      domain = NA)
> >>1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> >>FALSE)
> >>Interrupted execution
> >>See section 'System and foreign language interfaces' of the 'Writing R
> >>Extensions' manual.
> >>* checking Rd files ... OK
> >>* checking for missing documentation entries ... WARNING
> >>Error: .First.lib failed for 'tcltk'
> >>Call sequence:
> >>2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >>      domain = NA)
> >>1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> >>FALSE)
> >>Interrupted execution
> >>All user-level objects in a package should have documentation entries.
> >>See chapter 'Writing R documentation files' in manual 'Writing R
> >>Extensions'.
> >>* checking for code/documentation mismatches ... WARNING
> >>Error: .First.lib failed for 'tcltk'
> >>Call sequence:
> >>2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >>      domain = NA)
> >>1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> >>FALSE)
> >>Interrupted execution
> >>Error: .First.lib failed for 'tcltk'
> >>Call sequence:
> >>2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >>      domain = NA)
> >>1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> >>FALSE)
> >>Interrupted execution
> >>Error: .First.lib failed for 'tcltk'
> >>Call sequence:
> >>2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
> >>      domain = NA)
> >>1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> >>FALSE)
> >>Interrupted execution
> >>* checking Rd \usage sections ... OK
> >>* checking DVI version of manual ... OK
> >>
> >>[end log file]
> >>
> >>The problem is in the tcltk package. I have searched in the archives,
> >>but didin't find anything similar to this problem (maybe there is, but i
> >>was not able to identify it). I really don't know what all the warnings
> >>means. What i need to know is what am i missing, and if there is
> >>something i can fix it. Also, are there any problems if i continue the
> >>installation without fix something?
> >>
> >>P.S.: actually R is running already from where it was buit, but i want
> >>to install it. So i want to avoid future problems, if this warnings
> >>really make some.
> >>
> >>Any help is appreciated. Thanks!
> >>
> >>Fernando Mayer.
> >>
> >>version
> >>        _
> >>platform i686-pc-linux-gnu
> >>arch     i686
> >>os       linux-gnu [SuSE 9.3]
> >>system   i686, linux-gnu
> >>status
> >>major    2
> >>minor    1.1
> >>year     2005
> >>month    06
> >>day      20
> >>language R
> >>
> >>
> >>	[[alternative HTML version deleted]]
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>    
> >>
> >
> >  
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From bagchi.r at gmail.com  Wed Sep 28 13:13:56 2005
From: bagchi.r at gmail.com (Robert Bagchi)
Date: Wed, 28 Sep 2005 12:13:56 +0100
Subject: [R] anova on binomial LMER objects
In-Reply-To: <004401c5c381$faea8c40$6fee6fa0@satellitepro>
References: <004401c5c381$faea8c40$6fee6fa0@satellitepro>
Message-ID: <433A7AF4.20405@gmail.com>

Hi Patrick

thanks for your advice. I have now tried glmmPQL, and it worked fine - 
I'm getting consistent results between plots and models fitted by 
glmmPQL. Plus it allows predict() and resid() which is another advantage 
over lmer at present.

quick question though: why does one need to use PQL for binomial models? 
Is there a good reference for this?

A few of my colleagues have also had similar problems, so I'm copying 
this message on to R-help as it might be useful there.

Many thanks
Robert

Patrick A. Jansen wrote:

>
> Hi dr Bacghi,
>
> I ran into exactly the same problem with lmer models that had an 
> rbind() response variable, as you posted to the R-list.
>
> The sums of squares produced by anova() seem wrong. They are almost 
> identical to the mean squares, and hence F-values approach 1.
>
> Since you need PQL for binomial models anyway, you might want to use 
> GlmmPQL instead. Seems to work fine with anova().
>
> Best regards,
> Patrick Jansen
>
>
> *dr Patrick A. Jansen*
> University of Groningen
> Community and Conservation Ecology group
> E P.A.Jansen at .rug.nl
> W _www.rug.nl/fwn/onderzoek/programmas/biologie/cocon_ 
> <http://www.rug.nl/fwn/onderzoek/programmas/biologie/cocon>
>
> c/o:
> Instituto Smithsonian de Investigaciones Tropicales
> Att. Patrick A. Jansen – Gamboa
> Apartado 0843-03092, Balboa, Ancón, Panamá, República de Panamá
> or:
> Smithsonian Tropical Research Insititute
> Att: Patrick A. Jansen – Gamboa
> Unit 0948, APO AA, 34002-0948, U.S.A.
>
> T +507-212-8904 (office) / +507-6516-2008 (cell)
>
>



From ripley at stats.ox.ac.uk  Wed Sep 28 14:02:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Sep 2005 13:02:19 +0100 (BST)
Subject: [R] anova on binomial LMER objects
In-Reply-To: <433A7AF4.20405@gmail.com>
References: <004401c5c381$faea8c40$6fee6fa0@satellitepro>
	<433A7AF4.20405@gmail.com>
Message-ID: <Pine.LNX.4.61.0509281301330.9059@gannet.stats>

On Wed, 28 Sep 2005, Robert Bagchi wrote:

> Hi Patrick
>
> thanks for your advice. I have now tried glmmPQL, and it worked fine -
> I'm getting consistent results between plots and models fitted by
> glmmPQL. Plus it allows predict() and resid() which is another advantage
> over lmer at present.
>
> quick question though: why does one need to use PQL for binomial models?
> Is there a good reference for this?

Yes, the book which glmmPQL supports and the posting quide asks you to 
consult.

>
> A few of my colleagues have also had similar problems, so I'm copying
> this message on to R-help as it might be useful there.
>
> Many thanks
> Robert
>
> Patrick A. Jansen wrote:
>
>>
>> Hi dr Bacghi,
>>
>> I ran into exactly the same problem with lmer models that had an
>> rbind() response variable, as you posted to the R-list.
>>
>> The sums of squares produced by anova() seem wrong. They are almost
>> identical to the mean squares, and hence F-values approach 1.
>>
>> Since you need PQL for binomial models anyway, you might want to use
>> GlmmPQL instead. Seems to work fine with anova().
>>
>> Best regards,
>> Patrick Jansen
>>
>>
>> *dr Patrick A. Jansen*
>> University of Groningen
>> Community and Conservation Ecology group
>> E P.A.Jansen at .rug.nl
>> W _www.rug.nl/fwn/onderzoek/programmas/biologie/cocon_
>> <http://www.rug.nl/fwn/onderzoek/programmas/biologie/cocon>
>>
>> c/o:
>> Instituto Smithsonian de Investigaciones Tropicales
>> Att. Patrick A. Jansen  Gamboa
>> Apartado 0843-03092, Balboa, Anc?n, Panam?, Rep?blica de Panam?
>> or:
>> Smithsonian Tropical Research Insititute
>> Att: Patrick A. Jansen  Gamboa
>> Unit 0948, APO AA, 34002-0948, U.S.A.
>>
>> T +507-212-8904 (office) / +507-6516-2008 (cell)
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From chabotd at globetrotter.net  Wed Sep 28 15:30:41 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Wed, 28 Sep 2005 09:30:41 -0400
Subject: [R] p-level in packages mgcv and gam
References: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>
Message-ID: <86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>

I only got one reply to my message:

> No, this won't work.  The problem is the usual one with model  
> selection: the p-value is calculated as if the df had been fixed,  
> when really it was estimated.
>
> It is likely to be quite hard to get an honest p-value out of  
> something that does adaptive smoothing.
>
>     -thomas

I do not understand this: it seems that a lot of people chose df=4  
for no particular reason, but p-levels are correct. If instead I  
choose df=8 because a previous model has estimated this to be an  
optimal df, P-levels are no good because df are estimated?

Furthermore, shouldn't packages gam and mgcv give similar results  
when the same data and df are used? I tried this:

library(gam)
data(kyphosis)
kyp1 <- gam(Kyphosis ~ s(Age, 4), family=binomial, data=kyphosis)
kyp2 <- gam(Kyphosis ~ s(Number, 4), family=binomial, data=kyphosis)
kyp3 <- gam(Kyphosis ~ s(Start, 4), family=binomial, data=kyphosis)
anova.gam(kyp1)
anova.gam(kyp2)
anova.gam(kyp3)

detach(package:gam)
library(mgcv)
kyp4 <- gam(Kyphosis ~ s(Age, k=4, fx=T),  family=binomial,  
data=kyphosis)
kyp5 <- gam(Kyphosis ~ s(Number, k=4, fx=T),  family=binomial,  
data=kyphosis)
kyp6 <- gam(Kyphosis ~ s(Start, k=4, fx=T),  family=binomial,  
data=kyphosis)
anova.gam(kyp4)
anova.gam(kyp5)
anova.gam(kyp6)


P levels for these models, by pair

kyp1 vs kyp4: p= 0.083 and 0.068 respectively (not too bad)
kyp2 vs kyp5: p= 0.445 and 0.03 (wow!)
kyp3 vs kyp6: p= 0.053 and 0.008 (wow again)

Also if you plot all these you find that the mgcv plots are smoother  
than the gam plots, even the same df are used all the time.

I am really confused now!

Denis

D??but du message r??exp??di?? :

>> De : Denis Chabot <chabotd at globetrotter.net>
>> Date : 26 septembre 2005 12:25:04 HAE
>> ?? : r-help at stat.math.ethz.ch
>> Objet : p-level in packages mgcv and gam
>>
>>
>> Hi,
>>
>> I am fairly new to GAM and started using package mgcv. I like the  
>> fact that optimal smoothing is automatically used (i.e. df are not  
>> determined a priori but calculated by the gam procedure).
>>
>> But the mgcv manual warns that p-level for the smooth can be  
>> underestimated when df are estimated by the model. Most of the  
>> time my p-levels are so small that even doubling them would not  
>> result in a value close to the P=0.05 threshold, but I have one  
>> case with P=0.033.
>>
>> I thought, probably naively, that running a second model with  
>> fixed df, using the value of df found in the first model. I could  
>> not achieve this with mgcv: its gam function does not seem to  
>> accept fractional values of df (in my case 8.377).
>>
>> So I used the gam package and fixed df to 8.377. The P-value I  
>> obtained was slightly larger than with mgcv (0.03655 instead of  
>> 0.03328), but it is still < 0.05.
>>
>> Was this a correct way to get around the "underestimated P-level"?
>>
>> Furthermore, although the gam.check function of the mgcv package  
>> suggests to me that the gaussian family (and identity link) are  
>> adequate for my data, I must say the instructions in R help for  
>> "family" and in Hastie, T. and Tibshirani, R. (1990) Generalized  
>> Additive Models are too technical for me. If someone knows a  
>> reference that explains how to choose model and link, i.e. what  
>> tests to run on your data before choosing, I would really  
>> appreciate it.
>>
>> Thanks in advance,
>>
>> Denis Chabot
>>
>



From ajayshah at mayin.org  Wed Sep 28 15:41:13 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Wed, 28 Sep 2005 19:11:13 +0530
Subject: [R] Question on lm(): When does R-squared come out as NA?
In-Reply-To: <Pine.LNX.4.61.0509280822440.27891@gannet.stats>
References: <20050925144109.GT437@lubyanka.local>
	<Pine.LNX.4.61.0509280822440.27891@gannet.stats>
Message-ID: <20050928134113.GE437@lubyanka.local>

On Wed, Sep 28, 2005 at 08:23:59AM +0100, Prof Brian Ripley wrote:
> I've not seen a reply to this, nor ever seen it.
> Please make a reproducible example available (do see the posting guide).

It was a mistake on my part. Just in case others are able to
recognise the situation, what was going on was that all the objects
being used in the lm() call were zoo objects.

It is a mystery to me as to why everything should work correctly but
the R2 should break, but that happened. I found that when I switched
to coredata(z) all was well.

Gabor reminded me that I should really be using his dyn package so as
to avoid such situations. Sorry for the false alarm,

   -ans.

> >lm(formula = rj ~ rM + rM.1 + rM.2 + rM.3 + rM.4)
> >
> >Residuals:
> >1990-06-04 1994-11-14 1998-08-21 2002-03-13 2005-09-15
> > -5.64672   -0.59596   -0.04143    0.55412    8.18229
> >
> >Coefficients:
> >            Estimate Std. Error t value Pr(>|t|)
> >(Intercept) -0.003297   0.017603  -0.187    0.851
> >rM           0.845169   0.010522  80.322   <2e-16 ***
> >rM.1         0.116330   0.010692  10.880   <2e-16 ***
> >rM.2         0.002044   0.010686   0.191    0.848
> >rM.3         0.013181   0.010692   1.233    0.218
> >rM.4         0.009587   0.010525   0.911    0.362
> >---
> >Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> >Residual standard error: 1.044 on 3532 degrees of freedom
> >Multiple R-Squared:    NA,	Adjusted R-squared:    NA
> >F-statistic:    NA on 5 and 3532 DF,  p-value: NA
> >
> >
> >rM.1, rM.2, etc. are lagged values of rM. The OLS seems fine in every
> >respect, except that there is an NA as the multiple R-squared. I will
> >be happy to give sample data to someone curious about what is going
> >on. I wondered if this was a well-known pathology. The way I know it,
> >if the data allows computation of (X'X)^{-1}, one can compute the R2.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From karin.lagesen at medisin.uio.no  Wed Sep 28 15:50:06 2005
From: karin.lagesen at medisin.uio.no (Karin Lagesen)
Date: Wed, 28 Sep 2005 15:50:06 +0200
Subject: [R] boxplot and xlim confusion?
Message-ID: <ypx6zmpxjo8h.fsf@uracil.uio.no>


I have some code as shown below. Basically, I would like three
boxplots to be set next to each other with no ylabels on the two
"inner" plots, and I want the same x axis range on all three. However,
it seems like boxplot does not respect the xlim setting. I've tried
the various ways I thought would work (par, boxplot(...xlim=)) but
none of them seem to work. I then tried plot.window, that did not work.

I also have another curious question for you. With the code below I
tried to call plot.new and then plot.window before each new plot. What
happens then is that the first figure goes on one page whereas the two
others get put on the next page with a nice big gap in the
middle. Does any of you have an explanation for that?

names <- c(
"LSU>, stop",
">LSU, start",
"SSU>, stop",
">SSU, start",
"TSU>, stop",
">TSU, start")
elsustop <- read.table("28s.euk.accuracy.stop.dev")

[skipped lots of read.table, which just reads files with one number on
each line]

par(mfcol=c(1,3))
par(mai = c(0,0,0.5,0.2), omi = c(1,1,1,1))
xaxis = c(-6000,1000)
yaxis = c(0,7)
#plot.new()
#plot.window(xlim=xaxis, ylim=yaxis)
boxplot(alsustop$V1 ,alsustart$V1 ,assustop$V1 ,alsustart$V1 ,atsustop$V1 ,atsustart$V1 ,names=names,col=c("lightblue","orange","lightblue","orange","lightblue","orange") ,horizontal = TRUE, main="ARC", xaxs = "i", las=1)
#plot.new()
#plot.window(xlim=xaxis, ylim=yaxis)
boxplot(blsustop$V1 ,blsustart$V1 ,bssustop$V1 ,blsustart$V1 ,btsustop$V1 ,btsustart$V1 ,col=c("lightblue","orange","lightblue","orange","lightblue","orange") ,horizontal = TRUE, main="BAC", xaxs = "i")
#plot.new()
#plot.window(xlim=xaxis, ylim=yaxis)
boxplot(elsustop$V1 ,elsustart$V1 ,essustop$V1 ,elsustart$V1 ,etsustop$V1 ,etsustart$V1 ,col=c("lightblue","orange","lightblue","orange","lightblue","orange") ,horizontal = TRUE, main="EUK", xaxs = "i")

Karin (sorry if you're getting sick of me..:))
-- 
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://www.cmbn.no/rognes/



From loferre at univ-tlse2.fr  Tue Sep 27 14:44:57 2005
From: loferre at univ-tlse2.fr (Louis Ferre)
Date: Tue, 27 Sep 2005 14:44:57 +0200
Subject: [R] Random Forest with R
Message-ID: <008c01c5c361$4f6f5eb0$76ae1fac@iunivtlse2.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050927/052ae86e/attachment.pl

From prokopczuk at uni-mannheim.de  Wed Sep 28 15:36:25 2005
From: prokopczuk at uni-mannheim.de (Marcel Prokopczuk)
Date: Wed, 28 Sep 2005 15:36:25 +0200
Subject: [R] multidimensional integration
Message-ID: <200509281336.j8SDaSAT029890@rumms.uni-mannheim.de>

dear all,

i have the following problem: i want to integrate a two-dimensional
function. unfortunately R crashes when i try to use adapt() and i get a nice
windows message with some hex-code.
do anybody of you knows how to avoid this or knows another more stable
function than adapt()

thanks a lot

marcel



From gaelger at aept.ruhr-uni-bochum.de  Wed Sep 28 15:58:12 2005
From: gaelger at aept.ruhr-uni-bochum.de (=?ISO-8859-1?Q?Michael_G=E4lger?=)
Date: Wed, 28 Sep 2005 15:58:12 +0200
Subject: [R] confidence variability bands for kernel estimators
Message-ID: <6367eedd84a0ab0695acbf3851e9a799@egw.aept.rub.de>

I'm using nonparametric regression (packeges ksmooth and ks). My question:
is there any way to compute confidence bands (or variability bands) with R.
Confidence bands for functions are intervals [CLO(x);CUP(x)] such that with
probability 1-alpha the true curve is covered by the band [CLO(x);CUP(x)].

Thanks very much for any help you can offer. 

Michael G??lger



From dimitris.rizopoulos at med.kuleuven.be  Tue Sep 27 10:56:13 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 27 Sep 2005 10:56:13 +0200
Subject: [R] [R-pkgs] package 'ltm' -- version: 0.3-0
Message-ID: <00cd01c5c341$50444870$0540210a@www.domain>

Dear R users,

I'd like to announce the new version of the package "ltm" (available 
from CRAN), for fitting Latent Trait Models (including the Rasch and 
two-parameter logistic models) under the Item Response Theory 
approach. Three main extra features have been added: (i) now both 
ltm() and rasch() permit general fixed-value constraints (e.g., useful 
for scaling purposes), (ii) there is the option to report the 
estimated parameters under the usual IRT parameterization, and (iii) 
both plot.ltm() and plot.rasch() are now more flexible. For info about 
other new features check the help and CHANGES files. Future plans 
include development of functions for fitting the three-parameter 
logistic and the graded response models. Any kind of feedback 
(questions, suggestions, bug-reports, etc.) is more than welcome.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From sfalcon at fhcrc.org  Wed Sep 28 16:11:58 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 28 Sep 2005 07:11:58 -0700
Subject: [R] installation on mac os x
In-Reply-To: <0f80d22312c71dcf2a24ce388516d1eb@gmail.com> (richard's message
	of "Wed, 28 Sep 2005 11:30:40 +0200")
References: <0f80d22312c71dcf2a24ce388516d1eb@gmail.com>
Message-ID: <m28xxhmgcx.fsf@macaroni.local>

On 28 Sep 2005, mendes150 at gmail.com wrote:

> hello everybody
>
> I'm currently doing an internship where i need to build a pipeline
> between a database and R, programmed in python.  For the interface
> between python and R i installed the RPY package. but i still can't
> make a connection with R because on the mac os x system i can't
> install R with a shared library. does anybody know how to solve this
> problem ?

I think you will need to build R from source.  The r-sig-mac email
list would be a better place for this type of question.

+ seth



From Matthias.Templ at statistik.gv.at  Wed Sep 28 16:14:41 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 28 Sep 2005 16:14:41 +0200
Subject: [R] Random Forest with R
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAC22@xchg1.statistik.local>

Have a look at
http://cran.r-project.org/search.html 

or 

http://cran.at.r-project.org/src/contrib/PACKAGES.html

--> randomForest

Best,
Matthias


> Hi,
> what is the name of the package that provides Random Forest 
> with R. Sincerely Louis Ferr?? 
> http://www.univ-tlse2.fr/grimm/smash/ferre/index.html
> Equipe GRIMM-2254
> D??partement de Math-Info
> 5 all??es Antonio Machado
> 31058 Toulouse Cedex
> Tel: 0561504608
>        0561503982
> 
> 	[[alternative HTML version deleted]]
>



From ligges at statistik.uni-dortmund.de  Wed Sep 28 16:20:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 28 Sep 2005 16:20:39 +0200
Subject: [R] Random Forest with R
In-Reply-To: <008c01c5c361$4f6f5eb0$76ae1fac@iunivtlse2.fr>
References: <008c01c5c361$4f6f5eb0$76ae1fac@iunivtlse2.fr>
Message-ID: <433AA6B7.1030909@statistik.uni-dortmund.de>

Louis Ferre wrote:

> Hi,
> what is the name of the package that provides Random Forest with R.

Really, what about looking yourself on CRAN or just googling for it?
Most surprisingly the name of the package is "randomForest".

Uwe Ligges


> Sincerely
> Louis Ferr??
> http://www.univ-tlse2.fr/grimm/smash/ferre/index.html
> Equipe GRIMM-2254
> D??partement de Math-Info
> 5 all??es Antonio Machado
> 31058 Toulouse Cedex
> Tel: 0561504608
>        0561503982
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed Sep 28 16:24:19 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 28 Sep 2005 09:24:19 -0500
Subject: [R] Install and load packages
In-Reply-To: <20050924211742.13800.qmail@web34009.mail.mud.yahoo.com>
References: <20050924211742.13800.qmail@web34009.mail.mud.yahoo.com>
Message-ID: <433AA793.5080407@pdf.com>

	  In Rgui 2.1.1 patched, the fourth drop-down menu after "File", "Edit" 
and "Misc" is "Packages".  The last item on that menu is "Install 
package(s) from local zip files...".  Alternatively, 
RSiteSearch("install from a local zip file") produced 172 hits, some of 
which described other ways to do this.

	  spencer graves

Caio Lucidius Naberezny Azevedo wrote:
> Dear R-users,
>  
> I would like to know what are the commands to install (from a local zip file) a package and then to load it.
>  
> Thaks all,
>  
> Bests,
>  
> Caio
> 
> 
>  
> 
> 
> 
> "Perco a consciencia, mas n??o importa, encontro a maior serenidade na alucina??ao. ?? curioso como n??o sei dizer quem sou. Quer dizer, sei-o bem, mas n??o posso dizer. Sobretudo tenho medo de dizer, porque no momento em que tento falar, n??o s?? n??o exprimo o que sinto como o que sinto se transforma lentamente no que eu digo." 
> 
> - Perto do cora??ao selvagem, Clarice Lispector 
> 
> "Quando voc?? n??o conseguir olhar dentro da alma de algu??m, se afaste. V?? embora e depois de um tempo volte."
> 
> - Boris Pasternak
> 
> 
> /######################################################\
>  ********* Caio Lucidius Naberezny Azevedo ************
>  ********* Estudante de Doutorado - IME-USP ***********
>  ******* Orientador : Prof. Dr. Dalton Andrade ********
>  *** ??rea de Pesquisa : Modelos de Resposta ao Item ***
> \######################################################/
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From matthew_wiener at merck.com  Wed Sep 28 16:26:26 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 28 Sep 2005 10:26:26 -0400
Subject: [R] Random Forest with R
Message-ID: <4E9A692D8755DF478B56A2892388EE1F06B7CE@usctmx1118.merck.com>

It's randomForest.

Searching (simple text find) on the packages web page of CRAN using either
"random" or "forest" would find you this.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Louis Ferre
Sent: Tuesday, September 27, 2005 8:45 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Random Forest with R


Hi,
what is the name of the package that provides Random Forest with R.
Sincerely
Louis Ferr??
http://www.univ-tlse2.fr/grimm/smash/ferre/index.html
Equipe GRIMM-2254
D??partement de Math-Info
5 all??es Antonio Machado
31058 Toulouse Cedex
Tel: 0561504608
       0561503982

	[[alternative HTML version deleted]]



From viudez_ant at gva.es  Wed Sep 28 16:50:47 2005
From: viudez_ant at gva.es (Toni =?iso-8859-1?q?Vi=FAdez?=)
Date: Wed, 28 Sep 2005 16:50:47 +0200
Subject: [R] Access to particular predict value
Message-ID: <200509281650.47447.viudez_ant@gva.es>

Hi everybody:

I just generate interpolation maps with differents methods, like IDW and 
kriging, and now i want to compare the predict values versus real, and i 
don't who is the commant to do it. I want for example, if I pass from console 
the coordinates of a place, return me the predict value.
Could anybody tell me how should do?.

Thanks in advance
-- 
########################################
	   Antoni Vi??dez Mora	
    Dept. Din??mica de Contaminantes
	    Fundaci??n CEAM
        Paterna (Valencia)-Spain
        tel: 961318190. ext: 216
 e-mail: toni at ceam.es  viudez_ant at gva.es
########################################



From macq at llnl.gov  Wed Sep 28 16:47:51 2005
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 28 Sep 2005 07:47:51 -0700
Subject: [R] graphics guide?
In-Reply-To: <ypx6achyljy6.fsf@uracil.uio.no>
References: <ypx6achyljy6.fsf@uracil.uio.no>
Message-ID: <p06210202bf605cc27781@[128.115.153.6]>

At the R website, CRAN, in the Manuals section, you can download the 
document titled "An Introduction to R", which contains a substantial 
section on the basics of R graphics.

-Don

At 3:27 PM +0200 9/27/05, Karin Lagesen wrote:
>I am trying to create some graphs with R and it seems to be able to do
>what I need. However, I have so far not been able to find any sort of
>explanation of how the graphics system works. I am for instance trying
>to create a multiple figure, and I seem to have to call plot.new()
>before every new plot command, I have however not found any
>explanation of what this actually does. ?plot.new does give an
>explanation, but it is explained in R, so to speak. Where do I find
>out what for instance a graphics frame is?
>
>Thanks,
>
>Karin
>--
>Karin Lagesen, PhD student
>karin.lagesen at medisin.uio.no
>http://www.cmbn.no/rognes/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From andy_liaw at merck.com  Wed Sep 28 16:57:54 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 28 Sep 2005 10:57:54 -0400
Subject: [R] confidence variability bands for kernel estimators
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED4AE@usctmx1106.merck.com>

The `sm' package has functionality to produce pointwise band.  The `locfit'
package can produce simultaneous band.  Both are support software for books,
where you'll find more detail.

Andy

> From: Michael G??lger
> 
> I'm using nonparametric regression (packeges ksmooth and ks). 
> My question:
> is there any way to compute confidence bands (or variability 
> bands) with R.
> Confidence bands for functions are intervals [CLO(x);CUP(x)] 
> such that with
> probability 1-alpha the true curve is covered by the band 
> [CLO(x);CUP(x)].
> 
> Thanks very much for any help you can offer. 
> 
> Michael G??lger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From sato_t1975 at yahoo.co.jp  Wed Sep 28 17:03:44 2005
From: sato_t1975 at yahoo.co.jp (=?ISO-2022-JP?B?GyRCRURDZhsoQiAbJEJBbxsoQg==?=)
Date: Thu, 29 Sep 2005 00:03:44 +0900 (JST)
Subject: [R] Errors in odbcConnectExcel()
Message-ID: <20050928150344.56436.qmail@web3113.mail.bbt.yahoo.co.jp>

Dear R-help

I would like to read Excel Spreadsheets using
odbcConnectExcel()
in RODBC, but data in the first row can not be read.
For example, I tried to read Excel file 'Book1.xls' in the

current Work Directory with the following data
(Range("A1:B5") in Excel),
1 19
2 27
3 61
4 76
5 98

My commands and the result are as follows.

> library(RODBC)
> Book1 <- odbcConnectExcel("Book1.xls")
> X <- sqlQuery(Book1, "select * from [Sheet1$]")
> X
  F1 F2
1  2 27
2  3 61
3  4 76
4  5 98

You can easily see that first line has gone. 
I also changed range to Range("A2:B6") in Excel,
but the result did not change.
What is the problem about my procedure?

OS:Windows XP
R :Version 2.1.1

Sincerely yours.

Satoshi
sato_t1975 at yahoo.co.jp 

--------------------------------------
Know more about Breast Cancer



From ym at climpact.com  Wed Sep 28 17:05:49 2005
From: ym at climpact.com (Yves Magliulo)
Date: 28 Sep 2005 17:05:49 +0200
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>
References: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>
	<86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>
Message-ID: <1127919949.6577.97.camel@new-york.climpact.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050928/a94bf6c1/attachment.pl

From ripley at stats.ox.ac.uk  Wed Sep 28 17:23:47 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Sep 2005 16:23:47 +0100 (BST)
Subject: [R] Errors in odbcConnectExcel()
In-Reply-To: <20050928150344.56436.qmail@web3113.mail.bbt.yahoo.co.jp>
References: <20050928150344.56436.qmail@web3113.mail.bbt.yahoo.co.jp>
Message-ID: <Pine.LNX.4.61.0509281621210.1545@gannet.stats>

This is entirely a function of the database format the ODBC Excel driver 
expects.  It is *not* an error in odbcConnectExcel as your subject 
accuses.

To use a spreadsheet as a database you need to have column names.

On Thu, 29 Sep 2005, [ISO-2022-JP] ???? ?? wrote:

> Dear R-help
>
> I would like to read Excel Spreadsheets using
> odbcConnectExcel()
> in RODBC, but data in the first row can not be read.
> For example, I tried to read Excel file 'Book1.xls' in the
>
> current Work Directory with the following data
> (Range("A1:B5") in Excel),
> 1 19
> 2 27
> 3 61
> 4 76
> 5 98
>
> My commands and the result are as follows.
>
>> library(RODBC)
>> Book1 <- odbcConnectExcel("Book1.xls")
>> X <- sqlQuery(Book1, "select * from [Sheet1$]")
>> X
>  F1 F2
> 1  2 27
> 2  3 61
> 3  4 76
> 4  5 98
>
> You can easily see that first line has gone.
> I also changed range to Range("A2:B6") in Excel,
> but the result did not change.
> What is the problem about my procedure?
>
> OS:Windows XP
> R :Version 2.1.1
>
> Sincerely yours.
>
> Satoshi
> sato_t1975 at yahoo.co.jp
>
> --------------------------------------
> Know more about Breast Cancer
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From chabotd at globetrotter.net  Wed Sep 28 17:17:37 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Wed, 28 Sep 2005 11:17:37 -0400
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <1127919949.6577.97.camel@new-york.climpact.net>
References: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>
	<86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>
	<1127919949.6577.97.camel@new-york.climpact.net>
Message-ID: <BDD4B204-8F6F-4143-B00A-2541CB5D3BC1@globetrotter.net>

Hi Yves,
Le 05-09-28 ?? 11:05, Yves Magliulo a ??crit :

> hi,
>
> i'll try to help you, i send a mail about this subject last week...  
> and i did not have any response...
Sorry, I did not see your message last week.
>
> I'm using gam from package mgcv.
>
> 1)
> How to interpret the significance of smooth terms is hard for me to  
> understand perfectly :
> using UBRE, you fix df. p-value are estimated by chi-sq distribution
> using GCV, the best df are estimated by GAM. (that's what i want)  
> and p-values
> are estimated by an F distribution But in that case they said "use  
> at your own risk" in ?summary.gam
>
> so you can also look at the chi.sq : but i don't know how to choose  
> a criterion like for p-values... for me, chi.sq show the best  
> predictor in a model, but it's hard to reject one with it.
>
> so as far as i m concerned, i use GCV methods, and fix a 5% on the  
> null hypothesis (pvalue) to select significant predictor. after, i  
> look at my smooth, and if the parametrization look fine to me, i  
> validate.
>
> generaly, for p-values smaller than 0.001, you can be confident.  
> over 0.001, you have to check.
>
I think I follow you, but how do you "validate"? My fit goes very  
nicely in the middle of the data points and appears fine. In most  
cases p is way smaller than 0.001. I have one case that is bimodal in  
shape and more noisy, and p is only 0.03. How do I validate it, how  
do I check?

> 2)
> for difference between package gam and mgcv, i sent a mail about  
> this one year ago, here's the response :
>
> "
> - package gam is based very closely on the GAM approach presented in
> Hastie and Tibshirani's  "Generalized Additive Models" book.  
> Estimation is
> by back-fitting and model selection is based on step-wise regression
> methods based on approximate distributional results. A particular  
> strength
> of this approach is that local regression smoothers (`lo()' terms)  
> can be
> included in GAM models.
>
> - gam in package mgcv represents GAMs using penalized regression  
> splines.
> Estimation is by direct penalized likelihood maximization with
> integrated smoothness estimation via GCV or related criteria (there is
> also an alternative `gamm' function based on a mixed model approach).
> Strengths of the this approach are that s() terms can be functions  
> of more
> than one variable and that tensor product smooths are available via  
> te()
> terms - these are useful when different degrees of smoothness are
> appropriate relative to different arguments of a smooth.
>
> (...)
>
> Basically, if you want integrated smoothness selection, an underlying
> parametric representation, or want smooth interactions in your models
> then mgcv is probably worth a try (but I would say that). If you  
> want to
> use local regression smoothers and/or prefer the stepwise selection
> approach then package gam is for you.
> "
>
It is hard to evaluate the explanations based on the algorithm used  
to fit the data, but it seems to me that the answer, in terms of  
significance of the smooth, should be at least very similar.  
Otherwise, what do you do when an author cites one package? You  
wonder if the fit would have been significant using the other package?

> i think the difference of p-values between :gam and :mgcv, is  
> because you don't have same number of step iteration. mgcv : gam  
> choose the number of step and with gam : gam you have to choose it..
>
> hope it helps and someone gives us more details...
>
> Yves
Again, I can see that the p-values could differ a bit considering the  
differences between the 2 packages. But when the differences are huge  
and result in contradictory conclusions, I have a problem. Like you I  
hope more help is forthcoming.

Denis



From tlumley at u.washington.edu  Wed Sep 28 17:33:27 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 28 Sep 2005 08:33:27 -0700 (PDT)
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>
References: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>
	<86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>
Message-ID: <Pine.LNX.4.63a.0509280823360.745@homer23.u.washington.edu>

On Wed, 28 Sep 2005, Denis Chabot wrote:

> I only got one reply to my message:
>
>> No, this won't work.  The problem is the usual one with model
>> selection: the p-value is calculated as if the df had been fixed,
>> when really it was estimated.
>>
>> It is likely to be quite hard to get an honest p-value out of
>> something that does adaptive smoothing.
>>
>>     -thomas
>
> I do not understand this: it seems that a lot of people chose df=4
> for no particular reason, but p-levels are correct. If instead I
> choose df=8 because a previous model has estimated this to be an
> optimal df, P-levels are no good because df are estimated?

Yes. I know this sounds strange initially, but it really does make sense 
if you think about it carefully.

Suppose that Alice and Bob are kyphosis researchers, and that Alice always 
chooses 4df for smoothing Age.  We would all agree that her p-values are 
correct [in fact we wouldn't, but that is a separate issue]

Bob, on the other hand, chooses the amount of smoothing depending on the 
data. When a 4 df smooth fits best he ends up with the same model as Alice 
and the same p-value.  When some other df fits best he ends up with a 
different model and a *smaller* p-value than Alice.

In particular, this is still true under the null hypothesis that Age has 
no effect [If Alice and Bob are interested in p-values, the null 
hypothesis must be plausible.]

If Bob's p-values are always less than or equal to Alice's p-values under 
the null hypothesis, and Alice's p-values are less than 0.05 5% of the 
time, then Bob's p-values are less than 0.05 more than 5% of the time.


 	-thomas


> Furthermore, shouldn't packages gam and mgcv give similar results
> when the same data and df are used? I tried this:
>
> library(gam)
> data(kyphosis)
> kyp1 <- gam(Kyphosis ~ s(Age, 4), family=binomial, data=kyphosis)
> kyp2 <- gam(Kyphosis ~ s(Number, 4), family=binomial, data=kyphosis)
> kyp3 <- gam(Kyphosis ~ s(Start, 4), family=binomial, data=kyphosis)
> anova.gam(kyp1)
> anova.gam(kyp2)
> anova.gam(kyp3)
>
> detach(package:gam)
> library(mgcv)
> kyp4 <- gam(Kyphosis ~ s(Age, k=4, fx=T),  family=binomial,
> data=kyphosis)
> kyp5 <- gam(Kyphosis ~ s(Number, k=4, fx=T),  family=binomial,
> data=kyphosis)
> kyp6 <- gam(Kyphosis ~ s(Start, k=4, fx=T),  family=binomial,
> data=kyphosis)
> anova.gam(kyp4)
> anova.gam(kyp5)
> anova.gam(kyp6)
>
>
> P levels for these models, by pair
>
> kyp1 vs kyp4: p= 0.083 and 0.068 respectively (not too bad)
> kyp2 vs kyp5: p= 0.445 and 0.03 (wow!)
> kyp3 vs kyp6: p= 0.053 and 0.008 (wow again)
>
> Also if you plot all these you find that the mgcv plots are smoother
> than the gam plots, even the same df are used all the time.
>
> I am really confused now!
>
> Denis
>
> D?but du message r?exp?di? :
>
>>> De : Denis Chabot <chabotd at globetrotter.net>
>>> Date : 26 septembre 2005 12:25:04 HAE
>>> ? : r-help at stat.math.ethz.ch
>>> Objet : p-level in packages mgcv and gam
>>>
>>>
>>> Hi,
>>>
>>> I am fairly new to GAM and started using package mgcv. I like the
>>> fact that optimal smoothing is automatically used (i.e. df are not
>>> determined a priori but calculated by the gam procedure).
>>>
>>> But the mgcv manual warns that p-level for the smooth can be
>>> underestimated when df are estimated by the model. Most of the
>>> time my p-levels are so small that even doubling them would not
>>> result in a value close to the P=0.05 threshold, but I have one
>>> case with P=0.033.
>>>
>>> I thought, probably naively, that running a second model with
>>> fixed df, using the value of df found in the first model. I could
>>> not achieve this with mgcv: its gam function does not seem to
>>> accept fractional values of df (in my case 8.377).
>>>
>>> So I used the gam package and fixed df to 8.377. The P-value I
>>> obtained was slightly larger than with mgcv (0.03655 instead of
>>> 0.03328), but it is still < 0.05.
>>>
>>> Was this a correct way to get around the "underestimated P-level"?
>>>
>>> Furthermore, although the gam.check function of the mgcv package
>>> suggests to me that the gaussian family (and identity link) are
>>> adequate for my data, I must say the instructions in R help for
>>> "family" and in Hastie, T. and Tibshirani, R. (1990) Generalized
>>> Additive Models are too technical for me. If someone knows a
>>> reference that explains how to choose model and link, i.e. what
>>> tests to run on your data before choosing, I would really
>>> appreciate it.
>>>
>>> Thanks in advance,
>>>
>>> Denis Chabot
>>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From snunes at gmail.com  Wed Sep 28 12:09:25 2005
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Wed, 28 Sep 2005 11:09:25 +0100
Subject: [R] Change console language ?
Message-ID: <4c817d5305092803093c3d06e5@mail.gmail.com>

Hi,

I'm pretty new to R, I've just installed version 2.1.1 on Windows.
I have a simple (it seems) doubt - how do I change the Console default
Language ?

It's set to Portuguese but I would like to view it in English.

Thanks in advance,
S??rgio Nunes



From p.dalgaard at biostat.ku.dk  Wed Sep 28 18:04:58 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Sep 2005 18:04:58 +0200
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <Pine.LNX.4.63a.0509280823360.745@homer23.u.washington.edu>
References: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>
	<86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>
	<Pine.LNX.4.63a.0509280823360.745@homer23.u.washington.edu>
Message-ID: <x2fyrp41qt.fsf@viggo.kubism.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> Bob, on the other hand, chooses the amount of smoothing depending on
> the data. When a 4 df smooth fits best he ends up with the same model
> as Alice and the same p-value.  When some other df fits best he ends
> up with a different model and a *smaller* p-value than Alice.

This doesn't actually follow, unless the p-value (directly or
indirectly) found its way into the definition of "best fit". It does
show the danger, though.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Sep 28 18:05:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Sep 2005 17:05:54 +0100 (BST)
Subject: [R] Change console language ?
In-Reply-To: <4c817d5305092803093c3d06e5@mail.gmail.com>
References: <4c817d5305092803093c3d06e5@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0509281658070.2028@gannet.stats>

On Wed, 28 Sep 2005, S?rgio Nunes wrote:

> Hi,
>
> I'm pretty new to R, I've just installed version 2.1.1 on Windows.
> I have a simple (it seems) doubt - how do I change the Console default
> Language ?
>
> It's set to Portuguese but I would like to view it in English.


Do read the administration manual (on the help menu, via HTML help).
Or, from the rw-FAQ in R-2.2.0-beta

3.2 I want R in English (and not in French/Chinese/...)!
========================================================

The default behaviour of R is to try to run in the language you run
Windows in.  Apparently some users want Windows in their native
language, but not R.  To do so, set `LANGUAGE=en' as discussed in Q2.2
and Q2.15.

You have not even told us what locale you are in, but I would be suprised 
if this was Portugal and not Brazil.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From dsonneborn at ucdavis.edu  Wed Sep 28 18:06:57 2005
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Wed, 28 Sep 2005 09:06:57 -0700
Subject: [R] gee models summary
Message-ID: <433ABFA1.8040002@yellow.ucdavis.edu>

I'm running some GEE models but when I request the summary(pcb.gee) all 
I get are rows and rows of intercorelations and they fill up the screen 
buffer so I can not even scroll back to see what else might be in the 
summary. How do I get the summary function to NOT print the 
intercorrelations?
Thanks,

-- 
Dean Sonneborn
Programmer Analyst
Department of Public Health Sciences
University of California, Davis
(916) 734-6656



From fernandomayer at gmail.com  Wed Sep 28 18:19:03 2005
From: fernandomayer at gmail.com (Fernando Mayer)
Date: Wed, 28 Sep 2005 13:19:03 -0300
Subject: [R] Error in "make check-all"
In-Reply-To: <x2slvp4ici.fsf@viggo.kubism.ku.dk>
References: <4339687A.6040301@yahoo.com.br>	<Pine.LNX.4.61.0509271653360.10946@gannet.stats>	<433971D5.8050005@gmail.com>
	<x2slvp4ici.fsf@viggo.kubism.ku.dk>
Message-ID: <433AC277.9010907@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050928/8a36887a/attachment.pl

From renaud.lancelot at cirad.fr  Wed Sep 28 18:36:41 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Wed, 28 Sep 2005 18:36:41 +0200
Subject: [R] gee models summary
In-Reply-To: <433ABFA1.8040002@yellow.ucdavis.edu>
References: <433ABFA1.8040002@yellow.ucdavis.edu>
Message-ID: <433AC699.4050005@cirad.fr>

This inconvenience does not occur with function geese in package 
geepack. If you're using function gee in package gee, extract the 
components you want from the fitted object:

 > library(gee)
 > library(MASS)
 > data(OME)
 > fm <- gee(cbind(Correct, Trials-Correct) ~ Loud + Age + OME, id = ID,
+            data = OME, family = binomial, corstr = "exchangeable")
[1] "Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27"
[1] "running glm to get initial regression estimate"
[1] -5.90984676  0.15516993  0.01876291 -0.07569691 -0.31660542

 > summ.fm <- summary(fm)
 > str(summ.fm)
List of 11
  $ call               : language gee(formula = cbind(Correct, Trials - 
Correct) ~ Loud + Age +      OME, id = ID, data = OME, family = 
binomial, corstr = "exchangeable")
  $ version            : chr "gee S-function, version 4.13 modified 
98/01/27 (1998)"
  $ nobs               : int 1097
  $ residual.summary   : Named num [1:5] -0.959  0.551  2.041  3.121 12.024
   ..- attr(*, "names")= chr [1:5] "Min" "1Q" "Median" "3Q" ...
  $ model              :List of 3
   ..$ link  : chr "Logit"
   ..$ varfun: chr "Binomial"
   ..$ corstr: chr "Exchangeable"
  $ title              : chr "GEE:  GENERALIZED LINEAR MODELS FOR 
DEPENDENT DATA"
  $ coefficients       : num [1:5, 1:5] -5.9010  0.1551  0.0185 -0.0418 
-0.2856 ...
   ..- attr(*, "dimnames")=List of 2
   .. ..$ : chr [1:5] "(Intercept)" "Loud" "Age" "OMEhigh" ...
   .. ..$ : chr [1:5] "Estimate" "Naive S.E." "Naive z" "Robust S.E." ...
  $ working.correlation: num [1:30, 1:30]  1.0000 -0.0234 -0.0234 
-0.0234 -0.0234 ...
  $ scale              : num 1.25
  $ error              : chr "Error code was 0"
  $ iterations         : int 4
  - attr(*, "class")= chr "summary.gee"

 > summ.fm$coefficients
                Estimate  Naive S.E.     Naive z Robust S.E.    Robust z
(Intercept) -5.90100336 0.336455218 -17.5387482 0.231463368 -25.4943295
Loud         0.15507565 0.007513314  20.6401142 0.005321880  29.1392592
Age          0.01851319 0.003561640   5.1979385 0.003305525   5.6006793
OMEhigh     -0.04183516 0.160288285  -0.2609995 0.152182352  -0.2749015
OMElow      -0.28563252 0.131496124  -2.1721745 0.117510247  -2.4307031

or better:

 > coef(summ.fm)
                Estimate  Naive S.E.     Naive z Robust S.E.    Robust z
(Intercept) -5.90100336 0.336455218 -17.5387482 0.231463368 -25.4943295
Loud         0.15507565 0.007513314  20.6401142 0.005321880  29.1392592
Age          0.01851319 0.003561640   5.1979385 0.003305525   5.6006793
OMEhigh     -0.04183516 0.160288285  -0.2609995 0.152182352  -0.2749015
OMElow      -0.28563252 0.131496124  -2.1721745 0.117510247  -2.4307031

Best,

Renaud

Dean Sonneborn a ??crit :
> I'm running some GEE models but when I request the summary(pcb.gee) all 
> I get are rows and rows of intercorelations and they fill up the screen 
> buffer so I can not even scroll back to see what else might be in the 
> summary. How do I get the summary function to NOT print the 
> intercorrelations?
> Thanks,
> 

-- 
Renaud Lancelot
Directeur Adjoint charg?? des Affaires Scientifiques
Deputy Director for Scientific Affairs

D??partement EMVT du CIRAD, TA 30/B
Campus International de Baillarguet
34398 Montpellier Cedex 5 - France
Tel.  +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95



From KINLEY_ROBERT at Lilly.com  Wed Sep 28 18:52:37 2005
From: KINLEY_ROBERT at Lilly.com (Robert Kinley)
Date: Wed, 28 Sep 2005 17:52:37 +0100
Subject: [R] responses to my question on  non-central t
Message-ID: <OFE0D99785.DBADE7FA-ON8025708A.005B19E5-8025708A.005CB54A@EliLilly.lilly.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050928/8cbdd166/attachment.pl

From chabotd at globetrotter.net  Wed Sep 28 19:17:50 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Wed, 28 Sep 2005 13:17:50 -0400
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <x2fyrp41qt.fsf@viggo.kubism.ku.dk>
References: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>
	<86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>
	<Pine.LNX.4.63a.0509280823360.745@homer23.u.washington.edu>
	<x2fyrp41qt.fsf@viggo.kubism.ku.dk>
Message-ID: <36C5C2F0-7754-4BA8-8BB9-3FACE708FCB6@globetrotter.net>

But what about another analogy, that of polynomials? You may not be  
sure what degree polynomial to use, and you have not decided before  
analysing your data. You fit different polynomials to your data,  
checking if added degrees increase r2 sufficiently by doing F-tests.

I thought it was the same thing with GAMs. You can fit a model with 4  
df, and in some cases it is of interest to see if this is a better  
fit than a linear fit. But why can't you also check if 7df is better  
than 4df? And if you used mgcv first and it tells you that 7df is  
better than 4df, why bother repeating the comparison 7df against 4df,  
why not just take the p-value for the model with 7df (fixed)?

Denis

Maybe one is in
Le 05-09-28 ?? 12:04, Peter Dalgaard a ??crit :

> Thomas Lumley <tlumley at u.washington.edu> writes:
>
>
>> Bob, on the other hand, chooses the amount of smoothing depending on
>> the data. When a 4 df smooth fits best he ends up with the same model
>> as Alice and the same p-value.  When some other df fits best he ends
>> up with a different model and a *smaller* p-value than Alice.
>>
>
> This doesn't actually follow, unless the p-value (directly or
> indirectly) found its way into the definition of "best fit". It does
> show the danger, though.
>
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)  
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)  
> 35327907
>



From kthomps7 at gmu.edu  Wed Sep 28 19:18:34 2005
From: kthomps7 at gmu.edu (kthomps7@gmu.edu)
Date: Wed, 28 Sep 2005 13:18:34 -0400
Subject: [R] dumping plots
Message-ID: <f769c8801cda3.433a982a@gmu.edu>

  i've heard that you can dump a plot file, and in doing so set a pixel parameter (cbx?).  is anyone familar with this procedure?

 i would appreciate any insight.

thanks,
kevin



From ken.pierce at oregonstate.edu  Wed Sep 28 19:29:14 2005
From: ken.pierce at oregonstate.edu (Pierce, Ken)
Date: Wed, 28 Sep 2005 10:29:14 -0700
Subject: [R] dumping plots
Message-ID: <4D5DA98A54374044B7CC3F40A157B98B12EE39@thuja>

 There's the xpinch and ypinch parameters in the windows() device call.
However, as an addition to this question, how do you specify output, say
with the savePlot function, to maximize the plot area on an 8 1/2 x 11
sheet of paper?

Ken

~~~~~~~~~~~~~~~~~~~~~~~
Kenneth B. Pierce Jr. 
Research Ecologist
Forestry Sciences Laboratory
3200 SW Jefferson Way
Corvallis, OR 97331
ken.pierce at oregonstate.edu
541 750-7393
~~~~~~~~~~~~~~~~~~~~~~~~


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of kthomps7 at gmu.edu
Sent: Wednesday, September 28, 2005 10:19 AM
To: r-help at stat.math.ethz.ch
Subject: [R] dumping plots

  i've heard that you can dump a plot file, and in doing so set a pixel
parameter (cbx?).  is anyone familar with this procedure?

 i would appreciate any insight.

thanks,
kevin

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Sep 28 20:01:25 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 28 Sep 2005 14:01:25 -0400
Subject: [R] p-level in packages mgcv and gam
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED4B0@usctmx1106.merck.com>

Just change the df in what Thomas described to the degree of polynomial, and
everything he said still applies.  Any good book on regression that covers
polynomial regression ought to point this out.

Andy


> From: Denis Chabot
> 
> But what about another analogy, that of polynomials? You may not be  
> sure what degree polynomial to use, and you have not decided before  
> analysing your data. You fit different polynomials to your data,  
> checking if added degrees increase r2 sufficiently by doing F-tests.
> 
> I thought it was the same thing with GAMs. You can fit a 
> model with 4  
> df, and in some cases it is of interest to see if this is a better  
> fit than a linear fit. But why can't you also check if 7df is better  
> than 4df? And if you used mgcv first and it tells you that 7df is  
> better than 4df, why bother repeating the comparison 7df 
> against 4df,  
> why not just take the p-value for the model with 7df (fixed)?
> 
> Denis
> 
> Maybe one is in
> Le 05-09-28 ?? 12:04, Peter Dalgaard a ??crit :
> 
> > Thomas Lumley <tlumley at u.washington.edu> writes:
> >
> >
> >> Bob, on the other hand, chooses the amount of smoothing 
> depending on
> >> the data. When a 4 df smooth fits best he ends up with the 
> same model
> >> as Alice and the same p-value.  When some other df fits 
> best he ends
> >> up with a different model and a *smaller* p-value than Alice.
> >>
> >
> > This doesn't actually follow, unless the p-value (directly or
> > indirectly) found its way into the definition of "best fit". It does
> > show the danger, though.
> >
> > -- 
> >    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          
> Ph:  (+45)  
> > 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  
> FAX: (+45)  
> > 35327907
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tlumley at u.washington.edu  Wed Sep 28 20:35:26 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 28 Sep 2005 11:35:26 -0700 (PDT)
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <36C5C2F0-7754-4BA8-8BB9-3FACE708FCB6@globetrotter.net>
References: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>
	<86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>
	<Pine.LNX.4.63a.0509280823360.745@homer23.u.washington.edu>
	<x2fyrp41qt.fsf@viggo.kubism.ku.dk>
	<36C5C2F0-7754-4BA8-8BB9-3FACE708FCB6@globetrotter.net>
Message-ID: <Pine.LNX.4.63a.0509281118320.24294@homer24.u.washington.edu>

On Wed, 28 Sep 2005, Denis Chabot wrote:

> But what about another analogy, that of polynomials? You may not be sure what 
> degree polynomial to use, and you have not decided before analysing your 
> data. You fit different polynomials to your data, checking if added degrees 
> increase r2 sufficiently by doing F-tests.

Yes, you can. And this procedure gives you incorrect p-values.

  They may not be very incorrect -- it depends on how much model selection 
you do, and how strongly the feature you are selecting on is related to 
the one you are testing.

For example, using step() to choose a polynomial in x even when x is 
unrelated to y and z inflates the Type I error rate by giving a biased 
estimate of the residual mean squared error:

once<-function(){
   y<-rnorm(50);x<-runif(50);z<-rep(0:1,25)
   summary(step(lm(y~z),
         scope=list(lower=~z,upper=~z+x+I(x^2)+I(x^3)+I(x^4)),
         trace=0))$coef["z",4]
  }
> p<-replicate(1000,once())
> mean(p<0.05)
[1] 0.072

which is significantly higher than you would expect for an honest level 
0.05 test.

 	-thomas



From booopi at yahoo.com  Wed Sep 28 21:10:03 2005
From: booopi at yahoo.com (booop booop)
Date: Wed, 28 Sep 2005 12:10:03 -0700 (PDT)
Subject: [R] is it possible to form matrix of matrices...and multiple arrays
Message-ID: <20050928191003.36972.qmail@web61224.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050928/a892a052/attachment.pl

From N.L.Pace at m.cc.utah.edu  Wed Sep 28 22:01:50 2005
From: N.L.Pace at m.cc.utah.edu (Nathan Leon Pace, MD, MStat)
Date: Wed, 28 Sep 2005 14:01:50 -0600
Subject: [R] xyplots
Message-ID: <E56939FD-6F8F-4F49-8AA6-1E074D6C4163@utah.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050928/d77572c7/attachment.pl

From npaynter at jhsph.edu  Wed Sep 28 22:42:56 2005
From: npaynter at jhsph.edu (Nina Paynter)
Date: Wed, 28 Sep 2005 16:42:56 -0400
Subject: [R] Fast AUC computation
Message-ID: <0INJ00HOLO7NU6@jhuml1.jhmi.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050928/0f43842c/attachment.pl

From deepayan.sarkar at gmail.com  Wed Sep 28 22:43:56 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 28 Sep 2005 15:43:56 -0500
Subject: [R] xyplots
In-Reply-To: <E56939FD-6F8F-4F49-8AA6-1E074D6C4163@utah.edu>
References: <E56939FD-6F8F-4F49-8AA6-1E074D6C4163@utah.edu>
Message-ID: <eb555e6605092813434d354fed@mail.gmail.com>

On 9/28/05, Nathan Leon Pace, MD, MStat <N.L.Pace at m.cc.utah.edu> wrote:
> Hi All,
>
> I have a four panel xyplot. I wish to plot each point as an open or
> filled circle depending on the value of an indicator variable.
>
> I assume I need to use panel.superpose(), but I can't figure out the
> syntax from lattice documentation.

Not directly. You do need to change the plotting character (pch). Here
are two ways to do it (the second is longer but recommended):

dotplot(variety ~ yield | site, data = barley, groups = year, pch = c(1, 16))

dotplot(variety ~ yield | site, data = barley, groups = year,
        auto.key = TRUE,
        par.settings = list(superpose.symbol = list(pch = c(1, 16))))

Deepayan



From matthew_wiener at merck.com  Wed Sep 28 22:57:00 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 28 Sep 2005 16:57:00 -0400
Subject: [R] xyplots
Message-ID: <4E9A692D8755DF478B56A2892388EE1F06B7DE@usctmx1118.merck.com>

> library(lattice)
> temp1 <- data.frame(x = runif(100), y = runif(100), ind1 = sample(1:4,
100, replace = TRUE), ind2 = sample(1:4, 100, replace = TRUE))
> xyplot(y~x |ind1, groups = ind2, data = temp1, pch = c(1, 2, 3, 16))

You don't even need to tell it to use panel.superpose -- using "groups"
tells it (try it with & without and see!).

The pch values are set by group (here, value of ind2).  It might be more
elegant to use trellis.par.set to set the pch values, but I'm lazy.  (It
seems to have the desired effect -- try subsetting on ind2, and you get a
subset of the values.)

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nathan Leon Pace, MD,
MStat
Sent: Wednesday, September 28, 2005 4:02 PM
To: r-help at stat.math.ethz.ch
Cc: Nathan Leon Pace, MD, MStat
Subject: [R] xyplots


Hi All,

I have a four panel xyplot. I wish to plot each point as an open or  
filled circle depending on the value of an indicator variable.

I assume I need to use panel.superpose(), but I can't figure out the  
syntax from lattice documentation.

Running R 2.1 under Mac OS X 10.4.2.

Any suggestions would be appreciated.

Nathan




Nathan Leon Pace, MD, MStat
University of Utah
Salt Lake City, UT 84132
Office: 801.581.6393
Fax: 801.581.4367
Cell: 801.205.1019
Pager: 801.291.9019
Home: 801.467.2925


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mail at bymouth.com  Wed Sep 28 22:57:42 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Thu, 29 Sep 2005 06:57:42 +1000
Subject: [R] correct syntax
Message-ID: <007d01c5c46f$45c8f920$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050929/2c8f9e65/attachment.pl

From venomousanimal at web.de  Wed Sep 28 22:58:12 2005
From: venomousanimal at web.de (venomousanimal)
Date: Wed, 28 Sep 2005 22:58:12 +0200
Subject: [R] is it possible to form matrix of matrices...and multiple
 arrays
In-Reply-To: <20050928191003.36972.qmail@web61224.mail.yahoo.com>
References: <20050928191003.36972.qmail@web61224.mail.yahoo.com>
Message-ID: <433B03E4.6030108@web.de>

booop booop schrieb:

>Dear sirs,
>1...........Kindly tell me is it possible to form a matrix which contains a no of matrices..
>for eg..
>if a,b,c,d are matrices....
>and e is a matrix which contains a,b,c,d as rows and columns..
> 
>2..........Is it possible to form array of array of arrays
> 
>for eg..
>"A" contains two set of arrays (1,2)...and each A[1] and A[2] individually contains two set of arrays
>I tried like 
>p<-list()
>pa[[[1]]] [[1]] [1]<-matrix(1,2,2)
>pa[[[1]]] [[1]] [2]<-matrix(2,2,2)
> 
>But its not working..kindly tell me whether my approach is wrong or not?..
>
>
>kindly tell me the possible ways..
> 
>thank you..
> 
>with regards,
>shanmugam.
>
>		
>---------------------------------
>
> Click here to donate to the Hurricane Katrina relief effort. 
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>
I would say yes and yes.
A Matrix with matrix entries could be a dataframe or you just use the 
cbind or rbind command.
The array of arrays I would say is a table, then you can specify
table[row,coloumn]
or table[1..10,1..10].

Greetz, Sonja



From channoun at iro.umontreal.ca  Wed Sep 28 23:12:48 2005
From: channoun at iro.umontreal.ca (Nabil Channouf)
Date: Wed, 28 Sep 2005 17:12:48 -0400 (EDT)
Subject: [R] R-code for binormla distribution
Message-ID: <Pine.LNX.4.60.0509281710360.28322@olive.iro.umontreal.ca>


Dear users,
does any one have a code (S or R) to compute the binormal distribution 
(or the upper its quadrant area) other than the pmvnorm.
Thanks


-- 
Nabil Channouf
etudiant en Ph.D.
Bureau 3733
Departement d'Informatique et de Recherche Operationnelle (D.I.R.O.)
Universite de Montreal, C.P. 6128, succ. Centre-Ville, Montreal, H3C 3J7
Tel.: (514) 343-6111, poste 1796 Fax.: (514) 343-5834
courriel: channoun at iro.umontreal.ca



From N.L.Pace at m.cc.utah.edu  Wed Sep 28 23:14:10 2005
From: N.L.Pace at m.cc.utah.edu (Nathan Leon Pace, MD, MStat)
Date: Wed, 28 Sep 2005 15:14:10 -0600
Subject: [R] xyplots
Message-ID: <2009CC51-AFF6-4543-AA75-BEAB656E31D8@utah.edu>

Thanks for suggestions on plotting different symbols in an xyplot by  
Sarkar and Wiener in answer to my question:

"I have a four panel xyplot. I wish to plot each point as an open or  
filled circle depending on the value of an indicator variable."

Wiener:

You don't even need to tell it to use panel.superpose -- using "groups"
tells it (try it with & without and see!).

The pch values are set by group (here, value of ind2).  It might be more
elegant to use trellis.par.set to set the pch values, but I'm lazy.  (It
seems to have the desired effect -- try subsetting on ind2, and you  
get a
subset of the values.)

Hope this helps,

Sarkar:

Not directly. You do need to change the plotting character (pch). Here
are two ways to do it (the second is longer but recommended):

dotplot(variety ~ yield | site, data = barley, groups = year, pch = c 
(1, 16))

dotplot(variety ~ yield | site, data = barley, groups = year,
         auto.key = TRUE,
         par.settings = list(superpose.symbol = list(pch = c(1, 16))))



A related question:

My xyplot is essentially a time series (up-and-down experimental  
design). Thus I need to connect the points sequentially regardless of  
the group value. With the groups argument, type = 'b' gives two lines  
- one for each group.

Any more suggestions?

Thanks,

Nathan



From mschwartz at mn.rr.com  Wed Sep 28 23:16:41 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 28 Sep 2005 16:16:41 -0500
Subject: [R] correct syntax
In-Reply-To: <007d01c5c46f$45c8f920$9701a8c0@Tablet>
References: <007d01c5c46f$45c8f920$9701a8c0@Tablet>
Message-ID: <1127942201.6609.53.camel@localhost.localdomain>

On Thu, 2005-09-29 at 06:57 +1000, Stephen Choularton wrote:
> Hi 
>  
> I am trying to produce a little table like this:
>  
> 0                     1
> 0          601       408
> 1          290       2655
>  
> but I cannot get the syntax right.  
>  
> Can anyone help.
>  
> Stephen


Do you have the raw data or only the tabulated counts?

If you have the raw data, see ?table for ways to create n-dimensional
tables.

If you only have the tabulated counts, then you can use matrix():

> mat <- matrix(c(601, 290, 408, 2655), ncol = 2)
> dimnames(mat) <- list(c(0, 1), c(0, 1))

> mat
    0    1
0 601  408
1 290 2655


See ?matrix for more information.

HTH,

Marc Schwartz



From venomousanimal at web.de  Wed Sep 28 23:26:54 2005
From: venomousanimal at web.de (venomousanimal)
Date: Wed, 28 Sep 2005 23:26:54 +0200
Subject: [R] correct syntax
In-Reply-To: <007d01c5c46f$45c8f920$9701a8c0@Tablet>
References: <007d01c5c46f$45c8f920$9701a8c0@Tablet>
Message-ID: <433B0A9E.3030101@web.de>

Stephen Choularton schrieb:

>Hi 
> 
>I am trying to produce a little table like this:
> 
>0                     1
>0          601       408
>1          290       2655
> 
>but I cannot get the syntax right.  
> 
>Can anyone help.
> 
>Stephen
>
>  
>
 > vec1= c(0,0,1)
 > vec2= c(NA,601,290)
 > vec3= c(1,408,2655)
 > table = as.table(cbind(vec1,vec2,vec3))
 > table
  vec1 vec2 vec3
A    0         1
B    0  601  408
C    1  290 2655

Greetz, Sonja



From Soren.Hojsgaard at agrsci.dk  Thu Sep 29 00:18:50 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 29 Sep 2005 00:18:50 +0200
Subject: [R] After resizing a tkwidget, how do I get the size of the widget??
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC03877FED@DJFPOST01.djf.agrsci.dk>

Dear all,
I am trying make a small tcltk thing for drawing graphs with vertices, edges etc. I can draw the graph in a window.

- I would like it so that if I resize the window, the graph will also be resized. To do this, I guess I need the size of the window, so that I can calculate the new coordinates.
- I would also like to be able to save the graph as e.g. a jpg/ps file.
Can anyone help on this?

I find the documentation of the tcltk package difficult to comprehend so I have used the examples on http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/. Can anyone point me to other sources of examples/doc of the tcltk package?
Thanks in advance
S??ren



From W.E.Wolski at newcastle.ac.uk  Thu Sep 29 00:43:44 2005
From: W.E.Wolski at newcastle.ac.uk (nwew)
Date: Wed, 28 Sep 2005 23:43:44 +0100
Subject: [R] Problem with memory footprint of qq plot generated with lattice
Message-ID: <43396F07@webmail.ncl.ac.uk>

Dear R helpers,

I generate a qq plot using the following function call.

qqmath(~val|ind,data=xx
       ,distribution=function(p) qt(p,df=19)
       ,ylab="Sample Quatinles"
       ,xlab="Theoretical Quantiles"
       ,aspect=1
       ,prepanel = prepanel.qqmathline
       ,panel=function(x,y)
       {
         panel.qqmathline(y, distribution=function(p) qt(p,df=19),col=2)
         panel.qqmath(x, y , distribution=function(p) 
qt(p,df=19),pch=".",cex=2)
       }
)

dim(xx)
[1] 680237      2

unique(xx[,1])
[1] 500-530   1000-1110 2000-2200 3400-3700
Levels: 500-530 1000-1110 2000-2200 3400-3700

It means that actually four small qqplots are drawn.

The problem is that the generated pdf is extremely large.

-rw-r--r--  1 nwew bayes 19734746 2005-08-27 15:38 Distr-qqplot.pdf

Suggestions howe to reduce the size but keep the quality are wellcome.

cheers
Eryk



From aorchid at mac.com  Thu Sep 29 00:57:01 2005
From: aorchid at mac.com (Aric Gregson)
Date: Wed, 28 Sep 2005 18:57:01 -0400
Subject: [R] Plot Data Points in boxplots
In-Reply-To: <971536df0509271755308e5a33@mail.gmail.com>
References: <971536df0509271755308e5a33@mail.gmail.com>
Message-ID: <1551433AFCD0B35F3DD1C916@G4Desktop.local>

--On 9/27/05 8:55 PM -0400 Gabor Grothendieck sent:

> Try this:
>
> boxplot(Sepal.Length ~ Species, iris)
> with(iris, stripchart(Sepal.Length ~ Species, vertical = TRUE, add =
> TRUE))

Thanks very much for the hint. I did something very similar with decent 
results. The only problem I am having now is that I cannot figure out 
how to make stripchart plot without a box or axis. Alternatively, I can 
use DOTplot from UsingR, but I cannot find out how to rotate it for 
vertical.

thanks,

aric

> On 9/27/05, Aric Gregson <aorchid at mac.com> wrote:
>> Hello,
>>
>> I would like to plot my data in a fashion similar to a boxplot, but
>> plot the true data points without a box, just overlay lines of the
>> summary generated by the boxplot.



From p.murrell at auckland.ac.nz  Thu Sep 29 01:03:29 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 29 Sep 2005 11:03:29 +1200
Subject: [R] Plot Data Points in boxplots
References: <971536df0509271755308e5a33@mail.gmail.com>
	<1551433AFCD0B35F3DD1C916@G4Desktop.local>
Message-ID: <433B2141.6030005@stat.auckland.ac.nz>

Hi


Aric Gregson wrote:
> --On 9/27/05 8:55 PM -0400 Gabor Grothendieck sent:
> 
> 
>>Try this:
>>
>>boxplot(Sepal.Length ~ Species, iris)
>>with(iris, stripchart(Sepal.Length ~ Species, vertical = TRUE, add =
>>TRUE))
> 
> 
> Thanks very much for the hint. I did something very similar with decent 
> results. The only problem I am having now is that I cannot figure out 
> how to make stripchart plot without a box or axis. Alternatively, I can 
> use DOTplot from UsingR, but I cannot find out how to rotate it for 
> vertical.


You might also like to take a look at Figure 3.27 (and associated R 
code) on
http://www.stat.auckland.ac.nz/~paul/RGraphics/chapter3.html

Paul


>>On 9/27/05, Aric Gregson <aorchid at mac.com> wrote:
>>
>>>Hello,
>>>
>>>I would like to plot my data in a fashion similar to a boxplot, but
>>>plot the true data points without a box, just overlay lines of the
>>>summary generated by the boxplot.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From dhinds at sonic.net  Thu Sep 29 01:53:02 2005
From: dhinds at sonic.net (dhinds@sonic.net)
Date: Wed, 28 Sep 2005 23:53:02 +0000 (UTC)
Subject: [R] Problem with memory footprint of qq plot generated with
	lattice
References: <43396F07@webmail.ncl.ac.uk>
Message-ID: <dhfact$g2l$1@sea.gmane.org>

nwew <W.E.Wolski at newcastle.ac.uk> wrote:
> Dear R helpers,

> I generate a qq plot using the following function call.

...

> dim(xx)
> [1] 680237      2

How about doing something like this:

fn <- function(n,cut=0.001,m=1000)
{
    p <- ppoints(n)
    p <- p[pmin(p, 1-p) < cut]
    q <- pt(seq(qt(cut,df=19),qt(1-cut,df=19),length=m),df=19)
    sort(c(p,q))
}

then adding 'f.value=fn' to your qqmath arguments?  This essentially
says, plot the individual data points in the extreme tails of the
distribution (p < 0.001 or p > 0.999), and evaluate the distribution
at a sparse set of points in between, where the density means you
can't discern the individual values anyway.

-- Dave



From bitwrit at ozemail.com.au  Thu Sep 29 12:13:35 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 29 Sep 2005 10:13:35 +0000
Subject: [R] Plot Data Points in boxplots
In-Reply-To: <272357639150304F937C975E@G4Desktop.local>
References: <272357639150304F937C975E@G4Desktop.local>
Message-ID: <433BBE4F.6020101@ozemail.com.au>

Aric Gregson wrote:
> Hello,
> 
> I would like to plot my data in a fashion similar to a boxplot, but 
> plot the true data points without a box, just overlay lines of the 
> summary generated by the boxplot. I have less than 10 observations 
> within each group, so I think showing the actual data would be more 
> effective than the box of the boxplot. I have been unable to find a way 
> to do this.
> 
> Here is example data:
> 
>>d168teni
> 
>   d168dh10i d168hb10i d168icc10i d168rcs10i d168t410i d168tb410i
> 1        72        52         29         80        39         68
> 2        76        47         28         68        49         21
> 3       123        85         87         71       164        137
> 4        58        47         50         70        18          1
> 
> 
>>boxplot(d168teni)
> 
> 
> works to describe the data (each column a column in the plot). However, 
> instead of the boxes, I want the data plotted (in a column) with the 5 
> summary lines drawn over the points.
> 
> I have tried using functions from Design and have been unable to find a 
> solution. I think I am missing the point.
> 
> Any suggestions on where to look or how to approach this differently?
> 
I haven't seen anything like this posted, so I'll take a punt.

noboxplot<-function(x,plot=FALSE,...) {
  boxplot.info<-boxplot(x,plot,...)
  dimx<-dim(x)
  if(is.null(dimx)) plot(1,x)
  else {
   xpts<-1:dimx[2]
   # you may want to use text() here if you want the
   # actual values displayed
   matplot(t(as.matrix(x)),axes=FALSE,xlim=c(0.5,dimx[2]+0.5),
    ylab=deparse(substitute(x)))
  }
  box()
  axis(2)
  nbp.labels<-names(x)
  if(is.null(nbp.labels)) nbp.labels<-as.character(xpts)
  axis(1,at=xpts,labels=nbp.labels)
  for(xpos in 1:dimx[2]) {
   segments(xpos-0.2,boxplot.info$stats[,xpos],
            xpos+0.2,boxplot.info$stats[,xpos],
	   lwd=c(1,1,3,1,1))
  }
  return(boxplot.info)
}

Hope it's what you want.

Jim



From ggrothendieck at gmail.com  Thu Sep 29 02:34:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Sep 2005 20:34:00 -0400
Subject: [R] Plot Data Points in boxplots
In-Reply-To: <1551433AFCD0B35F3DD1C916@G4Desktop.local>
References: <971536df0509271755308e5a33@mail.gmail.com>
	<1551433AFCD0B35F3DD1C916@G4Desktop.local>
Message-ID: <971536df05092817347437b53b@mail.gmail.com>

Is this any better?  For more along these lines, check out ?bxp for
numerous options which are accepted by boxplot, not just bxp:

boxplot(Sepal.Length ~ Species, iris,
    boxcol = 1, medcol = 2, whiskcol = 0, staplecol = 4, outcol = 0,
    boxfill = "wheat", boxlty = 0)
with(iris, stripchart(Sepal.Length ~ Species, vertical = TRUE, add = TRUE))

On 9/28/05, Aric Gregson <aorchid at mac.com> wrote:
> --On 9/27/05 8:55 PM -0400 Gabor Grothendieck sent:
>
> > Try this:
> >
> > boxplot(Sepal.Length ~ Species, iris)
> > with(iris, stripchart(Sepal.Length ~ Species, vertical = TRUE, add =
> > TRUE))
>
> Thanks very much for the hint. I did something very similar with decent
> results. The only problem I am having now is that I cannot figure out
> how to make stripchart plot without a box or axis. Alternatively, I can
> use DOTplot from UsingR, but I cannot find out how to rotate it for
> vertical.
>
> thanks,
>
> aric
>
> > On 9/27/05, Aric Gregson <aorchid at mac.com> wrote:
> >> Hello,
> >>
> >> I would like to plot my data in a fashion similar to a boxplot, but
> >> plot the true data points without a box, just overlay lines of the
> >> summary generated by the boxplot.
>
>
>
>
>



From cyau at buckinstitute.org  Thu Sep 29 02:56:50 2005
From: cyau at buckinstitute.org (Christina Yau)
Date: Wed, 28 Sep 2005 17:56:50 -0700
Subject: [R] memory issues with large data set
Message-ID: <C7CAAAC7D2D14C409367E8D9026C1D781B204E@inverness.buckcenter.org>

Hi,
 
I am running R 2.0.1.1. on Windows.  It is a Dell Dimension with a 3.2 Ghz Processor and 4Gb RAM. 
 
When using the ReadAffy() function to read in 97 arrays, I get the below error messages:
Error: cannot allocate vector of size 393529
Reached total allocation of 1024Mb: see help(memory.size)
 
When I use the comman "memory.limit(size=4000)" to increase the memory size to the maximum available, I got a "NULL" as a response.
 
I proceeded to re-run ReadAffy().  This time, I only get the first error message.
Error: cannot allocate vector of size 393529
 
>From what I've read, this is more of a problem with Windows than with R.  But I am wondering if there is anything I can do, either with the set up of R or Windows, to solve this problem and read the data set into R using this machine.
 
Thank you for your attention,
Christina



From jmacdon at med.umich.edu  Thu Sep 29 03:17:06 2005
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Wed, 28 Sep 2005 21:17:06 -0400
Subject: [R] memory issues with large data set
In-Reply-To: <C7CAAAC7D2D14C409367E8D9026C1D781B204E@inverness.buckcenter.org>
References: <C7CAAAC7D2D14C409367E8D9026C1D781B204E@inverness.buckcenter.org>
Message-ID: <433B4092.6090405@med.umich.edu>

Christina Yau wrote:
> Hi,
> 
> I am running R 2.0.1.1. on Windows.  It is a Dell Dimension with a
> 3.2 Ghz Processor and 4Gb RAM.

This question concerns a BioC package, so the correct listserv is 
bioconductor at stat.math.ethz.ch, not the R-help listserv. In the future, 
you should direct questions about BioC packages there.

You don't have enough memory to read all 97 arrays into an AffyBatch, 
not to mention doing any further processing on them. You will have to 
use justRMA() or justGCRMA() to process your data.

In addition, I don't think you can access any more than 2 Gb of RAM 
anyway without making some changes. See 2.11 of the Windows FAQ.

HTH,

Jim


> 
> When using the ReadAffy() function to read in 97 arrays, I get the
> below error messages: Error: cannot allocate vector of size 393529 
> Reached total allocation of 1024Mb: see help(memory.size)
> 
> When I use the comman "memory.limit(size=4000)" to increase the
> memory size to the maximum available, I got a "NULL" as a response.
> 
> I proceeded to re-run ReadAffy().  This time, I only get the first
> error message. Error: cannot allocate vector of size 393529
> 
>> From what I've read, this is more of a problem with Windows than
>> with R.  But I am wondering if there is anything I can do, either
>> with the set up of R or Windows, to solve this problem and read the
>> data set into R using this machine.
> 
> Thank you for your attention, Christina
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html


-- 
James W. MacDonald
University of Michigan
Affymetrix and cDNA Microarray Core
1500 E Medical Center Drive
Ann Arbor MI 48109
734-647-5623



**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.



From ggrothendieck at gmail.com  Thu Sep 29 03:33:37 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Sep 2005 21:33:37 -0400
Subject: [R] is it possible to form matrix of matrices...and multiple
	arrays
In-Reply-To: <20050928191003.36972.qmail@web61224.mail.yahoo.com>
References: <20050928191003.36972.qmail@web61224.mail.yahoo.com>
Message-ID: <971536df050928183323eddcc5@mail.gmail.com>

On 9/28/05, booop booop <booopi at yahoo.com> wrote:
> Dear sirs,
> 1...........Kindly tell me is it possible to form a matrix which contains a no of matrices..
> for eg..
> if a,b,c,d are matrices....
> and e is a matrix which contains a,b,c,d as rows and columns..

Try this:

mat <- list(diag(2), diag(3), diag(4), diag(5))
dim(mat) <- c(2,2)
mat
mat[[1,1]]

>
> 2..........Is it possible to form array of array of arrays
>
> for eg..
> "A" contains two set of arrays (1,2)...and each A[1] and A[2] individually contains two set of arrays
> I tried like
> p<-list()
> pa[[[1]]] [[1]] [1]<-matrix(1,2,2)
> pa[[[1]]] [[1]] [2]<-matrix(2,2,2)
>
> But its not working..kindly tell me whether my approach is wrong or not?..

Try this. arr is set to a 2x2x2 array filled with diagonal matrices.
Then we set two elements of it to constant matrices.

arr <- lapply(2:9, diag)
dim(arr) <- c(2,2,2)
arr[[1,1,1]]
arr
arr[[1,1,1]] <- matrix(1,2,2)
arr[[1,1,2]] <- matrix(2,2,2)
arr[[1,1,1]]
arr[[1,1,2]]



From remigijus.lapinskas at mif.vu.lt  Thu Sep 29 06:31:10 2005
From: remigijus.lapinskas at mif.vu.lt (Remigijus Lapinskas)
Date: Thu, 29 Sep 2005 07:31:10 +0300
Subject: [R] priceIts
Message-ID: <433B6E0E.6020101@mif.vu.lt>

Dear All,

There is an example for the priceIts function (the its package) which 
does not work for me as expected.

 > ?priceIts
 >  x1 <- priceIts(instrument = c("^ftse"), start = "1998-01-01",
+                          quote = "Close")
Error in validObject(.Object) : invalid class "its" object: Missing 
values in dates
 >      x2 <- priceIts(instrument = c("^gdax"), start = "1998-01-01",
+                          quote = "Close")
Error in download.file(url, destfile, method = method, quiet = quiet) :
         cannot open URL 
'http://chart.yahoo.com/table.csv?s=^gdax&a=0&b=01&c=1998&d=8&e=28&f=2005&g=d&q=q&y=0&z=^gdax&x=.csv'
In addition: Warning message:
cannot open: HTTP status was '404 Not Found'
 >      x <- union(x1,x2)
Error: Object "x1" not found
Error in union(x1, x2) : unable to find the argument 'x' in selecting a 
method for function 'union'
 >      names(x) <- c("FTSE","DAX")
 >      plot(x,lab=TRUE)

Any help appreciated.

Best wishes,
Remigijus



From Tom.Mulholland at dpi.wa.gov.au  Thu Sep 29 06:59:12 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 29 Sep 2005 12:59:12 +0800
Subject: [R] priceIts
Message-ID: <4702645135092E4497088F71D9C8F51A128C4F@afhex01.dpi.wa.gov.au>

Well I downloaded the data using the link in your message which suggests that the code is right. I don't have its loaded (I assume it's from the irregular time series package) so I can't test the code as you have it. 

Have you tried checking to see it it is an issue with the internet (your browser, blocked sites etc) rather than a problem in R?

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Remigijus
> Lapinskas
> Sent: Thursday, 29 September 2005 12:31 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] priceIts
> 
> 
> Dear All,
> 
> There is an example for the priceIts function (the its package) which 
> does not work for me as expected.
> 
>  > ?priceIts
>  >  x1 <- priceIts(instrument = c("^ftse"), start = "1998-01-01",
> +                          quote = "Close")
> Error in validObject(.Object) : invalid class "its" object: Missing 
> values in dates
>  >      x2 <- priceIts(instrument = c("^gdax"), start = "1998-01-01",
> +                          quote = "Close")
> Error in download.file(url, destfile, method = method, quiet 
> = quiet) :
>          cannot open URL 
> 'http://chart.yahoo.com/table.csv?s=^gdax&a=0&b=01&c=1998&d=8&
> e=28&f=2005&g=d&q=q&y=0&z=^gdax&x=.csv'
> In addition: Warning message:
> cannot open: HTTP status was '404 Not Found'
>  >      x <- union(x1,x2)
> Error: Object "x1" not found
> Error in union(x1, x2) : unable to find the argument 'x' in 
> selecting a 
> method for function 'union'
>  >      names(x) <- c("FTSE","DAX")
>  >      plot(x,lab=TRUE)
> 
> Any help appreciated.
> 
> Best wishes,
> Remigijus
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Thu Sep 29 06:59:18 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Sep 2005 00:59:18 -0400
Subject: [R] priceIts
In-Reply-To: <433B6E0E.6020101@mif.vu.lt>
References: <433B6E0E.6020101@mif.vu.lt>
Message-ID: <971536df05092821594718c23@mail.gmail.com>

The error message indicates that a problem is occurring in download.file.

Try to use download.file to download some web page from the internet to
see if that works,
   download.file("http://www.google.com", destfile = "google.htm")
If that does NOT work then examine the download.file help page carefully.
You may need to set your proxy server.

If that did work then as a workaround try using get.hist.quote in package
tseries with the argument retclass = "its".  Be sure you have the latest
version of tseries.

On 9/29/05, Remigijus Lapinskas <remigijus.lapinskas at mif.vu.lt> wrote:
> Dear All,
>
> There is an example for the priceIts function (the its package) which
> does not work for me as expected.
>
>  > ?priceIts
>  >  x1 <- priceIts(instrument = c("^ftse"), start = "1998-01-01",
> +                          quote = "Close")
> Error in validObject(.Object) : invalid class "its" object: Missing
> values in dates
>  >      x2 <- priceIts(instrument = c("^gdax"), start = "1998-01-01",
> +                          quote = "Close")
> Error in download.file(url, destfile, method = method, quiet = quiet) :
>         cannot open URL
> 'http://chart.yahoo.com/table.csv?s=^gdax&a=0&b=01&c=1998&d=8&e=28&f=2005&g=d&q=q&y=0&z=^gdax&x=.csv'
> In addition: Warning message:
> cannot open: HTTP status was '404 Not Found'
>  >      x <- union(x1,x2)
> Error: Object "x1" not found
> Error in union(x1, x2) : unable to find the argument 'x' in selecting a
> method for function 'union'
>  >      names(x) <- c("FTSE","DAX")
>  >      plot(x,lab=TRUE)
>
> Any help appreciated.
>
> Best wishes,
> Remigijus
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From vincent at 7d4.com  Thu Sep 29 08:13:46 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Thu, 29 Sep 2005 08:13:46 +0200
Subject: [R] is it possible to form matrix of matrices...and multiple
 arrays
In-Reply-To: <20050928191003.36972.qmail@web61224.mail.yahoo.com>
References: <20050928191003.36972.qmail@web61224.mail.yahoo.com>
Message-ID: <433B861A.8080501@7d4.com>

booop booop a ??crit :

> 1...........Kindly tell me is it possible to form 
> a matrix which contains a no of matrices.. for eg..
> if a,b,c,d are matrices....
> and e is a matrix which contains a,b,c,d as rows and columns..

I don't think you can use matrix() to store other matrix() inside.
But array() is a solution to store matrix() inside.
(At least I have use it).



From vincent at 7d4.com  Thu Sep 29 08:30:10 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Thu, 29 Sep 2005 08:30:10 +0200
Subject: [R] Plot Data Points in boxplots
In-Reply-To: <433B2141.6030005@stat.auckland.ac.nz>
References: <971536df0509271755308e5a33@mail.gmail.com>	<1551433AFCD0B35F3DD1C916@G4Desktop.local>
	<433B2141.6030005@stat.auckland.ac.nz>
Message-ID: <433B89F2.5020201@7d4.com>

Paul Murrell a ??crit :

> You might also like to take a look at Figure 3.27 
> (and associated R code) on
> http://www.stat.auckland.ac.nz/~paul/RGraphics/chapter3.html

Thank you very much for making this page available.
The power of R graphics, and the number of options,
makes such examples very useful.
(At least for beginners like me !)
Thanks.



From maechler at stat.math.ethz.ch  Thu Sep 29 09:16:04 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 29 Sep 2005 09:16:04 +0200
Subject: [R] Plot Data Points in boxplots
In-Reply-To: <433B89F2.5020201@7d4.com>
References: <971536df0509271755308e5a33@mail.gmail.com>
	<1551433AFCD0B35F3DD1C916@G4Desktop.local>
	<433B2141.6030005@stat.auckland.ac.nz> <433B89F2.5020201@7d4.com>
Message-ID: <17211.38068.320699.512022@stat.math.ethz.ch>

>>>>> "vincent" == vincent  <vincent at 7d4.com>
>>>>>     on Thu, 29 Sep 2005 08:30:10 +0200 writes:

    vincent> Paul Murrell a ??crit :
    >> You might also like to take a look at Figure 3.27 (and
    >> associated R code) on
    >> http://www.stat.auckland.ac.nz/~paul/RGraphics/chapter3.html

    vincent> Thank you very much for making this page available.
    vincent> The power of R graphics, and the number of options,
    vincent> makes such examples very useful.  (At least for
    vincent> beginners like me !)  Thanks.

To really thank Paul, you should buy the book to which the above
web page refers :
  http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html

A very nice and useful book to have, indeed!

Martin Maechler, ETH Zurich



From henric.nilsson at statisticon.se  Thu Sep 29 09:55:19 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 29 Sep 2005 09:55:19 +0200
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <1127919949.6577.97.camel@new-york.climpact.net>
References: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>	<86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>
	<1127919949.6577.97.camel@new-york.climpact.net>
Message-ID: <433B9DE7.1070509@statisticon.se>

Yves Magliulo said the following on 2005-09-28 17:05:
> hi,
> 
> i'll try to help you, i send a mail about this subject last week... and
> i did not have any response...
> 
> I'm using gam from package mgcv. 
> 
> 1)
> How to interpret the significance of smooth terms is hard for me to
> understand perfectly : 
> using UBRE, you fix df. p-value are estimated by chi-sq distribution 
> using GCV, the best df are estimated by GAM. (that's what i want) and
> p-values 

This is not correct. The df are estimated in both cases (i.e. UBRE and 
GCV), but the scale parameter is fixed in the UBRE case. Hence, by 
default UBRE is used for family = binomial or poisson since the scale 
parameter is assumed to be 1. Similarly, GCV is the default for family = 
gaussian since we most often want the scale (usually denoted sigma^2) to 
be estimated.

> are estimated by an F distribution But in that case they said "use at
> your own risk" in ?summary.gam

The warning applies in both cases. The p-values are conditional on the 
smoothing parameters, and the uncertainty of the smooths is not taken 
into account when computing the p-values.

> so you can also look at the chi.sq : but i don't know how to choose a

No...

> criterion like for p-values... for me, chi.sq show the best predictor in
> a model, but it's hard to reject one with it.

Which version of mgcv do you use? The confusion probably stems from 
earlier versions of mgcv (< 1.3-5): the summary and anova methods used 
to have a column denoted Chi.sq even when the displayed statistic was 
computed as F. Recent versions of mgcv has

 > summary(b)

Family: gaussian
Link function: identity

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept)   7.9150     0.1049   75.44   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Approximate significance of smooth terms:
         edf Est.rank      F  p-value
s(x0) 5.173    9.000  3.785 0.000137 ***
s(x1) 2.357    9.000 34.631  < 2e-16 ***
s(x2) 8.517    9.000 84.694  < 2e-16 ***
s(x3) 1.000    1.000  0.444 0.505797
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-sq.(adj) =  0.726   Deviance explained = 73.7%
GCV score =  4.611   Scale est. = 4.4029    n = 400


If we assume that the scale is known and fixed at 4.4029, we get

 > summary(b, dispersion = 4.4029)

Family: gaussian
Link function: identity

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)   7.9150     0.1049   75.44   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Approximate significance of smooth terms:
         edf Est.rank  Chi.sq p-value
s(x0) 5.173    9.000  34.067 8.7e-05 ***
s(x1) 2.357    9.000 311.679 < 2e-16 ***
s(x2) 8.517    9.000 762.255 < 2e-16 ***
s(x3) 1.000    1.000   0.444   0.505
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-sq.(adj) =  0.726   Deviance explained = 73.7%
GCV score =  4.611   Scale est. = 4.4029    n = 400

Note that t/F changed into z/Chi.sq.

> 
> so as far as i m concerned, i use GCV methods, and fix a 5% on the null
> hypothesis (pvalue) to select significant predictor. after, i look at my
> smooth, and if the parametrization look fine to me, i validate.
> 
> generaly, for p-values smaller than 0.001, you can be confident. over
> 0.001, you have to check. 
> 
> 2)
> for difference between package gam and mgcv, i sent a mail about this

The underlying algorithms are very different.


HTH,
Henric

> one year ago, here's the response :
> 
> "
> - package gam is based very closely on the GAM approach presented in 
> Hastie and Tibshirani's  "Generalized Additive Models" book. Estimation
> is 
> by back-fitting and model selection is based on step-wise regression 
> methods based on approximate distributional results. A particular
> strength 
> of this approach is that local regression smoothers (`lo()' terms) can
> be 
> included in GAM models.
> 
> - gam in package mgcv represents GAMs using penalized regression
> splines. 
> Estimation is by direct penalized likelihood maximization with 
> integrated smoothness estimation via GCV or related criteria (there is 
> also an alternative `gamm' function based on a mixed model approach). 
> Strengths of the this approach are that s() terms can be functions of
> more 
> than one variable and that tensor product smooths are available via te()
> terms - these are useful when different degrees of smoothness are 
> appropriate relative to different arguments of a smooth.
> 
> (...)
> 
> Basically, if you want integrated smoothness selection, an underlying 
> parametric representation, or want smooth interactions in your models 
> then mgcv is probably worth a try (but I would say that). If you want to
> use local regression smoothers and/or prefer the stepwise selection 
> approach then package gam is for you. 
> "
> 
> i think the difference of p-values between :gam and :mgcv, is because
> you don't have same number of step iteration. mgcv : gam choose the
> number of step and with gam : gam you have to choose it..
> 
> hope it helps and someone gives us more details...
> 
> Yves
> 
> 
> Le mer 28/09/2005 ?? 15:30, Denis Chabot a ??crit :
> 
> 
>>I only got one reply to my message:
>>
>>
>>>No, this won't work.  The problem is the usual one with model  
>>>selection: the p-value is calculated as if the df had been fixed,  
>>>when really it was estimated.
>>>
>>>It is likely to be quite hard to get an honest p-value out of  
>>>something that does adaptive smoothing.
>>>
>>>    -thomas
>>
>>I do not understand this: it seems that a lot of people chose df=4  
>>for no particular reason, but p-levels are correct. If instead I  
>>choose df=8 because a previous model has estimated this to be an  
>>optimal df, P-levels are no good because df are estimated?
>>
>>Furthermore, shouldn't packages gam and mgcv give similar results  
>>when the same data and df are used? I tried this:
>>
>>library(gam)
>>data(kyphosis)
>>kyp1 <- gam(Kyphosis ~ s(Age, 4), family=binomial, data=kyphosis)
>>kyp2 <- gam(Kyphosis ~ s(Number, 4), family=binomial, data=kyphosis)
>>kyp3 <- gam(Kyphosis ~ s(Start, 4), family=binomial, data=kyphosis)
>>anova.gam(kyp1)
>>anova.gam(kyp2)
>>anova.gam(kyp3)
>>
>>detach(package:gam)
>>library(mgcv)
>>kyp4 <- gam(Kyphosis ~ s(Age, k=4, fx=T),  family=binomial,  
>>data=kyphosis)
>>kyp5 <- gam(Kyphosis ~ s(Number, k=4, fx=T),  family=binomial,  
>>data=kyphosis)
>>kyp6 <- gam(Kyphosis ~ s(Start, k=4, fx=T),  family=binomial,  
>>data=kyphosis)
>>anova.gam(kyp4)
>>anova.gam(kyp5)
>>anova.gam(kyp6)
>>
>>
>>P levels for these models, by pair
>>
>>kyp1 vs kyp4: p= 0.083 and 0.068 respectively (not too bad)
>>kyp2 vs kyp5: p= 0.445 and 0.03 (wow!)
>>kyp3 vs kyp6: p= 0.053 and 0.008 (wow again)
>>
>>Also if you plot all these you find that the mgcv plots are smoother  
>>than the gam plots, even the same df are used all the time.
>>
>>I am really confused now!
>>
>>Denis
>>
>>D??but du message r??exp??di?? :
>>
>>
>>>>De : Denis Chabot <chabotd at globetrotter.net>
>>>>Date : 26 septembre 2005 12:25:04 HAE
>>>>?? : r-help at stat.math.ethz.ch
>>>>Objet : p-level in packages mgcv and gam
>>>>
>>>>
>>>>Hi,
>>>>
>>>>I am fairly new to GAM and started using package mgcv. I like the  
>>>>fact that optimal smoothing is automatically used (i.e. df are not  
>>>>determined a priori but calculated by the gam procedure).
>>>>
>>>>But the mgcv manual warns that p-level for the smooth can be  
>>>>underestimated when df are estimated by the model. Most of the  
>>>>time my p-levels are so small that even doubling them would not  
>>>>result in a value close to the P=0.05 threshold, but I have one  
>>>>case with P=0.033.
>>>>
>>>>I thought, probably naively, that running a second model with  
>>>>fixed df, using the value of df found in the first model. I could  
>>>>not achieve this with mgcv: its gam function does not seem to  
>>>>accept fractional values of df (in my case 8.377).
>>>>
>>>>So I used the gam package and fixed df to 8.377. The P-value I  
>>>>obtained was slightly larger than with mgcv (0.03655 instead of  
>>>>0.03328), but it is still < 0.05.
>>>>
>>>>Was this a correct way to get around the "underestimated P-level"?
>>>>
>>>>Furthermore, although the gam.check function of the mgcv package  
>>>>suggests to me that the gaussian family (and identity link) are  
>>>>adequate for my data, I must say the instructions in R help for  
>>>>"family" and in Hastie, T. and Tibshirani, R. (1990) Generalized  
>>>>Additive Models are too technical for me. If someone knows a  
>>>>reference that explains how to choose model and link, i.e. what  
>>>>tests to run on your data before choosing, I would really  
>>>>appreciate it.
>>>>
>>>>Thanks in advance,
>>>>
>>>>Denis Chabot
>>>>
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mikewlcheung at hku.hk  Thu Sep 29 10:32:00 2005
From: mikewlcheung at hku.hk (Mike Cheung)
Date: Thu, 29 Sep 2005 16:32:00 +0800
Subject: [R] how to fix the level-1 variances in lme()?
Message-ID: <433BA680.4050106@hku.hk>

Dear all,

Edmond Ng (http://multilevel.ioe.ac.uk/softrev/reviewsplus.pdf) provides 
an example to fit the mixed effects meta-analysis in Splus 6.2. The 
syntax is:
lme(fixed=d~wks, data=meta, random=~1|study, weights=varFixed(~Vofd), 
control=lmeControl(sigma=1))
where d is the effect size, study is the study number, Vofd is the 
variance of the effect size and meta is the data frame.

"sigma=1" is required to constrain the level 1 variance in applying 
mixed-effects models in meta-analysis.

In Splus 6.1, I found that the help manual of nlme includes "sigma" as 
an optional argument in lmeControl() to fix the within-group standard 
error during the optimization.

However, both Pinheiro and Bates (2000, p.476) and the help manual of 
the nlme package in R for Version 3.1-65 
(http://cran.r-project.org/doc/packages/nlme.pdf, p.172) do not include 
"sigma" as an argument for lmeControl().

I would like to know how I could fix the level-1 variances as known 
values in lme().

Thanks in advance!

Best,
Mike

-- 
---------------------------------------------------------------------
  Mike W.L. Cheung               Phone: (852) 2857-8621
  Department of Psychology       Fax:   (852) 2858-3518
  The University of Hong Kong    E-mail: mikewlcheung at hku.hk	
  HONG KONG                      Website: http://web.hku.hk/~mikewlch



From ripley at stats.ox.ac.uk  Thu Sep 29 10:55:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Sep 2005 09:55:08 +0100 (BST)
Subject: [R] how to fix the level-1 variances in lme()?
In-Reply-To: <433BA680.4050106@hku.hk>
References: <433BA680.4050106@hku.hk>
Message-ID: <Pine.LNX.4.61.0509290946130.13451@gannet.stats>

On Thu, 29 Sep 2005, Mike Cheung wrote:

> Dear all,
>
> Edmond Ng (http://multilevel.ioe.ac.uk/softrev/reviewsplus.pdf) provides
> an example to fit the mixed effects meta-analysis in Splus 6.2. The
> syntax is:
> lme(fixed=d~wks, data=meta, random=~1|study, weights=varFixed(~Vofd),
> control=lmeControl(sigma=1))
> where d is the effect size, study is the study number, Vofd is the
> variance of the effect size and meta is the data frame.
>
> "sigma=1" is required to constrain the level 1 variance in applying
> mixed-effects models in meta-analysis.
>
> In Splus 6.1, I found that the help manual of nlme includes "sigma" as
> an optional argument in lmeControl() to fix the within-group standard
> error during the optimization.
>
> However, both Pinheiro and Bates (2000, p.476) and the help manual of
> the nlme package in R for Version 3.1-65
> (http://cran.r-project.org/doc/packages/nlme.pdf, p.172) do not include
> "sigma" as an argument for lmeControl().
>
> I would like to know how I could fix the level-1 variances as known
> values in lme().

Use S-PLUS and nlme >= 3.3 (as I recall).

R's and S-PLUS's nlme are not the same (they diverged from a common base 
around 1998).  It is a while (2 years?) since I needed this capability, 
but at the time it was not possible in R's lme.

Since you have access to S-PLUS, why not use it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Sep 29 10:59:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Sep 2005 09:59:26 +0100 (BST)
Subject: [R] is it possible to form matrix of matrices...and multiple
 arrays
In-Reply-To: <433B861A.8080501@7d4.com>
References: <20050928191003.36972.qmail@web61224.mail.yahoo.com>
	<433B861A.8080501@7d4.com>
Message-ID: <Pine.LNX.4.61.0509290955240.13451@gannet.stats>

On Thu, 29 Sep 2005 vincent at 7d4.com wrote:

> booop booop a ?crit :
>
>> 1...........Kindly tell me is it possible to form
>> a matrix which contains a no of matrices.. for eg..
>> if a,b,c,d are matrices....
>> and e is a matrix which contains a,b,c,d as rows and columns..
>
> I don't think you can use matrix() to store other matrix() inside.
> But array() is a solution to store matrix() inside.
> (At least I have use it).

You _can_ do this with matrix() (although that was not quite what was 
asked). Try

a <- b <- c <- d <- matrix(1:4, 2, 2)
e <- matrix(list(a,b,c,d), 2,2)
e
e[1,2][[1]]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Thu Sep 29 11:45:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Sep 2005 10:45:57 +0100 (BST)
Subject: [R] priceIts
In-Reply-To: <4702645135092E4497088F71D9C8F51A128C4F@afhex01.dpi.wa.gov.au>
References: <4702645135092E4497088F71D9C8F51A128C4F@afhex01.dpi.wa.gov.au>
Message-ID: <Pine.LNX.4.61.0509291037330.14465@gannet.stats>

On Thu, 29 Sep 2005, Mulholland, Tom wrote:

> Well I downloaded the data using the link in your message which suggests 
> that the code is right. I don't have its loaded (I assume it's from the 
> irregular time series package)

The package is called its, as the message correctly said.

> so I can't test the code as you have it.

chart.yahoo.com is notoriously fickle (as we have seen from the tseries 
example using it).  Note that the first command (which gave me no error) 
also downloaded from there, so this is not going to be an access issue. 
I do get a 404 error on that URL (from two separate ISPs), but not if gdax 
is twice replaced by ftse.

I do think the posting guide makes clear this is an issue for the 
package maintainer, not for R-help.


>
> Have you tried checking to see it it is an issue with the internet (your 
> browser, blocked sites etc) rather than a problem in R?
>
> Tom
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Remigijus
>> Lapinskas
>> Sent: Thursday, 29 September 2005 12:31 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] priceIts
>>
>>
>> Dear All,
>>
>> There is an example for the priceIts function (the its package) which
>> does not work for me as expected.
>>
>> > ?priceIts
>> >  x1 <- priceIts(instrument = c("^ftse"), start = "1998-01-01",
>> +                          quote = "Close")
>> Error in validObject(.Object) : invalid class "its" object: Missing
>> values in dates
>> >      x2 <- priceIts(instrument = c("^gdax"), start = "1998-01-01",
>> +                          quote = "Close")
>> Error in download.file(url, destfile, method = method, quiet
>> = quiet) :
>>          cannot open URL
>> 'http://chart.yahoo.com/table.csv?s=^gdax&a=0&b=01&c=1998&d=8&
>> e=28&f=2005&g=d&q=q&y=0&z=^gdax&x=.csv'
>> In addition: Warning message:
>> cannot open: HTTP status was '404 Not Found'
>> >      x <- union(x1,x2)
>> Error: Object "x1" not found
>> Error in union(x1, x2) : unable to find the argument 'x' in
>> selecting a
>> method for function 'union'
>> >      names(x) <- c("FTSE","DAX")
>> >      plot(x,lab=TRUE)
>>
>> Any help appreciated.
>>
>> Best wishes,
>> Remigijus
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Thu Sep 29 11:58:09 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 29 Sep 2005 11:58:09 +0200
Subject: [R] multiple plots on same x axis
In-Reply-To: <20050927143453.13395.qmail@web86709.mail.ukl.yahoo.com>
Message-ID: <433BD6D1.19672.E867C9@localhost>

Hi

You can use e.g. plot - points construction

plot(Day, gene1)
points(Day, gene2, col=2)

see
?points
?lines

You also need to set appropriate y range by ylim argument to first 
plot. And if you used search facility in CRAN and asked it

multiple plots on same x axis

you would have got many other posibilities for your question.

HTH
Petr



On 27 Sep 2005 at 15:34, IAIN GALLAGHER wrote:

Date sent:      	Tue, 27 Sep 2005 15:34:53 +0100 (BST)
From:           	IAIN GALLAGHER <iaingallagher at btopenworld.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] multiple plots on same x axis

> Hi.
> 
> I have two vectors of gene expression for each of
> several days. I want to plot both vectors on the same
> plot for a visual representation of up versus down
> regulation. I've tried using add=T but that doesn't
> work. 
> 
> eg
> 
> >plot(Day, gene1)
> >plot(Day, gene2, add=T)
> 
> Any help would be appreciated.
> 
> Iain
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From vincent at 7d4.com  Thu Sep 29 12:01:47 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Thu, 29 Sep 2005 12:01:47 +0200
Subject: [R] is it possible to form matrix of matrices...and multiple
 arrays
In-Reply-To: <Pine.LNX.4.61.0509290955240.13451@gannet.stats>
References: <20050928191003.36972.qmail@web61224.mail.yahoo.com>
	<433B861A.8080501@7d4.com>
	<Pine.LNX.4.61.0509290955240.13451@gannet.stats>
Message-ID: <433BBB8B.5090305@7d4.com>

Prof Brian Ripley a ??crit :

> On Thu, 29 Sep 2005 vincent at 7d4.com wrote:
>> I don't think you can use matrix() to store other matrix() inside.
>> But array() is a solution to store matrix() inside.
>> (At least I have use it).
> 
> You _can_ do this with matrix() (although that was not quite what was 
> asked). Try
> a <- b <- c <- d <- matrix(1:4, 2, 2)
> e <- matrix(list(a,b,c,d), 2,2)

sorry for the wrong answer, and thanks for the correction.
(I certainly do not use list() as much as it should be.)



From HDoran at air.org  Thu Sep 29 12:12:55 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 29 Sep 2005 06:12:55 -0400
Subject: [R] how to fix the level-1 variances in lme()?
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407E41C8E@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050929/3ac9e69e/attachment.pl

From vincent at 7d4.com  Thu Sep 29 12:19:47 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Thu, 29 Sep 2005 12:19:47 +0200
Subject: [R] Plot Data Points in boxplots
In-Reply-To: <17211.38068.320699.512022@stat.math.ethz.ch>
References: <971536df0509271755308e5a33@mail.gmail.com>	<1551433AFCD0B35F3DD1C916@G4Desktop.local>	<433B2141.6030005@stat.auckland.ac.nz>	<433B89F2.5020201@7d4.com>
	<17211.38068.320699.512022@stat.math.ethz.ch>
Message-ID: <433BBFC3.6060603@7d4.com>

Martin Maechler a ??crit :

> To really thank Paul, you should buy the book to which the above
> web page refers :
>   http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html
> A very nice and useful book to have, indeed!

I just broked my piggy bank for Peter Dalgaard's one,
.... so I'll have to wait a bit before it blows out again.

(Peter Dalgaard
Introductory Statistics With R
http://www.amazon.fr/exec/obidos/ASIN/0387954759/qid=1127989069/sr=1-1/ref=sr_1_0_1/171-5704479-0348209
, also very useful as an introduction.)



From ym at climpact.com  Thu Sep 29 12:20:49 2005
From: ym at climpact.com (Yves Magliulo)
Date: 29 Sep 2005 12:20:49 +0200
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <433B9DE7.1070509@statisticon.se>
References: <25CAC7B3-0CC0-4425-A3E5-8DA97DE04BC3@globetrotter.net>
	<86AB3A9D-9A58-4D25-938C-3FA4CB08D94F@globetrotter.net>
	<1127919949.6577.97.camel@new-york.climpact.net>
	<433B9DE7.1070509@statisticon.se>
Message-ID: <1127989249.9620.86.camel@new-york.climpact.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050929/544e34c8/attachment.pl

From petr.pikal at precheza.cz  Thu Sep 29 12:35:46 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 29 Sep 2005 12:35:46 +0200
Subject: [R] boxplot and xlim confusion?
In-Reply-To: <ypx6zmpxjo8h.fsf@uracil.uio.no>
Message-ID: <433BDFA2.11358.10AD572@localhost>

Hi Karin

I did not have seen any answer for your question yet so here is a 
try.

I gues you want the horizotal layout or your boxplot.

boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal =T, 
names=letters[1:3])

boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal =T, 
names=c(NA,"b",NA))

So this is closest what I could dig from your text.

***reproducible*** example would by good starting point what you want 
and was not able to produce.

HTH
Petr




On 28 Sep 2005 at 15:50, Karin Lagesen wrote:

To:             	r-help at r-project.org
From:           	Karin Lagesen <karin.lagesen at medisin.uio.no>
Date sent:      	Wed, 28 Sep 2005 15:50:06 +0200
Subject:        	[R] boxplot and xlim confusion?

> 
> I have some code as shown below. Basically, I would like three
> boxplots to be set next to each other with no ylabels on the two
> "inner" plots, and I want the same x axis range on all three. However,
> it seems like boxplot does not respect the xlim setting. I've tried
> the various ways I thought would work (par, boxplot(...xlim=)) but
> none of them seem to work. I then tried plot.window, that did not
> work.
> 
> I also have another curious question for you. With the code below I
> tried to call plot.new and then plot.window before each new plot. What
> happens then is that the first figure goes on one page whereas the two
> others get put on the next page with a nice big gap in the middle.
> Does any of you have an explanation for that?
> 
> names <- c(
> "LSU>, stop",
> ">LSU, start",
> "SSU>, stop",
> ">SSU, start",
> "TSU>, stop",
> ">TSU, start")
> elsustop <- read.table("28s.euk.accuracy.stop.dev")
> 
> [skipped lots of read.table, which just reads files with one number on
> each line]
> 
> par(mfcol=c(1,3))
> par(mai = c(0,0,0.5,0.2), omi = c(1,1,1,1))
> xaxis = c(-6000,1000)
> yaxis = c(0,7)
> #plot.new()
> #plot.window(xlim=xaxis, ylim=yaxis)
> boxplot(alsustop$V1 ,alsustart$V1 ,assustop$V1 ,alsustart$V1
> ,atsustop$V1 ,atsustart$V1
> ,names=names,col=c("lightblue","orange","lightblue","orange","lightblu
> e","orange") ,horizontal = TRUE, main="ARC", xaxs = "i", las=1)
> #plot.new() #plot.window(xlim=xaxis, ylim=yaxis) boxplot(blsustop$V1
> ,blsustart$V1 ,bssustop$V1 ,blsustart$V1 ,btsustop$V1 ,btsustart$V1
> ,col=c("lightblue","orange","lightblue","orange","lightblue","orange")
> ,horizontal = TRUE, main="BAC", xaxs = "i") #plot.new()
> #plot.window(xlim=xaxis, ylim=yaxis) boxplot(elsustop$V1 ,elsustart$V1
> ,essustop$V1 ,elsustart$V1 ,etsustop$V1 ,etsustart$V1
> ,col=c("lightblue","orange","lightblue","orange","lightblue","orange")
> ,horizontal = TRUE, main="EUK", xaxs = "i")
> 
> Karin (sorry if you're getting sick of me..:))
> -- 
> Karin Lagesen, PhD student
> karin.lagesen at medisin.uio.no
> http://www.cmbn.no/rognes/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From quesada at gmail.com  Thu Sep 29 13:12:00 2005
From: quesada at gmail.com (Jose Quesada)
Date: Thu, 29 Sep 2005 12:12:00 +0100
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <20050216172043.2153.qmail@web25801.mail.ukl.yahoo.com>
References: <20050216172043.2153.qmail@web25801.mail.ukl.yahoo.com>
Message-ID: <21c05c7d050929041244c9f17b@mail.gmail.com>

Sorry to revive and old topic, but writing to the clipboard seems to
have a problem for me: column names are ignored. Example:

# ~~~~~~~~~~~~~~~~~~~~~~~
# write.clipboard
# ~~~~~~~~~~~~~~~~~~~~~~~
write.clipboard = function(obj) {
	write.table(obj, file("clipboard"), sep="\t", row.names=F, col.names=T)
}

a= matrix(1:4,2,2)
colnames(a) = c("a", "b")

write.clipboard(a)
a = as.data.frame(a)
write.clipboard(a)

both attempts will paste the date without column names.
Any idea why?

Thanks,
-Jose

On 2/16/05, Werner Wernersen <pensterfuzzer at yahoo.de> wrote:
> Thank you all very much for the answers!
> The read.table / read.delim2 commands are exactly what
> I was looking for to get
> a couple of numbers or a little matrix quickly into R
> without creating an extra
> text file every time.
>
> And it works the other way around as well:
> write.table(x, file("clipboard"), sep="\t")
> Fantastic!
>
> Thanks again,
>    Werner
>
> Nick Drew wrote:
> > I've had good luck with the scan() function when I
> > want to get a few numbers from Excel into R quickly
> to
> > use it as a calculator. CAVEAT: you have to have the
> > numbers you want to copy in a column not a row in
> > Excel. For example:
> >
> > In Excel your data are in a column as follows:
> > Col A
> > 1
> > 2
> > 3
> >
> > Then copy the 3 cells (e.g. 1, 2,3) in Excel and
> open
> > R and type in:
> >
> >>data <- scan()
> >
> >
> > Then Paste using Ctrl-V. Hit the Enter key. You know
> > have an object called "data" that you can use and
> > manipulate in R.
> >
> > I've taken this even further by creating an R
> function
> > that will take a column of numbers from Excel and
> then
> > scan() them into R, create a matrix, and then
> perform
> > a Chi-square test. Let me know if you'd like to know
> > more. I'm a beginner and if I can do so can you!!
> >
> > ~Nick
> >
> >
> >
> > 		
> > __________________________________
> > Do you Yahoo!?
>
>
> > http://promotions.yahoo.com/new_mail
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


--
Jose Quesada, PhD.

j.quesada at warwick.ac.uk				ESRC Postdoctoral Fellow
http://lsa.colorado.edu/~quesadaj		Dept. of PSychology
http://www.andrew.cmu.edu/~jquesada		University of Warwick
office H114						Phone: +44 024 765 23 759
Coventry, UK



From chrish at stats.ucl.ac.uk  Thu Sep 29 13:18:50 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Thu, 29 Sep 2005 12:18:50 +0100 (BST)
Subject: [R] Regression slope confidence interval
Message-ID: <Pine.LNX.4.58.0509291214060.14546@egon.stats.ucl.ac.uk>

Hi list,

is there any direct way to obtain confidence intervals for the regression
slope from lm, predict.lm or the like?
(If not, is there any reason? This is also missing in some other statistics
softwares, and I thought this would be quite a standard application.)
I know that it's easy to implement but it's for
explanation to people who faint if they have to do their own
programming...

Christian

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From ccleland at optonline.net  Thu Sep 29 13:25:59 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 29 Sep 2005 07:25:59 -0400
Subject: [R] Regression slope confidence interval
In-Reply-To: <Pine.LNX.4.58.0509291214060.14546@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.58.0509291214060.14546@egon.stats.ucl.ac.uk>
Message-ID: <433BCF47.9060900@optonline.net>

?confint

For example:

 > ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
 >      trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
 >      group <- gl(2,10,20, labels=c("Ctl","Trt"))
 >      weight <- c(ctl, trt)
 > lm(weight ~ group)

Call:
lm(formula = weight ~ group)

Coefficients:
(Intercept)     groupTrt
       5.032       -0.371

 > confint(lm(weight ~ group))
                 2.5 %    97.5 %
(Intercept)  4.569340 5.4946602
groupTrt    -1.025300 0.2833003

Christian Hennig wrote:
> Hi list,
> 
> is there any direct way to obtain confidence intervals for the regression
> slope from lm, predict.lm or the like?
> (If not, is there any reason? This is also missing in some other statistics
> softwares, and I thought this would be quite a standard application.)
> I know that it's easy to implement but it's for
> explanation to people who faint if they have to do their own
> programming...
> 
> Christian
> 
> *** --- ***
> Christian Hennig
> University College London, Department of Statistical Science
> Gower St., London WC1E 6BT, phone +44 207 679 1698
> chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From Soren.Hojsgaard at agrsci.dk  Thu Sep 29 13:28:47 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 29 Sep 2005 13:28:47 +0200
Subject: [R] Regression slope confidence interval
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC03725889@DJFPOST01.djf.agrsci.dk>

?confint


 

> -----Oprindelig meddelelse-----
> Fra: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] P?? vegne af Christian Hennig
> Sendt: 29. september 2005 13:19
> Til: r-help-request Mailing List
> Emne: [R] Regression slope confidence interval
> 
> Hi list,
> 
> is there any direct way to obtain confidence intervals for 
> the regression slope from lm, predict.lm or the like?
> (If not, is there any reason? This is also missing in some 
> other statistics softwares, and I thought this would be quite 
> a standard application.) I know that it's easy to implement 
> but it's for explanation to people who faint if they have to 
> do their own programming...
> 
> Christian
> 
> *** --- ***
> Christian Hennig
> University College London, Department of Statistical Science 
> Gower St., London WC1E 6BT, phone +44 207 679 1698 
> chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Achim.Zeileis at wu-wien.ac.at  Thu Sep 29 13:27:57 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 29 Sep 2005 13:27:57 +0200 (CEST)
Subject: [R] Regression slope confidence interval
In-Reply-To: <Pine.LNX.4.58.0509291214060.14546@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.58.0509291214060.14546@egon.stats.ucl.ac.uk>
Message-ID: <Pine.LNX.4.58.0509291326530.3869@thorin.ci.tuwien.ac.at>

On Thu, 29 Sep 2005, Christian Hennig wrote:

> Hi list,
>
> is there any direct way to obtain confidence intervals for the regression
> slope from lm, predict.lm or the like?

There is a confint method: e.g.,

R> fm <- lm(dist ~ speed, data = cars)
R> confint(fm, parm = "speed")
         2.5 %   97.5 %
speed 3.096964 4.767853

hth,
Z

> (If not, is there any reason? This is also missing in some other statistics
> softwares, and I thought this would be quite a standard application.)
> I know that it's easy to implement but it's for
> explanation to people who faint if they have to do their own
> programming...
>
> Christian
>
> *** --- ***
> Christian Hennig
> University College London, Department of Statistical Science
> Gower St., London WC1E 6BT, phone +44 207 679 1698
> chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From chrish at stats.ucl.ac.uk  Thu Sep 29 13:31:15 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Thu, 29 Sep 2005 12:31:15 +0100 (BST)
Subject: [R] Regression slope confidence interval
In-Reply-To: <433BCF47.9060900@optonline.net>
References: <Pine.LNX.4.58.0509291214060.14546@egon.stats.ucl.ac.uk>
	<433BCF47.9060900@optonline.net>
Message-ID: <Pine.LNX.4.58.0509291229090.14546@egon.stats.ucl.ac.uk>

> ?confint
>

Thank you to all of you.

As far as I see this is not mentioned on the lm help page (though I
presumably don't have the recent version), which I would
suggest...

Best,
Christian

On Thu, 29 Sep 2005, Chuck Cleland wrote:

> ?confint
>
> For example:
>
>  > ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
>  >      trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>  >      group <- gl(2,10,20, labels=c("Ctl","Trt"))
>  >      weight <- c(ctl, trt)
>  > lm(weight ~ group)
>
> Call:
> lm(formula = weight ~ group)
>
> Coefficients:
> (Intercept)     groupTrt
>        5.032       -0.371
>
>  > confint(lm(weight ~ group))
>                  2.5 %    97.5 %
> (Intercept)  4.569340 5.4946602
> groupTrt    -1.025300 0.2833003
>
> Christian Hennig wrote:
> > Hi list,
> >
> > is there any direct way to obtain confidence intervals for the regression
> > slope from lm, predict.lm or the like?
> > (If not, is there any reason? This is also missing in some other statistics
> > softwares, and I thought this would be quite a standard application.)
> > I know that it's easy to implement but it's for
> > explanation to people who faint if they have to do their own
> > programming...
> >
> > Christian
> >
> > *** --- ***
> > Christian Hennig
> > University College London, Department of Statistical Science
> > Gower St., London WC1E 6BT, phone +44 207 679 1698
> > chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From W.E.Wolski at ncl.ac.uk  Thu Sep 29 13:33:40 2005
From: W.E.Wolski at ncl.ac.uk (Witold Eryk Wolski)
Date: Thu, 29 Sep 2005 13:33:40 +0200
Subject: [R] Problem with memory footprint of qq plot generated
	with	lattice
In-Reply-To: <dhfact$g2l$1@sea.gmane.org>
References: <43396F07@webmail.ncl.ac.uk> <dhfact$g2l$1@sea.gmane.org>
Message-ID: <433BD114.10309@ncl.ac.uk>

Dave,

qqmath(~val|ind,data=xx
        ,distribution=function(p) qt(p,df=19)
        ,ylab="Sample Quatinles"
        ,xlab="Theoretical Quantiles"
        ,aspect=1
        ,prepanel = prepanel.qqmathline
        ,panel=function(x,y)
        {
          panel.qqmathline(y, distribution=function(p) qt(p,df=19),col=2)
          panel.qqmath(x, y , distribution=function(p)
qt(p,df=19),pch=".",cex=2)
        }
)

Adding f.value=fn as argument to qqmath reduces the size of the image, 
but neither the axis (absicissae) nor the line added by panel.qqmathline 
are right.

Adding f.value=fn as argument to panel.qqmathline and panel.qqmath 
generates the right graphic, but the size of the image is again 20 MB.

Any Suggestions?

Eryk

dhinds at sonic.net wrote:
> nwew <W.E.Wolski at newcastle.ac.uk> wrote:
> 
>>Dear R helpers,
> 
> 
>>I generate a qq plot using the following function call.
> 
> 
> ...
> 
> 
>>dim(xx)
>>[1] 680237      2
> 
> 
> How about doing something like this:
> 
> fn <- function(n,cut=0.001,m=1000)
> {
>     p <- ppoints(n)
>     p <- p[pmin(p, 1-p) < cut]
>     q <- pt(seq(qt(cut,df=19),qt(1-cut,df=19),length=m),df=19)
>     sort(c(p,q))
> }
> 
> then adding 'f.value=fn' to your qqmath arguments?  This essentially
> says, plot the individual data points in the extreme tails of the
> distribution (p < 0.001 or p > 0.999), and evaluate the distribution
> at a sparse set of points in between, where the density means you
> can't discern the individual values anyway.
> 
> -- Dave
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

From renaud.lancelot at cirad.fr  Thu Sep 29 13:37:15 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Thu, 29 Sep 2005 13:37:15 +0200
Subject: [R] Regression slope confidence interval
In-Reply-To: <Pine.LNX.4.58.0509291214060.14546@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.58.0509291214060.14546@egon.stats.ucl.ac.uk>
Message-ID: <433BD1EB.5030302@cirad.fr>

Why not use vcov() and the normal approximation ?

 > ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
 > trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
 > group <- gl(2,10,20, labels=c("Ctl","Trt"))
 > weight <- c(ctl, trt)
 > lm.D9 <- lm(weight ~ group)
 >
 > cbind(estimate = coef(lm.D9),
+       lower = coef(lm.D9) - 1.96 * diag(vcov(lm.D9)),
+       upper = coef(lm.D9) + 1.96 * diag(vcov(lm.D9)))
             estimate      lower      upper
(Intercept)    5.032  4.9369482  5.1270518
groupTrt      -0.371 -0.5611037 -0.1808963

To address your needs, it might also be possible to write a method for 
the generic of intervals() in package nlme.

Best,

Renaud

Christian Hennig a ??crit :
> Hi list,
> 
> is there any direct way to obtain confidence intervals for the regression
> slope from lm, predict.lm or the like?
> (If not, is there any reason? This is also missing in some other statistics
> softwares, and I thought this would be quite a standard application.)
> I know that it's easy to implement but it's for
> explanation to people who faint if they have to do their own
> programming...
> 
> Christian
> 
> *** --- ***
> Christian Hennig
> University College London, Department of Statistical Science
> Gower St., London WC1E 6BT, phone +44 207 679 1698
> chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Renaud Lancelot
Directeur Adjoint charg?? des Affaires Scientifiques
Deputy Director for Scientific Affairs

D??partement EMVT du CIRAD, TA 30/B
Campus International de Baillarguet
34398 Montpellier Cedex 5 - France
Tel.  +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95



From renaud.lancelot at cirad.fr  Thu Sep 29 13:56:25 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Thu, 29 Sep 2005 13:56:25 +0200
Subject: [R] Regression slope confidence interval
In-Reply-To: <Pine.LNX.4.58.0509291214060.14546@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.58.0509291214060.14546@egon.stats.ucl.ac.uk>
Message-ID: <433BD669.5060005@cirad.fr>

Sorry, I forgot confint and I made a mistake in my suggestion which 
should be:

cbind(estimate = coef(lm.D9),
       lower = coef(lm.D9) - 1.96 * sqrt(diag(vcov(lm.D9))),
       upper = coef(lm.D9) + 1.96 * sqrt(diag(vcov(lm.D9))))

Best,

Renaud

Christian Hennig a ??crit :
> Hi list,
> 
> is there any direct way to obtain confidence intervals for the regression
> slope from lm, predict.lm or the like?
> (If not, is there any reason? This is also missing in some other statistics
> softwares, and I thought this would be quite a standard application.)
> I know that it's easy to implement but it's for
> explanation to people who faint if they have to do their own
> programming...
> 
> Christian
> 
> *** --- ***
> Christian Hennig
> University College London, Department of Statistical Science
> Gower St., London WC1E 6BT, phone +44 207 679 1698
> chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Renaud Lancelot
Directeur Adjoint charg?? des Affaires Scientifiques
Deputy Director for Scientific Affairs

D??partement EMVT du CIRAD, TA 30/B
Campus International de Baillarguet
34398 Montpellier Cedex 5 - France
Tel.  +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95



From roger.bos at gmail.com  Thu Sep 29 14:00:00 2005
From: roger.bos at gmail.com (roger bos)
Date: Thu, 29 Sep 2005 08:00:00 -0400
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <21c05c7d050929041244c9f17b@mail.gmail.com>
References: <20050216172043.2153.qmail@web25801.mail.ukl.yahoo.com>
	<21c05c7d050929041244c9f17b@mail.gmail.com>
Message-ID: <1db726800509290500245daeb4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050929/10514053/attachment.pl

From roger.bos at gmail.com  Thu Sep 29 14:06:19 2005
From: roger.bos at gmail.com (roger bos)
Date: Thu, 29 Sep 2005 08:06:19 -0400
Subject: [R] memory issues with large data set
In-Reply-To: <C7CAAAC7D2D14C409367E8D9026C1D781B204E@inverness.buckcenter.org>
References: <C7CAAAC7D2D14C409367E8D9026C1D781B204E@inverness.buckcenter.org>
Message-ID: <1db7268005092905066334e200@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050929/5ef5ac0c/attachment.pl

From alanc at umit.maine.edu  Thu Sep 29 14:53:19 2005
From: alanc at umit.maine.edu (Alan Cobo-Lewis)
Date: Thu, 29 Sep 2005 08:53:19 -0400
Subject: [R] anova on binomial LMER objects
In-Reply-To: <mailman.10.1127988001.20586.r-help@stat.math.ethz.ch>
References: <mailman.10.1127988001.20586.r-help@stat.math.ethz.ch>
Message-ID: <fc.004c4d1922c7bd3b3b9aca00ed2667fb.22c7c915@umit.maine.edu>

On Wed, 28 Sep 2005, Robert Bagchi wrote:
>>Hi Patrick
>>
>>thanks for your advice. I have now tried glmmPQL, and it worked fine - 
>>I'm getting consistent results between plots and models fitted by 
>>glmmPQL. Plus it allows predict() and resid() which is another advantage 
>>over lmer at present.
>>
>>quick question though: why does one need to use PQL for binomial models? 
>>Is there a good reference for this?
>
You don't have to use PQL for binomial models, but you can't use least-squares. PQL is an approximate solution. Laplace and Adaptive Gaussian Quadrature options in lmer are better approximations. So lmer would likely become the better option as it
progresses in its development (though the current issues you've found with the F ratios certainly sound like maybe lmer isn't better for you in its current incarnation).
alan

--
Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
Department of Psychology		(207) 581-6128 fax
University of Maine
Orono, ME 04469-5742     		alanc at maine.edu

http://www.umaine.edu/visualperception



From ggrothendieck at gmail.com  Thu Sep 29 15:01:48 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Sep 2005 09:01:48 -0400
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <21c05c7d050929041244c9f17b@mail.gmail.com>
References: <20050216172043.2153.qmail@web25801.mail.ukl.yahoo.com>
	<21c05c7d050929041244c9f17b@mail.gmail.com>
Message-ID: <971536df050929060117c919b9@mail.gmail.com>

See:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/26922.html

On 9/29/05, Jose Quesada <quesada at gmail.com> wrote:
> Sorry to revive and old topic, but writing to the clipboard seems to
> have a problem for me: column names are ignored. Example:
>
> # ~~~~~~~~~~~~~~~~~~~~~~~
> # write.clipboard
> # ~~~~~~~~~~~~~~~~~~~~~~~
> write.clipboard = function(obj) {
>        write.table(obj, file("clipboard"), sep="\t", row.names=F, col.names=T)
> }
>
> a= matrix(1:4,2,2)
> colnames(a) = c("a", "b")
>
> write.clipboard(a)
> a = as.data.frame(a)
> write.clipboard(a)
>
> both attempts will paste the date without column names.
> Any idea why?
>
> Thanks,
> -Jose
>
> On 2/16/05, Werner Wernersen <pensterfuzzer at yahoo.de> wrote:
> > Thank you all very much for the answers!
> > The read.table / read.delim2 commands are exactly what
> > I was looking for to get
> > a couple of numbers or a little matrix quickly into R
> > without creating an extra
> > text file every time.
> >
> > And it works the other way around as well:
> > write.table(x, file("clipboard"), sep="\t")
> > Fantastic!
> >
> > Thanks again,
> >    Werner
> >
> > Nick Drew wrote:
> > > I've had good luck with the scan() function when I
> > > want to get a few numbers from Excel into R quickly
> > to
> > > use it as a calculator. CAVEAT: you have to have the
> > > numbers you want to copy in a column not a row in
> > > Excel. For example:
> > >
> > > In Excel your data are in a column as follows:
> > > Col A
> > > 1
> > > 2
> > > 3
> > >
> > > Then copy the 3 cells (e.g. 1, 2,3) in Excel and
> > open
> > > R and type in:
> > >
> > >>data <- scan()
> > >
> > >
> > > Then Paste using Ctrl-V. Hit the Enter key. You know
> > > have an object called "data" that you can use and
> > > manipulate in R.
> > >
> > > I've taken this even further by creating an R
> > function
> > > that will take a column of numbers from Excel and
> > then
> > > scan() them into R, create a matrix, and then
> > perform
> > > a Chi-square test. Let me know if you'd like to know
> > > more. I'm a beginner and if I can do so can you!!
> > >
> > > ~Nick
> > >
> > >
> > >
> > >
> > > __________________________________
> > > Do you Yahoo!?
> >
> >
> > > http://promotions.yahoo.com/new_mail
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>
> --
> Jose Quesada, PhD.
>
> j.quesada at warwick.ac.uk                         ESRC Postdoctoral Fellow
> http://lsa.colorado.edu/~quesadaj               Dept. of PSychology
> http://www.andrew.cmu.edu/~jquesada             University of Warwick
> office H114                                             Phone: +44 024 765 23 759
> Coventry, UK
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dejongroel at gmail.com  Thu Sep 29 15:19:38 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Thu, 29 Sep 2005 15:19:38 +0200
Subject: [R] standard error of variances and covariances of the random
 effects with LME
Message-ID: <433BE9EA.5080003@gmail.com>

Hello,

how do I obtain standard errors of variances and covariances of the 
random effects with LME comparable to those of for example MlWin? I know 
you shouldn't use them because the distribution of the estimator isn't 
symmetric blablabla, but I need a measure of the variance of those 
estimates for pooling my multiple imputation results.

Regards,
   Roel.



From karin.lagesen at medisin.uio.no  Thu Sep 29 15:16:02 2005
From: karin.lagesen at medisin.uio.no (Karin Lagesen)
Date: Thu, 29 Sep 2005 15:16:02 +0200
Subject: [R] reshaping data?
Message-ID: <ypx6hdc4j9pp.fsf@uracil.uio.no>


I have a file like this:


a       0.1
a       0.2
a       0.9
b       0.5
b       0.9
b       0.7
c       0.6
c       0.99
c       0.88

Which I would like to get to be the following matrix:

      0.1     0.2    0.3    0.4  ...  
a     1        2     0       0 
b     0        0     0       0 
..

I.e: each place in the matrix denotes how many entries in each
category that are betwee 0.0 and 0.1, 0.1 and 0.2 and so on.

The way I was thinking of doing it was by constructing an empty matrix
and then doing a for loop testing each element and incrementing in the
matrix as appropriate. However, it struck me that this has to be
easier to do than that. Am I right?

Karin
-- 
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://www.cmbn.no/rognes/



From HDoran at air.org  Thu Sep 29 15:24:52 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 29 Sep 2005 09:24:52 -0400
Subject: [R] standard error of variances and covariances of the random
	effects with LME
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A31C81F@dc1ex2.air.org>

You cannot. Also, it's not that the distribution of the random effects
is not symmetric, but that it *may* not be symmetric, and this is an
assumption that should be checked. 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roel de Jong
Sent: Thursday, September 29, 2005 9:20 AM
To: r-help at stat.math.ethz.ch
Subject: [R] standard error of variances and covariances of the random
effects with LME

Hello,

how do I obtain standard errors of variances and covariances of the
random effects with LME comparable to those of for example MlWin? I know
you shouldn't use them because the distribution of the estimator isn't
symmetric blablabla, but I need a measure of the variance of those
estimates for pooling my multiple imputation results.

Regards,
   Roel.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From karin.lagesen at medisin.uio.no  Thu Sep 29 15:28:14 2005
From: karin.lagesen at medisin.uio.no (Karin Lagesen)
Date: Thu, 29 Sep 2005 15:28:14 +0200
Subject: [R] boxplot and xlim confusion?
In-Reply-To: <433BDFA2.11358.10AD572@localhost> (Petr Pikal's message of
	"Thu, 29 Sep 2005 12:35:46 +0200")
References: <433BDFA2.11358.10AD572@localhost>
Message-ID: <ypx67jd0j95d.fsf@uracil.uio.no>

"Petr Pikal" <petr.pikal at precheza.cz> writes:

> Hi Karin
>
> I did not have seen any answer for your question yet so here is a 
> try.
>
> I gues you want the horizotal layout or your boxplot.
>
> boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal =T, 
> names=letters[1:3])
>
> boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal =T, 
> names=c(NA,"b",NA))

These look like the plots I want, yes. However, is there a way of
locking the x-axis? When I do these several times in a row, the range
of the x axis moves with the data being plotted. If I set xlim in the
boxplot command, it makes no difference what so ever.


> So this is closest what I could dig from your text.
>
> ***reproducible*** example would by good starting point what you want 
> and was not able to produce.

Sorry about that...:)

Karin
-- 
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://www.cmbn.no/rognes/



From dimitris.rizopoulos at med.kuleuven.be  Thu Sep 29 16:00:55 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 29 Sep 2005 16:00:55 +0200
Subject: [R] reshaping data?
References: <ypx6hdc4j9pp.fsf@uracil.uio.no>
Message-ID: <007601c5c4fe$36229320$0540210a@www.domain>

you could use something like this (but maybe there are better 
proposals):

dat <- data.frame(g = rep(letters[1:3], each = 5), val = runif(15))

out <- do.call(rbind, lapply(split(dat$val, dat$g), function(x){
    f <- factor(findInterval(x, vec = seq(0, 1, 0.1)), levels = 1:10)
    table(f)
    }))
colnames(out) <- seq(0.1, 1, 0.1)
out


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Karin Lagesen" <karin.lagesen at medisin.uio.no>
To: <r-help at r-project.org>
Sent: Thursday, September 29, 2005 3:16 PM
Subject: [R] reshaping data?


>
> I have a file like this:
>
>
> a       0.1
> a       0.2
> a       0.9
> b       0.5
> b       0.9
> b       0.7
> c       0.6
> c       0.99
> c       0.88
>
> Which I would like to get to be the following matrix:
>
>      0.1     0.2    0.3    0.4  ...
> a     1        2     0       0
> b     0        0     0       0
> ..
>
> I.e: each place in the matrix denotes how many entries in each
> category that are betwee 0.0 and 0.1, 0.1 and 0.2 and so on.
>
> The way I was thinking of doing it was by constructing an empty 
> matrix
> and then doing a for loop testing each element and incrementing in 
> the
> matrix as appropriate. However, it struck me that this has to be
> easier to do than that. Am I right?
>
> Karin
> -- 
> Karin Lagesen, PhD student
> karin.lagesen at medisin.uio.no
> http://www.cmbn.no/rognes/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From tlumley at u.washington.edu  Thu Sep 29 16:05:16 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 29 Sep 2005 07:05:16 -0700 (PDT)
Subject: [R] reshaping data?
In-Reply-To: <ypx6hdc4j9pp.fsf@uracil.uio.no>
References: <ypx6hdc4j9pp.fsf@uracil.uio.no>
Message-ID: <Pine.LNX.4.63a.0509290703030.6956@homer21.u.washington.edu>

On Thu, 29 Sep 2005, Karin Lagesen wrote:

>
> I have a file like this:
>
>
> a       0.1
> a       0.2
> a       0.9
> b       0.5
> b       0.9
> b       0.7
> c       0.6
> c       0.99
> c       0.88
>
> Which I would like to get to be the following matrix:
>
>      0.1     0.2    0.3    0.4  ...
> a     1        2     0       0
> b     0        0     0       0
> ..
>
> I.e: each place in the matrix denotes how many entries in each
> category that are betwee 0.0 and 0.1, 0.1 and 0.2 and so on.
>

If your variables are called, say, id and value, and value is between 0 
and 1 then

table(id,cut(value,breaks=(0:10)/10))

works.

 	-thomas



From Eva.Cantoni at metri.unige.ch  Thu Sep 29 16:06:01 2005
From: Eva.Cantoni at metri.unige.ch (Eva Cantoni)
Date: Thu, 29 Sep 2005 16:06:01 +0200
Subject: [R] New family for gam in the mgcv library
Message-ID: <433BF4C9.9040304@metri.unige.ch>

Hi!

I'm using R 2.0.1 on a Sun, with mgcv library version 1.3-1.

I would like to implement a new family for the function gam in mgcv 
(truncated Poisson family as defined in Barry & Welsh (2002), Ecological 
Modelling).

I therefore defined a family function with all the necessary components 
(linkfun, linkinv, variance, etc). But then I run into problems because 
my link function is not a standard one, and fix.family.link() and 
fix.family.var() (called by gam.outer) won't work. As I understand, the 
purpose of these two functions is to modify families so to add a "dvar" 
component and a "d2link" component, necessary for the fitting.

The fix I can imagine (but I didn't try yet) is to add the two extra 
components (dvar and d2link) when defining the new family, and to 
redefine the function gam.outer to avoid the use of fix.family.*.

Does someone have experience with this? Would this be the end of my 
troubles?

Thank you for your help,
Eva Cantoni




-- 

  Dr Eva Cantoni                 phone  : (+41) 22 379 8240
  Econom??trie - Univ. Gen??ve     fax    : (+41) 22 379 8299
  40, Bd du Pont d'Arve          e-mail : Eva.Cantoni at metri.unige.ch
  CH-1211 Gen??ve 4               http://www.unige.ch/ses/metri/cantoni



From ggrothendieck at gmail.com  Thu Sep 29 16:09:45 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Sep 2005 10:09:45 -0400
Subject: [R] reshaping data?
In-Reply-To: <007601c5c4fe$36229320$0540210a@www.domain>
References: <ypx6hdc4j9pp.fsf@uracil.uio.no>
	<007601c5c4fe$36229320$0540210a@www.domain>
Message-ID: <971536df050929070927734014@mail.gmail.com>

This can be shortened slightly using cut:

table(data.frame(g = dat$g, val = cut(dat$val, 0:10/10, lab = 1:10/10)))

On 9/29/05, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be> wrote:
> you could use something like this (but maybe there are better
> proposals):
>
> dat <- data.frame(g = rep(letters[1:3], each = 5), val = runif(15))
>
> out <- do.call(rbind, lapply(split(dat$val, dat$g), function(x){
>    f <- factor(findInterval(x, vec = seq(0, 1, 0.1)), levels = 1:10)
>    table(f)
>    }))
> colnames(out) <- seq(0.1, 1, 0.1)
> out
>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://www.med.kuleuven.be/biostat/
>     http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
> ----- Original Message -----
> From: "Karin Lagesen" <karin.lagesen at medisin.uio.no>
> To: <r-help at r-project.org>
> Sent: Thursday, September 29, 2005 3:16 PM
> Subject: [R] reshaping data?
>
>
> >
> > I have a file like this:
> >
> >
> > a       0.1
> > a       0.2
> > a       0.9
> > b       0.5
> > b       0.9
> > b       0.7
> > c       0.6
> > c       0.99
> > c       0.88
> >
> > Which I would like to get to be the following matrix:
> >
> >      0.1     0.2    0.3    0.4  ...
> > a     1        2     0       0
> > b     0        0     0       0
> > ..
> >
> > I.e: each place in the matrix denotes how many entries in each
> > category that are betwee 0.0 and 0.1, 0.1 and 0.2 and so on.
> >
> > The way I was thinking of doing it was by constructing an empty
> > matrix
> > and then doing a for loop testing each element and incrementing in
> > the
> > matrix as appropriate. However, it struck me that this has to be
> > easier to do than that. Am I right?
> >
> > Karin
> > --
> > Karin Lagesen, PhD student
> > karin.lagesen at medisin.uio.no
> > http://www.cmbn.no/rognes/
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Thu Sep 29 16:22:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Sep 2005 10:22:35 -0400
Subject: [R] reshaping data?
In-Reply-To: <971536df050929070927734014@mail.gmail.com>
References: <ypx6hdc4j9pp.fsf@uracil.uio.no>
	<007601c5c4fe$36229320$0540210a@www.domain>
	<971536df050929070927734014@mail.gmail.com>
Message-ID: <971536df0509290722763f351a@mail.gmail.com>

And slightly more to:

table(g = dat$g, val = cut(dat$val, 0:10/10, lab = 1:10/10))

On 9/29/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> This can be shortened slightly using cut:
>
> table(data.frame(g = dat$g, val = cut(dat$val, 0:10/10, lab = 1:10/10)))
>
> On 9/29/05, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be> wrote:
> > you could use something like this (but maybe there are better
> > proposals):
> >
> > dat <- data.frame(g = rep(letters[1:3], each = 5), val = runif(15))
> >
> > out <- do.call(rbind, lapply(split(dat$val, dat$g), function(x){
> >    f <- factor(findInterval(x, vec = seq(0, 1, 0.1)), levels = 1:10)
> >    table(f)
> >    }))
> > colnames(out) <- seq(0.1, 1, 0.1)
> > out
> >
> >
> > I hope it helps.
> >
> > Best,
> > Dimitris
> >
> > ----
> > Dimitris Rizopoulos
> > Ph.D. Student
> > Biostatistical Centre
> > School of Public Health
> > Catholic University of Leuven
> >
> > Address: Kapucijnenvoer 35, Leuven, Belgium
> > Tel: +32/(0)16/336899
> > Fax: +32/(0)16/337015
> > Web: http://www.med.kuleuven.be/biostat/
> >     http://www.student.kuleuven.be/~m0390867/dimitris.htm
> >
> >
> > ----- Original Message -----
> > From: "Karin Lagesen" <karin.lagesen at medisin.uio.no>
> > To: <r-help at r-project.org>
> > Sent: Thursday, September 29, 2005 3:16 PM
> > Subject: [R] reshaping data?
> >
> >
> > >
> > > I have a file like this:
> > >
> > >
> > > a       0.1
> > > a       0.2
> > > a       0.9
> > > b       0.5
> > > b       0.9
> > > b       0.7
> > > c       0.6
> > > c       0.99
> > > c       0.88
> > >
> > > Which I would like to get to be the following matrix:
> > >
> > >      0.1     0.2    0.3    0.4  ...
> > > a     1        2     0       0
> > > b     0        0     0       0
> > > ..
> > >
> > > I.e: each place in the matrix denotes how many entries in each
> > > category that are betwee 0.0 and 0.1, 0.1 and 0.2 and so on.
> > >
> > > The way I was thinking of doing it was by constructing an empty
> > > matrix
> > > and then doing a for loop testing each element and incrementing in
> > > the
> > > matrix as appropriate. However, it struck me that this has to be
> > > easier to do than that. Am I right?
> > >
> > > Karin
> > > --
> > > Karin Lagesen, PhD student
> > > karin.lagesen at medisin.uio.no
> > > http://www.cmbn.no/rognes/
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> > Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From mschwartz at mn.rr.com  Thu Sep 29 16:34:06 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 29 Sep 2005 09:34:06 -0500
Subject: [R] boxplot and xlim confusion?
In-Reply-To: <ypx67jd0j95d.fsf@uracil.uio.no>
References: <433BDFA2.11358.10AD572@localhost> <ypx67jd0j95d.fsf@uracil.uio.no>
Message-ID: <1128004446.5402.7.camel@localhost.localdomain>

On Thu, 2005-09-29 at 15:28 +0200, Karin Lagesen wrote:
> "Petr Pikal" <petr.pikal at precheza.cz> writes:
> 
> > Hi Karin
> >
> > I did not have seen any answer for your question yet so here is a 
> > try.
> >
> > I gues you want the horizotal layout or your boxplot.
> >
> > boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal =T, 
> > names=letters[1:3])
> >
> > boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal =T, 
> > names=c(NA,"b",NA))
> 
> These look like the plots I want, yes. However, is there a way of
> locking the x-axis? When I do these several times in a row, the range
> of the x axis moves with the data being plotted. If I set xlim in the
> boxplot command, it makes no difference what so ever.

That's because when you use 'horizontal = TRUE' in boxplot(), the x and
y axes are rotated. Thus you need to set 'ylim' and not 'xlim', where
ylim is the range of values in your data irrespective of the axis
orientation.

This is done in bxp(), which is the actual function doing the plotting
for boxplot(). In bxp(), there is the follow code snippet:

        if (horizontal) 
            plot.window(ylim = c(0.5, n + 0.5), xlim = ylim, 
                        log = log)
        else plot.window(xlim = c(0.5, n + 0.5), ylim = ylim, 
                         log = log)

Note in the first case, that the xlim value for plot.window() is set to
the ylim value passed from boxplot() or from bxp(), if called directly.

So, the above examples by Petr should be something like:

boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal = TRUE, 
        names=letters[1:3], ylim = c(-4, 4))

boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal =TRUE, 
        names=c(NA,"b",NA), ylim = c(-4, 4))


HTH,

Marc Schwartz



From raphael.bolze at ens-lyon.fr  Thu Sep 29 16:42:20 2005
From: raphael.bolze at ens-lyon.fr (Raphael Bolze)
Date: Thu, 29 Sep 2005 16:42:20 +0200
Subject: [R] R-2.1.1 on AIX 5.*
Message-ID: <200509291642.20698.raphael.bolze@ens-lyon.fr>

Hi,

I need to install R-2.1.1 on a AIX machine. (i have tried several machine with 
different os version : 5.1, 5.2 and 5.3)
I have tried several configuration which was advise on the R-install and admin 
section :
but none goes to the end of compilation.

my configure command line is :
#>./configure --prefix=$HOME/local/R-2.1.1 CC=xlc_r CXX=xlC_r F77=xlf_r 
--without-x --without-readline OBJECT_MODE=64 LDFLAGS="-brtl" CFLAGS="-O 
-qstrict" FFLAGS="-O -qstrict" CXXFLAGS="-O -qstrict" 

then i compiled the source :
#>make
and i have the following error.
making vsnprintf.d from vsnprintf.c
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c Rmain.c -o Rmain.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c CConverters.c -o 
CConverters.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c CommandLineArgs.c -o 
CommandLin
eArgs.o
"CommandLineArgs.c", line 170.32: 1506-1298 (W) The subscript 31 is out of 
range. The valid range is 0 to 30.
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c Rdynload.c -o 
Rdynload.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c Renviron.c -o 
Renviron.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c RNG.c -o RNG.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c apply.c -o apply.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c arithmetic.c -o 
arithmetic.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c apse.c -o apse.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c array.c -o array.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c attrib.c -o attrib.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c base.c -o base.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c bind.c -o bind.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c builtin.c -o builtin.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c character.c -o 
character.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c coerce.c -o coerce.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c colors.c -o colors.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c complex.c -o complex.o
        xlc_r -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H   -O -qstrict -c connections.c -o 
connections.o
"connections.c", line 2580.18: 1506-052 (S) Duplicate case label for value 4. 
Labels must be unique.
"connections.c", line 2598.18: 1506-052 (S) Duplicate case label for value 4. 
Labels must be unique.
"connections.c", line 2757.18: 1506-052 (S) Duplicate case label for value 4. 
Labels must be unique.
"connections.c", line 2801.18: 1506-052 (S) Duplicate case label for value 4. 
Labels must be unique.
make: 1254-004 The error code from the last command is 1.


Stop.
make: 1254-004 The error code from the last command is 2.


Stop.
make: 1254-004 The error code from the last command is 1.


Stop.
make: 1254-004 The error code from the last command is 1.


Stop.


Anyone can help me !
Thanks for you help.

Rapha??l BOLZE



From je_lemaitre at hotmail.com  Thu Sep 29 16:42:25 2005
From: je_lemaitre at hotmail.com (=?iso-8859-1?B?Suly9G1lIExlbWHudHJl?=)
Date: Thu, 29 Sep 2005 10:42:25 -0400
Subject: [R] Repeated measure Generalized Linear Mixed Model (glmm) ?
Message-ID: <BAY103-DAV73D703964D1A454022CFB908C0@phx.gbl>

Dear all,

I want to know the probability of dying in a trap as a function of habitat
variables for a metapopulation of voles sampled in 44 stations. 
My dataset is as follow:
Station		tag number	dead	habitat1	habitat2
1		1		yes	20		26
1		2		no	20		26
2		3		no	15		16
..

As far as I know, I should use a mixed-model as:
glmmPQL<-(fixed = dead~habitat1+habitat2, random = ~1|station, family =
binomial) in the MASS library.
However, some vole individuals were recaptured (but it is not a study
designed for capture-mark-recapture!!!) therefore, I have for example, the
4th vole:
Station		tag number	dead	habitat1	habitat2
3		4		no	10		12
3		4		yes	10		12

Someone suggested me to use glmm with repeated measures with the number of
tag as the variable to repeat. Apparently, it is possible to do this in SAS.

QUESTIONS : 
1) Is this also possible to do this in R, and if yes, could you please tell
me how?
2) Is this a valid way to analyse this data, and if not, could someone
please put me in the right way?


Thank you very very much to all in advance

J??r??me Lema??tre

??tudiant au doctorat
Universit?? Laval
Qu??bec, QC  G1K 7P4



From David.Hannersjo at vabr.slu.se  Thu Sep 29 16:49:07 2005
From: David.Hannersjo at vabr.slu.se (=?iso-8859-1?Q?David_Hannersj=F6?=)
Date: Thu, 29 Sep 2005 16:49:07 +0200
Subject: [R] Cox regression on interval censoring
Message-ID: <CA871298CD1882459F7859BD08DC06E401BE8B14@slumail.ad.slu.se>

Regarding non-parametric regression with interval censored survival data: Does anyone know where to find an extension dealing with Cox regression for (overlapping) interval censored data? If not, maybe someone has some ML expressions for this interval scenario laying around that you are willing to share? I would be really really greatful!

Thanks and best regards // David

*************************************************
D. Hannersjoe
Department of Aquaculture
Swedish University of Agricultural Science
SWEDEN
E-mail: david.hannersjo at vabr.slu.se

*************************************************
###########################################

This message has been scanned by F-Secure Anti-Virus for Mic...{{dropped}}



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Sep 29 17:03:24 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 29 Sep 2005 11:03:24 -0400
Subject: [R] Fast AUC computation
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD503D35644@us-arlington-0668.mail.saic.com>

See colAUC in caTools (there is a problem with 1.3 version, 1.4 is on the
way). See "examples" for other functions calculating AUC. An alternative
approach is:

	  x1 = x[y==1]; n1 = length(x1); 
	  x2 = x[y==0]; n2 = length(x2);
	  r = rank(c(x1,x2))  
        auc = (sum(r[1:n1]) - n1*(n1+1)/2) / (n1*n2) 

Which is very fast.

Jarek 
====================================================\==== 
 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">  \ 
 Jaroslaw.W.Tuszynski at saic.com                     `   \ 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nina Paynter
Sent: Wednesday, September 28, 2005 4:43 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Fast AUC computation

I am doing a simulation with a relatively large data set (20,000
observations) for which I want to calculate the area under the Receiver
Operator Curve (AUC) for many parameter combinations.  I am using the ROC
library and the following commands to generate each AUC:

 

rocobj=rocdemo.sca(truth = ymis, data = model$fitted.values, rule =
dxrule.sca) #generation of observed ROC object

aucobj=AUC(rocobj) #pulling out just the observed AUC - trapezoidal not
integrated

 

but they are pretty slow.

 

Does anyone know of a faster way to get the AUC?

 

Thanks,

Nina


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From cniharral_rhelp at yahoo.es  Thu Sep 29 17:04:55 2005
From: cniharral_rhelp at yahoo.es (C NL)
Date: Thu, 29 Sep 2005 17:04:55 +0200 (CEST)
Subject: [R] Fisher's discriminant functions
Message-ID: <20050929150455.24592.qmail@web25411.mail.ukl.yahoo.com>

Hi everyone,

  I'm trying to solve a problem about how to get the
Fisher's discriminant functions of a "lda" (linear
discriminant analysis) object, I mean, the object
obtained from doing "lda(formula, data)" function of
the package MASS in R-project. This object gives me
the canonical linear functions (n-1 coefficients
matrix of n groups at least), and only with this
information I could predict the group of an
observation data using the "predict" function. But
what I need is the Fisher's discriminant functions (n
coefficients matrix of n groups) in order to classify
my future data.

   The object "predict" gives me only the following
attributes "x", "posterior" and "class", but none of
them are the coefficients matrix of the Fisher's
discriminant functions, and the reason why I'm not
using the "predict" function for my predictions is
because the time spent is very high for what I'm
expecting, about 0.5 seconds while I can obtain this
prediction with the Fisher's discriminant functions
faster.

   So, I don't know if there's a package which I can
use to obtain the mentioned coefficients matrix of the
Fisher's discriminant functions.

   I anyone can help, I would appreciate it greatly.

Thank you and regards.

   Carlos Niharra L??pez



From calj at mpia-hd.mpg.de  Thu Sep 29 17:24:52 2005
From: calj at mpia-hd.mpg.de (Coryn Bailer-Jones)
Date: Thu, 29 Sep 2005 17:24:52 +0200 (MEST)
Subject: [R] Error using a data frame as the "start" parameter in mle()
Message-ID: <Pine.GSO.4.52.0509291658190.20685@sun49>

Dear R-Users,

I am trying to use mle() to optimize two (or more) parameters, but I want
to specify those parmeters in a data frame rather than having to spell
them out separately in the "start" variable of mle().

My call is

> mle(negll, start=list(aps=init), fixed=list(measphot=newphot,
    formod=formod, Nbands=Nbands), method="BFGS")

where negll is a function I have written which uses the function
predict.loess(). negll works fine when called directly. The parameter I am
trying to optimize, "aps", is a data frame containing two parameters, e.g.

> init
  teff logg
1 8000  4.5

When I run mle I get the following error message

Error in predict.loess(formod[[band]], aps) :
        Argument "aps" is missing, with no default

As negll does work fine, I presume I am incorrectly passing "aps" into
mle(). Note that mle() works fine if I rewrite negll to work on a scalar
"aps" and then I use start=list(aps=500), for example. Can anyone help me
with this?

Incidentally, I am only using a data frame for "aps" because I am using
loess(), and this seems to require a formula with named variables in a
data frame (here "logg" and "teff"). I can't get it work with arrays:

> temp <- loess(formula = photd[, band] ~ gridaps[, 1] * gridaps[, 2])
> predict(temp, c(4,8000))
Error in predict.loess(temp, c(4, 8000)) :
        newdata does not contain the variables needed

Thanks in advance for any clues.

Coryn.

---------------------------------------------------------------------------
Coryn Bailer-Jones                    calj at mpia-hd.mpg.de
Max-Planck-Institut fuer Astronomie   http://www.mpia-hd.mpg.de/homes/calj/
Koenigstuhl 17                        tel: +49 6221 528-224        (direct)
D-69117 Heidelberg                         +49 6221 528-0       (reception)
Germany                               fax: +49 6221 528-246



From petr.pikal at precheza.cz  Thu Sep 29 17:29:55 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 29 Sep 2005 17:29:55 +0200
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <21c05c7d050929041244c9f17b@mail.gmail.com>
References: <20050216172043.2153.qmail@web25801.mail.ukl.yahoo.com>
Message-ID: <433C2493.12372.218257A@localhost>

Hi

Works for me.

write.excel <- function (tab, ...) write.table(tab, "clipboard", sep 
= "\t", row.names = F)

write.excel(a)

from your example shows in Excel after ctrl-V as a table with names.

HTH

Petr





On 29 Sep 2005 at 12:12, Jose Quesada wrote:

Date sent:      	Thu, 29 Sep 2005 12:12:00 +0100
From:           	Jose Quesada <quesada at gmail.com>
To:             	Werner Wernersen <pensterfuzzer at yahoo.de>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] Easy cut & paste from Excel to R?
Send reply to:  	Jose Quesada <quesada at gmail.com>
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Sorry to revive and old topic, but writing to the clipboard seems to
> have a problem for me: column names are ignored. Example:
> 
> # ~~~~~~~~~~~~~~~~~~~~~~~
> # write.clipboard
> # ~~~~~~~~~~~~~~~~~~~~~~~
> write.clipboard = function(obj) {
>  write.table(obj, file("clipboard"), sep="\t", row.names=F,
>  col.names=T)
> }
> 
> a= matrix(1:4,2,2)
> colnames(a) = c("a", "b")
> 
> write.clipboard(a)
> a = as.data.frame(a)
> write.clipboard(a)
> 
> both attempts will paste the date without column names.
> Any idea why?
> 
> Thanks,
> -Jose
> 
> On 2/16/05, Werner Wernersen <pensterfuzzer at yahoo.de> wrote:
> > Thank you all very much for the answers!
> > The read.table / read.delim2 commands are exactly what
> > I was looking for to get
> > a couple of numbers or a little matrix quickly into R
> > without creating an extra
> > text file every time.
> >
> > And it works the other way around as well:
> > write.table(x, file("clipboard"), sep="\t")
> > Fantastic!
> >
> > Thanks again,
> >    Werner
> >
> > Nick Drew wrote:
> > > I've had good luck with the scan() function when I
> > > want to get a few numbers from Excel into R quickly
> > to
> > > use it as a calculator. CAVEAT: you have to have the
> > > numbers you want to copy in a column not a row in
> > > Excel. For example:
> > >
> > > In Excel your data are in a column as follows:
> > > Col A
> > > 1
> > > 2
> > > 3
> > >
> > > Then copy the 3 cells (e.g. 1, 2,3) in Excel and
> > open
> > > R and type in:
> > >
> > >>data <- scan()
> > >
> > >
> > > Then Paste using Ctrl-V. Hit the Enter key. You know
> > > have an object called "data" that you can use and
> > > manipulate in R.
> > >
> > > I've taken this even further by creating an R
> > function
> > > that will take a column of numbers from Excel and
> > then
> > > scan() them into R, create a matrix, and then
> > perform
> > > a Chi-square test. Let me know if you'd like to know
> > > more. I'm a beginner and if I can do so can you!!
> > >
> > > ~Nick
> > >
> > >
> > >
> > > 		
> > > __________________________________
> > > Do you Yahoo!?
> >
> >
> > > http://promotions.yahoo.com/new_mail
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> 
> --
> Jose Quesada, PhD.
> 
> j.quesada at warwick.ac.uk				ESRC Postdoctoral Fellow
> http://lsa.colorado.edu/~quesadaj		Dept. of PSychology
> http://www.andrew.cmu.edu/~jquesada		University of Warwick
> office H114						Phone: +44 024 765 23 759
> Coventry, UK
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Thu Sep 29 17:39:04 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 29 Sep 2005 17:39:04 +0200
Subject: [R] boxplot and xlim confusion?
In-Reply-To: <ypx67jd0j95d.fsf@uracil.uio.no>
References: <433BDFA2.11358.10AD572@localhost> (Petr Pikal's message of	"Thu,
	29 Sep 2005 12:35:46 +0200")
Message-ID: <433C26B8.15415.2208537@localhost>

Hi

So you need x to be same in all subsequent plots e.g. from -5 to 5 
like in this.

boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal =T, 
names=letters[1:3], ylim=c(-5,5))

You find it in a thorough reading bxp man page.

Currently, 'ylim' is used 'along the boxplot', i.e.,
          vertically, when 'horizontal' is false. 

So if horizontal is true you still need to use ylim not xlim :-)

HTH
Petr


On 29 Sep 2005 at 15:28, Karin Lagesen wrote:

To:             	"Petr Pikal" <petr.pikal at precheza.cz>
From:           	Karin Lagesen <karin.lagesen at medisin.uio.no>
Date sent:      	Thu, 29 Sep 2005 15:28:14 +0200
Copies to:      	r-help at r-project.org,
	Karin Lagesen <karin.lagesen at medisin.uio.no>
Subject:        	Re: [R] boxplot and xlim confusion?

> "Petr Pikal" <petr.pikal at precheza.cz> writes:
> 
> > Hi Karin
> >
> > I did not have seen any answer for your question yet so here is a
> > try.
> >
> > I gues you want the horizotal layout or your boxplot.
> >
> > boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal =T, 
> > names=letters[1:3])
> >
> > boxplot(split(rnorm(30), rep(1:3, each=10)), horizontal =T, 
> > names=c(NA,"b",NA))
> 
> These look like the plots I want, yes. However, is there a way of
> locking the x-axis? When I do these several times in a row, the range
> of the x axis moves with the data being plotted. If I set xlim in the
> boxplot command, it makes no difference what so ever.
> 
> 
> > So this is closest what I could dig from your text.
> >
> > ***reproducible*** example would by good starting point what you
> > want and was not able to produce.
> 
> Sorry about that...:)
> 
> Karin
> -- 
> Karin Lagesen, PhD student
> karin.lagesen at medisin.uio.no
> http://www.cmbn.no/rognes/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From spencer.graves at pdf.com  Thu Sep 29 17:42:54 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Sep 2005 10:42:54 -0500
Subject: [R] quasi-random vector according to an independent graph
In-Reply-To: <003701c5c2a9$ee20b670$030ba8c0@IBM4D6040982F0>
References: <003701c5c2a9$ee20b670$030ba8c0@IBM4D6040982F0>
Message-ID: <433C0B7E.6000800@pdf.com>

	  Are you still interested in a reply to this post?  I have not seen 
any.  If you are, it might help if you were more specific, e.g., 
following the posting guide "www.R-project.org/posting-guide.html".  I'm 
not certain what you mean by "a joint distribution defined by an 
independent graph", and my efforts using "RSiteSearch" exposed several 
things that might be useful but none that seemed to me to be obvious 
answers to your question.

	  Sorry I could not be more helpful.
	  spencer graves

Jinfang Wang wrote:

> Dear R-users,
> 
> Is anyone aware of any function/package for generating a random vector from 
> a joint distribution defined by an independent graph? Or I have to work it 
> out myself?
> 
> Thanks.
> 
> Jinfang
> 
> ------------------------------
> Jinfang Wang, Associate Professor
> Chiba University, Japan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Thu Sep 29 17:50:51 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Sep 2005 10:50:51 -0500
Subject: [R] How to get the rowindices without using which?
In-Reply-To: <20050926083729.94774.qmail@web40506.mail.yahoo.com>
References: <20050926083729.94774.qmail@web40506.mail.yahoo.com>
Message-ID: <433C0D5B.7020008@pdf.com>

	  I'm  not certain what you are asking.  Consider the following:

set.seed(1)
(irows <- sample(1:nrow(iris), 10))
iris[irows,]

	  If you want more than this, PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html".  I believe that people who 
follow the posting guide generally get quicker and better answers than 
those who don't.

	  spencer graves

Martin Lam wrote:

> Hi,
> 
> I was wondering if it is possible to get the
> rowindices without using the function "which" because
> I don't have a restriction criteria. Here's an example
> of what I mean:
> # take 10 randomly selected instances
> iris[sample(1:nrow(iris), 10),]
> 
> # output
>     Sepal.Length Sepal.Width Petal.Length Petal.Width 
>   Species
> 76           6.6         3.0          4.4         1.4
> versicolor
> 105          6.5         3.0          5.8         2.2 
> virginica
> 131          7.4         2.8          6.1         1.9 
> virginica
> 79           6.0         2.9          4.5         1.5
> versicolor
> 69           6.2         2.2          4.5         1.5
> versicolor
> 42           4.5         2.3          1.3         0.3 
>    setosa
> 25           4.8         3.4          1.9         0.2 
>    setosa
> 129          6.4         2.8          5.6         2.1 
> virginica
> 60           5.2         2.7          3.9         1.4
> versicolor
> 80           5.7         2.6          3.5         1.0
> versicolor
> 
> What I want to get are their rownumbers: 76, 105, 131,
> 79, 69, 42, 25, 129, 60, 80.
> 
> Thanks in advance,
> 
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Thu Sep 29 18:03:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Sep 2005 11:03:01 -0500
Subject: [R] Producing empirical bayes estimates in disease mapping for
 lognormal model
In-Reply-To: <43393C9A.7090106@stams.strath.ac.uk>
References: <43393C9A.7090106@stams.strath.ac.uk>
Message-ID: <433C1035.4020409@pdf.com>

	  I just got 74 hits from RSiteSearch("empirical bayes"), 0 from 
RSiteSearch("empirical bayes lognormal"), and 18 from 
RSiteSearch("empirical bayes normal").  Have you tried this?

	  If you still would like information from this list, PLEASE do read 
the posting guide! "www.R-project.org/posting-guide.html".  I believe 
that people who do usually get better answers quicker.

	  spencer graves

Oarabile Molaodi wrote:

> I'm trying to produce empirical bayes estimates based on the lognormal 
> model  in disease mapping
> Is there a way this can be done in R?
> 
> thanks
> Oarabile
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From edd at debian.org  Thu Sep 29 18:12:17 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 29 Sep 2005 16:12:17 +0000 (UTC)
Subject: [R] priceIts
References: <4702645135092E4497088F71D9C8F51A128C4F@afhex01.dpi.wa.gov.au>
	<Pine.LNX.4.61.0509291037330.14465@gannet.stats>
Message-ID: <loom.20050929T180530-570@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
> chart.yahoo.com is notoriously fickle (as we have seen from the tseries 

Maybe we should update the base URL. I maintain CPAN's Finance::YahooQuote
(and am author of two apps, also on CPAN, which use it fairly heavily) and 
this has been nothing but rock solid in the six years that I've been scraping 
data from Yahoo, often at a once-a-minute frequency (for the smtm quote display
tool). Of note is that Finance::YahooQuote uses a different machine:

   $QURLbase = "http://quote.yahoo.com/d?f=";

but both chart.yahoo.com and quote.yahoo.com appear to be on the same subnet.
Not sure I'll have time to patch tseries, but it is something we may need to
keep in mind.

Hth, Dirk



From volker.rehbock at gmx.de  Thu Sep 29 14:34:28 2005
From: volker.rehbock at gmx.de (Volker Rehbock)
Date: Thu, 29 Sep 2005 14:34:28 +0200
Subject: [R] Display values in piechart/barplot
Message-ID: <000701c5c4f2$2d591630$d636a986@mast405>

Is it possible to automatically display the underlying values of a 
piechart/barplot in the graphic? If so, which package/function/argument do I 
need for it?
Thanks,
Volker



From Jan.Verbesselt at biw.kuleuven.be  Thu Sep 29 18:36:25 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Thu, 29 Sep 2005 18:36:25 +0200
Subject: [R] How to add frame (frame.plot=T) to Plot.Design?
Message-ID: <000301c5c513$efd6f620$1145210a@agr.ad10.intern.kuleuven.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050929/93a11d9b/attachment.pl

From spencer.graves at pdf.com  Thu Sep 29 18:36:35 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Sep 2005 11:36:35 -0500
Subject: [R] regsubsets selection criterion
In-Reply-To: <5.0.2.1.2.20050927150918.031edea0@mailhost.paris.ensam.fr>
References: <5.0.2.1.2.20050927150918.031edea0@mailhost.paris.ensam.fr>
Message-ID: <433C1813.7020202@pdf.com>

	  Questions like this are best directed to the package maintainer(s). 
 From help(package="leaps"), I learned that Thomas Lumley is the author 
and maintainer for "leaps";  I'm including him as a 'cc', so he can 
correct or add to my comments if he feels so inclined.

	  After some searching, I learned that "?plot.regsubsets" includes an 
argument scale=c("bic", "Cp", "adjr2", "r2").  From this, I infer that 
you have your choice of these four criteria.

	  hope this helps.
	  spencer graves

Samuel Bertrand wrote:

> Hello,
> 
> I am using the 'regsubsets' function
> (from leaps package)
> to get the best linear models
> to explain 1 variable
> from 1 to 5 explanatory variables
> (exhaustive search).
> 
> Is there anyone who can tell me
> on which criterion is based
> the 'regsubsets' function ?
> 
> Thank you.
> 
> samuel
> 
> 
> 
> 
> 
> Samuel BERTRAND
> Doctorant
> Laboratoire de Biomecanique
> LBM - ENSAM - CNRS UMR 8005
> http://bio-web.paris.ensam.fr
> 151, bd de l'Hopital
> 75013 PARIS
> Tel. +33 (0) 1 44 24 64 53
> Fax +33 (0) 1 44 24 63 66
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From tlumley at u.washington.edu  Thu Sep 29 18:39:10 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 29 Sep 2005 09:39:10 -0700 (PDT)
Subject: [R] regsubsets selection criterion
In-Reply-To: <433C1813.7020202@pdf.com>
References: <5.0.2.1.2.20050927150918.031edea0@mailhost.paris.ensam.fr>
	<433C1813.7020202@pdf.com>
Message-ID: <Pine.LNX.4.63a.0509290938380.26336@homer23.u.washington.edu>

On Thu, 29 Sep 2005, Spencer Graves wrote:

> 	  Questions like this are best directed to the package maintainer(s). 
> From help(package="leaps"), I learned that Thomas Lumley is the author and 
> maintainer for "leaps";  I'm including him as a 'cc', so he can correct or 
> add to my comments if he feels so inclined.

He has already answered this question when it was originally posted.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From reid_huntsinger at merck.com  Thu Sep 29 19:00:04 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 29 Sep 2005 13:00:04 -0400
Subject: [R] quasi-random vector according to an independent graph
Message-ID: <355C35514FEAC9458F75947F5270974D076CFA@usctmx1103.merck.com>

Might the "graphical models in R" packages be of interest?
http://www.r-project.org/gR/

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
Sent: Thursday, September 29, 2005 11:43 AM
To: Jinfang Wang
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] quasi-random vector according to an independent graph


	  Are you still interested in a reply to this post?  I have not seen

any.  If you are, it might help if you were more specific, e.g., 
following the posting guide "www.R-project.org/posting-guide.html".  I'm 
not certain what you mean by "a joint distribution defined by an 
independent graph", and my efforts using "RSiteSearch" exposed several 
things that might be useful but none that seemed to me to be obvious 
answers to your question.

	  Sorry I could not be more helpful.
	  spencer graves

Jinfang Wang wrote:

> Dear R-users,
> 
> Is anyone aware of any function/package for generating a random vector
from 
> a joint distribution defined by an independent graph? Or I have to work it

> out myself?
> 
> Thanks.
> 
> Jinfang
> 
> ------------------------------
> Jinfang Wang, Associate Professor
> Chiba University, Japan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mschwartz at mn.rr.com  Thu Sep 29 19:01:54 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 29 Sep 2005 12:01:54 -0500
Subject: [R] Display values in piechart/barplot
In-Reply-To: <000701c5c4f2$2d591630$d636a986@mast405>
References: <000701c5c4f2$2d591630$d636a986@mast405>
Message-ID: <1128013314.5402.30.camel@localhost.localdomain>

On Thu, 2005-09-29 at 14:34 +0200, Volker Rehbock wrote:
> Is it possible to automatically display the underlying values of a 
> piechart/barplot in the graphic? If so, which package/function/argument do I 
> need for it?
> Thanks,
> Volker


Using pie charts are not a particularly good way of displaying data,
even though the pie() function is available in R. See ?pie for more
information on this.

For barplots, the following provide two approaches:


# Place the bar values above the bars
# Note that I set 'ylim' to make room for 
# the text labels above the bars

vals <- 1:5
names(vals) <- LETTERS[1:5]
mp <- barplot(vals, ylim = c(0, 6))
text(mp, vals, labels = vals, pos = 3)



# Place the bar values below the x axis

vals <- 1:5
names(vals) <- LETTERS[1:5]
mp <- barplot(vals)
mtext(side = 1, at = mp, text = vals, line = 3)


Note that barplot() returns the bar midpoints, so you can use these
values for annotation placement.

See ?barplot, ?text and ?mtext for more information.

HTH,

Marc Schwartz



From christoph.lehmann at gmx.ch  Thu Sep 29 19:04:31 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Thu, 29 Sep 2005 19:04:31 +0200 (MEST)
Subject: [R] plot.augPred sorted and labelled according second factor
Message-ID: <5677.1128013471@www21.gmx.net>

Hi

using this code example:

library(nlme)
fm1 <- lme(Orthodont, random = ~1)
plot(augPred(fm1))

is there any way to have the plots in each cell labelled and ordered
according to Orthodont$Sex? I.e. in addition to the bar with the label for
Orthodont$Subject there is another bar labelling the Sex of the subject?

thanks a lot

christoph

--



From mlyman at byu.edu  Thu Sep 29 19:14:34 2005
From: mlyman at byu.edu (Mark Lyman)
Date: Thu, 29 Sep 2005 11:14:34 -0600
Subject: [R] lmer random effect model matrix question
Message-ID: <433C20FA.8010905@byu.edu>

I have one fixed effect, sor, with two levels. I have eight lots and 
three wafers from each lot. I have included the data below.

I would like to fit a mixed model that estimates a covariance parameter 
for wafer, which is nested in lot, and two covariance parameters for 
lot, one for each level of sor. The following command fits the model 
that I want, except for it estimates the correlation between the two 
covariance parameters for lot. Is there anyway to make R not estimate 
this correlation? Thank you.

lmer(y~sor+(sor-1|lot)+(1|wafer:lot),wafer)

For those familiar with proc mixed the following SAS code fits the model 
that I want:

proc mixed scoring=4;
   class sor lot wafer site;
   model y= sor/ddfm=satterth;
   random lot(sor)/group=sor;
   random wafer(lot);
run;

   sor lot wafer site    y
1    1   1     1    1 2006
2    1   1     1    2 1999
3    1   1     1    3 2007
4    1   1     2    1 1980
5    1   1     2    2 1988
6    1   1     2    3 1982
7    1   1     3    1 2000
8    1   1     3    2 1998
9    1   1     3    3 2007
10   1   2     1    1 1991
11   1   2     1    2 1990
12   1   2     1    3 1988
13   1   2     2    1 1987
14   1   2     2    2 1989
15   1   2     2    3 1988
16   1   2     3    1 1985
17   1   2     3    2 1983
18   1   2     3    3 1989
19   1   3     1    1 2000
20   1   3     1    2 2004
21   1   3     1    3 2004
22   1   3     2    1 2001
23   1   3     2    2 1996
24   1   3     2    3 2004
25   1   3     3    1 1999
26   1   3     3    2 2000
27   1   3     3    3 2002
28   1   4     1    1 1997
29   1   4     1    2 1994
30   1   4     1    3 1996
31   1   4     2    1 1996
32   1   4     2    2 2000
33   1   4     2    3 2002
34   1   4     3    1 1987
35   1   4     3    2 1990
36   1   4     3    3 1995
37   2   5     1    1 2013
38   2   5     1    2 2004
39   2   5     1    3 2009
40   2   5     2    1 2023
41   2   5     2    2 2018
42   2   5     2    3 2010
43   2   5     3    1 2020
44   2   5     3    2 2023
45   2   5     3    3 2015
46   2   6     1    1 2032
47   2   6     1    2 2036
48   2   6     1    3 2030
49   2   6     2    1 2018
50   2   6     2    2 2022
51   2   6     2    3 2026
52   2   6     3    1 2009
53   2   6     3    2 2010
54   2   6     3    3 2011
55   2   7     1    1 1984
56   2   7     1    2 1993
57   2   7     1    3 1993
58   2   7     2    1 1992
59   2   7     2    2 1992
60   2   7     2    3 1990
61   2   7     3    1 1996
62   2   7     3    2 1993
63   2   7     3    3 1987
64   2   8     1    1 1996
65   2   8     1    2 1989
66   2   8     1    3 1996
67   2   8     2    1 1997
68   2   8     2    2 1993
69   2   8     2    3 1996
70   2   8     3    1 1990
71   2   8     3    2 1989
72   2   8     3    3 1992



From mschwartz at mn.rr.com  Thu Sep 29 19:15:13 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 29 Sep 2005 12:15:13 -0500
Subject: [R] How to add frame (frame.plot=T) to Plot.Design?
In-Reply-To: <000301c5c513$efd6f620$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000301c5c513$efd6f620$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <1128014114.5402.33.camel@localhost.localdomain>

On Thu, 2005-09-29 at 18:36 +0200, Jan Verbesselt wrote:
> Hi R-help,
> 
>  
> 
> When using the package Design and the plot.Design function all the graphs of
> an lrm.fit (lrm()) are plotted without a frame (only with axes). How can the
> frame be added to these plots?
> 
>  
> 
> In the plot.default function the frame can be added/or removed via the
> frame.plot=T/F, respectively.
> 
> e.g., plot(1,axes=T,frame.plot=T). How can this be done with
> plot.Design(lrm.fit)
> 
>  
> 
> Thanks,
> 
> Jan


See ?box

HTH,

Marc Schwartz



From Ted.Harding at nessie.mcc.ac.uk  Thu Sep 29 19:45:31 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 29 Sep 2005 18:45:31 +0100 (BST)
Subject: [R] Select varying LS digits in long numbers?
Message-ID: <XFMail.050929184531.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I'm trying to find a neat solution to an apparently simple
problem, but one which turns out to be a bit more intricate
and tricky than one might expect.

Suppose I have numbers given to a large number of digits.
For example

  1234567021

where (though I don't know this beforehand) only the last
3 digits will be varying (and all 3 will vary).

What I want is, give a vector x of such numbers, to extract
the minimal set of final digits which will include the varying
digits (i.e. in this case the last 3 digits). And there may be
a decimal point somewhere along the line (though again I won't
know where, nor whether).

I can think of brute-force ways of doing this, but I'd like
a neat one!

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 29-Sep-05                                       Time: 18:45:26
------------------------------ XFMail ------------------------------



From charles.edwin.white at us.army.mil  Thu Sep 29 20:18:30 2005
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Thu, 29 Sep 2005 14:18:30 -0400
Subject: [R] Reading Health Level Seven (HL 7) data format into R
Message-ID: <8BAEC5E546879B4FAA536200A292C6149D0CB9@AMEDMLNARMC135.amed.ds.army.mil>

Has anyone developed code for reading HL 7 formatted data into R? HL 7
is an open standard for the digital transfer of health care information.
It's moving towards XML but it isn't there yet. A study I started
working on today will be receiving lab data in that format. I'd rather
not reinvent any more wheels than strictly necessary. Thanks for your
time.

Chuck



From spencer.graves at pdf.com  Thu Sep 29 20:43:27 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Sep 2005 13:43:27 -0500
Subject: [R] need suggestion about building formual
In-Reply-To: <200509281428.38761.qsb@nlbmol.ibp.ac.cn>
References: <200509281428.38761.qsb@nlbmol.ibp.ac.cn>
Message-ID: <433C35CF.50007@pdf.com>

	  I'm not certain what you are asking.

	  You can build expressions in R as character strings and then execute 
them.  Example:

expr <- paste("two <-", 1, "+", 1)
eval(parse(text=expr))
two

	  If this does not answer your question, PLEASE do read the posting 
guide, "www.R-project.org/posting-guide.html".  It can help increase the 
chances of a quick and useful reply.

	  spencer graves	

Simple wrote:

> hi,
> I'm an newbie for R,I want do some fitting in R.
> 
> I wander if it is possible to write a few of equations but only one formual 
> when fitting 
> 
> Currently,My problem is,in R, is there methods combination a few equations 
> into one formual?
> For example, 
> y=f1(k);
> k=f2(t);
> t=f3(x);
> although it is certain that the can be equations turn into one formual as 
> y~f(x),but write such a complexity string make me painful.
> 
> I have searched the web and found out there were only examples with one 
> formual.any suggestion? 
> 
> I hope that I have omit something.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From dmbates at gmail.com  Thu Sep 29 21:11:37 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 29 Sep 2005 14:11:37 -0500
Subject: [R] standard error of variances and covariances of the random
	effects with LME
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F740A31C81F@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F740A31C81F@dc1ex2.air.org>
Message-ID: <40e66e0b0509291211768422a9@mail.gmail.com>

With lme you could but it would take a while to work it out.  There is
an approximate Hessian matrix for the parameters that determine the
variance-covariance matrix in there somewhere and if you were
sufficiently persistent you could convert that apVar component to the
scale of the variances and covariances.  I believe it is in the scale
of the logarithm of the standard deviation and Fisher's z
transformation (i.e. the hyperbolic arc tangent) of the correlation.

On 9/29/05, Doran, Harold <HDoran at air.org> wrote:
> You cannot. Also, it's not that the distribution of the random effects
> is not symmetric, but that it *may* not be symmetric, and this is an
> assumption that should be checked.
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roel de Jong
> Sent: Thursday, September 29, 2005 9:20 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] standard error of variances and covariances of the random
> effects with LME
>
> Hello,
>
> how do I obtain standard errors of variances and covariances of the
> random effects with LME comparable to those of for example MlWin? I know
> you shouldn't use them because the distribution of the estimator isn't
> symmetric blablabla, but I need a measure of the variance of those
> estimates for pooling my multiple imputation results.
>
> Regards,
>    Roel.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmbates at gmail.com  Thu Sep 29 21:17:11 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 29 Sep 2005 14:17:11 -0500
Subject: [R] anova on binomial LMER objects
In-Reply-To: <fc.004c4d1922c7bd3b3b9aca00ed2667fb.22c7c915@umit.maine.edu>
References: <mailman.10.1127988001.20586.r-help@stat.math.ethz.ch>
	<fc.004c4d1922c7bd3b3b9aca00ed2667fb.22c7c915@umit.maine.edu>
Message-ID: <40e66e0b05092912172a006f4a@mail.gmail.com>

The issues with lmer and the analysis of variance are due to its not
make appropriate correction for the prior weights vector.  If you
convert your binomial response to the equivalent number of binary
responses you get an appropriate anova table.

It's on the "ToDo" list to fix this but a few other things have to
come first, like grading assignments in one of my courses and
repairing the computer in my office.  This is the third motherboard I
have torched in four months.



On 9/29/05, Alan Cobo-Lewis <alanc at umit.maine.edu> wrote:
> On Wed, 28 Sep 2005, Robert Bagchi wrote:
> >>Hi Patrick
> >>
> >>thanks for your advice. I have now tried glmmPQL, and it worked fine -
> >>I'm getting consistent results between plots and models fitted by
> >>glmmPQL. Plus it allows predict() and resid() which is another advantage
> >>over lmer at present.
> >>
> >>quick question though: why does one need to use PQL for binomial models?
> >>Is there a good reference for this?
> >
> You don't have to use PQL for binomial models, but you can't use least-squares. PQL is an approximate solution. Laplace and Adaptive Gaussian Quadrature options in lmer are better approximations. So lmer would likely become the better option as it
> progresses in its development (though the current issues you've found with the F ratios certainly sound like maybe lmer isn't better for you in its current incarnation).
> alan
>
> --
> Alan B. Cobo-Lewis, Ph.D.               (207) 581-3840 tel
> Department of Psychology                (207) 581-6128 fax
> University of Maine
> Orono, ME 04469-5742                    alanc at maine.edu
>
> http://www.umaine.edu/visualperception
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From aoganyan at niss.org  Thu Sep 29 21:22:51 2005
From: aoganyan at niss.org (Anna Oganyan)
Date: Thu, 29 Sep 2005 15:22:51 -0400
Subject: [R] solution of convolution equation
Message-ID: <433C3F0B.2020001@niss.org>

Hello,
May be somebody can help me...
I am trying to find a solution of a convolution equation using fft (and 
unfortunately I do not have a good background for this).
So I am just trying to figure out how it can be implemented in R. I have 
two multidimensional independent variables X and Z
and I know their densities fx and fz, which are multidimensional arrays. 
So I have to find the density of Y, such that X+Y=Z.
So, first I tried on a simple example, where the variables are 
one-dimensional, say X is N(0,1) and Z is N(7,3).
So I want to find the density of Y (which should be N(7, sqrt(8)).
I did:
x <- seq(-6, 20, length=500)
fx <- dnorm(x)
z <- seq(-6, 20, length=500)
fz <- dnorm(z, mean=7, sd=3)
ffty <- fft(fz)/fft(fx)
fy <- fft(ffty, inverse=T)/length(ffty)
plot(Re(fy))

But the plot is far from being normal. May be the problem is with the 
lengths of fx and fz? should they be of different lengths and fx padded 
with zeros? May be somebody could point out that?? Thanks!
Anna



From MikeJones at westat.com  Thu Sep 29 21:22:00 2005
From: MikeJones at westat.com (Mike Jones)
Date: Thu, 29 Sep 2005 15:22:00 -0400
Subject: [R] Saving Graphics
Message-ID: <403593359CA56C4CAE1F8F4F00DCFE7D01123457@MAILBE2.westat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050929/3332b808/attachment.pl

From dhinds at sonic.net  Thu Sep 29 21:23:25 2005
From: dhinds at sonic.net (dhinds@sonic.net)
Date: Thu, 29 Sep 2005 19:23:25 +0000 (UTC)
Subject: [R] Problem with memory footprint of qq plot
	generated	with	lattice
References: <43396F07@webmail.ncl.ac.uk> <dhfact$g2l$1@sea.gmane.org>
	<433BD114.10309@ncl.ac.uk>
Message-ID: <dhhevd$mpc$1@sea.gmane.org>

Witold Eryk Wolski <W.E.Wolski at ncl.ac.uk> wrote:

> Adding f.value=fn as argument to qqmath reduces the size of the image, 
> but neither the axis (absicissae) nor the line added by panel.qqmathline 
> are right.

> Adding f.value=fn as argument to panel.qqmathline and panel.qqmath 
> generates the right graphic, but the size of the image is again 20 MB.

> Any Suggestions?

Neither panel.qqmathline nor panel.qqmath pay any attention to an
f.value argument.

It looks like panel.qqmathline() effectively assumes that f.value is
uniformly distributed.  I guess you could rewrite panel.qqmathline and
prepanel.qqmathline but I don't see a clean solution.

-- Dave



From HDoran at air.org  Thu Sep 29 21:34:54 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 29 Sep 2005 15:34:54 -0400
Subject: [R] Saving Graphics
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A31C880@dc1ex2.air.org>

See ?pdf or ?postscript 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Jones
Sent: Thursday, September 29, 2005 3:22 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Saving Graphics

Hello all,

I'm having difficulty automatically saving graphs.

Is there a way to save graphs from the graphics window using commands in
the R console?  

Thanks very much.

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Thu Sep 29 21:37:35 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 29 Sep 2005 14:37:35 -0500
Subject: [R] lmer random effect model matrix question
In-Reply-To: <433C20FA.8010905@byu.edu>
References: <433C20FA.8010905@byu.edu>
Message-ID: <40e66e0b050929123718b09d7c@mail.gmail.com>

I would create two 0/1 variables for sor level 1 and sor level 2 and
use those as in

> mark$sor1 <- ifelse(mark$sor == 1, 1, 0)
> mark$sor2 <- ifelse(mark$sor == 2, 1, 0)
> (fm1 <- lmer(y ~ sor + (0+sor1|lot) + (0+sor2|lot) + (1|wafer:lot), mark))
Linear mixed-effects model fit by REML
Formula: y ~ sor + (0 + sor1 | lot) + (0 + sor2 | lot) + (1 | wafer:lot)
   Data: mark
      AIC      BIC    logLik MLdeviance REMLdeviance
 455.7631 469.4231 -221.8816   453.5174     443.7631
Random effects:
 Groups    Name        Variance Std.Dev.
 wafer:lot (Intercept)  35.866   5.9888
 lot       sor2        222.709  14.9234
 lot       sor1         17.076   4.1323
 Residual               12.569   3.5453
# of obs: 72, groups: wafer:lot, 24; lot, 8; lot, 8

Fixed effects:
             Estimate Std. Error DF  t value Pr(>|t|)
(Intercept) 1995.1111     2.7581 70 723.3703   <2e-16 ***
sor2          10.0833     8.1622 70   1.2354   0.2208
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


On 9/29/05, Mark Lyman <mlyman at byu.edu> wrote:
> I have one fixed effect, sor, with two levels. I have eight lots and
> three wafers from each lot. I have included the data below.
>
> I would like to fit a mixed model that estimates a covariance parameter
> for wafer, which is nested in lot, and two covariance parameters for
> lot, one for each level of sor. The following command fits the model
> that I want, except for it estimates the correlation between the two
> covariance parameters for lot. Is there anyway to make R not estimate
> this correlation? Thank you.
>
> lmer(y~sor+(sor-1|lot)+(1|wafer:lot),wafer)
>
> For those familiar with proc mixed the following SAS code fits the model
> that I want:
>
> proc mixed scoring=4;
>    class sor lot wafer site;
>    model y= sor/ddfm=satterth;
>    random lot(sor)/group=sor;
>    random wafer(lot);
> run;
>
>    sor lot wafer site    y
> 1    1   1     1    1 2006
> 2    1   1     1    2 1999
> 3    1   1     1    3 2007
> 4    1   1     2    1 1980
> 5    1   1     2    2 1988
> 6    1   1     2    3 1982
> 7    1   1     3    1 2000
> 8    1   1     3    2 1998
> 9    1   1     3    3 2007
> 10   1   2     1    1 1991
> 11   1   2     1    2 1990
> 12   1   2     1    3 1988
> 13   1   2     2    1 1987
> 14   1   2     2    2 1989
> 15   1   2     2    3 1988
> 16   1   2     3    1 1985
> 17   1   2     3    2 1983
> 18   1   2     3    3 1989
> 19   1   3     1    1 2000
> 20   1   3     1    2 2004
> 21   1   3     1    3 2004
> 22   1   3     2    1 2001
> 23   1   3     2    2 1996
> 24   1   3     2    3 2004
> 25   1   3     3    1 1999
> 26   1   3     3    2 2000
> 27   1   3     3    3 2002
> 28   1   4     1    1 1997
> 29   1   4     1    2 1994
> 30   1   4     1    3 1996
> 31   1   4     2    1 1996
> 32   1   4     2    2 2000
> 33   1   4     2    3 2002
> 34   1   4     3    1 1987
> 35   1   4     3    2 1990
> 36   1   4     3    3 1995
> 37   2   5     1    1 2013
> 38   2   5     1    2 2004
> 39   2   5     1    3 2009
> 40   2   5     2    1 2023
> 41   2   5     2    2 2018
> 42   2   5     2    3 2010
> 43   2   5     3    1 2020
> 44   2   5     3    2 2023
> 45   2   5     3    3 2015
> 46   2   6     1    1 2032
> 47   2   6     1    2 2036
> 48   2   6     1    3 2030
> 49   2   6     2    1 2018
> 50   2   6     2    2 2022
> 51   2   6     2    3 2026
> 52   2   6     3    1 2009
> 53   2   6     3    2 2010
> 54   2   6     3    3 2011
> 55   2   7     1    1 1984
> 56   2   7     1    2 1993
> 57   2   7     1    3 1993
> 58   2   7     2    1 1992
> 59   2   7     2    2 1992
> 60   2   7     2    3 1990
> 61   2   7     3    1 1996
> 62   2   7     3    2 1993
> 63   2   7     3    3 1987
> 64   2   8     1    1 1996
> 65   2   8     1    2 1989
> 66   2   8     1    3 1996
> 67   2   8     2    1 1997
> 68   2   8     2    2 1993
> 69   2   8     2    3 1996
> 70   2   8     3    1 1990
> 71   2   8     3    2 1989
> 72   2   8     3    3 1992
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ken.pierce at oregonstate.edu  Thu Sep 29 21:37:38 2005
From: ken.pierce at oregonstate.edu (Pierce, Ken)
Date: Thu, 29 Sep 2005 12:37:38 -0700
Subject: [R] Saving Graphics
Message-ID: <4D5DA98A54374044B7CC3F40A157B98B12EE57@thuja>

savePlot has been working quite well for me.

Ken

~~~~~~~~~~~~~~~~~~~~~~~
Kenneth B. Pierce Jr. 
Research Ecologist
Forestry Sciences Laboratory
3200 SW Jefferson Way
Corvallis, OR 97331
ken.pierce at oregonstate.edu
541 750-7393
~~~~~~~~~~~~~~~~~~~~~~~~
  

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Jones
Sent: Thursday, September 29, 2005 12:22 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Saving Graphics

Hello all,

I'm having difficulty automatically saving graphs.

Is there a way to save graphs from the graphics window using commands in
the R console?  

Thanks very much.

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Thu Sep 29 21:45:42 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Sep 2005 14:45:42 -0500
Subject: [R] multidimensional integration
In-Reply-To: <200509281336.j8SDaSAT029890@rumms.uni-mannheim.de>
References: <200509281336.j8SDaSAT029890@rumms.uni-mannheim.de>
Message-ID: <433C4466.50309@pdf.com>

	  Which version of R on which operating system crashes with which 
version of adapt?

	  What are you trying to integrate?  Is it pure R, or do you link to 
something else like C++?  RSitSearch("integrate in 2 dimensions") 
produced 10 hits.  Other searches produced more.  My bottom line is that 
"adapt" has been around for a while.

	  Can you provide a reproducible example of your problem -- preferably 
one as simple as possible, following the posting guide, 
"www.R-project.org/posting-guide.html".  (I believe that people who 
follow the posting guide usually get quicker, more useful answers.)

	  spencer graves

Marcel Prokopczuk wrote:
> dear all,
> 
> i have the following problem: i want to integrate a two-dimensional
> function. unfortunately R crashes when i try to use adapt() and i get a nice
> windows message with some hex-code.
> do anybody of you knows how to avoid this or knows another more stable
> function than adapt()
> 
> thanks a lot
> 
> marcel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From chabotd at globetrotter.net  Thu Sep 29 21:55:35 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Thu, 29 Sep 2005 15:55:35 -0400
Subject: [R] p-level in packages mgcv and gam
In-Reply-To: <10EFB5D9-AB44-430E-86F0-7E57A7B0FD3B@globetrotter.net>
References: <mailman.10.1127988001.20586.r-help@stat.math.ethz.ch>
	<10EFB5D9-AB44-430E-86F0-7E57A7B0FD3B@globetrotter.net>
Message-ID: <AC705FBD-B408-4BC1-89B8-9727C1571A3D@globetrotter.net>

OK, I think I understand better but still have two points to clarify.

The first one is about the number of df. I think those who replied on  
this objected to the way I chose df, not the fact that I would run a  
model with 7.4 df per se. If I read you correctly, I artificially  
reduce my p-value by using the estimated df found in a mgcv gam model  
into another model where I fix df. This is fine, I am quite willing  
not to run a second model with a fixed df and instead tell my readers  
that my model is "marginally significant" with a p-value of 0.03.

This being said, do you know of guidelines for choosing df? A  
colleague told me he does not go above 10% of the number of points.  
Should I be concerned when mgcv estimates 7.4 df for 34 points? Note  
that for this particular model P < 1e-16, and P is also very small if  
I fix df to either 4 or 7.

My second point is the difference between models fitted by packages  
gam and mgcv. Sure, some of you have said "different algorithms". And  
when I specify dfs, shouldn't P-values be very similar for the 2  
packages? If not, what does it say of the confidence we can have in  
the models?

I draw your attention to this exampl: I obtained P-values of 0.50 and  
0.03 with packages gam and mgcv respectively:


 > library(gam)
Loading required package: splines
 > data(kyphosis)
 > kyp1 <- gam(Kyphosis ~ s(Number, 3), family=binomial, data=kyphosis)
 > summary.gam(kyp1)

Call: gam(formula = Kyphosis ~ s(Number, 3), family = binomial, data  
= kyphosis)
Deviance Residuals:
     Min      1Q  Median      3Q     Max
-1.3646 -0.6233 -0.4853 -0.3133  2.0965

(Dispersion Parameter for binomial family taken to be 1)

     Null Deviance: 83.2345 on 80 degrees of freedom
Residual Deviance: 71.9973 on 76.9999 degrees of freedom
AIC: 79.9976

Number of Local Scoring Iterations: 7

DF for Terms and Chi-squares for Nonparametric Effects

              Df Npar Df Npar Chisq  P(Chi)
(Intercept)   1
s(Number, 3)  1       2    1.37149 0.50375

 > detach(package:gam)
 > library(mgcv)
This is mgcv 1.3-7
 > kyp2 <- gam(Kyphosis ~ s(Number, k=4, fx=T),  family=binomial,  
data=kyphosis)
 > summary.gam(kyp2)

Family: binomial
Link function: logit

Formula:
Kyphosis ~ s(Number, k = 4, fx = T)

Parametric coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)  -1.5504     0.3342   -4.64 3.49e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
           edf Est.rank Chi.sq p-value
s(Number)   3        3  8.898  0.0307 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.101   Deviance explained = 12.5%
UBRE score = 0.075202  Scale est. = 1         n = 81
 > kyp2$deviance
[1] 72.85893
 > kyp2$null.deviance
[1] 83.23447
 > kyp2$df.null
[1] 80
 > kyp2$df.residual
[1] 77

How can we explain this huge difference?

Denis


> Le 05-09-29 à 06:00, r-help-request at stat.math.ethz.ch a écrit :
>
> De : Henric Nilsson <henric.nilsson at statisticon.se>
> Date : 29 septembre 2005 03:55:19 HAE
> À : ym at climpact.com
> Cc : r-help at stat.math.ethz.ch
> Objet : Rép : [R] p-level in packages mgcv and gam
> Répondre à : Henric Nilsson <henric.nilsson at statisticon.se>
>
>
> Yves Magliulo said the following on 2005-09-28 17:05:
>
>> hi,
>> i'll try to help you, i send a mail about this subject last  
>> week... and
>> i did not have any response...
>> I'm using gam from package mgcv. 1)
>> How to interpret the significance of smooth terms is hard for me to
>> understand perfectly : using UBRE, you fix df. p-value are  
>> estimated by chi-sq distribution using GCV, the best df are  
>> estimated by GAM. (that's what i want) and
>> p-values
>
>
> This is not correct. The df are estimated in both cases (i.e. UBRE  
> and GCV), but the scale parameter is fixed in the UBRE case. Hence,  
> by default UBRE is used for family = binomial or poisson since the  
> scale parameter is assumed to be 1. Similarly, GCV is the default  
> for family = gaussian since we most often want the scale (usually  
> denoted sigma^2) to be estimated.
>
>
>> are estimated by an F distribution But in that case they said "use at
>> your own risk" in ?summary.gam
>
>
> The warning applies in both cases. The p-values are conditional on  
> the smoothing parameters, and the uncertainty of the smooths is not  
> taken into account when computing the p-values.
>
>
>> so you can also look at the chi.sq : but i don't know how to choose a
>
>
> No...
>
>
>> criterion like for p-values... for me, chi.sq show the best  
>> predictor in
>> a model, but it's hard to reject one with it.
>
>
> Which version of mgcv do you use? The confusion probably stems from  
> earlier versions of mgcv (< 1.3-5): the summary and anova methods  
> used to have a column denoted Chi.sq even when the displayed  
> statistic was computed as F. Recent versions of mgcv has
>
> > summary(b)
>
> Family: gaussian
> Link function: identity
>
> Formula:
> y ~ s(x0) + s(x1) + s(x2) + s(x3)
>
> Parametric coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)   7.9150     0.1049   75.44   <2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Approximate significance of smooth terms:
>         edf Est.rank      F  p-value
> s(x0) 5.173    9.000  3.785 0.000137 ***
> s(x1) 2.357    9.000 34.631  < 2e-16 ***
> s(x2) 8.517    9.000 84.694  < 2e-16 ***
> s(x3) 1.000    1.000  0.444 0.505797
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> R-sq.(adj) =  0.726   Deviance explained = 73.7%
> GCV score =  4.611   Scale est. = 4.4029    n = 400
>
>
> If we assume that the scale is known and fixed at 4.4029, we get
>
> > summary(b, dispersion = 4.4029)
>
> Family: gaussian
> Link function: identity
>
> Formula:
> y ~ s(x0) + s(x1) + s(x2) + s(x3)
>
> Parametric coefficients:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)   7.9150     0.1049   75.44   <2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Approximate significance of smooth terms:
>         edf Est.rank  Chi.sq p-value
> s(x0) 5.173    9.000  34.067 8.7e-05 ***
> s(x1) 2.357    9.000 311.679 < 2e-16 ***
> s(x2) 8.517    9.000 762.255 < 2e-16 ***
> s(x3) 1.000    1.000   0.444   0.505
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> R-sq.(adj) =  0.726   Deviance explained = 73.7%
> GCV score =  4.611   Scale est. = 4.4029    n = 400
>
> Note that t/F changed into z/Chi.sq.
>
>
>> so as far as i m concerned, i use GCV methods, and fix a 5% on the  
>> null
>> hypothesis (pvalue) to select significant predictor. after, i look  
>> at my
>> smooth, and if the parametrization look fine to me, i validate.
>> generaly, for p-values smaller than 0.001, you can be confident. over
>> 0.001, you have to check. 2)
>> for difference between package gam and mgcv, i sent a mail about this
>
>
> The underlying algorithms are very different.
>
>
> HTH,
> Henric
>
>
> De : "Liaw, Andy" <andy_liaw at merck.com>
> Date : 28 septembre 2005 14:01:25 HAE
> À : 'Denis Chabot' <chabotd at globetrotter.net>, Peter Dalgaard  
> <p.dalgaard at biostat.ku.dk>
> Cc : Thomas Lumley <tlumley at u.washington.edu>, R list <r- 
> help at stat.math.ethz.ch>
> Objet : RE: [R] p-level in packages mgcv and gam
>
> Just change the df in what Thomas described to the degree of  
> polynomial, and
> everything he said still applies.  Any good book on regression that  
> covers
> polynomial regression ought to point this out.
>
> Andy
>
>
>> From: Denis Chabot
>>
>> But what about another analogy, that of polynomials? You may not be
>> sure what degree polynomial to use, and you have not decided before
>> analysing your data. You fit different polynomials to your data,
>> checking if added degrees increase r2 sufficiently by doing F-tests.
>>
>> I thought it was the same thing with GAMs. You can fit a
>> model with 4
>> df, and in some cases it is of interest to see if this is a better
>> fit than a linear fit. But why can't you also check if 7df is better
>> than 4df? And if you used mgcv first and it tells you that 7df is
>> better than 4df, why bother repeating the comparison 7df
>> against 4df,
>> why not just take the p-value for the model with 7df (fixed)?
>>
>> Denis
>


> De : Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> Date : 28 septembre 2005 12:04:58 HAE
> À : Thomas Lumley <tlumley at u.washington.edu>
> Cc : Denis Chabot <chabotd at globetrotter.net>, R list <r- 
> help at stat.math.ethz.ch>
> Objet : Rép : [R] p-level in packages mgcv and gam
>
> Thomas Lumley <tlumley at u.washington.edu> writes:
>>
>> Bob, on the other hand, chooses the amount of smoothing depending on
>> the data. When a 4 df smooth fits best he ends up with the same model
>> as Alice and the same p-value.  When some other df fits best he ends
>> up with a different model and a *smaller* p-value than Alice.
>
> This doesn't actually follow, unless the p-value (directly or
> indirectly) found its way into the definition of "best fit". It does
> show the danger, though.
>
> -- 
>    O__  ---- Peter Dalgaard             Øster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)  
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)  
> 35327907
>

> De : Thomas Lumley <tlumley at u.washington.edu>
> Date : 26 septembre 2005 12:54:43 HAE
> À : Denis Chabot <chabotd at globetrotter.net>
> Cc : r-help at stat.math.ethz.ch
> Objet : Rép : [R] p-level in packages mgcv and gam
>
> On Mon, 26 Sep 2005, Denis Chabot wrote:
>
> But the mgcv manual warns that p-level for the smooth can be
> underestimated when df are estimated by the model. Most of the time
> my p-levels are so small that even doubling them would not result in
> a value close to the P=0.05 threshold, but I have one case with  
> P=0.033.
>
> I thought, probably naively, that running a second model with fixed
> df, using the value of df found in the first model. I could not
> achieve this with mgcv: its gam function does not seem to accept
> fractional values of df (in my case 8.377).
>
> No, this won't work.  The problem is the usual one with model  
> selection: the p-value is calculated as if the df had been fixed,  
> when really it was estimated.
>
> It is likely to be quite hard to get an honest p-value out of  
> something that does adaptive smoothing.
>
>     -thomas
> De : Thomas Lumley <tlumley at u.washington.edu>
> Date : 28 septembre 2005 11:33:27 HAE
> À : Denis Chabot <chabotd at globetrotter.net>
> Cc : R list <r-help at stat.math.ethz.ch>
> Objet : Rép : [R] p-level in packages mgcv and gam
>
> On Wed, 28 Sep 2005, Denis Chabot wrote:
>
> I only got one reply to my message:
>
> No, this won't work.  The problem is the usual one with model
> selection: the p-value is calculated as if the df had been fixed,
> when really it was estimated.
>
> It is likely to be quite hard to get an honest p-value out of
> something that does adaptive smoothing.
>
>     -thomas
>
> I do not understand this: it seems that a lot of people chose df=4
> for no particular reason, but p-levels are correct. If instead I
> choose df=8 because a previous model has estimated this to be an
> optimal df, P-levels are no good because df are estimated?
>
> Yes. I know this sounds strange initially, but it really does make  
> sense if you think about it carefully.
>
> Suppose that Alice and Bob are kyphosis researchers, and that Alice  
> always chooses 4df for smoothing Age.  We would all agree that her  
> p-values are correct [in fact we wouldn't, but that is a separate  
> issue]
>
> Bob, on the other hand, chooses the amount of smoothing depending  
> on the data. When a 4 df smooth fits best he ends up with the same  
> model as Alice and the same p-value.  When some other df fits best  
> he ends up with a different model and a *smaller* p-value than Alice.
>
> In particular, this is still true under the null hypothesis that  
> Age has no effect [If Alice and Bob are interested in p-values, the  
> null hypothesis must be plausible.]
>
> If Bob's p-values are always less than or equal to Alice's p-values  
> under the null hypothesis, and Alice's p-values are less than 0.05  
> 5% of the time, then Bob's p-values are less than 0.05 more than 5%  
> of the time.
>
>
>     -thomas
>
>
> Furthermore, shouldn't packages gam and mgcv give similar results
> when the same data and df are used? I tried this:
>
> library(gam)
> data(kyphosis)
> kyp1 <- gam(Kyphosis ~ s(Age, 4), family=binomial, data=kyphosis)
> kyp2 <- gam(Kyphosis ~ s(Number, 4), family=binomial, data=kyphosis)
> kyp3 <- gam(Kyphosis ~ s(Start, 4), family=binomial, data=kyphosis)
> anova.gam(kyp1)
> anova.gam(kyp2)
> anova.gam(kyp3)
>
> detach(package:gam)
> library(mgcv)
> kyp4 <- gam(Kyphosis ~ s(Age, k=4, fx=T),  family=binomial,
> data=kyphosis)
> kyp5 <- gam(Kyphosis ~ s(Number, k=4, fx=T),  family=binomial,
> data=kyphosis)
> kyp6 <- gam(Kyphosis ~ s(Start, k=4, fx=T),  family=binomial,
> data=kyphosis)
> anova.gam(kyp4)
> anova.gam(kyp5)
> anova.gam(kyp6)
>
>
> P levels for these models, by pair
>
> kyp1 vs kyp4: p= 0.083 and 0.068 respectively (not too bad)
> kyp2 vs kyp5: p= 0.445 and 0.03 (wow!)
> kyp3 vs kyp6: p= 0.053 and 0.008 (wow again)
>
> Also if you plot all these you find that the mgcv plots are smoother
> than the gam plots, even the same df are used all the time.
>
> I am really confused now!
>
> Denis
>

>
> Thomas Lumley            Assoc. Professor, Biostatistics
> tlumley at u.washington.edu    University of Washington, SeattleDe :  
> Thomas Lumley <tlumley at u.washington.edu>
> Date : 28 septembre 2005 14:35:26 HAE
> À : Denis Chabot <chabotd at globetrotter.net>
> Cc : Peter Dalgaard <p.dalgaard at biostat.ku.dk>, R list <r- 
> help at stat.math.ethz.ch>
> Objet : Rép : [R] p-level in packages mgcv and gam
>
> On Wed, 28 Sep 2005, Denis Chabot wrote:
>
> But what about another analogy, that of polynomials? You may not be  
> sure what degree polynomial to use, and you have not decided before  
> analysing your data. You fit different polynomials to your data,  
> checking if added degrees increase r2 sufficiently by doing F-tests.
>
> Yes, you can. And this procedure gives you incorrect p-values.
>
>  They may not be very incorrect -- it depends on how much model  
> selection you do, and how strongly the feature you are selecting on  
> is related to the one you are testing.
>
> For example, using step() to choose a polynomial in x even when x  
> is unrelated to y and z inflates the Type I error rate by giving a  
> biased estimate of the residual mean squared error:
>
> once<-function(){
>   y<-rnorm(50);x<-runif(50);z<-rep(0:1,25)
>   summary(step(lm(y~z),
>         scope=list(lower=~z,upper=~z+x+I(x^2)+I(x^3)+I(x^4)),
>         trace=0))$coef["z",4]
>  }
> p<-replicate(1000,once())
> mean(p<0.05)
> [1] 0.072
>
> which is significantly higher than you would expect for an honest  
> level 0.05 test.
>
>     -thomas



From reid_huntsinger at merck.com  Thu Sep 29 22:08:20 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 29 Sep 2005 16:08:20 -0400
Subject: [R] solution of convolution equation
Message-ID: <355C35514FEAC9458F75947F5270974D076CFD@usctmx1103.merck.com>

You need to pad both fx and fz with zeros to at least a length of length(fx)
+ length(fz) - 1, because you're really computing a circular convolution.
The same holds in higher dimensions (for each dimension).

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anna Oganyan
Sent: Thursday, September 29, 2005 3:23 PM
To: r-help at stat.math.ethz.ch
Subject: [R] solution of convolution equation


Hello,
May be somebody can help me...
I am trying to find a solution of a convolution equation using fft (and 
unfortunately I do not have a good background for this).
So I am just trying to figure out how it can be implemented in R. I have 
two multidimensional independent variables X and Z
and I know their densities fx and fz, which are multidimensional arrays. 
So I have to find the density of Y, such that X+Y=Z.
So, first I tried on a simple example, where the variables are 
one-dimensional, say X is N(0,1) and Z is N(7,3).
So I want to find the density of Y (which should be N(7, sqrt(8)).
I did:
x <- seq(-6, 20, length=500)
fx <- dnorm(x)
z <- seq(-6, 20, length=500)
fz <- dnorm(z, mean=7, sd=3)
ffty <- fft(fz)/fft(fx)
fy <- fft(ffty, inverse=T)/length(ffty)
plot(Re(fy))

But the plot is far from being normal. May be the problem is with the 
lengths of fx and fz? should they be of different lengths and fx padded 
with zeros? May be somebody could point out that...? Thanks!
Anna

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From kjetil at redcotel.bo  Thu Sep 29 22:22:00 2005
From: kjetil at redcotel.bo (Kjetil Holuerson)
Date: Thu, 29 Sep 2005 16:22:00 -0400
Subject: [R] Fisher's discriminant functions
In-Reply-To: <20050929150455.24592.qmail@web25411.mail.ukl.yahoo.com>
References: <20050929150455.24592.qmail@web25411.mail.ukl.yahoo.com>
Message-ID: <433C4CE8.20106@redcotel.bo>

This are in various packages, you could have a look at
ade4 (on CRAN).

Kjetil



C NL wrote:
> Hi everyone,
> 
>   I'm trying to solve a problem about how to get the
> Fisher's discriminant functions of a "lda" (linear
> discriminant analysis) object, I mean, the object
> obtained from doing "lda(formula, data)" function of
> the package MASS in R-project. This object gives me
> the canonical linear functions (n-1 coefficients
> matrix of n groups at least), and only with this
> information I could predict the group of an
> observation data using the "predict" function. But
> what I need is the Fisher's discriminant functions (n
> coefficients matrix of n groups) in order to classify
> my future data.
> 
>    The object "predict" gives me only the following
> attributes "x", "posterior" and "class", but none of
> them are the coefficients matrix of the Fisher's
> discriminant functions, and the reason why I'm not
> using the "predict" function for my predictions is
> because the time spent is very high for what I'm
> expecting, about 0.5 seconds while I can obtain this
> prediction with the Fisher's discriminant functions
> faster.
> 
>    So, I don't know if there's a package which I can
> use to obtain the mentioned coefficients matrix of the
> Fisher's discriminant functions.
> 
>    I anyone can help, I would appreciate it greatly.
> 
> Thank you and regards.
> 
>    Carlos Niharra L??pez
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 



--



From Matthias.Kohl at uni-bayreuth.de  Thu Sep 29 22:37:38 2005
From: Matthias.Kohl at uni-bayreuth.de (Matthias Kohl)
Date: Thu, 29 Sep 2005 22:37:38 +0200
Subject: [R] solution of convolution equation
In-Reply-To: <433C3F0B.2020001@niss.org>
References: <433C3F0B.2020001@niss.org>
Message-ID: <433C5092.8030605@uni-bayreuth.de>

Anna Oganyan wrote:

>Hello,
>May be somebody can help me...
>I am trying to find a solution of a convolution equation using fft (and 
>unfortunately I do not have a good background for this).
>So I am just trying to figure out how it can be implemented in R. I have 
>two multidimensional independent variables X and Z
>and I know their densities fx and fz, which are multidimensional arrays. 
>So I have to find the density of Y, such that X+Y=Z.
>So, first I tried on a simple example, where the variables are 
>one-dimensional, say X is N(0,1) and Z is N(7,3).
>So I want to find the density of Y (which should be N(7, sqrt(8)).
>I did:
>x <- seq(-6, 20, length=500)
>fx <- dnorm(x)
>z <- seq(-6, 20, length=500)
>fz <- dnorm(z, mean=7, sd=3)
>ffty <- fft(fz)/fft(fx)
>fy <- fft(ffty, inverse=T)/length(ffty)
>plot(Re(fy))
>
>But the plot is far from being normal. May be the problem is with the 
>lengths of fx and fz? should they be of different lengths and fx padded 
>with zeros? May be somebody could point out that?? Thanks!
>Anna
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>
Hello Anna,

in our R package "distr" (on CRAN) we have implemented a convolution 
algorithm via fft.
See also: 
http://www.uni-bayreuth.de/departments/math/org/mathe7/KOHL/pubs/comp.pdf
respectively

library(distr)
getMethod("+", signature=c("AbscontDistribution","AbscontDistribution"))

(or see the sources)

Unfortunatelly, we haven't implemented our algorithms for 
multidimensional distributions so far.

hope that helps,
Matthias



From deepayan.sarkar at gmail.com  Thu Sep 29 22:40:05 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 29 Sep 2005 15:40:05 -0500
Subject: [R] xyplots
In-Reply-To: <2009CC51-AFF6-4543-AA75-BEAB656E31D8@utah.edu>
References: <2009CC51-AFF6-4543-AA75-BEAB656E31D8@utah.edu>
Message-ID: <eb555e6605092913404a8480c6@mail.gmail.com>

On 9/28/05, Nathan Leon Pace, MD, MStat <N.L.Pace at m.cc.utah.edu> wrote:

> A related question:
>
> My xyplot is essentially a time series (up-and-down experimental
> design). Thus I need to connect the points sequentially regardless of
> the group value. With the groups argument, type = 'b' gives two lines
> - one for each group.

xyplot(rnorm(100) ~ 1:100,
       groups = sample(1:2, 100, TRUE),
       panel = function(x, y, ..., type) {
           panel.lines(x, y, ...)
           panel.superpose(x, y, ..., type = 'p')
       })



From mlyman at byu.edu  Thu Sep 29 22:44:38 2005
From: mlyman at byu.edu (Mark Lyman)
Date: Thu, 29 Sep 2005 14:44:38 -0600
Subject: [R] Bug in lmer?
Message-ID: <433C5236.4060104@byu.edu>

I am relatively new to R so I am not confident enough in what I am doing 
to be certain this is a bug. I am running R 2.1.1 on a Windows XP 
machine and the lme4 package version 0.98-1. The following code fits the 
model I want using the nlme package version 3.1-60.

mltloc$loc <- factor(mltloc$loc)
mltloc$block <- factor(mltloc$block)
mltloc$trt <- factor(mltloc$trt)
Mltloc <- groupedData(adg~trt|loc,mltloc)
plot(Mltloc)
plot(Mltloc,outer=~trt)

lme(adg~trt,
random=pdBlocked(list(pdIdent(~1),pdIdent(~block-1),pdIdent(~trt-1))),Mltloc)

The problem is that when I try fitting the model using the lmer function 
with the following code:

lmer(adg~trt+(1|loc)+(1|block:loc)+(1|loc:trt),mltloc)

I get this message from Windows and R closes.

R for Windows GUI front-end has encountered a problem and needs to 
close.  We are sorry for the inconvenience.

This same code works on a Macintosh. So it doesn't seem that I have made 
an error in my code. Also if anyone of the random effect terms is 
removed there is no problem. Is this something that is being looked at? 
Or I have I made a mistake somewhere? I have included the data that I am 
using below.


      X obs loc block trt     adg      fe
1     1   3   A     1   3 3.16454  7.1041
2     2   4   A     1   4 3.12500  6.6847
3     3   6   A     1   2 3.15944  6.8338
4     4   7   A     1   1 3.25000  6.5254
5     5   9   A     2   2 2.71301  8.2505
6     6  10   A     2   1 3.20281  7.5922
7     7  12   A     2   3 3.02423  7.3894
8     8  16   A     2   4 2.87245  7.4604
9     9  19   A     3   1 2.68878  8.2785
10   10  20   A     3   2 2.86862  7.9470
11   11  21   A     3   3 2.89923  7.9739
12   12  22   A     3   4 3.02806  8.4331
13   13  25   B     1   3 2.18131  6.6691
14   14  27   B     1   4 2.51914  5.6281
15   15  28   B     1   2 1.88739  7.0723
16   16  31   B     1   1 2.34685  6.0295
17   17  33   B     2   4 2.45608  5.6195
18   18  35   B     2   1 2.25225  6.3978
19   19  36   B     2   3 2.23649  6.1799
20   20  40   B     2   2 2.47523  5.9985
21   21  41   B     3   2 1.94200  7.2975
22   22  44   B     3   3 2.43243  6.4350
23   23  47   B     3   4 2.30180  6.3339
24   24  48   B     3   1 2.53378  6.1564
25   25  50   C     1   4 2.96014  7.5110
26   26  51   C     1   2 3.23551  7.4762
27   27  54   C     1   3 3.24638  7.2063
28   28  56   C     1   1 3.04710  7.6389
29   29  58   C     2   3 3.26449  7.5466
30   30  59   C     2   2 2.71377  9.0895
31   31  61   C     2   1 3.06522  7.8723
32   32  62   C     2   4 2.71739  8.2318
33   33  66   C     3   4 3.03623  7.9426
34   34  67   C     3   3 3.10507  8.4608
35   35  69   C     3   1 3.16304  8.5549
36   36  70   C     3   2 3.02899  8.5038
37   37  74   D     1   1 2.49164  9.5758
38   38  77   D     1   3 2.51833  9.5121
39   39  79   D     1   2 2.35631 10.3264
40   40  80   D     1   4 2.30331  9.7715
41   41  81   D     2   3 2.72688  9.5628
42   42  83   D     2   2 2.59512  9.9414
43   43  85   D     2   1 2.56516  9.3887
44   44  88   D     2   4 2.91523  8.3158
45   45  89   D     3   3 2.57943 10.4416
46   46  90   D     3   4 2.98159  8.7710
47   47  93   D     3   2 2.35370 11.0148
48   48  94   D     3   1 2.21953 11.2417
49   49  99   E     1   3 2.84158  8.7886
50   50 100   E     1   4 2.65264  8.6946
51   51 102   E     1   2 2.47112  9.7143
52   52 103   E     1   1 2.89769  9.2401
53   53 105   E     2   2 2.57343  9.5353
54   54 106   E     2   1 2.99752  8.7538
55   55 110   E     2   4 2.95380  8.8210
56   56 112   E     2   3 3.08663  8.9427
57   57 114   E     3   1 2.72525  9.4308
58   58 115   E     3   2 2.75825  9.7721
59   59 116   E     3   3 3.08333  8.9010
60   60 117   E     3   4 3.12129  8.4852
61   61 122   F     1   1 3.20600  6.3983
62   62 123   F     1   2 2.89500  6.6569
63   63 125   F     1   4 3.36900  6.0821
64   64 126   F     1   3 3.12000  6.5349
65   65 130   F     2   2 3.19300  6.6729
66   66 131   F     2   1 3.29800  6.5488
67   67 133   F     2   4 3.09700  6.6598
68   68 135   F     2   3 3.38500  6.2998
69   69 139   F     3   3 3.44900  6.2849
70   70 140   F     3   2 3.05000  6.9957
71   71 141   F     3   1 3.43500  6.7302
72   72 143   F     3   4 3.60600  6.3827
73   73 145   G     1   2 2.58669  8.1394
74   74 147   G     1   1 3.17892  7.0972
75   75 148   G     1   4 2.95284  7.3140
76   76 151   G     1   3 3.17924  6.9430
77   77 154   G     2   4 2.62344  7.5150
78   78 155   G     2   3 2.64286  8.0237
79   79 157   G     2   1 3.12760  7.3169
80   80 160   G     2   2 2.54993  8.1957
81   81 163   G     3   4 2.58322  7.9687
82   82 164   G     3   3 2.84813  7.9284
83   83 166   G     3   2 2.69279  8.5303
84   84 167   G     3   1 3.14424  7.3564
85   85 169   H     1   3 3.39974  6.5945
86   86 173   H     1   2 3.12370  6.7530
87   87 175   H     1   4 3.17969  6.4279
88   88 176   H     1   1 3.70052  6.4830
89   89 177   H     2   2 2.95192  7.3809
90   90 179   H     2   3 3.44661  6.7929
91   91 182   H     2   4 3.28906  6.4807
92   92 184   H     2   1 3.37500  6.8139
93   93 188   H     3   4 3.65104  6.3068
94   94 190   H     3   1 3.27734  7.4789
95   95 191   H     3   3 3.42708  6.9327
96   96 192   H     3   2 3.04818  7.7264
97   97 193   I     1   2 2.22105  8.4243
98   98 196   I     1   1 3.15526  6.7119
99   99 197   I     1   4 2.40263  7.7486
100 100 198   I     1   3 3.00000  6.9215
101 101 203   I     2   1 2.29079  8.8861
102 102 204   I     2   4 2.25395  8.6850
103 103 205   I     2   3 2.60526  8.4150
104 104 208   I     2   2 2.34737  8.7866
105 105 209   I     3   2 2.50505  8.5975
106 106 212   I     3   1 2.31316  8.9267
107 107 213   I     3   4 2.42105  8.7750
108 108 214   I     3   3 2.74211  8.1599



From kjetil at redcotel.bo  Thu Sep 29 23:00:02 2005
From: kjetil at redcotel.bo (Kjetil Holuerson)
Date: Thu, 29 Sep 2005 17:00:02 -0400
Subject: [R] standard error of variances and covariances of the random
 effects with LME
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F740A31C81F@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F740A31C81F@dc1ex2.air.org>
Message-ID: <433C55D2.8020000@redcotel.bo>

Doran, Harold wrote:
> You cannot.

Yes. But when it is really needed, as the original poster
said, what would be wrong with taking the length of a
95% confidence interval and dividing into 4?

(of course it will be wrong, but so much as to be useless?)

Kjetil



  Also, it's not that the distribution of the random effects
> is not symmetric, but that it *may* not be symmetric, and this is an
> assumption that should be checked. 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roel de Jong
> Sent: Thursday, September 29, 2005 9:20 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] standard error of variances and covariances of the random
> effects with LME
> 
> Hello,
> 
> how do I obtain standard errors of variances and covariances of the
> random effects with LME comparable to those of for example MlWin? I know
> you shouldn't use them because the distribution of the estimator isn't
> symmetric blablabla, but I need a measure of the variance of those
> estimates for pooling my multiple imputation results.
> 
> Regards,
>    Roel.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 



--



From Ted.Harding at nessie.mcc.ac.uk  Thu Sep 29 23:35:25 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 29 Sep 2005 22:35:25 +0100 (BST)
Subject: [R] Select varying LS digits in long numbers?
In-Reply-To: <XFMail.050929184531.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050929223525.Ted.Harding@nessie.mcc.ac.uk>

I've received what looks like a good -- and neat! -- solution
off-list from Hadley Wickham. Suppose "numbers" is a vector of
long integers. Then Hadley wrote:

  digits <- outer(numbers, 10:0,
                  function(x,y) numbers %/% 10^y %% 10)

  apply(digits,2, function(x) length(unique(x)))

This certainly looks cast-iron, and nicely passes my "slipperiness"
test:

  numbers<-c(1234566999,1234567001)
  digits <- outer(numbers, 10:0, function(x,y) numbers %/% 10^y %% 10)
  result<-apply(digits,2, function(x) length(unique(x))); result
# [1] 1 1 1 1 1 1 1 2 2 2 2

  d1<-min(which(result>1)); d1
# [1] 8

  numbers%%(10^(12-d1))
# [1] 6999 7001

as desired!

Thanks also to Patrick Burns and Jim Holtman for other suggestions
based (in effect) on diff(range(numbers)).

On 29-Sep-05 Ted Harding wrote:
> Hi Folks,
> 
> I'm trying to find a neat solution to an apparently simple
> problem, but one which turns out to be a bit more intricate
> and tricky than one might expect.
> 
> Suppose I have numbers given to a large number of digits.
> For example
> 
>   1234567021
> 
> where (though I don't know this beforehand) only the last
> 3 digits will be varying (and all 3 will vary).
> 
> What I want is, give a vector x of such numbers, to extract
> the minimal set of final digits which will include the varying
> digits (i.e. in this case the last 3 digits). And there may be
> a decimal point somewhere along the line (though again I won't
> know where, nor whether).
> 
> I can think of brute-force ways of doing this, but I'd like
> a neat one!
> 
> Best wishes to all,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 29-Sep-05                                       Time: 18:45:26
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 29-Sep-05                                       Time: 22:35:15
------------------------------ XFMail ------------------------------



From parkjoha at msu.edu  Fri Sep 30 00:08:33 2005
From: parkjoha at msu.edu (Johann Park)
Date: Thu, 29 Sep 2005 18:08:33 -0400
Subject: [R] Binary Logit Regression with R
Message-ID: <E1EL6Zt-0000Oh-UV@sys34.mail.msu.edu>

Hi to all, 

I am a PH.D Student doing statistical analysis.
I am totally new to R. I previously use Stata and am changing into R. I 
ususally do with logit regression with binary dependent variable (war 
occurence:1 or 0). 

I just want to know command to do that. More sepcifically, 

Let say, my Y is war occurence (occur=1, otherwise 0). And my independent 
variables (Xs) are trade, democracy, military power....etc. 

In Stata, I do like what follows: 

logit war trade democracy militarypower... 

Then I will get results. 

What are the equivalent command in R? 

Many thanks, 

JP



From mschwartz at mn.rr.com  Fri Sep 30 00:21:53 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 29 Sep 2005 17:21:53 -0500
Subject: [R] Binary Logit Regression with R
In-Reply-To: <E1EL6Zt-0000Oh-UV@sys34.mail.msu.edu>
References: <E1EL6Zt-0000Oh-UV@sys34.mail.msu.edu>
Message-ID: <1128032514.5402.68.camel@localhost.localdomain>

On Thu, 2005-09-29 at 18:08 -0400, Johann Park wrote:
> Hi to all, 
> 
> I am a PH.D Student doing statistical analysis.
> I am totally new to R. I previously use Stata and am changing into R. I 
> ususally do with logit regression with binary dependent variable (war 
> occurence:1 or 0). 
> 
> I just want to know command to do that. More sepcifically, 
> 
> Let say, my Y is war occurence (occur=1, otherwise 0). And my independent 
> variables (Xs) are trade, democracy, military power....etc. 
> 
> In Stata, I do like what follows: 
> 
> logit war trade democracy militarypower... 
> 
> Then I will get results. 
> 
> What are the equivalent command in R? 
> 
> Many thanks, 
> 
> JP

See ?glm in the base stats package or ?lrm in Frank Harrell's Design
package on CRAN.

BTW, doing:

  help.search("logit")

or 

  RSiteSearch("logit")

would provide you with the ability to do keyword searches of your
current R installation or the online e-mail list and documentation
archives, respectively.

You should also review Chapter 11 - Statistical Models in R in "An
Introduction to R", which is installed with R or available online under
the Documentation/Manuals link on the main R web site.

HTH,

Marc Schwartz



From Kevin.Wang at maths.anu.edu.au  Fri Sep 30 00:23:54 2005
From: Kevin.Wang at maths.anu.edu.au (Ko-Kang Kevin Wang)
Date: Fri, 30 Sep 2005 08:23:54 +1000
Subject: [R] Binary Logit Regression with R
In-Reply-To: <E1EL6Zt-0000Oh-UV@sys34.mail.msu.edu>
References: <E1EL6Zt-0000Oh-UV@sys34.mail.msu.edu>
Message-ID: <433C697A.1070806@maths.anu.edu.au>

Hi,

Johann Park wrote:

> Let say, my Y is war occurence (occur=1, otherwise 0). And my independent 
> variables (Xs) are trade, democracy, military power....etc. 

Take a look at ?glm.

HTH,

Kev

-- 
Ko-Kang Kevin Wang
PhD Student
Centre for Bioinformation Science
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 2601
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7488
Ph (M): +61-40-451-8301



From bitwrit at ozemail.com.au  Fri Sep 30 10:42:11 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 30 Sep 2005 08:42:11 +0000
Subject: [R] Display values in piechart/barplot
In-Reply-To: <000701c5c4f2$2d591630$d636a986@mast405>
References: <000701c5c4f2$2d591630$d636a986@mast405>
Message-ID: <433CFA63.5060506@ozemail.com.au>

Volker Rehbock wrote:
> Is it possible to automatically display the underlying values of a 
> piechart/barplot in the graphic? If so, which package/function/argument do I 
> need for it?

floating.pie and pie.labels in the plotrix package.

Jim



From bitwrit at ozemail.com.au  Fri Sep 30 10:58:16 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 30 Sep 2005 08:58:16 +0000
Subject: [R] reshaping data?
In-Reply-To: <ypx6hdc4j9pp.fsf@uracil.uio.no>
References: <ypx6hdc4j9pp.fsf@uracil.uio.no>
Message-ID: <433CFE28.4030508@ozemail.com.au>

Karin Lagesen wrote:
> I have a file like this:
> 
> 
> a       0.1
> a       0.2
> a       0.9
> b       0.5
> b       0.9
> b       0.7
> c       0.6
> c       0.99
> c       0.88
> 
> Which I would like to get to be the following matrix:
> 
>       0.1     0.2    0.3    0.4  ...  
> a     1        2     0       0 
> b     0        0     0       0 
> ..
> 
If you have a file named KL.dat with the above data in it:

KL.df<-read.table("KL.dat",header=FALSE)
KL.mat<-as.matrix(table(KL.df))

Jim



From dejongroel at gmail.com  Fri Sep 30 01:10:33 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Fri, 30 Sep 2005 01:10:33 +0200
Subject: [R] standard error of variances and covariances of the random
 effects with LME
In-Reply-To: <40e66e0b0509291211768422a9@mail.gmail.com>
References: <88EAF3512A55DF46B06B1954AEF73F740A31C81F@dc1ex2.air.org>
	<40e66e0b0509291211768422a9@mail.gmail.com>
Message-ID: <433C7469.7000200@gmail.com>

Well I finally figured it out. If you use the delta method with 
transformations exp(x) for the standard deviations and 
((exp(y)-1)/(exp(y)+1) for the correlation elements of the apVar 
structure, which is btw *not* the inverse of a fisher transformation, 
you get the standard errors.


Douglas Bates wrote:
> With lme you could but it would take a while to work it out.  There is
> an approximate Hessian matrix for the parameters that determine the
> variance-covariance matrix in there somewhere and if you were
> sufficiently persistent you could convert that apVar component to the
> scale of the variances and covariances.  I believe it is in the scale
> of the logarithm of the standard deviation and Fisher's z
> transformation (i.e. the hyperbolic arc tangent) of the correlation.
> 
> On 9/29/05, Doran, Harold <HDoran at air.org> wrote:
> 
>>You cannot. Also, it's not that the distribution of the random effects
>>is not symmetric, but that it *may* not be symmetric, and this is an
>>assumption that should be checked.
>>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roel de Jong
>>Sent: Thursday, September 29, 2005 9:20 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] standard error of variances and covariances of the random
>>effects with LME
>>
>>Hello,
>>
>>how do I obtain standard errors of variances and covariances of the
>>random effects with LME comparable to those of for example MlWin? I know
>>you shouldn't use them because the distribution of the estimator isn't
>>symmetric blablabla, but I need a measure of the variance of those
>>estimates for pooling my multiple imputation results.
>>
>>Regards,
>>   Roel.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
>



From c_naber at yahoo.com.br  Fri Sep 30 03:55:10 2005
From: c_naber at yahoo.com.br (Caio Lucidius Naberezny Azevedo)
Date: Fri, 30 Sep 2005 01:55:10 +0000 (GMT)
Subject: [R] mvtnorm package
Message-ID: <20050930015510.98773.qmail@web34007.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050930/0d578549/attachment.pl

From miguel_hoz at yahoo.es  Fri Sep 30 04:13:15 2005
From: miguel_hoz at yahoo.es (Miguel de la Hoz)
Date: Fri, 30 Sep 2005 04:13:15 +0200 (CEST)
Subject: [R] Code to read a SAS file
Message-ID: <20050930021315.97112.qmail@web86907.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050930/56dbf8bb/attachment.pl

From edd at debian.org  Fri Sep 30 04:23:19 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 29 Sep 2005 21:23:19 -0500
Subject: [R] mvtnorm package
In-Reply-To: <20050930015510.98773.qmail@web34007.mail.mud.yahoo.com>
References: <20050930015510.98773.qmail@web34007.mail.mud.yahoo.com>
Message-ID: <17212.41367.44898.584277@basebud.nulle.part>


On 30 September 2005 at 01:55, Caio Lucidius Naberezny Azevedo wrote:
| ==================================================
| downloaded 160Kb
| * Installing *source* package 'mvtnorm' ...
| ** libs
| g77   -fPIC  -g -O2 -c mvt.f -o mvt.o
| gcc -I/usr/lib/R/include     -fPIC  -g -O2 -c randomF77.c -o randomF77.o
| gcc -shared  -o mvtnorm.so mvt.o randomF77.o  -lg2c -lm -lgcc_s -L/usr/lib/R/lib -lR
| /usr/bin/ld: cannot find -lg2c
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

That's the error. 

If you're on Debian, you need libg2c0-dev. But on Debian you may as just
install the existing binary package r-cran-mvtnorm.

If you're not on Debian you need to figure out where to get the complete set
of your libg2c tools from.

| I don't know what's the problem because I've installed the package "sn" with sucess

Sn may not contain fortran sources.

Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From vincent at 7d4.com  Fri Sep 30 08:24:29 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Fri, 30 Sep 2005 08:24:29 +0200
Subject: [R] Saving Graphics
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D01123457@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D01123457@MAILBE2.westat.com>
Message-ID: <433CDA1D.1020102@7d4.com>

Mike Jones a ??crit :

> Is there a way to save graphs from the graphics 
> window using commands in the R console?  

you may try :
bmp("myimage.bmp");
plot(...);
dev.off();
# png(), jpeg() also available
hih



From maechler at stat.math.ethz.ch  Fri Sep 30 09:21:37 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 30 Sep 2005 09:21:37 +0200
Subject: [R] Bug in lmer?
In-Reply-To: <433C5236.4060104@byu.edu>
References: <433C5236.4060104@byu.edu>
Message-ID: <17212.59265.851874.843569@stat.math.ethz.ch>

>>>>> "Mark" == Mark Lyman <mlyman at byu.edu>
>>>>>     on Thu, 29 Sep 2005 14:44:38 -0600 writes:

    Mark> I am relatively new to R so I am not confident enough in what I am doing 
    Mark> to be certain this is a bug. 


    Mark> I am running R 2.1.1 on a Windows XP 
    Mark> machine and the lme4 package version 0.98-1. 

lme4 nowadays is heavily based on "Matrix" which version are you
using there?

    Mark> The following code fits the model I want using the
    Mark> nlme package version 3.1-60.

      < .............. >  {see a script at the end}

    Mark> The problem is that when I try fitting the model using
    Mark> the lmer function with the following code:

    Mark> lmer(adg~trt+(1|loc)+(1|block:loc)+(1|loc:trt),mltloc)

    Mark> I get this message from Windows and R closes.

    Mark> >> R for Windows GUI front-end has encountered a problem and needs to 
    Mark> >> close.  We are sorry for the inconvenience.

That definitely means there is a bug.
The question is *where* the bug is:  "lme4", "Matrix", "R", "Windows".

One first thing coming to mind is a mismatch of "lme4" and "Matri

    Mark> This same code works on a Macintosh. So it doesn't
    Mark> seem that I have made an error in my code. 

correct; I can also fit the model nice and quickly on Linux,
and summary() confirms the same fit {with the "usual problem" of
different estimates for the degrees of freedoms 'df'}.

So currently the bug only shows on the Windows platform. 
Could it be that you have a mismatching package "Matrix" version
there, but not on the Mac?

    Mark> Also if anyone of the random effect terms is removed there is
    Mark> no problem. Is this something that is being looked at?

not yet, AFAIK.

    Mark> Or I have I made a mistake somewhere? I have included
    Mark> the data that I am using below.

I'm putting the data and an R script up for FTP,
so that you or others can run this ``from anywhere'' via

 source("ftp://stat.ethz.ch/U/maechler/R/mltloc-ex.R", echo = TRUE)

Maybe this helps diagnosis,
Martin Maechler



From HStevens at MUOhio.edu  Fri Sep 30 09:40:41 2005
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Fri, 30 Sep 2005 03:40:41 -0400
Subject: [R] Bug in lmer?
In-Reply-To: <17212.59265.851874.843569@stat.math.ethz.ch>
References: <433C5236.4060104@byu.edu>
	<17212.59265.851874.843569@stat.math.ethz.ch>
Message-ID: <363499F9-ED9B-4261-85D0-224D21C02650@MUOhio.edu>

To All:
Mark and Martin's code ran fine on R : Version 2.1.1  (2005-06-20) on  
a Macintosh Dual 2 GHz G5 with 1.5 GB DDR SDRAM.
Hank

On Sep 30, 2005, at 3:21 AM, Martin Maechler wrote:

> source("ftp://stat.ethz.ch/U/maechler/R/mltloc-ex.R", echo = TRUE)



Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From lixiazhu at hotmail.com  Thu Sep 29 21:38:07 2005
From: lixiazhu at hotmail.com (Lixia ZHU)
Date: Thu, 29 Sep 2005 15:38:07 -0400
Subject: [R] cox proportional-hazards regress for interval censor data
Message-ID: <BAY101-F3271A5440706E684B9E602DD8C0@phx.gbl>

Hi. I used coxph(surv(start,end,event)~~event,data) to deal with interval 
censor data.
Does anyone know similar samples using 
coxph(surv(start,end,event)~~event,data)?
If you knows, can you tell me? I'll really appreciate it.

Thank you very much




R learner.



From alekhine_sv at yahoo.com  Fri Sep 30 06:47:34 2005
From: alekhine_sv at yahoo.com (=?iso-8859-1?q?Jos=E9=20Raul=20Capablanca?=)
Date: Thu, 29 Sep 2005 23:47:34 -0500 (CDT)
Subject: [R] List Email Statistic
Message-ID: <20050930044734.77405.qmail@web34203.mail.mud.yahoo.com>

Dear All, 
How can I can to know a mail list , to speak about
exclusively statistic. Thanks. 

__________________________________________________

Espacio para todos tus mensajes, antivirus y antispam ??gratis!



From HStevens at MUOhio.edu  Fri Sep 30 09:55:59 2005
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Fri, 30 Sep 2005 03:55:59 -0400
Subject: [R] Bug in lmer?
In-Reply-To: <363499F9-ED9B-4261-85D0-224D21C02650@MUOhio.edu>
References: <433C5236.4060104@byu.edu>
	<17212.59265.851874.843569@stat.math.ethz.ch>
	<363499F9-ED9B-4261-85D0-224D21C02650@MUOhio.edu>
Message-ID: <A875BBEE-826A-42B1-8DB8-8B4557BB51AA@MUOhio.edu>

I forgot to mention the OS - Mac OS 10.4.2 (Tiger with the latest  
update). {:-)
Hank
On Sep 30, 2005, at 3:40 AM, Martin Henry H. Stevens wrote:

> To All:
> Mark and Martin's code ran fine on R : Version 2.1.1  (2005-06-20) on
> a Macintosh Dual 2 GHz G5 with 1.5 GB DDR SDRAM.
> Hank
>
> On Sep 30, 2005, at 3:21 AM, Martin Maechler wrote:
>
>
>> source("ftp://stat.ethz.ch/U/maechler/R/mltloc-ex.R", echo = TRUE)
>>
>
>
>
> Dr. Martin Henry H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From ggrothendieck at gmail.com  Fri Sep 30 10:15:06 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 30 Sep 2005 04:15:06 -0400
Subject: [R] List Email Statistic
In-Reply-To: <20050930044734.77405.qmail@web34203.mail.mud.yahoo.com>
References: <20050930044734.77405.qmail@web34203.mail.mud.yahoo.com>
Message-ID: <971536df0509300115s10fda164g3822ee7b5fc15ac0@mail.gmail.com>

There is some discussion and data sources of the volume of email on
the list in:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/27532.html

On 9/30/05, Jos?? Raul Capablanca <alekhine_sv at yahoo.com> wrote:
> Dear All,
> How can I can to know a mail list , to speak about
> exclusively statistic. Thanks.
>
> __________________________________________________
>
> Espacio para todos tus mensajes, antivirus y antispam ??gratis!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bitwrit at ozemail.com.au  Fri Sep 30 20:23:51 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 30 Sep 2005 18:23:51 +0000
Subject: [R] List Email Statistic
In-Reply-To: <20050930044734.77405.qmail@web34203.mail.mud.yahoo.com>
References: <20050930044734.77405.qmail@web34203.mail.mud.yahoo.com>
Message-ID: <433D82B7.70104@ozemail.com.au>

Jos?? Raul Capablanca wrote:
> Dear All, 
> How can I can to know a mail list , to speak about
> exclusively statistic. Thanks. 
> 
Hola Jose,

If you mean a list devoted exclusively to statistics

(Si desea una lista solo para la statistica)

http://www.jiscmail.ac.uk/lists/allstat.html

(y una lista econometria en espanol)

http://www.rediris.es/list/info/econometria.html

Jim



From fabrizio at ffbg.net  Fri Sep 30 10:46:31 2005
From: fabrizio at ffbg.net (Fabrizio Ferri Benedetti)
Date: Fri, 30 Sep 2005 10:46:31 +0200 (CEST)
Subject: [R] Searching for specific functions
Message-ID: <3145.150.128.150.28.1128069991.squirrel@www.ffbg.net>


  Hello everybody (I'm new here)

  I am looking for 2 specific functions. The first one is a variance
formula that does not divide by n-1 (I don't want the unbiased formula,
that's it). In case it does not exist, how do I modify the default
function code? Yes, a workaround would be to multiply the result by n-1
and then divide it by n, but I'd like to use a separate function, if
possible.

  The other function I'm looking for it's a median formula like this one:

http://www.faberitius.com/prototipo/gfx/clip_image002_0000.gif

  Where "Li" is the exact lower limit of the critical interval, "nb" is
the number of cases that lie below the critical interval, "nd" the number
of cases inside the mentioned interval, and "I" is the range of the
interval itself. As you can see this formula calculates the median in
categorical structured data.

  I couldn't find any functions that match the results done by hand... is
there a contrib package out there that may help me? Should I program the
function on my own?

  Thanks in advance,

           Fabrizio Ferri Benedetti



-- 
"La mayor sabidur??a que existe es conocerse a uno mismo" - Galileo Galilei



From Ted.Harding at nessie.mcc.ac.uk  Fri Sep 30 11:11:15 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 30 Sep 2005 10:11:15 +0100 (BST)
Subject: [R] List Email Statistic
In-Reply-To: <433D82B7.70104@ozemail.com.au>
Message-ID: <XFMail.050930101115.Ted.Harding@nessie.mcc.ac.uk>

On 30-Sep-05 Jim Lemon wrote:
> Jos?? Raul Capablanca wrote:
>> Dear All, 
>> How can I can to know a mail list , to speak about
>> exclusively statistic. Thanks. 
>> 
> Hola Jose,
> 
> If you mean a list devoted exclusively to statistics
> 
> (Si desea una lista solo para la statistica)
> 
> http://www.jiscmail.ac.uk/lists/allstat.html

I should point out that the 'allstat' list is, by policy, NOT
a discussion list (as many have found out!). It is primarily
a list for announcements etc. People do post queries, but replies
are not supposed to be sent to the list thought they are often
summarised to the list by the person making the original query.
Anything that looks as though it will develop into an on-list
discussion will be brought to a halt by the list administrator.

There is a good statistics discussion list STAT-L. See:

  http://lists.mcgill.ca/archives/stat-l.html

where you can sample the archives and find pointers to the admin
pages (for subscribing etc.). However, it has a low activity rate.

I think R-help also serves as a general statistics discussion list
which is as good as anything else I have come across! There are of
course many statistics lists for special areas of application or
particular methodology or particular software.

With best wishes,
Ted. [Or should I sign as "Emmanuel Lasker"?]


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 30-Sep-05                                       Time: 10:03:51
------------------------------ XFMail ------------------------------



From viudez_ant at gva.es  Fri Sep 30 12:34:42 2005
From: viudez_ant at gva.es (Toni =?iso-8859-1?q?Vi=FAdez?=)
Date: Fri, 30 Sep 2005 12:34:42 +0200
Subject: [R] Compare predict and experimental values
Message-ID: <200509301234.42254.viudez_ant@gva.es>

Hi everybody:
I just generate different maps of interpolation with different methods but 
with the same package -(gstat, i used the krige command). And now for 
evaluate the best method i want to compare the predict values versus 
experimental, but i don't know how do it.
Could anybody tell me how should do it?.
Thanks in advance.
-- 
########################################
	   Antoni Vi??dez Mora	
    Dept. Din??mica de Contaminantes
	    Fundaci??n CEAM
        Paterna (Valencia)-Spain
        tel: 961318190. ext: 216
 e-mail: toni at ceam.es  viudez_ant at gva.es
########################################



From stefan.albrecht at allianz.com  Fri Sep 30 12:28:20 2005
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Fri, 30 Sep 2005 12:28:20 +0200
Subject: [R] Easy cut & paste from Excel to R
Message-ID: <OF31C94419.F6BAF50D-ONC125708C.0038FEAD-C125708C.003986F4@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050930/208c9610/attachment.pl

From Roger.Bivand at nhh.no  Fri Sep 30 12:54:29 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 30 Sep 2005 12:54:29 +0200 (CEST)
Subject: [R] Compare predict and experimental values
In-Reply-To: <200509301234.42254.viudez_ant@gva.es>
Message-ID: <Pine.LNX.4.44.0509301247440.15787-100000@reclus.nhh.no>

On Fri, 30 Sep 2005, Toni Vi??dez wrote:

> Hi everybody:
> I just generate different maps of interpolation with different methods but 
> with the same package -(gstat, i used the krige command). And now for 
> evaluate the best method i want to compare the predict values versus 
> experimental, but i don't know how do it.
> Could anybody tell me how should do it?.

?krige.cv or ?gstat.cv provide kriging cross validation, including IDW. 
The leave-one-out cross validation RMSE agree with those in Geostatistical 
Analyst in ArcGIS. That is one possibility that you may choose if it fits 
your needs, but "best method" is only valid up to the data you have and 
the relative placing of the point locations. Surprisingly often, external 
drift does matter, and once other variables are taken into account, the 
residual spatial autocorrelation does not contribute so much.

> Thanks in advance.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From RKrug at sun.ac.za  Fri Sep 30 13:01:26 2005
From: RKrug at sun.ac.za (Rainer M. Krug)
Date: Fri, 30 Sep 2005 13:01:26 +0200
Subject: [R] Creating an array [0 to 101] of a list
Message-ID: <433D1B06.1060106@sun.ac.za>

Hi

I looked, but I didn't find it:

I need an array [0 to 101] where each element is a list (the result of 
Kest in spatstat).
I.e. I want to do:

A[2] <- Kest(pp1)
A[3] <- Kest(pp2)
...
A[101] <- Kest(pp100)

How can I create A ?

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology
University of Stellenbosch
Matieland 7602
South Africa



From Matthias.Templ at statistik.gv.at  Fri Sep 30 13:13:36 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 30 Sep 2005 13:13:36 +0200
Subject: [R] Creating an array [0 to 101] of a list
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAC34@xchg1.statistik.local>

Probably, you could do it like

A <- list()
for( i in 1:101){
  A[[i]] <- Kest( get( paste("pp",i,sep="") ) )
}

Best,
Matthias

> Hi
> 
> I looked, but I didn't find it:
> 
> I need an array [0 to 101] where each element is a list (the 
> result of 
> Kest in spatstat).
> I.e. I want to do:
> 
> A[2] <- Kest(pp1)
> A[3] <- Kest(pp2)
> ...
> A[101] <- Kest(pp100)
> 
> How can I create A ?
> 
> Rainer
> 
> -- 
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
> Biology (UCT)
> 
> Department of Conservation Ecology
> University of Stellenbosch
> Matieland 7602
> South Africa
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From RKrug at sun.ac.za  Fri Sep 30 13:20:55 2005
From: RKrug at sun.ac.za (Rainer M. Krug)
Date: Fri, 30 Sep 2005 13:20:55 +0200
Subject: [R] Creating an array [0 to 101] of a list
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BAC34@xchg1.statistik.local>
References: <83536658864BC243BE3C06D7E936ABD5027BAC34@xchg1.statistik.local>
Message-ID: <433D1F97.9010208@sun.ac.za>

Thanks a lot Matthias,

Rainer


TEMPL Matthias wrote:
> Probably, you could do it like
> 
> A <- list()
> for( i in 1:101){
>   A[[i]] <- Kest( get( paste("pp",i,sep="") ) )
> }
> 
> Best,
> Matthias
> 
> 
>>Hi
>>
>>I looked, but I didn't find it:
>>
>>I need an array [0 to 101] where each element is a list (the 
>>result of 
>>Kest in spatstat).
>>I.e. I want to do:
>>
>>A[2] <- Kest(pp1)
>>A[3] <- Kest(pp2)
>>...
>>A[101] <- Kest(pp100)
>>
>>How can I create A ?
>>
>>Rainer
>>
>>-- 
>>Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
>>Biology (UCT)
>>
>>Department of Conservation Ecology
>>University of Stellenbosch
>>Matieland 7602
>>South Africa
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read 
>>the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 



-- 
NEW TELEPHONE NUMBER
Tel:		+27 - (0)72 808 2975 (w)

Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de



From christoph.lehmann at gmx.ch  Fri Sep 30 13:30:16 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 30 Sep 2005 13:30:16 +0200
Subject: [R] Creating an array [0 to 101] of a list
In-Reply-To: <433D1B06.1060106@sun.ac.za>
References: <433D1B06.1060106@sun.ac.za>
Message-ID: <433D21C8.4090704@gmx.ch>

a <- array(vector("list", 3), dim=c(3))
a[[1]] <- list(x = 1, y = 0, z = -1)
a[[2]] <- list(x = 0, y = 1, z = -1)
a[[3]] <- list(x = 0, y = -1, z = 0)

HTH
christoph
Rainer M. Krug wrote:
> Hi
> 
> I looked, but I didn't find it:
> 
> I need an array [0 to 101] where each element is a list (the result of 
> Kest in spatstat).
> I.e. I want to do:
> 
> A[2] <- Kest(pp1)
> A[3] <- Kest(pp2)
> ...
> A[101] <- Kest(pp100)
> 
> How can I create A ?
> 
> Rainer
>



From john.maindonald at anu.edu.au  Fri Sep 30 13:50:48 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Fri, 30 Sep 2005 21:50:48 +1000
Subject: [R] R-help Digest, Vol 31, Issue 30
In-Reply-To: <mailman.4.1128074401.10918.r-help@stat.math.ethz.ch>
References: <mailman.4.1128074401.10918.r-help@stat.math.ethz.ch>
Message-ID: <2742A734-9B8B-43A7-BBB2-89D81C4EC42D@anu.edu.au>

With lme4, use of mcmcsamp can be insightful.  (Douglas Bates
drew my attention to this function in a private exchange of emails.)
The distributions of random effects are simulated on a log scale,
where the distributions are much closer to symmetry than on the
scale of the random effects themselves.  As far as I can see, this is
a straightforward use of MCMC to estimate model parameters; it is
not clear to me the results from the lmer() fit are used.
John Maindonald.


On 30 Sep 2005, at 8:00 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Roel de Jong <dejongroel at gmail.com>
> Date: 29 September 2005 11:19:38 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] standard error of variances and covariances of the  
> random    effects with LME
>
>
> Hello,
>
> how do I obtain standard errors of variances and covariances of the  
> random effects with LME comparable to those of for example MlWin? I  
> know you shouldn't use them because the distribution of the  
> estimator isn't symmetric blablabla, but I need a measure of the  
> variance of those estimates for pooling my multiple imputation  
> results.
>
> Regards,
>   Roel.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From renaud.lancelot at gmail.com  Fri Sep 30 14:21:46 2005
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Fri, 30 Sep 2005 14:21:46 +0200
Subject: [R] Bug in lmer?
In-Reply-To: <17212.59265.851874.843569@stat.math.ethz.ch>
References: <433C5236.4060104@byu.edu>
	<17212.59265.851874.843569@stat.math.ethz.ch>
Message-ID: <c2ee56800509300521417d3a91@mail.gmail.com>

I got a crash too with my Win XP config running:

platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status   beta
major    2
minor    2.0
year     2005
month    09
day      24
svn rev  35666
language R

Package:       Matrix
Version:       0.98-7
Date:          2005-09-09

Package:       lme4
Version:       0.98-1
Date:          2005-07-27

The monitoring of the crash with Dr.MingW provided the following information:

Rgui.exe caused an Access Violation at location 1001ed89 in module
R.dll Writing to location 00000008.

Registers:
eax=00000000 ebx=00000040 ecx=011c8a18 edx=011c8058 esi=011c8a40 edi=00000018
eip=1001ed89 esp=0022dc20 ebp=0022dc48 iopl=0         nv up ei pl nz ac pe nc
cs=001b  ss=0023  ds=0023  es=0023  fs=003b  gs=0000             efl=00000212

Call stack:
1001ED89  R.dll:1001ED89  malloc_usable_size
1001FC2A  R.dll:1001FC2A  Rm_malloc
1009EF69  R.dll:1009EF69  Rf_allocVector
100389D8  R.dll:100389D8  do_makevector
100A6141  R.dll:100A6141  do_internal
10081FBF  R.dll:10081FBF  Rf_eval
10084C2B  R.dll:10084C2B  Rf_applyClosure
10081D50  R.dll:10081D50  Rf_eval
10084C2B  R.dll:10084C2B  Rf_applyClosure
10081D50  R.dll:10081D50  Rf_eval
10083985  R.dll:10083985  do_set
10081FBF  R.dll:10081FBF  Rf_eval
10083A55  R.dll:10083A55  do_begin
10081FBF  R.dll:10081FBF  Rf_eval
10084C2B  R.dll:10084C2B  Rf_applyClosure
10081D50  R.dll:10081D50  Rf_eval
10081FBF  R.dll:10081FBF  Rf_eval
10083985  R.dll:10083985  do_set
10081FBF  R.dll:10081FBF  Rf_eval
10083A55  R.dll:10083A55  do_begin
10081FBF  R.dll:10081FBF  Rf_eval
1008457D  R.dll:1008457D  do_if
1008489B  R.dll:1008489B  R_execMethod
016A2907  methods.dll:016A2907  R_standardGeneric
100A9254  R.dll:100A9254  do_standardGeneric
10082094  R.dll:10082094  Rf_eval
10084C2B  R.dll:10084C2B  Rf_applyClosure
10081D50  R.dll:10081D50  Rf_eval
10083985  R.dll:10083985  do_set
10081FBF  R.dll:10081FBF  Rf_eval
10083716  R.dll:10083716  Rf_DispatchOrEval
10081FBF  R.dll:10081FBF  Rf_eval
10083A55  R.dll:10083A55  do_begin
10081FBF  R.dll:10081FBF  Rf_eval
10081FBF  R.dll:10081FBF  Rf_eval
10083A55  R.dll:10083A55  do_begin
10081FBF  R.dll:10081FBF  Rf_eval
1008457D  R.dll:1008457D  do_if
1008489B  R.dll:1008489B  R_execMethod
016A2907  methods.dll:016A2907  R_standardGeneric
100A9254  R.dll:100A9254  do_standardGeneric
10082094  R.dll:10082094  Rf_eval
10084C2B  R.dll:10084C2B  Rf_applyClosure
10081D50  R.dll:10081D50  Rf_eval
10083985  R.dll:10083985  do_set
10081FBF  R.dll:10081FBF  Rf_eval
1009AAF5  R.dll:1009AAF5  Rf_ReplIteration
1009AC27  R.dll:1009AC27  Rf_ReplIteration
1009AEB7  R.dll:1009AEB7  run_Rmainloop
004013AA  Rgui.exe:004013AA
00401315  Rgui.exe:00401315
00401568  Rgui.exe:00401568
00401236  Rgui.exe:00401236
00401288  Rgui.exe:00401288
7C816D4F  kernel32.dll:7C816D4F  RegisterWaitForInputIdle

Best,

Renaud

2005/9/30, Martin Maechler <maechler at stat.math.ethz.ch>:
> >>>>> "Mark" == Mark Lyman <mlyman at byu.edu>
> >>>>>     on Thu, 29 Sep 2005 14:44:38 -0600 writes:
>
>     Mark> I am relatively new to R so I am not confident enough in what I am doing
>     Mark> to be certain this is a bug.
>
>
>     Mark> I am running R 2.1.1 on a Windows XP
>     Mark> machine and the lme4 package version 0.98-1.
>
> lme4 nowadays is heavily based on "Matrix" which version are you
> using there?
>
>     Mark> The following code fits the model I want using the
>     Mark> nlme package version 3.1-60.
>
>       < .............. >  {see a script at the end}
>
>     Mark> The problem is that when I try fitting the model using
>     Mark> the lmer function with the following code:
>
>     Mark> lmer(adg~trt+(1|loc)+(1|block:loc)+(1|loc:trt),mltloc)
>
>     Mark> I get this message from Windows and R closes.
>
>     Mark> >> R for Windows GUI front-end has encountered a problem and needs to
>     Mark> >> close.  We are sorry for the inconvenience.
>
> That definitely means there is a bug.
> The question is *where* the bug is:  "lme4", "Matrix", "R", "Windows".
>
> One first thing coming to mind is a mismatch of "lme4" and "Matri
>
>     Mark> This same code works on a Macintosh. So it doesn't
>     Mark> seem that I have made an error in my code.
>
> correct; I can also fit the model nice and quickly on Linux,
> and summary() confirms the same fit {with the "usual problem" of
> different estimates for the degrees of freedoms 'df'}.
>
> So currently the bug only shows on the Windows platform.
> Could it be that you have a mismatching package "Matrix" version
> there, but not on the Mac?
>
>     Mark> Also if anyone of the random effect terms is removed there is
>     Mark> no problem. Is this something that is being looked at?
>
> not yet, AFAIK.
>
>     Mark> Or I have I made a mistake somewhere? I have included
>     Mark> the data that I am using below.
>
> I'm putting the data and an R script up for FTP,
> so that you or others can run this ``from anywhere'' via
>
>  source("ftp://stat.ethz.ch/U/maechler/R/mltloc-ex.R", echo = TRUE)
>
> Maybe this helps diagnosis,
> Martin Maechler
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From vincent at 7d4.com  Fri Sep 30 14:30:45 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Fri, 30 Sep 2005 14:30:45 +0200
Subject: [R] correlation question
Message-ID: <433D2FF5.5080602@7d4.com>

Dear R-helpers,

First, double apologies because the question is not
directly related to R, and also probably quite simple.

I have observed (with many data) that given 2 series X and Y
cor(X,Y) and cor(log(X), log(Y)) differs only very slightly.
I suspect this is because log(x) is monotonous,
... but I am unable to demonstrate it.
(perhaps it is also wrong ?).

Would somebody here be able and kind enough to put me on
the rigth track (a piece of clue or a link would be nice).

Thanks
Vincent



From MSchwartz at mn.rr.com  Fri Sep 30 14:32:11 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 30 Sep 2005 07:32:11 -0500
Subject: [R] List Email Statistic
In-Reply-To: <20050930044734.77405.qmail@web34203.mail.mud.yahoo.com>
References: <20050930044734.77405.qmail@web34203.mail.mud.yahoo.com>
Message-ID: <1128083531.7208.14.camel@localhost.localdomain>

On Thu, 2005-09-29 at 23:47 -0500, JosÃ© Raul Capablanca wrote:
> Dear All, 
> How can I can to know a mail list , to speak about
> exclusively statistic. Thanks. 

If you have access to Usenet either via an NNTP server or via Google
Groups, there are three principal groups for general statistics
discussion:

  sci.stat.consult (http://groups.google.com/group/sci.stat.consult)
  sci.stat.math    (http://groups.google.com/group/sci.stat.math)

  sci.stat.edu     (http://groups.google.com/group/sci.stat.edu)


There is a greater level of volume on the first two for general
discussion. sci.stat.edu (which is targeted more to statistics education
discussion) has been split off as a gateway to the edstat-L list, which
has substantively reduced its daily volume.

HTH,

Marc Schwartz



From lmiceli at telos.org.br  Fri Sep 30 14:35:48 2005
From: lmiceli at telos.org.br (Leonardo L Miceli)
Date: Fri, 30 Sep 2005 09:35:48 -0300
Subject: [R] Clear Console
Message-ID: <OFAD7268D7.3C3169BD-ON0325708C.00450B86-0325708C.00458C4A@telos.org.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050930/49f91571/attachment.pl

From roger.bos at gmail.com  Fri Sep 30 14:48:08 2005
From: roger.bos at gmail.com (roger bos)
Date: Fri, 30 Sep 2005 08:48:08 -0400
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <433C2493.12372.218257A@localhost>
References: <20050216172043.2153.qmail@web25801.mail.ukl.yahoo.com>
	<21c05c7d050929041244c9f17b@mail.gmail.com>
	<433C2493.12372.218257A@localhost>
Message-ID: <1db726800509300548w1d6fe217p34f1e6df5e22cdba@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050930/6b3766b2/attachment.pl

From Matthias.Templ at statistik.gv.at  Fri Sep 30 15:02:09 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 30 Sep 2005 15:02:09 +0200
Subject: [R] correlation question
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAC35@xchg1.statistik.local>

First: The question is really not related to R.

What you can find very easly in books or in the internet:
The correlation coefficient is invariant under a linear transformation
(and the proof) 
#E.g. 
library(rrcov)
data(brain)
cor(brain)
cor(scale(brain))  #Is the same, BUT
cor(log(brain))    #Is VERY different.

Best,
Matthias



> Dear R-helpers,
> 
> First, double apologies because the question is not
> directly related to R, and also probably quite simple.
> 
> I have observed (with many data) that given 2 series X and Y
> cor(X,Y) and cor(log(X), log(Y)) differs only very slightly.
> I suspect this is because log(x) is monotonous,
> ... but I am unable to demonstrate it.
> (perhaps it is also wrong ?).
> 
> Would somebody here be able and kind enough to put me on
> the rigth track (a piece of clue or a link would be nice).
> 
> Thanks
> Vincent
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From HDoran at air.org  Fri Sep 30 15:27:19 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 30 Sep 2005 09:27:19 -0400
Subject: [R] Clear Console
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740A31C8C1@dc1ex2.air.org>

Ctrl L 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leonardo L Miceli
Sent: Friday, September 30, 2005 8:36 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Clear Console

Hi,

What is the function to clear the console?

tks
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sw283 at maths.bath.ac.uk  Fri Sep 30 15:34:13 2005
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Fri, 30 Sep 2005 14:34:13 +0100 (BST)
Subject: [R] Estimate predictor contribution in GAM models
In-Reply-To: <1127231710.7013.122.camel@new-york.climpact.net>
References: <1127231710.7013.122.camel@new-york.climpact.net>
Message-ID: <Pine.LNX.4.56.0509301428520.11262@mars>

On Tue, 20 Sep 2005, Yves Magliulo wrote:

> hi,
>
> i'm using gam() function from package mgcv.
>
> if G is my gam object, then
> >SG=summary(G)
> Formula:
> y ~ +s(x0, k = 5) + s(x1) + s(x2, k = 3)
>
> Parametric coefficients:
>               Estimate  std. err.    t ratio    Pr(>|t|)
> (Intercept)  3.462e+07  1.965e+05      176.2    < 2.22e-16
>
> Approximate significance of smooth terms:
>                edf         chi.sq     p-value
>  s(x0)      2.858       70.629     1.3129e-07
>  s(x1)      8.922       390.39     2.6545e-13
>  s(x2)      1.571        141.6     1.8150e-11
>
> R-sq.(adj) =  0.955   Deviance explained =   97%
> GCV score = 2.4081e+12   Scale est. = 1.5441e+12  n = 40
> --------------------------------------
>
> =>
> But how can i estimate numericaly the contribution of each smooth
> against the others. In others words, is there a way to quantify this
> significance like a percentage of how the model is improved by each of
> my predictors?

- The easiest thing to do is probably to refit the model without each
predictor, and look at how much the r^2 drops. You might want to fix the
smoothing parameters when you do this: G$sp gives the original smoothing
parameter estimates for the model with all terms, so you can pick out the
appropriate smoothing parameters to send to `gam' via the `sp' argument,
for the 2 term fits.

best,
Simon



From sw283 at maths.bath.ac.uk  Fri Sep 30 15:40:42 2005
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Fri, 30 Sep 2005 14:40:42 +0100 (BST)
Subject: [R] Smooth terms significance in GAM models
In-Reply-To: <1127468010.4977.43.camel@new-york.climpact.net>
References: <1127468010.4977.43.camel@new-york.climpact.net>
Message-ID: <Pine.LNX.4.56.0509301434260.11262@mars>

> i'm using gam() function from package mgcv with default option (edf
> estimated by GCV).
>
> >G=gam(y ~ s(x0, k = 5) + s(x1) + s(x2, k = 3))
> >SG=summary(G)
> Formula:
> y ~ +s(x0, k = 5) + s(x1) + s(x2, k = 3)
>
> Parametric coefficients:
>               Estimate  std. err.    t ratio    Pr(>|t|)
> (Intercept)  3.462e+07  1.965e+05      176.2    < 2.22e-16
>
> Approximate significance of smooth terms:
>                edf         chi.sq     p-value
>  s(x0)      2.858       70.629     1.3129e-07
>  s(x1)      8.922       390.39     2.6545e-13
>  s(x2)      1.571        141.6     1.8150e-11
>
> R-sq.(adj) =  0.955   Deviance explained =   97%
> GCV score = 2.4081e+12   Scale est. = 1.5441e+12  n = 40
> --------------------------------------
>
> I know i can estimate the significance of smooth terms with chi.sq &
> p.value.
>
> With GCV, p-value are obtained by comparing the statistic to an F
> distribution,isn't it?
> help(summary.gam) says "use at your own risk!".Does it mean i should
> only estimated signifiance of smooth terms by chi.sq?.Is there a way to
> link both information (p.value and chi.sq)?
No, using F as the reference distribution is always more conservative:
using chi.sq will be even worse. The p values are *very approximate* since
they are based on pretending that a penalized fit is equivalent to an
unpenalized fit with the same effective degrees of freedom, and neglect
the uncertainty associated with smoothing parameter estimation... they
provide a reasonable `rough guide' to significance, but are by no means
exact.

> Last question, using GAM with default, should i look at R-sq rather than
> Deviance explain, or both?

In this case devaince explained is just the unadjusted r^2... I'd look at
the r^2, which is adjusted (to take into account the degrees of freedom
`used up' when estimating the model).

best,
Simon



From opa04 at yahoo.com  Fri Sep 30 16:12:38 2005
From: opa04 at yahoo.com (DAVID CAMACHO)
Date: Fri, 30 Sep 2005 07:12:38 -0700 (PDT)
Subject: [R] Descriptive statistics for tables
In-Reply-To: <200509221627.j8MGRiWk022864@meitner.gene.com>
Message-ID: <20050930141238.71763.qmail@web30512.mail.mud.yahoo.com>

Thanks for your response,

What I need is extremely simple. And I suppose there
are so many way to do it.
But as I have so many files to do it, I am looking for
the simplest way (if possible)

What I meant was that I have tables with the same
numbers of rows and columns, (square form, should I
say?) like:

2,5,7
3,4,8
2,9,3

5,3,8
2,5,5
5,7,8

But I have hundreds of this onces, (bigger once).
Every one of this tables have the same size (columns,
rows). And I want to obtain the sd, z-score, and
p-value for the position [1,1], [1,2]....
[2,1],[2,2]... etc etc. That is, to obtain a table
with some simple descriptive statistics about all this
tables. 
I have try different methods, but I am no familiar
with R. (by the way, I could not find the way to do
loops with R)
Any suggestion is welcome.
David





--- Berton Gunter <gunter.berton at gene.com> wrote:

> I don't know what a "quadratic, same size" table is
> or what you mean. If you
> do not get a satisfactory reply I suggest:
> 
> 1. Read and follow the posting guide at the end of
> this message.
> 
> 2. In particular, provide a simple, reproducible
> example to show what you
> want to do and perhaps any error messages that you
> may have received.
> 
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On
> Behalf Of DAVID CAMACHO
> > Sent: Thursday, September 22, 2005 9:10 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Descriptive statistics for tables
> > 
> > 
> > I have a lot (more than one hundred) of files with
> tables of 
> > the same kind (quadratic, same size) and I want to
> obtain 
> > some statistics for every position on them.
> Therefore, as a 
> > result I want another table. I import every table,
> and create 
> > an object read.table for it, then I have try to
> create a 
> > list or a data frame and directly utilize some
> functions 
> > like sd( ) without success, because it calculate
> sd for the 
> > columns . Which kind of object should I create
> in order to 
> > utilize directly this kind of functions?  Should I
> program it 
> > throw for loops?
> > 
> > David
> > 
>



From mlyman at byu.edu  Fri Sep 30 17:13:23 2005
From: mlyman at byu.edu (Mark Lyman)
Date: Fri, 30 Sep 2005 09:13:23 -0600
Subject: [R] Bug in lmer?
In-Reply-To: <17212.59265.851874.843569@stat.math.ethz.ch>
References: <433C5236.4060104@byu.edu>
	<17212.59265.851874.843569@stat.math.ethz.ch>
Message-ID: <433D5613.1040207@byu.edu>

I am using version 0.98-7 of the Matrix package. I used the RGui 
"Install Packages..." menu option to get the lme4 package from CRAN and 
this version of the Matrix was automatically downloaded as well.

Martin Maechler wrote:

>>>>>>"Mark" == Mark Lyman <mlyman at byu.edu>
>>>>>>    on Thu, 29 Sep 2005 14:44:38 -0600 writes:
>>>>>>            
>>>>>>
>
>    Mark> I am relatively new to R so I am not confident enough in what I am doing 
>    Mark> to be certain this is a bug. 
>
>
>    Mark> I am running R 2.1.1 on a Windows XP 
>    Mark> machine and the lme4 package version 0.98-1. 
>
>lme4 nowadays is heavily based on "Matrix" which version are you
>using there?
>
>    Mark> The following code fits the model I want using the
>    Mark> nlme package version 3.1-60.
>
>      < .............. >  {see a script at the end}
>
>    Mark> The problem is that when I try fitting the model using
>    Mark> the lmer function with the following code:
>
>    Mark> lmer(adg~trt+(1|loc)+(1|block:loc)+(1|loc:trt),mltloc)
>
>    Mark> I get this message from Windows and R closes.
>
>    Mark> >> R for Windows GUI front-end has encountered a problem and needs to 
>    Mark> >> close.  We are sorry for the inconvenience.
>
>That definitely means there is a bug.
>The question is *where* the bug is:  "lme4", "Matrix", "R", "Windows".
>
>One first thing coming to mind is a mismatch of "lme4" and "Matri
>
>    Mark> This same code works on a Macintosh. So it doesn't
>    Mark> seem that I have made an error in my code. 
>
>correct; I can also fit the model nice and quickly on Linux,
>and summary() confirms the same fit {with the "usual problem" of
>different estimates for the degrees of freedoms 'df'}.
>
>So currently the bug only shows on the Windows platform. 
>Could it be that you have a mismatching package "Matrix" version
>there, but not on the Mac?
>
>    Mark> Also if anyone of the random effect terms is removed there is
>    Mark> no problem. Is this something that is being looked at?
>
>not yet, AFAIK.
>
>    Mark> Or I have I made a mistake somewhere? I have included
>    Mark> the data that I am using below.
>
>I'm putting the data and an R script up for FTP,
>so that you or others can run this ``from anywhere'' via
>
> source("ftp://stat.ethz.ch/U/maechler/R/mltloc-ex.R", echo = TRUE)
>
>Maybe this helps diagnosis,
>Martin Maechler
>
>
>  
>



From jeaneid at chass.utoronto.ca  Fri Sep 30 17:18:09 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 30 Sep 2005 11:18:09 -0400
Subject: [R] Descriptive statistics for tables
In-Reply-To: <20050930141238.71763.qmail@web30512.mail.mud.yahoo.com>
Message-ID: <Pine.SGI.4.40.0509301058220.336640-100000@origin.chass.utoronto.ca>

I do not totally understand your question as well. You seem to want a
descriptive statistic about a unitary number. What is the sd of a number?
or any other descriptive statictic. Maybe you mean for the columns or rows
or it could be that these are t-stats or z-stats that you need to get
p_values for them. In any case what I would do assuming that these tables
are in your env is the following

sapply(ls(), function(x){
  nam <- paste(x,"_summary", sep="")
  tt <- colMeans(get(x))
  assign(nam, tt, pos=1)})

The function above will output a number of tables with their original name
an extension _summary which contains the column means of the tables.

If you have the tables in different files on your box you need to add a
read.table line above. and the function becomes

sapply(dir(), function(x){

  nam <- paste(x,"_summary", sep="")
  assign(x, read.table(x))
	tt <- colMeans(x)
  assign(nam, tt, pos=1)})


HTH


On Fri, 30 Sep 2005, DAVID CAMACHO wrote:

> Thanks for your response,
>
> What I need is extremely simple. And I suppose there
> are so many way to do it.
> But as I have so many files to do it, I am looking for
> the simplest way (if possible)
>
> What I meant was that I have tables with the same
> numbers of rows and columns, (square form, should I
> say?) like:
>
> 2,5,7
> 3,4,8
> 2,9,3
>
> 5,3,8
> 2,5,5
> 5,7,8
>
> But I have hundreds of this onces, (bigger once).
> Every one of this tables have the same size (columns,
> rows). And I want to obtain the sd, z-score, and
> p-value for the position [1,1], [1,2]....
> [2,1],[2,2]... etc etc. That is, to obtain a table
> with some simple descriptive statistics about all this
> tables.
> I have try different methods, but I am no familiar
> with R. (by the way, I could not find the way to do
> loops with R)
> Any suggestion is welcome.
> David
>
>
>
>
>
> --- Berton Gunter <gunter.berton at gene.com> wrote:
>
> > I don't know what a "quadratic, same size" table is
> > or what you mean. If you
> > do not get a satisfactory reply I suggest:
> >
> > 1. Read and follow the posting guide at the end of
> > this message.
> >
> > 2. In particular, provide a simple, reproducible
> > example to show what you
> > want to do and perhaps any error messages that you
> > may have received.
> >
> >
> > -- Bert Gunter
> > Genentech Non-Clinical Statistics
> > South San Francisco, CA
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On
> > Behalf Of DAVID CAMACHO
> > > Sent: Thursday, September 22, 2005 9:10 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] Descriptive statistics for tables
> > >
> > >
> > > I have a lot (more than one hundred) of files with
> > tables of
> > > the same kind (quadratic, same size) and I want to
> > obtain
> > > some statistics for every position on them.
> > Therefore, as a
> > > result I want another table. I import every table,
> > and create
> > > an object read.table for it, then I have try to
> > create a
> > > list or a data frame and directly utilize some
> > functions
> > > like sd( ) without success, because it calculate
> > sd for the
> > > columns . Which kind of object should I create
> > in order to
> > > utilize directly this kind of functions?  Should I
> > program it
> > > throw for loops?
> > >
> > > David
> > >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From vincent at 7d4.com  Fri Sep 30 17:25:50 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Fri, 30 Sep 2005 17:25:50 +0200
Subject: [R] correlation question
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BAC35@xchg1.statistik.local>
References: <83536658864BC243BE3C06D7E936ABD5027BAC35@xchg1.statistik.local>
Message-ID: <433D58FE.1040707@7d4.com>

TEMPL Matthias a ??crit :

> First: The question is really not related to R.
> What you can find very easly in books or in the internet:
> The correlation coefficient is invariant under a linear transformation
> (and the proof) 

yes.

> #E.g. 
> library(rrcov)
> data(brain)
> cor(brain)
> cor(scale(brain))  #Is the same,

ok

> cor(log(brain))    # BUT Is VERY different.

Ah, thank you very much for this example.
So it seems cor(X,Y) ~ cor(log(X), log(Y)) is in fact
only a particular feature of my data (prices series)
(perhaps/probably because they are quite continuous ?).

Nothing general. I was completely off the track. Sorry.

> Best,
> Matthias

Thanks again for this counter-example and the enlightenment.
Vincent



From tsnall at mappi.helsinki.fi  Fri Sep 30 17:29:59 2005
From: tsnall at mappi.helsinki.fi (tsnall@mappi.helsinki.fi)
Date: Fri, 30 Sep 2005 18:29:59 +0300
Subject: [R] p-value for non-linear variable in overdispersed glm()
Message-ID: <1128094199.433d59f77566e@www2.helsinki.fi>

Dear all, 
I am fitting an nonlinear glm() using optim() by first minimising 
glm(resp~ var1 + var2, family=binomial, data=data)$deviance
where var1= exp(-a1*dist1), and var2= exp(-a2*dist2), where a1 and a2 are 
parameters and dist1 and dist2 are independent variables.

Next, I calculate the value of var1 (and var2) by plugging in the value of 
al1 (and al2) that minimises deviance,
and fit glm(resp~ var1+var2 , family=binomial, data=data)
Var1 in this model thus includes two parameters - the standard glm() 
coefficient and a1. This is (of course) not recognized by drop1().

Usually I extract a rough p-value for var1 in this model by 
1-pchisq(scaled deviance,df=2)
This gives the p-value reported by drop1():
1-pchisq(scaled deviance,df=1)

However, the model that I currently work on is overdispersed, and I have 
used family=quasibinomial. According to ?anova.glm the F-value should be 
used in likelihood-ratio tests of models fitted by quasibinomial. 
Again I want to extract a rough p-value and try the corresponding (I 
thought) for the overdispersed model:
1-pf(5.1,df1=2, df2=250)
[1] 0.006746671
which is lower than
1-pf(5.1,df1=1, df2=250)
[1] 0.02478842
which I didn't expect.

Could someone please give a clue on how to get the rough p-value for var1 
in my overdispersed model?


Thanks!

Cheers,
Tord



From ramasamy at cancer.org.uk  Fri Sep 30 18:43:41 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 30 Sep 2005 17:43:41 +0100
Subject: [R] Searching for specific functions
In-Reply-To: <3145.150.128.150.28.1128069991.squirrel@www.ffbg.net>
References: <3145.150.128.150.28.1128069991.squirrel@www.ffbg.net>
Message-ID: <1128098621.5885.72.camel@dhcp-123.wolf.ox.ac.uk>

1) I am not sure why you want the biased version but here are two ways
to do this. Also if you have large enough sample size, it would not
matter much.

	my.var1 <- function(x) mean( (x - mean(x))^2 )
	my.var2 <- function(x) mean( x^2 - mean(x)^2 )

2) Look at quantiles function. Your equation might correspond to one of
the types given there.

Regards, Adai



On Fri, 2005-09-30 at 10:46 +0200, Fabrizio Ferri Benedetti wrote:
>   Hello everybody (I'm new here)
> 
>   I am looking for 2 specific functions. The first one is a variance
> formula that does not divide by n-1 (I don't want the unbiased formula,
> that's it). In case it does not exist, how do I modify the default
> function code? Yes, a workaround would be to multiply the result by n-1
> and then divide it by n, but I'd like to use a separate function, if
> possible.
> 
>   The other function I'm looking for it's a median formula like this one:
> 
> http://www.faberitius.com/prototipo/gfx/clip_image002_0000.gif
> 
>   Where "Li" is the exact lower limit of the critical interval, "nb" is
> the number of cases that lie below the critical interval, "nd" the number
> of cases inside the mentioned interval, and "I" is the range of the
> interval itself. As you can see this formula calculates the median in
> categorical structured data.
> 
>   I couldn't find any functions that match the results done by hand... is
> there a contrib package out there that may help me? Should I program the
> function on my own?
> 
>   Thanks in advance,
> 
>            Fabrizio Ferri Benedetti
> 
> 
>



From mrabaa at jhsph.edu  Fri Sep 30 18:18:43 2005
From: mrabaa at jhsph.edu (Rabaa, Maia)
Date: Fri, 30 Sep 2005 12:18:43 -0400
Subject: [R] ast package?
Message-ID: <E619BDBD99B4F74D9DCA32F43BE9267102CAF174@XCH-VN02.sph.ad.jhsph.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050930/03c2d0be/attachment.pl

From vkwyau at gmail.com  Fri Sep 30 19:15:42 2005
From: vkwyau at gmail.com (Vincent Yau)
Date: Fri, 30 Sep 2005 10:15:42 -0700
Subject: [R] Compiling R on OpenSolaris
Message-ID: <f20482fc0509301015i5aca9730y1732c841f26ee3ab@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050930/a311bfe5/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Sep 30 19:23:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Sep 2005 19:23:07 +0200
Subject: [R] ast package?
In-Reply-To: <E619BDBD99B4F74D9DCA32F43BE9267102CAF174@XCH-VN02.sph.ad.jhsph.edu>
References: <E619BDBD99B4F74D9DCA32F43BE9267102CAF174@XCH-VN02.sph.ad.jhsph.edu>
Message-ID: <x27jcyjwqs.fsf@turmalin.kubism.ku.dk>

"Rabaa, Maia" <mrabaa at jhsph.edu> writes:

> According to the extremely helpful reference card for time series analysis provided at: http://cran.r-project.org/doc/contrib/Ricci-refcard-ts.pdf, the ast package is necessary to perform some of the functions.
> Where can this package be installed from, as I cannot find it from my pulldown list, nor can I find it on the web?
> Thanks.

It's not *that* hard to find, although it helps if you know a few
words of Italian...

http://sirio.stat.unipd.it/index.php?id=libast

I don't know why Guido doesn't want to put this on CRAN.
  
> Maia

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From droberts at montana.edu  Fri Sep 30 19:33:52 2005
From: droberts at montana.edu (Dave Roberts)
Date: Fri, 30 Sep 2005 11:33:52 -0600
Subject: [R] Descriptive statistics for tables
In-Reply-To: <Pine.SGI.4.40.0509301058220.336640-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0509301058220.336640-100000@origin.chass.utoronto.ca>
Message-ID: <433D7700.4000208@montana.edu>

If I understand the request, he wants to take a large number of matrices 
of identical size and stack them into a three dimentional array, and 
then calculate statistics on the the third dimension.   If the multiple 
arrays have object names they can be combined into a 3-d array

 > a <- matrix(rep(1,9),ncol=3)
 > b <- matrix(rep(2,9),ncol=3)
 > c <- matrix(rep(3,9),ncol=3)
 > z <- array(c(a,b,c),dim=c(3,3,3))

 > z
, , 1

      [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    1    1
[3,]    1    1    1

, , 2

      [,1] [,2] [,3]
[1,]    2    2    2
[2,]    2    2    2
[3,]    2    2    2

, , 3

      [,1] [,2] [,3]
[1,]    3    3    3
[2,]    3    3    3
[3,]    3    3    3


and then specific dimensions can be summarized

 > mean(z[1,1,])
[1] 2

 > sd(z[1,1,])
[1] 1

I don't know of any easy way to combine all the 2-d matrices other than 
listing them by name in a c() function in the array statement.  If they 
were cleverly names perhaps a for loop could be used.

Dave



Jean Eid wrote:
> I do not totally understand your question as well. You seem to want a
> descriptive statistic about a unitary number. What is the sd of a number?
> or any other descriptive statictic. Maybe you mean for the columns or rows
> or it could be that these are t-stats or z-stats that you need to get
> p_values for them. In any case what I would do assuming that these tables
> are in your env is the following
> 
> sapply(ls(), function(x){
>   nam <- paste(x,"_summary", sep="")
>   tt <- colMeans(get(x))
>   assign(nam, tt, pos=1)})
> 
> The function above will output a number of tables with their original name
> an extension _summary which contains the column means of the tables.
> 
> If you have the tables in different files on your box you need to add a
> read.table line above. and the function becomes
> 
> sapply(dir(), function(x){
> 
>   nam <- paste(x,"_summary", sep="")
>   assign(x, read.table(x))
> 	tt <- colMeans(x)
>   assign(nam, tt, pos=1)})
> 
> 
> HTH
> 
> 
> On Fri, 30 Sep 2005, DAVID CAMACHO wrote:
> 
> 
>>Thanks for your response,
>>
>>What I need is extremely simple. And I suppose there
>>are so many way to do it.
>>But as I have so many files to do it, I am looking for
>>the simplest way (if possible)
>>
>>What I meant was that I have tables with the same
>>numbers of rows and columns, (square form, should I
>>say?) like:
>>
>>2,5,7
>>3,4,8
>>2,9,3
>>
>>5,3,8
>>2,5,5
>>5,7,8
>>
>>But I have hundreds of this onces, (bigger once).
>>Every one of this tables have the same size (columns,
>>rows). And I want to obtain the sd, z-score, and
>>p-value for the position [1,1], [1,2]....
>>[2,1],[2,2]... etc etc. That is, to obtain a table
>>with some simple descriptive statistics about all this
>>tables.
>>I have try different methods, but I am no familiar
>>with R. (by the way, I could not find the way to do
>>loops with R)
>>Any suggestion is welcome.
>>David
>>
>>
>>
>>
>>
>>--- Berton Gunter <gunter.berton at gene.com> wrote:
>>
>>
>>>I don't know what a "quadratic, same size" table is
>>>or what you mean. If you
>>>do not get a satisfactory reply I suggest:
>>>
>>>1. Read and follow the posting guide at the end of
>>>this message.
>>>
>>>2. In particular, provide a simple, reproducible
>>>example to show what you
>>>want to do and perhaps any error messages that you
>>>may have received.
>>>
>>>
>>>-- Bert Gunter
>>>Genentech Non-Clinical Statistics
>>>South San Francisco, CA
>>>
>>>
>>>
>>>>-----Original Message-----
>>>>From: r-help-bounces at stat.math.ethz.ch
>>>>[mailto:r-help-bounces at stat.math.ethz.ch] On
>>>
>>>Behalf Of DAVID CAMACHO
>>>
>>>>Sent: Thursday, September 22, 2005 9:10 AM
>>>>To: r-help at stat.math.ethz.ch
>>>>Subject: [R] Descriptive statistics for tables
>>>>
>>>>
>>>>I have a lot (more than one hundred) of files with
>>>
>>>tables of
>>>
>>>>the same kind (quadratic, same size) and I want to
>>>
>>>obtain
>>>
>>>>some statistics for every position on them.
>>>
>>>Therefore, as a
>>>
>>>>result I want another table. I import every table,
>>>
>>>and create
>>>
>>>>an object read.table for it, then I have try to
>>>
>>>create a
>>>
>>>>list or a data frame and directly utilize some
>>>
>>>functions
>>>
>>>>like sd( ) without success, because it calculate
>>>
>>>sd for the
>>>
>>>>columns . Which kind of object should I create
>>>
>>>in order to
>>>
>>>>utilize directly this kind of functions?  Should I
>>>
>>>program it
>>>
>>>>throw for loops?
>>>>
>>>>David
>>>>
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From sledepi at operamail.com  Fri Sep 30 19:53:50 2005
From: sledepi at operamail.com (sloan jones)
Date: Fri, 30 Sep 2005 18:53:50 +0100
Subject: [R] ACCESS R and dates
Message-ID: <20050930175351.2181F203FE@ws5-1.us4.outblaze.com>

I have used the RODBC package to read in data I have stored in an Access file. When I am using data from files other than ACCESS I have no problem using the survival package to work with dates; however, with the ACCESS data the dates are reading-in in the following format: "2004-02-11 Pacific Standard Time". How do I remove the "Pacific Standard Time" part in ACCESS or R so that I can coerce the data into a workable format?  Any suggestions?

Sloan

-- 
_______________________________________________
Surf the Web in a faster, safer and easier way:
Download Opera 8 at http://www.opera.com



From Mike.Prager at noaa.gov  Fri Sep 30 19:56:10 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Fri, 30 Sep 2005 13:56:10 -0400
Subject: [R] Dots in function names
Message-ID: <433D7C3A.1030909@noaa.gov>

Recent R function names seem to be using CaseOfTheLetters to mark words 
rather than dots as was done previously.  Is the use of dots in function 
names deprecated, or is that simply a style choice?  Will function names 
with dots cause problems in future revisions?

Mike Prager



From tlumley at u.washington.edu  Fri Sep 30 20:06:38 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 30 Sep 2005 11:06:38 -0700 (PDT)
Subject: [R] Dots in function names
In-Reply-To: <433D7C3A.1030909@noaa.gov>
References: <433D7C3A.1030909@noaa.gov>
Message-ID: <Pine.LNX.4.63a.0509301100330.29284@homer23.u.washington.edu>

On Fri, 30 Sep 2005, Mike Prager wrote:

> Recent R function names seem to be using CaseOfTheLetters to mark words
> rather than dots as was done previously.  Is the use of dots in function
> names deprecated, or is that simply a style choice?  Will function names
> with dots cause problems in future revisions?
>

There are, and always have been, problems with using dots because of the 
way S3 methods work.  These are getting worse as more functions become 
generic but are getting better as more methods are in namespaces and thus 
registered.

I don't expect dots to go away any time soon, so the choice between 
name.with.dots(), name_with_underscores(), runonname(), and 
nameCamelCased() is really a stylistic one.

 	-thomas



From jwong at ices.on.ca  Fri Sep 30 20:29:45 2005
From: jwong at ices.on.ca (Wong, Jackson)
Date: Fri, 30 Sep 2005 14:29:45 -0400
Subject: [R] gbm package on Sun sparc
Message-ID: <2287E56F3AA6544AA96FB73BD896C07965D5@ices10.ices.on.ca>

Hi,

Does anyone know how to compile the gbm-1.5-1 package on a Sun sparc platform running Solaris 9?
I have been using gcc-3.4.2 and getting an NAN undeclared error.
Any help would be great.

thanks,

Jackson

___________________________________________________________________________ 
This email may contain confidential and/or privileged information for the 
sole use of the intended recipient. Any review or distribution by others is 
strictly prohibited. If you have received this email in error, please 
contact the sender and delete all copies. Opinions, conclusions or other 
information expressed or contained in this email are not given or endorsed 
by the sender unless otherwise affirmed independently by the sender.



From lizzylaws at yahoo.com  Fri Sep 30 20:45:53 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Fri, 30 Sep 2005 11:45:53 -0700 (PDT)
Subject: [R] .C help
Message-ID: <20050930184553.58902.qmail@web32106.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050930/c42488bd/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Sep 30 20:55:20 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Sep 2005 20:55:20 +0200
Subject: [R] Dots in function names
In-Reply-To: <433D7C3A.1030909@noaa.gov>
References: <433D7C3A.1030909@noaa.gov>
Message-ID: <x2y85eidwn.fsf@turmalin.kubism.ku.dk>

"Mike Prager" <Mike.Prager at noaa.gov> writes:

> Recent R function names seem to be using CaseOfTheLetters to mark words 
> rather than dots as was done previously.  Is the use of dots in function 
> names deprecated, or is that simply a style choice?  Will function names 
> with dots cause problems in future revisions?

Well, come the S4 revolution and dots will cause trouble no more...

The main reason dots have fallen from grace is that they cause
ambiguity in relation to S3 methods. In a nutshell: t.test is not a
transpose method for objects of class "test". Since we check S3
methods automatically,  it is problematic to keep track of things that
look like S3 methods without being so. Check out
.make_S3_methods_stop_list() (in the tools package).

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From lizzylaws at yahoo.com  Fri Sep 30 20:57:28 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Fri, 30 Sep 2005 11:57:28 -0700 (PDT)
Subject: [R] .C help
Message-ID: <20050930185728.61466.qmail@web32106.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050930/3e8a2a35/attachment.pl

From hodgess at gator.dt.uh.edu  Fri Sep 30 21:09:14 2005
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Fri, 30 Sep 2005 14:09:14 -0500
Subject: [R]  C program
Message-ID: <200509301909.j8UJ9Eum012622@gator.dt.uh.edu>

Dear R People:

I have R Version 2.1.1. for Windows in binary form.

I would like to look at the C code for massdist. It is part
of the density function.

How would I access this, please?

Thank you very much!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From Roger.Bivand at nhh.no  Fri Sep 30 21:21:58 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 30 Sep 2005 21:21:58 +0200 (CEST)
Subject: [R] C program
In-Reply-To: <200509301909.j8UJ9Eum012622@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.44.0509302121370.17069-100000@reclus.nhh.no>

On Fri, 30 Sep 2005, Erin Hodgess wrote:

> Dear R People:
> 
> I have R Version 2.1.1. for Windows in binary form.
> 
> I would like to look at the C code for massdist. It is part
> of the density function.
> 
> How would I access this, please?

https://svn.r-project.org/R/trunk/src/appl/massdist.c

is the online way.

> 
> Thank you very much!
> 
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From tlumley at u.washington.edu  Fri Sep 30 21:24:15 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 30 Sep 2005 12:24:15 -0700 (PDT)
Subject: [R] C program
In-Reply-To: <200509301909.j8UJ9Eum012622@gator.dt.uh.edu>
References: <200509301909.j8UJ9Eum012622@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.63a.0509301221510.29284@homer23.u.washington.edu>

On Fri, 30 Sep 2005, Erin Hodgess wrote:

> Dear R People:
>
> I have R Version 2.1.1. for Windows in binary form.
>
> I would like to look at the C code for massdist. It is part
> of the density function.
>

The .C() call shows that massdist is in the base package.  This means that 
it is likely to be in  the src/appl or src/nmath directories of the R 
source code. Searching these (using grep on Linux) shows that it is in 
src/appl/massdist.c

 	-thomas



From Mike.Prager at noaa.gov  Fri Sep 30 21:29:46 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Fri, 30 Sep 2005 15:29:46 -0400
Subject: [R] Dots in function names
In-Reply-To: <x2y85eidwn.fsf@turmalin.kubism.ku.dk>
References: <433D7C3A.1030909@noaa.gov> <x2y85eidwn.fsf@turmalin.kubism.ku.dk>
Message-ID: <433D922A.4010304@noaa.gov>

on 9/30/2005 2:55 PM Peter Dalgaard said the following:

>"Mike Prager" <Mike.Prager at noaa.gov> writes:
>  
>
>>Recent R function names seem to be using CaseOfTheLetters to mark words 
>>rather than dots as was done previously.  Is the use of dots in function 
>>names deprecated, or is that simply a style choice?  Will function names 
>>with dots cause problems in future revisions?
>>    
>>
>
>Well, come the S4 revolution and dots will cause trouble no more...
>
>The main reason dots have fallen from grace is that they cause
>ambiguity in relation to S3 methods. In a nutshell: t.test is not a
>transpose method for objects of class "test". Since we check S3
>methods automatically,  it is problematic to keep track of things that
>look like S3 methods without being so. Check out
>.make_S3_methods_stop_list() (in the tools package).
>
My thanks to Thomas Lumley and Peter Dalgaard for their replies.  Being 
skeptical about software revolutions, I infer that dots will be of some 
concern for a long time.

At the same time, I expect that having names with underscores would 
limit compatibility with S-PLUS. 

As a user, I have not encountered problems with dots, and I don't 
understand whether the concern is likely to apply at the user level.  (I 
was unable to locate the function that P.D. mentioned above.)  Sorry if 
this seems dense, but I am not particularly conversant with the 
internals of R nor experienced in object-oriented programming.

So while I understand Peter's example showing that dots can be 
ambiguous, I am still at a loss as to whether that is of real practical 
concern at a user level; for example, in writing functions that will 
have limited distribution, or for functions that eventually may be 
incorporated into an R package.  I am guessing not, but will appreciate 
more comments if I have that wrong....

Mike Prager



From Roger.Bivand at nhh.no  Fri Sep 30 21:33:56 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 30 Sep 2005 21:33:56 +0200 (CEST)
Subject: [R] .C help
In-Reply-To: <20050930185728.61466.qmail@web32106.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.44.0509302122050.17069-100000@reclus.nhh.no>

On Fri, 30 Sep 2005, Elizabeth Lawson wrote:

> Hi,
>  I am hoping some one can help me.  I am learning to use C and would
> like to learn how to call c code in R.  I have look at Writing R
> Extensions
> 
>  
> 
>  
> 
> and I tried to copy the example on page 38 
> 
>  
> 
> void convolve(double *a, int *na, double *b, int *nb, double *ab)
> 
> {
> 
> int i, j, nab = *na + *nb - 1;
> 
> for(i = 0; i < nab; i++)
> 
> ab[i] = 0.0;
> 
> for(i = 0; i < *na; i++)
> 
> for(j = 0; j < *nb; j++)
> 
> ab[i + j] += a[i] * b[j];
> 
> }
> 
> called from R by
> 
> conv <- function(a, b)
> 
> .C("convolve",
> 
> as.double(a),
> 
> as.integer(length(a)),
> 
> as.double(b),
> 
> as.integer(length(b)),
> 
> ab = double(length(a) + length(b) - 1))$ab
> 
>  
> 
>  
> 
> and I got the error "C" function name not in load table.
> 
>  
> 
> Do I need to compile the C code first?
> 
> Do I need a c copmiler at all?
> 

Yes, you need to compile the C code first, into a file in the correct 
format to dyn.load() into R. The format varies with operating system, and 
when the appropriate compilers and other software components are present, 
can be done convieniently by entering R CMD SHLIB convolve.c at the shell 
command prompt for your system. On Linux systems, the software needed is 
usually installed already, on Windows visit 

http://www.murdoch-sutherland.com/Rtools/

for advice on where to get and how to install the C compiler used to build 
R itself. 

Do look at documentation about C, having a compiler at hand to try out 
examples and exercises is very helpful.

>  
> 
> Any suggestions for a begginner?
> 
>  
> 
> Thanks,
> 
>  
> 
> Elizabeth Lawson
> 
> 
> 
> 		
> ---------------------------------
> 
>  Click here to donate to the Hurricane Katrina relief effort. 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From lisawang at uhnres.utoronto.ca  Fri Sep 30 23:22:46 2005
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Fri, 30 Sep 2005 16:22:46 -0500
Subject: [R] how to impose the lines (Apply?)
Message-ID: <433DACA6.FF2A31D2@uhnres.utoronto.ca>

Hello,

I have data like the following (38 patients) each have a lot of record
(some have more than others). I would like to plot time* pressure for
each patient in the same plot. I try to write a function and then split
the data by patients id and then sapply the function to the splited
list, but I have difficulty to link the function to the data point.
Please let me know if this is the rignt way to do it.
Thanks a lot

patients    time   pressure

1             1      24
1             2      34
2             2      45
2             2.5    50
2             1.5    30
3             8      9

Lisa Wang
Toronto, Ca
tel: 416 946 4501

-------------- next part --------------
This e-mail may contain confidential and/or privileged information for the sole use of the intended recipient. Any review or distribution by anyone other than the person for whom it was originally intended is strictly prohibited. If you have received this e-mail in error, please contact the sender and delete all copies. Opinions, conclusions or other information contained in this e-mail may not be that of the organization.

From JAROSLAW.W.TUSZYNSKI at saic.com  Fri Sep 30 22:24:11 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 30 Sep 2005 16:24:11 -0400
Subject: [R] .C help
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD503DF8465@us-arlington-0668.mail.saic.com>

See http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/ and "R
Installation and Administration" manual for details that will have to be
followed very precisely.

My own 2 cents below should be taken as fuzzy recollections, compared to
detailed directions in 2 sources above. I do not know if they are the
easiest or even correct, but they work.

There are 2 ways to compile that I tried (both use MinGW compiler):
- compile C into dll. Copy dll to the same directory as R functions. 
    I was compiling on WIN XT machine by double clicking on "any-name.bat"
text file containing:
         cl /MT /Ox /D "WIN32"  /c runfunc.c
         link /dll /EXPORT:runquantile /EXPORT:runmean /EXPORT:runmad
/out:runfunc.dll *.obj 
         pause
    My code was in "runfunc.c". C Functions I was planning on calling were:
"runquantile" "runmean" "runmad". DLL name was "runfunc.dll"

- follow the steps as if you were building a package (even if you are not
planing on releasing it). This way was much easier, but only after I
installed many other tools needed for package instalation/compilation. After
everything was correctly set up the actual package compilation is done by
double-clicking on "C:\programs\R\rw2011\src\gnuwin32\install_caTools.bat"
text file containing:
        RCMD install C:/programs/R/rw2011/src/library/caTools
        pause
  
Both of those approaches (I think) required R beeing installed in a path
with no spaces (for example R can not be installed in default spot
"c:\Program Files\R" since "program files" have space between two words.
Also I remember having a lot of trouble ordering directories on my PATH
(envirinmental variable - see "my computer/properties/advanced/Environment
Variables").

 Jarek 
====================================================\==== 
 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">  \ 
 Jaroslaw.W.Tuszynski at saic.com                     `   \ 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Elizabeth Lawson
Sent: Friday, September 30, 2005 2:46 PM
To: r-help at stat.math.ethz.ch
Subject: [R] .C help

Hi,
 
I am hoping some one can help me.  I am learning to use C and would like to
learn how to call c code in R.  I have look at Writing R Extensions

 

 

and I tried to copy the example on page 38 

 

void convolve(double *a, int *na, double *b, int *nb, double *ab)

{

int i, j, nab = *na + *nb - 1;

for(i = 0; i < nab; i++)

ab[i] = 0.0;

for(i = 0; i < *na; i++)

for(j = 0; j < *nb; j++)

ab[i + j] += a[i] * b[j];

}

called from R by

conv <- function(a, b)

.C("convolve",

as.double(a),

as.integer(length(a)),

as.double(b),

as.integer(length(b)),

ab = double(length(a) + length(b) - 1))$ab

 

 

and I got the error "C" function name not in load table.

 

Do I need to compile the C code first?

Do I need a c copmiler at all?

 

Any suggestions for a begginner?

 

Thanks,

 

Elizabeth Lawson

I 


		
---------------------------------

 Click here to donate to the Hurricane Katrina relief effort. 
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Sep 30 22:35:13 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 30 Sep 2005 22:35:13 +0200
Subject: [R] how to impose the lines (Apply?)
In-Reply-To: <433DACA6.FF2A31D2@uhnres.utoronto.ca>
References: <433DACA6.FF2A31D2@uhnres.utoronto.ca>
Message-ID: <433DA181.1060703@statistik.uni-dortmund.de>

Lisa Wang wrote:

> Hello,
> 
> I have data like the following (38 patients) each have a lot of record
> (some have more than others). I would like to plot time* pressure for
> each patient in the same plot. I try to write a function and then split
> the data by patients id and then sapply the function to the splited
> list, but I have difficulty to link the function to the data point.
> Please let me know if this is the rignt way to do it.

It is completely unclear what you are really going to do.
Some code snippet and a more explicit question would help us to help.

Please read the posting guide (cited at the bottom of the mail) which 
gives advice how to formulate questions properly.

Best,
Uwe Ligges



> Thanks a lot
> 
> patients    time   pressure
> 
> 1             1      24
> 1             2      34
> 2             2      45
> 2             2.5    50
> 2             1.5    30
> 3             8      9
> 
> Lisa Wang
> Toronto, Ca
> tel: 416 946 4501
> 
> 
> 
> ------------------------------------------------------------------------
> 
> This e-mail may contain confidential and/or privileged information for the sole use of the intended recipient. Any review or distribution by anyone other than the person for whom it was originally intended is strictly prohibited. If you have received this e-mail in error, please contact the sender and delete all copies. Opinions, conclusions or other information contained in this e-mail may not be that of the organization.
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From lisawang at uhnres.utoronto.ca  Fri Sep 30 23:57:55 2005
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Fri, 30 Sep 2005 16:57:55 -0500
Subject: [R] 'mean' function
Message-ID: <433DB4E3.5E70FEB2@uhnres.utoronto.ca>

Hello,

Could you please let me know how to see the "mean" function in R  . The
following is what I see when type in "mean" and "enter"

mean
function (x, ...)
UseMethod("mean")
<environment: namespace:base>

I would like to see how the function is writen.

Thanks a lot

Lisa Wang
Toronto, Ca

-------------- next part --------------
This e-mail may contain confidential and/or privileged information for the sole use of the intended recipient. Any review or distribution by anyone other than the person for whom it was originally intended is strictly prohibited. If you have received this e-mail in error, please contact the sender and delete all copies. Opinions, conclusions or other information contained in this e-mail may not be that of the organization.

From murdoch at stats.uwo.ca  Fri Sep 30 22:58:26 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 30 Sep 2005 16:58:26 -0400
Subject: [R] 'mean' function
In-Reply-To: <433DB4E3.5E70FEB2@uhnres.utoronto.ca>
References: <433DB4E3.5E70FEB2@uhnres.utoronto.ca>
Message-ID: <433DA6F2.2000807@stats.uwo.ca>

On 9/30/2005 5:57 PM, Lisa Wang wrote:
> Hello,
> 
> Could you please let me know how to see the "mean" function in R  . The
> following is what I see when type in "mean" and "enter"
> 
> mean
> function (x, ...)
> UseMethod("mean")
> <environment: namespace:base>
> 
> I would like to see how the function is writen.

The "UseMethod" part tells you that mean is a generic function.  There 
are lots of different implementations, depending on what sort of thing x 
is.

You can see the default one using "mean.default":

 > mean.default
function (x, trim = 0, na.rm = FALSE, ...)
{
     if (!is.numeric(x) && !is.complex(x) && !is.logical(x)) {
         warning("argument is not numeric or logical: returning NA")
         return(as.numeric(NA))
     }
     if (na.rm)
         x <- x[!is.na(x)]
     trim <- trim[1]
     n <- length(x)
     if (trim > 0 && n > 0) {
         if (is.complex(x))
             stop("trimmed means are not defined for complex data")
         if (trim >= 0.5)
             return(median(x, na.rm = FALSE))
         lo <- floor(n * trim) + 1
         hi <- n + 1 - lo
         x <- sort(x, partial = unique(c(lo, hi)))[lo:hi]
         n <- hi - lo + 1
     }
     if (is.integer(x))
         sum(as.numeric(x))/n
     else sum(x)/n
}
<environment: namespace:base>


Duncan Murdoch



From Soren.Hojsgaard at agrsci.dk  Fri Sep 30 23:04:01 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Fri, 30 Sep 2005 23:04:01 +0200
Subject: [R] 'mean' function
References: <433DB4E3.5E70FEB2@uhnres.utoronto.ca>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC03877FF6@DJFPOST01.djf.agrsci.dk>

Try
base::mean.default
S??ren
 

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p?? vegne af Lisa Wang
Sendt: fr 30-09-2005 23:57
Til: R-Help
Emne: [R] 'mean' function



Hello,

Could you please let me know how to see the "mean" function in R  . The
following is what I see when type in "mean" and "enter"

mean
function (x, ...)
UseMethod("mean")
<environment: namespace:base>

I would like to see how the function is writen.

Thanks a lot

Lisa Wang
Toronto, Ca



From chrisb at fcdarwin.org.ec  Fri Sep 30 23:16:16 2005
From: chrisb at fcdarwin.org.ec (Chris Buddenhagen)
Date: Fri, 30 Sep 2005 15:16:16 -0600
Subject: [R] nonparametric 2way repeated-measures anova
Message-ID: <011001c5c604$31d4b6c0$4c01a8c0@Chris>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050930/3eee407b/attachment.pl

From nepossiver at yahoo.com  Fri Sep 30 23:44:01 2005
From: nepossiver at yahoo.com (Horacio Montenegro)
Date: Fri, 30 Sep 2005 14:44:01 -0700 (PDT)
Subject: [R] Bug in lmer?
In-Reply-To: <17212.59265.851874.843569@stat.math.ethz.ch>
Message-ID: <20050930214401.77008.qmail@web50602.mail.yahoo.com>


    Just to add more info:

    I've got the same error with both lme4 0.95-10 &
Matrix 0.95-13 and lme4 0.98-1 & Matrix 0.98-7, always
running on Win98SE, R 2.1.1. So it seems to occur with
any Windows version.

    cheers,
         Horacio

> 
> I'm putting the data and an R script up for FTP,
> so that you or others can run this ``from anywhere''
> via
> 
> 
>
source("ftp://stat.ethz.ch/U/maechler/R/mltloc-ex.R",
> echo = TRUE)
> 
> Maybe this helps diagnosis,
> Martin Maechler
> 
>



From tmlammail at yahoo.com  Fri Sep 30 23:58:41 2005
From: tmlammail at yahoo.com (Martin Lam)
Date: Fri, 30 Sep 2005 14:58:41 -0700 (PDT)
Subject: [R] 'mean' function
In-Reply-To: <433DB4E3.5E70FEB2@uhnres.utoronto.ca>
Message-ID: <20050930215841.17697.qmail@web40501.mail.yahoo.com>

Type "?mean" and hit "enter" in the console to see the
description of the function.

# randomly draw 5 numbers between 1 and 10 without
replacement
randomlist = sample(1:10, 5)
# calculate the mean of the list
meanoflist = mean(randomlist)
# show the result
meanoflist

HTH,

Martin

--- Lisa Wang <lisawang at uhnres.utoronto.ca> wrote:

> Hello,
> 
> Could you please let me know how to see the "mean"
> function in R  . The
> following is what I see when type in "mean" and
> "enter"
> 
> mean
> function (x, ...)
> UseMethod("mean")
> <environment: namespace:base>
> 
> I would like to see how the function is writen.
> 
> Thanks a lot
> 
> Lisa Wang
> Toronto, Ca
> 
> > This e-mail may contain confidential and/or
> privileged information for the sole use of the
> intended recipient. Any review or distribution by
> anyone other than the person for whom it was
> originally intended is strictly prohibited. If you
> have received this e-mail in error, please contact
> the sender and delete all copies. Opinions,
> conclusions or other information contained in this
> e-mail may not be that of the organization.>
______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



