From thinkersx4 at yahoo.com  Sat Dec  1 00:24:09 2007
From: thinkersx4 at yahoo.com (Robert Harris)
Date: Fri, 30 Nov 2007 15:24:09 -0800 (PST)
Subject: [R] The R Book - great resource for R beginners
In-Reply-To: <eb555e660711301234g6360bd0fg61a7ae7118b1c143@mail.gmail.com>
Message-ID: <595605.20179.qm@web50706.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071130/e1c92c04/attachment.pl 

From alfieim21 at hotmail.co.uk  Sat Dec  1 00:25:53 2007
From: alfieim21 at hotmail.co.uk (alfieim29)
Date: Fri, 30 Nov 2007 15:25:53 -0800 (PST)
Subject: [R]  Quantiles and QQ plots
Message-ID: <14097909.post@talk.nabble.com>


I have 20 variables:

5,9,6,1,5,9,7,4,5,6,3,2,4,8,9,6,1,8,4,8

How do I calculate the corresponding quantiles from a normal distribution
with the same mean and variance as the sample?

Also, how do I draw a QQ plot of the data?

Thanks for any help!
-- 
View this message in context: http://www.nabble.com/Quantiles-and-QQ-plots-tf4925742.html#a14097909
Sent from the R help mailing list archive at Nabble.com.


From jholtman at gmail.com  Sat Dec  1 01:08:46 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 30 Nov 2007 19:08:46 -0500
Subject: [R] about col in heatmap.2
In-Reply-To: <5032046e0711301409q6b0b10b6u34463c0cc3314c01@mail.gmail.com>
References: <5032046e0711301409q6b0b10b6u34463c0cc3314c01@mail.gmail.com>
Message-ID: <644e1f320711301608r284235abpd4a535deb0e54921@mail.gmail.com>

If you are just going to have 3 values for coloring, then you could
just convert your data into 1, 2, 3:

new.data <- ifelse(old.data > 2, 3, ifelse(old.data < 2, 1, 2))
dim(new.data) <- dim(old.data)

and then plot it.

On Nov 30, 2007 5:09 PM, affy snp <affysnp at gmail.com> wrote:
> Hi list,
> My data set is comprised of 47 columns and about 700 rows.
> Most of the values would be around 2, while some will go beyond
> in either direction, higher or lower. Is there a way to specify the
> parameter of col or others if necessary to have the range of representing
> colors be 0----8, and make dark for 2, red for higher than 2 and green
> for lower than 2. Is there a way to do this? Thanks a lot for your
> suggestions.
>
> All the best,
>      Allen
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From affysnp at gmail.com  Sat Dec  1 01:58:40 2007
From: affysnp at gmail.com (affy snp)
Date: Fri, 30 Nov 2007 19:58:40 -0500
Subject: [R] about col in heatmap.2
In-Reply-To: <644e1f320711301608r284235abpd4a535deb0e54921@mail.gmail.com>
References: <5032046e0711301409q6b0b10b6u34463c0cc3314c01@mail.gmail.com>
	<644e1f320711301608r284235abpd4a535deb0e54921@mail.gmail.com>
Message-ID: <5032046e0711301658w7589fedax64ded2da1d7e2fd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071130/30bbede2/attachment.pl 

From jholtman at gmail.com  Sat Dec  1 02:25:14 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 30 Nov 2007 20:25:14 -0500
Subject: [R] about col in heatmap.2
In-Reply-To: <5032046e0711301658w7589fedax64ded2da1d7e2fd@mail.gmail.com>
References: <5032046e0711301409q6b0b10b6u34463c0cc3314c01@mail.gmail.com>
	<644e1f320711301608r284235abpd4a535deb0e54921@mail.gmail.com>
	<5032046e0711301658w7589fedax64ded2da1d7e2fd@mail.gmail.com>
Message-ID: <644e1f320711301725q4e3f2a1bs3b00ce0bd873aac8@mail.gmail.com>

Try this.  This will plot a similar scale for data.

require(lattice)
# generate some data
x <- matrix(sample(0:8,100,replace=TRUE),10) # create 9 value
# create a color palette for outpu
Lab.palette <- colorRampPalette(c("red", 'yellow','green','blue'))
levelplot(x,col.regions=Lab.palette(10))


On Nov 30, 2007 7:58 PM, affy snp <affysnp at gmail.com> wrote:
> Hi Jim,
>
> Thanks but I imagine the transformed data would
> not actually represent the pattern from the original
> data. Is there other alternatives? Ideally, I would
> like to have the color bar looking like the one
> in http://www.bme.unc.edu/research/Bioinformatics.FunctionalGenomics.html
>
> Best,
>      Allen
>
>
>
> On Nov 30, 2007 7:08 PM, jim holtman <jholtman at gmail.com> wrote:
> > If you are just going to have 3 values for coloring, then you could
> > just convert your data into 1, 2, 3:
> >
> > new.data <- ifelse(old.data > 2, 3, ifelse(old.data < 2, 1, 2))
> > dim(new.data) <- dim(old.data )
> >
> > and then plot it.
> >
> >
> >
> >
> > On Nov 30, 2007 5:09 PM, affy snp <affysnp at gmail.com> wrote:
> > > Hi list,
> > > My data set is comprised of 47 columns and about 700 rows.
> > > Most of the values would be around 2, while some will go beyond
> > > in either direction, higher or lower. Is there a way to specify the
> > > parameter of col or others if necessary to have the range of
> representing
> > > colors be 0----8, and make dark for 2, red for higher than 2 and green
> > > for lower than 2. Is there a way to do this? Thanks a lot for your
> > > suggestions.
> > >
> > > All the best,
> > >      Allen
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 646 9390
> >
> > What is the problem you are trying to solve?
> >
>
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From juryef at yahoo.com  Sat Dec  1 02:27:19 2007
From: juryef at yahoo.com (Judith Flores)
Date: Fri, 30 Nov 2007 17:27:19 -0800 (PST)
Subject: [R] (no subject)
Message-ID: <987405.20653.qm@web34712.mail.mud.yahoo.com>

Hi,

   I have a data frame that looks something like this:

     id day    k
   656  -1  566
   663  -1  680
   673  -1  773
   675  -1  761
   704  -1  685
   714  -1  636
   717  -1  421
   719  -1  645
   727  -1  761
  731  -1  663
11  735  -1  603
12  738  -1  865
13  742  -1  594
14  744  -1  601
15  747  -1  816
16  749  -1  802
17  753  -1  811
18  761  -1  585
19  768  -1  644
20  771  -1  649
21  772  -1  679
22  788  -1  467
23  799  -1  572
24   81  -1  446
25  656   2  298
26  663   2  273
27  673   2  837
28  675   2  830
29  704   2  297
30  714   2  255
31  717   2  197
32  719   2  756
33  727   2  844
34  731   2  265
35  735   2  228
36  738   2  913
37  742   2  250
38  744   2  837
39  747   2  316
40  749   2  871
41  753   2  455
42  761   2  893
43  768   2  916
44  771   2  797
45  772   2  371
46  788   2  659
47  799   2  398



      ____________________________________________________________________________________
Never miss a thing.  Make Yahoo your home page.


From affysnp at gmail.com  Sat Dec  1 02:34:49 2007
From: affysnp at gmail.com (affy snp)
Date: Fri, 30 Nov 2007 20:34:49 -0500
Subject: [R] about col in heatmap.2
In-Reply-To: <644e1f320711301725q4e3f2a1bs3b00ce0bd873aac8@mail.gmail.com>
References: <5032046e0711301409q6b0b10b6u34463c0cc3314c01@mail.gmail.com>
	<644e1f320711301608r284235abpd4a535deb0e54921@mail.gmail.com>
	<5032046e0711301658w7589fedax64ded2da1d7e2fd@mail.gmail.com>
	<644e1f320711301725q4e3f2a1bs3b00ce0bd873aac8@mail.gmail.com>
Message-ID: <5032046e0711301734o2bdd53a5p3da1276bc68b28ba@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071130/ac38ff27/attachment.pl 

From jholtman at gmail.com  Sat Dec  1 02:41:38 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 30 Nov 2007 20:41:38 -0500
Subject: [R] about col in heatmap.2
In-Reply-To: <5032046e0711301734o2bdd53a5p3da1276bc68b28ba@mail.gmail.com>
References: <5032046e0711301409q6b0b10b6u34463c0cc3314c01@mail.gmail.com>
	<644e1f320711301608r284235abpd4a535deb0e54921@mail.gmail.com>
	<5032046e0711301658w7589fedax64ded2da1d7e2fd@mail.gmail.com>
	<644e1f320711301725q4e3f2a1bs3b00ce0bd873aac8@mail.gmail.com>
	<5032046e0711301734o2bdd53a5p3da1276bc68b28ba@mail.gmail.com>
Message-ID: <644e1f320711301741qa715174i26054fd2ebd5807a@mail.gmail.com>

I was just using colorRampPalette to generate a range of color.  You might try:

 Lab.palette <- colorRampPalette(c("dark red", "yellow", "dark green"))

to see if this is closer.  You can use the palette in other plot
functions; this was just the easiest to show what was happening.

On Nov 30, 2007 8:34 PM, affy snp <affysnp at gmail.com> wrote:
> Jim,
>
> This is not right as I think. The color code generated did not
> show a grading pattern. For example, it should go from
> very red---red---less red---dark----green---very green coinciding
> with the descending order of values, just like the very left panel
> shown in
> http://www.bme.unc.edu/research/Bioinformatics.FunctionalGenomics.html
> You got my point. But the case you gave is not the case I think.
> Any thoughts on this? Moreover, I still prefer it be done by heatmap
> because I also want the dendrogram.
>
> Best,
>      Allen
>
>
>
>
> On Nov 30, 2007 8:25 PM, jim holtman < jholtman at gmail.com> wrote:
> > Try this.  This will plot a similar scale for data.
> >
> > require(lattice)
> > # generate some data
> > x <- matrix(sample(0:8,100,replace=TRUE),10) # create 9 value
> > # create a color palette for outpu
> > Lab.palette <- colorRampPalette(c("red", 'yellow','green','blue'))
> > levelplot(x,col.regions=Lab.palette(10))
> >
> >
> >
> >
> >
> > On Nov 30, 2007 7:58 PM, affy snp <affysnp at gmail.com> wrote:
> > > Hi Jim,
> > >
> > > Thanks but I imagine the transformed data would
> > > not actually represent the pattern from the original
> > > data. Is there other alternatives? Ideally, I would
> > > like to have the color bar looking like the one
> > > in
> http://www.bme.unc.edu/research/Bioinformatics.FunctionalGenomics.html
> > >
> > > Best,
> > >      Allen
> > >
> > >
> > >
> > > On Nov 30, 2007 7:08 PM, jim holtman <jholtman at gmail.com> wrote:
> > > > If you are just going to have 3 values for coloring, then you could
> > > > just convert your data into 1, 2, 3:
> > > >
> > > > new.data <- ifelse(old.data > 2, 3, ifelse(old.data < 2, 1, 2))
> > > > dim(new.data) <- dim(old.data )
> > > >
> > > > and then plot it.
> > > >
> > > >
> > > >
> > > >
> > > > On Nov 30, 2007 5:09 PM, affy snp <affysnp at gmail.com> wrote:
> > > > > Hi list,
> > > > > My data set is comprised of 47 columns and about 700 rows.
> > > > > Most of the values would be around 2, while some will go beyond
> > > > > in either direction, higher or lower. Is there a way to specify the
> > > > > parameter of col or others if necessary to have the range of
> > > representing
> > > > > colors be 0----8, and make dark for 2, red for higher than 2 and
> green
> > > > > for lower than 2. Is there a way to do this? Thanks a lot for your
> > > > > suggestions.
> > > > >
> > > > > All the best,
> > > > >      Allen
> > > > >
> > > > >        [[alternative HTML version deleted]]
> > > > >
> > > > > ______________________________________________
> > > > > R-help at r-project.org mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.
> > > > >
> > > >
> > > >
> > > >
> > > > --
> > > > Jim Holtman
> > > > Cincinnati, OH
> > > > +1 513 646 9390
> > > >
> > > > What is the problem you are trying to solve?
> > > >
> > >
> > >
> >
> >
> >
> > --
> >
> >
> >
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 646 9390
> >
> > What is the problem you are trying to solve?
> >
>
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From juryef at yahoo.com  Sat Dec  1 02:44:03 2007
From: juryef at yahoo.com (Judith Flores)
Date: Fri, 30 Nov 2007 17:44:03 -0800 (PST)
Subject: [R] Dismiss previous email
Message-ID: <133643.93288.qm@web34701.mail.mud.yahoo.com>

Sorry about that, it was sent by accident.

I have a data frame that looks something like this:

   id day    k
  56  -1  566
  63  -1  680
  73  -1  773
  56   2  298
  63   2  273
  
    Of course, it is a very simplified version of the
real data frame I am working on. I need to add another
column that would represent a percent change in k from
day -1, by id. I put only two ids at day 2 to
emphasize the fact that after day -1 some subjects
won't be on the data frame any more.

I tried something like this:

pck<-by(dat,dat[,c("id","day")], function(x) {
      pc<-((x$k-x$k[x$day==-1])/x$k[x$day==-1])*100
      })
but it didn't work. 

Then I tried:

for(i in dat$id) {

    for(s in dat$day) {
        pc<-((dat$k[dat$id==i &
dat$day==s]-dat$k[dat$id==i &
dat$day==-1])/dat$k[dat$id==i & dat$day==-1])*100
}
}
without success.

I am sure it is very simple to do, but I would
appreciate any hints.

Thank you,

Judith



 


      ____________________________________________________________________________________
Be a better pen pal.


From ggrothendieck at gmail.com  Sat Dec  1 03:06:11 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 30 Nov 2007 21:06:11 -0500
Subject: [R] Lookup prior value in data.frame (was: Dismiss previous email)
Message-ID: <971536df0711301806v5b78fe9au955a898d1b869dbe@mail.gmail.com>

Assuming that there is a -1 day for every day
in the data frame and all the -1 days are at
the beginning, try this where k[match(id, id)]
is the vector of corresponding day -1 k-values:

DF <- data.frame(id = c(56, 63, 73, 56, 63),
	day = c(-1, -1, -1, 2, 2),
	k = c(566, 680, 773, 298, 273))

transform(DF, percent = 100 * k / k[match(id, id)] - 100)

Also, please use meaningful subject headings so
that future persons having the same problem can
more easily locate it in the archive.

On Nov 30, 2007 8:44 PM, Judith Flores <juryef at yahoo.com> wrote:
> Sorry about that, it was sent by accident.
>
> I have a data frame that looks something like this:
>
>   id day    k
>  56  -1  566
>  63  -1  680
>  73  -1  773
>  56   2  298
>  63   2  273
>
>    Of course, it is a very simplified version of the
> real data frame I am working on. I need to add another
> column that would represent a percent change in k from
> day -1, by id. I put only two ids at day 2 to
> emphasize the fact that after day -1 some subjects
> won't be on the data frame any more.
>
> I tried something like this:
>
> pck<-by(dat,dat[,c("id","day")], function(x) {
>      pc<-((x$k-x$k[x$day==-1])/x$k[x$day==-1])*100
>      })
> but it didn't work.
>
> Then I tried:
>
> for(i in dat$id) {
>
>    for(s in dat$day) {
>        pc<-((dat$k[dat$id==i &
> dat$day==s]-dat$k[dat$id==i &
> dat$day==-1])/dat$k[dat$id==i & dat$day==-1])*100
> }
> }
> without success.
>
> I am sure it is very simple to do, but I would
> appreciate any hints.
>
> Thank you,
>
> Judith
>
>
>
>
>
>
>      ____________________________________________________________________________________
> Be a better pen pal.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From daniel at umd.edu  Sat Dec  1 03:28:15 2007
From: daniel at umd.edu (Daniel Malter)
Date: Fri, 30 Nov 2007 21:28:15 -0500
Subject: [R] Quantiles and QQ plots
In-Reply-To: <14097909.post@talk.nabble.com>
Message-ID: <200712010227.DCO56069@md0.mail.umd.edu>

Your vector:

x=c(5,9,6,1,5,9,7,4,5,6,3,2,4,8,9,6,1,8,4,8)

Check descriptives:

mean(x)
sd(x)

get quantiles for a normal with mean and standard deviation of x:

qnorm(c(0.25,0.5,0.75),mean=mean(x),sd=sd(x))

Output:

[1] 3.76997 5.50000 7.23003

to get the quantile plot install library "car" and load library by typing in
the R prompt: library(car) 

qq.plot(x)

But you can easily look at the histogram of your vector - hist(x) - to see
that this does not look very normal

Daniel



-----Urspr?ngliche Nachricht-----
Von: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Im
Auftrag von alfieim29
Gesendet: Friday, November 30, 2007 6:26 PM
An: r-help at r-project.org
Betreff: [R] Quantiles and QQ plots


I have 20 variables:

5,9,6,1,5,9,7,4,5,6,3,2,4,8,9,6,1,8,4,8

How do I calculate the corresponding quantiles from a normal distribution
with the same mean and variance as the sample?

Also, how do I draw a QQ plot of the data?

Thanks for any help!
--
View this message in context:
http://www.nabble.com/Quantiles-and-QQ-plots-tf4925742.html#a14097909
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From baj2107 at columbia.edu  Sat Dec  1 04:24:34 2007
From: baj2107 at columbia.edu (Bernd Jagla)
Date: Fri, 30 Nov 2007 22:24:34 -0500
Subject: [R] compare strings
Message-ID: <001001c833c9$b26ab5c0$791d919c@cgc.cpmc.columbia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071130/91d06586/attachment.pl 

From baj2107 at columbia.edu  Sat Dec  1 04:40:46 2007
From: baj2107 at columbia.edu (Bernd Jagla)
Date: Fri, 30 Nov 2007 22:40:46 -0500
Subject: [R] compare strings
In-Reply-To: <001001c833c9$b26ab5c0$791d919c@cgc.cpmc.columbia.edu>
References: <001001c833c9$b26ab5c0$791d919c@cgc.cpmc.columbia.edu>
Message-ID: <001501c833cb$f585bf60$791d919c@cgc.cpmc.columbia.edu>

Just found out that I am actually dealing with factors:
> t3[1,5] == t3[1,3]
Error in Ops.factor(t3[1, 5], t3[1, 3]) : level sets of factors are
different

But the problem remains... how can I compare them?

Thanks,

Bernd

|-----Original Message-----
|From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
|Behalf Of Bernd Jagla
|Sent: Friday, November 30, 2007 10:25 PM
|To: r-help at r-project.org
|Subject: [R] compare strings
|
|Sorry for the question, but I really cannot find the right search terms to
|find an answer..
|
|
|
|I have a data frame with strings in some of the columns.
|
|I want to know all the rows where the strings in both columns are equal.
|
|
|
|How do I do this?
|
|
|
|Thanks,
|
|
|
|Bernd
|
|
|	[[alternative HTML version deleted]]
|
|______________________________________________
|R-help at r-project.org mailing list
|https://stat.ethz.ch/mailman/listinfo/r-help
|PLEASE do read the posting guide http://www.R-project.org/posting-
|guide.html
|and provide commented, minimal, self-contained, reproducible code.


From baj2107 at columbia.edu  Sat Dec  1 04:47:59 2007
From: baj2107 at columbia.edu (Bernd Jagla)
Date: Fri, 30 Nov 2007 22:47:59 -0500
Subject: [R] compare strings
In-Reply-To: <001001c833c9$b26ab5c0$791d919c@cgc.cpmc.columbia.edu>
References: <001001c833c9$b26ab5c0$791d919c@cgc.cpmc.columbia.edu>
Message-ID: <001601c833cc$f7b68ca0$791d919c@cgc.cpmc.columbia.edu>

It helps writing down these question, you are then getting much closer to an
answer...

summary(as.integer(t3[,2]) == as.integer(t3[,4]) & as.integer(t3[,3]) ==
as.integer(t3[,5]))

will compare two pairs of column pairs and give a count of flase and true
rows...

-B

|-----Original Message-----
|From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
|Behalf Of Bernd Jagla
|Sent: Friday, November 30, 2007 10:25 PM
|To: r-help at r-project.org
|Subject: [R] compare strings
|
|Sorry for the question, but I really cannot find the right search terms to
|find an answer..
|
|
|
|I have a data frame with strings in some of the columns.
|
|I want to know all the rows where the strings in both columns are equal.
|
|
|
|How do I do this?
|
|
|
|Thanks,
|
|
|
|Bernd
|
|
|	[[alternative HTML version deleted]]
|
|______________________________________________
|R-help at r-project.org mailing list
|https://stat.ethz.ch/mailman/listinfo/r-help
|PLEASE do read the posting guide http://www.R-project.org/posting-
|guide.html
|and provide commented, minimal, self-contained, reproducible code.


From markleeds at verizon.net  Sat Dec  1 04:50:19 2007
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 30 Nov 2007 21:50:19 -0600 (CST)
Subject: [R] compare strings
Message-ID: <301729.2745341196481020010.JavaMail.root@vms062.mailsrvcs.net>

>From: Bernd Jagla <baj2107 at columbia.edu>
>Date: 2007/11/30 Fri PM 09:24:34 CST
>To: r-help at r-project.org
>Subject: [R] compare strings

below assumes that there are only 2 columns in
the dataframe or that the string columns are the 
first two columns ? 

which(df[,1] == df[,2])

>Sorry for the question, but I really cannot find the right search terms to
>find an answer..
>
> 
>
>I have a data frame with strings in some of the columns.
>
>I want to know all the rows where the strings in both columns are equal.
>
> 
>
>How do I do this?
>
> 
>
>Thanks,
>
> 
>
>Bernd
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Sat Dec  1 05:31:34 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 30 Nov 2007 23:31:34 -0500
Subject: [R] Dismiss previous email
In-Reply-To: <133643.93288.qm@web34701.mail.mud.yahoo.com>
References: <133643.93288.qm@web34701.mail.mud.yahoo.com>
Message-ID: <644e1f320711302031u541e1cd2g62403a8ee35b09d9@mail.gmail.com>

Does this do it for you?

> x <- read.table(textConnection(" id day    k
+  56  -1  566
+  63  -1  680
+  73  -1  773
+  56   2  298
+  63   2  273"), header=TRUE)
> x$percent <- ave(x$k, list(x$id), FUN=function(.data){
+     (.data - .data[1]) / .data[1] * 100
+ })
>
> x
  id day   k   percent
1 56  -1 566   0.00000
2 63  -1 680   0.00000
3 73  -1 773   0.00000
4 56   2 298 -47.34982
5 63   2 273 -59.85294
>


On Nov 30, 2007 8:44 PM, Judith Flores <juryef at yahoo.com> wrote:
> Sorry about that, it was sent by accident.
>
> I have a data frame that looks something like this:
>
>   id day    k
>  56  -1  566
>  63  -1  680
>  73  -1  773
>  56   2  298
>  63   2  273
>
>    Of course, it is a very simplified version of the
> real data frame I am working on. I need to add another
> column that would represent a percent change in k from
> day -1, by id. I put only two ids at day 2 to
> emphasize the fact that after day -1 some subjects
> won't be on the data frame any more.
>
> I tried something like this:
>
> pck<-by(dat,dat[,c("id","day")], function(x) {
>      pc<-((x$k-x$k[x$day==-1])/x$k[x$day==-1])*100
>      })
> but it didn't work.
>
> Then I tried:
>
> for(i in dat$id) {
>
>    for(s in dat$day) {
>        pc<-((dat$k[dat$id==i &
> dat$day==s]-dat$k[dat$id==i &
> dat$day==-1])/dat$k[dat$id==i & dat$day==-1])*100
> }
> }
> without success.
>
> I am sure it is very simple to do, but I would
> appreciate any hints.
>
> Thank you,
>
> Judith
>
>
>
>
>
>
>      ____________________________________________________________________________________
> Be a better pen pal.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From Bill.Venables at csiro.au  Sat Dec  1 07:37:31 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Sat, 1 Dec 2007 16:37:31 +1000
Subject: [R] Generating a value
References: <14086120.post@talk.nabble.com>
Message-ID: <B998A44C8986644EA8029CFE6396A924D89A79@exqld2-bne.nexus.csiro.au>

Here's how to generate a million

psample <- rpois(1000000, lambda = 20)

so guess how you would generate one.

The main point is that if you really want to generate more than one, do
it all at once like the above, do not do it in a loop. 


Bill Venables
CSIRO Laboratories
PO Box 120, Cleveland, 4163
AUSTRALIA
Office Phone (email preferred): +61 7 3826 7251
Fax (if absolutely necessary):  +61 7 3826 7304
Mobile:                         +61 4 8819 4402
Home Phone:                     +61 7 3286 7700
mailto:Bill.Venables at csiro.au
http://www.cmis.csiro.au/bill.venables/ 

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of alfieim29
Sent: Saturday, 1 December 2007 6:21 AM
To: r-help at r-project.org
Subject: [R] Generating a value


How do I generate a value in R from a poisson distribution with mean 20?

Thanks!
-- 
View this message in context:
http://www.nabble.com/Generating-a-value-tf4922234.html#a14086120
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Bill.Venables at csiro.au  Sat Dec  1 07:43:53 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Sat, 1 Dec 2007 16:43:53 +1000
Subject: [R] assign vector name
References: <14077275.post@talk.nabble.com>
Message-ID: <B998A44C8986644EA8029CFE6396A924D89A7A@exqld2-bne.nexus.csiro.au>

Well, you can do it:

assign(paste("myname", name[1], sep = "."), seq(1:10))

but this looks clumsy to me, as if you were trying to make R do
something the same way that you do things in some other system.  This is
nearly always a bad idea. My advice is to learn how to do things the R
way.


Bill Venables
CSIRO Laboratories
PO Box 120, Cleveland, 4163
AUSTRALIA
Office Phone (email preferred): +61 7 3826 7251
Fax (if absolutely necessary):  +61 7 3826 7304
Mobile:                         +61 4 8819 4402
Home Phone:                     +61 7 3286 7700
mailto:Bill.Venables at csiro.au
http://www.cmis.csiro.au/bill.venables/ 

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of livia
Sent: Saturday, 1 December 2007 2:12 AM
To: r-help at r-project.org
Subject: [R] assign vector name


Hello,

I have got a vector, for example seq(1:10), and I have an array "name"
defined before which contain the vector name I would like to use. So
name[1]= "price". I would like to name the vector in the following
codes:

paste("myname", name[1]) <- seq(1:10)

But it does not work. Could anyone give me some advice? Many thanks.
-- 
View this message in context:
http://www.nabble.com/assign-vector-name-tf4918696.html#a14077275
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Bill.Venables at csiro.au  Sat Dec  1 07:54:09 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Sat, 1 Dec 2007 16:54:09 +1000
Subject: [R] Quantiles and QQ plots
References: <14097909.post@talk.nabble.com>
Message-ID: <B998A44C8986644EA8029CFE6396A924D89A7B@exqld2-bne.nexus.csiro.au>

 


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of alfieim29
Sent: Saturday, 1 December 2007 9:26 AM
To: r-help at r-project.org
Subject: [R] Quantiles and QQ plots


I have 20 variables:

5,9,6,1,5,9,7,4,5,6,3,2,4,8,9,6,1,8,4,8

[WNV] I think you have 20 values, not variables.


How do I calculate the corresponding quantiles from a normal
distribution
with the same mean and variance as the sample?


[WNV] There is some ambiguity about this, but a simple way to do it
would be

x <- c(5,9,6,1,5,9,7,4,5,6,3,2,4,8,9,6,1,8,4,8)
n <- length(x)
p <- (1:n - 0.5)/n
z <- qnorm(p, mean(x), sd(x))[order(order(x))]




Also, how do I draw a QQ plot of the data?

[WNV] having done all this you can now just

plot(z, x)

but a much simpler way is

qqnorm(x)
qqline(x)

and don't bother calculating all that stuff!

Thanks for any help!
-- 
View this message in context:
http://www.nabble.com/Quantiles-and-QQ-plots-tf4925742.html#a14097909
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Bill.Venables at csiro.au  Sat Dec  1 08:21:28 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Sat, 1 Dec 2007 17:21:28 +1000
Subject: [R] Rating R Helpers
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
Message-ID: <B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>

This seems a little impractical to me.  People respond so much at random
and most only tackle questions with which they feel comfortable.  As
it's not a competition in any sense, it's going to be hard to rank
people in any effective way.  But suppose you succeed in doing so, then
what?

To me a much more urgent initiative is some kind of user online review
system for packages, even something as simple as that used by Amazon.com
has for customer review of books.

I think the need for this is rather urgent, in fact.  Most packages are
very good, but I regret to say some are pretty inefficient and others
downright dangerous.  You don't want to discourage people from
submitting their work to CRAN, but at the same time you do want some
mechanism that allows users to relate their experience with it, good or
bad.   


Bill Venables
CSIRO Laboratories
PO Box 120, Cleveland, 4163
AUSTRALIA
Office Phone (email preferred): +61 7 3826 7251
Fax (if absolutely necessary):  +61 7 3826 7304
Mobile:                         +61 4 8819 4402
Home Phone:                     +61 7 3286 7700
mailto:Bill.Venables at csiro.au
http://www.cmis.csiro.au/bill.venables/ 

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Doran, Harold
Sent: Saturday, 1 December 2007 6:13 AM
To: R Help
Subject: [R] Rating R Helpers

Since R is open source and help may come from varied levels of
experience on R-Help, I wonder if it might be helpful to construct a
method that can be used to "rate" those who provide help on this list.

This is something that is done on other comp lists, like
http://www.experts-exchange.com/.

I think some of the reasons for this are pretty transparent, but I
suppose one reason is that one could decide to implement the advise of
those with "superior" or "expert" levels. In other words, you can trust
the advice of someone who is more experienced more than someone who is
not. Currently, there is no way to discern who on this list is really an
R expert and who is not. Of course, there is R core, but most people
don't actually know who these people are (at least I surmise that to be
true).

If this is potentially useful, maybe one way to begin the development of
such ratings is to allow the original poster to "rate" the level of help
from those who responded. Maybe something like a very simple
questionnaire on a likert-like scale that the original poster would
respond to upon receiving help which would lead to the accumulation of
points for the responders. Higher points would result in higher levels
of expertise (e.g., novice, ..., wizaRd).

Just a random thought. What do others think?

Harold




	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Sat Dec  1 09:55:58 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sat, 01 Dec 2007 09:55:58 +0100
Subject: [R] compare strings
In-Reply-To: <001601c833cc$f7b68ca0$791d919c@cgc.cpmc.columbia.edu>
References: <001001c833c9$b26ab5c0$791d919c@cgc.cpmc.columbia.edu>
	<001601c833cc$f7b68ca0$791d919c@cgc.cpmc.columbia.edu>
Message-ID: <4751219E.6000907@biostat.ku.dk>

Bernd Jagla wrote:
> It helps writing down these question, you are then getting much closer to an
> answer...
>
> summary(as.integer(t3[,2]) == as.integer(t3[,4]) & as.integer(t3[,3]) ==
> as.integer(t3[,5]))
>
> will compare two pairs of column pairs and give a count of flase and true
> rows...
>
>   
If they really are factors with different level sets, I think you might 
prefer as.character() there.

 > x <- factor(c("a","b"))
 > y <- factor(c("b","c"))
 > x==y
Error in Ops.factor(x, y) : level sets of factors are different
 > as.integer(x)==as.integer(y)
[1] TRUE TRUE
 > as.character(x)==as.character(y)
[1] FALSE FALSE

Also, extending the above slightly:
 > d <- as.character(x)==as.character(y)
 > table(d)
d
FALSE  TRUE
    2     1
 > which(!d)
[1] 1 2



> -B
>
> |-----Original Message-----
> |From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
> |Behalf Of Bernd Jagla
> |Sent: Friday, November 30, 2007 10:25 PM
> |To: r-help at r-project.org
> |Subject: [R] compare strings
> |
> |Sorry for the question, but I really cannot find the right search terms to
> |find an answer..
> |
> |
> |
> |I have a data frame with strings in some of the columns.
> |
> |I want to know all the rows where the strings in both columns are equal.
> |
> |
> |
> |How do I do this?
> |
> |
> |
> |Thanks,
> |
> |
> |
> |Bernd
> |
> |
> |	[[alternative HTML version deleted]]
> |
> |______________________________________________
> |R-help at r-project.org mailing list
> |https://stat.ethz.ch/mailman/listinfo/r-help
> |PLEASE do read the posting guide http://www.R-project.org/posting-
> |guide.html
> |and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p.dalgaard at biostat.ku.dk  Sat Dec  1 10:07:29 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sat, 01 Dec 2007 10:07:29 +0100
Subject: [R] compare strings
In-Reply-To: <4751219E.6000907@biostat.ku.dk>
References: <001001c833c9$b26ab5c0$791d919c@cgc.cpmc.columbia.edu>	<001601c833cc$f7b68ca0$791d919c@cgc.cpmc.columbia.edu>
	<4751219E.6000907@biostat.ku.dk>
Message-ID: <47512451.9060105@biostat.ku.dk>

Peter Dalgaard wrote:
> Also, extending the above slightly:
>   
Oups. Two lines fell out. I meant:

 > y <- factor(c("b","c","b"))
 > x <- factor(c("a","b","b"))
 > d <- as.character(x)==as.character(y)
 > table(d)
d
FALSE  TRUE
    2     1
 > which(!d)
[1] 1 2


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From eugen_pircalabelu at yahoo.com  Sat Dec  1 12:02:36 2007
From: eugen_pircalabelu at yahoo.com (eugen pircalabelu)
Date: Sat, 1 Dec 2007 03:02:36 -0800 (PST)
Subject: [R] modeling time series with ARIMA
Message-ID: <802765.91180.qm@web38615.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071201/fb27b586/attachment.pl 

From tom.soyer at gmail.com  Sat Dec  1 12:37:06 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Sat, 1 Dec 2007 05:37:06 -0600
Subject: [R] R function for percentrank
Message-ID: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071201/965f3e57/attachment.pl 

From lauri.nikkinen at iki.fi  Sat Dec  1 14:05:26 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Sat, 1 Dec 2007 15:05:26 +0200
Subject: [R] How to cbind DF:s with differing number of rows?
Message-ID: <ba8c09910712010505g8facc58h8645b5d98627f192@mail.gmail.com>

#Hi R-users,
#Suppose that I have a data.frame like this:

y1 <- rnorm(10) + 6.8
y2 <- rnorm(10) + (1:10*1.7 + 1)
y3 <- rnorm(10) + (1:10*6.7 + 3.7)
y <- c(y1,y2,y3)
x <- rep(1:3,10)
f <- gl(2,15, labels=paste("lev", 1:2, sep=""))
g <- seq(as.Date("2000/1/1"), by="day", length=30)
DF <- data.frame(x=x,y=y, f=f, g=g)
DF$g[DF$x == 1] <- NA
DF$x[3:6] <- NA
DF$wdays <- weekdays(DF$g)

DF

#For EDA purposes, I would like to calculate frequences in each variable
g <- lapply(DF, function(x) as.data.frame(table(x)))

#After this, I would like to cbind these data.frames (in g) into a
single data.frame (which to export to MS Excel)
#do.call(cbind, g) does not seem to work because of the different
number of rows in each data.frame.
#The resulting data.frame shoul look like this (only two variables
printed here):

Rowid;x;Freq.x;y;Freq.y; # etc...
1;1;9;1.69151845313816;1;
2;2;9;5.03748767699799;1;
3;3;8;5.37387749444247;1;
4;Empty;Empty;6.83926626214299;1;
5;Empty;Empty;6.97484558968873;1;
6;Empty;Empty;7.11023821708323;1;
7;Empty;Empty;7.1348316549091;1;
8;Empty;Empty;7.16727166992407;1;
9;Empty;Empty;7.35983428577469;1;
10;Empty;Empty;7.7596470136235;1;
11;Empty;Empty;7.86369414967578;1;
12;Empty;Empty;7.97164674771006;1;
13;Empty;Empty;8.0787295301318;1;
14;Empty;Empty;8.14161030348166;1;
15;Empty;Empty;8.20134832959661;1;
16;Empty;Empty;10.1469115339016;1
17;Empty;Empty;12.7442067301746;1
18;Empty;Empty;14.0865167751202;1
19;Empty;Empty;15.8280312307450;1
20;Empty;Empty;16.0484499360756;1
21;Empty;Empty;17.0795222149999;1
22;Empty;Empty;18.1254057823357;1
23;Empty;Empty;22.7169729331525;1
24;Empty;Empty;30.7237748005358;1
25;Empty;Empty;37.2141271786934;1
26;Empty;Empty;44.4954633229803;1
27;Empty;Empty;50.2302409305761;1
28;Empty;Empty;57.8913405112114;1
29;Empty;Empty;64.849897477945;1
30;Empty;Empty;71.4205263353053;1


#Anyone have an idea how to do this?

#Thanks,
#Lauri


From ggrothendieck at gmail.com  Sat Dec  1 15:13:55 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 1 Dec 2007 09:13:55 -0500
Subject: [R] Rating R Helpers
In-Reply-To: <B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
	<B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
Message-ID: <971536df0712010613w6fd2314cofb6cb79630249ea5@mail.gmail.com>

On Dec 1, 2007 2:21 AM,  <Bill.Venables at csiro.au> wrote:
> To me a much more urgent initiative is some kind of user online review
> system for packages, even something as simple as that used by Amazon.com
> has for customer review of books.
>
> I think the need for this is rather urgent, in fact.  Most packages are
> very good, but I regret to say some are pretty inefficient and others
> downright dangerous.  You don't want to discourage people from
> submitting their work to CRAN, but at the same time you do want some
> mechanism that allows users to relate their experience with it, good or
> bad.

You can get a very rough idea of this automatically by making a list of which
packages  are dependents of other packages.

library(pkgDepTools) # from BioC
AP <- available.packages()
dep <- AP[, "Depends"]
deps <- unlist(sapply(dep, pkgDepTools:::cleanPkgField))
sort(table(deps))

Of course some packages are more naturally end-user oriented and so
would never make such a list and others may be good but just no one
knows about them so they have never been used.   Some packages
might get on the list because an author has two packages and one uses
the other so an enhancement could be to eliminate dependencies with
a common author.


From jholtman at gmail.com  Sat Dec  1 15:45:29 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 1 Dec 2007 09:45:29 -0500
Subject: [R] How to cbind DF:s with differing number of rows?
In-Reply-To: <ba8c09910712010505g8facc58h8645b5d98627f192@mail.gmail.com>
References: <ba8c09910712010505g8facc58h8645b5d98627f192@mail.gmail.com>
Message-ID: <644e1f320712010645s5808bfc0jd9316017225099d2@mail.gmail.com>

This should do it for you by padding out the rows so they are the same length:

> # use your 'g' and pad out the rows so they are the same length
> str(g)
List of 5
 $ x    :'data.frame':  3 obs. of  2 variables:
  ..$ x   : Factor w/ 3 levels "1","2","3": 1 2 3
  ..$ Freq: int [1:3] 9 9 8
 $ y    :'data.frame':  30 obs. of  2 variables:
  ..$ x   : Factor w/ 30 levels "4.21178116845085",..: 1 2 3 4 5 6 7 8 9 10 ...
  ..$ Freq: int [1:30] 1 1 1 1 1 1 1 1 1 1 ...
 $ f    :'data.frame':  2 obs. of  2 variables:
  ..$ x   : Factor w/ 2 levels "lev1","lev2": 1 2
  ..$ Freq: int [1:2] 15 15
 $ g    :'data.frame':  20 obs. of  2 variables:
  ..$ x   : Factor w/ 20 levels "2000-01-02","2000-01-03",..: 1 2 3 4
5 6 7 8 9 10 ...
  ..$ Freq: int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
 $ wdays:'data.frame':  7 obs. of  2 variables:
  ..$ x   : Factor w/ 7 levels "Friday","Monday",..: 1 2 3 4 5 6 7
  ..$ Freq: int [1:7] 2 3 3 4 3 2 3
> # determine max nrows
> max.rows <- max(sapply(g, nrow))
> g.new <- lapply(g, function(.x){
+     if (nrow(.x) < max.rows) .x <- rbind(.x, matrix(NA, ncol=2,
nrow=max.rows - nrow(.x),
+         dimnames=list(NULL, c('x', 'Freq'))))
+     .x
+ })
> do.call('cbind', g.new)
    x.x x.Freq              y.x y.Freq  f.x f.Freq        g.x g.Freq
wdays.x wdays.Freq
1     1      9 4.21178116845085      1 lev1     15 2000-01-02      1
 Friday          2
2     2      9 4.78984323641143      1 lev2     15 2000-01-03      1
 Monday          3
3     3      8  5.4787594194582      1 <NA>     NA 2000-01-05      1
Saturday          3
4  <NA>     NA  5.5853001128225      1 <NA>     NA 2000-01-06      1
 Sunday          4
5  <NA>     NA 5.96437138758995      1 <NA>     NA 2000-01-08      1
Thursday          3
6  <NA>     NA 5.97953161588198      1 <NA>     NA 2000-01-09      1
Tuesday          2
7  <NA>     NA 6.17354618925767      1 <NA>     NA 2000-01-11      1
Wednesday          3
8  <NA>     NA 6.49461161284364      1 <NA>     NA 2000-01-12      1
   <NA>         NA
9  <NA>     NA 6.98364332422208      1 <NA>     NA 2000-01-14      1
   <NA>         NA
10 <NA>     NA 7.12950777181536      1 <NA>     NA 2000-01-15      1
   <NA>         NA
11 <NA>     NA 7.28742905242849      1 <NA>     NA 2000-01-17      1
   <NA>         NA
12 <NA>     NA  7.3757813516535      1 <NA>     NA 2000-01-18      1
   <NA>         NA
13 <NA>     NA 7.53832470512922      1 <NA>     NA 2000-01-20      1
   <NA>         NA
14 <NA>     NA 8.39528080213779      1 <NA>     NA 2000-01-21      1
   <NA>         NA
15 <NA>     NA 10.6249309181431      1 <NA>     NA 2000-01-23      1
   <NA>         NA
16 <NA>     NA 11.1550663909848      1 <NA>     NA 2000-01-24      1
   <NA>         NA
17 <NA>     NA 11.3189773716082      1 <NA>     NA 2000-01-26      1
   <NA>         NA
18 <NA>     NA 12.8838097369011      1 <NA>     NA 2000-01-27      1
   <NA>         NA
19 <NA>     NA 15.5438362106853      1 <NA>     NA 2000-01-29      1
   <NA>         NA
20 <NA>     NA 17.1212211950981      1 <NA>     NA 2000-01-30      1
   <NA>         NA
21 <NA>     NA 17.8821363007311      1 <NA>     NA       <NA>     NA
   <NA>         NA
22 <NA>     NA 18.5939013212175      1 <NA>     NA       <NA>     NA
   <NA>         NA


On Dec 1, 2007 8:05 AM, Lauri Nikkinen <lauri.nikkinen at iki.fi> wrote:
> #Hi R-users,
> #Suppose that I have a data.frame like this:
>
> y1 <- rnorm(10) + 6.8
> y2 <- rnorm(10) + (1:10*1.7 + 1)
> y3 <- rnorm(10) + (1:10*6.7 + 3.7)
> y <- c(y1,y2,y3)
> x <- rep(1:3,10)
> f <- gl(2,15, labels=paste("lev", 1:2, sep=""))
> g <- seq(as.Date("2000/1/1"), by="day", length=30)
> DF <- data.frame(x=x,y=y, f=f, g=g)
> DF$g[DF$x == 1] <- NA
> DF$x[3:6] <- NA
> DF$wdays <- weekdays(DF$g)
>
> DF
>
> #For EDA purposes, I would like to calculate frequences in each variable
> g <- lapply(DF, function(x) as.data.frame(table(x)))
>
> #After this, I would like to cbind these data.frames (in g) into a
> single data.frame (which to export to MS Excel)
> #do.call(cbind, g) does not seem to work because of the different
> number of rows in each data.frame.
> #The resulting data.frame shoul look like this (only two variables
> printed here):
>
> Rowid;x;Freq.x;y;Freq.y; # etc...
> 1;1;9;1.69151845313816;1;
> 2;2;9;5.03748767699799;1;
> 3;3;8;5.37387749444247;1;
> 4;Empty;Empty;6.83926626214299;1;
> 5;Empty;Empty;6.97484558968873;1;
> 6;Empty;Empty;7.11023821708323;1;
> 7;Empty;Empty;7.1348316549091;1;
> 8;Empty;Empty;7.16727166992407;1;
> 9;Empty;Empty;7.35983428577469;1;
> 10;Empty;Empty;7.7596470136235;1;
> 11;Empty;Empty;7.86369414967578;1;
> 12;Empty;Empty;7.97164674771006;1;
> 13;Empty;Empty;8.0787295301318;1;
> 14;Empty;Empty;8.14161030348166;1;
> 15;Empty;Empty;8.20134832959661;1;
> 16;Empty;Empty;10.1469115339016;1
> 17;Empty;Empty;12.7442067301746;1
> 18;Empty;Empty;14.0865167751202;1
> 19;Empty;Empty;15.8280312307450;1
> 20;Empty;Empty;16.0484499360756;1
> 21;Empty;Empty;17.0795222149999;1
> 22;Empty;Empty;18.1254057823357;1
> 23;Empty;Empty;22.7169729331525;1
> 24;Empty;Empty;30.7237748005358;1
> 25;Empty;Empty;37.2141271786934;1
> 26;Empty;Empty;44.4954633229803;1
> 27;Empty;Empty;50.2302409305761;1
> 28;Empty;Empty;57.8913405112114;1
> 29;Empty;Empty;64.849897477945;1
> 30;Empty;Empty;71.4205263353053;1
>
>
> #Anyone have an idea how to do this?
>
> #Thanks,
> #Lauri
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From bates at stat.wisc.edu  Sat Dec  1 16:26:12 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 1 Dec 2007 09:26:12 -0600
Subject: [R] lmer and method call
In-Reply-To: <37787.70.71.25.135.1196388560.squirrel@www.cfenet.ubc.ca>
References: <37787.70.71.25.135.1196388560.squirrel@www.cfenet.ubc.ca>
Message-ID: <40e66e0b0712010726j599dc405ifac521481b651ee1@mail.gmail.com>

On Nov 29, 2007 8:09 PM, M-J Milloy <mjmilloy at cfenet.ubc.ca> wrote:
>
> Hello all,
>
> I'm attempting to fit a generalized linear mixed-effects model using lmer
> (R v 2.6.0, lmer 0.99875-9, Mac OS X 10.4.10) using the call:
>
> vidusLMER1 <- lmer(jail ~ visit + gender + house + cokefreq + cracfreq +
> herofreq + borcur + comc + (1 | code), data = vidusGD, family = binomial,
> correlation = corCompSymm(form = 1 | ID), method = "ML")
>
> Although the model fits, the summary indicates the model is a "Generalized
> linear mixed model fit using Laplace". I've tried any number of
> permutations; is only Laplace supported in lmer, despite the text of the
> help file?

The help file does say that for a generalized linear mixed model
(GLMM), which is what family = binomial implies, the estimation
criterion is always "ML" (maximum likelihood) as opposed to "REML"
(restricted, or residual, maximum likelihood).  So stating method =
"ML" is redundant.

For a GLMM, however, the log-likelihood cannot not be evaluated
directly and must be approximated.  Here the help file is misleading
because it implies that there are three possible approximations, "PQL"
(penalized quasi-likelihood), "Laplace" and "AGQ" (adaptive Gaussian
quadrature).  AGQ has not yet been implemented so the only effective
choices are PQL and Laplace.  The default is PQL, to refine the
starting estimates, followed by optimization of the Laplace
approximation.  In some cases it is an advantage to suppress the PQL
iterations which can be done with one of the settings for the control
argument.


From bates at stat.wisc.edu  Sat Dec  1 16:28:33 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 1 Dec 2007 09:28:33 -0600
Subject: [R] lmer and method call
In-Reply-To: <40e66e0b0712010726j599dc405ifac521481b651ee1@mail.gmail.com>
References: <37787.70.71.25.135.1196388560.squirrel@www.cfenet.ubc.ca>
	<40e66e0b0712010726j599dc405ifac521481b651ee1@mail.gmail.com>
Message-ID: <40e66e0b0712010728k177e9b9dp54a3a71dc3f30dd@mail.gmail.com>

On Dec 1, 2007 9:26 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Nov 29, 2007 8:09 PM, M-J Milloy <mjmilloy at cfenet.ubc.ca> wrote:
> >
> > Hello all,
> >
> > I'm attempting to fit a generalized linear mixed-effects model using lmer
> > (R v 2.6.0, lmer 0.99875-9, Mac OS X 10.4.10) using the call:
> >
> > vidusLMER1 <- lmer(jail ~ visit + gender + house + cokefreq + cracfreq +
> > herofreq + borcur + comc + (1 | code), data = vidusGD, family = binomial,
> > correlation = corCompSymm(form = 1 | ID), method = "ML")
> >
> > Although the model fits, the summary indicates the model is a "Generalized
> > linear mixed model fit using Laplace". I've tried any number of
> > permutations; is only Laplace supported in lmer, despite the text of the
> > help file?
>
> The help file does say that for a generalized linear mixed model
> (GLMM), which is what family = binomial implies, the estimation
> criterion is always "ML" (maximum likelihood) as opposed to "REML"
> (restricted, or residual, maximum likelihood).  So stating method =
> "ML" is redundant.
>
> For a GLMM, however, the log-likelihood cannot not be evaluated
> directly and must be approximated.  Here the help file is misleading
> because it implies that there are three possible approximations, "PQL"
> (penalized quasi-likelihood), "Laplace" and "AGQ" (adaptive Gaussian
> quadrature).  AGQ has not yet been implemented so the only effective
> choices are PQL and Laplace.  The default is PQL, to refine the
> starting estimates, followed by optimization of the Laplace
> approximation.  In some cases it is an advantage to suppress the PQL
> iterations which can be done with one of the settings for the control
> argument.

I forgot to mention that the correlation argument has no effect in
this call.  That argument is for the lme function in the nlme package.
 In lmer it is ignored.


From lauri.nikkinen at iki.fi  Sat Dec  1 17:15:21 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Sat, 1 Dec 2007 18:15:21 +0200
Subject: [R] How to cbind DF:s with differing number of rows?
In-Reply-To: <644e1f320712010645s5808bfc0jd9316017225099d2@mail.gmail.com>
References: <ba8c09910712010505g8facc58h8645b5d98627f192@mail.gmail.com>
	<644e1f320712010645s5808bfc0jd9316017225099d2@mail.gmail.com>
Message-ID: <ba8c09910712010815t4f83b6dfja272de13e4870172@mail.gmail.com>

Thanks Jim! That helped me a lot!

Cheers,
Lauri

2007/12/1, jim holtman <jholtman at gmail.com>:
> This should do it for you by padding out the rows so they are the same length:
>
> > # use your 'g' and pad out the rows so they are the same length
> > str(g)
> List of 5
>  $ x    :'data.frame':  3 obs. of  2 variables:
>  ..$ x   : Factor w/ 3 levels "1","2","3": 1 2 3
>  ..$ Freq: int [1:3] 9 9 8
>  $ y    :'data.frame':  30 obs. of  2 variables:
>  ..$ x   : Factor w/ 30 levels "4.21178116845085",..: 1 2 3 4 5 6 7 8 9 10 ...
>  ..$ Freq: int [1:30] 1 1 1 1 1 1 1 1 1 1 ...
>  $ f    :'data.frame':  2 obs. of  2 variables:
>  ..$ x   : Factor w/ 2 levels "lev1","lev2": 1 2
>  ..$ Freq: int [1:2] 15 15
>  $ g    :'data.frame':  20 obs. of  2 variables:
>  ..$ x   : Factor w/ 20 levels "2000-01-02","2000-01-03",..: 1 2 3 4
> 5 6 7 8 9 10 ...
>  ..$ Freq: int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
>  $ wdays:'data.frame':  7 obs. of  2 variables:
>  ..$ x   : Factor w/ 7 levels "Friday","Monday",..: 1 2 3 4 5 6 7
>  ..$ Freq: int [1:7] 2 3 3 4 3 2 3
> > # determine max nrows
> > max.rows <- max(sapply(g, nrow))
> > g.new <- lapply(g, function(.x){
> +     if (nrow(.x) < max.rows) .x <- rbind(.x, matrix(NA, ncol=2,
> nrow=max.rows - nrow(.x),
> +         dimnames=list(NULL, c('x', 'Freq'))))
> +     .x
> + })
> > do.call('cbind', g.new)
>    x.x x.Freq              y.x y.Freq  f.x f.Freq        g.x g.Freq
> wdays.x wdays.Freq
> 1     1      9 4.21178116845085      1 lev1     15 2000-01-02      1
>  Friday          2
> 2     2      9 4.78984323641143      1 lev2     15 2000-01-03      1
>  Monday          3
> 3     3      8  5.4787594194582      1 <NA>     NA 2000-01-05      1
> Saturday          3
> 4  <NA>     NA  5.5853001128225      1 <NA>     NA 2000-01-06      1
>  Sunday          4
> 5  <NA>     NA 5.96437138758995      1 <NA>     NA 2000-01-08      1
> Thursday          3
> 6  <NA>     NA 5.97953161588198      1 <NA>     NA 2000-01-09      1
> Tuesday          2
> 7  <NA>     NA 6.17354618925767      1 <NA>     NA 2000-01-11      1
> Wednesday          3
> 8  <NA>     NA 6.49461161284364      1 <NA>     NA 2000-01-12      1
>   <NA>         NA
> 9  <NA>     NA 6.98364332422208      1 <NA>     NA 2000-01-14      1
>   <NA>         NA
> 10 <NA>     NA 7.12950777181536      1 <NA>     NA 2000-01-15      1
>   <NA>         NA
> 11 <NA>     NA 7.28742905242849      1 <NA>     NA 2000-01-17      1
>   <NA>         NA
> 12 <NA>     NA  7.3757813516535      1 <NA>     NA 2000-01-18      1
>   <NA>         NA
> 13 <NA>     NA 7.53832470512922      1 <NA>     NA 2000-01-20      1
>   <NA>         NA
> 14 <NA>     NA 8.39528080213779      1 <NA>     NA 2000-01-21      1
>   <NA>         NA
> 15 <NA>     NA 10.6249309181431      1 <NA>     NA 2000-01-23      1
>   <NA>         NA
> 16 <NA>     NA 11.1550663909848      1 <NA>     NA 2000-01-24      1
>   <NA>         NA
> 17 <NA>     NA 11.3189773716082      1 <NA>     NA 2000-01-26      1
>   <NA>         NA
> 18 <NA>     NA 12.8838097369011      1 <NA>     NA 2000-01-27      1
>   <NA>         NA
> 19 <NA>     NA 15.5438362106853      1 <NA>     NA 2000-01-29      1
>   <NA>         NA
> 20 <NA>     NA 17.1212211950981      1 <NA>     NA 2000-01-30      1
>   <NA>         NA
> 21 <NA>     NA 17.8821363007311      1 <NA>     NA       <NA>     NA
>   <NA>         NA
> 22 <NA>     NA 18.5939013212175      1 <NA>     NA       <NA>     NA
>   <NA>         NA
>
>
> On Dec 1, 2007 8:05 AM, Lauri Nikkinen <lauri.nikkinen at iki.fi> wrote:
> > #Hi R-users,
> > #Suppose that I have a data.frame like this:
> >
> > y1 <- rnorm(10) + 6.8
> > y2 <- rnorm(10) + (1:10*1.7 + 1)
> > y3 <- rnorm(10) + (1:10*6.7 + 3.7)
> > y <- c(y1,y2,y3)
> > x <- rep(1:3,10)
> > f <- gl(2,15, labels=paste("lev", 1:2, sep=""))
> > g <- seq(as.Date("2000/1/1"), by="day", length=30)
> > DF <- data.frame(x=x,y=y, f=f, g=g)
> > DF$g[DF$x == 1] <- NA
> > DF$x[3:6] <- NA
> > DF$wdays <- weekdays(DF$g)
> >
> > DF
> >
> > #For EDA purposes, I would like to calculate frequences in each variable
> > g <- lapply(DF, function(x) as.data.frame(table(x)))
> >
> > #After this, I would like to cbind these data.frames (in g) into a
> > single data.frame (which to export to MS Excel)
> > #do.call(cbind, g) does not seem to work because of the different
> > number of rows in each data.frame.
> > #The resulting data.frame shoul look like this (only two variables
> > printed here):
> >
> > Rowid;x;Freq.x;y;Freq.y; # etc...
> > 1;1;9;1.69151845313816;1;
> > 2;2;9;5.03748767699799;1;
> > 3;3;8;5.37387749444247;1;
> > 4;Empty;Empty;6.83926626214299;1;
> > 5;Empty;Empty;6.97484558968873;1;
> > 6;Empty;Empty;7.11023821708323;1;
> > 7;Empty;Empty;7.1348316549091;1;
> > 8;Empty;Empty;7.16727166992407;1;
> > 9;Empty;Empty;7.35983428577469;1;
> > 10;Empty;Empty;7.7596470136235;1;
> > 11;Empty;Empty;7.86369414967578;1;
> > 12;Empty;Empty;7.97164674771006;1;
> > 13;Empty;Empty;8.0787295301318;1;
> > 14;Empty;Empty;8.14161030348166;1;
> > 15;Empty;Empty;8.20134832959661;1;
> > 16;Empty;Empty;10.1469115339016;1
> > 17;Empty;Empty;12.7442067301746;1
> > 18;Empty;Empty;14.0865167751202;1
> > 19;Empty;Empty;15.8280312307450;1
> > 20;Empty;Empty;16.0484499360756;1
> > 21;Empty;Empty;17.0795222149999;1
> > 22;Empty;Empty;18.1254057823357;1
> > 23;Empty;Empty;22.7169729331525;1
> > 24;Empty;Empty;30.7237748005358;1
> > 25;Empty;Empty;37.2141271786934;1
> > 26;Empty;Empty;44.4954633229803;1
> > 27;Empty;Empty;50.2302409305761;1
> > 28;Empty;Empty;57.8913405112114;1
> > 29;Empty;Empty;64.849897477945;1
> > 30;Empty;Empty;71.4205263353053;1
> >
> >
> > #Anyone have an idea how to do this?
> >
> > #Thanks,
> > #Lauri
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>


From dieter.menne at menne-biomed.de  Sat Dec  1 17:08:28 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 1 Dec 2007 16:08:28 +0000 (UTC)
Subject: [R] lmer and method call
References: <37787.70.71.25.135.1196388560.squirrel@www.cfenet.ubc.ca>
	<40e66e0b0712010726j599dc405ifac521481b651ee1@mail.gmail.com>
Message-ID: <loom.20071201T160227-587@post.gmane.org>

Douglas Bates <bates <at> stat.wisc.edu> writes:

(lmer)

> The default is PQL, to refine the
> starting estimates, followed by optimization of the Laplace
> approximation.  In some cases it is an advantage to suppress the PQL
> iterations which can be done with one of the settings for the control
> argument.

I had found out the hard way that it is often better to let PQL 
play the game rather loosely.  Yet I never dared to tell someone, for fear
the approximation could end up in the wrong slot,

Any rules (beside trying variants) if I can trust such a result?

Dieter


From b3i4old02 at sneakemail.com  Sat Dec  1 17:11:34 2007
From: b3i4old02 at sneakemail.com (Michael Hoffman)
Date: Sat, 01 Dec 2007 16:11:34 +0000
Subject: [R] Spellchecking Sweave documents
Message-ID: <fis13u$45q$1@ger.gmane.org>

I have been using Aspell on a Linux system, but it doesn't
understand the noweb chunks, which I'd rather it not spellcheck. I
can run it on the generated .tex files, but then changes I make
during the spellcheck will not be propagated back to the original
source. Any suggestions on how to spellcheck Sweave documents?

I see from a search that some people seem to be trying Flyspell on 
Emacs. I'd rather have a solution that runs outside of Emacs, but if 
anyone is using Flyspell successfully, I'd love to know of their 
experiences.


From b3i4old02 at sneakemail.com  Sat Dec  1 17:18:07 2007
From: b3i4old02 at sneakemail.com (Michael Hoffman)
Date: Sat, 01 Dec 2007 16:18:07 +0000
Subject: [R] Sweave: Variables in code chunk headers
Message-ID: <fis1g7$65a$1@ger.gmane.org>

I would like to be able to do something like this:

   <<echo=F,fig=T,width=mywidth>>=
   ...
   @

with mywidth set in a previous code chunk. Is there a way to do this in 
Sweave?

(Sorry for two questions in a row, I have been saving these up.)
-- 
Michael


From gginiu at gmail.com  Sat Dec  1 17:26:36 2007
From: gginiu at gmail.com (gginiu)
Date: Sat, 1 Dec 2007 17:26:36 +0100
Subject: [R] Problem with data editor when R built with --enable-mcfs
Message-ID: <f6d5b6a20712010826t760a3e08hf0b1d046c13251b2@mail.gmail.com>

Hi,

I encountered this problem when I was looking around in R, by default
R is built with --enable-mcfs and when I tried to edit any data I was
getting:

dataentry(datalist, modes) : invalid device
unable to create fontset -*-fixed-medium-r-normal--13-*-*-*-*-*-*-*

anyway I looked around code, noticed that it comes from fragment when
SUPPORT_MCFS is defined, and decided to try my luck without it,
rebuild with:

--disable-mfcs

and it worked like a charm, just doesn't have UTF-8 support now of
course what would be nice... am I missing something? I googled and
searched and found one similar message, but then it was stated it
works with current versions... my fonts in xorg.conf points to right
directories in my system and all other apps works. I run current
version of Arch linux

I would appreciate any help or hints how to solve that - whatever this
is bug or misconfiguration :)
thanks in advance,
Andrzej Giniewicz.


From bhh at xs4all.nl  Sat Dec  1 17:26:49 2007
From: bhh at xs4all.nl (Berend Hasselman)
Date: Sat, 1 Dec 2007 17:26:49 +0100
Subject: [R] finding roots (Max Like Est)
In-Reply-To: <14040895.post@talk.nabble.com>
References: <14040895.post@talk.nabble.com>
Message-ID: <6C000536-0553-4425-B19E-2EC94B8C42C3@xs4all.nl>


Why exactly the same question?

You were told what to do.
I am new to R. I did what the previous poster said.
I found it. Write your function in terms of vector operations. Avoid  
loops if you can.

Sample input for R follows
-------------------------

# par is the thing that has to be found
# x are the observations

f  <- function(par,x) sum(2*(x-par)/(1+(x-par)^2))

# trial stuff

x  <- 1:10

# use uniroot to find a value for par such that f(par,x) == 0
# interval for par is obvious (lower=1 and upper=10)

paropt  <- uniroot(f,c(1,10),tol=1e-8,x)

paropt
<END OF SAMPLEINPUT>

It works.

Berend Hasselman

On 30 Nov 2007, at 17:59, stathelp wrote:

>
> I have this maximum liklihood estimate problem
>
> i need to find the roots of the following:
>
> [sum (from i=1 to n) ] ((2(x[i]-parameter)/(1+(x[i]-parameter)^2))=0
>
> given to me is the x vector which has length 100
>
> how would I find the roots using R?
>
> I have 2 thoughts...... 1 is using a grid search ... eg. brute  
> force, just
> choosing a whole bunch of different values for my parameter ....  
> such as
> parameter=seq(0,100,.1) .... and this is what I have so far,
>
>
> 	john=rep(0,length(x))
> 	for(i in 1:length(x)) {
> 	john[i]=((x[i]-0)/(1+(x[i]-0)^2))
>             }
>             sum(john)
>
> then
>
> 	john=rep(0,length(x))
> 	for(i in 1:length(x)) {
> 	john[i]=((x[i]-.1)/(1+(x[i]-.1)^2))
>             }
>             sum(john)
>
> then
>
> 	john=rep(0,length(x))
> 	for(i in 1:length(x)) {
> 	john[i]=((x[i]-.2)/(1+(x[i]-.2)^2))
>             }
>             sum(john)
>
> something like this ...
>
>             theta=seq(0,100,.1)
> 	john=rep(0,length(x))
> 	for(i in 1:length(x)) {
> 	john[i]=((x[i]-theta[j])/(1+(x[i]-theta[j])^2))
>             }
>             sum(john)
>
> but something is wrong with my code because its not working. Anyone  
> have any
> ideas? (I am very new to R and statistical software in general)
>
> The 2nd thought was to use the Newton Raphson Method, but, I dont  
> even know
> where to start with that.
>
> Any thoughts help.
>
> Thanks
>
>
> -- 
> View this message in context: http://www.nabble.com/finding-roots-%28Max-Like-Est%29-tf4901659.html#a14040895
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mwkimpel at gmail.com  Sat Dec  1 17:28:28 2007
From: mwkimpel at gmail.com (Mark Kimpel)
Date: Sat, 1 Dec 2007 11:28:28 -0500
Subject: [R] Rating R Helpers
In-Reply-To: <B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
	<B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
Message-ID: <6b93d1830712010828l75857506gd2ee373a994ae456@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071201/ee07fed8/attachment.pl 

From jrkrideau at yahoo.ca  Sat Dec  1 17:33:12 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sat, 1 Dec 2007 11:33:12 -0500 (EST)
Subject: [R] R function for percentrank
In-Reply-To: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
Message-ID: <243517.68093.qm@web32807.mail.mud.yahoo.com>

I don't see one but that means nothing.   I think you
can write such a function in a few minutes

Will something like this work or am I
missunderstanding what Excel's percentrank does ?

aa <- rnorm(25);  aa  # data vector 
percentrank <- function(x) { 
var  <- sort(x)
p.rank <- 1:length(var)/length(var)*100
dd  <- cbind(var,p.rank)
}
pr <- percentrank(aa); pr


--- tom soyer <tom.soyer at gmail.com> wrote:

> Hi,
> 
> Does anyone know if R has a built-in function that
> is equvalent to Excel's
> percentrank, i.e., returns the rank of a value in a
> data set as a percentage
> of the data set?
> 
> Thanks,
> 
> -- 
> Tom
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From ggrothendieck at gmail.com  Sat Dec  1 17:59:33 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 1 Dec 2007 11:59:33 -0500
Subject: [R] Rating R Helpers
In-Reply-To: <6b93d1830712010828l75857506gd2ee373a994ae456@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
	<B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
	<6b93d1830712010828l75857506gd2ee373a994ae456@mail.gmail.com>
Message-ID: <971536df0712010859p58f7cd5fv9ffbfc094768a2a8@mail.gmail.com>

I don't think r-help is really intended for packages although for some
very popular packages questions appear on it anyways sometimes.

On Dec 1, 2007 11:28 AM, Mark Kimpel <mwkimpel at gmail.com> wrote:
> I'll throw one more idea into the mix. I agree with Bill that a rating
> system for respondents is probably not that practical and of not the highest
> importance. It also seems like a recipe for creating inter-personal problems
> that the list doesn't need.
>
> I do like Bill's idea of a review system for packages, which could be
> incorporated into my idea that follows...
>
> What I would find useful would be some sort of tagging system for messages.
> I can't count the times I've remembered seeing a message that addresses a
> question I have down the road but, when Googled, I can't find it. It would
> be so nice, for example, to reliably be able to find all messages related to
> a certain package or package function posted within the last X days. This
> could be implemented as simply as asking posters to provide keywords at the
> end of a message, but it would be great if they could somehow be pulled out
> of a message and stored in a DB. For instance keywords could be surrounded
> by a sequence of special characters, which a parser could then extract and
> store in a DB along with the message.
>
> Of course, this would be work to set up, but how many of our "experts" who
> so kindly give of their time, get exasperated when similar questions keep
> popping up on the list? Also, if we had a web-accessable DB, the responses,
> not the responders, could be rated as to how well a reply takes care of an
> issue. Thus, over time, a sort of auto-wiki could be born. I can think of
> more uses for this as well. For example a developer could quickly check to
> see what usability problems or suggestions have cropped up of on individual
> package.
>
> Mark
>
> On Dec 1, 2007 2:21 AM, <Bill.Venables at csiro.au> wrote:
>
>
> > This seems a little impractical to me.  People respond so much at random
> > and most only tackle questions with which they feel comfortable.  As
> > it's not a competition in any sense, it's going to be hard to rank
> > people in any effective way.  But suppose you succeed in doing so, then
> > what?
> >
> > To me a much more urgent initiative is some kind of user online review
> > system for packages, even something as simple as that used by Amazon.com
> > has for customer review of books.
> >
> > I think the need for this is rather urgent, in fact.  Most packages are
> > very good, but I regret to say some are pretty inefficient and others
> > downright dangerous.  You don't want to discourage people from
> > submitting their work to CRAN, but at the same time you do want some
> > mechanism that allows users to relate their experience with it, good or
> > bad.
> >
> >
> > Bill Venables
> > CSIRO Laboratories
> > PO Box 120, Cleveland, 4163
> > AUSTRALIA
> > Office Phone (email preferred): +61 7 3826 7251
> > Fax (if absolutely necessary):  +61 7 3826 7304
> > Mobile:                         +61 4 8819 4402
> > Home Phone:                     +61 7 3286 7700
> > mailto:Bill.Venables at csiro.au
> > http://www.cmis.csiro.au/bill.venables/
> >
> > -----Original Message-----
> > From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
> > On Behalf Of Doran, Harold
> > Sent: Saturday, 1 December 2007 6:13 AM
> > To: R Help
> > Subject: [R] Rating R Helpers
> >
> > Since R is open source and help may come from varied levels of
> > experience on R-Help, I wonder if it might be helpful to construct a
> > method that can be used to "rate" those who provide help on this list.
> >
> > This is something that is done on other comp lists, like
> > http://www.experts-exchange.com/.
> >
> > I think some of the reasons for this are pretty transparent, but I
> > suppose one reason is that one could decide to implement the advise of
> > those with "superior" or "expert" levels. In other words, you can trust
> > the advice of someone who is more experienced more than someone who is
> > not. Currently, there is no way to discern who on this list is really an
> > R expert and who is not. Of course, there is R core, but most people
> > don't actually know who these people are (at least I surmise that to be
> > true).
> >
> > If this is potentially useful, maybe one way to begin the development of
> > such ratings is to allow the original poster to "rate" the level of help
> > from those who responded. Maybe something like a very simple
> > questionnaire on a likert-like scale that the original poster would
> > respond to upon receiving help which would lead to the accumulation of
> > points for the responders. Higher points would result in higher levels
> > of expertise (e.g., novice, ..., wizaRd).
> >
> > Just a random thought. What do others think?
> >
> > Harold
> >
> >
> >
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> --
> Mark W. Kimpel MD
> Neuroinformatics
> Department of Psychiatry
> Indiana University School of Medicine
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dieter.menne at menne-biomed.de  Sat Dec  1 18:16:57 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 1 Dec 2007 17:16:57 +0000 (UTC)
Subject: [R] Sweave: Variables in code chunk headers
References: <fis1g7$65a$1@ger.gmane.org>
Message-ID: <loom.20071201T171543-701@post.gmane.org>

Michael Hoffman <b3i4old02 <at> sneakemail.com> writes:

> 
> I would like to be able to do something like this:
> 
>    <<echo=F,fig=T,width=mywidth>>=
>    ...
>    @
> 
> with mywidth set in a previous code chunk. Is there a way to do this in 
> Sweave?
> 

Not in the <<>>, but you could set a hook for fig:


>From Sweave docs:

If option "SweaveHooks" is defined as list(fig = foo), and foo is a function,
then it would be executed before the code in each figure chunk. This is
especially useful to set defaults for the graphical parameters in a series of
figure chunks.

Dieter


From tom.soyer at gmail.com  Sat Dec  1 18:51:31 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Sat, 1 Dec 2007 11:51:31 -0600
Subject: [R] R function for percentrank
In-Reply-To: <243517.68093.qm@web32807.mail.mud.yahoo.com>
References: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
	<243517.68093.qm@web32807.mail.mud.yahoo.com>
Message-ID: <65cc7bdf0712010951p451a993i70da89f285d801de@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071201/d46f4936/attachment.pl 

From bates at stat.wisc.edu  Sat Dec  1 18:59:23 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 1 Dec 2007 11:59:23 -0600
Subject: [R] lmer and method call
In-Reply-To: <loom.20071201T160227-587@post.gmane.org>
References: <37787.70.71.25.135.1196388560.squirrel@www.cfenet.ubc.ca>
	<40e66e0b0712010726j599dc405ifac521481b651ee1@mail.gmail.com>
	<loom.20071201T160227-587@post.gmane.org>
Message-ID: <40e66e0b0712010959j18ddf542x6798ab07c793e50b@mail.gmail.com>

On Dec 1, 2007 10:08 AM, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Douglas Bates <bates <at> stat.wisc.edu> writes:
>
> (lmer)
>
> > The default is PQL, to refine the
> > starting estimates, followed by optimization of the Laplace
> > approximation.  In some cases it is an advantage to suppress the PQL
> > iterations which can be done with one of the settings for the control
> > argument.
>
> I had found out the hard way that it is often better to let PQL
> play the game rather loosely.  Yet I never dared to tell someone, for fear
> the approximation could end up in the wrong slot,

> Any rules (beside trying variants) if I can trust such a result?

I'm not sure I understand the sense of your first statement.  Do you
mean that you have found that you should use PQL or you should not use
PQL?

I would advise using the Laplace approximation for the final
estimates.  At one time I thought it would be much slower than the PQL
iterations but it doesn't seem to be that bad.

I also thought that PQL would refine the starting estimates in the
sense that it would take comparatively crude starting values and get
you much closer to the optimum before you switched to Laplace.
However, because PQL is an algorithm that iterates on both the fixed
effects and the random effects with fixed weights, then updates the
weights, then goes back to the fixed effects and random effects, etc.
there is a possibility that the early weights can force poor values of
the fixed effects and later iterations do not recover.

I tend to prefer the Laplace approximation directly without any PQL
iterations.  That is

 method = "Laplace", control = list(usePQL = FALSE)

I would be interested in learning what experiences you or others have
had with the different approaches.

I am cc:ing this to the R-SIG-mixed-models list and suggest we switch
to that list only for further discussion.


From gsmkb_86 at hotmail.com  Sat Dec  1 18:59:01 2007
From: gsmkb_86 at hotmail.com (gsmkb86)
Date: Sat, 1 Dec 2007 09:59:01 -0800 (PST)
Subject: [R]  Need help on changing a table
Message-ID: <14107837.post@talk.nabble.com>


Hi all:
Im kind of new on R and I need help changing a table. The thing is, i read a
file on R using the read.table command and the table looks like this:
Item      3d Plot     XY plot    
001          1             0            
001          0             1            
001          0             1      
002          1             0
002          1             0
002          0             1
...            ..             ..
And what I want to do is generate a new table by item with the sum of the
numbres, the next one is an example:

Item     3d Plot       XY plot
001          1              2
002          2              1
003          ...            ...

Does anyone know how to do this? Thanks in advance, help is greatly
appreciated
-- 
View this message in context: http://www.nabble.com/Need-help-on-changing-a-table-tf4929014.html#a14107837
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Sat Dec  1 19:21:57 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 1 Dec 2007 18:21:57 +0000 (GMT)
Subject: [R] Need help on changing a table
In-Reply-To: <14107837.post@talk.nabble.com>
References: <14107837.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0712011815130.7277@gannet.stats.ox.ac.uk>

Note that read.table() reads a table and returns a _data frame_.  So it is 
the _data frame_ that you want to change (a table is something else in R). 
This is a simple application of aggregate(), e.g.

> aggregate(z[-1], list(Item=z$Item), sum)
   Item X3d_Plot XY_plot
1    1        1       2
2    2        2       1

where I got z from read.table("clipboard", header=TRUE).

On Sat, 1 Dec 2007, gsmkb86 wrote:

>
> Hi all:
> Im kind of new on R and I need help changing a table. The thing is, i read a
> file on R using the read.table command and the table looks like this:
> Item      3d Plot     XY plot
> 001          1             0
> 001          0             1
> 001          0             1
> 002          1             0
> 002          1             0
> 002          0             1
> ...            ..             ..
> And what I want to do is generate a new table by item with the sum of the
> numbres, the next one is an example:
>
> Item     3d Plot       XY plot
> 001          1              2
> 002          2              1
> 003          ...            ...
>
> Does anyone know how to do this? Thanks in advance, help is greatly
> appreciated
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dwinsemius at comcast.net  Sat Dec  1 19:32:10 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 1 Dec 2007 18:32:10 +0000 (UTC)
Subject: [R] R function for percentrank
References: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
	<243517.68093.qm@web32807.mail.mud.yahoo.com>
	<65cc7bdf0712010951p451a993i70da89f285d801de@mail.gmail.com>
Message-ID: <Xns99F989B3A3057dNOTwinscomcast@80.91.229.13>

"tom soyer" <tom.soyer at gmail.com> wrote in
news:65cc7bdf0712010951p451a993i70da89f285d801de at mail.gmail.com: 

> John,
> 
> The Excel's percentrank function works like this: if one has a number,
> x for example, and one wants to know the percentile of this number in
> a given data set, dataset, one would type =percentrank(dataset,x) in
> Excel to calculate the percentile. So for example, if the data set is
> c(1:10), and one wants to know the percentile of 2.5 in the data set,
> then using the percentrank function one would get 0.166, i.e., 2.5 is
> in the 16.6th percentile. 
> 
> I am not sure how to program this function in R. I couldn't find it as
> a built-in function in R either. It seems to be an obvious choice for
> a built-in function. I am very surprised, but maybe we both missed it.
 
My nomination for a function with a similar result would be ecdf(), the 
empirical cumulative distribution function. It is of class "function" so 
efforts to index ecdf(.)[.] failed for me.

> df4$V2
[1] 1 1 1 1 1 5 6 7 9
> ecdf.V2<-ecdf(df4$V2)
> ecdf.V2(df4$V2)
 [1] 0.2 0.2 0.4 0.4 0.5 0.6 0.7 0.8 1.0 0.9

Don't have Excel, but the OpenOffice.org Calc program has the same 
function. It produces:
x    percentrank(x)
1	0.0000000
1	0.0000000
3	0.2222222
3	0.2222222
4	0.4444444
5	0.5555556
6	0.6666667
7	0.7777778
10	1.0000000
9	0.8888889

(Not that I am saying that the OO.o/Excel function is what one _should_ 
want. Its behavior seems pathological to me.)

-- 
David Winsemius

> 
> On 12/1/07, John Kane <jrkrideau at yahoo.ca> wrote:
>>
>> I don't see one but that means nothing.   I think you
>> can write such a function in a few minutes
>>
>> Will something like this work or am I
>> missunderstanding what Excel's percentrank does ?
>>
>> aa <- rnorm(25);  aa  # data vector
>> percentrank <- function(x) {
>> var  <- sort(x)
>> p.rank <- 1:length(var)/length(var)*100
>> dd  <- cbind(var,p.rank)
>> }
>> pr <- percentrank(aa); pr
>>
>>
>> --- tom soyer <tom.soyer at gmail.com> wrote:
>>
>> > Hi,
>> >
>> > Does anyone know if R has a built-in function that
>> > is equvalent to Excel's
>> > percentrank, i.e., returns the rank of a value in a
>> > data set as a percentage
>> > of the data set?
>> >
>> > Thanks,
>> >


From dwinsemius at comcast.net  Sat Dec  1 19:40:14 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 1 Dec 2007 18:40:14 +0000 (UTC)
Subject: [R] R function for percentrank
References: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
	<243517.68093.qm@web32807.mail.mud.yahoo.com>
	<65cc7bdf0712010951p451a993i70da89f285d801de@mail.gmail.com>
	<Xns99F989B3A3057dNOTwinscomcast@80.91.229.13>
Message-ID: <Xns99F98B11F90B6dNOTwinscomcast@80.91.229.13>

David Winsemius <dwinsemius at comcast.net> wrote in 
news:Xns99F989B3A3057dNOTwinscomcast at 80.91.229.13:

> "tom soyer" <tom.soyer at gmail.com> wrote in
> news:65cc7bdf0712010951p451a993i70da89f285d801de at mail.gmail.com: 
> 
>> John,
>> 
>> The Excel's percentrank function works like this: if one has a number,
>> x for example, and one wants to know the percentile of this number in
>> a given data set, dataset, one would type =percentrank(dataset,x) in
>> Excel to calculate the percentile. So for example, if the data set is
>> c(1:10), and one wants to know the percentile of 2.5 in the data set,
>> then using the percentrank function one would get 0.166, i.e., 2.5 is
>> in the 16.6th percentile. 
>> 
>> I am not sure how to program this function in R. I couldn't find it as
>> a built-in function in R either. It seems to be an obvious choice for
>> a built-in function. I am very surprised, but maybe we both missed it.
>  
> My nomination for a function with a similar result would be ecdf(), the 
> empirical cumulative distribution function. It is of class "function" 
so 
> efforts to index ecdf(.)[.] failed for me.
> 
>> df4$V2
> [1] 1 1 1 1 1 5 6 7 9          #copied wrong line in R session

Make that;
df4$V2<-c(1,1,3,3,4,5,6,7,10,9)
[1] 1 1 3 3 5 6 7 9

>> ecdf.V2<-ecdf(df4$V2)
>> ecdf.V2(df4$V2)
>  [1] 0.2 0.2 0.4 0.4 0.5 0.6 0.7 0.8 1.0 0.9
> 
> Don't have Excel, but the OpenOffice.org Calc program has the same 
> function. It produces:
> x    percentrank(x)
> 1     0.0000000
> 1     0.0000000
> 3     0.2222222
> 3     0.2222222
> 4     0.4444444
> 5     0.5555556
> 6     0.6666667
> 7     0.7777778
> 10     1.0000000
> 9     0.8888889
> 
> (Not that I am saying that the OO.o/Excel function is what one _should_ 
> want. Its behavior seems pathological to me.)
>


From murdoch at stats.uwo.ca  Sat Dec  1 20:24:28 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 01 Dec 2007 14:24:28 -0500
Subject: [R] Spellchecking Sweave documents
In-Reply-To: <fis13u$45q$1@ger.gmane.org>
References: <fis13u$45q$1@ger.gmane.org>
Message-ID: <4751B4EC.7080305@stats.uwo.ca>

On 01/12/2007 11:11 AM, Michael Hoffman wrote:
> I have been using Aspell on a Linux system, but it doesn't
> understand the noweb chunks, which I'd rather it not spellcheck. I
> can run it on the generated .tex files, but then changes I make
> during the spellcheck will not be propagated back to the original
> source. Any suggestions on how to spellcheck Sweave documents?

If you want the concordance between the lines in the original file and 
those in the .tex file, you can get it (with the concordance=TRUE Sweave 
option).  Then it would be theoretically possible to convert reports 
about errors in the .tex into reports about the .Rnw.  I don't know if 
Aspell provides information in a form where you could actually make use 
of this, but if so, it might be a nice contribution to do so.

(For sample code decoding the concordance, see my patchDVI package:  it 
converts source links in .dvi files to point to the .Rnw instead of the 
.tex file.)

Duncan Murdoch

> 
> I see from a search that some people seem to be trying Flyspell on 
> Emacs. I'd rather have a solution that runs outside of Emacs, but if 
> anyone is using Flyspell successfully, I'd love to know of their 
> experiences.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Sat Dec  1 20:33:21 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 01 Dec 2007 13:33:21 -0600
Subject: [R] R function for percentrank
In-Reply-To: <Xns99F98B11F90B6dNOTwinscomcast@80.91.229.13>
References: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
	<243517.68093.qm@web32807.mail.mud.yahoo.com>
	<65cc7bdf0712010951p451a993i70da89f285d801de@mail.gmail.com>
	<Xns99F989B3A3057dNOTwinscomcast@80.91.229.13>
	<Xns99F98B11F90B6dNOTwinscomcast@80.91.229.13>
Message-ID: <1196537601.2980.15.camel@Bellerophon.localdomain>


On Sat, 2007-12-01 at 18:40 +0000, David Winsemius wrote:
> David Winsemius <dwinsemius at comcast.net> wrote in 
> news:Xns99F989B3A3057dNOTwinscomcast at 80.91.229.13:
> 
> > "tom soyer" <tom.soyer at gmail.com> wrote in
> > news:65cc7bdf0712010951p451a993i70da89f285d801de at mail.gmail.com: 
> > 
> >> John,
> >> 
> >> The Excel's percentrank function works like this: if one has a number,
> >> x for example, and one wants to know the percentile of this number in
> >> a given data set, dataset, one would type =percentrank(dataset,x) in
> >> Excel to calculate the percentile. So for example, if the data set is
> >> c(1:10), and one wants to know the percentile of 2.5 in the data set,
> >> then using the percentrank function one would get 0.166, i.e., 2.5 is
> >> in the 16.6th percentile. 
> >> 
> >> I am not sure how to program this function in R. I couldn't find it as
> >> a built-in function in R either. It seems to be an obvious choice for
> >> a built-in function. I am very surprised, but maybe we both missed it.
> >  
> > My nomination for a function with a similar result would be ecdf(), the 
> > empirical cumulative distribution function. It is of class "function" 
> so 
> > efforts to index ecdf(.)[.] failed for me.

You can use ls.str() to look into the function environment:

> ls.str(environment(ecdf(x)))
f :  num 0
method :  int 2
n :  int 25
x :  num [1:25] -2.215 -1.989 -0.836 -0.820 -0.626 ...
y :  num [1:25] 0.04 0.08 0.12 0.16 0.2 0.24 0.28 0.32 0.36 0.4 ...
yleft :  num 0
yright :  num 1



You can then use get() or mget() within the function environment to
return the requisite values. Something along the lines of the following
within the function percentrank():

percentrank <- function(x, val)
{
  env.x <- environment(ecdf(x))
  res <- mget(c("x", "y"), env.x)
  Ind <- which(sapply(seq(length(res$x)),
                      function(i) isTRUE(all.equal(res$x[i], val))))
  res$y[Ind]
}


Thus:

set.seed(1)
x <- rnorm(25)

> x
 [1] -0.62645381  0.18364332 -0.83562861  1.59528080  0.32950777
 [6] -0.82046838  0.48742905  0.73832471  0.57578135 -0.30538839
[11]  1.51178117  0.38984324 -0.62124058 -2.21469989  1.12493092
[16] -0.04493361 -0.01619026  0.94383621  0.82122120  0.59390132
[21]  0.91897737  0.78213630  0.07456498 -1.98935170  0.61982575


> percentrank(x, 0.48742905)
[1] 0.56


One other approach, which returns the values and their respective rank
percentiles is:

> cumsum(prop.table(table(x)))
   -2.2146998871775   -1.98935169586337  -0.835628612410047 
               0.04                0.08                0.12 
 -0.820468384118015  -0.626453810742333  -0.621240580541804 
               0.16                0.20                0.24 
 -0.305388387156356 -0.0449336090152308 -0.0161902630989461 
               0.28                0.32                0.36 
 0.0745649833651906   0.183643324222082   0.329507771815361 
               0.40                0.44                0.48 
  0.389843236411431   0.487429052428485   0.575781351653492 
               0.52                0.56                0.60 
  0.593901321217509    0.61982574789471   0.738324705129217 
               0.64                0.68                0.72 
  0.782136300731067   0.821221195098089   0.918977371608218 
               0.76                0.80                0.84 
    0.9438362106853    1.12493091814311    1.51178116845085 
               0.88                0.92                0.96 
   1.59528080213779 
               1.00 

HTH,

Marc Schwartz


From b3i4old02 at sneakemail.com  Sat Dec  1 20:38:21 2007
From: b3i4old02 at sneakemail.com (Michael Hoffman)
Date: Sat, 01 Dec 2007 19:38:21 +0000
Subject: [R] Sweave: Variables in code chunk headers
In-Reply-To: <loom.20071201T171543-701@post.gmane.org>
References: <fis1g7$65a$1@ger.gmane.org>
	<loom.20071201T171543-701@post.gmane.org>
Message-ID: <fisd7m$85f$1@ger.gmane.org>

Dieter Menne wrote:
> Michael Hoffman <b3i4old02 <at> sneakemail.com> writes:
> 
>> I would like to be able to do something like this:
>>
>>    <<echo=F,fig=T,width=mywidth>>=
>>    ...
>>    @
>>
>> with mywidth set in a previous code chunk. Is there a way to do this in 
>> Sweave?
>>
> 
> Not in the <<>>, but you could set a hook for fig:
> 
> 
>>From Sweave docs:
> 
> If option "SweaveHooks" is defined as list(fig = foo), and foo is a function,
> then it would be executed before the code in each figure chunk. This is
> especially useful to set defaults for the graphical parameters in a series of
> figure chunks.

Thanks. I guess what I really want to do is switch between one of two 
settings. Only one value can be in the defaults, and I would like some 
way of setting the other value. This might not be easily possible, but I 
thought I would ask.


From jrkrideau at yahoo.ca  Sat Dec  1 20:58:34 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sat, 1 Dec 2007 14:58:34 -0500 (EST)
Subject: [R] R function for percentrank
In-Reply-To: <Xns99F989B3A3057dNOTwinscomcast@80.91.229.13>
Message-ID: <75428.60872.qm@web32815.mail.mud.yahoo.com>


--- David Winsemius <dwinsemius at comcast.net> wrote:

> "tom soyer" <tom.soyer at gmail.com> wrote in
>
news:65cc7bdf0712010951p451a993i70da89f285d801de at mail.gmail.com:
> 
> 
> > John,
> > 
> > The Excel's percentrank function works like this:
> if one has a number,
> > x for example, and one wants to know the
> percentile of this number in
> > a given data set, dataset, one would type
> =percentrank(dataset,x) in
> > Excel to calculate the percentile. So for example,
> if the data set is
> > c(1:10), and one wants to know the percentile of
> 2.5 in the data set,
> > then using the percentrank function one would get
> 0.166, i.e., 2.5 is
> > in the 16.6th percentile. 
> > 
> > I am not sure how to program this function in R. I
> couldn't find it as
> > a built-in function in R either. It seems to be an
> obvious choice for
> > a built-in function. I am very surprised, but
> maybe we both missed it.
>  
> My nomination for a function with a similar result
> would be ecdf(), the 
> empirical cumulative distribution function. It is of
> class "function" so 
> efforts to index ecdf(.)[.] failed for me.
> 
> > df4$V2
> [1] 1 1 1 1 1 5 6 7 9
> > ecdf.V2<-ecdf(df4$V2)
> > ecdf.V2(df4$V2)
>  [1] 0.2 0.2 0.4 0.4 0.5 0.6 0.7 0.8 1.0 0.9
> 
> Don't have Excel, but the OpenOffice.org Calc
> program has the same 
> function. It produces:
> x    percentrank(x)
> 1	0.0000000
> 1	0.0000000
> 3	0.2222222
> 3	0.2222222
> 4	0.4444444
> 5	0.5555556
> 6	0.6666667
> 7	0.7777778
> 10	1.0000000
> 9	0.8888889
> 
> (Not that I am saying that the OO.o/Excel function
> is what one _should_ 
> want. Its behavior seems pathological to me.)
> 
Excel 
x  percentrank(x)
1	0
1	0
3	0.222
3	0.222
4	0.444
5	0.555
6	0.666
7	0.777
10	1
9	0.888

It seems that OOo is following Excel's distinguished
footsteps. 

How can one have a 0 percentile ranking? 


> -- 
> David Winsemius
> 
> > 
> > On 12/1/07, John Kane <jrkrideau at yahoo.ca> wrote:
> >>
> >> I don't see one but that means nothing.   I think
> you
> >> can write such a function in a few minutes
> >>
> >> Will something like this work or am I
> >> missunderstanding what Excel's percentrank does ?
> >>
> >> aa <- rnorm(25);  aa  # data vector
> >> percentrank <- function(x) {
> >> var  <- sort(x)
> >> p.rank <- 1:length(var)/length(var)*100
> >> dd  <- cbind(var,p.rank)
> >> }
> >> pr <- percentrank(aa); pr
> >>
> >>
> >> --- tom soyer <tom.soyer at gmail.com> wrote:
> >>
> >> > Hi,
> >> >
> >> > Does anyone know if R has a built-in function
> that
> >> > is equvalent to Excel's
> >> > percentrank, i.e., returns the rank of a value
> in a
> >> > data set as a percentage
> >> > of the data set?
> >> >
> >> > Thanks,
> >> >
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
> 



      Looking for the perfect gift? Give the gift of Flickr!


From ggrothendieck at gmail.com  Sat Dec  1 21:20:16 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 1 Dec 2007 15:20:16 -0500
Subject: [R] R function for percentrank
In-Reply-To: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
References: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
Message-ID: <971536df0712011220v47dceba3pcc46df1f4e618fff@mail.gmail.com>

Its a bit tricky if you want to get it to work exactly the same as
Excel even in the presence of runs but in terms of the R approx function
I think percentrank corresponds to ties = "min" if the value is among those
in the table and ties = "ordered" otherwise so:

percentrank <- function(table, x = table) {
   table <- sort(table)
   ties <- ifelse(match(x, table, nomatch = 0), "min", "ordered")
   len <- length(table)
   f <- function(x, ties)
      (approx(table, seq(0, len = len), x, ties = ties)$y) / (len - 1)
   mapply(f, x, ties)
}

# test
tab <- c(1, 1, 2, 2, 3, 3)
percentrank(tab, 2:6/2) # c(0, .3, .4, .7, .8)

which is the same result as Excel 2007 gives.

On Dec 1, 2007 6:37 AM, tom soyer <tom.soyer at gmail.com> wrote:
> Hi,
>
> Does anyone know if R has a built-in function that is equvalent to Excel's
> percentrank, i.e., returns the rank of a value in a data set as a percentage
> of the data set?
>
> Thanks,
>
> --
> Tom
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Sat Dec  1 21:36:17 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sat, 1 Dec 2007 15:36:17 -0500 (EST)
Subject: [R] Need help on changing a table
In-Reply-To: <14107837.post@talk.nabble.com>
Message-ID: <437280.10720.qm@web32814.mail.mud.yahoo.com>

That's no table that's a data.frame :). 

For what you want probably the easiest way is 
aggregate.  Type ?aggregate for the help information

Based on your example and assuming the data is called
"mydata" this shold do what you want.


aggregate(mydata[,2:3], by=list(item=mydata$Item),
sum)


--- gsmkb86 <gsmkb_86 at hotmail.com> wrote:

> 
> Hi all:
> Im kind of new on R and I need help changing a
> table. The thing is, i read a
> file on R using the read.table command and the table
> looks like this:
> Item      3d Plot     XY plot    
> 001          1             0            
> 001          0             1            
> 001          0             1      
> 002          1             0
> 002          1             0
> 002          0             1
> ...            ..             ..
> And what I want to do is generate a new table by
> item with the sum of the
> numbres, the next one is an example:
> 
> Item     3d Plot       XY plot
> 001          1              2
> 002          2              1
> 003          ...            ...
> 
> Does anyone know how to do this? Thanks in advance,
> help is greatly
> appreciated


From sheck at ucar.edu  Sat Dec  1 21:57:17 2007
From: sheck at ucar.edu (Sherri Heck)
Date: Sat, 01 Dec 2007 13:57:17 -0700
Subject: [R] creating conditional means
Message-ID: <4751CAAD.2040102@ucar.edu>

Hi all-

I have a dataset (year, month, hour, co2(ppm), num1,num2)


[49,] 2006   11    0 383.3709   28   28
[50,] 2006   11    1 383.3709   28   28
[51,] 2006   11    2 383.3709   28   28
[52,] 2006   11    3 383.3709   28   28
[53,] 2006   11    4 383.3709   28   28
[54,] 2006   11    5 383.3709   28   28
[55,] 2006   11    6 383.3709   28   28
[56,] 2006   11    7 383.3709   28   28
[57,] 2006   11    8 383.3709   28   28
[58,] 2006   11    9 383.3709   27   27
[59,] 2006   11   10 383.3709   28   28

that repeats in this style for each month.  I would like to compute the 
mean for each hour in three month intervals.
i.e.  average all 2pms for each day for months march, april and may. and 
then do this for each hour interval.
i have been messing around with 'for loops' but can't seem to get the 
output I want.

thanks in advance for any help-

s.heck
CU, Boulder


From johannes at huesing.name  Sat Dec  1 22:13:54 2007
From: johannes at huesing.name (Johannes Hsing)
Date: Sat, 1 Dec 2007 22:13:54 +0100
Subject: [R] Rating R Helpers
In-Reply-To: <6b93d1830712010828l75857506gd2ee373a994ae456@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
	<B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
	<6b93d1830712010828l75857506gd2ee373a994ae456@mail.gmail.com>
Message-ID: <20071201211353.GA6190@huesing.name>

Mark Kimpel <mwkimpel at gmail.com> [Sat, Dec 01, 2007 at 05:28:28PM CET]:
> What I would find useful would be some sort of tagging system for messages.

Hrm. I find tags immensely useful for entities which do not contain primarily
text, such as photos. I am at doubt how keywords are important when they are
not found in the text. There are situations where the first keyword that comes
to mind is tiptoed around in the message, but I don't know if this is often
the case.

> I can't count the times I've remembered seeing a message that addresses a
> question I have down the road but, when Googled, I can't find it. 

Is it a problem of way too many false positives or a problem of false negatives?
Tags may help out in the second case, but in my experiencd it is rare.

[...]
> 
> Of course, this would be work to set up, but how many of our "experts" who
> so kindly give of their time, get exasperated when similar questions keep
> popping up on the list? 

Do you think that people who keep asking similar questions do so because they
didn't do their homework first, or that they Googled and failed?

-- 
Johannes H?sing               There is something fascinating about science. 
                              One gets such wholesale returns of conjecture 
mailto:johannes at huesing.name  from such a trifling investment of fact.                
http://derwisch.wikidot.com         (Mark Twain, "Life on the Mississippi")


From pyliu8 at yahoo.com  Sat Dec  1 22:40:30 2007
From: pyliu8 at yahoo.com (Pengyuan Liu)
Date: Sat, 1 Dec 2007 13:40:30 -0800 (PST)
Subject: [R] Specify var-covar matrix in mixed linear model using lme?
Message-ID: <583792.43610.qm@web33202.mail.mud.yahoo.com>

Hi,All:
I have a question about specifying var-cov matrix in
mixed linear model using lme. For example, for
single-level mixed linear model:
yi = XiB + Ziui + ei
ui ~ N(0,D), ei ~ N(0,sigma^2R),
var(yi)=sigma^2(ZiAZi^T + R). R is an identify matrix
and A is a known var-covar matrix in my data.
In my data, there is only one random effect besides
ei. But this random effect is dependent among
different subjects within group (this dependence is
characterized in A which is known). corStruct class in
nlme can specify correlation structure for within
group errors (that is, specify R for ei). But I don't
know how to specify A for a specific random effect. In
other words, I want to fix matrix A in the analysis. 
Thanks a lot for your help.



Pengyuan Liu 
Dept of Surgery 
Washington Univ in St Louis


      ____________________________________________________________________________________
Be a better sports nut!  Let your teams follow you


From ggrothendieck at gmail.com  Sat Dec  1 22:44:46 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 1 Dec 2007 16:44:46 -0500
Subject: [R] creating conditional means
In-Reply-To: <4751CAAD.2040102@ucar.edu>
References: <4751CAAD.2040102@ucar.edu>
Message-ID: <971536df0712011344n5dc99ec0tbf542842edf5ee9@mail.gmail.com>

Try aggregate:


Lines <- "Year Month Hour co2 num1 num2
 2006   11    0 383.3709   28   28
 2006   11    1 383.3709   28   28
 2006   11    2 383.3709   28   28
 2006   11    3 383.3709   28   28
 2006   11    4 383.3709   28   28
 2006   11    5 383.3709   28   28
 2006   11    6 383.3709   28   28
 2006   11    7 383.3709   28   28
 2006   11    8 383.3709   28   28
 2006   11    9 383.3709   27   27
 2006   11   10 383.3709   28   28
"
DF <- read.table(textConnection(Lines), header = TRUE)
aggregate(DF[4:6],
   with(DF, data.frame(Year, Qtr = (Month - 1) %/% 3 + 1, Hour)),
   mean)

On Dec 1, 2007 3:57 PM, Sherri Heck <sheck at ucar.edu> wrote:
> Hi all-
>
> I have a dataset (year, month, hour, co2(ppm), num1,num2)
>
>
> [49,] 2006   11    0 383.3709   28   28
> [50,] 2006   11    1 383.3709   28   28
> [51,] 2006   11    2 383.3709   28   28
> [52,] 2006   11    3 383.3709   28   28
> [53,] 2006   11    4 383.3709   28   28
> [54,] 2006   11    5 383.3709   28   28
> [55,] 2006   11    6 383.3709   28   28
> [56,] 2006   11    7 383.3709   28   28
> [57,] 2006   11    8 383.3709   28   28
> [58,] 2006   11    9 383.3709   27   27
> [59,] 2006   11   10 383.3709   28   28
>
> that repeats in this style for each month.  I would like to compute the
> mean for each hour in three month intervals.
> i.e.  average all 2pms for each day for months march, april and may. and
> then do this for each hour interval.
> i have been messing around with 'for loops' but can't seem to get the
> output I want.
>
> thanks in advance for any help-
>
> s.heck
> CU, Boulder
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sheck at ucar.edu  Sat Dec  1 23:21:33 2007
From: sheck at ucar.edu (Sherri Heck)
Date: Sat, 01 Dec 2007 15:21:33 -0700
Subject: [R] creating conditional means
In-Reply-To: <971536df0712011344n5dc99ec0tbf542842edf5ee9@mail.gmail.com>
References: <4751CAAD.2040102@ucar.edu>
	<971536df0712011344n5dc99ec0tbf542842edf5ee9@mail.gmail.com>
Message-ID: <4751DE6D.4070605@ucar.edu>

Hi Gabor,

Thank you for your help.  I think I need to clarify a bit more.  I am 
trying to say

average all 2pms for months march + april + may (for example). I hope this is clearer.  

here's a larger subset of my data set:

year, month, hour, co2(ppm), num1,num2

2006 1 0 384.2055 14 14
2006 1 1 384.0304 14 14
2006 1 2 383.9672 14 14
2006 1 3 383.8452 14 14
2006 1 4 383.8594 14 14
2006 1 5 383.7318 14 14
2006 1 6 383.6439 14 14
2006 1 7 383.7019 14 14
2006 1 8 383.7487 14 14
2006 1 9 383.8376 14 14
2006 1 10 383.8684 14 14
2006 1 11 383.8301 14 14
2006 1 12 383.8058 14 14
2006 1 13 383.9419 14 14
2006 1 14 383.7876 14 14
2006 1 15 383.7744 14 14
2006 1 16 383.8566 14 14
2006 1 17 384.1014 14 14
2006 1 18 384.1312 14 14
2006 1 19 384.1551 14 14
2006 1 20 384.099 14 14
2006 1 21 384.1408 14 14
2006 1 22 384.3637 14 14
2006 1 23 384.1491 14 14
2006 2 0 384.7082 27 27
2006 2 1 384.6139 27 27
2006 2 2 384.7453 26 26
2006 2 3 384.9224 28 28
2006 2 4 384.8581 28 28
2006 2 5 384.9208 28 28
2006 2 6 384.9086 28 28
2006 2 7 384.837 28 28
2006 2 8 384.6163 27 27
2006 2 9 384.7406 28 28
2006 2 10 384.7468 28 28
2006 2 11 384.6992 28 28
2006 2 12 384.6388 28 28
2006 2 13 384.6346 28 28
2006 2 14 384.6037 28 28
2006 2 15 384.5295 28 28
2006 2 16 384.5654 28 28
2006 2 17 384.6466 28 28
2006 2 18 384.6344 28 28
2006 2 19 384.5911 28 28
2006 2 20 384.6084 28 28
2006 2 21 384.6318 28 28
2006 2 22 384.6181 27 27
2006 2 23 384.6087 27 27


thanks you again for your assistance-

s.heck


Gabor Grothendieck wrote:
> Try aggregate:
>
>
> Lines <- "Year Month Hour co2 num1 num2
>  2006   11    0 383.3709   28   28
>  2006   11    1 383.3709   28   28
>  2006   11    2 383.3709   28   28
>  2006   11    3 383.3709   28   28
>  2006   11    4 383.3709   28   28
>  2006   11    5 383.3709   28   28
>  2006   11    6 383.3709   28   28
>  2006   11    7 383.3709   28   28
>  2006   11    8 383.3709   28   28
>  2006   11    9 383.3709   27   27
>  2006   11   10 383.3709   28   28
> "
> DF <- read.table(textConnection(Lines), header = TRUE)
> aggregate(DF[4:6],
>    with(DF, data.frame(Year, Qtr = (Month - 1) %/% 3 + 1, Hour)),
>    mean)
>
> On Dec 1, 2007 3:57 PM, Sherri Heck <sheck at ucar.edu> wrote:
>   
>> Hi all-
>>
>> I have a dataset (year, month, hour, co2(ppm), num1,num2)
>>
>>
>> [49,] 2006   11    0 383.3709   28   28
>> [50,] 2006   11    1 383.3709   28   28
>> [51,] 2006   11    2 383.3709   28   28
>> [52,] 2006   11    3 383.3709   28   28
>> [53,] 2006   11    4 383.3709   28   28
>> [54,] 2006   11    5 383.3709   28   28
>> [55,] 2006   11    6 383.3709   28   28
>> [56,] 2006   11    7 383.3709   28   28
>> [57,] 2006   11    8 383.3709   28   28
>> [58,] 2006   11    9 383.3709   27   27
>> [59,] 2006   11   10 383.3709   28   28
>>
>> that repeats in this style for each month.  I would like to compute the
>> mean for each hour in three month intervals.
>> i.e.  average all 2pms for each day for months march, april and may. and
>> then do this for each hour interval.
>> i have been messing around with 'for loops' but can't seem to get the
>> output I want.
>>
>> thanks in advance for any help-
>>
>> s.heck
>> CU, Boulder
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>


From ggrothendieck at gmail.com  Sun Dec  2 01:21:45 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 1 Dec 2007 19:21:45 -0500
Subject: [R] creating conditional means
In-Reply-To: <4751DE6D.4070605@ucar.edu>
References: <4751CAAD.2040102@ucar.edu>
	<971536df0712011344n5dc99ec0tbf542842edf5ee9@mail.gmail.com>
	<4751DE6D.4070605@ucar.edu>
Message-ID: <971536df0712011621n12b19eaeidab396f7729ac891@mail.gmail.com>

Just adjust the formula for Qtr appropriately if your quarters
are not Jan/Feb/Mar, Apr/May/Jun, Jul/Aug/Sep, Oct/Nov/Dec
as I assumed.

On Dec 1, 2007 5:21 PM, Sherri Heck <sheck at ucar.edu> wrote:
> Hi Gabor,
>
> Thank you for your help.  I think I need to clarify a bit more.  I am
> trying to say
>
> average all 2pms for months march + april + may (for example). I hope this is clearer.
>
> here's a larger subset of my data set:
>
> year, month, hour, co2(ppm), num1,num2
>
> 2006 1 0 384.2055 14 14
> 2006 1 1 384.0304 14 14
> 2006 1 2 383.9672 14 14
> 2006 1 3 383.8452 14 14
> 2006 1 4 383.8594 14 14
> 2006 1 5 383.7318 14 14
> 2006 1 6 383.6439 14 14
> 2006 1 7 383.7019 14 14
> 2006 1 8 383.7487 14 14
> 2006 1 9 383.8376 14 14
> 2006 1 10 383.8684 14 14
> 2006 1 11 383.8301 14 14
> 2006 1 12 383.8058 14 14
> 2006 1 13 383.9419 14 14
> 2006 1 14 383.7876 14 14
> 2006 1 15 383.7744 14 14
> 2006 1 16 383.8566 14 14
> 2006 1 17 384.1014 14 14
> 2006 1 18 384.1312 14 14
> 2006 1 19 384.1551 14 14
> 2006 1 20 384.099 14 14
> 2006 1 21 384.1408 14 14
> 2006 1 22 384.3637 14 14
> 2006 1 23 384.1491 14 14
> 2006 2 0 384.7082 27 27
> 2006 2 1 384.6139 27 27
> 2006 2 2 384.7453 26 26
> 2006 2 3 384.9224 28 28
> 2006 2 4 384.8581 28 28
> 2006 2 5 384.9208 28 28
> 2006 2 6 384.9086 28 28
> 2006 2 7 384.837 28 28
> 2006 2 8 384.6163 27 27
> 2006 2 9 384.7406 28 28
> 2006 2 10 384.7468 28 28
> 2006 2 11 384.6992 28 28
> 2006 2 12 384.6388 28 28
> 2006 2 13 384.6346 28 28
> 2006 2 14 384.6037 28 28
> 2006 2 15 384.5295 28 28
> 2006 2 16 384.5654 28 28
> 2006 2 17 384.6466 28 28
> 2006 2 18 384.6344 28 28
> 2006 2 19 384.5911 28 28
> 2006 2 20 384.6084 28 28
> 2006 2 21 384.6318 28 28
> 2006 2 22 384.6181 27 27
> 2006 2 23 384.6087 27 27
>
>
> thanks you again for your assistance-
>
> s.heck
>
>
>
> Gabor Grothendieck wrote:
> > Try aggregate:
> >
> >
> > Lines <- "Year Month Hour co2 num1 num2
> >  2006   11    0 383.3709   28   28
> >  2006   11    1 383.3709   28   28
> >  2006   11    2 383.3709   28   28
> >  2006   11    3 383.3709   28   28
> >  2006   11    4 383.3709   28   28
> >  2006   11    5 383.3709   28   28
> >  2006   11    6 383.3709   28   28
> >  2006   11    7 383.3709   28   28
> >  2006   11    8 383.3709   28   28
> >  2006   11    9 383.3709   27   27
> >  2006   11   10 383.3709   28   28
> > "
> > DF <- read.table(textConnection(Lines), header = TRUE)
> > aggregate(DF[4:6],
> >    with(DF, data.frame(Year, Qtr = (Month - 1) %/% 3 + 1, Hour)),
> >    mean)
> >
> > On Dec 1, 2007 3:57 PM, Sherri Heck <sheck at ucar.edu> wrote:
> >
> >> Hi all-
> >>
> >> I have a dataset (year, month, hour, co2(ppm), num1,num2)
> >>
> >>
> >> [49,] 2006   11    0 383.3709   28   28
> >> [50,] 2006   11    1 383.3709   28   28
> >> [51,] 2006   11    2 383.3709   28   28
> >> [52,] 2006   11    3 383.3709   28   28
> >> [53,] 2006   11    4 383.3709   28   28
> >> [54,] 2006   11    5 383.3709   28   28
> >> [55,] 2006   11    6 383.3709   28   28
> >> [56,] 2006   11    7 383.3709   28   28
> >> [57,] 2006   11    8 383.3709   28   28
> >> [58,] 2006   11    9 383.3709   27   27
> >> [59,] 2006   11   10 383.3709   28   28
> >>
> >> that repeats in this style for each month.  I would like to compute the
> >> mean for each hour in three month intervals.
> >> i.e.  average all 2pms for each day for months march, april and may. and
> >> then do this for each hour interval.
> >> i have been messing around with 'for loops' but can't seem to get the
> >> output I want.
> >>
> >> thanks in advance for any help-
> >>
> >> s.heck
> >> CU, Boulder
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
>


From taekyunk at gmail.com  Sun Dec  2 03:16:09 2007
From: taekyunk at gmail.com (T.K.)
Date: Sat, 1 Dec 2007 18:16:09 -0800
Subject: [R] Help with tables
In-Reply-To: <000a01c8337c$498a2290$327412ac@FARMER0001>
References: <000a01c8337c$498a2290$327412ac@FARMER0001>
Message-ID: <36923f1d0712011816n7b47574dl59b99f116cfa5ba8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071201/40524168/attachment.pl 

From jim at bitwrit.com.au  Sun Dec  2 12:16:28 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sun, 02 Dec 2007 22:16:28 +1100
Subject: [R] using color coded colorbars for bar plots
In-Reply-To: <A710A72A775FA24B8B6943719F3DE1B0520914@extas4-hba.tas.csiro.au>
References: <A710A72A775FA24B8B6943719F3DE1B052090C@extas4-hba.tas.csiro.au>
	<474FF1E1.1050207@bitwrit.com.au>
	<A710A72A775FA24B8B6943719F3DE1B0520914@extas4-hba.tas.csiro.au>
Message-ID: <4752940C.3040008@bitwrit.com.au>

James.Dell at csiro.au wrote:
> Hi Jim,
> Thanks for getting back to me so quickly.
> 
> I did look at color.legend, but that seems to plot colored blocks for
> the observations (in this case the mean) and not for the color.scale
> (which represents variance in this case).  Unless there is a
> functionality that I haven't discovered yet.  If you have created a
> similar plot and would be happy to share some code I'd be very
> apprecitive.
> 
Part of the problem is that you seem to have two names for the same 
variable in your code (Standard.Deviance and Standard.Deviation - unless 
that was a typo). Notice how I calculate the colors twice, the second 
time with a simple integer sequence to get the right number of evenly 
spaced colors. In your example, you calculated the colors for 
RankVar$Standard.Deviance again, but you don't need all those colors for 
the legend, and they're in the wrong order anyway. What is generally 
wanted for a color legend is the minimum and maximum values on the ends 
and a few linear interpolations in the middle.

barplot(RankVar$MeanDecreaseAccuracy,
  col=color.scale(RankVar$Standard.Deviance,
  c(0,1,1),c(1,1,0),0),
  ylab = "Variable Importance",
  names.arg = rownames(RankVar),
  cex.names = .7,
  main = "Variables from RandomFishForest",
  sub= "Mean Decrease in Accuracy")
col.labels<- c("Low","Mid","High")
color.legend(6,13,11,14,col.labels,
  rect.col=color.scale(1:5,c(0,1,1),c(1,1,0),0))

Jim


From jrkrideau at yahoo.ca  Sun Dec  2 17:42:36 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 2 Dec 2007 11:42:36 -0500 (EST)
Subject: [R] R function for percentrank
In-Reply-To: <971536df0712011220v47dceba3pcc46df1f4e618fff@mail.gmail.com>
Message-ID: <891470.5000.qm@web32805.mail.mud.yahoo.com>

Ah so ! Thank you .
--- Gabor Grothendieck <ggrothendieck at gmail.com>
wrote:

> Its a bit tricky if you want to get it to work
> exactly the same as
> Excel even in the presence of runs but in terms of
> the R approx function
> I think percentrank corresponds to ties = "min" if
> the value is among those
> in the table and ties = "ordered" otherwise so:
> 
> percentrank <- function(table, x = table) {
>    table <- sort(table)
>    ties <- ifelse(match(x, table, nomatch = 0),
> "min", "ordered")
>    len <- length(table)
>    f <- function(x, ties)
>       (approx(table, seq(0, len = len), x, ties =
> ties)$y) / (len - 1)
>    mapply(f, x, ties)
> }
> 
> # test
> tab <- c(1, 1, 2, 2, 3, 3)
> percentrank(tab, 2:6/2) # c(0, .3, .4, .7, .8)
> 
> which is the same result as Excel 2007 gives.
> 
> On Dec 1, 2007 6:37 AM, tom soyer
> <tom.soyer at gmail.com> wrote:
> > Hi,
> >
> > Does anyone know if R has a built-in function that
> is equvalent to Excel's
> > percentrank, i.e., returns the rank of a value in
> a data set as a percentage
> > of the data set?
> >
> > Thanks,
> >
> > --
> > Tom
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From mcintosh at research.telcordia.com  Sun Dec  2 17:49:41 2007
From: mcintosh at research.telcordia.com (Allen McIntosh)
Date: Sun, 2 Dec 2007 11:49:41 -0500
Subject: [R] array() misfeature
Message-ID: <200712021649.lB2GnfCx030277@mc-pc.research.telcordia.com>

Version: 2.5.1

array() is inconsistent when given non-integral dimensions:

> zz <- array(0,dim=c(4,3.01))
> dim(zz)
[1] 4 3
> zz <- array(0,dim=c(201,4.05))
Error in dim(data) <- dim : dim<- : dims [product 804] do not match the length of object [814]


[IMHO the code that did this is broken.  My copy of it has been
fixed.  Consistent behavior and/or a clearer error message would
just have made it easier to find the problem.]


From alfieim21 at hotmail.co.uk  Sun Dec  2 18:43:43 2007
From: alfieim21 at hotmail.co.uk (jimbib webber)
Date: Sun, 2 Dec 2007 17:43:43 +0000
Subject: [R] Please help!
In-Reply-To: <644e1f320712011945x4495de41wbf94a74863ddea00@mail.gmail.com>
References: <14086120.post@talk.nabble.com>
	<BLU108-W3907FA55DD407F04CA15DEF2720@phx.gbl>
	<644e1f320712010701s914060dg3558b94043203ada@mail.gmail.com>
	<BLU108-W30003C1C359962848525EF2720@phx.gbl>
	<644e1f320712010803i4eaff8fbpcaa049f0f79c4d92@mail.gmail.com>
	<BLU108-W2A8A9277FC558D1FED1C6F2720@phx.gbl>
	<644e1f320712010831i7446cc07gd57ac061ed997d93@mail.gmail.com>
	<BLU108-W2625552B3523F22AD5D2BEF2720@phx.gbl>
	<644e1f320712010840x1b197f08s14430cf8d1dbe409@mail.gmail.com>
	<BLU108-W1004C9A583B1779AAFE909F2730@phx.gbl> 
	<644e1f320712011945x4495de41wbf94a74863ddea00@mail.gmail.com>
Message-ID: <BLU108-W44A9C8E34EC3B26B0D73C1F2730@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071202/0385252e/attachment.pl 

From basu.15 at osu.edu  Sun Dec  2 18:49:34 2007
From: basu.15 at osu.edu (DEEPANKAR BASU)
Date: Sun, 02 Dec 2007 12:49:34 -0500
Subject: [R] speeding up likelihood computation
Message-ID: <110f9d445.d445110f9@osu.edu>

R Users:

I am trying to estimate a model of fertility behaviour using birth history data with maximum likelihood. My code works but is extremely slow (because of several for loops and my programming inefficiencies); when I use the genetic algorithm to optimize the likelihood function, it takes several days to complete (on a machine with Intel Core 2 processor [2.66GHz] and 2.99 GB RAM). Computing the hessian and using it to calculate the standard errors takes a large chunk of this time. 

I am copying the code for my likelihood function below; it would be great if someone could suggest any method to speed up the code (by avoiding the for loops or by any other method).

I am not providing details of my model or what exactly I am trying to do in each step of the computation below; i would be happy to provide these details if they are deemed necessary for re-working the code.

Thanks.
Deepankar


--------- begin code -----------------------

LLK1 <- function(paramets, data.frame, ...) {  # DEFINING THE LOGLIKELIHOOD FUNCTION

# paramets IS A 1x27 VECTOR OF PARAMETERS OVER WHICH THE FUNCTION WILL BE MAXIMISED
# data.frame IS A DATA FRAME. THE DATA FRAME CONTAINS OBSERVATIONS ON SEVERAL VARIABLES
# (LIKE EDUCATION, AGE, ETC.) FOR EACH RESPONDENT. COLUMNS REFER TO VARIABLES AND ROWS REFER
# TO OBSERVATIONS.

########## PARAMETERS ###############################

# alpha: interaction between son targeting and family size
# beta : son targeting
# gamma : family size
# delta : a 1x6 vector of probabilities of male birth at various parities (q1, q2, q3, q4, q5, q6)
# zeta : a 1x11 vector of conditional probabilities with zeta[1]=1 always

alpha <- paramets[1]      # FIRST PARAMETER
beta <- paramets[2:9]     # SECOND TO SEVENTH PARAMETER
gamma <- paramets[10:16]
delta <- paramets[17]
zeta <- paramets[18:27]   # LAST 10 PARAMETERS

################# VARIABLES ###############################
# READING IN THE VARIABLES IN THE DATA FRAME
# AND RENAMING THEM FOR USE IN THE LIKELIHOOD FUNCTION

everborn <- data.frame$v201
alive <- data.frame$alive
age <- data.frame$age
edu <- data.frame$edu
rural <- data.frame$rur
rich <- data.frame$rich
middle <- data.frame$middle
poor <- data.frame$poor
work <- data.frame$work
jointfam <- data.frame$jfam
contracep <- data.frame$contra
hindu <- data.frame$hindu
muslim <- data.frame$muslim
scaste <- data.frame$scaste
stribe <- data.frame$stribe
obc <- data.frame$obc
ucaste <- data.frame$ucaste
N <- data.frame$dfsize
indN <- data.frame$dfsize1  # INDICATOR FUNCTION THAT dfsize==alive
nb <- data.frame$nboy
ng <- data.frame$ngirl
ncord1 <- data.frame$ncord1  # FIRST CHILD: BOY=0; GIRL=1  
ncord2 <- data.frame$ncord2  #SECOND CHILD: BOY=0; GIRL=1
ncord3 <- data.frame$ncord3
ncord4 <- data.frame$ncord4
ncord5 <- data.frame$ncord5
ncord6 <- data.frame$ncord6  # SIXTH CHILD: BOY=0; GIRL=1



######### POSITION OF i^th BOY ################################################
boy1 <- data.frame$boy1     # BIRTH POSITION OF FIRST BOY (ZERO IF THE FAMILY HAS NO BOYS)
boy2 <- data.frame$boy2     # BIRTH POSITION OF SECOND BOY (ZERO IF THE FAMILY HAS ONLY ONE BOY)
boy3 <- data.frame$boy3
boy4 <- data.frame$boy4
boy5 <- data.frame$boy5
boy6 <- data.frame$boy6     # BIRTH POSITION OF SIXTH BOY (ZERO IF THE FAMILY HAS ONLY FIVE BOYS)


######################## CONDITIONAL PROBABILITIES ##########################
qq21 <- 1

qq31 <- 1/(1+exp(zeta[1]))
qq32 <- exp(zeta[1])/(1+exp(zeta[1]))

qq41 <- 1/(1+exp(zeta[2])+exp(zeta[3]))
qq42 <- exp(zeta[2])/(1+exp(zeta[2])+exp(zeta[3]))
qq43 <- exp(zeta[3])/(1+exp(zeta[2])+exp(zeta[3]))

qq51 <- 1/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
qq52 <- exp(zeta[4])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
qq53 <- exp(zeta[5])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
qq54 <- exp(zeta[6])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))

qq61 <- 1/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
qq62 <- exp(zeta[7])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
qq63 <- exp(zeta[8])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
qq64 <- exp(zeta[9])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
qq65 <- exp(zeta[10])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))

zeta1 <- c(qq21,qq31,qq32,qq41,qq42,qq43,qq51,qq52,qq53,qq54,qq61,qq62,qq63,qq64,qq65)

#########################################################################

n <- length(N)         # LENGTH OF SAMPLE; SIZE OF THE MAIN LOOP

lglik <- numeric(n)    # INITIALIZING THE LOGLIKELIHOOD FUNCTION
                       # CREATES A 1xn VECTOR OF ZEROS

 for (j in 1:n) {      # START OF MAIN LOOP

    S <- matrix(0, 6, 6)  # CREATE A 6x6 MATRIX OF ZEROS
    y <- numeric(15)      # CREATE A 1x15 VECTOR OF ZEROS  
    N1 <- N[j]       # DESIRED FAMILY SIZE

      
      q <- 1/(1+exp(delta))   # PROBABILITY OF MALE BIRTH
      

    if (alive[j]==N1) {
 
	for (i in 1:(N1-1)) {
		S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))  
	}
    }

    else {
	for (i in 1:(N1-1)) {
		S[N1,i] <- 0
	}

    }	

######### CREATE A 1x6 VECTOR WITH POSITION OF BOYS WITHIN FAMILY
      x <- c(boy1[j], boy2[j], boy3[j], boy4[j], boy5[j]) 

      if (N1>1) {    
                     for (i in 1:(N1-1)) {
     		               if (alive[j]>x[i] & x[i]>0) { 
        	                   S[N1,i] <- 0
        	               } 
     		               if (x[i] == alive[j] ) { 
        	                   S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
        	               }
                     }
      }

   y <- c(S[2,1],S[3,1],S[3,2],S[4,1],S[4,2],S[4,3],S[5,1],S[5,2],S[5,3],S[5,4],S[6,1],S[6,2],S[6,3],S[6,4],S[6,5])

   
   z1 <- c(age[j],edu[j],work[j],rural[j],poor[j],middle[j],hindu[j])         # DETERMINANTS OF FAMILY SIZE
   z2 <- c(1,age[j],edu[j],work[j],contracep[j],rural[j],jointfam[j],hindu[j])  # DETERMINANTS OF SON TARGETING

   t1 <- (indN[j])*((q^(nb[j]))*((1-q)^(ng[j])))*(exp(-exp(sum(z1*gamma)))*((exp(sum(z1*gamma)))^N1)*pnorm(-sum(z2*beta)))/factorial(N1)
   t2 <- (sum(y*zeta1))*(exp(-exp(sum(z1*gamma) + alpha))*((exp(sum(z1*gamma) + alpha))^N1)*(1-pnorm(-sum(z2*beta)))/factorial(N1))
   lglik[j] <- log(t1+t2)
 }

 return(-sum(lglik)) # RETURNING THE NEGATIVE OF THE LOGLIKELIHOOD
                     # SUMMED OVER ALL OBSERVATIONS


} 

------------ end code ----------------------


From lauri.nikkinen at iki.fi  Sun Dec  2 18:53:55 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Sun, 2 Dec 2007 19:53:55 +0200
Subject: [R] How to recode a factor level (within the list)?
Message-ID: <ba8c09910712020953p5889a0eak66bcc2ea24d1d661@mail.gmail.com>

#Dear R-users,
#I have a data.frame like this:

y1 <- rnorm(10) + 6.8
y2 <- rnorm(10) + (1:10*1.7 + 1)
y3 <- rnorm(10) + (1:10*6.7 + 3.7)
y <- c(y1,y2,y3)
x <- rep(1:3,10)
f <- gl(2,15, labels=paste("lev", 1:2, sep=""))
g <- seq(as.Date("2000/1/1"), by="day", length=30)
DF <- data.frame(x=x,y=y, f=f, g=g)
DF$g[DF$x == 1] <- NA
DF$x[3:6] <- NA
DF$wdays <- weekdays(DF$g)

DF

#Frequences
g <- lapply(DF, function(x) as.data.frame(table(format(x))))
g

#NA:s are now part of factor levels. How to recode NA:s into e.g. "missing"?

#Thanks
#Lauri


From murdoch at stats.uwo.ca  Sun Dec  2 19:02:52 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 02 Dec 2007 13:02:52 -0500
Subject: [R] array() misfeature
In-Reply-To: <200712021649.lB2GnfCx030277@mc-pc.research.telcordia.com>
References: <200712021649.lB2GnfCx030277@mc-pc.research.telcordia.com>
Message-ID: <4752F34C.9080609@stats.uwo.ca>

On 02/12/2007 11:49 AM, Allen McIntosh wrote:
> Version: 2.5.1

That's an obsolete version, but the issue is still present in R-devel.

> array() is inconsistent when given non-integral dimensions:
> 
>> zz <- array(0,dim=c(4,3.01))
>> dim(zz)
> [1] 4 3
>> zz <- array(0,dim=c(201,4.05))
> Error in dim(data) <- dim : dim<- : dims [product 804] do not match the length of object [814]
> 
> 
> [IMHO the code that did this is broken.  My copy of it has been
> fixed.  Consistent behavior and/or a clearer error message would
> just have made it easier to find the problem.]
> 

I'll add code to coerce dim to integer before using it.  Thanks for the 
report.

Duncan Murdoch


From bolker at ufl.edu  Sun Dec  2 19:31:09 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Sun, 2 Dec 2007 10:31:09 -0800 (PST)
Subject: [R] array() misfeature
In-Reply-To: <4752F34C.9080609@stats.uwo.ca>
References: <200712021649.lB2GnfCx030277@mc-pc.research.telcordia.com>
	<4752F34C.9080609@stats.uwo.ca>
Message-ID: <14118356.post@talk.nabble.com>




Duncan Murdoch-2 wrote:
> 
> On 02/12/2007 11:49 AM, Allen McIntosh wrote:
>> Version: 2.5.1
> 
> That's an obsolete version, but the issue is still present in R-devel.
> 
>> array() is inconsistent when given non-integral dimensions:
>> 
>>> zz <- array(0,dim=c(4,3.01))
>>> dim(zz)
>> [1] 4 3
>>> zz <- array(0,dim=c(201,4.05))
>> Error in dim(data) <- dim : dim<- : dims [product 804] do not match the
>> length of object [814]
>> 
>> 
>> [IMHO the code that did this is broken.  My copy of it has been
>> fixed.  Consistent behavior and/or a clearer error message would
>> just have made it easier to find the problem.]
>> 
> 
> I'll add code to coerce dim to integer before using it.  Thanks for the 
> report.
> 
> Duncan Murdoch
> 
> 

  While you're at it could you fix a trivial typo in ?dim   ?

  in the description of the value argument: "... or a numeric
vector which [is] coerced ..."
 (I would also say that either there should be a comma after
"vector" or "which" should be changed to "that", but never mind ...)

  Ben

-- 
View this message in context: http://www.nabble.com/array%28%29-misfeature-tf4932338.html#a14118356
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Sun Dec  2 19:59:36 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 2 Dec 2007 18:59:36 +0000 (GMT)
Subject: [R] How to recode a factor level (within the list)?
In-Reply-To: <ba8c09910712020953p5889a0eak66bcc2ea24d1d661@mail.gmail.com>
References: <ba8c09910712020953p5889a0eak66bcc2ea24d1d661@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0712021835180.22480@gannet.stats.ox.ac.uk>

On Sun, 2 Dec 2007, Lauri Nikkinen wrote:

> #Dear R-users,
> #I have a data.frame like this:
>
> y1 <- rnorm(10) + 6.8
> y2 <- rnorm(10) + (1:10*1.7 + 1)
> y3 <- rnorm(10) + (1:10*6.7 + 3.7)
> y <- c(y1,y2,y3)
> x <- rep(1:3,10)
> f <- gl(2,15, labels=paste("lev", 1:2, sep=""))
> g <- seq(as.Date("2000/1/1"), by="day", length=30)
> DF <- data.frame(x=x,y=y, f=f, g=g)
> DF$g[DF$x == 1] <- NA
> DF$x[3:6] <- NA
> DF$wdays <- weekdays(DF$g)
>
> DF
>
> #Frequences
> g <- lapply(DF, function(x) as.data.frame(table(format(x))))
> g
>
> #NA:s are now part of factor levels. How to recode NA:s into e.g. "missing"?

Not so:

> sapply(DF, class)
           x           y           f           g       wdays
   "integer"   "numeric"    "factor"      "Date" "character"

and DF$f does not have any NA levels.

The place you may think you have got NAs is in format(wdays):
they are not NA nor "NA" but "NA     ".  I am not sure what exactly you 
want (NA is not appearing in the tables: see the 'exclude' argument),
but perhaps

lapply(DF, function(x) {
   if(is.character(x)) x[is.na(x)] <- "missing"
   as.data.frame(table(format(x)))
})

or

lapply(DF, function(x) {
   z <- table(format(x))
   names(z)[grep("^NA", names(z))] <- "missing"
   as.data.frame(z)
})

or

lapply(DF, function(x) {
   z <- table(x, exclude=character(0))
   names(z)[is.na(names(z))] <- "missing"
   as.data.frame(z)
})

?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From p_connolly at slingshot.co.nz  Sun Dec  2 20:03:25 2007
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Mon, 3 Dec 2007 08:03:25 +1300
Subject: [R] Controlling Postscript output, size and orientation
In-Reply-To: <14035096.post@talk.nabble.com>
References: <14035096.post@talk.nabble.com>
Message-ID: <20071202190325.GF6584@slingshot.co.nz>

On Thu, 29-Nov-2007 at 01:22PM -0800, Nathan Vandergrift wrote:

|> 
|> I'm trying to get my graphics so that I can use them in LaTeX to create (via
|> ) a pdf presentation.
|> 
|> I've tried controlling inner and outer margins and figure size using par(),
|> to no avail. The ps output keeps appearing as a portrait page with a
|> centered figure. Nothing I have been able to do so far has changed that.

Check out the paper argument to the postscript device.  I think you'll
be more sucessful.


HTH

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From lauri.nikkinen at iki.fi  Sun Dec  2 20:06:30 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Sun, 2 Dec 2007 21:06:30 +0200
Subject: [R] How to recode a factor level (within the list)?
In-Reply-To: <Pine.LNX.4.64.0712021835180.22480@gannet.stats.ox.ac.uk>
References: <ba8c09910712020953p5889a0eak66bcc2ea24d1d661@mail.gmail.com>
	<Pine.LNX.4.64.0712021835180.22480@gannet.stats.ox.ac.uk>
Message-ID: <ba8c09910712021106l1003d247x368f86214f7e6460@mail.gmail.com>

Thank you Prof Ripley for your solutions, I'll get by with these.

Lauri


2007/12/2, Prof Brian Ripley <ripley at stats.ox.ac.uk>:
> On Sun, 2 Dec 2007, Lauri Nikkinen wrote:
>
> > #Dear R-users,
> > #I have a data.frame like this:
> >
> > y1 <- rnorm(10) + 6.8
> > y2 <- rnorm(10) + (1:10*1.7 + 1)
> > y3 <- rnorm(10) + (1:10*6.7 + 3.7)
> > y <- c(y1,y2,y3)
> > x <- rep(1:3,10)
> > f <- gl(2,15, labels=paste("lev", 1:2, sep=""))
> > g <- seq(as.Date("2000/1/1"), by="day", length=30)
> > DF <- data.frame(x=x,y=y, f=f, g=g)
> > DF$g[DF$x == 1] <- NA
> > DF$x[3:6] <- NA
> > DF$wdays <- weekdays(DF$g)
> >
> > DF
> >
> > #Frequences
> > g <- lapply(DF, function(x) as.data.frame(table(format(x))))
> > g
> >
> > #NA:s are now part of factor levels. How to recode NA:s into e.g. "missing"?
>
> Not so:
>
> > sapply(DF, class)
>           x           y           f           g       wdays
>   "integer"   "numeric"    "factor"      "Date" "character"
>
> and DF$f does not have any NA levels.
>
> The place you may think you have got NAs is in format(wdays):
> they are not NA nor "NA" but "NA     ".  I am not sure what exactly you
> want (NA is not appearing in the tables: see the 'exclude' argument),
> but perhaps
>
> lapply(DF, function(x) {
>   if(is.character(x)) x[is.na(x)] <- "missing"
>   as.data.frame(table(format(x)))
> })
>
> or
>
> lapply(DF, function(x) {
>   z <- table(format(x))
>   names(z)[grep("^NA", names(z))] <- "missing"
>   as.data.frame(z)
> })
>
> or
>
> lapply(DF, function(x) {
>   z <- table(x, exclude=character(0))
>   names(z)[is.na(names(z))] <- "missing"
>   as.data.frame(z)
> })
>
> ?
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From milton_ruser at yahoo.com.br  Sun Dec  2 20:08:01 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sun, 2 Dec 2007 11:08:01 -0800 (PST)
Subject: [R] fitting "power model" in nls()
Message-ID: <923357.44257.qm@web56009.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071202/009ec8f2/attachment.pl 

From mgorlin at optonline.net  Sun Dec  2 20:12:49 2007
From: mgorlin at optonline.net (Margaret Gorlin)
Date: Sun, 02 Dec 2007 14:12:49 -0500
Subject: [R] error message in lmer for logistic regression model with random
 intercept and slope
Message-ID: <000a01c83517$54948a80$0301a8c0@dell001>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071202/d15c877a/attachment.pl 

From klebyn at yahoo.com.br  Sun Dec  2 20:23:54 2007
From: klebyn at yahoo.com.br (Cleber N. Borges)
Date: Sun, 02 Dec 2007 17:23:54 -0200
Subject: [R] odfWeave error
Message-ID: <4753064A.6030703@yahoo.com.br>


hello all,
I trying to use the package 'odfWeave'
and I get the follow error:



###  error message
#############################################################
...
  Removing content.xml

  Post-processing the contents
Error in .Call("RS_XML_Parse", file, handlers, as.logical(addContext),  :
  Error in the XML event driven parser for content_1.xml: error parsing 
attribute name



The piece of the code is:

###  code
############################################################
...

<<makeGraph, echo=T, results=xml>>=
fileplot='C:\\DADOS\\tmp\\plotx.emf'
win.metafile(fileplot)
plot( rnorm(300), col='red', t='l', lwd=2 ); grid()
dev.off()
@

<<insertGraph, echo=T, results=xml>>=
odfInsertPlot(file=fileplot, height=5, width=5 )
@




[[replacing trailing spam]]
Cleber Borges

### sessionInfo
############################################################
 >
 > sessionInfo()
R version 2.6.1 (2007-11-26)
i386-pc-mingw32

locale:
LC_COLLATE=C;LC_CTYPE=C;LC_MONETARY=Portuguese_Brazil.1252;LC_NUMERIC=C;LC_TIME=Portuguese_Brazil.1252

attached base packages:
[1] grid      stats     graphics  grDevices utils     datasets  
methods   base    

other attached packages:
[1] MASS_7.2-38    odfWeave_0.6.0 XML_1.93-2.1   lattice_0.17-2

loaded via a namespace (and not attached):
[1] rcompgen_0.1-17 tools_2.6.1   
 >



	

	
		
_______________________________________________________ 

Experimente j? e veja as novidades.


From mcintosh at research.telcordia.com  Sun Dec  2 20:16:30 2007
From: mcintosh at research.telcordia.com (Allen McIntosh)
Date: Sun, 2 Dec 2007 14:16:30 -0500
Subject: [R] persp() misfeature
Message-ID: <200712021916.lB2JGUNK032455@mc-pc.research.telcordia.com>

Version: Observed in 2.5.1

> x <- 1:10
> y <- 1
> z <- array(1:10,dim=c(10,1))
> persp(x,y,z)
Error in persp(x, y, z, xlim, ylim, zlim, theta, phi, r, d, scale, expand,  :
        invalid 'x' argument


The problem isn't 'x'.  It's 'y'.


From mgorlin at optonline.net  Sun Dec  2 20:20:45 2007
From: mgorlin at optonline.net (Margaret Gorlin)
Date: Sun, 02 Dec 2007 14:20:45 -0500
Subject: [R] error messgage in lmer for random intercept and slope model
Message-ID: <000c01c83518$706343e0$0301a8c0@dell001>

Greetings,

I am trying to run a logistic regression model for binary data with a random
intercept and slope in R 2.6.1.  When I use the code:

lmer1<-lmer(infect ~  time+gender + (1+time|id), family=binomial, data=ichs,
method="Laplace")

Then from:
 summary(lmer1)
I get the message:

Error in if (any(sd < 0)) return("'sd' slot has negative entries") :
  missing value where TRUE/FALSE needed

The model with just a random intercept has no difficulty.  That is the
model:

lmer1<-lmer(infect ~  time+gender + (1|id), family=binomial, data=ichs,
method="Laplace")

yields no such message.  Can someone please tell me what this is about?

Thank you,
Margaret Gorlin
PhD Student
UMDNJ SPH


From r.turner at auckland.ac.nz  Sun Dec  2 20:50:26 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 3 Dec 2007 08:50:26 +1300
Subject: [R] fitting "power model" in nls()
In-Reply-To: <923357.44257.qm@web56009.mail.re3.yahoo.com>
References: <923357.44257.qm@web56009.mail.re3.yahoo.com>
Message-ID: <EAFD896B-5F57-4C1B-96D9-0DE0B46F9D6B@auckland.ac.nz>


Rule number 1:  Read the help for the function you are using.
You must supply starting values for the fit --- which the code you gave
doesn't do.

Rule number 2:  Don't use nls()!  Endless grief results.

Instead try:

foo <- function(par,x,y){
  Const <- par[1]
  B <- par[2]
  A <- par[3]
  sum((y-(Const+B*(x^A)))^2)
  }

fit <- optim(c 
(1,2.5,0.2),foo,x=area,y=richness,method="BFGS",control=list 
(maxit=1000))

plot(area,richness)
ccc <- fit$par
curve(ccc[1]+ccc[2]*(x^ccc[3]),from=0,to=2000,add=TRUE,col="red") #  
Fit doesn't look too bad.

HTH.

		cheers,

			Rolf Turner

On 3/12/2007, at 8:08 AM, Milton Cezar Ribeiro wrote:

> Dear all,
> I am still fighting against my "power model".
> I tryed several times to use nls() but I can?t run it.
> I am sending my variables and also the model which I would like to  
> fit.
> As you can see, this "power model" is not the best model to be fit,  
> but I really need also to fit it.
>
> The model which I would like to fit is Richness = B*(Area^A)
>
> richness<-c 
> (44,36,31,39,38,26,37,33,34,48,25,22,44,5,9,13,17,15,21,10,16,22,13,20 
> ,9,15,14,21,23,23,32,29,20,
> 26,31,4,20,25,24,32,23,33,34,23,28,30,10,29,40,10,8,12,13,14,56,47,44, 
> 37,27,17,32,31,26,23,31,34,
> 37,32,26,37,28,38,35,27,34,35,32,27,22,23,13,28,13,22,45,33,46,37,21,2 
> 8,38,21,18,21,18,24,18,23,22,
> 38,40,52,31,38,15,21)
> area<-c 
> (26.22,20.45,128.68,117.24,19.61,295.21,31.83,30.36,13.57,60.47,205.30 
> ,40.21,
> 7.99,1.18,5.40,13.37,4.51,36.61,7.56,10.30,7.29,9.54,6.93,12.60,
> 2.43,18.89,15.03,14.49,28.46,36.03,38.52,45.16,58.27,67.13,92.33,1.17,
> 29.52,84.38,87.57,109.08,72.28,66.15,142.27,76.41,105.76,73.47,1.71,30 
> 5.75,
> 325.78,3.71,6.48,19.26,3.69,6.27,1689.67,95.23,13.47,8.60,96.00,436.97 
> ,
> 472.78,441.01,467.24,1169.11,1309.10,1905.16,135.92,438.25,526.68,88.8 
> 8,31.43,21.22,
> 640.88,14.09,28.91,103.38,178.99,120.76,161.15,137.38,158.31,179.36,21 
> 4.36,187.05,
> 140.92,258.42,85.86,47.70,44.09,18.04,127.84,1694.32,34.27,75.19,54.39 
> ,79.88,
> 63.84,82.24,88.23,202.66,148.93,641.76,20.45,145.31,27.52,30.70)
> plot(richness~area)
>
> I tryed to fit the following model:
>
> m1<-nls(richness ~ Const+B*(area^A))
>
> Thanks a lot,
> miltinho
> Brazil.
>
>
>
>  para armazenamento!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

######################################################################
Attention: 
This e-mail message is privileged and confidential. If you are not the 
intended recipient please delete the message and notify the sender. 
Any views or opinions presented are solely those of the author.

This e-mail has been scanned and cleared by MailMarshal 
www.marshalsoftware.com
######################################################################


From murdoch at stats.uwo.ca  Sun Dec  2 20:52:33 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 02 Dec 2007 14:52:33 -0500
Subject: [R] array() misfeature
In-Reply-To: <14118356.post@talk.nabble.com>
References: <200712021649.lB2GnfCx030277@mc-pc.research.telcordia.com>	<4752F34C.9080609@stats.uwo.ca>
	<14118356.post@talk.nabble.com>
Message-ID: <47530D01.9090004@stats.uwo.ca>

On 02/12/2007 1:31 PM, Ben Bolker wrote:
> 
> 
> Duncan Murdoch-2 wrote:
>> On 02/12/2007 11:49 AM, Allen McIntosh wrote:
>>> Version: 2.5.1
>> That's an obsolete version, but the issue is still present in R-devel.
>>
>>> array() is inconsistent when given non-integral dimensions:
>>>
>>>> zz <- array(0,dim=c(4,3.01))
>>>> dim(zz)
>>> [1] 4 3
>>>> zz <- array(0,dim=c(201,4.05))
>>> Error in dim(data) <- dim : dim<- : dims [product 804] do not match the
>>> length of object [814]
>>>
>>>
>>> [IMHO the code that did this is broken.  My copy of it has been
>>> fixed.  Consistent behavior and/or a clearer error message would
>>> just have made it easier to find the problem.]
>>>
>> I'll add code to coerce dim to integer before using it.  Thanks for the 
>> report.
>>
>> Duncan Murdoch
>>
>>
> 
>   While you're at it could you fix a trivial typo in ?dim   ?

Too late, but I'll fix the typo.
> 
>   in the description of the value argument: "... or a numeric
> vector which [is] coerced ..."
>  (I would also say that either there should be a comma after
> "vector" or "which" should be changed to "that", but never mind ...)

I think "which" is correct and have added a comma.

Duncan Murdoch

> 
>   Ben
>


From murdoch at stats.uwo.ca  Sun Dec  2 21:00:54 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 02 Dec 2007 15:00:54 -0500
Subject: [R] persp() misfeature
In-Reply-To: <200712021916.lB2JGUNK032455@mc-pc.research.telcordia.com>
References: <200712021916.lB2JGUNK032455@mc-pc.research.telcordia.com>
Message-ID: <47530EF6.7000305@stats.uwo.ca>

On 02/12/2007 2:16 PM, Allen McIntosh wrote:
> Version: Observed in 2.5.1
> 
>> x <- 1:10
>> y <- 1
>> z <- array(1:10,dim=c(10,1))
>> persp(x,y,z)
> Error in persp(x, y, z, xlim, ylim, zlim, theta, phi, r, d, scale, expand,  :
>         invalid 'x' argument
> 
> 
> The problem isn't 'x'.  It's 'y'.

Right, also still there in R-devel.  It's just a typo, which I'll fix.

Duncan Murdoch


From lauri.nikkinen at iki.fi  Sun Dec  2 21:07:34 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Sun, 2 Dec 2007 22:07:34 +0200
Subject: [R] How to recode a factor level (within the list)?
In-Reply-To: <Pine.LNX.4.64.0712021835180.22480@gannet.stats.ox.ac.uk>
References: <ba8c09910712020953p5889a0eak66bcc2ea24d1d661@mail.gmail.com>
	<Pine.LNX.4.64.0712021835180.22480@gannet.stats.ox.ac.uk>
Message-ID: <ba8c09910712021207k73d9b08ci4785c692c6ae9c22@mail.gmail.com>

It seems I didn't get by with your previous solutions after all... I
would still need some more advice on the subject. I edited the DF so
that now all variables contain missing values (NAs).

y1 <- rnorm(10) + 6.8
y2 <- rnorm(10) + (1:10*1.7 + 1)
y3 <- rnorm(10) + (1:10*6.7 + 3.7)
y <- c(y1,y2,y3)
x <- rep(1:3,10)
f <- gl(2,15, labels=paste("lev", 1:2, sep=""))
g <- seq(as.Date("2000/1/1"), by="day", length=30)
DF <- data.frame(x=x,y=y, f=f, g=g)
DF$g[DF$x == 1] <- NA
DF$x[3:6] <- NA
DF$y[1] <- NA
DF$f[1] <- NA
DF$wdays <- weekdays(DF$g)

DF

Is it possible to create a function using the lapply function that
would produce those frequency data.frames into a list and replace all
NAs with the word "missing" in all these variables (which have a
different class)?

> sapply(DF, class)
          x           y           f           g       wdays
  "integer"   "numeric"    "factor"      "Date" "character"

Really appreciate the help,

Lauri


2007/12/2, Prof Brian Ripley <ripley at stats.ox.ac.uk>:
> On Sun, 2 Dec 2007, Lauri Nikkinen wrote:
>
> > #Dear R-users,
> > #I have a data.frame like this:
> >
> > y1 <- rnorm(10) + 6.8
> > y2 <- rnorm(10) + (1:10*1.7 + 1)
> > y3 <- rnorm(10) + (1:10*6.7 + 3.7)
> > y <- c(y1,y2,y3)
> > x <- rep(1:3,10)
> > f <- gl(2,15, labels=paste("lev", 1:2, sep=""))
> > g <- seq(as.Date("2000/1/1"), by="day", length=30)
> > DF <- data.frame(x=x,y=y, f=f, g=g)
> > DF$g[DF$x == 1] <- NA
> > DF$x[3:6] <- NA
> > DF$wdays <- weekdays(DF$g)
> >
> > DF
> >
> > #Frequences
> > g <- lapply(DF, function(x) as.data.frame(table(format(x))))
> > g
> >
> > #NA:s are now part of factor levels. How to recode NA:s into e.g. "missing"?
>
> Not so:
>
> > sapply(DF, class)
>           x           y           f           g       wdays
>   "integer"   "numeric"    "factor"      "Date" "character"
>
> and DF$f does not have any NA levels.
>
> The place you may think you have got NAs is in format(wdays):
> they are not NA nor "NA" but "NA     ".  I am not sure what exactly you
> want (NA is not appearing in the tables: see the 'exclude' argument),
> but perhaps
>
> lapply(DF, function(x) {
>   if(is.character(x)) x[is.na(x)] <- "missing"
>   as.data.frame(table(format(x)))
> })
>
> or
>
> lapply(DF, function(x) {
>   z <- table(format(x))
>   names(z)[grep("^NA", names(z))] <- "missing"
>   as.data.frame(z)
> })
>
> or
>
> lapply(DF, function(x) {
>   z <- table(x, exclude=character(0))
>   names(z)[is.na(names(z))] <- "missing"
>   as.data.frame(z)
> })
>
> ?
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From j.van_den_hoff at fzd.de  Sun Dec  2 21:26:23 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Sun, 2 Dec 2007 21:26:23 +0100
Subject: [R] fitting "power model" in nls()
In-Reply-To: <923357.44257.qm@web56009.mail.re3.yahoo.com>
References: <923357.44257.qm@web56009.mail.re3.yahoo.com>
Message-ID: <20071202202623.GG5400@kati.fz-rossendorf.de>

On Sun, Dec 02, 2007 at 11:08:01AM -0800, Milton Cezar Ribeiro wrote:
> Dear all,
> I am still fighting against my "power model".
> I tryed several times to use nls() but I can??t run it.
> I am sending my variables and also the model which I would like to fit. 
> As you can see, this "power model" is not the best model to be fit, but I really need also to fit it.
> 
> The model which I would like to fit is Richness = B*(Area^A)
> 
> richness<-c(44,36,31,39,38,26,37,33,34,48,25,22,44,5,9,13,17,15,21,10,16,22,13,20,9,15,14,21,23,23,32,29,20,
> 26,31,4,20,25,24,32,23,33,34,23,28,30,10,29,40,10,8,12,13,14,56,47,44,37,27,17,32,31,26,23,31,34,
> 37,32,26,37,28,38,35,27,34,35,32,27,22,23,13,28,13,22,45,33,46,37,21,28,38,21,18,21,18,24,18,23,22,
> 38,40,52,31,38,15,21)
> area<-c(26.22,20.45,128.68,117.24,19.61,295.21,31.83,30.36,13.57,60.47,205.30,40.21,
> 7.99,1.18,5.40,13.37,4.51,36.61,7.56,10.30,7.29,9.54,6.93,12.60,
> 2.43,18.89,15.03,14.49,28.46,36.03,38.52,45.16,58.27,67.13,92.33,1.17,
> 29.52,84.38,87.57,109.08,72.28,66.15,142.27,76.41,105.76,73.47,1.71,305.75,
> 325.78,3.71,6.48,19.26,3.69,6.27,1689.67,95.23,13.47,8.60,96.00,436.97,
> 472.78,441.01,467.24,1169.11,1309.10,1905.16,135.92,438.25,526.68,88.88,31.43,21.22,
> 640.88,14.09,28.91,103.38,178.99,120.76,161.15,137.38,158.31,179.36,214.36,187.05,
> 140.92,258.42,85.86,47.70,44.09,18.04,127.84,1694.32,34.27,75.19,54.39,79.88,
> 63.84,82.24,88.23,202.66,148.93,641.76,20.45,145.31,27.52,30.70)
> plot(richness~area)
> 
> I tryed to fit the following model:
> 
> m1<-nls(richness ~ Const+B*(area^A))
> 
> Thanks a lot, 
> miltinho
> Brazil.
> 

for easier notation, let y=richness, x=area, C=const in the following.

then 

nls(y~B*x^A + C, start = c(A=3.2, B=0.002, C=0))

converges alright. where's the problem (apart from this being not a very good
model for the data)? the critical point is to provide some reasonable estimate
of the parameters as starting values.

to get reasonable start values, I'd use:


y = B*x^A + C --> log(y-C) = log(B) + A*log(x) --> ly = b + a*lx,

estimate C from the x -> 0 asymptotic value (approx. 0)

and use lm(ly~lx) which yields a and b estimates which you could use in `nls'.

and, contrary to other assessments you've received, I definitely would prefer `nls'
for least squares fitting instead of using `optim' or other general minimization routines.

hth

joerg


From ggrothendieck at gmail.com  Sun Dec  2 21:28:23 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 2 Dec 2007 15:28:23 -0500
Subject: [R] fitting "power model" in nls()
In-Reply-To: <923357.44257.qm@web56009.mail.re3.yahoo.com>
References: <923357.44257.qm@web56009.mail.re3.yahoo.com>
Message-ID: <971536df0712021228q7571a3f1u562d94b0e65130cc@mail.gmail.com>

Is that really the model we want?  When we have problems sometimes
its just a sign that the model is not very good in the first place.

plot(richness ~ area)

shows most of the points crowded the left and just a few points out to
the right.  This
does not seem like a very good pattern for model fitting.

plot(richness ~ log(area))
plot(log(richness) ~ log(area))

both look nicer.

On Dec 2, 2007 2:08 PM, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
> Dear all,
> I am still fighting against my "power model".
> I tryed several times to use nls() but I can?t run it.
> I am sending my variables and also the model which I would like to fit.
> As you can see, this "power model" is not the best model to be fit, but I really need also to fit it.
>
> The model which I would like to fit is Richness = B*(Area^A)
>
> richness<-c(44,36,31,39,38,26,37,33,34,48,25,22,44,5,9,13,17,15,21,10,16,22,13,20,9,15,14,21,23,23,32,29,20,
> 26,31,4,20,25,24,32,23,33,34,23,28,30,10,29,40,10,8,12,13,14,56,47,44,37,27,17,32,31,26,23,31,34,
> 37,32,26,37,28,38,35,27,34,35,32,27,22,23,13,28,13,22,45,33,46,37,21,28,38,21,18,21,18,24,18,23,22,
> 38,40,52,31,38,15,21)
> area<-c(26.22,20.45,128.68,117.24,19.61,295.21,31.83,30.36,13.57,60.47,205.30,40.21,
> 7.99,1.18,5.40,13.37,4.51,36.61,7.56,10.30,7.29,9.54,6.93,12.60,
> 2.43,18.89,15.03,14.49,28.46,36.03,38.52,45.16,58.27,67.13,92.33,1.17,
> 29.52,84.38,87.57,109.08,72.28,66.15,142.27,76.41,105.76,73.47,1.71,305.75,
> 325.78,3.71,6.48,19.26,3.69,6.27,1689.67,95.23,13.47,8.60,96.00,436.97,
> 472.78,441.01,467.24,1169.11,1309.10,1905.16,135.92,438.25,526.68,88.88,31.43,21.22,
> 640.88,14.09,28.91,103.38,178.99,120.76,161.15,137.38,158.31,179.36,214.36,187.05,
> 140.92,258.42,85.86,47.70,44.09,18.04,127.84,1694.32,34.27,75.19,54.39,79.88,
> 63.84,82.24,88.23,202.66,148.93,641.76,20.45,145.31,27.52,30.70)
> plot(richness~area)
>
> I tryed to fit the following model:
>
> m1<-nls(richness ~ Const+B*(area^A))
>
> Thanks a lot,
> miltinho
> Brazil.
>
>
>
>  para armazenamento!
>
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From milton_ruser at yahoo.com.br  Sun Dec  2 21:39:46 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sun, 2 Dec 2007 12:39:46 -0800 (PST)
Subject: [R] Res:  fitting "power model" in nls()
Message-ID: <589039.44736.qm@web56012.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071202/9577be0a/attachment.pl 

From Dan.Kelley at dal.ca  Sun Dec  2 21:46:52 2007
From: Dan.Kelley at dal.ca (dankelley)
Date: Sun, 2 Dec 2007 12:46:52 -0800 (PST)
Subject: [R] problem installing 2.6.1 on OSX Leopard (after failed MacPorts
 install)
Message-ID: <14119813.post@talk.nabble.com>


This message reports a problem, and its solution.  I found the solution while
posting.  Since others may have the same problem, I am continuing with the
post.

PROBLEM
I am having a different problem than the one that others have reported (e.g.
http://www.nabble.com/Problem-Installing-2.6.0-on-Mac-tf4778736.html#a13674247).

Whether I try to install R as a whole, or individual packages, the installer
has a problem near the end.  The message is as follows.

<pre>Install Failed

The Installer could not install some files in "/".
Contact the software manager for assistance.</pre>

I looked at the installer script and found the error message as follows.
<pre>
Dec  2 16:14:40 Macintosh pkgExtractor[5356]: BomFileError 62: Too many
levels of symbolic links - ///usr/local/lib
</pre>

SOLUTION

A quick check shows that, indeed,

/usr/local/lib

is messed up.  It is an infinite link to itself.  But, another directory

/usr/local/lib 1

seems to be the original directory, that was moved aside by something.  (I
think that something was a failed installation of MacPorts, based on the
directory creation time, but that's onlly a guess.)

So, the workaround is to 

sudo rm /usr/local/lib

followed by 

sudo mv /usr/local/lib\ 1 /usr/local/lib

(Note the use of backslash to protect the space in the directory name.)

After this, I found that I can install the individual packages (gfortran
etc) or the whole R application, without problem.

Other information: I've recently upgraded from Tiger to Leopard.  There was
a pre-existing R on the system.
-- 
View this message in context: http://www.nabble.com/problem-installing-2.6.1-on-OSX-Leopard-%28after-failed-MacPorts-install%29-tf4933141.html#a14119813
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Sun Dec  2 22:06:47 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 2 Dec 2007 16:06:47 -0500
Subject: [R] fitting "power model" in nls()
In-Reply-To: <589039.44736.qm@web56012.mail.re3.yahoo.com>
References: <589039.44736.qm@web56012.mail.re3.yahoo.com>
Message-ID: <971536df0712021306u79cd9fc6u50b012cf851823a9@mail.gmail.com>

OK.  Since the model is linear except for A lets use brute force to
repeatedly evaluate the sum of squares for values of A between
-2 and 2 proceeding in steps of .01 solving the other parameters using
lm. That will give us better starting values and we should be able to
use nls on that.
> x <- seq(-2, 2, .01)
> ss <- sapply(x, function(A) sum(resid(lm(richness ~ I(area^A)))^2))
> plot(ss ~ x)
> x[which.min(ss)]
[1] -0.45
> model.lm <- lm(richness ~ I(area^-0.45))
> # use starting values based on lm and A = -0.45
> st <- c(Const = coef(model.lm)[[1]], B = coef(model.lm)[[2]], A = x[which.min(ss)])
> nls(richness ~ Const+B*(area^A), st = st)
Nonlinear regression model
  model:  richness ~ Const + B * (area^A)
   data:  parent.frame()
   Const        B        A
 33.9289 -33.4595  -0.4464
 residual sum-of-squares: 8751

Number of iterations to convergence: 2
Achieved convergence tolerance: 3.368e-06

Note that our A value is suspiciously close to A = -0.5 and sqrt(area)
is length so I wonder if there is an argument based on units of
measurement that might support a model of the form:

richness = Const + B / sqrt(area)




On Dec 2, 2007 3:39 PM, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
>
> Dear Gabor,
>
> Thank you for your reply.
> In fact I am ajusting several models at same time, like linear, log-linear,
> log-log, piecewise etc. One of the models are the power model. I really need
> to fit a power model because it one of the hypothesis which have been
> suggested on literature.
>
> In addition, there are other variables which are beeing tested as
> explanatory.
>
> Kind regards,
>
> miltinho
> ----- Mensagem original ----
> De: Gabor Grothendieck <ggrothendieck at gmail.com>
> Para: Milton Cezar Ribeiro <milton_ruser at yahoo.com.br>
> Cc: R-help <r-help at stat.math.ethz.ch>
> Enviadas: Domingo, 2 de Dezembro de 2007 17:28:23
> Assunto: Re: [R] fitting "power model" in nls()
>
>
>
> Is that really the model we want?  When we have problems sometimes
> its just a sign that the model is not very good in the first place.
>
> plot(richness ~ area)
>
> shows most of the points crowded the left and just a few points out to
> the right.  This
> does not seem like a very good pattern for model fitting.
>
> plot(richness ~ log(area))
> plot(log(richness) ~ log(area))
>
> both look nicer.
>
> On Dec 2, 2007 2:08 PM, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br>
> wrote:
> > Dear all,
> > I am still fighting against my "power model".
> > I tryed several times to use nls() but I can?t run it.
> > I am sending my variables and also the model which I would like to fit.
> > As you can see, this "power model" is not the best model to be fit, but I
> really need also to fit it.
> >
> > The model which I would like to fit is Richness = B*(Area^A)
> >
> >
> richness<-c(44,36,31,39,38,26,37,33,34,48,25,22,44,5,9,13,17,15,21,10,16,22,13,20,9,15,14,21,23,23,32,29,20,
> >
> 26,31,4,20,25,24,32,23,33,34,23,28,30,10,29,40,10,8,12,13,14,56,47,44,37,27,17,32,31,26,23,31,34,
> >
> 37,32,26,37,28,38,35,27,34,35,32,27,22,23,13,28,13,22,45,33,46,37,21,28,38,21,18,21,18,24,18,23,22,
> > 38,40,52,31,38,15,21)
> >
> area<-c(26.22,20.45,128.68,117.24,19.61,295.21,31.83,30.36,13.57,60.47,205.30,40.21,
> > 7.99,1.18,5.40,13.37,4.51,36.61,7.56,10.30,7.29,9.54,6.93,12.60,
> > 2.43,18.89,15.03,14.49,28.46,36.03,38.52,45.16,58.27,67.13,92.33,1.17,
> >
> 29.52,84.38,87.57,109.08,72.28,66.15,142.27,76.41,105.76,73.47,1.71,305.75,
> > 325.78,3.71,6.48,19.26,3.69,6.27,1689.67,95.23,13.47,8.60,96.00,436.97,
> >
> 472.78,441.01,467.24,1169.11,1309.10,1905.16,135.92,438.25,526.68,88.88,31.43,21.22,
> >
> 640.88,14.09,28.91,103.38,178.99,120.76,161.15,137.38,158.31,179.36,214.36,187.05,
> >
> 140.92,258.42,85.86,47.70,44.09,18.04,127.84,1694.32,34.27,75.19,54.39,79.88,
> > 63.84,82.24,88.23,202.66,148.93,641.76,20.45,145.31,27.52,30.70)
> > plot(richness~area)
> >
> > I tryed to fit the following model:
> >
> > m1<-nls(richness ~ Const+B*(area^A))
> >
> > Thanks a lot,
> > miltinho
> > Brazil.
> >
> >
> >
> >  para armazenamento!
> >
> >        [[alternative HTML version deleted]]
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
>
>
> ________________________________
> Abra sua conta no Yahoo! Mail, o ?nico sem limite de espa?o para
> armazenamento!


From r.turner at auckland.ac.nz  Sun Dec  2 22:38:45 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 3 Dec 2007 10:38:45 +1300
Subject: [R] fitting "power model" in nls()
In-Reply-To: <20071202202623.GG5400@kati.fz-rossendorf.de>
References: <923357.44257.qm@web56009.mail.re3.yahoo.com>
	<20071202202623.GG5400@kati.fz-rossendorf.de>
Message-ID: <7966939F-1FA0-41E7-8251-47AED702945C@auckland.ac.nz>


On 3/12/2007, at 9:26 AM, Joerg van den Hoff wrote:

	<snip>

> and, contrary to other assessments you've received, I definitely  
> would prefer `nls'
> for least squares fitting instead of using `optim' or other general  
> minimization routines.

Clearly you are far cleverer than I at getting nls() to work.

I attempted to replicate your recipe ***exactly***, right down to the  
very last
detail of notation and the ordering of terms in the expression:

  > x <- area
  > y <- richness
  > ml2 <- nls(y ~ B*x^A + C,start=c(A=3.2,B=0.002,C=0))

and got the following error:

Error in nls(y ~ B * x^A + C, start = c(A = 3.2, B = 0.002, C = 0)) :
   singular gradient

	cheers,

		Rolf Turner

P.S.:

Version information:

 > version
                _
platform       i386-apple-darwin8.10.1
arch           i386
os             darwin8.10.1
system         i386, darwin8.10.1
status
major          2
minor          6.0
year           2007
month          10
day            03
svn rev        43063
language       R
version.string R version 2.6.0 (2007-10-03)

######################################################################
Attention:\ This e-mail message is privileged and confid...{{dropped:9}}


From S.Ellison at lgc.co.uk  Mon Dec  3 00:20:27 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Sun, 02 Dec 2007 23:20:27 +0000
Subject: [R] Rating R Helpers
Message-ID: <s7533dcf.025@tedmail.lgc.co.uk>

Package review is a nice idea. But you raise a worrying point.
Are any of the 'downright dangerous' packages on CRAN?
If so, er... why?


>>> <Bill.Venables at csiro.au> 12/01/07 7:21 AM >>>
>I think the need for this is rather urgent, in fact.  Most packages are
>very good, but I regret to say some are pretty inefficient and others
>downright dangerous.


From ggrothendieck at gmail.com  Mon Dec  3 00:27:22 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 2 Dec 2007 18:27:22 -0500
Subject: [R] fitting "power model" in nls()
In-Reply-To: <971536df0712021306u79cd9fc6u50b012cf851823a9@mail.gmail.com>
References: <589039.44736.qm@web56012.mail.re3.yahoo.com>
	<971536df0712021306u79cd9fc6u50b012cf851823a9@mail.gmail.com>
Message-ID: <971536df0712021527h7c20142em4515ab37f731563@mail.gmail.com>

Also the fitted values satisfy Const = -B = 33 (approximately) so we could try:

> plot(richness ~ area)
> nls(richness ~ C * (1 - 1/sqrt(area)), start = c(C = 33))
Nonlinear regression model
  model:  richness ~ C * (1 - 1/sqrt(area))
   data:  parent.frame()
    C
32.85
 residual sum-of-squares: 8764

Number of iterations to convergence: 1
Achieved convergence tolerance: 5.595e-10
> simple.nls <- .Last.value
> points(fitted(simple.nls) ~ area, pch = "+", col = "red")
>


On Dec 2, 2007 4:06 PM, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> OK.  Since the model is linear except for A lets use brute force to
> repeatedly evaluate the sum of squares for values of A between
> -2 and 2 proceeding in steps of .01 solving the other parameters using
> lm. That will give us better starting values and we should be able to
> use nls on that.
> > x <- seq(-2, 2, .01)
> > ss <- sapply(x, function(A) sum(resid(lm(richness ~ I(area^A)))^2))
> > plot(ss ~ x)
> > x[which.min(ss)]
> [1] -0.45
> > model.lm <- lm(richness ~ I(area^-0.45))
> > # use starting values based on lm and A = -0.45
> > st <- c(Const = coef(model.lm)[[1]], B = coef(model.lm)[[2]], A = x[which.min(ss)])
> > nls(richness ~ Const+B*(area^A), st = st)
> Nonlinear regression model
>  model:  richness ~ Const + B * (area^A)
>   data:  parent.frame()
>   Const        B        A
>  33.9289 -33.4595  -0.4464
>  residual sum-of-squares: 8751
>
> Number of iterations to convergence: 2
> Achieved convergence tolerance: 3.368e-06
>
> Note that our A value is suspiciously close to A = -0.5 and sqrt(area)
> is length so I wonder if there is an argument based on units of
> measurement that might support a model of the form:
>
> richness = Const + B / sqrt(area)
>
>
>
>
>
> On Dec 2, 2007 3:39 PM, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
> >
> > Dear Gabor,
> >
> > Thank you for your reply.
> > In fact I am ajusting several models at same time, like linear, log-linear,
> > log-log, piecewise etc. One of the models are the power model. I really need
> > to fit a power model because it one of the hypothesis which have been
> > suggested on literature.
> >
> > In addition, there are other variables which are beeing tested as
> > explanatory.
> >
> > Kind regards,
> >
> > miltinho
> > ----- Mensagem original ----
> > De: Gabor Grothendieck <ggrothendieck at gmail.com>
> > Para: Milton Cezar Ribeiro <milton_ruser at yahoo.com.br>
> > Cc: R-help <r-help at stat.math.ethz.ch>
> > Enviadas: Domingo, 2 de Dezembro de 2007 17:28:23
> > Assunto: Re: [R] fitting "power model" in nls()
> >
> >
> >
> > Is that really the model we want?  When we have problems sometimes
> > its just a sign that the model is not very good in the first place.
> >
> > plot(richness ~ area)
> >
> > shows most of the points crowded the left and just a few points out to
> > the right.  This
> > does not seem like a very good pattern for model fitting.
> >
> > plot(richness ~ log(area))
> > plot(log(richness) ~ log(area))
> >
> > both look nicer.
> >
> > On Dec 2, 2007 2:08 PM, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br>
> > wrote:
> > > Dear all,
> > > I am still fighting against my "power model".
> > > I tryed several times to use nls() but I can?t run it.
> > > I am sending my variables and also the model which I would like to fit.
> > > As you can see, this "power model" is not the best model to be fit, but I
> > really need also to fit it.
> > >
> > > The model which I would like to fit is Richness = B*(Area^A)
> > >
> > >
> > richness<-c(44,36,31,39,38,26,37,33,34,48,25,22,44,5,9,13,17,15,21,10,16,22,13,20,9,15,14,21,23,23,32,29,20,
> > >
> > 26,31,4,20,25,24,32,23,33,34,23,28,30,10,29,40,10,8,12,13,14,56,47,44,37,27,17,32,31,26,23,31,34,
> > >
> > 37,32,26,37,28,38,35,27,34,35,32,27,22,23,13,28,13,22,45,33,46,37,21,28,38,21,18,21,18,24,18,23,22,
> > > 38,40,52,31,38,15,21)
> > >
> > area<-c(26.22,20.45,128.68,117.24,19.61,295.21,31.83,30.36,13.57,60.47,205.30,40.21,
> > > 7.99,1.18,5.40,13.37,4.51,36.61,7.56,10.30,7.29,9.54,6.93,12.60,
> > > 2.43,18.89,15.03,14.49,28.46,36.03,38.52,45.16,58.27,67.13,92.33,1.17,
> > >
> > 29.52,84.38,87.57,109.08,72.28,66.15,142.27,76.41,105.76,73.47,1.71,305.75,
> > > 325.78,3.71,6.48,19.26,3.69,6.27,1689.67,95.23,13.47,8.60,96.00,436.97,
> > >
> > 472.78,441.01,467.24,1169.11,1309.10,1905.16,135.92,438.25,526.68,88.88,31.43,21.22,
> > >
> > 640.88,14.09,28.91,103.38,178.99,120.76,161.15,137.38,158.31,179.36,214.36,187.05,
> > >
> > 140.92,258.42,85.86,47.70,44.09,18.04,127.84,1694.32,34.27,75.19,54.39,79.88,
> > > 63.84,82.24,88.23,202.66,148.93,641.76,20.45,145.31,27.52,30.70)
> > > plot(richness~area)
> > >
> > > I tryed to fit the following model:
> > >
> > > m1<-nls(richness ~ Const+B*(area^A))
> > >
> > > Thanks a lot,
> > > miltinho
> > > Brazil.
> > >
> > >
> > >
> > >  para armazenamento!
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> >
> >
> >
> > ________________________________
> > Abra sua conta no Yahoo! Mail, o ?nico sem limite de espa?o para
> > armazenamento!
>


From tomfool at as220.org  Mon Dec  3 00:24:20 2007
From: tomfool at as220.org (Tom Sgouros)
Date: Sun, 02 Dec 2007 18:24:20 -0500
Subject: [R] documenting yoru progress
Message-ID: <20071202232420.3348AFAC8C4@as220.org>


Hello all:

I have a function that writes a fairly elaborate report based on some
survey data.  For documentation and bookkeeping purposes, I'd like to
write out in the report the function call that produced the report, or
at least enough information to help me recreate the steps that led to
that report.  I've been generating all the reports with scripts, in
order to be able to recreate the steps, but apart from the file name, I
don't yet have a way to match the report to the script that created it.

Can anyone suggest easy ways to do this?  From within a function, is the
function call text available somehow, or the names of the arguments used
in the function invocation?

Many thanks,

 -tom


-- 
 ------------------------
 tomfool at as220 dot org
 http://sgouros.com  
 http://whatcheer.net


From ggrothendieck at gmail.com  Mon Dec  3 00:57:33 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 2 Dec 2007 18:57:33 -0500
Subject: [R] documenting yoru progress
In-Reply-To: <20071202232420.3348AFAC8C4@as220.org>
References: <20071202232420.3348AFAC8C4@as220.org>
Message-ID: <971536df0712021557wd9f3ec1kf21a2ff5d16bbfa3@mail.gmail.com>

If what you mean is that you have a file, test.R, of R commands
and you are using source("test.R") and you wish to discover the
name "test.R" without hard coding it in your file, then place this in
test.R:

ofile <- parent.frame(2)$ofile

and ofile will be set to "test.R".  Note that the line shown should not
be within a function or other local environment within the file but
directly at top level.

This is hack which may need to be modified if the internals of the
source command change.

On Dec 2, 2007 6:24 PM, Tom Sgouros <tomfool at as220.org> wrote:
>
> Hello all:
>
> I have a function that writes a fairly elaborate report based on some
> survey data.  For documentation and bookkeeping purposes, I'd like to
> write out in the report the function call that produced the report, or
> at least enough information to help me recreate the steps that led to
> that report.  I've been generating all the reports with scripts, in
> order to be able to recreate the steps, but apart from the file name, I
> don't yet have a way to match the report to the script that created it.
>
> Can anyone suggest easy ways to do this?  From within a function, is the
> function call text available somehow, or the names of the arguments used
> in the function invocation?
>
> Many thanks,
>
>  -tom
>
>
> --
>  ------------------------
>  tomfool at as220 dot org
>  http://sgouros.com
>  http://whatcheer.net
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tomfool at as220.org  Mon Dec  3 00:55:41 2007
From: tomfool at as220.org (tom sgouros)
Date: Sun, 02 Dec 2007 18:55:41 -0500
Subject: [R] documenting yoru progress
In-Reply-To: <971536df0712021557wd9f3ec1kf21a2ff5d16bbfa3@mail.gmail.com> 
References: <20071202232420.3348AFAC8C4@as220.org>
	<971536df0712021557wd9f3ec1kf21a2ff5d16bbfa3@mail.gmail.com>
Message-ID: <20071202235541.C9622FAC44B@as220.org>



Gabor Grothendieck <ggrothendieck at gmail.com> wrote:

> If what you mean is that you have a file, test.R, of R commands
> and you are using source("test.R") and you wish to discover the
> name "test.R" without hard coding it in your file, then place this in
> test.R:
> 
> ofile <- parent.frame(2)$ofile
> 
> and ofile will be set to "test.R".  Note that the line shown should not
> be within a function or other local environment within the file but
> directly at top level.

Thank you, that is an excellent hack, and I will use it carefully
(though often), but it isn't quite what I had in mind.

In my script called test.R, there is a function called "survey.write",
which writes a report based on some survey results.  It is called
several times at the top level.  I would like the report to say how
"survey.write" was invoked, whether it was invoked as
"survey.write(res.raw,res.q)" or "survey.write(bus.raw,bus.q)".
Obviously I have access to the values of the arguments, but I want the
names of those arguments, or a string containing it all.

Thanks,

 -tom


> 
> This is hack which may need to be modified if the internals of the
> source command change.
> 
> On Dec 2, 2007 6:24 PM, Tom Sgouros <tomfool at as220.org> wrote:
> >
> > Hello all:
> >
> > I have a function that writes a fairly elaborate report based on some
> > survey data.  For documentation and bookkeeping purposes, I'd like to
> > write out in the report the function call that produced the report, or
> > at least enough information to help me recreate the steps that led to
> > that report.  I've been generating all the reports with scripts, in
> > order to be able to recreate the steps, but apart from the file name, I
> > don't yet have a way to match the report to the script that created it.
> >
> > Can anyone suggest easy ways to do this?  From within a function, is the
> > function call text available somehow, or the names of the arguments used
> > in the function invocation?
> >
> > Many thanks,
> >
> >  -tom
> >
> >
> > --
> >  ------------------------
> >  tomfool at as220 dot org
> >  http://sgouros.com
> >  http://whatcheer.net
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 


-- 
 ------------------------
 tomfool at as220 dot org
 http://sgouros.com  
 http://whatcheer.net


From ggrothendieck at gmail.com  Mon Dec  3 01:09:56 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 2 Dec 2007 19:09:56 -0500
Subject: [R] documenting yoru progress
In-Reply-To: <20071202235541.C9622FAC44B@as220.org>
References: <20071202232420.3348AFAC8C4@as220.org>
	<971536df0712021557wd9f3ec1kf21a2ff5d16bbfa3@mail.gmail.com>
	<20071202235541.C9622FAC44B@as220.org>
Message-ID: <971536df0712021609n73f36385s80dca485596003c1@mail.gmail.com>

Try this:

> survey.write <- function(x) {
+    print(match.call())
+    x
+ }
> out <- survey.write(pi+3)
survey.write(x = pi + 3)


On Dec 2, 2007 6:55 PM, tom sgouros <tomfool at as220.org> wrote:
>
>
> Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
> > If what you mean is that you have a file, test.R, of R commands
> > and you are using source("test.R") and you wish to discover the
> > name "test.R" without hard coding it in your file, then place this in
> > test.R:
> >
> > ofile <- parent.frame(2)$ofile
> >
> > and ofile will be set to "test.R".  Note that the line shown should not
> > be within a function or other local environment within the file but
> > directly at top level.
>
> Thank you, that is an excellent hack, and I will use it carefully
> (though often), but it isn't quite what I had in mind.
>
> In my script called test.R, there is a function called "survey.write",
> which writes a report based on some survey results.  It is called
> several times at the top level.  I would like the report to say how
> "survey.write" was invoked, whether it was invoked as
> "survey.write(res.raw,res.q)" or "survey.write(bus.raw,bus.q)".
> Obviously I have access to the values of the arguments, but I want the
> names of those arguments, or a string containing it all.
>
> Thanks,
>
>  -tom
>
>
> >
> > This is hack which may need to be modified if the internals of the
> > source command change.
> >
> > On Dec 2, 2007 6:24 PM, Tom Sgouros <tomfool at as220.org> wrote:
> > >
> > > Hello all:
> > >
> > > I have a function that writes a fairly elaborate report based on some
> > > survey data.  For documentation and bookkeeping purposes, I'd like to
> > > write out in the report the function call that produced the report, or
> > > at least enough information to help me recreate the steps that led to
> > > that report.  I've been generating all the reports with scripts, in
> > > order to be able to recreate the steps, but apart from the file name, I
> > > don't yet have a way to match the report to the script that created it.
> > >
> > > Can anyone suggest easy ways to do this?  From within a function, is the
> > > function call text available somehow, or the names of the arguments used
> > > in the function invocation?
> > >
> > > Many thanks,
> > >
> > >  -tom
> > >
> > >
> > > --
> > >  ------------------------
> > >  tomfool at as220 dot org
> > >  http://sgouros.com
> > >  http://whatcheer.net
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>
>
> --
>
>  ------------------------
>  tomfool at as220 dot org
>  http://sgouros.com
>  http://whatcheer.net
>


From dwinsemius at comcast.net  Mon Dec  3 01:13:14 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 3 Dec 2007 00:13:14 +0000 (UTC)
Subject: [R] Please help!
References: <14086120.post@talk.nabble.com>
	<BLU108-W3907FA55DD407F04CA15DEF2720@phx.gbl>
	<644e1f320712010701s914060dg3558b94043203ada@mail.gmail.com>
	<BLU108-W30003C1C359962848525EF2720@phx.gbl>
	<644e1f320712010803i4eaff8fbpcaa049f0f79c4d92@mail.gmail.com>
	<BLU108-W2A8A9277FC558D1FED1C6F2720@phx.gbl>
	<644e1f320712010831i7446cc07gd57ac061ed997d93@mail.gmail.com>
	<BLU108-W2625552B3523F22AD5D2BEF2720@phx.gbl>
	<644e1f320712010840x1b197f08s14430cf8d1dbe409@mail.gmail.com>
	<BLU108-W1004C9A583B1779AAFE909F2730@phx.gbl>
	<644e1f320712011945x4495de41wbf94a74863ddea00@mail.gmail.com>
	<BLU108-W44A9C8E34EC3B26B0D73C1F2730@phx.gbl>
Message-ID: <Xns99FAC3885645FdNOTwinscomcast@80.91.229.13>

jimbib webber <alfieim21 at hotmail.co.uk> wrote in
news:BLU108-W44A9C8E34EC3B26B0D73C1F2730 at phx.gbl: 

> Each number in the below list resides in a quantile. When put in
> order, there are 10 numbers, so the first is in the 0.1 quantile, the
> second in the 0.2 etc.
>
> Lets say we have 10 examples of systolic blood pressure from 30 year
> olds: 
> 104,95,106,105,110,150,101,98,85,104

> What I want to do is in R, calculate the corresponding quantiles from
> So, using the same mean and variance as the above random sample,
> create a normal distribution. From this normal distribution, I want to
> calculate 10 corresponding quantiles.
> 
> Then, I want to plot a qqplot of both data sets to see the
> distribution. 
> 
> One person told me to do this:
> 
> qnorm(c(0.25,0.5,0.75),mean=mean(x),sd=sd(x))
>  
> Output: 
> [1] 3.76997 5.50000 7.23003
> 
> ...But this does not give me 10 corresponding quantiles?
> 
> Another person told me to do this:
> 
>> > > x=c(104,95,106,105,110,150,101,98,85,104)
> 
>> > > n=length(x)
> 
>> > > p=(1:n-0.5)/n
> 
>> > > z=qnorm(p, mean(x), sd(x),)[order(order(x))]
> 
> But this seems to generate 10 new numbers. And not give corresponding
> quantiles from a normal distribution. 

Try:
qqnorm(x); qqline(x, col = 2)

....as suggested in the examples in the help message from ?qqplot.

If you want the quantiles, invoking str(qqnorm(x)) suggests that the "x-
values" can be recovered by:

 qqnorm(x)$x

 ....and therefore the quantiles from:

pnorm(qqnorm(x)$x)

-- 
David Winsemius


From dylan.beaudette at gmail.com  Mon Dec  3 01:29:37 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Sun, 2 Dec 2007 16:29:37 -0800
Subject: [R] plotting step functions in plot vs. xyplot
Message-ID: <3c5546140712021629t2ce52081l2ba64640c6e7a27b@mail.gmail.com>

Hi,

I have noticed an odd inconsistency when plotting a 'step' function
(type='s') in xyplot() vs. plot().

For example, given the following data:

## generate some profile depths: 0 - 150, in 10 cm increments
depth <- seq(0,150, by=10)
## generate some property: random numbers in this case
prop <- rnorm(n=length(depth), mean=15, sd=2)
## since the 0 is not a depth, and we would like the graph to start from 0
## make the first property row (associated with depth 0) the same as the second
## property row
prop[1] <- prop[2]
## combine into a table: data read in from a spread sheet would
already be in this format
soil <- data.frame(depth=depth, prop=prop)

## simple depth plot, as steps: looks good!
plot(depth ~ prop, data=soil, ylim=c(150,0), type='s', ylab='Depth',
xlab='Property', main='Property vs. Depth Plot')

## now try it with lattice graphics:yuck!
xyplot(depth ~ prop, data=soil, ylim=c(160,-5), type='s',
ylab='Depth', xlab='Property', main='Property vs. Depth Plot')

it looks like the data isn't rotated (?) correctly - i.e. the axis and
data do not match.

Now, am I mis-interpreting the meaning of type='s' in lattice graphics?

here is my R session info:
 sessionInfo()
R version 2.4.1 (2006-12-18)
i686-pc-linux-gnu

locale:
LC_CTYPE=en_US;LC_NUMERIC=C;LC_TIME=en_US;LC_COLLATE=en_US;LC_MONETARY=en_US;LC_MESSAGES=en_US;LC_PAPER=en_US;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US;LC_IDENTIFICATION=C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
  lattice
"0.14-17"

could it be that my version of R and lattice is just too old?

thanks in advance!

Dylan


From hoontaechung at gmail.com  Mon Dec  3 01:40:00 2007
From: hoontaechung at gmail.com (Tae-Hoon Chung)
Date: Mon, 3 Dec 2007 09:40:00 +0900
Subject: [R] question on 64-bit compiling in leopard
Message-ID: <ff71f6280712021640w4ccc40f6mddaa7be540c67d6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: ?????? ?? ????????.
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/362fc38f/attachment.pl 

From milton_ruser at yahoo.com.br  Mon Dec  3 02:16:09 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sun, 2 Dec 2007 17:16:09 -0800 (PST)
Subject: [R] Res:  fitting "power model" in nls()
Message-ID: <892561.15961.qm@web56008.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071202/b6d8a742/attachment.pl 

From jholtman at gmail.com  Mon Dec  3 02:54:13 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 2 Dec 2007 20:54:13 -0500
Subject: [R] speeding up likelihood computation
In-Reply-To: <110f9d445.d445110f9@osu.edu>
References: <110f9d445.d445110f9@osu.edu>
Message-ID: <644e1f320712021754o5cc72303me0d7d3d4c681bb7@mail.gmail.com>

One thing that I would suggest that you do is to use Rprof on a subset
of the data that runs for 10-15 minutes and see where some of the hot
spots are.  Since you have not provided commented, minimal,
self-contained, reproducible code, it is hard to determine where the
inefficiencies are since we don't have any data to run against it.
Some of the loop look like you are just assigning a value to a vector,
e.g.,


>    if (alive[j]==N1) {
>
>        for (i in 1:(N1-1)) {
>                S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
>        }
>    }
>
>    else {
>        for (i in 1:(N1-1)) {
>                S[N1,i] <- 0
>        }
>
>    }

that can be done without loops, but without data, it is hard to
determine.  Run Rprof and see what summary.Rprof shows to indicate
where to focus on.

On Dec 2, 2007 12:49 PM, DEEPANKAR BASU <basu.15 at osu.edu> wrote:
> R Users:
>
> I am trying to estimate a model of fertility behaviour using birth history data with maximum likelihood. My code works but is extremely slow (because of several for loops and my programming inefficiencies); when I use the genetic algorithm to optimize the likelihood function, it takes several days to complete (on a machine with Intel Core 2 processor [2.66GHz] and 2.99 GB RAM). Computing the hessian and using it to calculate the standard errors takes a large chunk of this time.
>
> I am copying the code for my likelihood function below; it would be great if someone could suggest any method to speed up the code (by avoiding the for loops or by any other method).
>
> I am not providing details of my model or what exactly I am trying to do in each step of the computation below; i would be happy to provide these details if they are deemed necessary for re-working the code.
>
> Thanks.
> Deepankar
>
>
> --------- begin code -----------------------
>
> LLK1 <- function(paramets, data.frame, ...) {  # DEFINING THE LOGLIKELIHOOD FUNCTION
>
> # paramets IS A 1x27 VECTOR OF PARAMETERS OVER WHICH THE FUNCTION WILL BE MAXIMISED
> # data.frame IS A DATA FRAME. THE DATA FRAME CONTAINS OBSERVATIONS ON SEVERAL VARIABLES
> # (LIKE EDUCATION, AGE, ETC.) FOR EACH RESPONDENT. COLUMNS REFER TO VARIABLES AND ROWS REFER
> # TO OBSERVATIONS.
>
> ########## PARAMETERS ###############################
>
> # alpha: interaction between son targeting and family size
> # beta : son targeting
> # gamma : family size
> # delta : a 1x6 vector of probabilities of male birth at various parities (q1, q2, q3, q4, q5, q6)
> # zeta : a 1x11 vector of conditional probabilities with zeta[1]=1 always
>
> alpha <- paramets[1]      # FIRST PARAMETER
> beta <- paramets[2:9]     # SECOND TO SEVENTH PARAMETER
> gamma <- paramets[10:16]
> delta <- paramets[17]
> zeta <- paramets[18:27]   # LAST 10 PARAMETERS
>
> ################# VARIABLES ###############################
> # READING IN THE VARIABLES IN THE DATA FRAME
> # AND RENAMING THEM FOR USE IN THE LIKELIHOOD FUNCTION
>
> everborn <- data.frame$v201
> alive <- data.frame$alive
> age <- data.frame$age
> edu <- data.frame$edu
> rural <- data.frame$rur
> rich <- data.frame$rich
> middle <- data.frame$middle
> poor <- data.frame$poor
> work <- data.frame$work
> jointfam <- data.frame$jfam
> contracep <- data.frame$contra
> hindu <- data.frame$hindu
> muslim <- data.frame$muslim
> scaste <- data.frame$scaste
> stribe <- data.frame$stribe
> obc <- data.frame$obc
> ucaste <- data.frame$ucaste
> N <- data.frame$dfsize
> indN <- data.frame$dfsize1  # INDICATOR FUNCTION THAT dfsize==alive
> nb <- data.frame$nboy
> ng <- data.frame$ngirl
> ncord1 <- data.frame$ncord1  # FIRST CHILD: BOY=0; GIRL=1
> ncord2 <- data.frame$ncord2  #SECOND CHILD: BOY=0; GIRL=1
> ncord3 <- data.frame$ncord3
> ncord4 <- data.frame$ncord4
> ncord5 <- data.frame$ncord5
> ncord6 <- data.frame$ncord6  # SIXTH CHILD: BOY=0; GIRL=1
>
>
>
> ######### POSITION OF i^th BOY ################################################
> boy1 <- data.frame$boy1     # BIRTH POSITION OF FIRST BOY (ZERO IF THE FAMILY HAS NO BOYS)
> boy2 <- data.frame$boy2     # BIRTH POSITION OF SECOND BOY (ZERO IF THE FAMILY HAS ONLY ONE BOY)
> boy3 <- data.frame$boy3
> boy4 <- data.frame$boy4
> boy5 <- data.frame$boy5
> boy6 <- data.frame$boy6     # BIRTH POSITION OF SIXTH BOY (ZERO IF THE FAMILY HAS ONLY FIVE BOYS)
>
>
> ######################## CONDITIONAL PROBABILITIES ##########################
> qq21 <- 1
>
> qq31 <- 1/(1+exp(zeta[1]))
> qq32 <- exp(zeta[1])/(1+exp(zeta[1]))
>
> qq41 <- 1/(1+exp(zeta[2])+exp(zeta[3]))
> qq42 <- exp(zeta[2])/(1+exp(zeta[2])+exp(zeta[3]))
> qq43 <- exp(zeta[3])/(1+exp(zeta[2])+exp(zeta[3]))
>
> qq51 <- 1/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> qq52 <- exp(zeta[4])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> qq53 <- exp(zeta[5])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> qq54 <- exp(zeta[6])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
>
> qq61 <- 1/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> qq62 <- exp(zeta[7])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> qq63 <- exp(zeta[8])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> qq64 <- exp(zeta[9])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> qq65 <- exp(zeta[10])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
>
> zeta1 <- c(qq21,qq31,qq32,qq41,qq42,qq43,qq51,qq52,qq53,qq54,qq61,qq62,qq63,qq64,qq65)
>
> #########################################################################
>
> n <- length(N)         # LENGTH OF SAMPLE; SIZE OF THE MAIN LOOP
>
> lglik <- numeric(n)    # INITIALIZING THE LOGLIKELIHOOD FUNCTION
>                       # CREATES A 1xn VECTOR OF ZEROS
>
>  for (j in 1:n) {      # START OF MAIN LOOP
>
>    S <- matrix(0, 6, 6)  # CREATE A 6x6 MATRIX OF ZEROS
>    y <- numeric(15)      # CREATE A 1x15 VECTOR OF ZEROS
>    N1 <- N[j]       # DESIRED FAMILY SIZE
>
>
>      q <- 1/(1+exp(delta))   # PROBABILITY OF MALE BIRTH
>
>
>    if (alive[j]==N1) {
>
>        for (i in 1:(N1-1)) {
>                S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
>        }
>    }
>
>    else {
>        for (i in 1:(N1-1)) {
>                S[N1,i] <- 0
>        }
>
>    }
>
> ######### CREATE A 1x6 VECTOR WITH POSITION OF BOYS WITHIN FAMILY
>      x <- c(boy1[j], boy2[j], boy3[j], boy4[j], boy5[j])
>
>      if (N1>1) {
>                     for (i in 1:(N1-1)) {
>                               if (alive[j]>x[i] & x[i]>0) {
>                                   S[N1,i] <- 0
>                               }
>                               if (x[i] == alive[j] ) {
>                                   S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
>                               }
>                     }
>      }
>
>   y <- c(S[2,1],S[3,1],S[3,2],S[4,1],S[4,2],S[4,3],S[5,1],S[5,2],S[5,3],S[5,4],S[6,1],S[6,2],S[6,3],S[6,4],S[6,5])
>
>
>   z1 <- c(age[j],edu[j],work[j],rural[j],poor[j],middle[j],hindu[j])         # DETERMINANTS OF FAMILY SIZE
>   z2 <- c(1,age[j],edu[j],work[j],contracep[j],rural[j],jointfam[j],hindu[j])  # DETERMINANTS OF SON TARGETING
>
>   t1 <- (indN[j])*((q^(nb[j]))*((1-q)^(ng[j])))*(exp(-exp(sum(z1*gamma)))*((exp(sum(z1*gamma)))^N1)*pnorm(-sum(z2*beta)))/factorial(N1)
>   t2 <- (sum(y*zeta1))*(exp(-exp(sum(z1*gamma) + alpha))*((exp(sum(z1*gamma) + alpha))^N1)*(1-pnorm(-sum(z2*beta)))/factorial(N1))
>   lglik[j] <- log(t1+t2)
>  }
>
>  return(-sum(lglik)) # RETURNING THE NEGATIVE OF THE LOGLIKELIHOOD
>                     # SUMMED OVER ALL OBSERVATIONS
>
>
> }
>
> ------------ end code ----------------------
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From tomfool at as220.org  Mon Dec  3 02:46:23 2007
From: tomfool at as220.org (tom sgouros)
Date: Sun, 02 Dec 2007 20:46:23 -0500
Subject: [R] documenting yoru progress
In-Reply-To: <971536df0712021609n73f36385s80dca485596003c1@mail.gmail.com> 
References: <20071202232420.3348AFAC8C4@as220.org>
	<971536df0712021557wd9f3ec1kf21a2ff5d16bbfa3@mail.gmail.com>
	<20071202235541.C9622FAC44B@as220.org>
	<971536df0712021609n73f36385s80dca485596003c1@mail.gmail.com>
Message-ID: <20071203014624.53CE5FAC8C2@as220.org>


Gabor Grothendieck <ggrothendieck at gmail.com> wrote:

> Try this:
> 
> > survey.write <- function(x) {
> +    print(match.call())
> +    x
> + }
> > out <- survey.write(pi+3)
> survey.write(x = pi + 3)

That's exactly what I need.  Thank you.

But now, another question.  I can't seem to get the value returned by
print() into a character variable.  I tried examining the relevant print
function, to see what it's printing, but there seems not to be a
print.call().  This is not fatal, since I can use print(), but is there
some obvious way to do this?  as.character doesn't do it.

More generally, is there a way to inspect an object of some class I
don't know about to find its members and methods?  I suspect that will
save me some posts like this in the future.

Thanks,

 -tom



> 
> 
> On Dec 2, 2007 6:55 PM, tom sgouros <tomfool at as220.org> wrote:
> >
> >
> > Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> > > If what you mean is that you have a file, test.R, of R commands
> > > and you are using source("test.R") and you wish to discover the
> > > name "test.R" without hard coding it in your file, then place this in
> > > test.R:
> > >
> > > ofile <- parent.frame(2)$ofile
> > >
> > > and ofile will be set to "test.R".  Note that the line shown should not
> > > be within a function or other local environment within the file but
> > > directly at top level.
> >
> > Thank you, that is an excellent hack, and I will use it carefully
> > (though often), but it isn't quite what I had in mind.
> >
> > In my script called test.R, there is a function called "survey.write",
> > which writes a report based on some survey results.  It is called
> > several times at the top level.  I would like the report to say how
> > "survey.write" was invoked, whether it was invoked as
> > "survey.write(res.raw,res.q)" or "survey.write(bus.raw,bus.q)".
> > Obviously I have access to the values of the arguments, but I want the
> > names of those arguments, or a string containing it all.
> >
> > Thanks,
> >
> >  -tom
> >
> >
> > >
> > > This is hack which may need to be modified if the internals of the
> > > source command change.
> > >
> > > On Dec 2, 2007 6:24 PM, Tom Sgouros <tomfool at as220.org> wrote:
> > > >
> > > > Hello all:
> > > >
> > > > I have a function that writes a fairly elaborate report based on some
> > > > survey data.  For documentation and bookkeeping purposes, I'd like to
> > > > write out in the report the function call that produced the report, or
> > > > at least enough information to help me recreate the steps that led to
> > > > that report.  I've been generating all the reports with scripts, in
> > > > order to be able to recreate the steps, but apart from the file name, I
> > > > don't yet have a way to match the report to the script that created it.
> > > >
> > > > Can anyone suggest easy ways to do this?  From within a function, is the
> > > > function call text available somehow, or the names of the arguments used
> > > > in the function invocation?
> > > >
> > > > Many thanks,
> > > >
> > > >  -tom
> > > >
> > > >
> > > > --
> > > >  ------------------------
> > > >  tomfool at as220 dot org
> > > >  http://sgouros.com
> > > >  http://whatcheer.net
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> >
> >
> > --
> >
> >  ------------------------
> >  tomfool at as220 dot org
> >  http://sgouros.com
> >  http://whatcheer.net
> >
> 


-- 
 ------------------------
 tomfool at as220 dot org
 http://sgouros.com  
 http://whatcheer.net


From ggrothendieck at gmail.com  Mon Dec  3 03:01:38 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 2 Dec 2007 21:01:38 -0500
Subject: [R] documenting yoru progress
In-Reply-To: <20071203014624.53CE5FAC8C2@as220.org>
References: <20071202232420.3348AFAC8C4@as220.org>
	<971536df0712021557wd9f3ec1kf21a2ff5d16bbfa3@mail.gmail.com>
	<20071202235541.C9622FAC44B@as220.org>
	<971536df0712021609n73f36385s80dca485596003c1@mail.gmail.com>
	<20071203014624.53CE5FAC8C2@as220.org>
Message-ID: <971536df0712021801h168e83del15340f4ebff3551b@mail.gmail.com>

Use format:

f <- function(x) {
	format(match.call())
}
f(pi + 3)



On Dec 2, 2007 8:46 PM, tom sgouros <tomfool at as220.org> wrote:
>
> Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
> > Try this:
> >
> > > survey.write <- function(x) {
> > +    print(match.call())
> > +    x
> > + }
> > > out <- survey.write(pi+3)
> > survey.write(x = pi + 3)
>
> That's exactly what I need.  Thank you.
>
> But now, another question.  I can't seem to get the value returned by
> print() into a character variable.  I tried examining the relevant print
> function, to see what it's printing, but there seems not to be a
> print.call().  This is not fatal, since I can use print(), but is there
> some obvious way to do this?  as.character doesn't do it.
>
> More generally, is there a way to inspect an object of some class I
> don't know about to find its members and methods?  I suspect that will
> save me some posts like this in the future.
>
> Thanks,
>
>  -tom
>
>
>
>
> >
> >
> > On Dec 2, 2007 6:55 PM, tom sgouros <tomfool at as220.org> wrote:
> > >
> > >
> > > Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > >
> > > > If what you mean is that you have a file, test.R, of R commands
> > > > and you are using source("test.R") and you wish to discover the
> > > > name "test.R" without hard coding it in your file, then place this in
> > > > test.R:
> > > >
> > > > ofile <- parent.frame(2)$ofile
> > > >
> > > > and ofile will be set to "test.R".  Note that the line shown should not
> > > > be within a function or other local environment within the file but
> > > > directly at top level.
> > >
> > > Thank you, that is an excellent hack, and I will use it carefully
> > > (though often), but it isn't quite what I had in mind.
> > >
> > > In my script called test.R, there is a function called "survey.write",
> > > which writes a report based on some survey results.  It is called
> > > several times at the top level.  I would like the report to say how
> > > "survey.write" was invoked, whether it was invoked as
> > > "survey.write(res.raw,res.q)" or "survey.write(bus.raw,bus.q)".
> > > Obviously I have access to the values of the arguments, but I want the
> > > names of those arguments, or a string containing it all.
> > >
> > > Thanks,
> > >
> > >  -tom
> > >
> > >
> > > >
> > > > This is hack which may need to be modified if the internals of the
> > > > source command change.
> > > >
> > > > On Dec 2, 2007 6:24 PM, Tom Sgouros <tomfool at as220.org> wrote:
> > > > >
> > > > > Hello all:
> > > > >
> > > > > I have a function that writes a fairly elaborate report based on some
> > > > > survey data.  For documentation and bookkeeping purposes, I'd like to
> > > > > write out in the report the function call that produced the report, or
> > > > > at least enough information to help me recreate the steps that led to
> > > > > that report.  I've been generating all the reports with scripts, in
> > > > > order to be able to recreate the steps, but apart from the file name, I
> > > > > don't yet have a way to match the report to the script that created it.
> > > > >
> > > > > Can anyone suggest easy ways to do this?  From within a function, is the
> > > > > function call text available somehow, or the names of the arguments used
> > > > > in the function invocation?
> > > > >
> > > > > Many thanks,
> > > > >
> > > > >  -tom
> > > > >
> > > > >
> > > > > --
> > > > >  ------------------------
> > > > >  tomfool at as220 dot org
> > > > >  http://sgouros.com
> > > > >  http://whatcheer.net
> > > > >
> > > > > ______________________________________________
> > > > > R-help at r-project.org mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.
> > > > >
> > > >
> > >
> > >
> > > --
> > >
> > >  ------------------------
> > >  tomfool at as220 dot org
> > >  http://sgouros.com
> > >  http://whatcheer.net
> > >
> >
>
>
> --
>
>  ------------------------
>  tomfool at as220 dot org
>  http://sgouros.com
>  http://whatcheer.net
>


From deepayan.sarkar at gmail.com  Mon Dec  3 03:01:58 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sun, 2 Dec 2007 18:01:58 -0800
Subject: [R] plotting step functions in plot vs. xyplot
In-Reply-To: <3c5546140712021629t2ce52081l2ba64640c6e7a27b@mail.gmail.com>
References: <3c5546140712021629t2ce52081l2ba64640c6e7a27b@mail.gmail.com>
Message-ID: <eb555e660712021801n1aebd985tba9186854ea56482@mail.gmail.com>

On 12/2/07, Dylan Beaudette <dylan.beaudette at gmail.com> wrote:
> Hi,
>
> I have noticed an odd inconsistency when plotting a 'step' function
> (type='s') in xyplot() vs. plot().
>
> For example, given the following data:
>
> ## generate some profile depths: 0 - 150, in 10 cm increments
> depth <- seq(0,150, by=10)
> ## generate some property: random numbers in this case
> prop <- rnorm(n=length(depth), mean=15, sd=2)
> ## since the 0 is not a depth, and we would like the graph to start from 0
> ## make the first property row (associated with depth 0) the same as the second
> ## property row
> prop[1] <- prop[2]
> ## combine into a table: data read in from a spread sheet would
> already be in this format
> soil <- data.frame(depth=depth, prop=prop)
>
> ## simple depth plot, as steps: looks good!
> plot(depth ~ prop, data=soil, ylim=c(150,0), type='s', ylab='Depth',
> xlab='Property', main='Property vs. Depth Plot')
>
> ## now try it with lattice graphics:yuck!
> xyplot(depth ~ prop, data=soil, ylim=c(160,-5), type='s',
> ylab='Depth', xlab='Property', main='Property vs. Depth Plot')
>
> it looks like the data isn't rotated (?) correctly - i.e. the axis and
> data do not match.
>
> Now, am I mis-interpreting the meaning of type='s' in lattice graphics?

Yes (though not through any fault of your own). Unile plot(), type='s'
in panel.xyplot() sorts the values before doing the steps. This
difference was not intentional (originally), but it's been around long
enough that I decided to make it a "feature" (i.e., document it) when
I discovered it.

You can still get what you want by sorting on the y-axis, and you can
do that by adding 'horizontal = TRUE'. The more general fix that I had
planned was to have panel.points() etc work as plot, so that you could
do

xyplot(..., type = "s", panel = panel.points)

It turns out that I haven't implemented that yet, but it should be in
the next update (but you need at least R 2.5.0 to use it).

-Deepayan

>
> here is my R session info:
>  sessionInfo()
> R version 2.4.1 (2006-12-18)
> i686-pc-linux-gnu
>
> locale:
> LC_CTYPE=en_US;LC_NUMERIC=C;LC_TIME=en_US;LC_COLLATE=en_US;LC_MONETARY=en_US;LC_MESSAGES=en_US;LC_PAPER=en_US;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US;LC_IDENTIFICATION=C
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> [7] "base"
>
> other attached packages:
>   lattice
> "0.14-17"
>
> could it be that my version of R and lattice is just too old?
>
> thanks in advance!
>
> Dylan
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dale.w.steele at gmail.com  Mon Dec  3 03:41:49 2007
From: dale.w.steele at gmail.com (Dale Steele)
Date: Sun, 2 Dec 2007 21:41:49 -0500
Subject: [R] Function to find boundary of an irregular sample?
Message-ID: <72e8303a0712021841x2211cb26t314865e779edffcd@mail.gmail.com>

Given a set of coordinates that form an irregular sampling area, is
there an R function to determine boundary points (coordinates defining
the limits of the area), either with or without user interaction ?

# for example, given the following irregular sampling area, how could
I define the boundary polygon?
library(SemiPar)
data(scallop)
plot(scallop$longitude, scallop$latitude)

Thanks.  --Dale


From r.turner at auckland.ac.nz  Mon Dec  3 04:27:26 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 3 Dec 2007 16:27:26 +1300
Subject: [R] Function to find boundary of an irregular sample?
In-Reply-To: <72e8303a0712021841x2211cb26t314865e779edffcd@mail.gmail.com>
References: <72e8303a0712021841x2211cb26t314865e779edffcd@mail.gmail.com>
Message-ID: <358F9A01-9768-4648-9BD6-0B1692F63B3C@auckland.ac.nz>


library(spatstat)
?ripras

Also, ``with user interaction'': ?clickpoly

HTH

	cheers,

		Rolf Turner

(Having said that, let me point out that it is a pretty dubious  
practice to
``let the data choose the window''.  The observation window is  
*always* determined
by separate considerations, and letting the data choose the window  
distorts the
information in the observed pattern.  Remember:  There is information  
in where
the data ***aren't*** as well as in where they are; to know where  
they aren't
you have to know the a priori determined observation window.)

		R. T.

On 3/12/2007, at 3:41 PM, Dale Steele wrote:

> Given a set of coordinates that form an irregular sampling area, is
> there an R function to determine boundary points (coordinates defining
> the limits of the area), either with or without user interaction ?
>
> # for example, given the following irregular sampling area, how could
> I define the boundary polygon?
> library(SemiPar)
> data(scallop)
> plot(scallop$longitude, scallop$latitude)
>
> Thanks.  --Dale
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


######################################################################
Attention:\ This e-mail message is privileged and confid...{{dropped:9}}


From m_olshansky at yahoo.com  Mon Dec  3 04:43:46 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Sun, 2 Dec 2007 19:43:46 -0800 (PST)
Subject: [R] creating conditional means
In-Reply-To: <4751DE6D.4070605@ucar.edu>
Message-ID: <569349.42841.qm@web32213.mail.mud.yahoo.com>

Following Gabor's suggestion, if x is your data.frame
you can do 

y <- x[x$month %in% c(3,4,5),]
aggregate(y[,4:6],list(y$hour),mean)

--- Sherri Heck <sheck at ucar.edu> wrote:

> Hi Gabor,
> 
> Thank you for your help.  I think I need to clarify
> a bit more.  I am 
> trying to say
> 
> average all 2pms for months march + april + may (for
> example). I hope this is clearer.  
> 
> here's a larger subset of my data set:
> 
> year, month, hour, co2(ppm), num1,num2
> 
> 2006 1 0 384.2055 14 14
> 2006 1 1 384.0304 14 14
> 2006 1 2 383.9672 14 14
> 2006 1 3 383.8452 14 14
> 2006 1 4 383.8594 14 14
> 2006 1 5 383.7318 14 14
> 2006 1 6 383.6439 14 14
> 2006 1 7 383.7019 14 14
> 2006 1 8 383.7487 14 14
> 2006 1 9 383.8376 14 14
> 2006 1 10 383.8684 14 14
> 2006 1 11 383.8301 14 14
> 2006 1 12 383.8058 14 14
> 2006 1 13 383.9419 14 14
> 2006 1 14 383.7876 14 14
> 2006 1 15 383.7744 14 14
> 2006 1 16 383.8566 14 14
> 2006 1 17 384.1014 14 14
> 2006 1 18 384.1312 14 14
> 2006 1 19 384.1551 14 14
> 2006 1 20 384.099 14 14
> 2006 1 21 384.1408 14 14
> 2006 1 22 384.3637 14 14
> 2006 1 23 384.1491 14 14
> 2006 2 0 384.7082 27 27
> 2006 2 1 384.6139 27 27
> 2006 2 2 384.7453 26 26
> 2006 2 3 384.9224 28 28
> 2006 2 4 384.8581 28 28
> 2006 2 5 384.9208 28 28
> 2006 2 6 384.9086 28 28
> 2006 2 7 384.837 28 28
> 2006 2 8 384.6163 27 27
> 2006 2 9 384.7406 28 28
> 2006 2 10 384.7468 28 28
> 2006 2 11 384.6992 28 28
> 2006 2 12 384.6388 28 28
> 2006 2 13 384.6346 28 28
> 2006 2 14 384.6037 28 28
> 2006 2 15 384.5295 28 28
> 2006 2 16 384.5654 28 28
> 2006 2 17 384.6466 28 28
> 2006 2 18 384.6344 28 28
> 2006 2 19 384.5911 28 28
> 2006 2 20 384.6084 28 28
> 2006 2 21 384.6318 28 28
> 2006 2 22 384.6181 27 27
> 2006 2 23 384.6087 27 27
> 
> 
> thanks you again for your assistance-
> 
> s.heck
> 
> 
> Gabor Grothendieck wrote:
> > Try aggregate:
> >
> >
> > Lines <- "Year Month Hour co2 num1 num2
> >  2006   11    0 383.3709   28   28
> >  2006   11    1 383.3709   28   28
> >  2006   11    2 383.3709   28   28
> >  2006   11    3 383.3709   28   28
> >  2006   11    4 383.3709   28   28
> >  2006   11    5 383.3709   28   28
> >  2006   11    6 383.3709   28   28
> >  2006   11    7 383.3709   28   28
> >  2006   11    8 383.3709   28   28
> >  2006   11    9 383.3709   27   27
> >  2006   11   10 383.3709   28   28
> > "
> > DF <- read.table(textConnection(Lines), header =
> TRUE)
> > aggregate(DF[4:6],
> >    with(DF, data.frame(Year, Qtr = (Month - 1) %/%
> 3 + 1, Hour)),
> >    mean)
> >
> > On Dec 1, 2007 3:57 PM, Sherri Heck
> <sheck at ucar.edu> wrote:
> >   
> >> Hi all-
> >>
> >> I have a dataset (year, month, hour, co2(ppm),
> num1,num2)
> >>
> >>
> >> [49,] 2006   11    0 383.3709   28   28
> >> [50,] 2006   11    1 383.3709   28   28
> >> [51,] 2006   11    2 383.3709   28   28
> >> [52,] 2006   11    3 383.3709   28   28
> >> [53,] 2006   11    4 383.3709   28   28
> >> [54,] 2006   11    5 383.3709   28   28
> >> [55,] 2006   11    6 383.3709   28   28
> >> [56,] 2006   11    7 383.3709   28   28
> >> [57,] 2006   11    8 383.3709   28   28
> >> [58,] 2006   11    9 383.3709   27   27
> >> [59,] 2006   11   10 383.3709   28   28
> >>
> >> that repeats in this style for each month.  I
> would like to compute the
> >> mean for each hour in three month intervals.
> >> i.e.  average all 2pms for each day for months
> march, april and may. and
> >> then do this for each hour interval.
> >> i have been messing around with 'for loops' but
> can't seem to get the
> >> output I want.
> >>
> >> thanks in advance for any help-
> >>
> >> s.heck
> >> CU, Boulder
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained,
> reproducible code.
> >>
> >>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From jsorkin at grecc.umaryland.edu  Mon Dec  3 04:51:24 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sun, 02 Dec 2007 22:51:24 -0500
Subject: [R] Rating R Helpers
In-Reply-To: <B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
	<B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
Message-ID: <475336DE.91DF.00CB.0@grecc.umaryland.edu>

I believe we need to know the following about packages:
(1) Does the package do what it purports to do, i.e. are the results valid?
(2) Have the results generated by the package been validate against some other statistical package, or hand-worked example?
(3) Are the methods used in the soundly based?
(4) Does the package documentation refer to referred papers or textbooks?
(5) In addition to the principle result, does the package return ancillary values that allow for proper interpretation of the main result, (e.g. lm gives estimates of the betas and their SEs, but also generates residuals)?.
(6) Is the package easy to use, i.e. do the parameters used when invoking the package chosen so as to allow the package to be flexible?
(7) Are the error messages produced by the package helpful?
(8) Does the package conform to standards of R coding and good programming principles in general?
(9) Does the package interact will with the larger R environment, e.g. does it have a plot method etc.?
(10) Is the package well documented internally, i.e. is the code easy to follow, are the comments in the code adequate?
(11) Is the package well documented externally, i.e. through man pages and perhaps other documentation (e.g. MASS and its associated textbook)?

In addition to package evaluation and reviews, we also need some plan for the future of R. Who will maintain, modify, and extend packages after the principle author, or authors, retire? Software is never "done". Errors need to be corrected, programs need to be modified to accommodate changes in software and hardware. I have reasonable certainty that commercial software (e.g. SAS) will be available in 10-years (and that PROC MIXED will still be a part of SAS). I am far less sanguine about any number of R packages.
John 



John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)

>>> <Bill.Venables at csiro.au> 12/1/2007 2:21 AM >>>
This seems a little impractical to me.  People respond so much at random
and most only tackle questions with which they feel comfortable.  As
it's not a competition in any sense, it's going to be hard to rank
people in any effective way.  But suppose you succeed in doing so, then
what?

To me a much more urgent initiative is some kind of user online review
system for packages, even something as simple as that used by Amazon.com
has for customer review of books.

I think the need for this is rather urgent, in fact.  Most packages are
very good, but I regret to say some are pretty inefficient and others
downright dangerous.  You don't want to discourage people from
submitting their work to CRAN, but at the same time you do want some
mechanism that allows users to relate their experience with it, good or
bad.   


Bill Venables
CSIRO Laboratories
PO Box 120, Cleveland, 4163
AUSTRALIA
Office Phone (email preferred): +61 7 3826 7251
Fax (if absolutely necessary):  +61 7 3826 7304
Mobile:                         +61 4 8819 4402
Home Phone:                     +61 7 3286 7700
mailto:Bill.Venables at csiro.au 
http://www.cmis.csiro.au/bill.venables/ 

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] 
On Behalf Of Doran, Harold
Sent: Saturday, 1 December 2007 6:13 AM
To: R Help
Subject: [R] Rating R Helpers

Since R is open source and help may come from varied levels of
experience on R-Help, I wonder if it might be helpful to construct a
method that can be used to "rate" those who provide help on this list.

This is something that is done on other comp lists, like
http://www.experts-exchange.com/.

I think some of the reasons for this are pretty transparent, but I
suppose one reason is that one could decide to implement the advise of
those with "superior" or "expert" levels. In other words, you can trust
the advice of someone who is more experienced more than someone who is
not. Currently, there is no way to discern who on this list is really an
R expert and who is not. Of course, there is R core, but most people
don't actually know who these people are (at least I surmise that to be
true).

If this is potentially useful, maybe one way to begin the development of
such ratings is to allow the original poster to "rate" the level of help
from those who responded. Maybe something like a very simple
questionnaire on a likert-like scale that the original poster would
respond to upon receiving help which would lead to the accumulation of
points for the responders. Higher points would result in higher levels
of expertise (e.g., novice, ..., wizaRd).

Just a random thought. What do others think?

Harold




	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.

Confidentiality Statement:
This email message, including any attachments, is for th...{{dropped:6}}


From hoontaechung at gmail.com  Mon Dec  3 05:08:14 2007
From: hoontaechung at gmail.com (Tae-Hoon Chung)
Date: Mon, 3 Dec 2007 13:08:14 +0900
Subject: [R] trouble while installing 64-bit R in leopard
Message-ID: <ff71f6280712022008o6a30133fgdb9aa5757013e994@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: ?????? ?? ????????.
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/b96cb1fb/attachment.pl 

From dylan.beaudette at gmail.com  Mon Dec  3 05:31:58 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Sun, 2 Dec 2007 20:31:58 -0800
Subject: [R] plotting step functions in plot vs. xyplot
In-Reply-To: <eb555e660712021801n1aebd985tba9186854ea56482@mail.gmail.com>
References: <3c5546140712021629t2ce52081l2ba64640c6e7a27b@mail.gmail.com>
	<eb555e660712021801n1aebd985tba9186854ea56482@mail.gmail.com>
Message-ID: <200712022031.59145.dylan.beaudette@gmail.com>

On Sunday 02 December 2007 06:01:58 pm Deepayan Sarkar wrote:
> On 12/2/07, Dylan Beaudette <dylan.beaudette at gmail.com> wrote:
> > Hi,
> >
> > I have noticed an odd inconsistency when plotting a 'step' function
> > (type='s') in xyplot() vs. plot().
> >
> > For example, given the following data:
> >
> > ## generate some profile depths: 0 - 150, in 10 cm increments
> > depth <- seq(0,150, by=10)
> > ## generate some property: random numbers in this case
> > prop <- rnorm(n=length(depth), mean=15, sd=2)
> > ## since the 0 is not a depth, and we would like the graph to start from
> > 0 ## make the first property row (associated with depth 0) the same as
> > the second ## property row
> > prop[1] <- prop[2]
> > ## combine into a table: data read in from a spread sheet would
> > already be in this format
> > soil <- data.frame(depth=depth, prop=prop)
> >
> > ## simple depth plot, as steps: looks good!
> > plot(depth ~ prop, data=soil, ylim=c(150,0), type='s', ylab='Depth',
> > xlab='Property', main='Property vs. Depth Plot')
> >
> > ## now try it with lattice graphics:yuck!
> > xyplot(depth ~ prop, data=soil, ylim=c(160,-5), type='s',
> > ylab='Depth', xlab='Property', main='Property vs. Depth Plot')
> >
> > it looks like the data isn't rotated (?) correctly - i.e. the axis and
> > data do not match.
> >
> > Now, am I mis-interpreting the meaning of type='s' in lattice graphics?

Thanks for getting back so quick!

> Yes (though not through any fault of your own). Unile plot(), type='s'
> in panel.xyplot() sorts the values before doing the steps. This
> difference was not intentional (originally), but it's been around long
> enough that I decided to make it a "feature" (i.e., document it) when
> I discovered it.
>
> You can still get what you want by sorting on the y-axis, and you can
> do that by adding 'horizontal = TRUE'. The more general fix that I had
> planned was to have panel.points() etc work as plot, so that you could
> do
>
> xyplot(..., type = "s", panel = panel.points)
>
> It turns out that I haven't implemented that yet, but it should be in
> the next update (but you need at least R 2.5.0 to use it).
>
> -Deepayan

Thanks! I just updated to today's 2.6.1 and... it looks like adding 
horizontal=TRUE makes it work as expected (by me).

Looking forward to the planned fix,

cheers,

Dylan


> > here is my R session info:
> >  sessionInfo()
> > R version 2.4.1 (2006-12-18)
> > i686-pc-linux-gnu
> >
> > locale:
> > LC_CTYPE=en_US;LC_NUMERIC=C;LC_TIME=en_US;LC_COLLATE=en_US;LC_MONETARY=en
> >_US;LC_MESSAGES=en_US;LC_PAPER=en_US;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C
> >;LC_MEASUREMENT=en_US;LC_IDENTIFICATION=C
> >
> > attached base packages:
> > [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> > [7] "base"
> >
> > other attached packages:
> >   lattice
> > "0.14-17"
> >
> > could it be that my version of R and lattice is just too old?
> >
> > thanks in advance!
> >
> > Dylan
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.


From m_olshansky at yahoo.com  Mon Dec  3 05:39:36 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Sun, 2 Dec 2007 20:39:36 -0800 (PST)
Subject: [R] How to cbind DF:s with differing number of rows?
In-Reply-To: <ba8c09910712010505g8facc58h8645b5d98627f192@mail.gmail.com>
Message-ID: <769097.89667.qm@web32204.mail.mud.yahoo.com>

Hi Lauri,

I see two possibilities.

Let say that you have
a <-c(1:5)
b <- c(1:7)
c <- c(1:4)
l <- list(a,b,c)

and you want to create an Excel file with columnA (1)
containing a (5 rows), column B (2) containing b and
column C containing c.

One possibility would be to write each ROW of the
output file separately (7 rows in your case) writing
nothing (just \t ) when you reach the end of the
particular item in the list. 

Another possibility would be to use xlsReadWritePro
package which allows to write each item (a,b,c) (or
list element - l[[1]],l[[2]],... etc) to a given
column of the output file.

Regards,

Moshe.

--- Lauri Nikkinen <lauri.nikkinen at iki.fi> wrote:

> #Hi R-users,
> #Suppose that I have a data.frame like this:
> 
> y1 <- rnorm(10) + 6.8
> y2 <- rnorm(10) + (1:10*1.7 + 1)
> y3 <- rnorm(10) + (1:10*6.7 + 3.7)
> y <- c(y1,y2,y3)
> x <- rep(1:3,10)
> f <- gl(2,15, labels=paste("lev", 1:2, sep=""))
> g <- seq(as.Date("2000/1/1"), by="day", length=30)
> DF <- data.frame(x=x,y=y, f=f, g=g)
> DF$g[DF$x == 1] <- NA
> DF$x[3:6] <- NA
> DF$wdays <- weekdays(DF$g)
> 
> DF
> 
> #For EDA purposes, I would like to calculate
> frequences in each variable
> g <- lapply(DF, function(x) as.data.frame(table(x)))
> 
> #After this, I would like to cbind these data.frames
> (in g) into a
> single data.frame (which to export to MS Excel)
> #do.call(cbind, g) does not seem to work because of
> the different
> number of rows in each data.frame.
> #The resulting data.frame shoul look like this (only
> two variables
> printed here):
> 
> Rowid;x;Freq.x;y;Freq.y; # etc...
> 1;1;9;1.69151845313816;1;
> 2;2;9;5.03748767699799;1;
> 3;3;8;5.37387749444247;1;
> 4;Empty;Empty;6.83926626214299;1;
> 5;Empty;Empty;6.97484558968873;1;
> 6;Empty;Empty;7.11023821708323;1;
> 7;Empty;Empty;7.1348316549091;1;
> 8;Empty;Empty;7.16727166992407;1;
> 9;Empty;Empty;7.35983428577469;1;
> 10;Empty;Empty;7.7596470136235;1;
> 11;Empty;Empty;7.86369414967578;1;
> 12;Empty;Empty;7.97164674771006;1;
> 13;Empty;Empty;8.0787295301318;1;
> 14;Empty;Empty;8.14161030348166;1;
> 15;Empty;Empty;8.20134832959661;1;
> 16;Empty;Empty;10.1469115339016;1
> 17;Empty;Empty;12.7442067301746;1
> 18;Empty;Empty;14.0865167751202;1
> 19;Empty;Empty;15.8280312307450;1
> 20;Empty;Empty;16.0484499360756;1
> 21;Empty;Empty;17.0795222149999;1
> 22;Empty;Empty;18.1254057823357;1
> 23;Empty;Empty;22.7169729331525;1
> 24;Empty;Empty;30.7237748005358;1
> 25;Empty;Empty;37.2141271786934;1
> 26;Empty;Empty;44.4954633229803;1
> 27;Empty;Empty;50.2302409305761;1
> 28;Empty;Empty;57.8913405112114;1
> 29;Empty;Empty;64.849897477945;1
> 30;Empty;Empty;71.4205263353053;1
> 
> 
> #Anyone have an idea how to do this?
> 
> #Thanks,
> #Lauri
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From petr.pikal at precheza.cz  Mon Dec  3 09:30:26 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Mon, 3 Dec 2007 09:30:26 +0100
Subject: [R] Help with tables
In-Reply-To: <36923f1d0712011816n7b47574dl59b99f116cfa5ba8@mail.gmail.com>
Message-ID: <OFA8D61202.7C9DFE4F-ONC12573A6.002E7813-C12573A6.002EB42D@precheza.cz>

Hi

I am not sure but if I remember correctly I had seen similar output in 
Frank Harrel's book. So you could check some function from Hmisc package, 
probably summarize.

Regards

Petr
petr.pikal at precheza.cz

r-help-bounces at r-project.org napsal dne 02.12.2007 03:16:09:

> I guess you can get the result by
> 1) concatenating all the variables (P2_A, P2_B, P2_C) into one variable 
,
> 2) replicating segment membership properly,
> 3) make the table of 1) and 2)
> 
> For example, the following may do the job.
> 
> > ## (1) Generate data set
> > # Set random seed
> > set.seed(0)
> >
> > n.obs <- 15 # Number of patients
> > disease.lv  <- 1:10 ## different types of disease
> > data1 <- as.data.frame(t(replicate(n.obs, sample(disease.lv, 3)))) #
> Create data set
> > ## Create segment membership
> > segment <- sample(LETTERS[1:3], n.obs, replace=TRUE)
> >
> > cbind(data1, segment)
>    V1 V2 V3 segment
> 1   9  3 10       B
> 2   6  9  2       C
> 3   9 10  6       A
> 4   7  1  2       B
> 5   2  7  4       C
> 6   8  5  6       C
> 7  10  4  7       B
> 8  10  2  6       C
> 9   2  3  4       B
> 10  1  4  7       A
> 11  4  5  9       A
> 12  5  2  7       A
> 13  7  8  1       A
> 14  8  4  7       B
> 15  7  8  5       B
> >
> >
> > ## (2) Table
> > infec.all <- unlist(data1) # Concatenate all variables into one 
variable
> > segment.all <- rep(segment, ncol(data1)) # Replicate the segment
> membership as necessary
> > table(infec.all, segment.all)
>          segment.all
> infec.all A B C
>        1  2 1 0
>        2  1 2 3
>        3  0 2 0
>        4  2 3 1
>        5  2 1 1
>        6  1 0 3
>        7  3 4 1
>        8  1 2 1
>        9  2 1 1
>        10 1 2 1
> 
> 
> 
> On Nov 30, 2007 10:10 AM, Alejandro Rodr?guez 
<rodrigueza at schwabe.com.mx>
> wrote:
> 
> > Hello,  I'm new using R and developing tables.
> >
> > I have a problem in developing a table.  In a questionaire I made I 
ask
> > this
> > question "Please tell me the first three sympthoms caused by 
Respiratory
> > tract infection you've caught this year", then the people answer three
> > sympthoms, the first mention (Top of mind) is saved in a variable 
called
> > "P2_A", the second mention in a variable called "P2_B" and the third
> > mention
> > in "P2_C".  Each answer is coded with numbers like this:
> >
> > 1 = Flu
> > 2 = Cough
> > 3 = Asthma
> > ....
> >
> > 13 = Fever
> >
> > I've already done a K-cluster analysis and segmented my data base.  So 
my
> > first task is to develop tables to develop tables of frequencies 
crossing
> > Cluster vs. "P2_A" in order to know which are the top of mind products 
and
> > their frequencies by cluster, then the second mention and third 
mention.
> > I've used this instruction which worked well:
> >
> > > table(infec1,aglomera)
> >      aglomera
> > infec1   1   2   3   4
> >    1  117  88  76  83
> >    2   10  10   9   7
> >    3   15  11  14  14
> >    4    2   0   1   1
> >    5    2   3   1   0
> >    6    1   0   1   0
> >    8    3   3   0   1
> >    9    3   1   1   0
> >    11   0   0   1   1
> >
> > Where "infec1" is a factor of "P2_A" and "aglomera" is a factor of the
> > variable "Cluster" I made.  It worked well when I study them
> > separately...however I would like to know the TOTAL mentions of each
> > sympthom by cluster.  I've done this exercise in SPSS using the 
"Multiple
> > Response" instruction first grouping my three variables (i.e. "P2_A",
> > "P2_B"
> > and "P2_C") into a variable called "sick" and cross tabulating it vs.
> > QCL_1
> > (my cluster variable) and it gave me the table I need in this way 
(showed
> > at
> > the bottom of this mail):
> >
> > How can I made a table like this in R???.  I've tried combining my
> > variables
> > in a matrix and using xtabs, ftable, table, structable and a lot of
> > combination of them but I haven't had succed with any of them.
> >
> > Please help me with this issue, I don't want to keep using SPSS any 
more.
> >
> > Thanx in advance.
> >
> > P.D. Result from SPSS is shown below.
> >
> >
> >
> >                * * *  C R O S S T A B U L A T I O N  * * *
> >
> >   $SICK (group)  mr sick
> > by QCL_1  Cluster Number of Case
> >
> >
> >                       QCL_1
> >
> >                Count  I
> >                       I                                      Row
> >                       I                                     Total
> >                       I     1  I     2  I     3  I     4  I
> > $SICK          --------+--------+--------+--------+--------+
> >                    1  I   130  I    97  I    83  I    89  I   399
> >  Gripe, influenza, ca I        I        I        I        I  83.1
> >                       +--------+--------+--------+--------+
> >                    2  I    53  I    55  I    42  I    46  I   196
> >  Tos de cualquier tip I        I        I        I        I  40.8
> >                       +--------+--------+--------+--------+
> >                    3  I    33  I    36  I    36  I    39  I   144
> >  Dolor irritaci?n     I        I        I        I        I  30.0
> >                       +--------+--------+--------+--------+
> >                    4  I     5  I     1  I     2  I     3  I    11
> >  Bronquitis           I        I        I        I        I   2.3
> >                       +--------+--------+--------+--------+
> >                    5  I     5  I     4  I     1  I     0  I    10
> >  Sinusitis            I        I        I        I        I   2.1
> >                       +--------+--------+--------+--------+
> >                    6  I     1  I     1  I     1  I     3  I     6
> >  Rinitis              I        I        I        I        I   1.3
> >                       +--------+--------+--------+--------+
> >                    8  I     8  I     6  I     4  I     4  I    22
> >  Amigdalitis          I        I        I        I        I   4.6
> >                       +--------+--------+--------+--------+
> >                    9  I     6  I     4  I     1  I     2  I    13
> >  Faringitis           I        I        I        I        I   2.7
> >                       +--------+--------+--------+--------+
> >                   10  I     1  I     2  I     2  I     3  I     8
> >  Laringitis           I        I        I        I        I   1.7
> >                       +--------+--------+--------+--------+
> >                   11  I     1  I     1  I     1  I     1  I     4
> >  Neumonia             I        I        I        I        I    .8
> >                       +--------+--------+--------+--------+
> >                   13  I     0  I     0  I     1  I     0  I     1
> >  Asma                 I        I        I        I        I    .2
> >                       +--------+--------+--------+--------+
> >               Column      153      116      104      107      480
> >                Total     31.9     24.2     21.7     22.3    100.0
> >
> > Percents and totals based on respondents
> >
> > 480 valid cases;  0 missing cases
> >
> >
> >
> > Act. Calef Alejandro Rodr?guez Cuevas
> > Analista de mercado
> >
> > Laboratorios Farmasa S.A. de C.V.
> > Schwabe Mexico, S.A. de C.V.
> >
> > Bufalo Nr. 27
> > Col. del Valle 03100
> > Mexico, D.F.
> > Mexico
> >
> > Tel. 52 00 26 80
> > email: rodrigueza at schwabe.com.mx
> >
> > www.schwabe.com.mx
> > www.umckaloabo.com.mx
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> 
> -- 
> ======================================
> T.K. (Tae-kyun) Kim
> Ph.D. student
> Department of Marketing
> Marshall School of Business
> University of Southern California
> ======================================
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From alfieim21 at hotmail.co.uk  Sun Dec  2 19:07:00 2007
From: alfieim21 at hotmail.co.uk (jimbib webber)
Date: Sun, 2 Dec 2007 18:07:00 +0000
Subject: [R] Quantiles from vectors
In-Reply-To: <644e1f320712011945x4495de41wbf94a74863ddea00@mail.gmail.com>
References: <14086120.post@talk.nabble.com>
	<BLU108-W3907FA55DD407F04CA15DEF2720@phx.gbl>
	<644e1f320712010701s914060dg3558b94043203ada@mail.gmail.com>
	<BLU108-W30003C1C359962848525EF2720@phx.gbl>
	<644e1f320712010803i4eaff8fbpcaa049f0f79c4d92@mail.gmail.com>
	<BLU108-W2A8A9277FC558D1FED1C6F2720@phx.gbl>
	<644e1f320712010831i7446cc07gd57ac061ed997d93@mail.gmail.com>
	<BLU108-W2625552B3523F22AD5D2BEF2720@phx.gbl>
	<644e1f320712010840x1b197f08s14430cf8d1dbe409@mail.gmail.com>
	<BLU108-W1004C9A583B1779AAFE909F2730@phx.gbl> 
	<644e1f320712011945x4495de41wbf94a74863ddea00@mail.gmail.com>
Message-ID: <BLU108-W4995DC377BC79204AC5417F2730@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071202/48248f40/attachment.pl 

From bussia89 at yahoo.com  Sun Dec  2 22:27:20 2007
From: bussia89 at yahoo.com (Nathan Vandergrift)
Date: Sun, 2 Dec 2007 13:27:20 -0800 (PST)
Subject: [R] Controlling Postscript output, size and orientation
In-Reply-To: <20071202190325.GF6584@slingshot.co.nz>
References: <14035096.post@talk.nabble.com>
	<20071202190325.GF6584@slingshot.co.nz>
Message-ID: <14120261.post@talk.nabble.com>




Patrick Connolly-4 wrote:
> 
> On Thu, 29-Nov-2007 at 01:22PM -0800, Nathan Vandergrift wrote:
> 
> |> 
> |> I'm trying to get my graphics so that I can use them in LaTeX to create
> (via
> |> ) a pdf presentation.
> |> 
> |> I've tried controlling inner and outer margins and figure size using
> par(),
> |> to no avail. The ps output keeps appearing as a portrait page with a
> |> centered figure. Nothing I have been able to do so far has changed
> that.
> 
> Check out the paper argument to the postscript device.  I think you'll
> be more sucessful.
> 

The issue isn't so much viewing is gsview (I've looked at previous threads
on this and all my settings in gsview are the ones recommended), but
creating a postscript file that is ready to be dumped into the LaTeX prosper
package and have a good looking graph for a presentation. Currently, the
graph comes out with lots of "white space" on a portrait oriented page.

My work around has been to open the file in Adobe and to crop the file
(interestingly, when Adobe opens the file, it does not read in the excess
"white space"). This works fine, but it is pretty inefficient.

I find it hard to believe that I can't control these things in R, but I have
been unable to so using the reference manual and this site.

Trying to do it with lattice plots is even worse...

Using curve, line, and plot, I should be able to control these things using
par(). In a lattice environment, I should be able to control these things
using par.settings().

Oh, well, I'll keep plugging away...




-----
-------------------------------
Project Scientist
University of California, Irvine
-- 
View this message in context: http://www.nabble.com/Controlling-Postscript-output%2C-size-and-orientation-tf4899986.html#a14120261
Sent from the R help mailing list archive at Nabble.com.


From ebballller1584 at yahoo.com  Sun Dec  2 23:33:47 2007
From: ebballller1584 at yahoo.com (stathelp)
Date: Sun, 2 Dec 2007 14:33:47 -0800 (PST)
Subject: [R]  Help with a Loop
Message-ID: <14120625.post@talk.nabble.com>


I am having trouble getting a loop to work for the following problem. Any
help would be much appreciated. Thanks.

I need to find the slope and intercept from the linear regression of Drug
Level on Day by Participant. There are a total of 37 Participants. I need to
store the Participant, Label, Slope, and Intercept in a new data frame. 


This data is ordered by Participant number 37 total participants. A sample
of the data is given below. 


Label Participant Day DrugLevel 
17      0       1  15 1.84179  
121     0       1   5 2.10772  
147     0       1   7 3.00658  
152     0       1  11 2.91729  
250     0       1  10 2.75816  
289     0       1  13 3.20468  
321     0       1   6 2.43389  
362     0       1  12 2.77770  
433     0       1   9 3.03167  
469     0       1   8 2.97613  
475     0       1  14 2.86934  
70      0       2  13 0.68022  
210     0       2   8 1.41767  
243     0       2  11 1.28867  
246     0       2   9 1.53601  
247     0       2   6 1.64863  
280     0       2   5 1.19795  
310     0       2  12 1.24440  
343     0       2  10 1.18929  
413     0       2   7 1.57207  
41      0       3   7 1.87884  
74      0       3   8 1.82477  
100     0       3   5 2.09422  
133     0       3   6 1.91853  
134     0       3  12 0.90422  
149     0       3  11 1.38232  
172     0       3  10 1.55323  
216     0       3   9 1.24088  
65      0       4   8 2.49412  
69      0       4   5 1.79840  

This is my thought process of what the loop needs to do but I cant get the
correct loop. 

X1=Day[Participant=="1"]
Y1=DrugLevel[Participant=="1"]

Coeffs=function(X1,Y1)
{
lmfirst=lm(Y1~X1)
lmfirst$coefficients
}

Coeffs(X1,Y1)

# output slope and intercept here
# do same for the next participant

X2=Day[Participant=="2"]
Y2=DrugLevel[Participant=="2"]

Coeffs=function(X2,Y2)
{
lmfirst=lm(Y2~X2)
lmfirst$coefficients
}

Coeffs(X2,Y2)

# output slope and intercept here
# do same for the next participant

X3=Day[Participant=="3"]
Y3=DrugLevel[Participant=="3"]

Coeffs=function(X3,Y3)
{
lmfirst=lm(Y3~X3)
lmfirst$coefficients
}

Coeffs(X3,Y3)

# output slope and intercept here
# do same for the next participant

# etc for the rest of the participants

# any ideas? 

# thanks 

-- 
View this message in context: http://www.nabble.com/Help-with-a-Loop-tf4933354.html#a14120625
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Mon Dec  3 10:02:45 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 3 Dec 2007 09:02:45 +0000 (GMT)
Subject: [R] Controlling Postscript output, size and orientation
In-Reply-To: <14120261.post@talk.nabble.com>
References: <14035096.post@talk.nabble.com>
	<20071202190325.GF6584@slingshot.co.nz>
	<14120261.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0712030856560.1873@gannet.stats.ox.ac.uk>

Please do tell us exactly what you are doing via a reproducible example 
(see the footer to every R-help message).

I added paper="special" to postscript() to make this easier: are you using 
it?  From the help page

      The postscript produced for a single R plot is EPS (_Encapsulated
      PostScript_) compatible, and can be included into other documents,
      e.g., into LaTeX, using '\includegraphics{<filename>}'.  For use
      in this way you will probably want to set 'horizontal = FALSE,
      onefile = FALSE, paper = "special"'.  Note that the bounding box
      is for the device region: if you find the white space around the
      plot region excessive, reduce the margins of the figure region via
      'par(mar=)'.

Further, I wrote a pdf() driver to make this easier, so why use 
postscript) to make a PDF presentation?

'Adobe' is a company, not a software package.  Which of its products did 
you mean?


On Sun, 2 Dec 2007, Nathan Vandergrift wrote:

> Patrick Connolly-4 wrote:
>>
>> On Thu, 29-Nov-2007 at 01:22PM -0800, Nathan Vandergrift wrote:
>>
>> |>
>> |> I'm trying to get my graphics so that I can use them in LaTeX to create
>> (via
>> |> ) a pdf presentation.
>> |>
>> |> I've tried controlling inner and outer margins and figure size using
>> par(),
>> |> to no avail. The ps output keeps appearing as a portrait page with a
>> |> centered figure. Nothing I have been able to do so far has changed
>> that.
>>
>> Check out the paper argument to the postscript device.  I think you'll
>> be more sucessful.
>>
>
> The issue isn't so much viewing is gsview (I've looked at previous threads
> on this and all my settings in gsview are the ones recommended), but
> creating a postscript file that is ready to be dumped into the LaTeX prosper
> package and have a good looking graph for a presentation. Currently, the
> graph comes out with lots of "white space" on a portrait oriented page.
>
> My work around has been to open the file in Adobe and to crop the file
> (interestingly, when Adobe opens the file, it does not read in the excess
> "white space"). This works fine, but it is pretty inefficient.
>
> I find it hard to believe that I can't control these things in R, but I have
> been unable to so using the reference manual and this site.

Perhaps reading the help pages would solve this?  See the quote above.

> Trying to do it with lattice plots is even worse...
>
> Using curve, line, and plot, I should be able to control these things using
> par(). In a lattice environment, I should be able to control these things
> using par.settings().
>
> Oh, well, I'll keep plugging away...
>
>
>
>
> -----
> -------------------------------
> Project Scientist
> University of California, Irvine
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From johannes_graumann at web.de  Mon Dec  3 10:02:37 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 03 Dec 2007 10:02:37 +0100
Subject: [R] Attempt at package documentation on debian: link to
	"data.frame" broken
Message-ID: <fj0gn5$fcu$1@ger.gmane.org>

Hello,

I'm trying to document this little package I'm working on and have the
following issue:
one of the man files (*.Rd) contains this bit

\value{
  Returns a \code{\link[base]{data.frame}} representing the tabular data.
}

The resulting link is broken on my debian system. It points
to "file:///usr/local/lib/R/site-library/base/html/data.frame.html", while
the real file resides at "/usr/lib/R/library/base/html/data.frame.html".

Can someone please let me know how I would fix this?

Thanks, Joh


From petr.pikal at precheza.cz  Mon Dec  3 10:07:58 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Mon, 3 Dec 2007 10:07:58 +0100
Subject: [R] Odp:   Help with a Loop
In-Reply-To: <14120625.post@talk.nabble.com>
Message-ID: <OF8BE19E15.B6B1B3C2-ONC12573A6.0031CFAA-C12573A6.00322402@precheza.cz>

Hi

r-help-bounces at r-project.org napsal dne 02.12.2007 23:33:47:

> 
> I am having trouble getting a loop to work for the following problem. 
Any
> help would be much appreciated. Thanks.

Lets try it without loop.

Does this

lapply(split(data[,4:3], data$Participant), function(x) coef(lm(x)))

give you results you want?

If yes, just put it in some object e.g. lll <- and 

do.call(rbind, lll)

gives you a table output.

Regards
Petr


> 
> I need to find the slope and intercept from the linear regression of 
Drug
> Level on Day by Participant. There are a total of 37 Participants. I 
need to
> store the Participant, Label, Slope, and Intercept in a new data frame. 
> 
> 
> This data is ordered by Participant number 37 total participants. A 
sample
> of the data is given below. 
> 
> 
> Label Participant Day DrugLevel 
> 17      0       1  15 1.84179 
> 121     0       1   5 2.10772 
> 147     0       1   7 3.00658 
> 152     0       1  11 2.91729 
> 250     0       1  10 2.75816 
> 289     0       1  13 3.20468 
> 321     0       1   6 2.43389 
> 362     0       1  12 2.77770 
> 433     0       1   9 3.03167 
> 469     0       1   8 2.97613 
> 475     0       1  14 2.86934 
> 70      0       2  13 0.68022 
> 210     0       2   8 1.41767 
> 243     0       2  11 1.28867 
> 246     0       2   9 1.53601 
> 247     0       2   6 1.64863 
> 280     0       2   5 1.19795 
> 310     0       2  12 1.24440 
> 343     0       2  10 1.18929 
> 413     0       2   7 1.57207 
> 41      0       3   7 1.87884 
> 74      0       3   8 1.82477 
> 100     0       3   5 2.09422 
> 133     0       3   6 1.91853 
> 134     0       3  12 0.90422 
> 149     0       3  11 1.38232 
> 172     0       3  10 1.55323 
> 216     0       3   9 1.24088 
> 65      0       4   8 2.49412 
> 69      0       4   5 1.79840 
> 
> This is my thought process of what the loop needs to do but I cant get 
the
> correct loop. 
> 
> X1=Day[Participant=="1"]
> Y1=DrugLevel[Participant=="1"]
> 
> Coeffs=function(X1,Y1)
> {
> lmfirst=lm(Y1~X1)
> lmfirst$coefficients
> }
> 
> Coeffs(X1,Y1)
> 
> # output slope and intercept here
> # do same for the next participant
> 
> X2=Day[Participant=="2"]
> Y2=DrugLevel[Participant=="2"]
> 
> Coeffs=function(X2,Y2)
> {
> lmfirst=lm(Y2~X2)
> lmfirst$coefficients
> }
> 
> Coeffs(X2,Y2)
> 
> # output slope and intercept here
> # do same for the next participant
> 
> X3=Day[Participant=="3"]
> Y3=DrugLevel[Participant=="3"]
> 
> Coeffs=function(X3,Y3)
> {
> lmfirst=lm(Y3~X3)
> lmfirst$coefficients
> }
> 
> Coeffs(X3,Y3)
> 
> # output slope and intercept here
> # do same for the next participant
> 
> # etc for the rest of the participants
> 
> # any ideas? 
> 
> # thanks 
> 
> -- 
> View this message in context: http://www.nabble.com/Help-with-a-Loop-
> tf4933354.html#a14120625
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dimitris.rizopoulos at med.kuleuven.be  Mon Dec  3 10:12:32 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 3 Dec 2007 10:12:32 +0100
Subject: [R] Help with a Loop
References: <14120625.post@talk.nabble.com>
Message-ID: <00a001c8358c$a2b31cc0$0540210a@www.domain>

look at function lmList() from package 'nlme', e.g.,

# say 'dat' is you data.frame
library(nlme)
fm <- lmList(DrugLevel ~ Day | Participant, data = dat)
fm
summary(fm)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "stathelp" <ebballller1584 at yahoo.com>
To: <r-help at r-project.org>
Sent: Sunday, December 02, 2007 11:33 PM
Subject: [R] Help with a Loop


>
> I am having trouble getting a loop to work for the following 
> problem. Any
> help would be much appreciated. Thanks.
>
> I need to find the slope and intercept from the linear regression of 
> Drug
> Level on Day by Participant. There are a total of 37 Participants. I 
> need to
> store the Participant, Label, Slope, and Intercept in a new data 
> frame.
>
>
> This data is ordered by Participant number 37 total participants. A 
> sample
> of the data is given below.
>
>
> Label Participant Day DrugLevel
> 17      0       1  15 1.84179
> 121     0       1   5 2.10772
> 147     0       1   7 3.00658
> 152     0       1  11 2.91729
> 250     0       1  10 2.75816
> 289     0       1  13 3.20468
> 321     0       1   6 2.43389
> 362     0       1  12 2.77770
> 433     0       1   9 3.03167
> 469     0       1   8 2.97613
> 475     0       1  14 2.86934
> 70      0       2  13 0.68022
> 210     0       2   8 1.41767
> 243     0       2  11 1.28867
> 246     0       2   9 1.53601
> 247     0       2   6 1.64863
> 280     0       2   5 1.19795
> 310     0       2  12 1.24440
> 343     0       2  10 1.18929
> 413     0       2   7 1.57207
> 41      0       3   7 1.87884
> 74      0       3   8 1.82477
> 100     0       3   5 2.09422
> 133     0       3   6 1.91853
> 134     0       3  12 0.90422
> 149     0       3  11 1.38232
> 172     0       3  10 1.55323
> 216     0       3   9 1.24088
> 65      0       4   8 2.49412
> 69      0       4   5 1.79840
>
> This is my thought process of what the loop needs to do but I cant 
> get the
> correct loop.
>
> X1=Day[Participant=="1"]
> Y1=DrugLevel[Participant=="1"]
>
> Coeffs=function(X1,Y1)
> {
> lmfirst=lm(Y1~X1)
> lmfirst$coefficients
> }
>
> Coeffs(X1,Y1)
>
> # output slope and intercept here
> # do same for the next participant
>
> X2=Day[Participant=="2"]
> Y2=DrugLevel[Participant=="2"]
>
> Coeffs=function(X2,Y2)
> {
> lmfirst=lm(Y2~X2)
> lmfirst$coefficients
> }
>
> Coeffs(X2,Y2)
>
> # output slope and intercept here
> # do same for the next participant
>
> X3=Day[Participant=="3"]
> Y3=DrugLevel[Participant=="3"]
>
> Coeffs=function(X3,Y3)
> {
> lmfirst=lm(Y3~X3)
> lmfirst$coefficients
> }
>
> Coeffs(X3,Y3)
>
> # output slope and intercept here
> # do same for the next participant
>
> # etc for the rest of the participants
>
> # any ideas?
>
> # thanks
>
> -- 
> View this message in context: 
> http://www.nabble.com/Help-with-a-Loop-tf4933354.html#a14120625
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Mon Dec  3 10:32:31 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 3 Dec 2007 09:32:31 +0000 (GMT)
Subject: [R] Attempt at package documentation on debian: link to
 "data.frame" broken
In-Reply-To: <fj0gn5$fcu$1@ger.gmane.org>
References: <fj0gn5$fcu$1@ger.gmane.org>
Message-ID: <Pine.LNX.4.64.0712030924120.5695@gannet.stats.ox.ac.uk>

On Mon, 3 Dec 2007, Johannes Graumann wrote:

> Hello,
>
> I'm trying to document this little package I'm working on and have the
> following issue:
> one of the man files (*.Rd) contains this bit
>
> \value{
>  Returns a \code{\link[base]{data.frame}} representing the tabular data.
> }
>
> The resulting link is broken on my debian system. It points
> to "file:///usr/local/lib/R/site-library/base/html/data.frame.html", while
> the real file resides at "/usr/lib/R/library/base/html/data.frame.html".
>
> Can someone please let me know how I would fix this?

So you are talking about the HTML conversion of your help (.Rd) file? 
(Links appear in other versions too.)

The HTML links are intended to be used via help.start(), not directly, and 
I think you find it actually points to ../../base/html/data.frame.html, 
which is correct for use from help.start.  (You are probably quoting your 
browser's intepretation of a relative URL.)

Note what the documentation says:

   There are two other forms of optional argument specified as
   \link[pkg]{foo} and \link[pkg:bar]{foo} to link to the package pkg, to
   files foo.html and bar.html respectively. These are rarely needed,
   perhaps to refer to not-yet-installed packages (but there the
   HTML help system  will resolve the link at run time) or in the normally
   undesirable event that more than one package offers help on a topic
   (in which case the present package has precedence so this is only
   needed to refer to other packages).

So unless you define data.frame() in your package, you don't need this 
form.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pgoikoetxea at neiker.net  Mon Dec  3 10:37:51 2007
From: pgoikoetxea at neiker.net (Pablo G Goicoechea)
Date: Mon, 03 Dec 2007 10:37:51 +0100
Subject: [R] Rating R Helpers
In-Reply-To: <6b93d1830712010828l75857506gd2ee373a994ae456@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>	<B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
	<6b93d1830712010828l75857506gd2ee373a994ae456@mail.gmail.com>
Message-ID: <4753CE6F.3090504@neiker.net>


   I'm just a R user who joined the list searching solution for a problem.
   I do not think rating helpers is a good idea. For once, they do it freely;
   no  need  to harrash (?) anybody. On the other hand, it could have the
   opposite  effect;  people afraid to get a bad rating do not post their
   potentially valid answers.
   But more importantly, the list is full with examples of how to accomplish
   the same result with different approaches. Some might be more elegant than
   others, but for sure they show R potentialities. And I have seen several
   corrections/discussions among old timers themselves.
   Packages reviews are another issue. But if anybody is going through all that
   work, why not to make the appropriate corrections to the packages? They are
   GPL, aren't they?
   Best
   Pablo
   Mark Kimpel escribi??:

I'll throw one more idea into the mix. I agree with Bill that a rating
system for respondents is probably not that practical and of not the highest
importance. It also seems like a recipe for creating inter-personal problems
that the list doesn't need.

I do like Bill's idea of a review system for packages, which could be
incorporated into my idea that follows...

What I would find useful would be some sort of tagging system for messages.
I can't count the times I've remembered seeing a message that addresses a
question I have down the road but, when Googled, I can't find it. It would
be so nice, for example, to reliably be able to find all messages related to
a certain package or package function posted within the last X days. This
could be implemented as simply as asking posters to provide keywords at the
end of a message, but it would be great if they could somehow be pulled out
of a message and stored in a DB. For instance keywords could be surrounded
by a sequence of special characters, which a parser could then extract and
store in a DB along with the message.

Of course, this would be work to set up, but how many of our "experts" who
so kindly give of their time, get exasperated when similar questions keep
popping up on the list? Also, if we had a web-accessable DB, the responses,
not the responders, could be rated as to how well a reply takes care of an
issue. Thus, over time, a sort of auto-wiki could be born. I can think of
more uses for this as well. For example a developer could quickly check to
see what usability problems or suggestions have cropped up of on individual
package.

Mark

On Dec 1, 2007 2:21 AM, [1]<Bill.Venables at csiro.au> wrote:



This seems a little impractical to me.  People respond so much at random
and most only tackle questions with which they feel comfortable.  As
it's not a competition in any sense, it's going to be hard to rank
people in any effective way.  But suppose you succeed in doing so, then
what?

To me a much more urgent initiative is some kind of user online review
system for packages, even something as simple as that used by Amazon.com
has for customer review of books.

I think the need for this is rather urgent, in fact.  Most packages are
very good, but I regret to say some are pretty inefficient and others
downright dangerous.  You don't want to discourage people from
submitting their work to CRAN, but at the same time you do want some
mechanism that allows users to relate their experience with it, good or
bad.


Bill Venables
CSIRO Laboratories
PO Box 120, Cleveland, 4163
AUSTRALIA
Office Phone (email preferred): +61 7 3826 7251
Fax (if absolutely necessary):  +61 7 3826 7304
Mobile:                         +61 4 8819 4402
Home Phone:                     +61 7 3286 7700
[2]mailto:Bill.Venables at csiro.au
[3]http://www.cmis.csiro.au/bill.venables/

-----Original Message-----
From: [4]r-help-bounces at r-project.org [[5]mailto:r-help-bounces at r-project.org]
On Behalf Of Doran, Harold
Sent: Saturday, 1 December 2007 6:13 AM
To: R Help
Subject: [R] Rating R Helpers

Since R is open source and help may come from varied levels of
experience on R-Help, I wonder if it might be helpful to construct a
method that can be used to "rate" those who provide help on this list.

This is something that is done on other comp lists, like
[6]http://www.experts-exchange.com/.

I think some of the reasons for this are pretty transparent, but I
suppose one reason is that one could decide to implement the advise of
those with "superior" or "expert" levels. In other words, you can trust
the advice of someone who is more experienced more than someone who is
not. Currently, there is no way to discern who on this list is really an
R expert and who is not. Of course, there is R core, but most people
don't actually know who these people are (at least I surmise that to be
true).

If this is potentially useful, maybe one way to begin the development of
such ratings is to allow the original poster to "rate" the level of help
from those who responded. Maybe something like a very simple
questionnaire on a likert-like scale that the original poster would
respond to upon receiving help which would lead to the accumulation of
points for the responders. Higher points would result in higher levels
of expertise (e.g., novice, ..., wizaRd).

Just a random thought. What do others think?

Harold




       [[alternative HTML version deleted]]

______________________________________________
[7]R-help at r-project.org mailing list
[8]https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
[9]http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
[10]R-help at r-project.org mailing list
[11]https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
[12]http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.







   --

   Pablo G Goicoechea

   Bioteknolog??a Saila / Dpto Biotecnolog??a

   NEIKER-Tecnalia

   Apdo 46

   01080 Vitoria-Gasteiz (SPAIN)

   Phone: +34 902 540 546 Fax: +34 902 540 547

   [13]pgoikoetxea at neiker.net

References

   1. mailto:Bill.Venables at csiro.au
   2. mailto:Bill.Venables at csiro.au
   3. http://www.cmis.csiro.au/bill.venables/
   4. mailto:r-help-bounces at r-project.org
   5. mailto:r-help-bounces at r-project.org
   6. http://www.experts-exchange.com/
   7. mailto:R-help at r-project.org
   8. https://stat.ethz.ch/mailman/listinfo/r-help
   9. http://www.R-project.org/posting-guide.html
  10. mailto:R-help at r-project.org
  11. https://stat.ethz.ch/mailman/listinfo/r-help
  12. http://www.R-project.org/posting-guide.html
  13. mailto:pgoikoetxea at neiker.net

From charpent at bacbuc.dyndns.org  Mon Dec  3 10:43:07 2007
From: charpent at bacbuc.dyndns.org (Emmanuel Charpentier)
Date: Mon, 03 Dec 2007 10:43:07 +0100
Subject: [R] odfWeave error
In-Reply-To: <4753064A.6030703@yahoo.com.br>
References: <4753064A.6030703@yahoo.com.br>
Message-ID: <fj0j3b$me8$1@ger.gmane.org>

Cleber N. Borges a ?crit :
> hello all,
> I trying to use the package 'odfWeave'
> and I get the follow error:
> 
> 
> 
> ###  error message
> #############################################################
> ...
>   Removing content.xml
> 
>   Post-processing the contents
> Error in .Call("RS_XML_Parse", file, handlers, as.logical(addContext),  :
>   Error in the XML event driven parser for content_1.xml: error parsing 
> attribute name
> 
> 
> 
> The piece of the code is:
> 
> ###  code
> ############################################################
> ...
> 
> <<makeGraph, echo=T, results=xml>>=
> fileplot='C:\\DADOS\\tmp\\plotx.emf'
> win.metafile(fileplot)
> plot( rnorm(300), col='red', t='l', lwd=2 ); grid()
> dev.off()
> @

???? This chunk is pure R code and shouldn't output anything directly in
the output file. Since you are telling "echo=T', whatever is output by
your code is sent to your output file, and since you assert
"results=xml",this output is interpreted as xml ; since this isn't XML,
your're getting guff from the XML interpreter.

I'd rather do :
<<makeGraph, echo=FALSE>>=
invisible({
# Whatever you did ...
})
@
instead. Your second chunk :

> <<insertGraph, echo=T, results=xml>>=
> odfInsertPlot(file=fileplot, height=5, width=5 )
> @

should insert your plot in the output file.

[Snip ... ]

BTW : Windows (enhanced) metafile are Windows-specific. As far as I can
tell, recent versions of Office and OpenOffice.org correctly render
Encapsulated Postcript files, thus freeing you from another Windows
demendency. Unless you *have* to have an EMF output (it happens, I know
...), youd'better use use this format directly.

HTH

					Emmanuel Charpentier


From Thierry.ONKELINX at inbo.be  Mon Dec  3 12:02:42 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 3 Dec 2007 12:02:42 +0100
Subject: [R] ggplot2: Choosing colours
Message-ID: <2E9C414912813E4EB981326983E0A10403F40E45@inboexch.inbo.be>

Dear useRs,

I'm trying to specify the colour of a factor with ggplot2. The example
below gets me close to what I want, but it's missing a legend.

Any ideas?

Thanks,

Thierry

library(ggplot2)
dataset <- data.frame(x = rnorm(40), y = runif(40), z = gl(4, 10, labels
= LETTERS[1:4]))
ggplot(data = dataset, aes(x = x, y = y, group = z)) + geom_point(colour
= c("red", "green", "blue", "black"))


------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be 
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


From MUEHGE at de.ibm.com  Mon Dec  3 12:04:33 2007
From: MUEHGE at de.ibm.com (Thorsten Muehge)
Date: Mon, 3 Dec 2007 12:04:33 +0100
Subject: [R] help on qcc
Message-ID: <OF7566C5AF.C84AC5C2-ONC12573A6.0036ACCB-C12573A6.003D04FA@de.ibm.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/261c6c2d/attachment.pl 

From johannes_graumann at web.de  Mon Dec  3 13:13:49 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 03 Dec 2007 13:13:49 +0100
Subject: [R] Attempt at package documentation on debian: link to
	"data.frame" broken
References: <fj0gn5$fcu$1@ger.gmane.org>
	<Pine.LNX.4.64.0712030924120.5695@gannet.stats.ox.ac.uk>
Message-ID: <fj0rtk$fl4$1@ger.gmane.org>

Prof Brian Ripley wrote:

> So you are talking about the HTML conversion of your help (.Rd) file?
> (Links appear in other versions too.)
> 
> The HTML links are intended to be used via help.start(), not directly, and
> I think you find it actually points to ../../base/html/data.frame.html,
> which is correct for use from help.start.  (You are probably quoting your
> browser's intepretation of a relative URL.)
Yes. I'm actually using (the really nice) rkward, which calls the html-help
pages upon "?<something>". May this be specific to that usage ... now that
I'm looking deeper into this I find that rkward actually tells me:
Using non-linked HTML file: style sheet and hyperlinks may be incorrect
- which is what happens ... how to switch to "linked HTML file"?

> Note what the documentation says:
> 
>    There are two other forms of optional argument specified as
>    \link[pkg]{foo} and \link[pkg:bar]{foo} to link to the package pkg, to
>    files foo.html and bar.html respectively. These are rarely needed,
>    perhaps to refer to not-yet-installed packages (but there the
>    HTML help system  will resolve the link at run time) or in the normally
>    undesirable event that more than one package offers help on a topic
>    (in which case the present package has precedence so this is only
>    needed to refer to other packages).
> 
> So unless you define data.frame() in your package, you don't need this
> form.
I see. This was actually part of the desperate attempt to find a solution to
this wrong link target issue. I'm using a plain "\link(data.frame)" now
again.

Thanks, Joh


From ripley at stats.ox.ac.uk  Mon Dec  3 13:35:25 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 3 Dec 2007 12:35:25 +0000 (GMT)
Subject: [R] Attempt at package documentation on debian: link to
 "data.frame" broken
In-Reply-To: <fj0rtk$fl4$1@ger.gmane.org>
References: <fj0gn5$fcu$1@ger.gmane.org>
	<Pine.LNX.4.64.0712030924120.5695@gannet.stats.ox.ac.uk>
	<fj0rtk$fl4$1@ger.gmane.org>
Message-ID: <Pine.LNX.4.64.0712031224030.12056@gannet.stats.ox.ac.uk>

On Mon, 3 Dec 2007, Johannes Graumann wrote:

> Prof Brian Ripley wrote:
>
>> So you are talking about the HTML conversion of your help (.Rd) file?
>> (Links appear in other versions too.)
>>
>> The HTML links are intended to be used via help.start(), not directly, and
>> I think you find it actually points to ../../base/html/data.frame.html,
>> which is correct for use from help.start.  (You are probably quoting your
>> browser's intepretation of a relative URL.)

> Yes. I'm actually using (the really nice) rkward, which calls the html-help
> pages upon "?<something>". May this be specific to that usage ... now that
> I'm looking deeper into this I find that rkward actually tells me:

No, it was R that warned you.  (It seem you have swapped the credit and 
blame in your message.)

> Using non-linked HTML file: style sheet and hyperlinks may be incorrect
> - which is what happens ... how to switch to "linked HTML file"?

It all works via help.start() (as I did say), at least in a reliable R 
front end.  Running help.start() is how you set this up.

>> Note what the documentation says:
>>
>>    There are two other forms of optional argument specified as
>>    \link[pkg]{foo} and \link[pkg:bar]{foo} to link to the package pkg, to
>>    files foo.html and bar.html respectively. These are rarely needed,
>>    perhaps to refer to not-yet-installed packages (but there the
>>    HTML help system  will resolve the link at run time) or in the normally
>>    undesirable event that more than one package offers help on a topic
>>    (in which case the present package has precedence so this is only
>>    needed to refer to other packages).
>>
>> So unless you define data.frame() in your package, you don't need this
>> form.

> I see. This was actually part of the desperate attempt to find a solution to
> this wrong link target issue. I'm using a plain "\link(data.frame)" now
> again.
>
> Thanks, Joh
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Mon Dec  3 13:41:30 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 03 Dec 2007 07:41:30 -0500
Subject: [R] documenting yoru progress
In-Reply-To: <20071202232420.3348AFAC8C4@as220.org>
References: <20071202232420.3348AFAC8C4@as220.org>
Message-ID: <4753F97A.60308@stats.uwo.ca>

On 12/2/2007 6:24 PM, Tom Sgouros wrote:
> Hello all:
> 
> I have a function that writes a fairly elaborate report based on some
> survey data.  For documentation and bookkeeping purposes, I'd like to
> write out in the report the function call that produced the report, or
> at least enough information to help me recreate the steps that led to
> that report.  I've been generating all the reports with scripts, in
> order to be able to recreate the steps, but apart from the file name, I
> don't yet have a way to match the report to the script that created it.
> 
> Can anyone suggest easy ways to do this?  From within a function, is the
> function call text available somehow, or the names of the arguments used
> in the function invocation?

I think Gabor has answered your question directly, but another approach 
to solve the same underlying problem might be to work with Sweave.   (Or 
ODFweave, etc.)

These allow you to mix R code right into a document that explains the 
reasoning and includes both the input and output of the script.

Duncan Murdoch


From ggrothendieck at gmail.com  Mon Dec  3 13:47:09 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Dec 2007 07:47:09 -0500
Subject: [R] Help with a Loop
In-Reply-To: <14120625.post@talk.nabble.com>
References: <14120625.post@talk.nabble.com>
Message-ID: <971536df0712030447o575b10d4xc9cd05dcece5b87b@mail.gmail.com>

There are three ways listed here:
https://stat.ethz.ch/pipermail/r-help/2007-May/132866.html

On Dec 2, 2007 5:33 PM, stathelp <ebballller1584 at yahoo.com> wrote:
>
> I am having trouble getting a loop to work for the following problem. Any
> help would be much appreciated. Thanks.
>
> I need to find the slope and intercept from the linear regression of Drug
> Level on Day by Participant. There are a total of 37 Participants. I need to
> store the Participant, Label, Slope, and Intercept in a new data frame.
>
>
> This data is ordered by Participant number 37 total participants. A sample
> of the data is given below.
>
>
> Label Participant Day DrugLevel
> 17      0       1  15 1.84179
> 121     0       1   5 2.10772
> 147     0       1   7 3.00658
> 152     0       1  11 2.91729
> 250     0       1  10 2.75816
> 289     0       1  13 3.20468
> 321     0       1   6 2.43389
> 362     0       1  12 2.77770
> 433     0       1   9 3.03167
> 469     0       1   8 2.97613
> 475     0       1  14 2.86934
> 70      0       2  13 0.68022
> 210     0       2   8 1.41767
> 243     0       2  11 1.28867
> 246     0       2   9 1.53601
> 247     0       2   6 1.64863
> 280     0       2   5 1.19795
> 310     0       2  12 1.24440
> 343     0       2  10 1.18929
> 413     0       2   7 1.57207
> 41      0       3   7 1.87884
> 74      0       3   8 1.82477
> 100     0       3   5 2.09422
> 133     0       3   6 1.91853
> 134     0       3  12 0.90422
> 149     0       3  11 1.38232
> 172     0       3  10 1.55323
> 216     0       3   9 1.24088
> 65      0       4   8 2.49412
> 69      0       4   5 1.79840
>
> This is my thought process of what the loop needs to do but I cant get the
> correct loop.
>
> X1=Day[Participant=="1"]
> Y1=DrugLevel[Participant=="1"]
>
> Coeffs=function(X1,Y1)
> {
> lmfirst=lm(Y1~X1)
> lmfirst$coefficients
> }
>
> Coeffs(X1,Y1)
>
> # output slope and intercept here
> # do same for the next participant
>
> X2=Day[Participant=="2"]
> Y2=DrugLevel[Participant=="2"]
>
> Coeffs=function(X2,Y2)
> {
> lmfirst=lm(Y2~X2)
> lmfirst$coefficients
> }
>
> Coeffs(X2,Y2)
>
> # output slope and intercept here
> # do same for the next participant
>
> X3=Day[Participant=="3"]
> Y3=DrugLevel[Participant=="3"]
>
> Coeffs=function(X3,Y3)
> {
> lmfirst=lm(Y3~X3)
> lmfirst$coefficients
> }
>
> Coeffs(X3,Y3)
>
> # output slope and intercept here
> # do same for the next participant
>
> # etc for the rest of the participants
>
> # any ideas?
>
> # thanks
>
> --
> View this message in context: http://www.nabble.com/Help-with-a-Loop-tf4933354.html#a14120625
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From johannes_graumann at web.de  Mon Dec  3 13:49:07 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 03 Dec 2007 13:49:07 +0100
Subject: [R] Attempt at package documentation on debian: link to
	"data.frame" broken
References: <fj0gn5$fcu$1@ger.gmane.org>
	<Pine.LNX.4.64.0712030924120.5695@gannet.stats.ox.ac.uk>
	<fj0rtk$fl4$1@ger.gmane.org>
	<Pine.LNX.4.64.0712031224030.12056@gannet.stats.ox.ac.uk>
Message-ID: <fj0tvq$p81$1@ger.gmane.org>

<posted & mailed>

Prof Brian Ripley wrote:

>> Using non-linked HTML file: style sheet and hyperlinks may be incorrect
>> - which is what happens ... how to switch to "linked HTML file"?
> 
> It all works via help.start() (as I did say), at least in a reliable R
> front end.  Running help.start() is how you set this up.
You are right - naturally. After calling 'help.start()' everything works
just fine. Is there any way one can call 'help.start()' transparently to
the user when s/he accesses the html-help?

Thanks for your patience, Joh


From rodrigueza at schwabe.com.mx  Mon Dec  3 13:55:55 2007
From: rodrigueza at schwabe.com.mx (=?ISO-8859-2?Q?Alejandro_Rodr=EDguez?=)
Date: Mon, 3 Dec 2007 06:55:55 -0600
Subject: [R] Help with tables
In-Reply-To: <OFA8D61202.7C9DFE4F-ONC12573A6.002E7813-C12573A6.002EB42D@precheza.cz>
Message-ID: <001901c835ab$d8dc5fe0$327412ac@FARMER0001>

Hello everyone.  I want to thank you for your response.

I tried this on weekend to attack my problem:


1)  Turned into factors all my variables to tabulate:

	infec1<-bamas$P2_A
	infec2<-bamas$P2_B
	infec3<-bamas$P2_C
	aglomera<-bamas$QCL_1

	Where bamas is my data frame, P2_A, P2_B and P2_C the infections and QCL_1
the clusters.

2)  Defined a table for each one of the variables I wanted:

	t1<-table(infec1,aglomera)
	t2<-table(infec2,aglomera)
	t3<-table(infec3,aglomera)

3)  Tables generated above are of n1 x 4, n2 x 4, n3 x 4 where n1, n2 and n3
are different and so the dimmensions of my matrices, then if I want to sum
the three tables R showed me the error that they were of
     different dimmensions, so I decided to make them of the same
dimmensions by "filling the holes" (missing rows) with zeros of each row in
each  table in order to transform them into a matrix of N x 4:

     For example, comparing my table t1 with t2, I could see that, in table
1 there where missing rows 7, 10, 12 and 13, and in the case of t3 rows 7
and 12 ;

t1

	aglomera
infec1   1   2   3   4
    1  117  88  76  83
    2   10  10   9   7
    3   15  11  14  14
    4    2   0   1   1
    5    2   3   1   0
    6    1   0   1   0
    8    3   3   0   1
    9    3   1   1   0
    11   0   0   1   1

t3

      aglomera
infec2  1  2  3  4
    1  12  9  6  6
    2  37 45 33 37
    3  11 11  8 11
    4   3  0  0  2
    5   2  1  0  0
    6   0  0  0  1
    8   3  1  2  2
    9   2  2  0  2
    10  1  1  2  1
    11  1  0  0  0
    13  0  0  1  0

Then I did the following instructions

	u<-cbind(0,0,0,0)
	rt1<-rbind(t1[1:6,],u,t1[7:8,],u,t1[9,],u,u)

	rt2<-rbind(t2[1:6,],u,t2[7:10,],u,u,t2[11,])

	rt3<-rbind(t3[1:6,],u,t3[7:10,],u,u)

4) Now all my tables are 13 x 4 so I defined as matrix:

	mrt1<-matrix(rt1,nrow=13,ncol=4)
	mrt2<-matrix(rt2,nrow=13,ncol=4)
	mrt3<-matrix(rt2,nrow=13,ncol=4)

5) And then I added them:

	total<-mrt1+mrt2+mrt3
	total

      [,1] [,2] [,3] [,4]
 [1,]  130   97   83   89
 [2,]   53   55   42   46
 [3,]   33   36   36   39
 [4,]    5    1    2    3
 [5,]    5    4    1    0
 [6,]    1    1    1    3
 [7,]    0    0    0    0
 [8,]    8    6    4    4
 [9,]    6    4    1    2
[10,]    1    2    2    3
[11,]    1    1    1    1
[12,]    0    0    0    0
[13,]    0    0    1    0

which is the result I wanted.

I don't know if this is the best way to make this kind of tables, but at
least it worked well on weekend late at night :P

If someone have a better (faster) way to make them please help me.

Warm regards from Mexico.



-----Mensaje original-----
De: Petr PIKAL [mailto:petr.pikal at precheza.cz]
Enviado el: Lunes, 03 de Diciembre de 2007 02:30 a.m.
Para: rodrigueza at schwabe.com.mx
CC: r-help at r-project.org
Asunto: Re: [R] Help with tables


Hi

I am not sure but if I remember correctly I had seen similar output in
Frank Harrel's book. So you could check some function from Hmisc package,
probably summarize.

Regards

Petr
petr.pikal at precheza.cz

r-help-bounces at r-project.org napsal dne 02.12.2007 03:16:09:

> I guess you can get the result by
> 1) concatenating all the variables (P2_A, P2_B, P2_C) into one variable
,
> 2) replicating segment membership properly,
> 3) make the table of 1) and 2)
>
> For example, the following may do the job.
>
> > ## (1) Generate data set
> > # Set random seed
> > set.seed(0)
> >
> > n.obs <- 15 # Number of patients
> > disease.lv  <- 1:10 ## different types of disease
> > data1 <- as.data.frame(t(replicate(n.obs, sample(disease.lv, 3)))) #
> Create data set
> > ## Create segment membership
> > segment <- sample(LETTERS[1:3], n.obs, replace=TRUE)
> >
> > cbind(data1, segment)
>    V1 V2 V3 segment
> 1   9  3 10       B
> 2   6  9  2       C
> 3   9 10  6       A
> 4   7  1  2       B
> 5   2  7  4       C
> 6   8  5  6       C
> 7  10  4  7       B
> 8  10  2  6       C
> 9   2  3  4       B
> 10  1  4  7       A
> 11  4  5  9       A
> 12  5  2  7       A
> 13  7  8  1       A
> 14  8  4  7       B
> 15  7  8  5       B
> >
> >
> > ## (2) Table
> > infec.all <- unlist(data1) # Concatenate all variables into one
variable
> > segment.all <- rep(segment, ncol(data1)) # Replicate the segment
> membership as necessary
> > table(infec.all, segment.all)
>          segment.all
> infec.all A B C
>        1  2 1 0
>        2  1 2 3
>        3  0 2 0
>        4  2 3 1
>        5  2 1 1
>        6  1 0 3
>        7  3 4 1
>        8  1 2 1
>        9  2 1 1
>        10 1 2 1
>
>
>
> On Nov 30, 2007 10:10 AM, Alejandro Rodr?guez
<rodrigueza at schwabe.com.mx>
> wrote:
>
> > Hello,  I'm new using R and developing tables.
> >
> > I have a problem in developing a table.  In a questionaire I made I
ask
> > this
> > question "Please tell me the first three sympthoms caused by
Respiratory
> > tract infection you've caught this year", then the people answer three
> > sympthoms, the first mention (Top of mind) is saved in a variable
called
> > "P2_A", the second mention in a variable called "P2_B" and the third
> > mention
> > in "P2_C".  Each answer is coded with numbers like this:
> >
> > 1 = Flu
> > 2 = Cough
> > 3 = Asthma
> > ....
> >
> > 13 = Fever
> >
> > I've already done a K-cluster analysis and segmented my data base.  So
my
> > first task is to develop tables to develop tables of frequencies
crossing
> > Cluster vs. "P2_A" in order to know which are the top of mind products
and
> > their frequencies by cluster, then the second mention and third
mention.
> > I've used this instruction which worked well:
> >
> > > table(infec1,aglomera)
> >      aglomera
> > infec1   1   2   3   4
> >    1  117  88  76  83
> >    2   10  10   9   7
> >    3   15  11  14  14
> >    4    2   0   1   1
> >    5    2   3   1   0
> >    6    1   0   1   0
> >    8    3   3   0   1
> >    9    3   1   1   0
> >    11   0   0   1   1
> >
> > Where "infec1" is a factor of "P2_A" and "aglomera" is a factor of the
> > variable "Cluster" I made.  It worked well when I study them
> > separately...however I would like to know the TOTAL mentions of each
> > sympthom by cluster.  I've done this exercise in SPSS using the
"Multiple
> > Response" instruction first grouping my three variables (i.e. "P2_A",
> > "P2_B"
> > and "P2_C") into a variable called "sick" and cross tabulating it vs.
> > QCL_1
> > (my cluster variable) and it gave me the table I need in this way
(showed
> > at
> > the bottom of this mail):
> >
> > How can I made a table like this in R???.  I've tried combining my
> > variables
> > in a matrix and using xtabs, ftable, table, structable and a lot of
> > combination of them but I haven't had succed with any of them.
> >
> > Please help me with this issue, I don't want to keep using SPSS any
more.
> >
> > Thanx in advance.
> >
> > P.D. Result from SPSS is shown below.
> >
> >
> >
> >                * * *  C R O S S T A B U L A T I O N  * * *
> >
> >   $SICK (group)  mr sick
> > by QCL_1  Cluster Number of Case
> >
> >
> >                       QCL_1
> >
> >                Count  I
> >                       I                                      Row
> >                       I                                     Total
> >                       I     1  I     2  I     3  I     4  I
> > $SICK          --------+--------+--------+--------+--------+
> >                    1  I   130  I    97  I    83  I    89  I   399
> >  Gripe, influenza, ca I        I        I        I        I  83.1
> >                       +--------+--------+--------+--------+
> >                    2  I    53  I    55  I    42  I    46  I   196
> >  Tos de cualquier tip I        I        I        I        I  40.8
> >                       +--------+--------+--------+--------+
> >                    3  I    33  I    36  I    36  I    39  I   144
> >  Dolor irritaci?n     I        I        I        I        I  30.0
> >                       +--------+--------+--------+--------+
> >                    4  I     5  I     1  I     2  I     3  I    11
> >  Bronquitis           I        I        I        I        I   2.3
> >                       +--------+--------+--------+--------+
> >                    5  I     5  I     4  I     1  I     0  I    10
> >  Sinusitis            I        I        I        I        I   2.1
> >                       +--------+--------+--------+--------+
> >                    6  I     1  I     1  I     1  I     3  I     6
> >  Rinitis              I        I        I        I        I   1.3
> >                       +--------+--------+--------+--------+
> >                    8  I     8  I     6  I     4  I     4  I    22
> >  Amigdalitis          I        I        I        I        I   4.6
> >                       +--------+--------+--------+--------+
> >                    9  I     6  I     4  I     1  I     2  I    13
> >  Faringitis           I        I        I        I        I   2.7
> >                       +--------+--------+--------+--------+
> >                   10  I     1  I     2  I     2  I     3  I     8
> >  Laringitis           I        I        I        I        I   1.7
> >                       +--------+--------+--------+--------+
> >                   11  I     1  I     1  I     1  I     1  I     4
> >  Neumonia             I        I        I        I        I    .8
> >                       +--------+--------+--------+--------+
> >                   13  I     0  I     0  I     1  I     0  I     1
> >  Asma                 I        I        I        I        I    .2
> >                       +--------+--------+--------+--------+
> >               Column      153      116      104      107      480
> >                Total     31.9     24.2     21.7     22.3    100.0
> >
> > Percents and totals based on respondents
> >
> > 480 valid cases;  0 missing cases
> >
> >
> >
> > Act. Calef Alejandro Rodr?guez Cuevas
> > Analista de mercado
> >
> > Laboratorios Farmasa S.A. de C.V.
> > Schwabe Mexico, S.A. de C.V.
> >
> > Bufalo Nr. 27
> > Col. del Valle 03100
> > Mexico, D.F.
> > Mexico
> >
> > Tel. 52 00 26 80
> > email: rodrigueza at schwabe.com.mx
> >
> > www.schwabe.com.mx
> > www.umckaloabo.com.mx
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> ======================================
> T.K. (Tae-kyun) Kim
> Ph.D. student
> Department of Marketing
> Marshall School of Business
> University of Southern California
> ======================================
>
>    [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tomfool at as220.org  Mon Dec  3 13:49:16 2007
From: tomfool at as220.org (tom sgouros)
Date: Mon, 03 Dec 2007 07:49:16 -0500
Subject: [R] documenting yoru progress
In-Reply-To: <4753F97A.60308@stats.uwo.ca> 
References: <20071202232420.3348AFAC8C4@as220.org>
	<4753F97A.60308@stats.uwo.ca>
Message-ID: <20071203124916.71708FAC8C7@as220.org>


Duncan Murdoch <murdoch at stats.uwo.ca> wrote:

> I think Gabor has answered your question directly, but another
> approach to solve the same underlying problem might be to work with
> Sweave.   (Or ODFweave, etc.)

Thank you, that is pretty marvelous.  I will use it next time around.

 -tom


-- 
 ------------------------
 tomfool at as220 dot org
 http://sgouros.com  
 http://whatcheer.net


From bolker at ufl.edu  Mon Dec  3 14:04:18 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 3 Dec 2007 05:04:18 -0800 (PST)
Subject: [R] comparison of two vectors
In-Reply-To: <14129032.post@talk.nabble.com>
References: <14129032.post@talk.nabble.com>
Message-ID: <14129523.post@talk.nabble.com>




tintin_et_milou wrote:
> 
> Hello,
> 
> I have a vector of two columns like this:
> m/Z                I
> 1000.235       125
> 1000.356       126.5
> ....
> ...
> 
> and a second vector with only one column:
> m/Z 
> 995.547
> 1000.320
> ...
> 
> For each value of the second vector I want to associate the value of the
> intensity of the nearest m/Z associated.
> I can do this with loops (Comparison value by value) but I think that
> maybe a command exist...
> 
> Thanks
> 
> 

## construct sample data
m1 =
  matrix(c(1000.235,1000.356,
    125,126.5),ncol=2,
         dimnames=list(NULL,c("mZ","I")))

y = c(995.547,1000.320)

## compute distances, set diagonal to infinity
d = as.matrix(dist(cbind(m1[,1],y)))
diag(d) <- Inf

## find minimum distances, extract values
cbind(y,m1[apply(d,2,which.min),2])

  Ben Bolker

-- 
View this message in context: http://www.nabble.com/comparison-of-two-vectors-tf4936213.html#a14129523
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Mon Dec  3 14:08:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 3 Dec 2007 13:08:47 +0000 (GMT)
Subject: [R] Attempt at package documentation on debian: link to
 "data.frame" broken
In-Reply-To: <fj0tvq$p81$1@ger.gmane.org>
References: <fj0gn5$fcu$1@ger.gmane.org>
	<Pine.LNX.4.64.0712030924120.5695@gannet.stats.ox.ac.uk>
	<fj0rtk$fl4$1@ger.gmane.org>
	<Pine.LNX.4.64.0712031224030.12056@gannet.stats.ox.ac.uk>
	<fj0tvq$p81$1@ger.gmane.org>
Message-ID: <Pine.LNX.4.64.0712031306310.7474@gannet.stats.ox.ac.uk>

On Mon, 3 Dec 2007, Johannes Graumann wrote:

> <posted & mailed>
>
> Prof Brian Ripley wrote:
>
>>> Using non-linked HTML file: style sheet and hyperlinks may be incorrect
>>> - which is what happens ... how to switch to "linked HTML file"?
>>
>> It all works via help.start() (as I did say), at least in a reliable R
>> front end.  Running help.start() is how you set this up.
> You are right - naturally. After calling 'help.start()' everything works
> just fine. Is there any way one can call 'help.start()' transparently to
> the user when s/he accesses the html-help?

It is potentially very slow (ca 40 secs on our dept system with 1500 
NFS-mounted packages) in R 2.6.1.  But you could put utils::help.start() 
in ~/.Rprofile.

In R-devel you will be able to have

 	utils:::make.packages.html()

in ~/.Rprofile and that takes a few seconds at most.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From vistocco at unicas.it  Mon Dec  3 14:07:50 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Mon, 03 Dec 2007 14:07:50 +0100
Subject: [R] ggplot2: Choosing colours
In-Reply-To: <2E9C414912813E4EB981326983E0A10403F40E45@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A10403F40E45@inboexch.inbo.be>
Message-ID: <4753FFA6.3060602@unicas.it>

qplot(data=dataset, x, y, colour=z)

ONKELINX, Thierry wrote:
> Dear useRs,
>
> I'm trying to specify the colour of a factor with ggplot2. The example
> below gets me close to what I want, but it's missing a legend.
>
> Any ideas?
>
> Thanks,
>
> Thierry
>
> library(ggplot2)
> dataset <- data.frame(x = rnorm(40), y = runif(40), z = gl(4, 10, labels
> = LETTERS[1:4]))
> ggplot(data = dataset, aes(x = x, y = y, group = z)) + geom_point(colour
> = c("red", "green", "blue", "black"))
>
>
> ------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium 
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be 
> www.inbo.be 
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From jzhang1982 at gmail.com  Mon Dec  3 14:12:37 2007
From: jzhang1982 at gmail.com (Jian Zhang)
Date: Mon, 3 Dec 2007 21:12:37 +0800
Subject: [R] Why is the program too slow?
Message-ID: <200712032112261250392@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/b268facd/attachment.pl 

From johannes_graumann at web.de  Mon Dec  3 14:18:31 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 3 Dec 2007 14:18:31 +0100
Subject: [R] Attempt at package documentation on debian: link to
	"data.frame" broken
In-Reply-To: <Pine.LNX.4.64.0712031306310.7474@gannet.stats.ox.ac.uk>
References: <fj0gn5$fcu$1@ger.gmane.org> <fj0tvq$p81$1@ger.gmane.org>
	<Pine.LNX.4.64.0712031306310.7474@gannet.stats.ox.ac.uk>
Message-ID: <200712031418.32349.johannes_graumann@web.de>

Hmmm, I see. This is all local. So there does not seem to be a way to make any 
old R installation that loads the package initialize this functionality, so 
that the help files will be correctly interlinked directly after "require" 
or "library"? How do other packages manage that?

Joh

On Monday 03 December 2007 14:08:47 Prof Brian Ripley wrote:
> On Mon, 3 Dec 2007, Johannes Graumann wrote:
> > <posted & mailed>
> >
> > Prof Brian Ripley wrote:
> >>> Using non-linked HTML file: style sheet and hyperlinks may be incorrect
> >>> - which is what happens ... how to switch to "linked HTML file"?
> >>
> >> It all works via help.start() (as I did say), at least in a reliable R
> >> front end.  Running help.start() is how you set this up.
> >
> > You are right - naturally. After calling 'help.start()' everything works
> > just fine. Is there any way one can call 'help.start()' transparently to
> > the user when s/he accesses the html-help?
>
> It is potentially very slow (ca 40 secs on our dept system with 1500
> NFS-mounted packages) in R 2.6.1.  But you could put utils::help.start()
> in ~/.Rprofile.
>
> In R-devel you will be able to have
>
>  	utils:::make.packages.html()
>
> in ~/.Rprofile and that takes a few seconds at most.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 827 bytes
Desc: This is a digitally signed message part.
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20071203/0731fa5b/attachment.bin 

From ripley at stats.ox.ac.uk  Mon Dec  3 14:21:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 3 Dec 2007 13:21:47 +0000 (GMT)
Subject: [R] Attempt at package documentation on debian: link to
 "data.frame" broken
In-Reply-To: <200712031418.32349.johannes_graumann@web.de>
References: <fj0gn5$fcu$1@ger.gmane.org> <fj0tvq$p81$1@ger.gmane.org>
	<Pine.LNX.4.64.0712031306310.7474@gannet.stats.ox.ac.uk>
	<200712031418.32349.johannes_graumann@web.de>
Message-ID: <Pine.LNX.4.64.0712031320300.11000@gannet.stats.ox.ac.uk>

On Mon, 3 Dec 2007, Johannes Graumann wrote:

> Hmmm, I see. This is all local. So there does not seem to be a way to make any
> old R installation that loads the package initialize this functionality, so
> that the help files will be correctly interlinked directly after "require"
> or "library"? How do other packages manage that?

They don't: it is up to the users.  Many users never use HTML help, and 
many do not install in /usr/lib/R/site-library.

>
> Joh
>
> On Monday 03 December 2007 14:08:47 Prof Brian Ripley wrote:
>> On Mon, 3 Dec 2007, Johannes Graumann wrote:
>>> <posted & mailed>
>>>
>>> Prof Brian Ripley wrote:
>>>>> Using non-linked HTML file: style sheet and hyperlinks may be incorrect
>>>>> - which is what happens ... how to switch to "linked HTML file"?
>>>>
>>>> It all works via help.start() (as I did say), at least in a reliable R
>>>> front end.  Running help.start() is how you set this up.
>>>
>>> You are right - naturally. After calling 'help.start()' everything works
>>> just fine. Is there any way one can call 'help.start()' transparently to
>>> the user when s/he accesses the html-help?
>>
>> It is potentially very slow (ca 40 secs on our dept system with 1500
>> NFS-mounted packages) in R 2.6.1.  But you could put utils::help.start()
>> in ~/.Rprofile.
>>
>> In R-devel you will be able to have
>>
>>  	utils:::make.packages.html()
>>
>> in ~/.Rprofile and that takes a few seconds at most.
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Thierry.ONKELINX at inbo.be  Mon Dec  3 14:43:06 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 3 Dec 2007 14:43:06 +0100
Subject: [R] ggplot2: Choosing colours
In-Reply-To: <4753FFA6.3060602@unicas.it>
References: <2E9C414912813E4EB981326983E0A10403F40E45@inboexch.inbo.be>
	<4753FFA6.3060602@unicas.it>
Message-ID: <2E9C414912813E4EB981326983E0A10403F40F1B@inboexch.inbo.be>

Dear Domenico,

It looks like my question wasn't very clear. I want to specify the
colours meself. Colour = z will use default colours and I'd like to
change those. I need to change them into red, green, blue and black
instead of pink, brown, green and blue.

The solution should have the layout of:

ggplot(data = dataset, aes(x = x, y = y, colour = z)) + geom_point()

But the colours of:

ggplot(data = dataset, aes(x = x, y = y, group = z)) + geom_point(colour
= c("red", "green", "blue", "black"))


Cheers,

Thierry


------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be 
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: Domenico Vistocco [mailto:vistocco op unicas.it] 
Verzonden: maandag 3 december 2007 14:08
Aan: ONKELINX, Thierry
CC: r-help op r-project.org
Onderwerp: Re: [R] ggplot2: Choosing colours

qplot(data=dataset, x, y, colour=z)

ONKELINX, Thierry wrote:
> Dear useRs,
>
> I'm trying to specify the colour of a factor with ggplot2. The example
> below gets me close to what I want, but it's missing a legend.
>
> Any ideas?
>
> Thanks,
>
> Thierry
>
> library(ggplot2)
> dataset <- data.frame(x = rnorm(40), y = runif(40), z = gl(4, 10,
labels
> = LETTERS[1:4]))
> ggplot(data = dataset, aes(x = x, y = y, group = z)) +
geom_point(colour
> = c("red", "green", "blue", "black"))
>
>
>
------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium 
> tel. + 32 54/436 185
> Thierry.Onkelinx op inbo.be 
> www.inbo.be 
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection
of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
> ______________________________________________
> R-help op r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   


From bolker at ufl.edu  Mon Dec  3 14:49:46 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 3 Dec 2007 05:49:46 -0800 (PST)
Subject: [R] Why is the program too slow?
In-Reply-To: <200712032112261250392@gmail.com>
References: <200712032112261250392@gmail.com>
Message-ID: <14130312.post@talk.nabble.com>




zhang jian wrote:
> 
> Hi,everyone.
> I use the following program calculates Fisher's alpha from counts of
> individuals and species. The program is wrote by Prof. Kyle Harm.
> However, when I run the program, it can work very quickly sometimes, but
> it can not work very well sometimes. It depends on the counts of
> individuals and species.
> For example,
> 
>> calc.alpha(1000,70)
> [1] 17.14375
>> calc.alpha(10000,70)
> [1] 10.15460
>> calc.alpha(100,7)
> [1] 1.714375
> 
> But,
>> calc.alpha(1580,30)
>> calc.alpha(1000,7)
> 
> It is very slow.
> 
> So, what is the problem?
> Thanks very much.
>                                                       Jian Zhang
> 

Don't know, but I would try out the alternatives fisherfit [vegan package]
and fisher [untb package] and see if either behaves better (it was faster
for me to look these up than to dig through the code and see what's
going on for these examples).

   You could also try contacting Prof. Harms [sic] and asking him ...

  cheers
    Ben Bolker

-- 
View this message in context: http://www.nabble.com/Why-is-the-program-too-slow--tf4936535.html#a14130312
Sent from the R help mailing list archive at Nabble.com.


From yn19832 at msn.com  Mon Dec  3 14:58:55 2007
From: yn19832 at msn.com (livia)
Date: Mon, 3 Dec 2007 05:58:55 -0800 (PST)
Subject: [R]  Linear Regression, Data is a list
Message-ID: <14130322.post@talk.nabble.com>


Hello,

I would like to perform a linear regression and the data is a list.e.g
lm(list$abc~., data=list) or lm(abc~., data=list), which would give the same
result.

The problem is I would like to call the response variable in a more general
form. What I try to achieve is sth like lm(list$(paste("a","b","c"))~.,
data=list), but it does not work. 

Could anyone give me some advice?Many thanks.


-- 
View this message in context: http://www.nabble.com/Linear-Regression%2C-Data-is-a-list-tf4936685.html#a14130322
Sent from the R help mailing list archive at Nabble.com.


From dimitris.rizopoulos at med.kuleuven.be  Mon Dec  3 15:02:30 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 3 Dec 2007 15:02:30 +0100
Subject: [R] Why is the program too slow?
References: <200712032112261250392@gmail.com>
Message-ID: <003101c835b5$254fd240$0540210a@www.domain>

try

RSiteSearch("Fisher's alpha")

which indicates that there is the fishers.alpha() function in package 
'untb' that does the same thing, e.g.,

fishers.alpha(1000, 70)

fishers.alpha(1580, 30)
fishers.alpha(1000, 7)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Jian Zhang" <jzhang1982 at gmail.com>
To: "R-help" <r-help at stat.math.ethz.ch>
Sent: Monday, December 03, 2007 2:12 PM
Subject: [R] Why is the program too slow?


> Hi,everyone.
> I use the following program calculates Fisher's alpha from counts of 
> individuals and species. The program is wrote by Prof. Kyle Harm.
> However, when I run the program, it can work very quickly sometimes, 
> but it can not work very well sometimes. It depends on the counts of 
> individuals and species.
> For example,
>
>> calc.alpha(1000,70)
> [1] 17.14375
>> calc.alpha(10000,70)
> [1] 10.15460
>> calc.alpha(100,7)
> [1] 1.714375
>
> But,
>> calc.alpha(1580,30)
>> calc.alpha(1000,7)
>
> It is very slow.
>
> So, what is the problem?
> Thanks very much.
>                                                      Jian Zhang
>
>
> # The following function calculates Fisher's alpha from counts of 
> individuals and species.
> # Note that this program assumes that the true value of alpha lies 
> within the range 0.001?C10000
> # (a likely assumption for local assemblages of organisms).
> # The function returns "-1" if there is a problem.
>
> calc.alpha=function(n.orig, s.orig)
> {
>  a=numeric()
>
>  len.n=length(n.orig)
>  len.s=length(s.orig)
>
>  if(len.n != len.s)
>    { return(-1)  }
>
>  for(i in 1:len.n)
>   {
>     if(n.orig[i]<=0 | s.orig[i]<=0 | n.orig[i]<=s.orig[i])
> { a[i]=(-1) }
>
>     else
>      {
>       low.a=0.001
>       high.a=10000
>       low.s = low.a*log(1+(n.orig[i]/low.a))
>       high.s = high.a*log(1+(n.orig[i]/high.a))
>
>       if((s.orig[i]<=low.s) | (s.orig[i]>=high.s))
>        { a[i]=(-1) }
>
>       else
>        {
>         use.s=s.orig[i]+1
>         while(s.orig[i] != use.s)
>          {
>           use.a=(low.a+high.a)/2
>           use.s=use.a*log(1+(n.orig[i]/use.a))
>
>           if(s.orig[i]<use.s)
>  { high.a=use.a }
>
>           if(s.orig[i]>use.s)
>  { low.a=use.a }
>          }
>         a[i]=use.a
>        }
>      }
>   }
>
>   return(a)
> }
>
> [[alternative HTML version deleted]]
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From h.wickham at gmail.com  Mon Dec  3 15:22:39 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 3 Dec 2007 08:22:39 -0600
Subject: [R] ggplot2: Choosing colours
In-Reply-To: <2E9C414912813E4EB981326983E0A10403F40E45@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A10403F40E45@inboexch.inbo.be>
Message-ID: <f8e6ff050712030622p274b0dbeyaa124e3b885eec0d@mail.gmail.com>

Hi Thierry,

Have a look at http://had.co.nz/ggplot2/scale_identity.  (The way you
are currently doing it only works because you are not using any kind
of grouping/facetting, and I'm thinking about adding an explicit
message/warning for this case)

Hadley

On 12/3/07, ONKELINX, Thierry <Thierry.ONKELINX at inbo.be> wrote:
> Dear useRs,
>
> I'm trying to specify the colour of a factor with ggplot2. The example
> below gets me close to what I want, but it's missing a legend.
>
> Any ideas?
>
> Thanks,
>
> Thierry
>
> library(ggplot2)
> dataset <- data.frame(x = rnorm(40), y = runif(40), z = gl(4, 10, labels
> = LETTERS[1:4]))
> ggplot(data = dataset, aes(x = x, y = y, group = z)) + geom_point(colour
> = c("red", "green", "blue", "black"))
>
>
> ------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be
> www.inbo.be
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://had.co.nz/


From dwinsemius at comcast.net  Mon Dec  3 15:24:12 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 3 Dec 2007 14:24:12 +0000 (UTC)
Subject: [R] help on qcc
References: <OF7566C5AF.C84AC5C2-ONC12573A6.0036ACCB-C12573A6.003D04FA@de.ibm.com>
Message-ID: <Xns99FB5FAACDDCdNOTwinscomcast@80.91.229.13>

Thorsten Muehge <MUEHGE at de.ibm.com> wrote in
news:OF7566C5AF.C84AC5C2-ONC12573A6.0036ACCB-C12573A6.003D04FA at de.ibm.com

> Hello R Experts,
> I started to work with the qcc package and it wprks quite nicely.
> 
> Heres My Code:
> n <- 
> c(55,5,94,25,10,15,15,40,44,34,90,114,204,37,30,28,12,68,64,29,24,14,31
> ,16,62,45,55,20,24,14,9,19,76,57,55,42,6, 
> 54,32,117,19,32,9,11,13,31,27,33,44,28) x <-
> c(6,0,30,5,2,1,4,5,2,9,12,24,64,3,9,21,9,48,15,4,3,0,1,0,6, 
> 7,2,0,3,2,0,5,1,2,6,9,1,6,2,0,1,4,8,0,1,1,3,0,0,0) 
> 
> qcc(x,type="p",size=n,title = "p-Charts Failed data Transmission");
> 
> I see the numbers beyond limits and the number of violating runs.
> 
> How can I switch of the number of violating runs?
> How can I specify the number of violating runs?

I am not an experienced user of qcc, (nor am I an expert), but I doubt 
that very many of the other readers of r-help are users of qcc either. 
Looking at the code of qcc and then at help for the package, the number 
of violating runs is determined through the shewhart.rules() function and 
the gcc.options$("run.length") argument. It looks as though you could 
send arguments to the rules= parameter when calling qcc() that would 
override the default. Looking further at the package documantation and 
the qcc.options() function my suggestion (which I tested and seems to 
have the effect I believe you requested) would be to try something like:

qcc.options("run.length"=5) #before the qcc(...) invocation

....and if you wanted to "turn it off", then set "run.length" to 
something impossibly high.

qcc.options("run.length"=1000) # number of violating runs becomes zero

If that does not work, then another method for getting help with a 
package is to send an email to the package maintainer.

-- 
David Winsemius


From friendly at yorku.ca  Mon Dec  3 15:27:07 2007
From: friendly at yorku.ca (Michael Friendly)
Date: Mon, 03 Dec 2007 09:27:07 -0500
Subject: [R] cor(data.frame) infelicities
Message-ID: <4754123B.5030902@yorku.ca>

In using cor(data.frame), it is annoying that you have to explicitly 
filter out non-numeric columns, and when you don't, the error message
is misleading:

 > cor(iris)
Error in cor(iris) : missing observations in cov/cor
In addition: Warning message:
In cor(iris) : NAs introduced by coercion

It would be nicer if stats:::cor() did the equivalent *itself* of the 
following for a data.frame:
 > cor(iris[,sapply(iris, is.numeric)])
              Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
 >

A change could be implemented here:
     if (is.data.frame(x))
         x <- as.matrix(x)

Second, the default, use="all" throws an error if there are any
NAs.  It would be nicer if the default was use="complete.cases",
which would generate warnings instead.  Most other statistical
software is more tolerant of missing data.

 > library(corrgram)
 > data(auto)
 > cor(auto[,sapply(auto, is.numeric)])
Error in cor(auto[, sapply(auto, is.numeric)]) :
   missing observations in cov/cor
 > cor(auto[,sapply(auto, is.numeric)],use="complete")
# works; output elided

-Michael

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA


From ggrothendieck at gmail.com  Mon Dec  3 15:31:45 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Dec 2007 09:31:45 -0500
Subject: [R] cor(data.frame) infelicities
In-Reply-To: <4754123B.5030902@yorku.ca>
References: <4754123B.5030902@yorku.ca>
Message-ID: <971536df0712030631g647edbb2g155d3ffd5cfe0241@mail.gmail.com>

You can calculate the Kendall rank correlation with such a matrix
so you would not want to exclude factors in that case:

> cor(iris, method = "kendall")
             Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
Sepal.Length   1.00000000 -0.07699679    0.7185159   0.6553086  0.6704444
Sepal.Width   -0.07699679  1.00000000   -0.1859944  -0.1571257 -0.3376144
Petal.Length   0.71851593 -0.18599442    1.0000000   0.8068907  0.8229112
Petal.Width    0.65530856 -0.15712566    0.8068907   1.0000000  0.8396874
Species        0.67044444 -0.33761438    0.8229112   0.8396874  1.0000000


On Dec 3, 2007 9:27 AM, Michael Friendly <friendly at yorku.ca> wrote:
> In using cor(data.frame), it is annoying that you have to explicitly
> filter out non-numeric columns, and when you don't, the error message
> is misleading:
>
>  > cor(iris)
> Error in cor(iris) : missing observations in cov/cor
> In addition: Warning message:
> In cor(iris) : NAs introduced by coercion
>
> It would be nicer if stats:::cor() did the equivalent *itself* of the
> following for a data.frame:
>  > cor(iris[,sapply(iris, is.numeric)])
>              Sepal.Length Sepal.Width Petal.Length Petal.Width
> Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
> Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
> Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
> Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
>  >
>
> A change could be implemented here:
>     if (is.data.frame(x))
>         x <- as.matrix(x)
>
> Second, the default, use="all" throws an error if there are any
> NAs.  It would be nicer if the default was use="complete.cases",
> which would generate warnings instead.  Most other statistical
> software is more tolerant of missing data.
>
>  > library(corrgram)
>  > data(auto)
>  > cor(auto[,sapply(auto, is.numeric)])
> Error in cor(auto[, sapply(auto, is.numeric)]) :
>   missing observations in cov/cor
>  > cor(auto[,sapply(auto, is.numeric)],use="complete")
> # works; output elided
>
> -Michael
>
> --
> Michael Friendly     Email: friendly AT yorku DOT ca
> Professor, Psychology Dept.
> York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> Toronto, ONT  M3J 1P3 CANADA
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From J.Hadfield at ed.ac.uk  Mon Dec  3 15:54:29 2007
From: J.Hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 3 Dec 2007 14:54:29 +0000
Subject: [R] linking C/C++ external libraries.
Message-ID: <7132FEC5-FA92-406F-A8F0-1A020577CE97@ed.ac.uk>

Hi Everyone,

I'm trying to load some C++ code using dyn.load but I'm getting  
unresolved symbols associated with some external libraries  
(CSparse).  I gather this is something to do with linking as the the  
code compiles fine.  However, I've passed

-L/home/jarrod/My_Programs/SuiteSparse/CSparse/Lib -lcsparse

to the complier (g++), either directly using R CMD SHLIB or as  
PKG_LIBS in a Makevars file, and I cannot resolve the problem.  I'm  
working with R 2.6.0 on fedora 6

Any help would be appreciated.

Thanks,

Jarrod


From Thierry.ONKELINX at inbo.be  Mon Dec  3 15:51:55 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 3 Dec 2007 15:51:55 +0100
Subject: [R] ggplot2: Choosing colours
In-Reply-To: <f8e6ff050712030622p274b0dbeyaa124e3b885eec0d@mail.gmail.com>
References: <2E9C414912813E4EB981326983E0A10403F40E45@inboexch.inbo.be>
	<f8e6ff050712030622p274b0dbeyaa124e3b885eec0d@mail.gmail.com>
Message-ID: <2E9C414912813E4EB981326983E0A10403F40FA3@inboexch.inbo.be>

Thanks Hadley.

The page you referred to was actually
http://had.co.nz/ggplot2/scale_identity.html

The solution to my problem was

library(ggplot2)
dataset <- data.frame(x = rnorm(40), y = runif(40), z = gl(4, 10, labels
= LETTERS[1:4]))
dataset$MyColour <- factor(dataset$z, labels = c("red", "green", "blue",
"black"))
ggplot(data = dataset, aes(x = x, y = y, colour = MyColour)) +
geom_point() + scale_colour_identity(labels=levels(dataset$z),
grob="tile", name="z") 


------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be 
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: hadley wickham [mailto:h.wickham op gmail.com] 
Verzonden: maandag 3 december 2007 15:23
Aan: ONKELINX, Thierry
CC: r-help op r-project.org
Onderwerp: Re: [R] ggplot2: Choosing colours

Hi Thierry,

Have a look at http://had.co.nz/ggplot2/scale_identity.  (The way you
are currently doing it only works because you are not using any kind
of grouping/facetting, and I'm thinking about adding an explicit
message/warning for this case)

Hadley

On 12/3/07, ONKELINX, Thierry <Thierry.ONKELINX op inbo.be> wrote:
> Dear useRs,
>
> I'm trying to specify the colour of a factor with ggplot2. The example
> below gets me close to what I want, but it's missing a legend.
>
> Any ideas?
>
> Thanks,
>
> Thierry
>
> library(ggplot2)
> dataset <- data.frame(x = rnorm(40), y = runif(40), z = gl(4, 10,
labels
> = LETTERS[1:4]))
> ggplot(data = dataset, aes(x = x, y = y, group = z)) +
geom_point(colour
> = c("red", "green", "blue", "black"))
>
>
>
------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx op inbo.be
> www.inbo.be
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection
of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
> ______________________________________________
> R-help op r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://had.co.nz/


From 07419287 at hkbu.edu.hk  Sun Dec  2 16:26:41 2007
From: 07419287 at hkbu.edu.hk (fancy hou)
Date: Sun, 2 Dec 2007 23:26:41 +0800
Subject: [R] find the numerical of acf
Message-ID: <611cfe5a0712020726p3bed7d5dt9632f210dc38d33b@mail.gmail.com>

I want to know how to use R to get the
numerical values of the estimated ACF b(k) and PACF b
kk ?


From wwwhsd at gmail.com  Mon Dec  3 16:05:01 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 3 Dec 2007 13:05:01 -0200
Subject: [R] find the numerical of acf
In-Reply-To: <611cfe5a0712020726p3bed7d5dt9632f210dc38d33b@mail.gmail.com>
References: <611cfe5a0712020726p3bed7d5dt9632f210dc38d33b@mail.gmail.com>
Message-ID: <da79af330712030705o248f7477i18be51f5148110c7@mail.gmail.com>

Perhaps:

acf(rnorm(100), plot=FALSE)$acf[,,1]
pacf(rnorm(100), plot=FALSE)$acf[,,1]

On 02/12/2007, fancy hou <07419287 at hkbu.edu.hk> wrote:
> I want to know how to use R to get the
> numerical values of the estimated ACF b (k) and PACF b kk ?
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From knapp at lifesci.ucsb.edu  Mon Dec  3 16:28:38 2007
From: knapp at lifesci.ucsb.edu (Roland Knapp)
Date: Mon, 3 Dec 2007 07:28:38 -0800
Subject: [R] permutation tests of regression trees
Message-ID: <002701c835c1$45f2cf00$d1d86d00$@ucsb.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/26c18e65/attachment.pl 

From wwwhsd at gmail.com  Mon Dec  3 16:31:05 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 3 Dec 2007 13:31:05 -0200
Subject: [R] Linear Regression, Data is a list
In-Reply-To: <14130322.post@talk.nabble.com>
References: <14130322.post@talk.nabble.com>
Message-ID: <da79af330712030731m7881a655k426600a3c49708c8@mail.gmail.com>

If i understand your question, you can try this:

 lapply(paste("lis$", names(lis), " ~ . ", sep=""), lm, data=lis)

On 03/12/2007, livia <yn19832 at msn.com> wrote:
>
> Hello,
>
> I would like to perform a linear regression and the data is a list.e.g
> lm(list$abc~., data=list) or lm(abc~., data=list), which would give the same
> result.
>
> The problem is I would like to call the response variable in a more general
> form. What I try to achieve is sth like lm(list$(paste("a","b","c"))~.,
> data=list), but it does not work.
>
> Could anyone give me some advice?Many thanks.
>
>
> --
> View this message in context: http://www.nabble.com/Linear-Regression%2C-Data-is-a-list-tf4936685.html#a14130322
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From remisorama at gmail.com  Mon Dec  3 16:31:36 2007
From: remisorama at gmail.com (rem la)
Date: Mon, 3 Dec 2007 17:31:36 +0200
Subject: [R] restore NAs in residuals
Message-ID: <bba82c9f0712030731o235bdb5ch8abdca6891ac3805@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/e67be5be/attachment.pl 

From ggrothendieck at gmail.com  Mon Dec  3 16:43:20 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Dec 2007 10:43:20 -0500
Subject: [R] Linear Regression, Data is a list
In-Reply-To: <14130322.post@talk.nabble.com>
References: <14130322.post@talk.nabble.com>
Message-ID: <971536df0712030743m503ad248yc7904c842540580a@mail.gmail.com>

See:

http://www.nabble.com/matrix-(column-wise)-multiple-regression-t4862261.html

On Dec 3, 2007 8:58 AM, livia <yn19832 at msn.com> wrote:
>
> Hello,
>
> I would like to perform a linear regression and the data is a list.e.g
> lm(list$abc~., data=list) or lm(abc~., data=list), which would give the same
> result.
>
> The problem is I would like to call the response variable in a more general
> form. What I try to achieve is sth like lm(list$(paste("a","b","c"))~.,
> data=list), but it does not work.
>
> Could anyone give me some advice?Many thanks.
>
>
> --
> View this message in context: http://www.nabble.com/Linear-Regression%2C-Data-is-a-list-tf4936685.html#a14130322
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Mon Dec  3 16:45:19 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Dec 2007 10:45:19 -0500
Subject: [R] restore NAs in residuals
In-Reply-To: <bba82c9f0712030731o235bdb5ch8abdca6891ac3805@mail.gmail.com>
References: <bba82c9f0712030731o235bdb5ch8abdca6891ac3805@mail.gmail.com>
Message-ID: <971536df0712030745g566469b7y4408be5e8e6d69d0@mail.gmail.com>

Try this:

> resid(lm(tt~year, na.action = na.exclude))
         1          2          3          4          5          6          7
 0.7114894  1.2231383  0.1447872 -1.8035638 -0.8519149 -0.5302660         NA
         8          9         10
        NA         NA  1.1063298


On Dec 3, 2007 10:31 AM, rem la <remisorama at gmail.com> wrote:
> Dear All,
>
> I have two vectors:
>
> tt = c(6.87, 7.43, 6.4, 4.5, 5.5, 5.87, NA, NA, NA, 7.7)
> year = 1966:1975
>
> Residuals
>
> lm(tt~year)$res
>
> do not contain NAs for the three years of missing temperature tt. Is there a
> simple way to get these NAs back into the residual's vector?
>
> Thank you,
> Sorama
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Mon Dec  3 17:27:49 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 3 Dec 2007 16:27:49 +0000 (GMT)
Subject: [R] restore NAs in residuals
In-Reply-To: <bba82c9f0712030731o235bdb5ch8abdca6891ac3805@mail.gmail.com>
References: <bba82c9f0712030731o235bdb5ch8abdca6891ac3805@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0712031621480.17196@gannet.stats.ox.ac.uk>

You need to use na.action=na.exclude, _and_ to use the proper extractor 
function residuals() rather than pick up a component of the fitted object 
by partial matching.

See ?residuals.lm and ?naresid

On Mon, 3 Dec 2007, rem la wrote:

> Dear All,
>
> I have two vectors:
>
> tt = c(6.87, 7.43, 6.4, 4.5, 5.5, 5.87, NA, NA, NA, 7.7)
> year = 1966:1975
>
> Residuals
>
> lm(tt~year)$res
>
> do not contain NAs for the three years of missing temperature tt. Is there a
> simple way to get these NAs back into the residual's vector?
>
> Thank you,
> Sorama

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From h.wickham at gmail.com  Mon Dec  3 17:30:19 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 3 Dec 2007 10:30:19 -0600
Subject: [R] using color coded colorbars for bar plots
In-Reply-To: <4752940C.3040008@bitwrit.com.au>
References: <A710A72A775FA24B8B6943719F3DE1B052090C@extas4-hba.tas.csiro.au>
	<474FF1E1.1050207@bitwrit.com.au>
	<A710A72A775FA24B8B6943719F3DE1B0520914@extas4-hba.tas.csiro.au>
	<4752940C.3040008@bitwrit.com.au>
Message-ID: <f8e6ff050712030830s6f990d17q547857c2638f743a@mail.gmail.com>

On 12/2/07, Jim Lemon <jim at bitwrit.com.au> wrote:
> James.Dell at csiro.au wrote:
> > Hi Jim,
> > Thanks for getting back to me so quickly.
> >
> > I did look at color.legend, but that seems to plot colored blocks for
> > the observations (in this case the mean) and not for the color.scale
> > (which represents variance in this case).  Unless there is a
> > functionality that I haven't discovered yet.  If you have created a
> > similar plot and would be happy to share some code I'd be very
> > apprecitive.
> >
> Part of the problem is that you seem to have two names for the same
> variable in your code (Standard.Deviance and Standard.Deviation - unless
> that was a typo). Notice how I calculate the colors twice, the second
> time with a simple integer sequence to get the right number of evenly
> spaced colors. In your example, you calculated the colors for
> RankVar$Standard.Deviance again, but you don't need all those colors for
> the legend, and they're in the wrong order anyway. What is generally
> wanted for a color legend is the minimum and maximum values on the ends
> and a few linear interpolations in the middle.
>
> barplot(RankVar$MeanDecreaseAccuracy,
>   col=color.scale(RankVar$Standard.Deviance,
>   c(0,1,1),c(1,1,0),0),
>   ylab = "Variable Importance",
>   names.arg = rownames(RankVar),
>   cex.names = .7,
>   main = "Variables from RandomFishForest",
>   sub= "Mean Decrease in Accuracy")
> col.labels<- c("Low","Mid","High")
> color.legend(6,13,11,14,col.labels,
>   rect.col=color.scale(1:5,c(0,1,1),c(1,1,0),0))

Another option would be to use ggplot2:

install.packages("ggplot2")
library(ggplot2)

qplot(rownames(RankVar), MeanDecreaseAccuracy, data = RankVar, colour
= Standard.Deviance)

and then use http://had.co.nz/ggplot2/scale_colour_gradient.html to
control the choice of colours for the mapping.

Regards,

Hadley

-- 
http://had.co.nz/


From hkiws at gmx.de  Mon Dec  3 17:51:07 2007
From: hkiws at gmx.de (EUROPOL)
Date: Mon, 03 Dec 2007 17:51:07 +0100
Subject: [R] Problem with scan() from UTF-8 encoded URL
In-Reply-To: <bba82c9f0712030731o235bdb5ch8abdca6891ac3805@mail.gmail.com>
References: <bba82c9f0712030731o235bdb5ch8abdca6891ac3805@mail.gmail.com>
Message-ID: <475433FB.7090308@gmx.de>

Hallo,

I am trying to import a website and structure it from within R:

The following code:

data <-
scan(file='http://en.wikipedia.org/wiki/Special:Recentchanges',what='character')

results in the error:

Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open: HTTP status was '403 Forbidden' in: file(file, "r")

It seems that the error is connected to the UTF-8-format of wikipedia,
since the following line works:

data <- scan(file='http://www.google.de',what='character')

I am looking forward to your answers.

Greetings

Marc Schwenzer


From edd at debian.org  Mon Dec  3 17:57:09 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 3 Dec 2007 10:57:09 -0600
Subject: [R] linking C/C++ external libraries.
In-Reply-To: <7132FEC5-FA92-406F-A8F0-1A020577CE97@ed.ac.uk>
References: <7132FEC5-FA92-406F-A8F0-1A020577CE97@ed.ac.uk>
Message-ID: <18260.13669.959844.92169@ron.nulle.part>


On 3 December 2007 at 14:54, Jarrod Hadfield wrote:
| I'm trying to load some C++ code using dyn.load but I'm getting  
| unresolved symbols associated with some external libraries  
| (CSparse).  I gather this is something to do with linking as the the  
| code compiles fine.  However, I've passed
| 
| -L/home/jarrod/My_Programs/SuiteSparse/CSparse/Lib -lcsparse
| 
| to the complier (g++), either directly using R CMD SHLIB or as  
| PKG_LIBS in a Makevars file, and I cannot resolve the problem.  I'm  
| working with R 2.6.0 on fedora 6
| 
| Any help would be appreciated.

Can you show us your error message upon load?  What does ldd say when pointed
at your package's library?  How exactly is the linking done?

Compare all that to a working library such as eg Matrix. That may provide
further clues.

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From john.seers at bbsrc.ac.uk  Mon Dec  3 18:00:26 2007
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Mon, 3 Dec 2007 17:00:26 -0000
Subject: [R] Problem with scan() from UTF-8 encoded URL
In-Reply-To: <475433FB.7090308@gmx.de>
References: <bba82c9f0712030731o235bdb5ch8abdca6891ac3805@mail.gmail.com>
	<475433FB.7090308@gmx.de>
Message-ID: <AAD49F46EAE3F6479E1D46428FAC31CB02DD2DAA@NBIE2KSRV1.nbi.bbsrc.ac.uk>

 

Hello

Works fine for me:

> data
<-scan(file='http://en.wikipedia.org/wiki/Special:Recentchanges',what='c
haracter')
Read 3581 items
> 

So I don't think it is the Wikipedia end.

Regards

John Seers


 
---

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of EUROPOL
Sent: 03 December 2007 16:51
To: r-help at stat.math.ethz.ch
Subject: [R] Problem with scan() from UTF-8 encoded URL

Hallo,

I am trying to import a website and structure it from within R:

The following code:

data <-
scan(file='http://en.wikipedia.org/wiki/Special:Recentchanges',what='cha
racter')

results in the error:

Error in file(file, "r") : unable to open connection In addition:
Warning message:
cannot open: HTTP status was '403 Forbidden' in: file(file, "r")

It seems that the error is connected to the UTF-8-format of wikipedia,
since the following line works:

data <- scan(file='http://www.google.de',what='character')

I am looking forward to your answers.

Greetings

Marc Schwenzer

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From J.Hadfield at ed.ac.uk  Mon Dec  3 18:28:12 2007
From: J.Hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 3 Dec 2007 17:28:12 +0000
Subject: [R] linking C/C++ external libraries.
In-Reply-To: <18260.13669.959844.92169@ron.nulle.part>
References: <7132FEC5-FA92-406F-A8F0-1A020577CE97@ed.ac.uk>
	<18260.13669.959844.92169@ron.nulle.part>
Message-ID: <ACB79147-959D-43EC-83E7-C80C6E7858B7@ed.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/19b7d3c8/attachment.pl 

From vdemart1 at tin.it  Mon Dec  3 18:11:47 2007
From: vdemart1 at tin.it (vittorio)
Date: Mon, 3 Dec 2007 18:11:47 +0100
Subject: [R] Plotting monthly timeseries with an x-axis in "time format"
Message-ID: <200712031811.47949.vdemart1@tin.it>

I have the following timeseries "tab"
=====================================
> str(tab)
 mts [1:23, 1:2] 79.5 89.1 84.9 75.7 72.8 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:2] "Ipex...I" "Omel...E"
 - attr(*, "tsp")= num [1:3] 2006 2008   12
 - attr(*, "class")= chr [1:2] "mts" "ts"

> tab
         Ipex...I Omel...E
Jan 2006    79.47    77.95
Feb 2006    89.13    76.73
Mar 2006    84.86    51.20
Apr 2006    75.68    51.86
May 2006    72.82    51.29
Jun 2006    78.87    49.45
Jul 2006    93.46    53.88
Aug 2006    78.18    47.96
Sep 2006    82.46    55.07
Oct 2006    77.25    45.34
Nov 2006    80.95    37.07
Dec 2006    84.39    37.53
Jan 2007    81.70    47.79
Feb 2007    74.76    37.50
Mar 2007    65.29    30.35
Apr 2007    60.30    37.78
May 2007    66.59    34.13
Jun 2007    73.19    39.14
Jul 2007    92.39    39.89
Aug 2007    65.76    35.46
Sep 2007    77.45    36.54
Oct 2007    74.22    38.39
Nov 2007   101.36    47.33
Dec 2007   100.01  45.34
===============================

Plotting tab with a simple "plot(tab,plot.type="single")" I'm obtaining a 
graph  with the x axis in an orrible decimal format so that,e.g., Jan 2006 is 
2006.0 and Nov 2006 is 2006.8(33)!

 Instead I would like to see the x-axis in a more human-readable format, for 
instance, 12 tics for each year and a label at the beginning of each quarter 
of the year: 2006.1, 2006.4,2006.7.
 - OR -
more elegantly, I would like to have the 12 tics with the month shortened 
labels: Jan, Feb, etc. and below, say June, one label for the year.

Please help.

Ciao
Vittorio


From Filippo.Biscarini at wur.nl  Mon Dec  3 18:21:08 2007
From: Filippo.Biscarini at wur.nl (Biscarini, Filippo)
Date: Mon, 3 Dec 2007 18:21:08 +0100
Subject: [R] overlapping labels
Message-ID: <7124EE4D7E556844A654A607A4203FDD3ED3CD@scomp0038.wurnet.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/678cd62c/attachment.pl 

From basu.15 at osu.edu  Mon Dec  3 18:37:36 2007
From: basu.15 at osu.edu (Deepankar Basu)
Date: Mon, 03 Dec 2007 12:37:36 -0500
Subject: [R] speeding up likelihood computation
In-Reply-To: <644e1f320712021754o5cc72303me0d7d3d4c681bb7@mail.gmail.com>
References: <110f9d445.d445110f9@osu.edu>
	<644e1f320712021754o5cc72303me0d7d3d4c681bb7@mail.gmail.com>
Message-ID: <1196703456.10417.4.camel@localhost>

Hi Jim,

Thanks for your suggestion. My guess is that most of the time is taken
by various kinds of assignments that I am making using loops; and these
can be done without loops. In a follow-up post, I will try to explain in
greater detail each part of my code (especially the parts where
assignments are being made) with comments.

Meanwhile I am copying the output from Rprof below. Any suggestions on
how to increase efficiency on the basis of this output?

> summaryRprof("example1.out")
$by.self
                     self.time self.pct total.time total.pct
fn                      419.64     74.2     564.72      99.9
pnorm                    28.16      5.0      35.02       6.2
matrix                   25.58      4.5     213.26      37.7
sum                      17.02      3.0      17.02       3.0
*                        10.90      1.9      10.90       1.9
exp                       7.76      1.4       7.76       1.4
factorial                 7.38      1.3      11.16       2.0
^                         7.20      1.3       7.20       1.3
-                         6.50      1.1       6.50       1.1
as.vector                 5.12      0.9     196.52      34.8
vector                    4.88      0.9       4.88       0.9
+                         3.66      0.6       3.66       0.6
(                         3.42      0.6       3.42       0.6
>                         2.78      0.5       2.78       0.5
gamma                     2.68      0.5       2.68       0.5
numeric                   2.42      0.4       7.30       1.3
&                         2.36      0.4       2.36       0.4
/                         2.24      0.4       2.24       0.4
==                        2.08      0.4       2.08       0.4
:                         1.58      0.3       1.58       0.3
$                         1.10      0.2       1.10       0.2
dimnames<-                0.28      0.0       0.28       0.0
FUN                       0.26      0.0     193.72      34.3
.Call                     0.24      0.0     395.60      70.0
<Anonymous>               0.08      0.0     565.18     100.0
apply                     0.04      0.0     193.50      34.2
paste                     0.02      0.0       0.28       0.0
as.data.frame.matrix      0.02      0.0       0.02       0.0
is.null                   0.02      0.0       0.02       0.0
genoud                    0.00      0.0     565.42     100.0
t                         0.00      0.0     193.50      34.2
optim                     0.00      0.0     158.70      28.1
do.call                   0.00      0.0       0.30       0.1
mfunc                     0.00      0.0       0.30       0.1
f                         0.00      0.0       0.28       0.0
lapply                    0.00      0.0       0.26       0.0
as.data.frame             0.00      0.0       0.02       0.0

$by.total
                     total.time total.pct self.time self.pct
genoud                   565.42     100.0      0.00      0.0
<Anonymous>              565.18     100.0      0.08      0.0
fn                       564.72      99.9    419.64     74.2
.Call                    395.60      70.0      0.24      0.0
matrix                   213.26      37.7     25.58      4.5
as.vector                196.52      34.8      5.12      0.9
FUN                      193.72      34.3      0.26      0.0
apply                    193.50      34.2      0.04      0.0
t                        193.50      34.2      0.00      0.0
optim                    158.70      28.1      0.00      0.0
pnorm                     35.02       6.2     28.16      5.0
sum                       17.02       3.0     17.02      3.0
factorial                 11.16       2.0      7.38      1.3
*                         10.90       1.9     10.90      1.9
exp                        7.76       1.4      7.76      1.4
numeric                    7.30       1.3      2.42      0.4
^                          7.20       1.3      7.20      1.3
-                          6.50       1.1      6.50      1.1
vector                     4.88       0.9      4.88      0.9
+                          3.66       0.6      3.66      0.6
(                          3.42       0.6      3.42      0.6
>                          2.78       0.5      2.78      0.5
gamma                      2.68       0.5      2.68      0.5
&                          2.36       0.4      2.36      0.4
/                          2.24       0.4      2.24      0.4
==                         2.08       0.4      2.08      0.4
:                          1.58       0.3      1.58      0.3
$                          1.10       0.2      1.10      0.2
do.call                    0.30       0.1      0.00      0.0
mfunc                      0.30       0.1      0.00      0.0
dimnames<-                 0.28       0.0      0.28      0.0
paste                      0.28       0.0      0.02      0.0
f                          0.28       0.0      0.00      0.0
lapply                     0.26       0.0      0.00      0.0
as.data.frame.matrix       0.02       0.0      0.02      0.0
is.null                    0.02       0.0      0.02      0.0
as.data.frame              0.02       0.0      0.00      0.0

$sampling.time
[1] 565.42

> 

Deepankar

On Sun, 2007-12-02 at 20:54 -0500, jim holtman wrote:
> One thing that I would suggest that you do is to use Rprof on a subset
> of the data that runs for 10-15 minutes and see where some of the hot
> spots are.  Since you have not provided commented, minimal,
> self-contained, reproducible code, it is hard to determine where the
> inefficiencies are since we don't have any data to run against it.
> Some of the loop look like you are just assigning a value to a vector,
> e.g.,
> 
> 
> >    if (alive[j]==N1) {
> >
> >        for (i in 1:(N1-1)) {
> >                S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
> >        }
> >    }
> >
> >    else {
> >        for (i in 1:(N1-1)) {
> >                S[N1,i] <- 0
> >        }
> >
> >    }
> 
> that can be done without loops, but without data, it is hard to
> determine.  Run Rprof and see what summary.Rprof shows to indicate
> where to focus on.
> 
> On Dec 2, 2007 12:49 PM, DEEPANKAR BASU <basu.15 at osu.edu> wrote:
> > R Users:
> >
> > I am trying to estimate a model of fertility behaviour using birth history data with maximum likelihood. My code works but is extremely slow (because of several for loops and my programming inefficiencies); when I use the genetic algorithm to optimize the likelihood function, it takes several days to complete (on a machine with Intel Core 2 processor [2.66GHz] and 2.99 GB RAM). Computing the hessian and using it to calculate the standard errors takes a large chunk of this time.
> >
> > I am copying the code for my likelihood function below; it would be great if someone could suggest any method to speed up the code (by avoiding the for loops or by any other method).
> >
> > I am not providing details of my model or what exactly I am trying to do in each step of the computation below; i would be happy to provide these details if they are deemed necessary for re-working the code.
> >
> > Thanks.
> > Deepankar
> >
> >
> > --------- begin code -----------------------
> >
> > LLK1 <- function(paramets, data.frame, ...) {  # DEFINING THE LOGLIKELIHOOD FUNCTION
> >
> > # paramets IS A 1x27 VECTOR OF PARAMETERS OVER WHICH THE FUNCTION WILL BE MAXIMISED
> > # data.frame IS A DATA FRAME. THE DATA FRAME CONTAINS OBSERVATIONS ON SEVERAL VARIABLES
> > # (LIKE EDUCATION, AGE, ETC.) FOR EACH RESPONDENT. COLUMNS REFER TO VARIABLES AND ROWS REFER
> > # TO OBSERVATIONS.
> >
> > ########## PARAMETERS ###############################
> >
> > # alpha: interaction between son targeting and family size
> > # beta : son targeting
> > # gamma : family size
> > # delta : a 1x6 vector of probabilities of male birth at various parities (q1, q2, q3, q4, q5, q6)
> > # zeta : a 1x11 vector of conditional probabilities with zeta[1]=1 always
> >
> > alpha <- paramets[1]      # FIRST PARAMETER
> > beta <- paramets[2:9]     # SECOND TO SEVENTH PARAMETER
> > gamma <- paramets[10:16]
> > delta <- paramets[17]
> > zeta <- paramets[18:27]   # LAST 10 PARAMETERS
> >
> > ################# VARIABLES ###############################
> > # READING IN THE VARIABLES IN THE DATA FRAME
> > # AND RENAMING THEM FOR USE IN THE LIKELIHOOD FUNCTION
> >
> > everborn <- data.frame$v201
> > alive <- data.frame$alive
> > age <- data.frame$age
> > edu <- data.frame$edu
> > rural <- data.frame$rur
> > rich <- data.frame$rich
> > middle <- data.frame$middle
> > poor <- data.frame$poor
> > work <- data.frame$work
> > jointfam <- data.frame$jfam
> > contracep <- data.frame$contra
> > hindu <- data.frame$hindu
> > muslim <- data.frame$muslim
> > scaste <- data.frame$scaste
> > stribe <- data.frame$stribe
> > obc <- data.frame$obc
> > ucaste <- data.frame$ucaste
> > N <- data.frame$dfsize
> > indN <- data.frame$dfsize1  # INDICATOR FUNCTION THAT dfsize==alive
> > nb <- data.frame$nboy
> > ng <- data.frame$ngirl
> > ncord1 <- data.frame$ncord1  # FIRST CHILD: BOY=0; GIRL=1
> > ncord2 <- data.frame$ncord2  #SECOND CHILD: BOY=0; GIRL=1
> > ncord3 <- data.frame$ncord3
> > ncord4 <- data.frame$ncord4
> > ncord5 <- data.frame$ncord5
> > ncord6 <- data.frame$ncord6  # SIXTH CHILD: BOY=0; GIRL=1
> >
> >
> >
> > ######### POSITION OF i^th BOY ################################################
> > boy1 <- data.frame$boy1     # BIRTH POSITION OF FIRST BOY (ZERO IF THE FAMILY HAS NO BOYS)
> > boy2 <- data.frame$boy2     # BIRTH POSITION OF SECOND BOY (ZERO IF THE FAMILY HAS ONLY ONE BOY)
> > boy3 <- data.frame$boy3
> > boy4 <- data.frame$boy4
> > boy5 <- data.frame$boy5
> > boy6 <- data.frame$boy6     # BIRTH POSITION OF SIXTH BOY (ZERO IF THE FAMILY HAS ONLY FIVE BOYS)
> >
> >
> > ######################## CONDITIONAL PROBABILITIES ##########################
> > qq21 <- 1
> >
> > qq31 <- 1/(1+exp(zeta[1]))
> > qq32 <- exp(zeta[1])/(1+exp(zeta[1]))
> >
> > qq41 <- 1/(1+exp(zeta[2])+exp(zeta[3]))
> > qq42 <- exp(zeta[2])/(1+exp(zeta[2])+exp(zeta[3]))
> > qq43 <- exp(zeta[3])/(1+exp(zeta[2])+exp(zeta[3]))
> >
> > qq51 <- 1/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> > qq52 <- exp(zeta[4])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> > qq53 <- exp(zeta[5])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> > qq54 <- exp(zeta[6])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> >
> > qq61 <- 1/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> > qq62 <- exp(zeta[7])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> > qq63 <- exp(zeta[8])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> > qq64 <- exp(zeta[9])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> > qq65 <- exp(zeta[10])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> >
> > zeta1 <- c(qq21,qq31,qq32,qq41,qq42,qq43,qq51,qq52,qq53,qq54,qq61,qq62,qq63,qq64,qq65)
> >
> > #########################################################################
> >
> > n <- length(N)         # LENGTH OF SAMPLE; SIZE OF THE MAIN LOOP
> >
> > lglik <- numeric(n)    # INITIALIZING THE LOGLIKELIHOOD FUNCTION
> >                       # CREATES A 1xn VECTOR OF ZEROS
> >
> >  for (j in 1:n) {      # START OF MAIN LOOP
> >
> >    S <- matrix(0, 6, 6)  # CREATE A 6x6 MATRIX OF ZEROS
> >    y <- numeric(15)      # CREATE A 1x15 VECTOR OF ZEROS
> >    N1 <- N[j]       # DESIRED FAMILY SIZE
> >
> >
> >      q <- 1/(1+exp(delta))   # PROBABILITY OF MALE BIRTH
> >
> >
> >    if (alive[j]==N1) {
> >
> >        for (i in 1:(N1-1)) {
> >                S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
> >        }
> >    }
> >
> >    else {
> >        for (i in 1:(N1-1)) {
> >                S[N1,i] <- 0
> >        }
> >
> >    }
> >
> > ######### CREATE A 1x6 VECTOR WITH POSITION OF BOYS WITHIN FAMILY
> >      x <- c(boy1[j], boy2[j], boy3[j], boy4[j], boy5[j])
> >
> >      if (N1>1) {
> >                     for (i in 1:(N1-1)) {
> >                               if (alive[j]>x[i] & x[i]>0) {
> >                                   S[N1,i] <- 0
> >                               }
> >                               if (x[i] == alive[j] ) {
> >                                   S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
> >                               }
> >                     }
> >      }
> >
> >   y <- c(S[2,1],S[3,1],S[3,2],S[4,1],S[4,2],S[4,3],S[5,1],S[5,2],S[5,3],S[5,4],S[6,1],S[6,2],S[6,3],S[6,4],S[6,5])
> >
> >
> >   z1 <- c(age[j],edu[j],work[j],rural[j],poor[j],middle[j],hindu[j])         # DETERMINANTS OF FAMILY SIZE
> >   z2 <- c(1,age[j],edu[j],work[j],contracep[j],rural[j],jointfam[j],hindu[j])  # DETERMINANTS OF SON TARGETING
> >
> >   t1 <- (indN[j])*((q^(nb[j]))*((1-q)^(ng[j])))*(exp(-exp(sum(z1*gamma)))*((exp(sum(z1*gamma)))^N1)*pnorm(-sum(z2*beta)))/factorial(N1)
> >   t2 <- (sum(y*zeta1))*(exp(-exp(sum(z1*gamma) + alpha))*((exp(sum(z1*gamma) + alpha))^N1)*(1-pnorm(-sum(z2*beta)))/factorial(N1))
> >   lglik[j] <- log(t1+t2)
> >  }
> >
> >  return(-sum(lglik)) # RETURNING THE NEGATIVE OF THE LOGLIKELIHOOD
> >                     # SUMMED OVER ALL OBSERVATIONS
> >
> >
> > }
> >
> > ------------ end code ----------------------
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
>


From gunter.berton at gene.com  Mon Dec  3 18:46:59 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 3 Dec 2007 09:46:59 -0800
Subject: [R] using color coded colorbars for bar plots
In-Reply-To: <f8e6ff050712030830s6f990d17q547857c2638f743a@mail.gmail.com>
References: <A710A72A775FA24B8B6943719F3DE1B052090C@extas4-hba.tas.csiro.au><474FF1E1.1050207@bitwrit.com.au><A710A72A775FA24B8B6943719F3DE1B0520914@extas4-hba.tas.csiro.au><4752940C.3040008@bitwrit.com.au>
	<f8e6ff050712030830s6f990d17q547857c2638f743a@mail.gmail.com>
Message-ID: <007b01c835d4$8352f510$3a0b2c0a@gne.windows.gene.com>

... but the best option is not to do this kind of technicolor extravaganza
at all!

See ?dotplot  (in lattice) and ?dotchart for better alternatives. 


Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of hadley wickham
Sent: Monday, December 03, 2007 8:30 AM
To: Jim Lemon
Cc: R-help at r-project.org; James.Dell at csiro.au
Subject: Re: [R] using color coded colorbars for bar plots

On 12/2/07, Jim Lemon <jim at bitwrit.com.au> wrote:
> James.Dell at csiro.au wrote:
> > Hi Jim,
> > Thanks for getting back to me so quickly.
> >
> > I did look at color.legend, but that seems to plot colored blocks for
> > the observations (in this case the mean) and not for the color.scale
> > (which represents variance in this case).  Unless there is a
> > functionality that I haven't discovered yet.  If you have created a
> > similar plot and would be happy to share some code I'd be very
> > apprecitive.
> >
> Part of the problem is that you seem to have two names for the same
> variable in your code (Standard.Deviance and Standard.Deviation - unless
> that was a typo). Notice how I calculate the colors twice, the second
> time with a simple integer sequence to get the right number of evenly
> spaced colors. In your example, you calculated the colors for
> RankVar$Standard.Deviance again, but you don't need all those colors for
> the legend, and they're in the wrong order anyway. What is generally
> wanted for a color legend is the minimum and maximum values on the ends
> and a few linear interpolations in the middle.
>
> barplot(RankVar$MeanDecreaseAccuracy,
>   col=color.scale(RankVar$Standard.Deviance,
>   c(0,1,1),c(1,1,0),0),
>   ylab = "Variable Importance",
>   names.arg = rownames(RankVar),
>   cex.names = .7,
>   main = "Variables from RandomFishForest",
>   sub= "Mean Decrease in Accuracy")
> col.labels<- c("Low","Mid","High")
> color.legend(6,13,11,14,col.labels,
>   rect.col=color.scale(1:5,c(0,1,1),c(1,1,0),0))

Another option would be to use ggplot2:

install.packages("ggplot2")
library(ggplot2)

qplot(rownames(RankVar), MeanDecreaseAccuracy, data = RankVar, colour
= Standard.Deviance)

and then use http://had.co.nz/ggplot2/scale_colour_gradient.html to
control the choice of colours for the mapping.

Regards,

Hadley

-- 
http://had.co.nz/

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From daniel.valverde at uab.cat  Mon Dec  3 19:10:45 2007
From: daniel.valverde at uab.cat (Dani Valverde)
Date: Mon, 03 Dec 2007 19:10:45 +0100
Subject: [R] Installing packages
Message-ID: <475446A5.3010601@uab.cat>

Hello,
I have a problem when making packages with version 2.6.1. I have a 
package which I could install in version 2.5.1. I have made some 
modifications of the package, and I would like to install it to 2.6.1. I 
check the package with R CMD check, I build it with R CMD build and when 
I try to install it it gives me an error, it tries to install something 
like R.css and tells me that cannot open the DESCRIPTION file. Then, I 
made the same procedure with version 2.5.1 and it worked fine. I am 
using Windows Vista and I changed the PATH environment variable to 
version 2.6.1. Any suggestion to solve the problem?
Best regards,

Dani

-- 
Daniel Valverde Saub?

Grup de Biologia Molecular de Llevats
Facultat de Veterin?ria de la Universitat Aut?noma de Barcelona
Edifici V, Campus UAB
08193 Cerdanyola del Vall?s- SPAIN

Centro de Investigaci?n Biom?dica en Red
en Bioingenier?a, Biomateriales y
Nanomedicina (CIBER-BBN)

Grup d'Aplicacions Biom?diques de la RMN
Facultat de Bioci?ncies
Universitat Aut?noma de Barcelona
Edifici Cs, Campus UAB
08193 Cerdanyola del Vall?s- SPAIN
+34 93 5814126


From mike.prager at noaa.gov  Mon Dec  3 19:07:54 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Mon, 03 Dec 2007 13:07:54 -0500
Subject: [R] Rating R Helpers
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
	<B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
	<475336DE.91DF.00CB.0@grecc.umaryland.edu>
Message-ID: <u2h8l39euq9qgn4b1vqenk4lni2it5uro9@4ax.com>

"John Sorkin" <jsorkin at grecc.umaryland.edu> wrote:

> I believe we need to know the following about packages:
> (1) Does the package do what it purports to do, i.e. are the results valid?
> (2) Have the results generated by the package been validate against some other statistical package, or hand-worked example?
> (3) Are the methods used in the soundly based?
> (4) Does the package documentation refer to referred papers or textbooks?
> (5) In addition to the principle result, does the package return ancillary values that allow for proper interpretation of the main result, (e.g. lm gives estimates of the betas and their SEs, but also generates residuals)?.
> (6) Is the package easy to use, i.e. do the parameters used when invoking the package chosen so as to allow the package to be flexible?
> (7) Are the error messages produced by the package helpful?
> (8) Does the package conform to standards of R coding and good programming principles in general?
> (9) Does the package interact will with the larger R environment, e.g. does it have a plot method etc.?
> (10) Is the package well documented internally, i.e. is the code easy to follow, are the comments in the code adequate?
> (11) Is the package well documented externally, i.e. through man pages and perhaps other documentation (e.g. MASS and its associated textbook)?
> 
> In addition to package evaluation and reviews, we also need some plan for the future of R. Who will maintain, modify, and extend packages after the principle author, or authors, retire? Software is never "done". Errors need to be corrected, programs need to be modified to accommodate changes in software and hardware. I have reasonable certainty that commercial software (e.g. SAS) will be available in 10-years (and that PROC MIXED will still be a part of SAS). I am far less sanguine about any number of R packages.
> John 

Interesting questions.

Re, the future : LaTeX provides an example. The more complex
packages tend to stop developing when the original programmer
loses interest. Sometimes another person picks one up, but not
frequently. I think, for example, of the many slide-preparation
packages, each more complex than the next, that have come and
gone during my relatively short (15 yr) professional use of
LaTeX.

At its root, this is a rather deep question: how open-source,
largely volunteer-developed software can survive over the long
term, while continuing to improve and maintain high standards.
We are rather early in the history of free software development
to know the answer.

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From hkiws at gmx.de  Mon Dec  3 19:15:17 2007
From: hkiws at gmx.de (EUROPOL)
Date: Mon, 03 Dec 2007 19:15:17 +0100
Subject: [R] Problem with scan() from UTF-8 encoded URL
In-Reply-To: <AAD49F46EAE3F6479E1D46428FAC31CB02DD2DAA@NBIE2KSRV1.nbi.bbsrc.ac.uk>
References: <bba82c9f0712030731o235bdb5ch8abdca6891ac3805@mail.gmail.com>
	<475433FB.7090308@gmx.de>
	<AAD49F46EAE3F6479E1D46428FAC31CB02DD2DAA@NBIE2KSRV1.nbi.bbsrc.ac.uk>
Message-ID: <475447B5.2020303@gmx.de>

    ,

Thank you for trying. Strange.

I am using R version 2.6.0 Patched (2007-11-09 r43408) on OSX and it is
not working. I guess it has something to do with the language settings.

However.

Regards

Marc Schwenzer

john seers (IFR) wrote:
>  
>
> Hello
>
> Works fine for me:
>
>   
>> data
>>     
> <-scan(file='http://en.wikipedia.org/wiki/Special:Recentchanges',what='c
> haracter')
> Read 3581 items
>   
>
> So I don't think it is the Wikipedia end.
>
> Regards
>
> John Seers
>
>
>  
> ---
>
> -----Original Message-----
> From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
> On Behalf Of EUROPOL
> Sent: 03 December 2007 16:51
> To: r-help at stat.math.ethz.ch
> Subject: [R] Problem with scan() from UTF-8 encoded URL
>
> Hallo,
>
> I am trying to import a website and structure it from within R:
>
> The following code:
>
> data <-
> scan(file='http://en.wikipedia.org/wiki/Special:Recentchanges',what='cha
> racter')
>
> results in the error:
>
> Error in file(file, "r") : unable to open connection In addition:
> Warning message:
> cannot open: HTTP status was '403 Forbidden' in: file(file, "r")
>
> It seems that the error is connected to the UTF-8-format of wikipedia,
> since the following line works:
>
> data <- scan(file='http://www.google.de',what='character')
>
> I am looking forward to your answers.
>
> Greetings
>
> Marc Schwenzer
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From h.wickham at gmail.com  Mon Dec  3 19:15:21 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 3 Dec 2007 12:15:21 -0600
Subject: [R] using color coded colorbars for bar plots
In-Reply-To: <007b01c835d4$8352f510$3a0b2c0a@gne.windows.gene.com>
References: <A710A72A775FA24B8B6943719F3DE1B052090C@extas4-hba.tas.csiro.au>
	<474FF1E1.1050207@bitwrit.com.au>
	<A710A72A775FA24B8B6943719F3DE1B0520914@extas4-hba.tas.csiro.au>
	<4752940C.3040008@bitwrit.com.au>
	<f8e6ff050712030830s6f990d17q547857c2638f743a@mail.gmail.com>
	<007b01c835d4$8352f510$3a0b2c0a@gne.windows.gene.com>
Message-ID: <f8e6ff050712031015r5d53cf88ka639dd0b8d465037@mail.gmail.com>

On 12/3/07, Bert Gunter <gunter.berton at gene.com> wrote:
> ... but the best option is not to do this kind of technicolor extravaganza
> at all!

Yes, good point!  And of a course of a scatterplot of mean vs variance
will best reveal the relationship between the two variables.

Hadley

-- 
http://had.co.nz/


From jholtman at gmail.com  Mon Dec  3 19:20:23 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 3 Dec 2007 10:20:23 -0800
Subject: [R] overlapping labels
In-Reply-To: <7124EE4D7E556844A654A607A4203FDD3ED3CD@scomp0038.wurnet.nl>
References: <7124EE4D7E556844A654A607A4203FDD3ED3CD@scomp0038.wurnet.nl>
Message-ID: <644e1f320712031020g25f19715j86c0f1674ebc1104@mail.gmail.com>

You might look at 'jitter' to add a little offset to the labels being plotted.

On Dec 3, 2007 9:21 AM, Biscarini, Filippo <Filippo.Biscarini at wur.nl> wrote:
> Good evening,
>
> I am trying to add labels to the point of a simple plot, using the
> text() function; the problem is that sometimes, if two points are too
> close to each other, labels overlap and are no longer readable.
> I was wondering whether there are options that I can use to prevent this
> overlapping (by, for example, placing labels alternatively above and
> below the plotted curve), or whether I should use another set of
> graphical functions (grid or lattice packages).
> Does anybody have suggestions?
>
> Thank you,
>
> Filippo Biscarini
>
> PS: These are the lines of codes that I have been using to generate the
> plot.
>
> plot(
>  prova$x,
>  prova$y,
>  type="b",
>  mar=c(0.5,0.5,0.5,0.5),
>  xlab="basepairs",
>  ylab=""
> );
>
> text(
>  prova$x,
>  prova$y,
>  prova$lab,
>  pos=3,
>  offset=2,
>  cex=0.5
> );
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From murdoch at stats.uwo.ca  Mon Dec  3 19:31:26 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 03 Dec 2007 13:31:26 -0500
Subject: [R] overlapping labels
In-Reply-To: <7124EE4D7E556844A654A607A4203FDD3ED3CD@scomp0038.wurnet.nl>
References: <7124EE4D7E556844A654A607A4203FDD3ED3CD@scomp0038.wurnet.nl>
Message-ID: <47544B7E.9050508@stats.uwo.ca>

On 12/3/2007 12:21 PM, Biscarini, Filippo wrote:
> Good evening,
>  
> I am trying to add labels to the point of a simple plot, using the
> text() function; the problem is that sometimes, if two points are too
> close to each other, labels overlap and are no longer readable.
> I was wondering whether there are options that I can use to prevent this
> overlapping (by, for example, placing labels alternatively above and
> below the plotted curve), or whether I should use another set of
> graphical functions (grid or lattice packages).
> Does anybody have suggestions?

I don't think there are any functions to automatically avoid overlaps, 
but it is not hard to manually move some of the labels.  For example, 
you could alternate labels above and below with code like this:

 > x <- 1:10
 > y <- rep(0, 10)
 > plot(x,y)
 > text(x, y, rep("label", 10), pos=rep(c(1,3), 5))

because the pos parameter can be specified separately for each observation.

Duncan Murdoch

>  
> Thank you,
>  
> Filippo Biscarini
>  
> PS: These are the lines of codes that I have been using to generate the
> plot.
> 
> plot(
>  prova$x,
>  prova$y,
>  type="b",
>  mar=c(0.5,0.5,0.5,0.5),
>  xlab="basepairs",
>  ylab=""
> );
>  
> text(
>  prova$x,
>  prova$y,
>  prova$lab,
>  pos=3,
>  offset=2,
>  cex=0.5
> );


From basu.15 at osu.edu  Mon Dec  3 19:29:57 2007
From: basu.15 at osu.edu (Deepankar Basu)
Date: Mon, 03 Dec 2007 13:29:57 -0500
Subject: [R] speeding up likelihood computation
In-Reply-To: <644e1f320712021754o5cc72303me0d7d3d4c681bb7@mail.gmail.com>
References: <110f9d445.d445110f9@osu.edu>
	<644e1f320712021754o5cc72303me0d7d3d4c681bb7@mail.gmail.com>
Message-ID: <1196706597.10417.9.camel@localhost>

Jim and others,

As far as I can see the computation of certain conditional probabilities
(required for computing my likelihood function) is what is slowing down
the evaluation of the likelihood function. Let me explain what these
conditional probabilities are and how I am (no doubt inefficiently)
calculating them. I apologize for the long post but I could not explain
the whole thing without some detailed examples, etc. 

For every family, we are given a completed birth sequence (call it S_i)
and the desired maximum number of children (call it N_i); for instance
S_i might be GBB (where G stands for a girl and B stands for a boy) and
N_i might be 4. For each family, we want to compute the the probability
of observing the birth sequence, S_i, given that the family is
"targeting" sons. Since, a priori, we do not know the desired target
(for sons) for family i, we need to allow for all the feasible
possibilities. So, when a family states that the maximum number of
children that it desires is N_i, we need to allow for the possibilities
that the family targets 1 son, 2 sons, ... , (N_i-1) sons. Of course the
actual birth sequence might assign zero probability to some of these
possibilities; but we cannot rule out any of these a priori. 

Let us denote a target for sons as k_i. So, for family i with birth
sequence S_i and desired maximum number of children N_i, we need to
compute the following (N_i-1) conditional probabilities: P(S_i|N_i,
k_i=1, T_i=1), P(S_i|N_i, k_i=2, T_i=1), ... , P(S_i|N_i, k_i=N_i-1,
T_i=1). 

Let q be the probability of male birth; it is a parameter in my model
and will be estimated. Now, to compute something like P(S_i|N_i, k_i,
T_i=1), we merely need to observe whether the family has any child after
the k_i^{th} son. If there is a child after the k_i^{th} son, then we
assign zero probability to P(S_i|N_i, k_i, T_i=1); else we assign it a
probability of (q^(nb[j]))*((1-q)^(ng[j])), where q is the probability
of a male birth, nb is the number of boys in the sequence S_i and ng is
the number of girls in the sequence.

An example might clarify matters. Suppose a family reports that the
maximum number of children it desires to have is 4 and the birth
sequence (starting with the first born child) for the family is observed
to be GGBG (where G stands for a girl and B stands for a boy). For such
a family, we need to compute the following probabilities: $(GGBG|N_i=4,
k_i=1, T_i=1), P(GGBG|N_i=4, k_i=2, T_i=1), and P(GGBG|N_i=4, k_i=3,
T_i=1). Since there is a child after the first boy, this family could
not possibly be targeting one son; hence P(GGBG|N_i=4, k_i=1, T_i=1)=0.
But the family could conceivably be targeting two or even three sons;
these possibilities are not ruled out by the observed birth sequence.
Hence P(GGBG|N_i=4, k_i=2, T_i=1)=(q)*((1-q)^3), and similarly P(GGBG|
N_i=4, k_i=3, T_i=1)=(q)*((1-q)^3). 

To clarify matters further, take another example. Suppose the family in
question reports a maximum desired family size (number of children) of 4
and we observe the following completed birth sequence for the same
family: BGB. Since there is a child after the first boy, this family
could not possibly be targeting one son; hence P(BGB|N_i=4, k_i=1,
T_i=1)=0. Since there is no child after the second boy, the family could
possibly be targeting 2 sons; hence P(BGB|N_i=4, k_i=2,
T_i=1)=(q^2)*((1-q)). And since the family stops at three children (with
two sons), it cannot be targeting three sons; to target three sons, the
family should have gone for another child and not stopped at the third
child (and second son). Hence, P(BGB|N_i=4, k_i=3, T_i=1)=0.

In my sample I let N_i run from 2 to 6. So, depending on whether N_i is
2 , 3, 4, 5 or 6, I need to compute these conditional probabilities. For
instance, if N_i is 2, I need to compute only one conditional
probability; if N_i is 6, I need to compute five of these conditional
probabilities. This is how I proceed.

I start by creating a 6x6 matrix of zeros and a 1x15 vector of zeros.

   S <- matrix(0, 6, 6)  # CREATE A 6x6 MATRIX OF ZEROS
   y <- numeric(15)      # CREATE A 1x15 VECTOR OF ZEROS 

Then the following part of the code computes the conditional
probabilities as rows of the matrix S. The code picks up the jth row if
the family has N_i=j. Once this is done, I store the lower triangle of
the matrix (i.e., entries below the principal diagonal) in the y vector.

--------------- begin code fragment -------------------------

    q <- 1/(1+exp(delta))   # PROBABILITY OF MALE BIRTH

 ###### N1 IS "DESIRED FAMILY SIZE" ######################
 ##### alive IS THE NUMBER OF CHILDREN ALIVE IN THE FAMILY
 ##### nb IS NUMBER OF BOYS IN THE FAMILY
 ##### ng IS THE NUMBER OF GIRLS IN THE FAMILY

    if (alive[j]==N1) {

        for (i in 1:(N1-1)) {
                S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
        }
    }

    else {
        for (i in 1:(N1-1)) {
                S[N1,i] <- 0
        }

    }

 ######### CREATE A 1x6 VECTOR WITH POSITION OF BOYS WITHIN FAMILY
 # boy1 GIVES BIRTH POSITION OF FIRST BOY (ZERO IF THE FAMILY HAS NO
BOYS)
 # boy2 GIVES BIRTH POSITION OF FIRST BOY (ZERO IF THE FAMILY HAS ONLY 1
BOY)
 # boy3 GIVES BIRTH POSITION OF FIRST BOY (ZERO IF THE FAMILY HAS ONLY 2
BOYS)
 # boy4 GIVES BIRTH POSITION OF FIRST BOY (ZERO IF THE FAMILY HAS ONLY 3
BOYS)
 # boy5 GIVES BIRTH POSITION OF FIRST BOY (ZERO IF THE FAMILY HAS ONLY 4
BOYS)
 # boy6 GIVES BIRTH POSITION OF FIRST BOY (ZERO IF THE FAMILY HAS ONLY 5
BOYS)
  
      x <- c(boy1[j], boy2[j], boy3[j], boy4[j], boy5[j])

      if (N1>1) {
                     for (i in 1:(N1-1)) {
                               if (alive[j]>x[i] & x[i]>0) {
                                   S[N1,i] <- 0
                               }
                               if (x[i] == alive[j] ) {
                                   S[N1,i] <-
(q^(nb[j]))*((1-q)^(ng[j]))
                               }
                     }
      }

   y <-
c(S[2,1],S[3,1],S[3,2],S[4,1],S[4,2],S[4,3],S[5,1],S[5,2],S[5,3],S[5,4],S[6,1],S[6,2],S[6,3],S[6,4],S[6,5])

------------------ end code fragment ------------------------------

Later, I use the entries in the y vector for computing the likelihood.
Any suggestions on how to rework this part of the code will, I think,
improve overall efficiency.

Thanks in advance for your time.

Deepankar

On Sun, 2007-12-02 at 20:54 -0500, jim holtman wrote:
> One thing that I would suggest that you do is to use Rprof on a subset
> of the data that runs for 10-15 minutes and see where some of the hot
> spots are.  Since you have not provided commented, minimal,
> self-contained, reproducible code, it is hard to determine where the
> inefficiencies are since we don't have any data to run against it.
> Some of the loop look like you are just assigning a value to a vector,
> e.g.,
> 
> 
> >    if (alive[j]==N1) {
> >
> >        for (i in 1:(N1-1)) {
> >                S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
> >        }
> >    }
> >
> >    else {
> >        for (i in 1:(N1-1)) {
> >                S[N1,i] <- 0
> >        }
> >
> >    }
> 
> that can be done without loops, but without data, it is hard to
> determine.  Run Rprof and see what summary.Rprof shows to indicate
> where to focus on.
> 
> On Dec 2, 2007 12:49 PM, DEEPANKAR BASU <basu.15 at osu.edu> wrote:
> > R Users:
> >
> > I am trying to estimate a model of fertility behaviour using birth history data with maximum likelihood. My code works but is extremely slow (because of several for loops and my programming inefficiencies); when I use the genetic algorithm to optimize the likelihood function, it takes several days to complete (on a machine with Intel Core 2 processor [2.66GHz] and 2.99 GB RAM). Computing the hessian and using it to calculate the standard errors takes a large chunk of this time.
> >
> > I am copying the code for my likelihood function below; it would be great if someone could suggest any method to speed up the code (by avoiding the for loops or by any other method).
> >
> > I am not providing details of my model or what exactly I am trying to do in each step of the computation below; i would be happy to provide these details if they are deemed necessary for re-working the code.
> >
> > Thanks.
> > Deepankar
> >
> >
> > --------- begin code -----------------------
> >
> > LLK1 <- function(paramets, data.frame, ...) {  # DEFINING THE LOGLIKELIHOOD FUNCTION
> >
> > # paramets IS A 1x27 VECTOR OF PARAMETERS OVER WHICH THE FUNCTION WILL BE MAXIMISED
> > # data.frame IS A DATA FRAME. THE DATA FRAME CONTAINS OBSERVATIONS ON SEVERAL VARIABLES
> > # (LIKE EDUCATION, AGE, ETC.) FOR EACH RESPONDENT. COLUMNS REFER TO VARIABLES AND ROWS REFER
> > # TO OBSERVATIONS.
> >
> > ########## PARAMETERS ###############################
> >
> > # alpha: interaction between son targeting and family size
> > # beta : son targeting
> > # gamma : family size
> > # delta : a 1x6 vector of probabilities of male birth at various parities (q1, q2, q3, q4, q5, q6)
> > # zeta : a 1x11 vector of conditional probabilities with zeta[1]=1 always
> >
> > alpha <- paramets[1]      # FIRST PARAMETER
> > beta <- paramets[2:9]     # SECOND TO SEVENTH PARAMETER
> > gamma <- paramets[10:16]
> > delta <- paramets[17]
> > zeta <- paramets[18:27]   # LAST 10 PARAMETERS
> >
> > ################# VARIABLES ###############################
> > # READING IN THE VARIABLES IN THE DATA FRAME
> > # AND RENAMING THEM FOR USE IN THE LIKELIHOOD FUNCTION
> >
> > everborn <- data.frame$v201
> > alive <- data.frame$alive
> > age <- data.frame$age
> > edu <- data.frame$edu
> > rural <- data.frame$rur
> > rich <- data.frame$rich
> > middle <- data.frame$middle
> > poor <- data.frame$poor
> > work <- data.frame$work
> > jointfam <- data.frame$jfam
> > contracep <- data.frame$contra
> > hindu <- data.frame$hindu
> > muslim <- data.frame$muslim
> > scaste <- data.frame$scaste
> > stribe <- data.frame$stribe
> > obc <- data.frame$obc
> > ucaste <- data.frame$ucaste
> > N <- data.frame$dfsize
> > indN <- data.frame$dfsize1  # INDICATOR FUNCTION THAT dfsize==alive
> > nb <- data.frame$nboy
> > ng <- data.frame$ngirl
> > ncord1 <- data.frame$ncord1  # FIRST CHILD: BOY=0; GIRL=1
> > ncord2 <- data.frame$ncord2  #SECOND CHILD: BOY=0; GIRL=1
> > ncord3 <- data.frame$ncord3
> > ncord4 <- data.frame$ncord4
> > ncord5 <- data.frame$ncord5
> > ncord6 <- data.frame$ncord6  # SIXTH CHILD: BOY=0; GIRL=1
> >
> >
> >
> > ######### POSITION OF i^th BOY ################################################
> > boy1 <- data.frame$boy1     # BIRTH POSITION OF FIRST BOY (ZERO IF THE FAMILY HAS NO BOYS)
> > boy2 <- data.frame$boy2     # BIRTH POSITION OF SECOND BOY (ZERO IF THE FAMILY HAS ONLY ONE BOY)
> > boy3 <- data.frame$boy3
> > boy4 <- data.frame$boy4
> > boy5 <- data.frame$boy5
> > boy6 <- data.frame$boy6     # BIRTH POSITION OF SIXTH BOY (ZERO IF THE FAMILY HAS ONLY FIVE BOYS)
> >
> >
> > ######################## CONDITIONAL PROBABILITIES ##########################
> > qq21 <- 1
> >
> > qq31 <- 1/(1+exp(zeta[1]))
> > qq32 <- exp(zeta[1])/(1+exp(zeta[1]))
> >
> > qq41 <- 1/(1+exp(zeta[2])+exp(zeta[3]))
> > qq42 <- exp(zeta[2])/(1+exp(zeta[2])+exp(zeta[3]))
> > qq43 <- exp(zeta[3])/(1+exp(zeta[2])+exp(zeta[3]))
> >
> > qq51 <- 1/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> > qq52 <- exp(zeta[4])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> > qq53 <- exp(zeta[5])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> > qq54 <- exp(zeta[6])/(1+exp(zeta[4])+exp(zeta[5])+exp(zeta[6]))
> >
> > qq61 <- 1/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> > qq62 <- exp(zeta[7])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> > qq63 <- exp(zeta[8])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> > qq64 <- exp(zeta[9])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> > qq65 <- exp(zeta[10])/(1+exp(zeta[7])+exp(zeta[8])+exp(zeta[9])+exp(zeta[10]))
> >
> > zeta1 <- c(qq21,qq31,qq32,qq41,qq42,qq43,qq51,qq52,qq53,qq54,qq61,qq62,qq63,qq64,qq65)
> >
> > #########################################################################
> >
> > n <- length(N)         # LENGTH OF SAMPLE; SIZE OF THE MAIN LOOP
> >
> > lglik <- numeric(n)    # INITIALIZING THE LOGLIKELIHOOD FUNCTION
> >                       # CREATES A 1xn VECTOR OF ZEROS
> >
> >  for (j in 1:n) {      # START OF MAIN LOOP
> >
> >    S <- matrix(0, 6, 6)  # CREATE A 6x6 MATRIX OF ZEROS
> >    y <- numeric(15)      # CREATE A 1x15 VECTOR OF ZEROS
> >    N1 <- N[j]       # DESIRED FAMILY SIZE
> >
> >
> >      q <- 1/(1+exp(delta))   # PROBABILITY OF MALE BIRTH
> >
> >
> >    if (alive[j]==N1) {
> >
> >        for (i in 1:(N1-1)) {
> >                S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
> >        }
> >    }
> >
> >    else {
> >        for (i in 1:(N1-1)) {
> >                S[N1,i] <- 0
> >        }
> >
> >    }
> >
> > ######### CREATE A 1x6 VECTOR WITH POSITION OF BOYS WITHIN FAMILY
> >      x <- c(boy1[j], boy2[j], boy3[j], boy4[j], boy5[j])
> >
> >      if (N1>1) {
> >                     for (i in 1:(N1-1)) {
> >                               if (alive[j]>x[i] & x[i]>0) {
> >                                   S[N1,i] <- 0
> >                               }
> >                               if (x[i] == alive[j] ) {
> >                                   S[N1,i] <- (q^(nb[j]))*((1-q)^(ng[j]))
> >                               }
> >                     }
> >      }
> >
> >   y <- c(S[2,1],S[3,1],S[3,2],S[4,1],S[4,2],S[4,3],S[5,1],S[5,2],S[5,3],S[5,4],S[6,1],S[6,2],S[6,3],S[6,4],S[6,5])
> >
> >
> >   z1 <- c(age[j],edu[j],work[j],rural[j],poor[j],middle[j],hindu[j])         # DETERMINANTS OF FAMILY SIZE
> >   z2 <- c(1,age[j],edu[j],work[j],contracep[j],rural[j],jointfam[j],hindu[j])  # DETERMINANTS OF SON TARGETING
> >
> >   t1 <- (indN[j])*((q^(nb[j]))*((1-q)^(ng[j])))*(exp(-exp(sum(z1*gamma)))*((exp(sum(z1*gamma)))^N1)*pnorm(-sum(z2*beta)))/factorial(N1)
> >   t2 <- (sum(y*zeta1))*(exp(-exp(sum(z1*gamma) + alpha))*((exp(sum(z1*gamma) + alpha))^N1)*(1-pnorm(-sum(z2*beta)))/factorial(N1))
> >   lglik[j] <- log(t1+t2)
> >  }
> >
> >  return(-sum(lglik)) # RETURNING THE NEGATIVE OF THE LOGLIKELIHOOD
> >                     # SUMMED OVER ALL OBSERVATIONS
> >
> >
> > }
> >
> > ------------ end code ----------------------
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
>


From fsaldan1 at gmail.com  Mon Dec  3 19:31:03 2007
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Mon, 3 Dec 2007 13:31:03 -0500
Subject: [R] Fwd: source('clipboard')
In-Reply-To: <10dee4690712021610x35625a15s85bde70cd4c4073f@mail.gmail.com>
References: <10dee4690712021610x35625a15s85bde70cd4c4073f@mail.gmail.com>
Message-ID: <10dee4690712031031t6574c8edg1afa34d0a9e35839@mail.gmail.com>

In the code below the first source command works fine, but the second
does not, as can be seen from the error message. Is there a way to
have the second command work?

I am using R 2.6.1 on Windows Vista.

(The command that is in the clipboard is just "x <- 3")

> source("clipboard", echo = F)
> source("clipboard", echo = T)
Error in file(srcfile$filename, open = "rt", encoding = srcfile$encoding) :
  'mode' for the clipboard must be 'r' or 'w'
>

Thanks for the help.

FS


From david.rees at citi.com  Mon Dec  3 19:41:06 2007
From: david.rees at citi.com (Rees, David )
Date: Mon, 3 Dec 2007 18:41:06 -0000
Subject: [R]  Putting a NULL in a list (as cannot pass NA to C++)
Message-ID: <C07600F3B08324418B460FD82E3ADC340C605DAA@exukmb12.eur.nsroot.net>

Hi,

If I do the following I can have a NULL in a list 

> x <- list( 1, list(3,NULL,4), 5 )
> x
[[1]]
[1] 1

[[2]]
[[2]][[1]]
[1] 3

[[2]][[2]]
NULL

[[2]][[3]]
[1] 4


[[3]]
[1] 5

This is a good thing for me as it can be passed through into C++ where I
can know the value is missing. (Can't seem to detect a NA from C++ for
some reason. There is RF_isNull in the API but no RF_isNA that I can
find!)

Is it possible to put a null in an already created list. e.g. now do

> x[[2]][[2]] <- NULL
> x
[[1]]
[1] 1

[[2]]
[[2]][[1]]
[1] 3

[[2]][[2]]
[1] 4


[[3]]
[1] 5


But x[[2]][[2]] has been deleted.

Many thanks

Regards,
David


From pisicandru at hotmail.com  Mon Dec  3 19:41:43 2007
From: pisicandru at hotmail.com (Monica Pisica)
Date: Mon, 3 Dec 2007 18:41:43 +0000
Subject: [R] coplot and xyplot and panel functions
Message-ID: <BAY104-W17A9FFB9C36F14CB85F7F8C36C0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/eacc7dd8/attachment.pl 

From murdoch at stats.uwo.ca  Mon Dec  3 19:51:00 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 03 Dec 2007 13:51:00 -0500
Subject: [R] Fwd: source('clipboard')
In-Reply-To: <10dee4690712031031t6574c8edg1afa34d0a9e35839@mail.gmail.com>
References: <10dee4690712021610x35625a15s85bde70cd4c4073f@mail.gmail.com>
	<10dee4690712031031t6574c8edg1afa34d0a9e35839@mail.gmail.com>
Message-ID: <47545014.8070108@stats.uwo.ca>

On 12/3/2007 1:31 PM, Fernando Saldanha wrote:
> In the code below the first source command works fine, but the second
> does not, as can be seen from the error message. Is there a way to
> have the second command work?
> 
> I am using R 2.6.1 on Windows Vista.
> 
> (The command that is in the clipboard is just "x <- 3")
> 
>> source("clipboard", echo = F)
>> source("clipboard", echo = T)
> Error in file(srcfile$filename, open = "rt", encoding = srcfile$encoding) :
>   'mode' for the clipboard must be 'r' or 'w'
>>
> 
> Thanks for the help.

That looks like a little bug.  I'll fix it in a day or two, but in the 
meantime you can work around it with code like this (which you'd 
probably want to put in a function, not type all the time yourself):

f <- file("clipboard", open="r")
source(f, echo=T)
close(f)

Duncan Murdoch


From deshon at msu.edu  Mon Dec  3 19:57:54 2007
From: deshon at msu.edu (Rick DeShon)
Date: Mon, 3 Dec 2007 13:57:54 -0500
Subject: [R] Efficient computation of average covariance matrix over a list
Message-ID: <c3cb73d50712031057g8daad55s6daca6647fa1ddcd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/3140dab3/attachment.pl 

From andy_liaw at merck.com  Mon Dec  3 19:58:21 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 3 Dec 2007 13:58:21 -0500
Subject: [R] cor(data.frame) infelicities
In-Reply-To: <971536df0712030631g647edbb2g155d3ffd5cfe0241@mail.gmail.com>
References: <4754123B.5030902@yorku.ca>
	<971536df0712030631g647edbb2g155d3ffd5cfe0241@mail.gmail.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA04FEA333@usctmx1106.merck.com>

I'd call that another infelicity.  Species is supposed to be nominal,
not ordinal, so rank correlation wouldn't make much sense.  So what does
cor(, method="kendall") do?  It looks like it simply uses the underlying
numeric code.  (Change Species to numerics and you'll see the same
answer.)  However, reordering the levels changes the result:

R> iris2 <- iris
R> levels(iris2$Species) <- levels(iris2$Species)[c(2, 1, 3)]
R> cor(iris2, method = "kendall")
             Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
Sepal.Length   1.00000000 -0.07699679    0.7185159   0.6553086 0.1897778
Sepal.Width   -0.07699679  1.00000000   -0.1859944  -0.1571257 0.1439793
Petal.Length   0.71851593 -0.18599442    1.0000000   0.8068907 0.2677154
Petal.Width    0.65530856 -0.15712566    0.8068907   1.0000000 0.2724843
Species        0.18977778  0.14397927    0.2677154   0.2724843 1.0000000

To me, this is dangerous!

Andy
 

From: Gabor Grothendieck
> 
> You can calculate the Kendall rank correlation with such a matrix
> so you would not want to exclude factors in that case:
> 
> > cor(iris, method = "kendall")
>              Sepal.Length Sepal.Width Petal.Length 
> Petal.Width    Species
> Sepal.Length   1.00000000 -0.07699679    0.7185159   
> 0.6553086  0.6704444
> Sepal.Width   -0.07699679  1.00000000   -0.1859944  
> -0.1571257 -0.3376144
> Petal.Length   0.71851593 -0.18599442    1.0000000   
> 0.8068907  0.8229112
> Petal.Width    0.65530856 -0.15712566    0.8068907   
> 1.0000000  0.8396874
> Species        0.67044444 -0.33761438    0.8229112   
> 0.8396874  1.0000000
> 
> 
> On Dec 3, 2007 9:27 AM, Michael Friendly <friendly at yorku.ca> wrote:
> > In using cor(data.frame), it is annoying that you have to explicitly
> > filter out non-numeric columns, and when you don't, the 
> error message
> > is misleading:
> >
> >  > cor(iris)
> > Error in cor(iris) : missing observations in cov/cor
> > In addition: Warning message:
> > In cor(iris) : NAs introduced by coercion
> >
> > It would be nicer if stats:::cor() did the equivalent 
> *itself* of the
> > following for a data.frame:
> >  > cor(iris[,sapply(iris, is.numeric)])
> >              Sepal.Length Sepal.Width Petal.Length Petal.Width
> > Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
> > Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
> > Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
> > Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
> >  >
> >
> > A change could be implemented here:
> >     if (is.data.frame(x))
> >         x <- as.matrix(x)
> >
> > Second, the default, use="all" throws an error if there are any
> > NAs.  It would be nicer if the default was use="complete.cases",
> > which would generate warnings instead.  Most other statistical
> > software is more tolerant of missing data.
> >
> >  > library(corrgram)
> >  > data(auto)
> >  > cor(auto[,sapply(auto, is.numeric)])
> > Error in cor(auto[, sapply(auto, is.numeric)]) :
> >   missing observations in cov/cor
> >  > cor(auto[,sapply(auto, is.numeric)],use="complete")
> > # works; output elided
> >
> > -Michael
> >
> > --
> > Michael Friendly     Email: friendly AT yorku DOT ca
> > Professor, Psychology Dept.
> > York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> > 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> > Toronto, ONT  M3J 1P3 CANADA
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachme...{{dropped:15}}


From ggrothendieck at gmail.com  Mon Dec  3 20:05:36 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Dec 2007 14:05:36 -0500
Subject: [R] cor(data.frame) infelicities
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA04FEA333@usctmx1106.merck.com>
References: <4754123B.5030902@yorku.ca>
	<971536df0712030631g647edbb2g155d3ffd5cfe0241@mail.gmail.com>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA04FEA333@usctmx1106.merck.com>
Message-ID: <971536df0712031105l7c5df515of62b439affd3b3ba@mail.gmail.com>

You are right but I was just trying to stick to the same example.
In reality it would be ok as long as its an ordered factor.  One could
restrict it to those of class "ordered".


On Dec 3, 2007 1:58 PM, Liaw, Andy <andy_liaw at merck.com> wrote:
> I'd call that another infelicity.  Species is supposed to be nominal,
> not ordinal, so rank correlation wouldn't make much sense.  So what does
> cor(, method="kendall") do?  It looks like it simply uses the underlying
> numeric code.  (Change Species to numerics and you'll see the same
> answer.)  However, reordering the levels changes the result:
>
> R> iris2 <- iris
> R> levels(iris2$Species) <- levels(iris2$Species)[c(2, 1, 3)]
> R> cor(iris2, method = "kendall")
>             Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
> Sepal.Length   1.00000000 -0.07699679    0.7185159   0.6553086 0.1897778
> Sepal.Width   -0.07699679  1.00000000   -0.1859944  -0.1571257 0.1439793
> Petal.Length   0.71851593 -0.18599442    1.0000000   0.8068907 0.2677154
> Petal.Width    0.65530856 -0.15712566    0.8068907   1.0000000 0.2724843
> Species        0.18977778  0.14397927    0.2677154   0.2724843 1.0000000
>
> To me, this is dangerous!
>
> Andy
>
>
> From: Gabor Grothendieck
>
> >
> > You can calculate the Kendall rank correlation with such a matrix
> > so you would not want to exclude factors in that case:
> >
> > > cor(iris, method = "kendall")
> >              Sepal.Length Sepal.Width Petal.Length
> > Petal.Width    Species
> > Sepal.Length   1.00000000 -0.07699679    0.7185159
> > 0.6553086  0.6704444
> > Sepal.Width   -0.07699679  1.00000000   -0.1859944
> > -0.1571257 -0.3376144
> > Petal.Length   0.71851593 -0.18599442    1.0000000
> > 0.8068907  0.8229112
> > Petal.Width    0.65530856 -0.15712566    0.8068907
> > 1.0000000  0.8396874
> > Species        0.67044444 -0.33761438    0.8229112
> > 0.8396874  1.0000000
> >
> >
> > On Dec 3, 2007 9:27 AM, Michael Friendly <friendly at yorku.ca> wrote:
> > > In using cor(data.frame), it is annoying that you have to explicitly
> > > filter out non-numeric columns, and when you don't, the
> > error message
> > > is misleading:
> > >
> > >  > cor(iris)
> > > Error in cor(iris) : missing observations in cov/cor
> > > In addition: Warning message:
> > > In cor(iris) : NAs introduced by coercion
> > >
> > > It would be nicer if stats:::cor() did the equivalent
> > *itself* of the
> > > following for a data.frame:
> > >  > cor(iris[,sapply(iris, is.numeric)])
> > >              Sepal.Length Sepal.Width Petal.Length Petal.Width
> > > Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
> > > Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
> > > Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
> > > Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
> > >  >
> > >
> > > A change could be implemented here:
> > >     if (is.data.frame(x))
> > >         x <- as.matrix(x)
> > >
> > > Second, the default, use="all" throws an error if there are any
> > > NAs.  It would be nicer if the default was use="complete.cases",
> > > which would generate warnings instead.  Most other statistical
> > > software is more tolerant of missing data.
> > >
> > >  > library(corrgram)
> > >  > data(auto)
> > >  > cor(auto[,sapply(auto, is.numeric)])
> > > Error in cor(auto[, sapply(auto, is.numeric)]) :
> > >   missing observations in cov/cor
> > >  > cor(auto[,sapply(auto, is.numeric)],use="complete")
> > > # works; output elided
> > >
> > > -Michael
> > >
> > > --
> > > Michael Friendly     Email: friendly AT yorku DOT ca
> > > Professor, Psychology Dept.
> > > York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> > > 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> > > Toronto, ONT  M3J 1P3 CANADA
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attach...{{dropped:15}}


From efg at stowers-institute.org  Mon Dec  3 20:26:11 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Mon, 3 Dec 2007 13:26:11 -0600
Subject: [R] about col in heatmap.2
References: <5032046e0711301409q6b0b10b6u34463c0cc3314c01@mail.gmail.com><644e1f320711301608r284235abpd4a535deb0e54921@mail.gmail.com><5032046e0711301658w7589fedax64ded2da1d7e2fd@mail.gmail.com><644e1f320711301725q4e3f2a1bs3b00ce0bd873aac8@mail.gmail.com>
	<5032046e0711301734o2bdd53a5p3da1276bc68b28ba@mail.gmail.com>
Message-ID: <fj1l8m$jqd$1@ger.gmane.org>

"affy snp" <affysnp at gmail.com> wrote in message 
news:5032046e0711301734o2bdd53a5p3da1276bc68b28ba at mail.gmail.com...
> For example, it should go from
> very red---red---less red---dark----green---very green coinciding
> with the descending order of values, just like the very left panel
> shown in
> http://www.bme.unc.edu/research/Bioinformatics.FunctionalGenomics.html

This looks like the MatLab palette that's in tim.colors:

library(fields)  # tim.colors:  Matlab-like color palette

N <- 100
par(lend="square")
plot(rep(1,N), type="h", col=tim.colors(N), lwd=6, ylim=c(0,1))


efg

Earl F. Glynn
Scientific Programmer
Stowers Institute for Medical Research


From ggrothendieck at gmail.com  Mon Dec  3 20:40:16 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Dec 2007 14:40:16 -0500
Subject: [R] Plotting monthly timeseries with an x-axis in "time format"
In-Reply-To: <200712031811.47949.vdemart1@tin.it>
References: <200712031811.47949.vdemart1@tin.it>
Message-ID: <971536df0712031140g1e3f03a4r514a80f591a853eb@mail.gmail.com>

This can be done with plot.zoo and a panel function:

tab <- ts(cbind(A = c(79.47, 89.13, 84.86, 75.68, 72.82, 78.87, 93.46,
78.18, 82.46, 77.25, 80.95, 84.39, 81.7, 74.76, 65.29, 60.3,
66.59, 73.19, 92.39, 65.76, 77.45, 74.22, 101.36, 100.01), B = c(77.95,
76.73, 51.2, 51.86, 51.29, 49.45, 53.88, 47.96, 55.07, 45.34,
37.07, 37.53, 47.79, 37.5, 30.35, 37.78, 34.13, 39.14, 39.89,
35.46, 36.54, 38.39, 47.33, 45.34)), start = c(2006, 1), freq = 12)

library(zoo)
pnl.xaxis <- function(...) {
     lines(...)
     panel.number <- parent.frame()$panel.number
     nser <- parent.frame()$nser
     # if bottom panel
     if (!length(panel.number) || panel.number == nser) {
           tt <- list(...)[[1]]
           ym <- as.yearmon(tt)
           mon <- as.numeric(format(ym, "%m"))
           yy <- format(ym, "%y")
           mm <- substring(month.abb[mon], 1, 1)
           axis(1, tt[mon == 1], yy[mon == 1], cex.axis = 0.7)
           axis(1, tt[mon > 1], mm[mon > 1], cex.axis = 0.5, tcl = -0.3)
     }
}
plot(as.zoo(tab), panel = pnl.xaxis, xaxt = "n")



On Dec 3, 2007 12:11 PM, vittorio <vdemart1 at tin.it> wrote:
> I have the following timeseries "tab"
> =====================================
> > str(tab)
>  mts [1:23, 1:2] 79.5 89.1 84.9 75.7 72.8 ...
>  - attr(*, "dimnames")=List of 2
>  ..$ : NULL
>  ..$ : chr [1:2] "Ipex...I" "Omel...E"
>  - attr(*, "tsp")= num [1:3] 2006 2008   12
>  - attr(*, "class")= chr [1:2] "mts" "ts"
>
> > tab
>         Ipex...I Omel...E
> Jan 2006    79.47    77.95
> Feb 2006    89.13    76.73
> Mar 2006    84.86    51.20
> Apr 2006    75.68    51.86
> May 2006    72.82    51.29
> Jun 2006    78.87    49.45
> Jul 2006    93.46    53.88
> Aug 2006    78.18    47.96
> Sep 2006    82.46    55.07
> Oct 2006    77.25    45.34
> Nov 2006    80.95    37.07
> Dec 2006    84.39    37.53
> Jan 2007    81.70    47.79
> Feb 2007    74.76    37.50
> Mar 2007    65.29    30.35
> Apr 2007    60.30    37.78
> May 2007    66.59    34.13
> Jun 2007    73.19    39.14
> Jul 2007    92.39    39.89
> Aug 2007    65.76    35.46
> Sep 2007    77.45    36.54
> Oct 2007    74.22    38.39
> Nov 2007   101.36    47.33
> Dec 2007   100.01  45.34
> ===============================
>
> Plotting tab with a simple "plot(tab,plot.type="single")" I'm obtaining a
> graph  with the x axis in an orrible decimal format so that,e.g., Jan 2006 is
> 2006.0 and Nov 2006 is 2006.8(33)!
>
>  Instead I would like to see the x-axis in a more human-readable format, for
> instance, 12 tics for each year and a label at the beginning of each quarter
> of the year: 2006.1, 2006.4,2006.7.
>  - OR -
> more elegantly, I would like to have the 12 tics with the month shortened
> labels: Jan, Feb, etc. and below, say June, one label for the year.
>
> Please help.
>
> Ciao
> Vittorio
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dkp at gwu.edu  Mon Dec  3 20:45:57 2007
From: dkp at gwu.edu (David Park)
Date: Mon, 3 Dec 2007 14:45:57 -0500
Subject: [R] difficulties getting coef() to work in some lmer() calls
Message-ID: <f255bd8a0712031145v39ca0878tb45d0040cdf217be@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/02e71b93/attachment.pl 

From kate at few.vu.nl  Mon Dec  3 20:41:55 2007
From: kate at few.vu.nl (Katharine Mullen)
Date: Mon, 3 Dec 2007 20:41:55 +0100 (CET)
Subject: [R] [R-pkgs] new package 'bvls', update of 'nnls'
Message-ID: <Pine.GSO.4.56.0712032032140.15974@laurel.few.vu.nl>

A new package 'bvls' is available on CRAN.

The package provides an R interface to the Stark-Parker algorithm for
bounded-variable least squares (BVLS) that solves A x = b with the
constraint l <= x <= u under least squares criteria, where l,x,u \in R^n,
b \in R^m and A is an m \times n matrix.

The Stark-Parker BVLS algorithm was published in

 Stark PB, Parker RL (1995). Bounded-variable least-squares: an
 algorithm and applications, Computational Statistics, 10, 129-141.

The packages interfaces the Fortran77 code distributed via the statlib
on-line software repository at Carnegie Mellon University
(http://lib.stat.cmu.edu/general/bvls), modified very slightly for
compatibility with the gfortran compiler.  Stark and Parker have agreed to
distribution under GPL version 2 or newer.

The function 'bvls::bvls' returns an object of (S3) class 'bvls', which
has methods for 'coefficients', 'fitted.values', 'deviance' and
'residuals'.

====

Version 1.1 of the package 'nnls' is available on CRAN.
Changes between Version 1.0 and 1.1:

	o The function 'nnls::nnls' returns an object of (S3) class
	  'nnls', which has methods for 'coefficients',
	  'fitted.values', 'deviance' and 'residuals'

	o The function 'nnnpls::nnnpls' allows each element of x to be
	  constrained to either a non-positive or a non-negative value

----
Katharine Mullen
mail: Department of Physics and Astronomy, Faculty of Sciences
Vrije Universiteit Amsterdam, de Boelelaan 1081
1081 HV Amsterdam, The Netherlands
room: T.1.06
tel: +31 205987870
fax: +31 205987992
e-mail: kate at nat.vu.nl
homepage: http://www.nat.vu.nl/~kate/

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From gavin.simpson at ucl.ac.uk  Mon Dec  3 20:44:49 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 03 Dec 2007 19:44:49 +0000
Subject: [R] overlapping labels
In-Reply-To: <47544B7E.9050508@stats.uwo.ca>
References: <7124EE4D7E556844A654A607A4203FDD3ED3CD@scomp0038.wurnet.nl>
	<47544B7E.9050508@stats.uwo.ca>
Message-ID: <47545CB1.8070606@ucl.ac.uk>

Duncan Murdoch wrote:
> On 12/3/2007 12:21 PM, Biscarini, Filippo wrote:
>> Good evening,
>>  
>> I am trying to add labels to the point of a simple plot, using the
>> text() function; the problem is that sometimes, if two points are too
>> close to each other, labels overlap and are no longer readable.
>> I was wondering whether there are options that I can use to prevent this
>> overlapping (by, for example, placing labels alternatively above and
>> below the plotted curve), or whether I should use another set of
>> graphical functions (grid or lattice packages).
>> Does anybody have suggestions?
> 
> I don't think there are any functions to automatically avoid overlaps, 
> but it is not hard to manually move some of the labels.  

The OP might like to take a look at the orditorp function in package 
vegan, which is designed to add information from complex ecological 
data sets with many species to ordinations. It works out if a label 
can be placed and if not draws a point. The OP would have to modify it 
to try another place to plot, but that might be a basis from which to 
work.

Of course, if this is a one-off project Duncan's solution is more than 
adequate.

G

For example,
> you could alternate labels above and below with code like this:
> 
>  > x <- 1:10
>  > y <- rep(0, 10)
>  > plot(x,y)
>  > text(x, y, rep("label", 10), pos=rep(c(1,3), 5))
> 
> because the pos parameter can be specified separately for each observation.
> 
> Duncan Murdoch
> 
>>  
>> Thank you,
>>  
>> Filippo Biscarini
>>  
>> PS: These are the lines of codes that I have been using to generate the
>> plot.
>>
>> plot(
>>  prova$x,
>>  prova$y,
>>  type="b",
>>  mar=c(0.5,0.5,0.5,0.5),
>>  xlab="basepairs",
>>  ylab=""
>> );
>>  
>> text(
>>  prova$x,
>>  prova$y,
>>  prova$lab,
>>  pos=3,
>>  offset=2,
>>  cex=0.5
>> );
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
  Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
  ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
  UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From badgones65 at hotmail.com  Mon Dec  3 15:19:36 2007
From: badgones65 at hotmail.com (tintin_et_milou)
Date: Mon, 3 Dec 2007 06:19:36 -0800 (PST)
Subject: [R] comparison of two vectors
In-Reply-To: <14129523.post@talk.nabble.com>
References: <14129032.post@talk.nabble.com> <14129523.post@talk.nabble.com>
Message-ID: <14130713.post@talk.nabble.com>


Thanks for your help, but there is some more problem. The two vectors have
not the same length so there is a problem with cbind. I give you an example.
My first vector is

 >g[g[,1]>2035 & g[,1]<2050,]
    
     M.Z     Intensity
 2035.836  652.9494
 2035.939  664.5841
 2036.043  696.0554
 2036.146  719.8969
 2036.250  750.7660
 2036.767  816.5243
 2036.870  806.8539
 2036.974  774.2397
 2037.491  777.2780
 2039.147  589.9075
 2042.978  807.7167
 2043.082  820.9365
 2043.289  849.4942
 2043.393  883.8975
 2043.495  900.9681
 2043.600  922.3238
 2043.704  956.5985
 2043.911  978.9377
 2044.015  969.1999

and the second one is:
> f[1:9,]
    M.Z      Intensity  Echantillon Position
 1802.809 1064.1210           1       A1
 1865.615 8799.4880           1       A1
 1896.426 1667.5908           1       A1
 2001.064  515.6214           1       A1
 2012.016  837.5599           1       A1
 2021.589 4373.6364           1       A1
 2028.425  832.6896           1       A1
 2036.663    0.0000           1       A1
 2043.497    0.0000           1       A1


The two vectors have not the same number of observations and in fact i would
like to replace all the intensity of zero of the second vector by the value
of the intensity of the nearest m/Z of the first vector.
In this case, the value 816.5243 for the m/Z 2036.663 of the second vector
and 900.9681 for the intensity corresponding at m/Z=2043.497.

Your method give me the intensity of the farest m/Z.

Thanks you again.
Lo?c


Ben Bolker wrote:
> 
> 
> 
> ## construct sample data
> m1 =
>   matrix(c(1000.235,1000.356,
>     125,126.5),ncol=2,
>          dimnames=list(NULL,c("mZ","I")))
> 
> y = c(995.547,1000.320)
> 
> ## compute distances, set diagonal to infinity
> d = as.matrix(dist(cbind(m1[,1],y)))
> diag(d) <- Inf
> 
> ## find minimum distances, extract values
> cbind(y,m1[apply(d,2,which.min),2])
> 
>   Ben Bolker
> 
> 

-- 
View this message in context: http://www.nabble.com/comparison-of-two-vectors-tf4936213.html#a14130713
Sent from the R help mailing list archive at Nabble.com.


From bussia89 at yahoo.com  Mon Dec  3 20:13:49 2007
From: bussia89 at yahoo.com (Nathan Vandergrift)
Date: Mon, 3 Dec 2007 11:13:49 -0800 (PST)
Subject: [R] Controlling Postscript output, size and orientation
In-Reply-To: <Pine.LNX.4.64.0712030856560.1873@gannet.stats.ox.ac.uk>
References: <14035096.post@talk.nabble.com>
	<20071202190325.GF6584@slingshot.co.nz>
	<14120261.post@talk.nabble.com>
	<Pine.LNX.4.64.0712030856560.1873@gannet.stats.ox.ac.uk>
Message-ID: <14136587.post@talk.nabble.com>




Prof Brian Ripley wrote:
> 
> Please do tell us exactly what you are doing via a reproducible example 
> (see the footer to every R-help message).
> 

That code was in my original message, here it is again:
par(	bg="yellow",
	lab=c(10,6,7),
	#mai=c(1.25, 1, 0.2, 0.2),
	pin=c(6,4)
)

curve(300-(200*(exp(-.4*x)-1)), 
	from=0, 
	to=9, 
	n=1000, 
	add=F, 
	type= "l",
	lwd=3,
	xlab="Ocassion of Measurement", 
	ylab="y",
)

# doesn't really work, have to edit in Acrobat to fix...

savePlot("M:/mono", type="ps")



> I added paper="special" to postscript() to make this easier: are you using 
> it?  From the help page
> 
>       The postscript produced for a single R plot is EPS (_Encapsulated
>       PostScript_) compatible, and can be included into other documents,
>       e.g., into LaTeX, using '\includegraphics{<filename>}'.  For use
>       in this way you will probably want to set 'horizontal = FALSE,
>       onefile = FALSE, paper = "special"'.  Note that the bounding box
>       is for the device region: if you find the white space around the
>       plot region excessive, reduce the margins of the figure region via
>       'par(mar=)'.
> 
> Further, I wrote a pdf() driver to make this easier, so why use 
> postscript) to make a PDF presentation?
> 

What I am using is LaTeX with the prosper package to create a presentation
which I give using Adobe Reader (or Acrobat if it is available).

My issue is that it just seems like too many steps to get a "publication
ready" figure. I'll try what you suggested above, thanks.



> 'Adobe' is a company, not a software package.  Which of its products did 
> you mean?
> 

Sorry, Acrobat, thought that went without saying, my bad.

Thanks for the help.



> On Sun, 2 Dec 2007, Nathan Vandergrift wrote:
> 
>> Patrick Connolly-4 wrote:
>>>
>>> On Thu, 29-Nov-2007 at 01:22PM -0800, Nathan Vandergrift wrote:
>>>
>>> |>
>>> |> I'm trying to get my graphics so that I can use them in LaTeX to
>>> create
>>> (via
>>> |> ) a pdf presentation.
>>> |>
>>> |> I've tried controlling inner and outer margins and figure size using
>>> par(),
>>> |> to no avail. The ps output keeps appearing as a portrait page with a
>>> |> centered figure. Nothing I have been able to do so far has changed
>>> that.
>>>
>>> Check out the paper argument to the postscript device.  I think you'll
>>> be more sucessful.
>>>
>>
>> The issue isn't so much viewing is gsview (I've looked at previous
>> threads
>> on this and all my settings in gsview are the ones recommended), but
>> creating a postscript file that is ready to be dumped into the LaTeX
>> prosper
>> package and have a good looking graph for a presentation. Currently, the
>> graph comes out with lots of "white space" on a portrait oriented page.
>>
>> My work around has been to open the file in Adobe and to crop the file
>> (interestingly, when Adobe opens the file, it does not read in the excess
>> "white space"). This works fine, but it is pretty inefficient.
>>
>> I find it hard to believe that I can't control these things in R, but I
>> have
>> been unable to so using the reference manual and this site.
> 
> Perhaps reading the help pages would solve this?  See the quote above.
> 
>> Trying to do it with lattice plots is even worse...
>>
>> Using curve, line, and plot, I should be able to control these things
>> using
>> par(). In a lattice environment, I should be able to control these things
>> using par.settings().
>>
>> Oh, well, I'll keep plugging away...
>>
>>
>>
>>
>> -----
>> -------------------------------
>> Project Scientist
>> University of California, Irvine
>>
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 


-----
-------------------------------
Project Scientist
University of California, Irvine
-- 
View this message in context: http://www.nabble.com/Controlling-Postscript-output%2C-size-and-orientation-tf4899986.html#a14136587
Sent from the R help mailing list archive at Nabble.com.


From deepayan.sarkar at gmail.com  Mon Dec  3 21:20:45 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 3 Dec 2007 12:20:45 -0800
Subject: [R] coplot and xyplot and panel functions
In-Reply-To: <BAY104-W17A9FFB9C36F14CB85F7F8C36C0@phx.gbl>
References: <BAY104-W17A9FFB9C36F14CB85F7F8C36C0@phx.gbl>
Message-ID: <eb555e660712031220r59501dc2xf0263cd44ad8b938@mail.gmail.com>

On 12/3/07, Monica Pisica <pisicandru at hotmail.com> wrote:
>
> Hi,
>
> I wrote a panel function called panelwhite.corplot. If i use this function in a coplot is working fine, but if i use same function with xyplot i get the error: Error using packet 1: plot.new has not been called yet ..... and so on filling all my plots.
>
> So with:
>
> coplot(be~ch|dbh1, envmetr1, panel=panelwhite.corplot)
>
> I get the expected result
>
> with:
>
> xyplot(be~ch | dbh1, data=envmetr1, panel = panelwhite.corplot)
>
> i get the error.
>
> Can you point me to the error i am making here?

Lattice panel functions need to use grid graphics primitives.
Traditional graphics functions, like your panelwhite.corplot, do not
qualify, and will not (easily) work with lattice.

You might find the ?panel.points manual page helpful.

-Deepayan


From dieterbest_2000 at yahoo.com  Mon Dec  3 21:52:19 2007
From: dieterbest_2000 at yahoo.com (Dieter Best)
Date: Mon, 3 Dec 2007 12:52:19 -0800 (PST)
Subject: [R] Make error while installing R 2.6.1 on GNU/linux
Message-ID: <169229.10346.qm@web38403.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/cd32491a/attachment.pl 

From deshon at msu.edu  Mon Dec  3 22:02:30 2007
From: deshon at msu.edu (Rick DeShon)
Date: Mon, 3 Dec 2007 16:02:30 -0500
Subject: [R] Efficient computation of average covariance matrix over a list
In-Reply-To: <c3cb73d50712031057g8daad55s6daca6647fa1ddcd@mail.gmail.com>
References: <c3cb73d50712031057g8daad55s6daca6647fa1ddcd@mail.gmail.com>
Message-ID: <c3cb73d50712031302m3cf75621q89476c5d25798065@mail.gmail.com>

Hi All.

I would like to compute a separate covariance matrix for a set of
variables for each of the levels of a factor and then compute the
average covariance matrix over the factor levels.  I can loop through
this computation but I need to perform the calculation for a large
number of levels and am looking for something more elegant.  To be
concrete....

u    <- 3
n    <- 10

x    <- rnorm((id*u))
y    <- rnorm((id*u))
z    <- rnorm((id*u))
id   <- gl(u,n)

df   <- data.frame(id,x,y,z)
df.s <- split(xxx,id)

lcov <- lapply(df.s,cov)
lcov

What's an efficient way to compute the average covariance matrix over
the list members in "lcov"?

Thanks in advance,

Rick DeShon


From cberry at tajo.ucsd.edu  Mon Dec  3 22:10:39 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Mon, 3 Dec 2007 13:10:39 -0800
Subject: [R] Putting a NULL in a list (as cannot pass NA to C++)
In-Reply-To: <C07600F3B08324418B460FD82E3ADC340C605DAA@exukmb12.eur.nsroot.net>
References: <C07600F3B08324418B460FD82E3ADC340C605DAA@exukmb12.eur.nsroot.net>
Message-ID: <Pine.LNX.4.64.0712031306230.15933@tajo.ucsd.edu>

On Mon, 3 Dec 2007, Rees, David  wrote:

> Hi,
>
> If I do the following I can have a NULL in a list
>
>> x <- list( 1, list(3,NULL,4), 5 )
>> x
> [[1]]
> [1] 1
>
> [[2]]
> [[2]][[1]]
> [1] 3
>
> [[2]][[2]]
> NULL
>
> [[2]][[3]]
> [1] 4
>
>
> [[3]]
> [1] 5
>
> This is a good thing for me as it can be passed through into C++ where I
> can know the value is missing. (Can't seem to detect a NA from C++ for
> some reason. There is RF_isNull in the API but no RF_isNA that I can
> find!)

Is this what you were looking for?


Writing R Extensions

6.4 Missing and IEEE special values

A set of functions is provided to test for NA, Inf, -Inf and NaN. These 
functions are accessed via macros:

      ISNA(x)        True for R's NA only
      ISNAN(x)       True for R's NA and IEEE NaN
      R_FINITE(x)    False for Inf, -Inf, NA, NaN

HTH,

Chuck

p.s. grep '<something>' within R's include directory can quickly resolve 
questions like this.

>
> Is it possible to put a null in an already created list. e.g. now do
>
>> x[[2]][[2]] <- NULL
>> x
> [[1]]
> [1] 1
>
> [[2]]
> [[2]][[1]]
> [1] 3
>
> [[2]][[2]]
> [1] 4
>
>
> [[3]]
> [1] 5
>
>
> But x[[2]][[2]] has been deleted.
>
> Many thanks
>
> Regards,
> David
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From alc at sanger.ac.uk  Mon Dec  3 22:12:50 2007
From: alc at sanger.ac.uk (Avril Coghlan)
Date: Mon, 3 Dec 2007 21:12:50 UT
Subject: [R] question about extreme value distribution
Message-ID: <E1IzIaw-0005db-Qg@web-2-05.internal.sanger.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/cb6e105c/attachment.pl 

From vdemart1 at tin.it  Mon Dec  3 22:16:12 2007
From: vdemart1 at tin.it (vittorio)
Date: Mon, 3 Dec 2007 22:16:12 +0100
Subject: [R] Plotting monthly timeseries with an x-axis in "time format"
In-Reply-To: <971536df0712031140g1e3f03a4r514a80f591a853eb@mail.gmail.com>
References: <200712031811.47949.vdemart1@tin.it>
	<971536df0712031140g1e3f03a4r514a80f591a853eb@mail.gmail.com>
Message-ID: <200712032216.13133.vdemart1@tin.it>

Unfortunately something doesn't work:
> tab <- ts(cbind(A = c(79.47, 89.13, 84.86, 75.68, 72.82, 78.87, 93.46,
+ 78.18, 82.46, 77.25, 80.95, 84.39, 81.7, 74.76, 65.29, 60.3,
+ 66.59, 73.19, 92.39, 65.76, 77.45, 74.22, 101.36, 100.01), B = c(77.95,
+ 76.73, 51.2, 51.86, 51.29, 49.45, 53.88, 47.96, 55.07, 45.34,
+ 37.07, 37.53, 47.79, 37.5, 30.35, 37.78, 34.13, 39.14, 39.89,
+ 35.46, 36.54, 38.39, 47.33, 45.34)), start = c(2006, 1), freq = 12)
>
> library(zoo)
> pnl.xaxis <- function(...) {
+ ? ? ?lines(...)
Error: unexpected input in:
"pnl.xaxis <- function(...) {
?"

What's wrong with it?
Ciao Vittorio



Il Monday 03 December 2007 20:40:16 Gabor Grothendieck ha scritto:
> This can be done with plot.zoo and a panel function:
>
> tab <- ts(cbind(A = c(79.47, 89.13, 84.86, 75.68, 72.82, 78.87, 93.46,
> 78.18, 82.46, 77.25, 80.95, 84.39, 81.7, 74.76, 65.29, 60.3,
> 66.59, 73.19, 92.39, 65.76, 77.45, 74.22, 101.36, 100.01), B = c(77.95,
> 76.73, 51.2, 51.86, 51.29, 49.45, 53.88, 47.96, 55.07, 45.34,
> 37.07, 37.53, 47.79, 37.5, 30.35, 37.78, 34.13, 39.14, 39.89,
> 35.46, 36.54, 38.39, 47.33, 45.34)), start = c(2006, 1), freq = 12)
>
> library(zoo)
> pnl.xaxis <- function(...) {
>      lines(...)
>      panel.number <- parent.frame()$panel.number
>      nser <- parent.frame()$nser
>      # if bottom panel
>      if (!length(panel.number) || panel.number == nser) {
>            tt <- list(...)[[1]]
>            ym <- as.yearmon(tt)
>            mon <- as.numeric(format(ym, "%m"))
>            yy <- format(ym, "%y")
>            mm <- substring(month.abb[mon], 1, 1)
>            axis(1, tt[mon == 1], yy[mon == 1], cex.axis = 0.7)
>            axis(1, tt[mon > 1], mm[mon > 1], cex.axis = 0.5, tcl = -0.3)
>      }
> }
> plot(as.zoo(tab), panel = pnl.xaxis, xaxt = "n")
>
> On Dec 3, 2007 12:11 PM, vittorio <vdemart1 at tin.it> wrote:
> > I have the following timeseries "tab"
> > =====================================
> >
> > > str(tab)
> >
> >  mts [1:23, 1:2] 79.5 89.1 84.9 75.7 72.8 ...
> >  - attr(*, "dimnames")=List of 2
> >  ..$ : NULL
> >  ..$ : chr [1:2] "Ipex...I" "Omel...E"
> >  - attr(*, "tsp")= num [1:3] 2006 2008   12
> >  - attr(*, "class")= chr [1:2] "mts" "ts"
> >
> > > tab
> >
> >         Ipex...I Omel...E
> > Jan 2006    79.47    77.95
> > Feb 2006    89.13    76.73
> > Mar 2006    84.86    51.20
> > Apr 2006    75.68    51.86
> > May 2006    72.82    51.29
> > Jun 2006    78.87    49.45
> > Jul 2006    93.46    53.88
> > Aug 2006    78.18    47.96
> > Sep 2006    82.46    55.07
> > Oct 2006    77.25    45.34
> > Nov 2006    80.95    37.07
> > Dec 2006    84.39    37.53
> > Jan 2007    81.70    47.79
> > Feb 2007    74.76    37.50
> > Mar 2007    65.29    30.35
> > Apr 2007    60.30    37.78
> > May 2007    66.59    34.13
> > Jun 2007    73.19    39.14
> > Jul 2007    92.39    39.89
> > Aug 2007    65.76    35.46
> > Sep 2007    77.45    36.54
> > Oct 2007    74.22    38.39
> > Nov 2007   101.36    47.33
> > Dec 2007   100.01  45.34
> > ===============================
> >
> > Plotting tab with a simple "plot(tab,plot.type="single")" I'm obtaining a
> > graph  with the x axis in an orrible decimal format so that,e.g., Jan
> > 2006 is 2006.0 and Nov 2006 is 2006.8(33)!
> >
> >  Instead I would like to see the x-axis in a more human-readable format,
> > for instance, 12 tics for each year and a label at the beginning of each
> > quarter of the year: 2006.1, 2006.4,2006.7.
> >  - OR -
> > more elegantly, I would like to have the 12 tics with the month shortened
> > labels: Jan, Feb, etc. and below, say June, one label for the year.
> >
> > Please help.
> >
> > Ciao
> > Vittorio
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.


From juryef at yahoo.com  Mon Dec  3 22:21:09 2007
From: juryef at yahoo.com (Judith Flores)
Date: Mon, 3 Dec 2007 13:21:09 -0800 (PST)
Subject: [R] Ordering the levels of a vector
Message-ID: <653581.80700.qm@web34708.mail.mud.yahoo.com>

Hi,

   I have a vector in a data frame that looks
something like this:

day<-c('Day -1','Day 6','Day 10')


   This vector specifies the order in which several
panel will appear in a lattice plots. But the order in
which such plots will appear will be the following:
Day -1, Day 10, Day 6. Which makes sense, but I cannot
name the Days like this: Day -01,Day 10, Day 06, which
would put the levels in the order I want them to be.

  Now, this vector won't always have the same values,
it could be:

day<-c('Day -1, 'Day 2','Day 14')

   So I cannot set the levels manually:

levels(day)<-c('Day -1', 'Day something','Day
something else')
 I tried as.ordered, but I guess I am not using the
right function. 

How can I command the script to put the panels in the
original  order given of the vector in a data frame?

Thank  you,

Judith








      ____________________________________________________________________________________
Be a better friend, newshound, and


From v-nijs at kellogg.northwestern.edu  Mon Dec  3 22:21:15 2007
From: v-nijs at kellogg.northwestern.edu (Vincent Nijs)
Date: Mon, 03 Dec 2007 15:21:15 -0600
Subject: [R] R-help google group
In-Reply-To: <c3cb73d50712031302m3cf75621q89476c5d25798065@mail.gmail.com>
Message-ID: <C379CF6B.AE15%v-nijs@kellogg.northwestern.edu>

I made a google group archive of current and future R-help posts at
http://groups.google.com/group/r-help-archive

If you are signed-up for the R-help mailing list with a gmail account you
can reply to posts through the google group pages. Note that this is not a
separate mailing-list, just a copy of the original. Only posts after
December 3rd 2007 will be available.

I assume there are no objections to this. In case I am wrong please let me
know.

Vincent

 

On 12/3/07 3:02 PM, "Rick DeShon" <deshon at msu.edu> wrote:

> Hi All.
> 
> I would like to compute a separate covariance matrix for a set of
> variables for each of the levels of a factor and then compute the
> average covariance matrix over the factor levels.  I can loop through
> this computation but I need to perform the calculation for a large
> number of levels and am looking for something more elegant.  To be
> concrete....
> 
> u    <- 3
> n    <- 10
> 
> x    <- rnorm((id*u))
> y    <- rnorm((id*u))
> z    <- rnorm((id*u))
> id   <- gl(u,n)
> 
> df   <- data.frame(id,x,y,z)
> df.s <- split(xxx,id)
> 
> lcov <- lapply(df.s,cov)
> lcov
> 
> What's an efficient way to compute the average covariance matrix over
> the list members in "lcov"?
> 
> Thanks in advance,
> 
> Rick DeShon
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

--


From friendly at yorku.ca  Mon Dec  3 22:31:12 2007
From: friendly at yorku.ca (Michael Friendly)
Date: Mon, 03 Dec 2007 16:31:12 -0500
Subject: [R] cor(data.frame) infelicities
In-Reply-To: <971536df0712031105l7c5df515of62b439affd3b3ba@mail.gmail.com>
References: <4754123B.5030902@yorku.ca>	
	<971536df0712030631g647edbb2g155d3ffd5cfe0241@mail.gmail.com>	
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA04FEA333@usctmx1106.merck.com>
	<971536df0712031105l7c5df515of62b439affd3b3ba@mail.gmail.com>
Message-ID: <475475A0.2090704@yorku.ca>

Returning to my original post, I still believe that a basic work-horse
like cor(data.frame) with the default method="pearson" should try to do 
something more useful in this case than barf with a misleading error
message if the data frame contains character variables.

To paraphrase Einstein,
``Things [in R] should be made as simple as possible, but not any simpler''

The case that Andy Liaw cited is a good example of the 'not any
simpler' part.

-Michael

Gabor Grothendieck wrote:
> You are right but I was just trying to stick to the same example.
> In reality it would be ok as long as its an ordered factor.  One could
> restrict it to those of class "ordered".
> 
> 
> On Dec 3, 2007 1:58 PM, Liaw, Andy <andy_liaw at merck.com> wrote:
>> I'd call that another infelicity.  Species is supposed to be nominal,
>> not ordinal, so rank correlation wouldn't make much sense.  So what does
>> cor(, method="kendall") do?  It looks like it simply uses the underlying
>> numeric code.  (Change Species to numerics and you'll see the same
>> answer.)  However, reordering the levels changes the result:
>>
>> R> iris2 <- iris
>> R> levels(iris2$Species) <- levels(iris2$Species)[c(2, 1, 3)]
>> R> cor(iris2, method = "kendall")
>>             Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
>> Sepal.Length   1.00000000 -0.07699679    0.7185159   0.6553086 0.1897778
>> Sepal.Width   -0.07699679  1.00000000   -0.1859944  -0.1571257 0.1439793
>> Petal.Length   0.71851593 -0.18599442    1.0000000   0.8068907 0.2677154
>> Petal.Width    0.65530856 -0.15712566    0.8068907   1.0000000 0.2724843
>> Species        0.18977778  0.14397927    0.2677154   0.2724843 1.0000000
>>
>> To me, this is dangerous!
>>
>> Andy
>>
>>
>> From: Gabor Grothendieck
>>
>>> You can calculate the Kendall rank correlation with such a matrix
>>> so you would not want to exclude factors in that case:
>>>
>>>> cor(iris, method = "kendall")
>>>              Sepal.Length Sepal.Width Petal.Length
>>> Petal.Width    Species
>>> Sepal.Length   1.00000000 -0.07699679    0.7185159
>>> 0.6553086  0.6704444
>>> Sepal.Width   -0.07699679  1.00000000   -0.1859944
>>> -0.1571257 -0.3376144
>>> Petal.Length   0.71851593 -0.18599442    1.0000000
>>> 0.8068907  0.8229112
>>> Petal.Width    0.65530856 -0.15712566    0.8068907
>>> 1.0000000  0.8396874
>>> Species        0.67044444 -0.33761438    0.8229112
>>> 0.8396874  1.0000000
>>>
>>>
>>> On Dec 3, 2007 9:27 AM, Michael Friendly <friendly at yorku.ca> wrote:
>>>> In using cor(data.frame), it is annoying that you have to explicitly
>>>> filter out non-numeric columns, and when you don't, the
>>> error message
>>>> is misleading:
>>>>
>>>>  > cor(iris)
>>>> Error in cor(iris) : missing observations in cov/cor
>>>> In addition: Warning message:
>>>> In cor(iris) : NAs introduced by coercion
>>>>
>>>> It would be nicer if stats:::cor() did the equivalent
>>> *itself* of the
>>>> following for a data.frame:
>>>>  > cor(iris[,sapply(iris, is.numeric)])
>>>>              Sepal.Length Sepal.Width Petal.Length Petal.Width
>>>> Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
>>>> Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
>>>> Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
>>>> Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
>>>>  >
>>>>
>>>> A change could be implemented here:
>>>>     if (is.data.frame(x))
>>>>         x <- as.matrix(x)
>>>>
>>>> Second, the default, use="all" throws an error if there are any
>>>> NAs.  It would be nicer if the default was use="complete.cases",
>>>> which would generate warnings instead.  Most other statistical
>>>> software is more tolerant of missing data.
>>>>
>>>>  > library(corrgram)
>>>>  > data(auto)
>>>>  > cor(auto[,sapply(auto, is.numeric)])
>>>> Error in cor(auto[, sapply(auto, is.numeric)]) :
>>>>   missing observations in cov/cor
>>>>  > cor(auto[,sapply(auto, is.numeric)],use="complete")
>>>> # works; output elided
>>>>
>>>> -Michael
-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA


From ggrothendieck at gmail.com  Mon Dec  3 22:34:48 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Dec 2007 16:34:48 -0500
Subject: [R] Plotting monthly timeseries with an x-axis in "time format"
In-Reply-To: <200712032216.13133.vdemart1@tin.it>
References: <200712031811.47949.vdemart1@tin.it>
	<971536df0712031140g1e3f03a4r514a80f591a853eb@mail.gmail.com>
	<200712032216.13133.vdemart1@tin.it>
Message-ID: <971536df0712031334g768234afx60d51dd18c3db0c8@mail.gmail.com>

Maybe your email software corrupted it somehow.  Often
email software will cause weird line wrappings, for example.
Or maybe you have an old version of R or zoo.  I am running zoo
1.4.0 and "R version 2.6.0 Patched (2007-10-08 r43124)"

I just located my post in the archives
https://stat.ethz.ch/pipermail/r-help/2007-December/147481.html

and copied it from there pasting it into a fresh R session and
it worked.

On Dec 3, 2007 4:16 PM, vittorio <vdemart1 at tin.it> wrote:
> Unfortunately something doesn't work:
> > tab <- ts(cbind(A = c(79.47, 89.13, 84.86, 75.68, 72.82, 78.87, 93.46,
> + 78.18, 82.46, 77.25, 80.95, 84.39, 81.7, 74.76, 65.29, 60.3,
> + 66.59, 73.19, 92.39, 65.76, 77.45, 74.22, 101.36, 100.01), B = c(77.95,
> + 76.73, 51.2, 51.86, 51.29, 49.45, 53.88, 47.96, 55.07, 45.34,
> + 37.07, 37.53, 47.79, 37.5, 30.35, 37.78, 34.13, 39.14, 39.89,
> + 35.46, 36.54, 38.39, 47.33, 45.34)), start = c(2006, 1), freq = 12)
> >
> > library(zoo)
> > pnl.xaxis <- function(...) {
> + lines(...)
> Error: unexpected input in:
> "pnl.xaxis <- function(...) {
> "
>
> What's wrong with it?
> Ciao Vittorio
>
>
>
> Il Monday 03 December 2007 20:40:16 Gabor Grothendieck ha scritto:
>
> > This can be done with plot.zoo and a panel function:
> >
> > tab <- ts(cbind(A = c(79.47, 89.13, 84.86, 75.68, 72.82, 78.87, 93.46,
> > 78.18, 82.46, 77.25, 80.95, 84.39, 81.7, 74.76, 65.29, 60.3,
> > 66.59, 73.19, 92.39, 65.76, 77.45, 74.22, 101.36, 100.01), B = c(77.95,
> > 76.73, 51.2, 51.86, 51.29, 49.45, 53.88, 47.96, 55.07, 45.34,
> > 37.07, 37.53, 47.79, 37.5, 30.35, 37.78, 34.13, 39.14, 39.89,
> > 35.46, 36.54, 38.39, 47.33, 45.34)), start = c(2006, 1), freq = 12)
> >
> > library(zoo)
> > pnl.xaxis <- function(...) {
> >      lines(...)
> >      panel.number <- parent.frame()$panel.number
> >      nser <- parent.frame()$nser
> >      # if bottom panel
> >      if (!length(panel.number) || panel.number == nser) {
> >            tt <- list(...)[[1]]
> >            ym <- as.yearmon(tt)
> >            mon <- as.numeric(format(ym, "%m"))
> >            yy <- format(ym, "%y")
> >            mm <- substring(month.abb[mon], 1, 1)
> >            axis(1, tt[mon == 1], yy[mon == 1], cex.axis = 0.7)
> >            axis(1, tt[mon > 1], mm[mon > 1], cex.axis = 0.5, tcl = -0.3)
> >      }
> > }
> > plot(as.zoo(tab), panel = pnl.xaxis, xaxt = "n")
> >
> > On Dec 3, 2007 12:11 PM, vittorio <vdemart1 at tin.it> wrote:
> > > I have the following timeseries "tab"
> > > =====================================
> > >
> > > > str(tab)
> > >
> > >  mts [1:23, 1:2] 79.5 89.1 84.9 75.7 72.8 ...
> > >  - attr(*, "dimnames")=List of 2
> > >  ..$ : NULL
> > >  ..$ : chr [1:2] "Ipex...I" "Omel...E"
> > >  - attr(*, "tsp")= num [1:3] 2006 2008   12
> > >  - attr(*, "class")= chr [1:2] "mts" "ts"
> > >
> > > > tab
> > >
> > >         Ipex...I Omel...E
> > > Jan 2006    79.47    77.95
> > > Feb 2006    89.13    76.73
> > > Mar 2006    84.86    51.20
> > > Apr 2006    75.68    51.86
> > > May 2006    72.82    51.29
> > > Jun 2006    78.87    49.45
> > > Jul 2006    93.46    53.88
> > > Aug 2006    78.18    47.96
> > > Sep 2006    82.46    55.07
> > > Oct 2006    77.25    45.34
> > > Nov 2006    80.95    37.07
> > > Dec 2006    84.39    37.53
> > > Jan 2007    81.70    47.79
> > > Feb 2007    74.76    37.50
> > > Mar 2007    65.29    30.35
> > > Apr 2007    60.30    37.78
> > > May 2007    66.59    34.13
> > > Jun 2007    73.19    39.14
> > > Jul 2007    92.39    39.89
> > > Aug 2007    65.76    35.46
> > > Sep 2007    77.45    36.54
> > > Oct 2007    74.22    38.39
> > > Nov 2007   101.36    47.33
> > > Dec 2007   100.01  45.34
> > > ===============================
> > >
> > > Plotting tab with a simple "plot(tab,plot.type="single")" I'm obtaining a
> > > graph  with the x axis in an orrible decimal format so that,e.g., Jan
> > > 2006 is 2006.0 and Nov 2006 is 2006.8(33)!
> > >
> > >  Instead I would like to see the x-axis in a more human-readable format,
> > > for instance, 12 tics for each year and a label at the beginning of each
> > > quarter of the year: 2006.1, 2006.4,2006.7.
> > >  - OR -
> > > more elegantly, I would like to have the 12 tics with the month shortened
> > > labels: Jan, Feb, etc. and below, say June, one label for the year.
> > >
> > > Please help.
> > >
> > > Ciao
> > > Vittorio
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Mon Dec  3 22:36:38 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Dec 2007 16:36:38 -0500
Subject: [R] Ordering the levels of a vector
In-Reply-To: <653581.80700.qm@web34708.mail.mud.yahoo.com>
References: <653581.80700.qm@web34708.mail.mud.yahoo.com>
Message-ID: <971536df0712031336o21bcacdey678f734a8d13f05e@mail.gmail.com>

Try mixedsort in the gtools package.

On Dec 3, 2007 4:21 PM, Judith Flores <juryef at yahoo.com> wrote:
> Hi,
>
>   I have a vector in a data frame that looks
> something like this:
>
> day<-c('Day -1','Day 6','Day 10')
>
>
>   This vector specifies the order in which several
> panel will appear in a lattice plots. But the order in
> which such plots will appear will be the following:
> Day -1, Day 10, Day 6. Which makes sense, but I cannot
> name the Days like this: Day -01,Day 10, Day 06, which
> would put the levels in the order I want them to be.
>
>  Now, this vector won't always have the same values,
> it could be:
>
> day<-c('Day -1, 'Day 2','Day 14')
>
>   So I cannot set the levels manually:
>
> levels(day)<-c('Day -1', 'Day something','Day
> something else')
>  I tried as.ordered, but I guess I am not using the
> right function.
>
> How can I command the script to put the panels in the
> original  order given of the vector in a data frame?
>
> Thank  you,
>
> Judith
>
>
>
>
>
>
>
>
>      ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From deepayan.sarkar at gmail.com  Mon Dec  3 22:42:15 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 3 Dec 2007 13:42:15 -0800
Subject: [R] Ordering the levels of a vector
In-Reply-To: <653581.80700.qm@web34708.mail.mud.yahoo.com>
References: <653581.80700.qm@web34708.mail.mud.yahoo.com>
Message-ID: <eb555e660712031342x60f4a476i651b1bd7136475e8@mail.gmail.com>

On 12/3/07, Judith Flores <juryef at yahoo.com> wrote:
> Hi,
>
>    I have a vector in a data frame that looks
> something like this:
>
> day<-c('Day -1','Day 6','Day 10')
>
>
>    This vector specifies the order in which several
> panel will appear in a lattice plots. But the order in
> which such plots will appear will be the following:
> Day -1, Day 10, Day 6. Which makes sense, but I cannot
> name the Days like this: Day -01,Day 10, Day 06, which
> would put the levels in the order I want them to be.
>
>   Now, this vector won't always have the same values,
> it could be:
>
> day<-c('Day -1, 'Day 2','Day 14')
>
>    So I cannot set the levels manually:
>
> levels(day)<-c('Day -1', 'Day something','Day
> something else')

Well, you need some sort of rule that can be used to determine the
order of the levels. The default (see ?factor) is 'levels =
sort(unique(x))'. If you instead want, say, levels in the order of
first appearance (assuming that's what you mean by "original order"),
you could define

my.factor = function(x) { factor(x, levels = unique(x)) }

and then use

> day<- my.factor(c('Day -1','Day 6','Day 10'))
> day
[1] Day -1 Day 6  Day 10
Levels: Day -1 Day 6 Day 10

>  I tried as.ordered, but I guess I am not using the
> right function.
>
> How can I command the script to put the panels in the
> original  order given of the vector in a data frame?

-Deepayan


From marc_schwartz at comcast.net  Mon Dec  3 22:53:59 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 03 Dec 2007 15:53:59 -0600
Subject: [R] Ordering the levels of a vector
In-Reply-To: <653581.80700.qm@web34708.mail.mud.yahoo.com>
References: <653581.80700.qm@web34708.mail.mud.yahoo.com>
Message-ID: <1196718839.2923.69.camel@Bellerophon.localdomain>


On Mon, 2007-12-03 at 13:21 -0800, Judith Flores wrote:
> Hi,
> 
>    I have a vector in a data frame that looks
> something like this:
> 
> day<-c('Day -1','Day 6','Day 10')
> 
> 
>    This vector specifies the order in which several
> panel will appear in a lattice plots. But the order in
> which such plots will appear will be the following:
> Day -1, Day 10, Day 6. Which makes sense, but I cannot
> name the Days like this: Day -01,Day 10, Day 06, which
> would put the levels in the order I want them to be.
> 
>   Now, this vector won't always have the same values,
> it could be:
> 
> day<-c('Day -1, 'Day 2','Day 14')
> 
>    So I cannot set the levels manually:
> 
> levels(day)<-c('Day -1', 'Day something','Day
> something else')
>  I tried as.ordered, but I guess I am not using the
> right function. 
> 
> How can I command the script to put the panels in the
> original  order given of the vector in a data frame?
> 
> Thank  you,
> 
> Judith

You could strip the 'Day' part of the elements using gsub(), sort the
numeric part and then paste() 'Day' back to the result:

set.seed(1)
day <- paste("Day", sample(-2:10))

> day
 [1] "Day 1"  "Day 2"  "Day 4"  "Day 7"  "Day -1" "Day 5"  "Day 8" 
 [8] "Day 10" "Day 3"  "Day -2" "Day 9"  "Day 0"  "Day 6" 

day.tmp <- sort(as.numeric(gsub("[^0-9\\-]", "", day)))

> day.tmp
 [1] -2 -1  0  1  2  3  4  5  6  7  8  9 10

day.levels <- paste("Day", day.tmp)

> day.levels
 [1] "Day -2" "Day -1" "Day 0"  "Day 1"  "Day 2"  "Day 3"  "Day 4" 
 [8] "Day 5"  "Day 6"  "Day 7"  "Day 8"  "Day 9"  "Day 10"

day.ord <- factor(day, levels = day.levels)

# Note the order of the levels versus the order of the factor output
> day.ord
 [1] Day 1  Day 2  Day 4  Day 7  Day -1 Day 5  Day 8  Day 10 Day 3 
[10] Day -2 Day 9  Day 0  Day 6 
13 Levels: Day -2 Day -1 Day 0 Day 1 Day 2 Day 3 Day 4 Day 5 ... Day 10


HTH,

Marc Schwartz


From taekyunk at gmail.com  Mon Dec  3 23:12:37 2007
From: taekyunk at gmail.com (T.K.)
Date: Mon, 3 Dec 2007 14:12:37 -0800
Subject: [R] Help with tables
In-Reply-To: <001901c835ab$d8dc5fe0$327412ac@FARMER0001>
References: <OFA8D61202.7C9DFE4F-ONC12573A6.002E7813-C12573A6.002EB42D@precheza.cz>
	<001901c835ab$d8dc5fe0$327412ac@FARMER0001>
Message-ID: <36923f1d0712031412l539f86c4u27ae154e8a373623@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/c321bd6c/attachment.pl 

From g.abraham at ms.unimelb.edu.au  Mon Dec  3 23:34:43 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Tue, 04 Dec 2007 09:34:43 +1100
Subject: [R] modeling time series with ARIMA
In-Reply-To: <802765.91180.qm@web38615.mail.mud.yahoo.com>
References: <802765.91180.qm@web38615.mail.mud.yahoo.com>
Message-ID: <47548483.4000903@ms.unimelb.edu.au>

eugen pircalabelu wrote:
> Good afternoon!
> 
> I'm trying to model a time series on the following data, which represent a monthly consumption of juices:
> 
>> x<-scan()
> 1: 2859  3613  3930  5193  4523  3226  4280  3436  3235  3379  3517  6022
> 13:  4465  4604  5441  6575  6092  6607  6390  6150  6488  5912  6228 10196
> 25:  7612  7270  8617  9535  8449  8520  9148  8077  7824  7991  7660 12130
> 37:  9135  9512  9631 12642 11369 12140 13953 12421 11081
> 46: 
> Read 45 items
> 
>> arima(x,order=c(2,1,2), seasonal=list(order=c(0,1,0), period=12))->l
>> acf(l$resid)
>> sd(l$resid)
>> Box.test(l$resid)
> 
> Now, my problem:
> 1. All the analysis that i have seen regarding ARIMA modeling, had the residuals acf,  within the confidence interval, while my residual acf at first lag is very close to one (and going out of the confidence interval), even if the Box.test can not reject the null hypothesis of a significant acf for all my residuals.
> I imagine that i am doing something wrong with my model. Is the acf at lag 1 a sign that my residuals are not white noise, or what is wrong here?

Perhaps you misread the plot? I've tried your code and the there is no 
significant correlation at lag 1, but at lag zero, which is 1 by definition.

> 
> 2. What would be the impact of an inappropriate model on the confidence interval for a future prediction? (disregarding the fact  that an inappropriate model would give a bad forecast on future value, could it have also an impact on enlarging the interval?)

Are you referring to the 95% prediction limits (as in Box 1994 "Time 
series analysis: forecasting and control" pp 139?145)? If so, a bad 
model would mean that the variance of the shocks (errors) is higher, 
therefore the prediction limits would be wider. (In other words, you've 
explained less of the variance of the time series.)

> 
> 3. As a rule of thumb, do you chose your model by selecting the lowest AIC, or by the lowest standard deviation of the residuals ?

I've found that for a given time series you can fit several different 
ARIMA models with very similar results. Out of a group of "sensible" 
models (judged by residuals and cross-validated forecast MSE), I'd 
choose the simplest model(s).


-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From vincent.nijs at gmail.com  Mon Dec  3 22:23:44 2007
From: vincent.nijs at gmail.com (Vincent)
Date: Mon, 3 Dec 2007 13:23:44 -0800 (PST)
Subject: [R] R-help google group archive
Message-ID: <51096989-98dc-44a4-b627-dc01c005ca92@s8g2000prg.googlegroups.com>

I made a google group archive of current and future R-help posts at
http://groups.google.com/group/r-help-archive

If you are signed-up for the R-help mailing list with a gmail account
you can post/reply through the google group pages. Note that this is
not a separate mailing-list, just a copy of the original. Only posts
after December 2nd 2007 will be available.

I assume there are no objections to this. In case I am wrong please
let me know.

Vincent


From bolker at ufl.edu  Tue Dec  4 00:05:58 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 3 Dec 2007 15:05:58 -0800 (PST)
Subject: [R] comparison of two vectors
In-Reply-To: <14130713.post@talk.nabble.com>
References: <14129032.post@talk.nabble.com> <14129523.post@talk.nabble.com>
	<14130713.post@talk.nabble.com>
Message-ID: <14140387.post@talk.nabble.com>



tintin_et_milou wrote:
> 
> Thanks for your help, but there is some more problem. The two vectors have
> not the same length so there is a problem with cbind. I give you an
> example. My first vector is
> 
>  >g[g[,1]>2035 & g[,1]<2050,]
>     
>      M.Z     Intensity
>  2035.836  652.9494
>  2035.939  664.5841
>  2036.043  696.0554
>  2036.146  719.8969
>  2036.250  750.7660
>  2036.767  816.5243
>  2036.870  806.8539
>  2036.974  774.2397
>  2037.491  777.2780
>  2039.147  589.9075
>  2042.978  807.7167
>  2043.082  820.9365
>  2043.289  849.4942
>  2043.393  883.8975
>  2043.495  900.9681
>  2043.600  922.3238
>  2043.704  956.5985
>  2043.911  978.9377
>  2044.015  969.1999
> 
> and the second one is:
>> f[1:9,]
>     M.Z      Intensity  Echantillon Position
>  1802.809 1064.1210           1       A1
>  1865.615 8799.4880           1       A1
>  1896.426 1667.5908           1       A1
>  2001.064  515.6214           1       A1
>  2012.016  837.5599           1       A1
>  2021.589 4373.6364           1       A1
>  2028.425  832.6896           1       A1
>  2036.663    0.0000           1       A1
>  2043.497    0.0000           1       A1
> 
> 
> The two vectors have not the same number of observations and in fact i
> would like to replace all the intensity of zero of the second vector by
> the value of the intensity of the nearest m/Z of the first vector.
> In this case, the value 816.5243 for the m/Z 2036.663 of the second vector
> and 900.9681 for the intensity corresponding at m/Z=2043.497.
> 
> Your method give me the intensity of the farest m/Z.
> 
> Thanks you again.
> Lo?c
> 
> 
> Ben Bolker wrote:
>> 
>> 
>> 
>> ## construct sample data
>> m1 =
>>   matrix(c(1000.235,1000.356,
>>     125,126.5),ncol=2,
>>          dimnames=list(NULL,c("mZ","I")))
>> 
>> y = c(995.547,1000.320)
>> 
>> ## compute distances, set diagonal to infinity
>> d = as.matrix(dist(cbind(m1[,1],y)))
>> diag(d) <- Inf
>> 
>> ## find minimum distances, extract values
>> cbind(y,m1[apply(d,2,which.min),2])
>> 
>>   Ben Bolker
>> 
>> 
> 
> 

  oops -- I think you should get the distance matrix via some form of

distfun <- function(x1,x2) { (x1-x2)^2 }

outer(m1[,1],y,distfun)

  my first answer was simply wrong.
  (I don't have time to test this -- play around to make sure
I put the vectors in the right order.)

  cheers
    Ben

-- 
View this message in context: http://www.nabble.com/comparison-of-two-vectors-tf4936213.html#a14140387
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Tue Dec  4 00:08:28 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Dec 2007 18:08:28 -0500
Subject: [R] fitting "power model" in nls()
In-Reply-To: <971536df0712021306u79cd9fc6u50b012cf851823a9@mail.gmail.com>
References: <589039.44736.qm@web56012.mail.re3.yahoo.com>
	<971536df0712021306u79cd9fc6u50b012cf851823a9@mail.gmail.com>
Message-ID: <971536df0712031508u11825432r8552b647de53f924@mail.gmail.com>

I played around with this a bit more and noticed that the "plinear"
algorithm of nls converged using nearly every starting value I tried.  In fact
A = 0 was the only starting value that I could find that did not converge.
Note that with "plinear" you only specify the starting values for non-linear
parameters, in this case A, while the unnamed linear parameters are implied
as coefficients of the columns of the matrix defined in the rhs.

> nls(richness ~ cbind(1, area^A), start = c(A = 1), algorithm = "plinear")
Nonlinear regression model
  model:  richness ~ cbind(1, area^A)
   data:  parent.frame()
       A    .lin1    .lin2
 -0.4464  33.9290 -33.4595
 residual sum-of-squares: 8751

Number of iterations to convergence: 6
Achieved convergence tolerance: 4.968e-07


On Dec 2, 2007 4:06 PM, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> OK.  Since the model is linear except for A lets use brute force to
> repeatedly evaluate the sum of squares for values of A between
> -2 and 2 proceeding in steps of .01 solving the other parameters using
> lm. That will give us better starting values and we should be able to
> use nls on that.
> > x <- seq(-2, 2, .01)
> > ss <- sapply(x, function(A) sum(resid(lm(richness ~ I(area^A)))^2))
> > plot(ss ~ x)
> > x[which.min(ss)]
> [1] -0.45
> > model.lm <- lm(richness ~ I(area^-0.45))
> > # use starting values based on lm and A = -0.45
> > st <- c(Const = coef(model.lm)[[1]], B = coef(model.lm)[[2]], A = x[which.min(ss)])
> > nls(richness ~ Const+B*(area^A), st = st)
> Nonlinear regression model
>  model:  richness ~ Const + B * (area^A)
>   data:  parent.frame()
>   Const        B        A
>  33.9289 -33.4595  -0.4464
>  residual sum-of-squares: 8751
>
> Number of iterations to convergence: 2
> Achieved convergence tolerance: 3.368e-06
>
> Note that our A value is suspiciously close to A = -0.5 and sqrt(area)
> is length so I wonder if there is an argument based on units of
> measurement that might support a model of the form:
>
> richness = Const + B / sqrt(area)
>
>
>
>
>
> On Dec 2, 2007 3:39 PM, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
> >
> > Dear Gabor,
> >
> > Thank you for your reply.
> > In fact I am ajusting several models at same time, like linear, log-linear,
> > log-log, piecewise etc. One of the models are the power model. I really need
> > to fit a power model because it one of the hypothesis which have been
> > suggested on literature.
> >
> > In addition, there are other variables which are beeing tested as
> > explanatory.
> >
> > Kind regards,
> >
> > miltinho
> > ----- Mensagem original ----
> > De: Gabor Grothendieck <ggrothendieck at gmail.com>
> > Para: Milton Cezar Ribeiro <milton_ruser at yahoo.com.br>
> > Cc: R-help <r-help at stat.math.ethz.ch>
> > Enviadas: Domingo, 2 de Dezembro de 2007 17:28:23
> > Assunto: Re: [R] fitting "power model" in nls()
> >
> >
> >
> > Is that really the model we want?  When we have problems sometimes
> > its just a sign that the model is not very good in the first place.
> >
> > plot(richness ~ area)
> >
> > shows most of the points crowded the left and just a few points out to
> > the right.  This
> > does not seem like a very good pattern for model fitting.
> >
> > plot(richness ~ log(area))
> > plot(log(richness) ~ log(area))
> >
> > both look nicer.
> >
> > On Dec 2, 2007 2:08 PM, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br>
> > wrote:
> > > Dear all,
> > > I am still fighting against my "power model".
> > > I tryed several times to use nls() but I can?t run it.
> > > I am sending my variables and also the model which I would like to fit.
> > > As you can see, this "power model" is not the best model to be fit, but I
> > really need also to fit it.
> > >
> > > The model which I would like to fit is Richness = B*(Area^A)
> > >
> > >
> > richness<-c(44,36,31,39,38,26,37,33,34,48,25,22,44,5,9,13,17,15,21,10,16,22,13,20,9,15,14,21,23,23,32,29,20,
> > >
> > 26,31,4,20,25,24,32,23,33,34,23,28,30,10,29,40,10,8,12,13,14,56,47,44,37,27,17,32,31,26,23,31,34,
> > >
> > 37,32,26,37,28,38,35,27,34,35,32,27,22,23,13,28,13,22,45,33,46,37,21,28,38,21,18,21,18,24,18,23,22,
> > > 38,40,52,31,38,15,21)
> > >
> > area<-c(26.22,20.45,128.68,117.24,19.61,295.21,31.83,30.36,13.57,60.47,205.30,40.21,
> > > 7.99,1.18,5.40,13.37,4.51,36.61,7.56,10.30,7.29,9.54,6.93,12.60,
> > > 2.43,18.89,15.03,14.49,28.46,36.03,38.52,45.16,58.27,67.13,92.33,1.17,
> > >
> > 29.52,84.38,87.57,109.08,72.28,66.15,142.27,76.41,105.76,73.47,1.71,305.75,
> > > 325.78,3.71,6.48,19.26,3.69,6.27,1689.67,95.23,13.47,8.60,96.00,436.97,
> > >
> > 472.78,441.01,467.24,1169.11,1309.10,1905.16,135.92,438.25,526.68,88.88,31.43,21.22,
> > >
> > 640.88,14.09,28.91,103.38,178.99,120.76,161.15,137.38,158.31,179.36,214.36,187.05,
> > >
> > 140.92,258.42,85.86,47.70,44.09,18.04,127.84,1694.32,34.27,75.19,54.39,79.88,
> > > 63.84,82.24,88.23,202.66,148.93,641.76,20.45,145.31,27.52,30.70)
> > > plot(richness~area)
> > >
> > > I tryed to fit the following model:
> > >
> > > m1<-nls(richness ~ Const+B*(area^A))
> > >
> > > Thanks a lot,
> > > miltinho
> > > Brazil.
> > >
> > >
> > >
> > >  para armazenamento!
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> >
> >
> >
> > ________________________________
> > Abra sua conta no Yahoo! Mail, o ?nico sem limite de espa?o para
> > armazenamento!
>


From Dan.Kelley at dal.ca  Tue Dec  4 00:22:36 2007
From: Dan.Kelley at dal.ca (dankelley)
Date: Mon, 3 Dec 2007 15:22:36 -0800 (PST)
Subject: [R]  strsplit on comma, with a trailing comma in input
Message-ID: <14141151.post@talk.nabble.com>


I have a comma-separated data file in which trailing commas sometimes occur. 
I am using strsplit to extract the data from this file, and it seems great
except in cases with trailing comma characters.

The example below illustrates.  What I'd like is to get a fourth element in
the answer, being an empty string just like the second element.  Is there a
way I can express my patter (or perhaps specify perl or extended) to get
that?


> strsplit("a,,b,", ",")[[1]]
[1] "a" ""  "b"



-- 
View this message in context: http://www.nabble.com/strsplit-on-comma%2C-with-a-trailing-comma-in-input-tf4940023.html#a14141151
Sent from the R help mailing list archive at Nabble.com.


From bcarvalh at jhsph.edu  Tue Dec  4 00:33:28 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 3 Dec 2007 18:33:28 -0500
Subject: [R] strsplit on comma, with a trailing comma in input
In-Reply-To: <14141151.post@talk.nabble.com>
References: <14141151.post@talk.nabble.com>
Message-ID: <89FAD696-1CBE-4267-8688-FF3F0DC1E733@jhsph.edu>

my understanding is that this behavior is known (the help file  
mentions something along these lines in the example).

i'd use something like:

theText <- "a,,b,"
theText <- gsub("\\,$", ", ", theText)

and then use strsplit() on "theText"

b

On Dec 3, 2007, at 6:22 PM, dankelley wrote:

>
> I have a comma-separated data file in which trailing commas  
> sometimes occur.
> I am using strsplit to extract the data from this file, and it seems  
> great
> except in cases with trailing comma characters.
>
> The example below illustrates.  What I'd like is to get a fourth  
> element in
> the answer, being an empty string just like the second element.  Is  
> there a
> way I can express my patter (or perhaps specify perl or extended) to  
> get
> that?
>
>
>> strsplit("a,,b,", ",")[[1]]
> [1] "a" ""  "b"
>
>
>
> -- 
> View this message in context: http://www.nabble.com/strsplit-on-comma%2C-with-a-trailing-comma-in-input-tf4940023.html#a14141151
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Tue Dec  4 00:33:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Dec 2007 18:33:41 -0500
Subject: [R] strsplit on comma, with a trailing comma in input
In-Reply-To: <14141151.post@talk.nabble.com>
References: <14141151.post@talk.nabble.com>
Message-ID: <971536df0712031533q3d27743ak88c7e4b3f22657f6@mail.gmail.com>

Try appending another comma:

strsplit(paste("a,,b,", ",", sep = ""), ",")


On Dec 3, 2007 6:22 PM, dankelley <Dan.Kelley at dal.ca> wrote:
>
> I have a comma-separated data file in which trailing commas sometimes occur.
> I am using strsplit to extract the data from this file, and it seems great
> except in cases with trailing comma characters.
>
> The example below illustrates.  What I'd like is to get a fourth element in
> the answer, being an empty string just like the second element.  Is there a
> way I can express my patter (or perhaps specify perl or extended) to get
> that?
>
>
> > strsplit("a,,b,", ",")[[1]]
> [1] "a" ""  "b"
>
>
>
> --
> View this message in context: http://www.nabble.com/strsplit-on-comma%2C-with-a-trailing-comma-in-input-tf4940023.html#a14141151
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Tue Dec  4 00:44:28 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 04 Dec 2007 00:44:28 +0100
Subject: [R] strsplit on comma, with a trailing comma in input
In-Reply-To: <14141151.post@talk.nabble.com>
References: <14141151.post@talk.nabble.com>
Message-ID: <475494DC.7030504@biostat.ku.dk>

dankelley wrote:
> I have a comma-separated data file in which trailing commas sometimes occur. 
> I am using strsplit to extract the data from this file, and it seems great
> except in cases with trailing comma characters.
>
> The example below illustrates.  What I'd like is to get a fourth element in
> the answer, being an empty string just like the second element.  Is there a
> way I can express my patter (or perhaps specify perl or extended) to get
> that?
>
>
>   
>> strsplit("a,,b,", ",")[[1]]
>>     
> [1] "a" ""  "b"
>   
Hmm, I don't think strsplit can do that. However:

 > scan(textConnection("a,,b,"), sep=",", what="")
Read 4 items
[1] "a" ""  "b" ""

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Dan.Kelley at dal.ca  Tue Dec  4 01:19:57 2007
From: Dan.Kelley at dal.ca (dankelley)
Date: Mon, 3 Dec 2007 16:19:57 -0800 (PST)
Subject: [R] strsplit on comma, with a trailing comma in input
In-Reply-To: <475494DC.7030504@biostat.ku.dk>
References: <14141151.post@talk.nabble.com> <475494DC.7030504@biostat.ku.dk>
Message-ID: <14142103.post@talk.nabble.com>


This works perfectly.  Thanks!


Peter Dalgaard wrote:
> 
>>   
> Hmm, I don't think strsplit can do that. However:
> 
>  > scan(textConnection("a,,b,"), sep=",", what="")
> Read 4 items
> [1] "a" ""  "b" ""
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> 35327907
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/strsplit-on-comma%2C-with-a-trailing-comma-in-input-tf4940023.html#a14142103
Sent from the R help mailing list archive at Nabble.com.


From rkozar at interia.pl  Tue Dec  4 01:24:41 2007
From: rkozar at interia.pl (threshold)
Date: Mon, 3 Dec 2007 16:24:41 -0800 (PST)
Subject: [R]  3D array
Message-ID: <14142187.post@talk.nabble.com>


Hi, I deal with 3D array say:

, , 1

     [,1] [,2] [,3] [,4] [,5]
[1,]    1    4    7   10   13
[2,]    2    5    8   11   14
[3,]    3    6    9   12   15

, , 2

     [,1] [,2] [,3] [,4] [,5]
[1,]   16   19   22   25   28
[2,]   17   20   23   26   29
[3,]   18   21   24   27   30

, , 3

     [,1] [,2] [,3] [,4] [,5]
[1,]   31   34   37   40   43
[2,]   32   35   38   41   44
[3,]   33   36   39   42   45

I want to calculate means in the cells with respect to dimention 3rd so for
example (1+16+31)/3 or (15+30+45)/3  AND avoid taking zeros into account.

I did it with a loop but I guess there is the other more straightforward
method. will be very grateful for help...

best, rob
-- 
View this message in context: http://www.nabble.com/3D-array-tf4940356.html#a14142187
Sent from the R help mailing list archive at Nabble.com.


From m_olshansky at yahoo.com  Tue Dec  4 01:34:48 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Mon, 3 Dec 2007 16:34:48 -0800 (PST)
Subject: [R] Efficient computation of average covariance matrix over a
	list
In-Reply-To: <c3cb73d50712031302m3cf75621q89476c5d25798065@mail.gmail.com>
Message-ID: <408311.78896.qm@web32204.mail.mud.yahoo.com>

I believe that computing covariance matrices takes
much more time than computing their average and so it
does not matter how you do this, but one possibility
is:

z<- lcov[[1]]*0
y <- sapply(lcov,function(x) {z<<-z+x;0;})
y <- y/length(lcov)

--- Rick DeShon <deshon at msu.edu> wrote:

> Hi All.
> 
> I would like to compute a separate covariance matrix
> for a set of
> variables for each of the levels of a factor and
> then compute the
> average covariance matrix over the factor levels.  I
> can loop through
> this computation but I need to perform the
> calculation for a large
> number of levels and am looking for something more
> elegant.  To be
> concrete....
> 
> u    <- 3
> n    <- 10
> 
> x    <- rnorm((id*u))
> y    <- rnorm((id*u))
> z    <- rnorm((id*u))
> id   <- gl(u,n)
> 
> df   <- data.frame(id,x,y,z)
> df.s <- split(xxx,id)
> 
> lcov <- lapply(df.s,cov)
> lcov
> 
> What's an efficient way to compute the average
> covariance matrix over
> the list members in "lcov"?
> 
> Thanks in advance,
> 
> Rick DeShon
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From bcarvalh at jhsph.edu  Tue Dec  4 01:35:53 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 3 Dec 2007 19:35:53 -0500
Subject: [R] 3D array
In-Reply-To: <14142187.post@talk.nabble.com>
References: <14142187.post@talk.nabble.com>
Message-ID: <7C3691D4-4C73-477C-92F6-5DF6A3FDA8C8@jhsph.edu>

say your array is called "no.named.array".

then:

no.named.array[no.named.array == 0] <- NA
rowMeans(no.named.array, dims=2, na.rm=T)

b

On Dec 3, 2007, at 7:24 PM, threshold wrote:

>
> Hi, I deal with 3D array say:
>
> , , 1
>
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    1    4    7   10   13
> [2,]    2    5    8   11   14
> [3,]    3    6    9   12   15
>
> , , 2
>
>     [,1] [,2] [,3] [,4] [,5]
> [1,]   16   19   22   25   28
> [2,]   17   20   23   26   29
> [3,]   18   21   24   27   30
>
> , , 3
>
>     [,1] [,2] [,3] [,4] [,5]
> [1,]   31   34   37   40   43
> [2,]   32   35   38   41   44
> [3,]   33   36   39   42   45
>
> I want to calculate means in the cells with respect to dimention 3rd  
> so for
> example (1+16+31)/3 or (15+30+45)/3  AND avoid taking zeros into  
> account.
>
> I did it with a loop but I guess there is the other more  
> straightforward
> method. will be very grateful for help...
>
> best, rob
> -- 
> View this message in context: http://www.nabble.com/3D-array-tf4940356.html#a14142187
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From lsmithingm at gmail.com  Tue Dec  4 02:41:13 2007
From: lsmithingm at gmail.com (Linda Smith)
Date: Mon, 3 Dec 2007 17:41:13 -0800
Subject: [R] color palette from red to blue passing white
Message-ID: <f22a33d30712031741i58c3b9a3hf4dd02e04e1bc4db@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/c90a8fc0/attachment.pl 

From jholtman at gmail.com  Tue Dec  4 02:52:58 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 3 Dec 2007 17:52:58 -0800
Subject: [R] color palette from red to blue passing white
In-Reply-To: <f22a33d30712031741i58c3b9a3hf4dd02e04e1bc4db@mail.gmail.com>
References: <f22a33d30712031741i58c3b9a3hf4dd02e04e1bc4db@mail.gmail.com>
Message-ID: <644e1f320712031752k27ac3b14l9b5a4e6e075b799e@mail.gmail.com>

see if this is what you need:

require(lattice)
x <- matrix(1:100,10)
levelplot(x,col.regions=colorRampPalette(c('dark red','white','dark blue')))



On Dec 3, 2007 5:41 PM, Linda Smith <lsmithingm at gmail.com> wrote:
> Hi All,
>
> I am looking for a color palette like this:
> http://www.ncl.ucar.edu/Applications/Images/h_long_5_lg.png
>
> I think I found out how some time ago (something like Colors[1:n]), but when
> I now wanna use it, I could not remember how I did it.
>
> Does anyone know which package I could use?
>
> Many thanks!
>
> Linda
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From engrav at u.washington.edu  Tue Dec  4 02:56:49 2007
From: engrav at u.washington.edu (Loren Engrav)
Date: Mon, 03 Dec 2007 17:56:49 -0800
Subject: [R] Junk or not Junk
Message-ID: <C379F3E1.15549%engrav@u.washington.edu>

So a message from 

Benilton Carvalho <bcarvalh at jhsph.edu> (sent by
r-help-bounces at r-project.org)

 arrives and goes in the Junk Mail even tho I have set @r-project.org to not
be junk

Why does this go in Junk mail if @r-project.org is defined as not junk?


-- 
If we knew what we were doing, it would not be called research.
Einstein


From murdoch at stats.uwo.ca  Tue Dec  4 04:10:18 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 03 Dec 2007 22:10:18 -0500
Subject: [R] Junk or not Junk
In-Reply-To: <C379F3E1.15549%engrav@u.washington.edu>
References: <C379F3E1.15549%engrav@u.washington.edu>
Message-ID: <4754C51A.4010109@stats.uwo.ca>

On 03/12/2007 8:56 PM, Loren Engrav wrote:
> So a message from 
> 
> Benilton Carvalho <bcarvalh at jhsph.edu> (sent by
> r-help-bounces at r-project.org)
> 
>  arrives and goes in the Junk Mail even tho I have set @r-project.org to not
> be junk
> 
> Why does this go in Junk mail if @r-project.org is defined as not junk?

Why are you asking us about how you have your mail filters set up?

If you didn't set them up yourself, you should find out from your local 
admin who did, and ask them.

Duncan Murdoch


From Achim.Zeileis at wu-wien.ac.at  Tue Dec  4 05:08:51 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 4 Dec 2007 05:08:51 +0100 (CET)
Subject: [R] color palette from red to blue passing white
In-Reply-To: <644e1f320712031752k27ac3b14l9b5a4e6e075b799e@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0712040504230.6948-100000@disco.wu-wien.ac.at>



On Mon, 3 Dec 2007, jim holtman wrote:

> see if this is what you need:
>
> require(lattice)
> x <- matrix(1:100,10)
> levelplot(x,col.regions=colorRampPalette(c('dark red','white','dark blue')))

Instead of colorRampPalette(), you could also use diverge_hcl() in package
"vcd" to get a perceptually-based version, e.g.,

  levelplot(x, col.regions = diverg_hcl(16))


>
>
> On Dec 3, 2007 5:41 PM, Linda Smith <lsmithingm at gmail.com> wrote:
> > Hi All,
> >
> > I am looking for a color palette like this:
> > http://www.ncl.ucar.edu/Applications/Images/h_long_5_lg.png
> >
> > I think I found out how some time ago (something like Colors[1:n]), but when
> > I now wanna use it, I could not remember how I did it.
> >
> > Does anyone know which package I could use?
> >
> > Many thanks!
> >
> > Linda
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From jones3745 at verizon.net  Tue Dec  4 06:51:23 2007
From: jones3745 at verizon.net (Thomas L Jones, PhD)
Date: Tue, 4 Dec 2007 00:51:23 -0500
Subject: [R] Problem with a global variable
Message-ID: <000301c83639$b619f790$2f01a8c0@dell2400>

From: Thomas Jones

I have several user-defined functions. As is standard practice, I am 
defining a logical vector named idebug in order to control debugging 
printouts. For example, if idebug [1] has the value TRUE, such-and-such 
debugging printouts are enabled. After the function works, some or all of 
the debugging printouts can be inhibited. idebug is a global variable; 
otherwise, it would have to be moved around with function arguments in a way 
which is clunky and hard to get right. However, I am getting an error 
message. This message was emitted during the loading of the program and is 
NOT inside any function.

Or perhaps idebug wants a special slot on the search path.

-------------------------------------------------------------------

> debug_l <- 5 # length of debug vector
> idebug <<- logical (5)
> idebug [1] <<- TRUE
Error in idebug[1] <<- TRUE : object "idebug" not found
>
[snip]
---------------------------------------------------------------

Your advice?

Tom Jones


From ggrothendieck at gmail.com  Tue Dec  4 06:59:38 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 4 Dec 2007 00:59:38 -0500
Subject: [R] Problem with a global variable
In-Reply-To: <000301c83639$b619f790$2f01a8c0@dell2400>
References: <000301c83639$b619f790$2f01a8c0@dell2400>
Message-ID: <971536df0712032159l7cde0044hbb67ec079305bf75@mail.gmail.com>

You may wish to use the options command.  You can create
your own options, not just use pre-existing ones.

Please read the last line of every message to r-help.

On Dec 4, 2007 12:51 AM, Thomas L Jones, PhD <jones3745 at verizon.net> wrote:
> From: Thomas Jones
>
> I have several user-defined functions. As is standard practice, I am
> defining a logical vector named idebug in order to control debugging
> printouts. For example, if idebug [1] has the value TRUE, such-and-such
> debugging printouts are enabled. After the function works, some or all of
> the debugging printouts can be inhibited. idebug is a global variable;
> otherwise, it would have to be moved around with function arguments in a way
> which is clunky and hard to get right. However, I am getting an error
> message. This message was emitted during the loading of the program and is
> NOT inside any function.
>
> Or perhaps idebug wants a special slot on the search path.
>
> -------------------------------------------------------------------
>
> > debug_l <- 5 # length of debug vector
> > idebug <<- logical (5)
> > idebug [1] <<- TRUE
> Error in idebug[1] <<- TRUE : object "idebug" not found
> >
> [snip]
> ---------------------------------------------------------------
>
> Your advice?
>
> Tom Jones
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From zhaozongting at gmail.com  Tue Dec  4 08:09:54 2007
From: zhaozongting at gmail.com (Hayes)
Date: Mon, 3 Dec 2007 23:09:54 -0800 (PST)
Subject: [R]  How can I use the rho value in the cor.test() summary?
Message-ID: <14145737.post@talk.nabble.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/478f42b9/attachment.pl 

From zhaozongting at gmail.com  Tue Dec  4 08:10:44 2007
From: zhaozongting at gmail.com (Hayes)
Date: Mon, 3 Dec 2007 23:10:44 -0800 (PST)
Subject: [R]  How can I use the rho value in the cor.test() summary?
Message-ID: <14145737.post@talk.nabble.com>


I want to give the "rho" value below to another variable.How ?

> Spearman's rank correlation rho
>
>
>
> data:  a[, 3] and a[, 2]
>
> S = 22, p-value = 0.001174
>
> alternative hypothesis: true rho is not equal to 0
>
> sample estimates:
>
>       rho
>
> 0.8666667
-- 
View this message in context: http://www.nabble.com/How-can-I-use-the-rho-value-in-the-cor.test%28%29-summary--tf4941501.html#a14145737
Sent from the R help mailing list archive at Nabble.com.


From daniel at umd.edu  Tue Dec  4 09:10:02 2007
From: daniel at umd.edu (Daniel Malter)
Date: Tue, 4 Dec 2007 03:10:02 -0500
Subject: [R] How can I use the rho value in the cor.test() summary?
In-Reply-To: <14145737.post@talk.nabble.com>
Message-ID: <200712040809.DCR07076@md0.mail.umd.edu>

x=c(1,2,3,4,5,6,7,8,9)
y=c(3,5,4,6,7,8,8,7,10)

z=cor.test(x,y)

othervariable=z$p.value

Cheers,
Daniel 

-------------------------
cuncta stricte discussurus
-------------------------

-----Urspr?ngliche Nachricht-----
Von: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Im
Auftrag von Hayes
Gesendet: Tuesday, December 04, 2007 2:11 AM
An: r-help at r-project.org
Betreff: [R] How can I use the rho value in the cor.test() summary?


I want to give the "rho" value below to another variable.How ?

> Spearman's rank correlation rho
>
>
>
> data:  a[, 3] and a[, 2]
>
> S = 22, p-value = 0.001174
>
> alternative hypothesis: true rho is not equal to 0
>
> sample estimates:
>
>       rho
>
> 0.8666667
--
View this message in context:
http://www.nabble.com/How-can-I-use-the-rho-value-in-the-cor.test%28%29-summ
ary--tf4941501.html#a14145737
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From shubhak at ambaresearch.com  Tue Dec  4 09:19:09 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Tue, 4 Dec 2007 13:49:09 +0530
Subject: [R] FW:  Rolling Correlations
Message-ID: <A36876D3F8A5734FA84A4338135E7CC302D7446C@BAN-MAILSRV03.Amba.com>


Hi,

Any help on this please....

BR, Shubha
-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Shubha Vishwanath Karanth
Sent: Friday, November 30, 2007 6:09 PM
To: r-help at stat.math.ethz.ch
Cc: shubhakaranth at gmail.com
Subject: [R] Rolling Correlations

Hi R,

 

I want to do some rolling correlations. But before, I searched for
"?rollingCorrelation" and tried the example in it. But I was not
successful. What could be the problem? Here is the code I tried:

 

> library(zoo)

> library(PerformanceAnalytics)

> rollingCorrelation(manager.ts at Data[,1],edhec.ts at Data,n=12)

Error in inherits(object, "zoo") : object "manager.ts" not found

 

 

BR, Shubha


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Tue Dec  4 09:31:19 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 4 Dec 2007 03:31:19 -0500
Subject: [R] Rolling Correlations
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC302D2F11D@BAN-MAILSRV03.Amba.com>
References: <AcgzTe2RrswZgtfhS9WtEo15eGUqCg==>
	<A36876D3F8A5734FA84A4338135E7CC302D2F11D@BAN-MAILSRV03.Amba.com>
Message-ID: <971536df0712040031p768bee9bmd5219f87d8b923c5@mail.gmail.com>

If you want to use just zoo alone then its done like this.  See
?rollapply and the two zoo vignettes.

> library(zoo)
> set.seed(1)
> z <- zoo(cbind(a = rnorm(25), b = 1:25))
> rollapply(z, 5, function(x) cor(x[,1], x[,2]), by.column = FALSE)
          3           4           5           6           7           8
 0.54680491 -0.13300909  0.03576759 -0.28297639  0.52062842  0.26441877
          9          10          11          12          13          14
 0.24494114  0.05774863 -0.32294217 -0.68658780 -0.35636555  0.11012008
         15          16          17          18          19          20
 0.43888843  0.61623274  0.10862581  0.71947286  0.60338376 -0.25653111
         21          22          23
-0.61338395 -0.79316968 -0.44360374


On Nov 30, 2007 7:38 AM, Shubha Vishwanath Karanth
<shubhak at ambaresearch.com> wrote:
> Hi R,
>
>
>
> I want to do some rolling correlations. But before, I searched for
> "?rollingCorrelation" and tried the example in it. But I was not
> successful. What could be the problem? Here is the code I tried:
>
>
>
> > library(zoo)
>
> > library(PerformanceAnalytics)
>
> > rollingCorrelation(manager.ts at Data[,1],edhec.ts at Data,n=12)
>
> Error in inherits(object, "zoo") : object "manager.ts" not found
>
>
>
>
>
> BR, Shubha
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Tue Dec  4 09:33:27 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 04 Dec 2007 09:33:27 +0100
Subject: [R] How can I use the rho value in the cor.test() summary?
In-Reply-To: <200712040809.DCR07076@md0.mail.umd.edu>
References: <200712040809.DCR07076@md0.mail.umd.edu>
Message-ID: <475510D7.2010103@biostat.ku.dk>

Daniel Malter wrote:
> x=c(1,2,3,4,5,6,7,8,9)
> y=c(3,5,4,6,7,8,8,7,10)
>
> z=cor.test(x,y)
>
> othervariable=z$p.value
>   
z$statistic, more likely.

> Cheers,
> Daniel 
>
> -------------------------
> cuncta stricte discussurus
> -------------------------
>
> -----Urspr?ngliche Nachricht-----
> Von: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Im
> Auftrag von Hayes
> Gesendet: Tuesday, December 04, 2007 2:11 AM
> An: r-help at r-project.org
> Betreff: [R] How can I use the rho value in the cor.test() summary?
>
>
> I want to give the "rho" value below to another variable.How ?
>
>   
>> Spearman's rank correlation rho
>>
>>
>>
>> data:  a[, 3] and a[, 2]
>>
>> S = 22, p-value = 0.001174
>>
>> alternative hypothesis: true rho is not equal to 0
>>
>> sample estimates:
>>
>>       rho
>>
>> 0.8666667
>>     
> --
> View this message in context:
> http://www.nabble.com/How-can-I-use-the-rho-value-in-the-cor.test%28%29-summ
> ary--tf4941501.html#a14145737
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p_connolly at slingshot.co.nz  Tue Dec  4 09:35:34 2007
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Tue, 4 Dec 2007 21:35:34 +1300
Subject: [R] Rating R Helpers
In-Reply-To: <s7533dcf.025@tedmail.lgc.co.uk>
References: <s7533dcf.025@tedmail.lgc.co.uk>
Message-ID: <20071204083534.GH6584@slingshot.co.nz>

On Sun, 02-Dec-2007 at 11:20PM +0000, S Ellison wrote:

|> Package review is a nice idea. But you raise a worrying point.
|> Are any of the 'downright dangerous' packages on CRAN?

Don't know about "dangerous", but I would like the opportunity to
provide feedback or to have seen feedback from others when I
contemplating using a package I see on CRAN.  This is particularly
true for package maintainers who seem to have vanished.

Such a "rating" would have a natural place.  I don't see where ratings
for helpers would exist in cyberspace for enquirers to the list.  A
compulsory part of everyone's email signatures?  Don't like that idea.


-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From maechler at stat.math.ethz.ch  Tue Dec  4 09:57:02 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 4 Dec 2007 09:57:02 +0100
Subject: [R] color palette from red to blue passing white
In-Reply-To: <Pine.LNX.4.44.0712040504230.6948-100000@disco.wu-wien.ac.at>
References: <644e1f320712031752k27ac3b14l9b5a4e6e075b799e@mail.gmail.com>
	<Pine.LNX.4.44.0712040504230.6948-100000@disco.wu-wien.ac.at>
Message-ID: <18261.5726.555747.583528@stat.math.ethz.ch>

>>>>> "AZ" == Achim Zeileis <Achim.Zeileis at wu-wien.ac.at>
>>>>>     on Tue, 4 Dec 2007 05:08:51 +0100 (CET) writes:

    AZ> On Mon, 3 Dec 2007, jim holtman wrote:

    >> see if this is what you need:
    >> 
    >> require(lattice) x <- matrix(1:100,10)
    >> levelplot(x,col.regions=colorRampPalette(c('dark
    >> red','white','dark blue')))

    AZ> Instead of colorRampPalette(), you could also use
    AZ> diverge_hcl() in package "vcd" to get a
    AZ> perceptually-based version, e.g.,

    AZ>   levelplot(x, col.regions = diverg_hcl(16))

Hmm,  I would have recommended

  colorRampPalette(c('dark red','white','dark blue'), 
                   space = "Lab")

where the 'space = "Lab"' part also makes sure that a
"perceptually-based" space rather than RGB is used.

I think the functions colorRamp() and (even more)
colorRampPalette()  are very nice, part of "standard R" and 
still not known and used enough.
Note that they are based on 'convertColor()' and other color
space functionality in R all of which deserve more usage 
in my oppinion and also in my own code ! ;-) 

Package 'vcd' (and others) use package 'colorspace', 
and I have wondered in the past if these color space computations
should not be merged into to standard R (package 'grDevices').
But that's really a topic for another thread, on R-devel, not R-help..

Martin Maechler, ETH Zurich


    >> On Dec 3, 2007 5:41 PM, Linda Smith
    >> <lsmithingm at gmail.com> wrote: > Hi All,
    >> >
    >> > I am looking for a color palette like this: >
    >> http://www.ncl.ucar.edu/Applications/Images/h_long_5_lg.png
    >> >
    >> > I think I found out how some time ago (something like
    >> Colors[1:n]), but when > I now wanna use it, I could not
    >> remember how I did it.
    >> >
    >> > Does anyone know which package I could use?
    >> >
    >> > Many thanks!
    >> >
    >> > Linda


From jim at bitwrit.com.au  Tue Dec  4 10:47:33 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 04 Dec 2007 20:47:33 +1100
Subject: [R] overlapping labels
In-Reply-To: <7124EE4D7E556844A654A607A4203FDD3ED3CD@scomp0038.wurnet.nl>
References: <7124EE4D7E556844A654A607A4203FDD3ED3CD@scomp0038.wurnet.nl>
Message-ID: <47552235.4090009@bitwrit.com.au>

Biscarini, Filippo wrote:
> Good evening,
>  
> I am trying to add labels to the point of a simple plot, using the
> text() function; the problem is that sometimes, if two points are too
> close to each other, labels overlap and are no longer readable.
> I was wondering whether there are options that I can use to prevent this
> overlapping (by, for example, placing labels alternatively above and
> below the plotted curve), or whether I should use another set of
> graphical functions (grid or lattice packages).
> Does anybody have suggestions?
>  
Hi Filippo,
Have a look at thigmophobe.labels in the plotrix package.

Jim


From r at stepputtis.net  Tue Dec  4 10:47:06 2007
From: r at stepputtis.net (Daniel Stepputtis)
Date: Tue, 04 Dec 2007 10:47:06 +0100
Subject: [R] comparison of two vectors
In-Reply-To: <14140387.post@talk.nabble.com>
References: <14129032.post@talk.nabble.com>
	<14129523.post@talk.nabble.com>	<14130713.post@talk.nabble.com>
	<14140387.post@talk.nabble.com>
Message-ID: <4755221A.8010700@stepputtis.net>

Dear Ben,
I was searching for the same problem. Thank you very much, it helped me a lot and I will use it quite often!

In addition to the problem given by tintin_et_milou. I have to compare a two pairs of vectors.

I.e. I have two datasets each with latitude and longitude (which defines the geographical position of data points.)
As you might imagine, it is meaningful to take into account both latitude and longitude, when searching for the nearest data point from the otehr dataset.

Do you have any idea how to do this efficiently (actually I used loops ;-( ?
Best regards
Daniel


Ben Bolker schrieb:
> 
> distfun <- function(x1,x2) { (x1-x2)^2 }
> 
> outer(m1[,1],y,distfun)

>   cheers
>     Ben


From jim at bitwrit.com.au  Tue Dec  4 11:12:13 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 04 Dec 2007 21:12:13 +1100
Subject: [R] color palette from red to blue passing white
In-Reply-To: <f22a33d30712031741i58c3b9a3hf4dd02e04e1bc4db@mail.gmail.com>
References: <f22a33d30712031741i58c3b9a3hf4dd02e04e1bc4db@mail.gmail.com>
Message-ID: <475527FD.2050500@bitwrit.com.au>

Linda Smith wrote:
> Hi All,
> 
> I am looking for a color palette like this:
> http://www.ncl.ucar.edu/Applications/Images/h_long_5_lg.png
> 
> I think I found out how some time ago (something like Colors[1:n]), but when
> I now wanna use it, I could not remember how I did it.
> 
> Does anyone know which package I could use?
> 
Hi Linda,
You can do it with color.legend in plotrix.

plot(1:10)
color.legend(1,8,7,8.5,align="rb",
rect.col=color.scale(1:21,c(0,1,0.5),c(0,1,0),c(0.5,1,0)),
legend=seq(-50,50,by=10))

Jim


From florencio.gonzalez at GI.IEO.ES  Tue Dec  4 12:06:56 2007
From: florencio.gonzalez at GI.IEO.ES (=?iso-8859-1?Q?Florencio_Gonz=E1lez?=)
Date: Tue, 4 Dec 2007 12:06:56 +0100 
Subject: [R] confidence intervals for y predicted in non linear regression
Message-ID: <9BD63C7EDBFC234DB982DF636397395E06C957@oceanoastur.gi.ieo.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/696d8888/attachment.pl 

From murdoch at stats.uwo.ca  Tue Dec  4 12:37:38 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 04 Dec 2007 06:37:38 -0500
Subject: [R] Problem with a global variable
In-Reply-To: <000301c83639$b619f790$2f01a8c0@dell2400>
References: <000301c83639$b619f790$2f01a8c0@dell2400>
Message-ID: <47553C02.5000200@stats.uwo.ca>

On 04/12/2007 12:51 AM, Thomas L Jones, PhD wrote:
> From: Thomas Jones
> 
> I have several user-defined functions. As is standard practice, I am 
> defining a logical vector named idebug in order to control debugging 
> printouts. For example, if idebug [1] has the value TRUE, such-and-such 
> debugging printouts are enabled. After the function works, some or all of 
> the debugging printouts can be inhibited. idebug is a global variable; 
> otherwise, it would have to be moved around with function arguments in a way 
> which is clunky and hard to get right. However, I am getting an error 
> message. This message was emitted during the loading of the program and is 
> NOT inside any function.
> 
> Or perhaps idebug wants a special slot on the search path.
> 
> -------------------------------------------------------------------
> 
>> debug_l <- 5 # length of debug vector
>> idebug <<- logical (5)
>> idebug [1] <<- TRUE
> Error in idebug[1] <<- TRUE : object "idebug" not found
> [snip]
> ---------------------------------------------------------------
> 
> Your advice?

Don't use <<- at the top level.  Use regular assignment, and things 
should be fine.

What is happening in your case is this:

1.  idebug <<- logical(5)

looks for idebug in the parents of the global environment, fails to find 
it, and creates a new one in the global environment.


2.  idebug[1] <<- TRUE

looks for idebug in the parents again, but this time it needs to find 
it, because you are trying to index it.  That's why you get an error on 
the second call.

Duncan Murdoch


From Luisr at frs.fo  Tue Dec  4 12:40:58 2007
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Tue, 04 Dec 2007 11:40:58 +0000
Subject: [R] reduce the code used by "for"
Message-ID: <s7553cd2.093@ffdata.setur.fo>

R-help,

I have a 3-way array:

> dim(bugvinP)
[1] 13 14  3

The array looks something like this (object trimmed for readability)
, , slag = 1

    ar
      1994  1995  1996  1997  1998
  1     NA 0.000 0.000 0.000 0.000
  2  0.036 0.059 0.027 0.000 0.000
  3  0.276 0.475 0.491 0.510 0.559
  10 1.000 1.000 1.000 1.000 1.000
  11    NA 1.000 1.000    NA 1.000
  12    NA 1.000 1.000 1.000 1.000
  13    NA 1.000    NA 1.000    NA

, , slag = 2

    ar
      1994  1995  1996  1997  1998
  1     NA 0.000 0.000 0.000 0.000
  2  0.129 0.029 0.011 0.026 0.000
  9  1.000 1.000 1.000 1.000 1.000
  10 1.000 1.000 1.000 1.000 1.000
  11 1.000 1.000 1.000 1.000 1.000
  12 1.000 1.000 1.000 1.000 1.000
  13    NA    NA 1.000 1.000    NA


I want to set NAs to 0 if the row names are e.g. 3 
and 1 otherwise. To implement this I do the following which
is OK but I wish to find out a more compact/shorter version.

for(i in 1:3)
{
bugvinP[,,i] <- ifelse(as.numeric(rownames(bugvinP[,,i])) < 3 &
is.na(bugvinP[,,i]), 0
, ifelse(as.numeric(rownames(bugvinP[,,i])) > 9 & is.na(bugvinP[,,i]),
1, bugvinP))
}


Thanks in advance

> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          6.1                         
year           2007                        
month          11                          
day            26                          
svn rev        43537                       
language       R                           
version.string R version 2.6.1 (2007-11-26)
>


From vistocco at unicas.it  Tue Dec  4 12:48:20 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Tue, 04 Dec 2007 12:48:20 +0100
Subject: [R] ggplot2: Choosing colours
In-Reply-To: <2E9C414912813E4EB981326983E0A10403F40E45@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A10403F40E45@inboexch.inbo.be>
Message-ID: <47553E84.1020708@unicas.it>

qplot(data=dataset, x, y, colour=z)

ONKELINX, Thierry wrote:
> Dear useRs,
>
> I'm trying to specify the colour of a factor with ggplot2. The example
> below gets me close to what I want, but it's missing a legend.
>
> Any ideas?
>
> Thanks,
>
> Thierry
>
> library(ggplot2)
> dataset <- data.frame(x = rnorm(40), y = runif(40), z = gl(4, 10, labels
> = LETTERS[1:4]))
> ggplot(data = dataset, aes(x = x, y = y, group = z)) + geom_point(colour
> = c("red", "green", "blue", "black"))
>
>
> ------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium 
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be 
> www.inbo.be 
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From manme_olv at yahoo.es  Tue Dec  4 13:09:53 2007
From: manme_olv at yahoo.es (Maria del Carmen olvera)
Date: Tue, 4 Dec 2007 12:09:53 +0000 (GMT)
Subject: [R]  library(RII) and smoothing parameter
Message-ID: <607427.62068.qm@web27813.mail.ukl.yahoo.com>

Se ha borrado un texto insertado con un juego de caracteres sin especificar...
Nombre: no disponible
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/83d596f3/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Dec  4 13:27:20 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 4 Dec 2007 12:27:20 +0000 (GMT)
Subject: [R] confidence intervals for y predicted in non linear
	regression
In-Reply-To: <9BD63C7EDBFC234DB982DF636397395E06C957@oceanoastur.gi.ieo.es>
References: <9BD63C7EDBFC234DB982DF636397395E06C957@oceanoastur.gi.ieo.es>
Message-ID: <Pine.LNX.4.64.0712041216120.8012@gannet.stats.ox.ac.uk>

On Tue, 4 Dec 2007, Florencio Gonz?lez wrote:

>
> Hi, I?m trying to plot a nonlinear regresion with the confidence bands for
> the curve obtained, similar to what nlintool or nlpredci functions in Matlab
> does, but I no figure how to. In nls the option is there but not implemented
> yet.
>
> Is there a plan to implement the in a relative near future?

No (and it is confidence intervals not confidence bands that are 
unimplemented, and have been for years).

Doing what you ask (confidence bands) is very tricky indeed to do 
accurately (and is not what nlpredci says it does), and even accurate 
pointwise confidence intervals are problematic.  See MASS4 sections 
8.4/5 and its on-line complements for some details about why even 
confidence intervals for parameters are frequently tricky in non-linear 
models.

You could use resampling methods: see the reference in the previous 
paragraph.

> Thanks in advance, Florencio
>
> 	[[alternative HTML version deleted]]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From oasilkan at gmail.com  Tue Dec  4 13:28:18 2007
From: oasilkan at gmail.com (Ozcan Asilkan)
Date: Tue, 4 Dec 2007 13:28:18 +0100
Subject: [R] Best forecasting methods with Time Series ?
Message-ID: <934356850712040428nb6e4879o48f5d649df0747f4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/e23f28ab/attachment.pl 

From sbartell at uci.edu  Tue Dec  4 14:00:54 2007
From: sbartell at uci.edu (Scott Bartell)
Date: Tue, 4 Dec 2007 05:00:54 -0800
Subject: [R] weighted Cox proportional hazards regression
Message-ID: <fb240dd50712040500h6e28785fh24ce1933955f71d8@mail.gmail.com>

I'm getting unexpected results from the coxph function when using
weights from counter-matching.  For example, the following code
produces a parameter estimate of -1.59 where I expect 0.63:

d2 = structure(list(x = c(1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
1, 0, 0, 1, 0, 1, 0, 1, 0, 1), wt = c(5, 42, 40, 4, 43, 4, 42,
4, 44, 5, 38, 4, 39, 4, 4, 37, 40, 4, 44, 5, 45, 5, 44, 5), riskset = c(1L,
1L, 4L, 4L, 6L, 6L, 12L, 12L, 13L, 13L, 19L, 19L, 23L, 23L, 31L,
31L, 42L, 42L, 45L, 45L, 70L, 70L, 93L, 93L), cc = c(1, 0, 1,
0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0
), pseudotime = rep(1,24)), .Names = c("x", "wt", "riskset",
"cc", "pseudotime"), class = "data.frame", row.names=1:24)

coxph( Surv(pseudotime, cc) ~ x + strata(riskset), weights=wt,
robust=T, method="breslow",data=d2)

I'm expecting a value of about 0.63 to 0.64 based on the data source
(simulated) and the following hand-coded MLE:

negloglik = function(beta,dat) {
  dat$wexb = dat$wt * exp(dat$x * beta)
  agged = aggregate(dat$wexb,list(riskset=dat$riskset),sum)
  names(agged)[2] = "denom"
  dat = merge(dat[dat$cc==1,],agged,by="riskset")
  -sum(log(dat$wexb)-log(dat$denom))
  }
nlm(negloglik,0,hessian=T,dat=d2)

Am I misunderstanding the meaning of case weights in the coxph
function?  The help file is pretty terse.

Scott Bartell, PhD
Assistant Professor
Department of Epidemiology
University of California, Irvine


From niederlein-rstat at yahoo.de  Tue Dec  4 11:46:31 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Tue, 04 Dec 2007 11:46:31 +0100
Subject: [R] Dataframe manipulation
Message-ID: <47553007.7080003@yahoo.de>

Hello,

can anybody help me with this problem?
I have a dataframe, which contains its values as factors though I have numbers 
but it was read as factors with "scan". Now I would like to convert these 
columns (multiple) to a numeric format.


# this example creates a similar situation

testdata <- as.factor(c("1.1",NA,"2.3","5.5"))
testdata2 <- as.factor(c("1.7","4.3","8.5",10.0))

df <- data.frame(testdata, testdata2)

what do I have to do to get the same datafram but with numeric values???

Antje


From ritchieblackmore72 at gmail.com  Tue Dec  4 14:57:48 2007
From: ritchieblackmore72 at gmail.com (Duccio -)
Date: Tue, 4 Dec 2007 14:57:48 +0100
Subject: [R] plotting SE
Message-ID: <eec6d4c20712040557g3de70738v683c19fa672c1ded@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/253d2e02/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Tue Dec  4 15:03:31 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 04 Dec 2007 15:03:31 +0100
Subject: [R] plotting SE
In-Reply-To: <eec6d4c20712040557g3de70738v683c19fa672c1ded@mail.gmail.com>
References: <eec6d4c20712040557g3de70738v683c19fa672c1ded@mail.gmail.com>
Message-ID: <47555E33.1070004@biostat.ku.dk>

Duccio - wrote:
> I used the lm function for a univariate regression model.
> Then I plotted it within the x/y scatterplot by abline(lm).
> I would like to plot standard error (or confidence intervals but SE should
> be better)...
> Any suggestion?
>   

help(predict.lm) has an example of this.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From scruveil at genoscope.cns.fr  Tue Dec  4 15:03:40 2007
From: scruveil at genoscope.cns.fr (=?ISO-8859-1?Q?St=E9phane_CRUVEILLER?=)
Date: Tue, 04 Dec 2007 15:03:40 +0100
Subject: [R] Multiple stacked barplots on the same graph?
Message-ID: <47555E3C.9030301@genoscope.cns.fr>

Dear R-Users,

I would like to know whether it is possible to draw several
stacked barplots (i.e. side by side on the same sheet)...


my data look like :

                            Cond1  Cond1' Cond2   Cond2'
Compartment 1    11,81    2,05    12,49    0,70   
Compartment 2     10,51    1,98    13,56    0,85
Compartment 3     1,95    0,63    2,81    0,22  
Compartment 4     2,08    0,17    3,13    0,06   
Compartment 5     2,51    0,20    4,58    0,03

ps: Cond1' values should be stacked on Cond1, Cond2' on Cond 2 and so 
on... and series 1 and 2
should be side by side for each compartement....

Thanks for any help.

St?phane.

-- 
"La science a certes quelques magnifiques r?ussites ? son actif mais
? tout prendre, je pr?f?re de loin ?tre heureux plut?t qu'avoir raison." 
D. Adams
-- 
AGC website <http://www.genoscope.cns.fr/agc>
	St?phane CRUVEILLER Ph. D.
Genoscope - Centre National de S?quencage
Atelier de G?nomique Comparative
2, Rue Gaston Cremieux CP 5706
91057 Evry Cedex - France
Phone: +33 (0)1 60 87 84 58
Fax: +33 (0)1 60 87 25 14
scruveil at genoscope.cns.fr


From dimitris.rizopoulos at med.kuleuven.be  Tue Dec  4 15:10:17 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 4 Dec 2007 15:10:17 +0100
Subject: [R] Dataframe manipulation
References: <47553007.7080003@yahoo.de>
Message-ID: <002001c8367f$65a8d8d0$0540210a@www.domain>

try this (also look at R-FAQ 7.10):

sapply(df, function (x) as.numeric(levels(x))[as.integer(x)])


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Antje" <niederlein-rstat at yahoo.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, December 04, 2007 11:46 AM
Subject: [R] Dataframe manipulation


> Hello,
>
> can anybody help me with this problem?
> I have a dataframe, which contains its values as factors though I 
> have numbers
> but it was read as factors with "scan". Now I would like to convert 
> these
> columns (multiple) to a numeric format.
>
>
> # this example creates a similar situation
>
> testdata <- as.factor(c("1.1",NA,"2.3","5.5"))
> testdata2 <- as.factor(c("1.7","4.3","8.5",10.0))
>
> df <- data.frame(testdata, testdata2)
>
> what do I have to do to get the same datafram but with numeric 
> values???
>
> Antje
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From zhaozongting at gmail.com  Tue Dec  4 02:39:39 2007
From: zhaozongting at gmail.com (=?GB2312?B?1dTX2s2l?=)
Date: Tue, 4 Dec 2007 09:39:39 +0800
Subject: [R] R-help
Message-ID: <20013690712031739r768bafebs4fbbae207c9ccb27@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/69951a03/attachment.pl 

From toxinli at hotmail.com  Tue Dec  4 05:27:47 2007
From: toxinli at hotmail.com (li xin)
Date: Mon, 3 Dec 2007 23:27:47 -0500
Subject: [R] Syntax for Trim and Fill Method
Message-ID: <BAY116-W139CC4954AC15B59313B21AE6D0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071203/0d0d3ef2/attachment.pl 

From rvaradhan at jhmi.edu  Tue Dec  4 15:40:04 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 4 Dec 2007 09:40:04 -0500
Subject: [R] 3D array
In-Reply-To: <14142187.post@talk.nabble.com>
References: <14142187.post@talk.nabble.com>
Message-ID: <000c01c83683$8f36d450$7c94100a@win.ad.jhu.edu>

The following will work:

apply(my.array, c(1,2), mean)

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of threshold
Sent: Monday, December 03, 2007 7:25 PM
To: r-help at r-project.org
Subject: [R] 3D array


Hi, I deal with 3D array say:

, , 1

     [,1] [,2] [,3] [,4] [,5]
[1,]    1    4    7   10   13
[2,]    2    5    8   11   14
[3,]    3    6    9   12   15

, , 2

     [,1] [,2] [,3] [,4] [,5]
[1,]   16   19   22   25   28
[2,]   17   20   23   26   29
[3,]   18   21   24   27   30

, , 3

     [,1] [,2] [,3] [,4] [,5]
[1,]   31   34   37   40   43
[2,]   32   35   38   41   44
[3,]   33   36   39   42   45

I want to calculate means in the cells with respect to dimention 3rd so for
example (1+16+31)/3 or (15+30+45)/3  AND avoid taking zeros into account.

I did it with a loop but I guess there is the other more straightforward
method. will be very grateful for help...

best, rob
-- 
View this message in context:
http://www.nabble.com/3D-array-tf4940356.html#a14142187
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From vistocco at unicas.it  Tue Dec  4 15:44:34 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Tue, 04 Dec 2007 15:44:34 +0100
Subject: [R] reduce the code used by "for"
In-Reply-To: <s7553cd2.093@ffdata.setur.fo>
References: <s7553cd2.093@ffdata.setur.fo>
Message-ID: <475567D2.5000708@unicas.it>

 > x=c(1,2,NA,3,4,5)*10
 > y=array(rep(x,15),c(5,3,2))
 > dimnames(y)=list(1:5,letters[1:3],NULL)

So to have in the workspace:
 > y
, , 1
   a  b  c
1 10 50 40
2 20 10 50
3 NA 20 10
4 30 NA 20
5 40 30 NA

, , 2
   a  b  c
1 30 NA 20
2 40 30 NA
3 50 40 30
4 10 50 40
5 20 10 50

Then to set the missing in the third row to 0:
 > y[3,,][which(is.na(y[3,,]))]=0

and to set the missing in the other rows to 1:
 > y[-3,,][which(is.na(y[-3,,]))]=1

domenico vistocco


Luis Ridao Cruz wrote:
> R-help,
>
> I have a 3-way array:
>
>   
>> dim(bugvinP)
>>     
> [1] 13 14  3
>
> The array looks something like this (object trimmed for readability)
> , , slag = 1
>
>     ar
>       1994  1995  1996  1997  1998
>   1     NA 0.000 0.000 0.000 0.000
>   2  0.036 0.059 0.027 0.000 0.000
>   3  0.276 0.475 0.491 0.510 0.559
>   10 1.000 1.000 1.000 1.000 1.000
>   11    NA 1.000 1.000    NA 1.000
>   12    NA 1.000 1.000 1.000 1.000
>   13    NA 1.000    NA 1.000    NA
>
> , , slag = 2
>
>     ar
>       1994  1995  1996  1997  1998
>   1     NA 0.000 0.000 0.000 0.000
>   2  0.129 0.029 0.011 0.026 0.000
>   9  1.000 1.000 1.000 1.000 1.000
>   10 1.000 1.000 1.000 1.000 1.000
>   11 1.000 1.000 1.000 1.000 1.000
>   12 1.000 1.000 1.000 1.000 1.000
>   13    NA    NA 1.000 1.000    NA
>
>
> I want to set NAs to 0 if the row names are e.g. 3 
> and 1 otherwise. To implement this I do the following which
> is OK but I wish to find out a more compact/shorter version.
>
> for(i in 1:3)
> {
> bugvinP[,,i] <- ifelse(as.numeric(rownames(bugvinP[,,i])) < 3 &
> is.na(bugvinP[,,i]), 0
> , ifelse(as.numeric(rownames(bugvinP[,,i])) > 9 & is.na(bugvinP[,,i]),
> 1, bugvinP))
> }
>
>
> Thanks in advance
>
>   
>> version
>>     
>                _                           
> platform       i386-pc-mingw32             
> arch           i386                        
> os             mingw32                     
> system         i386, mingw32               
> status                                     
> major          2                           
> minor          6.1                         
> year           2007                        
> month          11                          
> day            26                          
> svn rev        43537                       
> language       R                           
> version.string R version 2.6.1 (2007-11-26)
>   
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From petr.pikal at precheza.cz  Tue Dec  4 15:48:37 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Tue, 4 Dec 2007 15:48:37 +0100
Subject: [R] Odp:  R-help
In-Reply-To: <20013690712031739r768bafebs4fbbae207c9ccb27@mail.gmail.com>
Message-ID: <OFB6E2C61F.DA39A75C-ONC12573A7.0050C7C3-C12573A7.00514B0F@precheza.cz>

Hi

r-help-bounces at r-project.org napsal dne 04.12.2007 02:39:39:

> > I recently picked up R for econometrics modeling, and I am confronted 
with
> a
> > problem. I use cor.test() for spearman test, and want to get the "rho" 
and
> > "P-value" in the summary. Would you please tell me how to get them? 
Thank
> you very much!
> >
> >
> >
> > Here is the cor.test() summary:
> >
> > Spearman's rank correlation rho
> >
> >
> >
> > data:  a[, 3] and a[, 2]
> >
> > S = 22, p-value = 0.001174
> >
> > alternative hypothesis: true rho is not equal to 0
> >
> > sample estimates:
> >
> >       rho
> >
> > 0.8666667

See how result of cor.test() is organised by

str(your.result)

and pick up the value you want from list. See ...$estimate

Regards
Petr

> >
> >
> >
> > Best Regards,
> >
> > Hayes
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at yahoo.ca  Tue Dec  4 15:48:47 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 4 Dec 2007 09:48:47 -0500 (EST)
Subject: [R] Dataframe manipulation
In-Reply-To: <47553007.7080003@yahoo.de>
Message-ID: <16294.73463.qm@web32814.mail.mud.yahoo.com>

See  R-FAQ # 7-11 for the solution.


Have a look at
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/98227.html
for a discussion of this type of problem and ways to
get around the issue.


--- Antje <niederlein-rstat at yahoo.de> wrote:

> Hello,
> 
> can anybody help me with this problem?
> I have a dataframe, which contains its values as
> factors though I have numbers 
> but it was read as factors with "scan". Now I would
> like to convert these 
> columns (multiple) to a numeric format.
> 
> 
> # this example creates a similar situation
> 
> testdata <- as.factor(c("1.1",NA,"2.3","5.5"))
> testdata2 <- as.factor(c("1.7","4.3","8.5",10.0))
> 
> df <- data.frame(testdata, testdata2)
> 
> what do I have to do to get the same datafram but
> with numeric values???
> 
> Antje
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From P.Dalgaard at biostat.ku.dk  Tue Dec  4 15:55:43 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 04 Dec 2007 15:55:43 +0100
Subject: [R] How can I use the rho value in the cor.test() summary?
In-Reply-To: <475510D7.2010103@biostat.ku.dk>
References: <200712040809.DCR07076@md0.mail.umd.edu>
	<475510D7.2010103@biostat.ku.dk>
Message-ID: <47556A6F.80304@biostat.ku.dk>

Peter Dalgaard wrote:
> Daniel Malter wrote:
>   
>> x=c(1,2,3,4,5,6,7,8,9)
>> y=c(3,5,4,6,7,8,8,7,10)
>>
>> z=cor.test(x,y)
>>
>> othervariable=z$p.value
>>   
>>     
> z$statistic, more likely.
>
>   
z$estimate, even more likely.... (D'oh!!!)
>> Cheers,
>> Daniel 
>>
>> -------------------------
>> cuncta stricte discussurus
>> -------------------------
>>
>> -----Urspr?ngliche Nachricht-----
>> Von: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Im
>> Auftrag von Hayes
>> Gesendet: Tuesday, December 04, 2007 2:11 AM
>> An: r-help at r-project.org
>> Betreff: [R] How can I use the rho value in the cor.test() summary?
>>
>>
>> I want to give the "rho" value below to another variable.How ?
>>
>>   
>>     
>>> Spearman's rank correlation rho
>>>
>>>
>>>
>>> data:  a[, 3] and a[, 2]
>>>
>>> S = 22, p-value = 0.001174
>>>
>>> alternative hypothesis: true rho is not equal to 0
>>>
>>> sample estimates:
>>>
>>>       rho
>>>
>>> 0.8666667
>>>     
>>>       
>> --
>> View this message in context:
>> http://www.nabble.com/How-can-I-use-the-rho-value-in-the-cor.test%28%29-summ
>> ary--tf4941501.html#a14145737
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>   
>>     
>
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bady at univ-lyon1.fr  Tue Dec  4 16:07:57 2007
From: bady at univ-lyon1.fr (bady at univ-lyon1.fr)
Date: Tue, 04 Dec 2007 16:07:57 +0100
Subject: [R] confidence intervals for y predicted in non linear
	regression
In-Reply-To: <9BD63C7EDBFC234DB982DF636397395E06C957@oceanoastur.gi.ieo.es>
References: <9BD63C7EDBFC234DB982DF636397395E06C957@oceanoastur.gi.ieo.es>
Message-ID: <1196780877.47556d4d7feca@webmail.univ-lyon1.fr>

hi, hi all,

you can consult these links:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/43008.html
https://stat.ethz.ch/pipermail/r-help/2004-October/058703.html

hope this help


pierre


Selon Florencio Gonz?lez <florencio.gonzalez at GI.IEO.ES>:

>
> Hi, I?m trying to plot a nonlinear regresion with the confidence bands for
> the curve obtained, similar to what nlintool or nlpredci functions in Matlab
> does, but I no figure how to. In nls the option is there but not implemented
> yet.
>
> Is there a plan to implement the in a relative near future?
>
> Thanks in advance, Florencio
>
>
>
> La informaci?n contenida en este e-mail y sus ficheros adjuntos es totalmente
> confidencial y no deber?a ser usado si no fuera usted alguno de los
> destinatarios. Si ha recibido este e-mail por error, por favor avise al
> remitente y b?rrelo de su buz?n o de cualquier otro medio de almacenamiento.
>  This email is confidential and should not be used by anyone who is not the
> original intended  recipient. If you have received this e-mail in  error
> please inform the sender and delete it from  your mailbox or any other
> storage mechanism.
> 	[[alternative HTML version deleted]]
>
>


From vistocco at unicas.it  Tue Dec  4 16:41:29 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Tue, 04 Dec 2007 16:41:29 +0100
Subject: [R] Multiple stacked barplots on the same graph?
In-Reply-To: <47555E3C.9030301@genoscope.cns.fr>
References: <47555E3C.9030301@genoscope.cns.fr>
Message-ID: <47557529.3050806@unicas.it>

Perhaps this could be useful:
 > x=scan()
11.81 10.51  1.95  2.08  2.51  2.05  1.98  0.63  0.17  0.20
12.49 13.56 2.81  3.13  4.58  0.70  0.85  0.22  0.06  0.03

 > x=matrix(x,5,4,byrow=T)
 > rownames(x)=paste("comp",1:5,sep="")
 > colnames(x)=paste("c",1:4,sep="")

 > library(ggplot2)
 > dfm=melt(x)
 > qplot(as.factor(x=X1),y=value,geom="histogram",data=dfm,fill=X2)

domenico vistocco

St?phane CRUVEILLER wrote:
> Dear R-Users,
>
> I would like to know whether it is possible to draw several
> stacked barplots (i.e. side by side on the same sheet)...
>
>
> my data look like :
>
>                             Cond1  Cond1' Cond2   Cond2'
> Compartment 1    11,81    2,05    12,49    0,70   
> Compartment 2     10,51    1,98    13,56    0,85
> Compartment 3     1,95    0,63    2,81    0,22  
> Compartment 4     2,08    0,17    3,13    0,06   
> Compartment 5     2,51    0,20    4,58    0,03
>
> ps: Cond1' values should be stacked on Cond1, Cond2' on Cond 2 and so 
> on... and series 1 and 2
> should be side by side for each compartement....
>
> Thanks for any help.
>
> St?phane.
>
>


From dieter.menne at menne-biomed.de  Tue Dec  4 16:41:16 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 4 Dec 2007 15:41:16 +0000 (UTC)
Subject: [R] Multiple stacked barplots on the same graph?
References: <47555E3C.9030301@genoscope.cns.fr>
Message-ID: <loom.20071204T153556-637@post.gmane.org>

St?phane CRUVEILLER <scruveil <at> genoscope.cns.fr> writes:

> I would like to know whether it is possible to draw several
> stacked barplots (i.e. side by side on the same sheet)...
> 
> my data look like :
> 
>                             Cond1  Cond1' Cond2   Cond2'
> Compartment 1    11,81    2,05    12,49    0,70   
> Compartment 2     10,51    1,98    13,56    0,85
> Compartment 3     1,95    0,63    2,81    0,22  
> Compartment 4     2,08    0,17    3,13    0,06   
> Compartment 5     2,51    0,20    4,58    0,03
> 
> ps: Cond1' values should be stacked on Cond1, Cond2' on Cond 2 and so 
> on... and series 1 and 2
> should be side by side for each compartement....

Check the following example on xyplot/lattice:

barchart(yield ~ variety | site, data = barley,
         groups = year, layout = c(1,6), stack = TRUE, 
         auto.key = list(points = FALSE, rectangles = TRUE, space = "right"),
         ylab = "Barley Yield (bushels/acre)",
         scales = list(x = list(rot = 45)))

You have to reformat your data to the long format first for this, e.g. by using
reshape.

Comp  Cond  x
1     1     11.81
1     2      2.05

With allow.multiple=TRUE, it might be possible to avoid the reformatting, but I
never have used that form, because I the "long" had many advantages.

Dieter


From Serguei.Kaniovski at wifo.ac.at  Tue Dec  4 16:49:31 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Tue, 4 Dec 2007 16:49:31 +0100
Subject: [R] Inserting a subsequence between values of a vector
Message-ID: <OF1013D776.0A0AE339-ONC12573A7.0056EE4B-C12573A7.0056EE4E@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/a69df9ef/attachment.pl 

From david at davidkatzconsulting.com  Tue Dec  4 16:51:09 2007
From: david at davidkatzconsulting.com (David Katz)
Date: Tue, 4 Dec 2007 07:51:09 -0800 (PST)
Subject: [R] R-help google group archive
In-Reply-To: <51096989-98dc-44a4-b627-dc01c005ca92@s8g2000prg.googlegroups.com>
References: <51096989-98dc-44a4-b627-dc01c005ca92@s8g2000prg.googlegroups.com>
Message-ID: <14152914.post@talk.nabble.com>


Also see www.nabble.com for a very nice interface to current and archived
posts.



vince-28 wrote:
> 
> I made a google group archive of current and future R-help posts at
> http://groups.google.com/group/r-help-archive
> 
> If you are signed-up for the R-help mailing list with a gmail account
> you can post/reply through the google group pages. Note that this is
> not a separate mailing-list, just a copy of the original. Only posts
> after December 2nd 2007 will be available.
> 
> I assume there are no objections to this. In case I am wrong please
> let me know.
> 
> Vincent
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/R-help-google-group-archive-tf4940000.html#a14152914
Sent from the R help mailing list archive at Nabble.com.


From mnevill at exitcheck.net  Tue Dec  4 16:47:10 2007
From: mnevill at exitcheck.net (Max)
Date: Tue, 04 Dec 2007 07:47:10 -0800
Subject: [R] Best forecasting methods with Time Series ?
References: <934356850712040428nb6e4879o48f5d649df0747f4@mail.gmail.com>
Message-ID: <mn.21d37d7c5d1827ba.83239@exitcheck.net>

It depends on the nature of your data. Have you used the stl function 
to decompose your time series data?

plot(stl(time series, s.window="periodic"))

Are you looking at the ACF and PCF to see how strong the 
autocorrelations are? You may need to use a differencing operator to 
make your series stationary? Have you considered a box jenkins 
transformation of your data? (taking some kind of log of your data)

Also, what is a very useful measure that I'm not quite sure how to 
calculate in R, (I had to copy my fits into excel and compare with my 
actuals) is MAPE or Mean Average Proportional Error.

The formula looks like:

 SUM for i from 1 to n absolutevalue (1/n *(Actual_i-Fit_i)/Actual_i )

If youre MAPE is below 0.2 your model should be ok for forecasting. In 
my experience having it below 0.05 helps make better forecasts. MAPE 
isn't a perfect measure (I've found some literature on improved 
measures,) ideally you still want the AICC low but it helps figure out 
on average how by much your model is off. Sometimes ARIMA models can 
have a high MAPE but (relatively )low AICC.

Another model that you could use if there is a trend and seasonality is 
Holt Winters method. It's a fairly simple model that works However, 
your forecasts have to stay inside the seasonal length measure for the 
HW method or the bounds will become rediculously large.

If I knew how state space/GARCH models worked I'd let you know. Another 
list to check out is gmae.comp.lang.r.r-metrics

Hope that helps,

-Max

Ozcan Asilkan wrote on 12/04/2007 :
> Hello,
>
> In order to do a future forecast based on my past Time Series data sets
> (salespricesproduct1, salespricesproduct2, etc..), I used arima() functions
> with different parameter combinations which give the smallest AIC. I also
> used auto.arima() which finds the parameters with the smallest AICs.  But
> unfortuanetly I could not get satisfactory forecast() results, even
> sometimes catastrophic results which made me very disappointed.
>
> Note that, I basically use plot(forecast(auto.arima(invecTS), 24)) statement
> to construct model with arima, forecast 24 future values & plot the results.
>
> Could you suggest me better forecasting methods that I can apply in R ?
>
> Thanks, best regards..
>
> Ozcan
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Tue Dec  4 16:47:38 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 4 Dec 2007 15:47:38 +0000 (UTC)
Subject: [R] Dataframe manipulation
References: <47553007.7080003@yahoo.de>
	<002001c8367f$65a8d8d0$0540210a@www.domain>
Message-ID: <Xns99FC6DD11C2FDdNOTwinscomcast@80.91.229.13>

"Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be> wrote in
news:002001c8367f$65a8d8d0$0540210a at www.domain: 

> try this (also look at R-FAQ 7.10):
> 
> sapply(df, function (x) as.numeric(levels(x))[as.integer(x)])

That looks rather dangerous. By the time I saw your suggestion, I had 
already added an extra variable with:

df$testdata1<-as.numeric(levels(df$testdata))[as.integer(df$testdata)]
 
When I tried your suggestion I got no error, but there was also no 
effect. When I tried:

df2<-sapply(df, function (x) as.numeric(levels(x))[as.integer(x)])

I discovered that the numeric variable, testdata1, had been entirely 
coverted to NA's and str(df2) did not look data.frame-like. 

> is.data.frame(df2)
[1] FALSE

-- 
David Winsemius

> 
> ----- Original Message ----- 
> From: "Antje" <niederlein-rstat at yahoo.de>
> To: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, December 04, 2007 11:46 AM
> Subject: [R] Dataframe manipulation
> 
> 
>> Hello,
>>
>> can anybody help me with this problem?
>> I have a dataframe, which contains its values as factors though I 
>> have numbers
>> but it was read as factors with "scan". Now I would like to convert 
>> these
>> columns (multiple) to a numeric format.
>>
>>
>> # this example creates a similar situation
>>
>> testdata <- as.factor(c("1.1",NA,"2.3","5.5"))
>> testdata2 <- as.factor(c("1.7","4.3","8.5",10.0))
>>
>> df <- data.frame(testdata, testdata2)
>>
>> what do I have to do to get the same datafram but with numeric 
>> values???
>>


From jo.irisson at gmail.com  Tue Dec  4 16:55:40 2007
From: jo.irisson at gmail.com (jiho)
Date: Tue, 4 Dec 2007 16:55:40 +0100
Subject: [R] 2/3d interpolation from a regular grid to another regular grid
Message-ID: <17EF7ABB-B4F8-4279-B9D0-620C02C91E84@gmail.com>

Hello R users,

I have numerical data sampled on two grids, each one shifted by 0.5  
from the other.

For example:

grid1 = expand.grid(x=0:3, y=0.5:2.5)
grid2 = expand.grid(x=0.5:2.5, y=0:3)
gridFinal = expand.grid(x=0.5:2.5, y=0.5:2.5)

plot(gridFinal, xlim=c(0,3), ylim=c(0,3), col="black", pch=19)
points(grid1, xlim=c(0,3), ylim=c(0,3), col="red", pch=19)
points(grid2, xlim=c(0,3), ylim=c(0,3), col="blue", pch=19)

I would like to interpolate the quantities on grid1 (red) and grid2  
(blue) on the same grid (black). This scenario is very common in  
geophysical data and models. I only found:
- functions in package akima which are designed for irregular grids
- krigging in package fields, which also requires irregular spaced data
- approx or spline which works in 1D and which I could apply line by  
line and column by column and use a mean of both estimates
I am sure there are plenty of functions already available to do this  
but searching R-help and the packages site did not help. Pointer to a  
function/package would be highly appreciated.

Eventually, the same scenario will occur in 3D so if the function is  
3D capable it would be a plus (but I am sure the solution to this is  
generic enough to work in nD)

Thank you in advance.

JiHO
---
http://jo.irisson.free.fr/


From backer at psych.uib.no  Tue Dec  4 17:03:11 2007
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Tue, 04 Dec 2007 17:03:11 +0100
Subject: [R] Is R portable?
Message-ID: <47557A3F.9070502@psych.uib.no>

Recently I came across an interesting web site: 
http://portableapps.com/.  The idea is simple, this is software that 
is possible to install and run on some type of USB memory, a stick or 
one of these hard disks.  I can think of a number of situations where 
this could be handy.  In addition memory sticks are getting cheaper 
and more powerful by the day.

So:  Is it possible to run R off one of these sticks?

I am also informed that it is possible to run Latex in this manner.

Tom


From wildscop at yahoo.com  Tue Dec  4 17:06:21 2007
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Tue, 4 Dec 2007 08:06:21 -0800 (PST)
Subject: [R] Metropolis-Hastings within Gibbs coding error
Message-ID: <772405.93715.qm@web32414.mail.mud.yahoo.com>

Dear list,

After running for a while, it crashes and gives the following error message: can anybody suggest how to deal with this?

Error in if (ratio0[i] < log(runif(1))) { : 
  missing value where TRUE/FALSE needed



################### original program ########
p2 <- function (Nsim=1000){

x<- c(0.301,0,-0.301,-0.602,-0.903,-1.208, -1.309,-1.807,-2.108,-2.71) # logdose
n<-c(19,20,19,21,19,20,16,19,40,81) # total subject in dose-response experiment
y<-c(19,18,19,14,15,4,0,0,0,2) # success in each trials
dta<-cbind(x,n,y)
dta<-as.data.frame(dta) # creating data frame

proposal.b0 = current.b0 = ratio0 = double(Nsim) # blank vector
proposal.b1 = current.b1 = ratio1 = double(Nsim) # blank vector
index <- 1:Nsim # creating index
a0 <- 10 # initial value (assumed) for tau
b0 <- (.01) # initial value (assumed) for tau
fit <- glm((y/n)~x,family=binomial, weight = n, data=dta) # initial value for beta

parameters <- c("Beta0", "Beta1", "Tau")
parameter.matrix <- array(NA,c(Nsim,3)) # blank array
parameter.matrix <- as.data.frame(parameter.matrix) # creating data frame
parameter.matrix[1,] <- c(fit$coef[1],fit$coef[2],rgamma(1, shape=a0, scale = b0)) # putting initial values

for (i in 2:Nsim){
# generating Gibbs sampler

parameter.matrix[i,]<- c(rnorm(1, 0, (1/parameter.matrix[i-1,3])), rnorm(1, 0, (1/parameter.matrix[i-1,3])), rgamma(1, shape=(a0+1), rate=(1/b0+(parameter.matrix[i-1,1]^2+parameter.matrix[i-1,2]^2)/2)))
# as the Gamma with specified parameters is the conditional for tau given beta, data

# implementing Metropolis-Hastings within Gibbs to get estimates of beta0 and beta1      

proposal.b0[i]<-sum(log( ((exp(parameter.matrix[i,1])^y)/((1+exp(parameter.matrix[i,1])^n))*exp(-parameter.matrix[i-1,3]*(parameter.matrix[i,1]^2)/2))))
proposal.b1[i]<-sum(log( ((exp(parameter.matrix[i,2]*x)^y) / ((1+exp(parameter.matrix[i,2]*x))^n) * exp(-parameter.matrix[i-1,3]*(parameter.matrix[i,2]^2)/2) )))
# as the above is the conditional for beta's given tau, data

# took logarithm to take care of the 0 problem in product space, but does not help much 

current.b0[i]<-sum(log( (( exp(parameter.matrix[i-1,1])^y)/((1+exp(parameter.matrix[i-1,1])^n))*exp(-parameter.matrix[i-1,3]*(parameter.matrix[i-1,1]^2)/2))))
current.b1[i]<-sum(log( (( exp(parameter.matrix[i-1,2]*x)^y) / ((1+exp(parameter.matrix[i-1,2]*x))^n) * exp(-parameter.matrix[i-1,3]*(parameter.matrix[i-1,2]^2)/2))))

# ratio0 id for beta0
if(current.b0[i]==0) {ratio0[i]=1} else {ratio0[i] <- proposal.b0[i]-current.b0[i]}
if (ratio0[i] < log(runif(1))) {parameter.matrix[i,1] <- parameter.matrix[i-1,1]}
# for beta0

# ratio1 id for beta1
if(current.b1[i]==0) {ratio1[i]=1} else {ratio1[i]=proposal.b1[i]-current.b1[i]}
if (ratio1[i] < log(runif(1))) {parameter.matrix[i,2] <- parameter.matrix[i-1,2]}
# for beta1
cat("At Iteration ", i, "ratio0 and ratio1 are", ratio0[i], ratio1[i], "\n" )
}

x11()
plot(parameter.matrix[,1], parameter.matrix[,2], type="b", xlab="beta.0", ylab="beta.1")
write.table(parameter.matrix, file="z:\\paramaters.txt", quote = F, sep = " ")
x11()

par(mfrow=c(3,1))
plot(index, parameter.matrix[index,1], type="l", xlab="Index", ylab="beta0")
plot(index, parameter.matrix[index,2], type="l", xlab="Index", ylab="beta1")
plot(index, parameter.matrix[index,3], type="l", xlab="Index", ylab="tau")
}

p2(Nsim=1000)





      ____________________________________________________________________________________


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: mhwg.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/78e50b58/attachment.txt 

From tlumley at u.washington.edu  Tue Dec  4 17:06:16 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 4 Dec 2007 08:06:16 -0800 (PST)
Subject: [R] color palette from red to blue passing white
In-Reply-To: <18261.5726.555747.583528@stat.math.ethz.ch>
References: <644e1f320712031752k27ac3b14l9b5a4e6e075b799e@mail.gmail.com>
	<Pine.LNX.4.44.0712040504230.6948-100000@disco.wu-wien.ac.at>
	<18261.5726.555747.583528@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0712040804320.20908@homer24.u.washington.edu>

On Tue, 4 Dec 2007, Martin Maechler wrote:
> Package 'vcd' (and others) use package 'colorspace',
> and I have wondered in the past if these color space computations
> should not be merged into to standard R (package 'grDevices').
> But that's really a topic for another thread, on R-devel, not R-help..

At one time, the fact that colorspace used S4 classes was a problem for 
including it in base R.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From dimitris.rizopoulos at med.kuleuven.be  Tue Dec  4 17:25:54 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 4 Dec 2007 17:25:54 +0100
Subject: [R] Dataframe manipulation
References: <47553007.7080003@yahoo.de><002001c8367f$65a8d8d0$0540210a@www.domain>
	<Xns99FC6DD11C2FDdNOTwinscomcast@80.91.229.13>
Message-ID: <002401c83692$578f18a0$0540210a@www.domain>

my original reply was intended for the original version of 'df', in 
which both columns were factors. In your example you have added a 
numeric column, so not exactly the case I've replied for. For your 
example can use the following:

testdata <- as.factor(c("1.1",NA,"2.3","5.5"))
testdata2 <- as.factor(c("1.7","4.3","8.5",10.0))
df <- data.frame(testdata, testdata2)

df$testdata1 <- 
as.numeric(levels(df$testdata))[as.integer(df$testdata)]

fcts <- sapply(df, is.factor)
df[fcts] <- lapply(df[fcts], function (x) 
as.numeric(levels(x))[as.integer(x)])
df
str(df)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "David Winsemius" <dwinsemius at comcast.net>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, December 04, 2007 4:47 PM
Subject: Re: [R] Dataframe manipulation


> "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be> wrote in
> news:002001c8367f$65a8d8d0$0540210a at www.domain:
>
>> try this (also look at R-FAQ 7.10):
>>
>> sapply(df, function (x) as.numeric(levels(x))[as.integer(x)])
>
> That looks rather dangerous. By the time I saw your suggestion, I 
> had
> already added an extra variable with:
>
> df$testdata1<-as.numeric(levels(df$testdata))[as.integer(df$testdata)]
>
> When I tried your suggestion I got no error, but there was also no
> effect. When I tried:
>
> df2<-sapply(df, function (x) as.numeric(levels(x))[as.integer(x)])
>
> I discovered that the numeric variable, testdata1, had been entirely
> coverted to NA's and str(df2) did not look data.frame-like.
>
>> is.data.frame(df2)
> [1] FALSE
>
> -- 
> David Winsemius
>
>>
>> ----- Original Message ----- 
>> From: "Antje" <niederlein-rstat at yahoo.de>
>> To: <r-help at stat.math.ethz.ch>
>> Sent: Tuesday, December 04, 2007 11:46 AM
>> Subject: [R] Dataframe manipulation
>>
>>
>>> Hello,
>>>
>>> can anybody help me with this problem?
>>> I have a dataframe, which contains its values as factors though I
>>> have numbers
>>> but it was read as factors with "scan". Now I would like to 
>>> convert
>>> these
>>> columns (multiple) to a numeric format.
>>>
>>>
>>> # this example creates a similar situation
>>>
>>> testdata <- as.factor(c("1.1",NA,"2.3","5.5"))
>>> testdata2 <- as.factor(c("1.7","4.3","8.5",10.0))
>>>
>>> df <- data.frame(testdata, testdata2)
>>>
>>> what do I have to do to get the same datafram but with numeric
>>> values???
>>>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From stubben at lanl.gov  Tue Dec  4 17:27:02 2007
From: stubben at lanl.gov (Chris Stubben)
Date: Tue, 4 Dec 2007 08:27:02 -0800 (PST)
Subject: [R] Inserting a subsequence between values of a vector
In-Reply-To: <OF1013D776.0A0AE339-ONC12573A7.0056EE4B-C12573A7.0056EE4E@wsr.ac.at>
References: <OF1013D776.0A0AE339-ONC12573A7.0056EE4B-C12573A7.0056EE4E@wsr.ac.at>
Message-ID: <14154029.post@talk.nabble.com>


You could use a combination of rle, cumsum and append.

> x <- c(1,1,1,2,2,3,3,3,3,3,4) 
> y<-rle(x)$lengths
> y
[1] 3 2 5 1
> z<-cumsum(y)[y>1]
> z
[1]  3  5 10
>
> for(i in rev(z)) x <- append(x, c(0,0,0), after = i)
> x
 [1] 1 1 1 0 0 0 2 2 0 0 0 3 3 3 3 3 0 0 0 4


Chris


Serguei Kaniovski-3 wrote:
> 
> 
> Hallo,
> 
> suppose I have a vector:
> 
> x <- c(1,1,1,2,2,3,3,3,3,3,4)
> 
> How can I generate a vector/sequence in which a fixed number of zeroes
> (say
> 3) is inserted between the consecutive values, so I get
> 
> 1,1,1,0,0,0,2,2,0,0,0,3,3,3,3,3,0,0,0,4
> 
> thanks a lot,
> Serguei
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Inserting-a-subsequence-between-values-of-a-vector-tf4943930.html#a14154029
Sent from the R help mailing list archive at Nabble.com.


From murdoch at stats.uwo.ca  Tue Dec  4 17:33:29 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 04 Dec 2007 11:33:29 -0500
Subject: [R] Metropolis-Hastings within Gibbs coding error
In-Reply-To: <772405.93715.qm@web32414.mail.mud.yahoo.com>
References: <772405.93715.qm@web32414.mail.mud.yahoo.com>
Message-ID: <47558159.5030802@stats.uwo.ca>

On 12/4/2007 11:06 AM, Mohammad Ehsanul Karim wrote:
> Dear list,
> 
> After running for a while, it crashes and gives the following error message: can anybody suggest how to deal with this?
> 
> Error in if (ratio0[i] < log(runif(1))) { : 
>   missing value where TRUE/FALSE needed

Use options(error=recover) to drop to the debugger when the error 
occurs, and you could examine the value of ratio0.  runif(1) should not 
give a 0, so log(runif(1)) should succeed.

Duncan Murdoch

> 
> 
> 
> ################### original program ########
> p2 <- function (Nsim=1000){
> 
> x<- c(0.301,0,-0.301,-0.602,-0.903,-1.208, -1.309,-1.807,-2.108,-2.71) # logdose
> n<-c(19,20,19,21,19,20,16,19,40,81) # total subject in dose-response experiment
> y<-c(19,18,19,14,15,4,0,0,0,2) # success in each trials
> dta<-cbind(x,n,y)
> dta<-as.data.frame(dta) # creating data frame
> 
> proposal.b0 = current.b0 = ratio0 = double(Nsim) # blank vector
> proposal.b1 = current.b1 = ratio1 = double(Nsim) # blank vector
> index <- 1:Nsim # creating index
> a0 <- 10 # initial value (assumed) for tau
> b0 <- (.01) # initial value (assumed) for tau
> fit <- glm((y/n)~x,family=binomial, weight = n, data=dta) # initial value for beta
> 
> parameters <- c("Beta0", "Beta1", "Tau")
> parameter.matrix <- array(NA,c(Nsim,3)) # blank array
> parameter.matrix <- as.data.frame(parameter.matrix) # creating data frame
> parameter.matrix[1,] <- c(fit$coef[1],fit$coef[2],rgamma(1, shape=a0, scale = b0)) # putting initial values
> 
> for (i in 2:Nsim){
> # generating Gibbs sampler
> 
> parameter.matrix[i,]<- c(rnorm(1, 0, (1/parameter.matrix[i-1,3])), rnorm(1, 0, (1/parameter.matrix[i-1,3])), rgamma(1, shape=(a0+1), rate=(1/b0+(parameter.matrix[i-1,1]^2+parameter.matrix[i-1,2]^2)/2)))
> # as the Gamma with specified parameters is the conditional for tau given beta, data
> 
> # implementing Metropolis-Hastings within Gibbs to get estimates of beta0 and beta1      
> 
> proposal.b0[i]<-sum(log( ((exp(parameter.matrix[i,1])^y)/((1+exp(parameter.matrix[i,1])^n))*exp(-parameter.matrix[i-1,3]*(parameter.matrix[i,1]^2)/2))))
> proposal.b1[i]<-sum(log( ((exp(parameter.matrix[i,2]*x)^y) / ((1+exp(parameter.matrix[i,2]*x))^n) * exp(-parameter.matrix[i-1,3]*(parameter.matrix[i,2]^2)/2) )))
> # as the above is the conditional for beta's given tau, data
> 
> # took logarithm to take care of the 0 problem in product space, but does not help much 
> 
> current.b0[i]<-sum(log( (( exp(parameter.matrix[i-1,1])^y)/((1+exp(parameter.matrix[i-1,1])^n))*exp(-parameter.matrix[i-1,3]*(parameter.matrix[i-1,1]^2)/2))))
> current.b1[i]<-sum(log( (( exp(parameter.matrix[i-1,2]*x)^y) / ((1+exp(parameter.matrix[i-1,2]*x))^n) * exp(-parameter.matrix[i-1,3]*(parameter.matrix[i-1,2]^2)/2))))
> 
> # ratio0 id for beta0
> if(current.b0[i]==0) {ratio0[i]=1} else {ratio0[i] <- proposal.b0[i]-current.b0[i]}
> if (ratio0[i] < log(runif(1))) {parameter.matrix[i,1] <- parameter.matrix[i-1,1]}
> # for beta0
> 
> # ratio1 id for beta1
> if(current.b1[i]==0) {ratio1[i]=1} else {ratio1[i]=proposal.b1[i]-current.b1[i]}
> if (ratio1[i] < log(runif(1))) {parameter.matrix[i,2] <- parameter.matrix[i-1,2]}
> # for beta1
> cat("At Iteration ", i, "ratio0 and ratio1 are", ratio0[i], ratio1[i], "\n" )
> }
> 
> x11()
> plot(parameter.matrix[,1], parameter.matrix[,2], type="b", xlab="beta.0", ylab="beta.1")
> write.table(parameter.matrix, file="z:\\paramaters.txt", quote = F, sep = " ")
> x11()
> 
> par(mfrow=c(3,1))
> plot(index, parameter.matrix[index,1], type="l", xlab="Index", ylab="beta0")
> plot(index, parameter.matrix[index,2], type="l", xlab="Index", ylab="beta1")
> plot(index, parameter.matrix[index,3], type="l", xlab="Index", ylab="tau")
> }
> 
> p2(Nsim=1000)
> 
> 
> 
> 
> 
>       ____________________________________________________________________________________
> 
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Tue Dec  4 17:37:53 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 4 Dec 2007 11:37:53 -0500
Subject: [R] Inserting a subsequence between values of a vector
In-Reply-To: <OF1013D776.0A0AE339-ONC12573A7.0056EE4B-C12573A7.0056EE4E@wsr.ac.at>
References: <OF1013D776.0A0AE339-ONC12573A7.0056EE4B-C12573A7.0056EE4E@wsr.ac.at>
Message-ID: <971536df0712040837t4143c137nc37dcb6917756378@mail.gmail.com>

Take the rle, fix up the result and take the invese.rle.  Our formula
adds 0's to the end too so remove those with head:

head(inverse.rle(with(rle(x), list(lengths = c(rbind(lengths, 3)),
values = c(rbind(values, 0))))), -3)

On Dec 4, 2007 10:49 AM, Serguei Kaniovski <Serguei.Kaniovski at wifo.ac.at> wrote:
>
> Hallo,
>
> suppose I have a vector:
>
> x <- c(1,1,1,2,2,3,3,3,3,3,4)
>
> How can I generate a vector/sequence in which a fixed number of zeroes (say
> 3) is inserted between the consecutive values, so I get
>
> 1,1,1,0,0,0,2,2,0,0,0,3,3,3,3,3,0,0,0,4
>
> thanks a lot,
> Serguei
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From scruveil at genoscope.cns.fr  Tue Dec  4 17:34:53 2007
From: scruveil at genoscope.cns.fr (=?ISO-8859-1?Q?St=E9phane_CRUVEILLER?=)
Date: Tue, 04 Dec 2007 17:34:53 +0100
Subject: [R] Multiple stacked barplots on the same graph?
In-Reply-To: <47557529.3050806@unicas.it>
References: <47555E3C.9030301@genoscope.cns.fr> <47557529.3050806@unicas.it>
Message-ID: <475581AD.9030000@genoscope.cns.fr>

Hi,

I tried this method but it seems that there is something wrong with my 
data frame:


when I type in:

 > qplot(x=as.factor(Categorie),y=Total,data=mydata)

It displays a graph with 2 points in each category...
but if  I add the parameter geom="histogram"

 > qplot(x=as.factor(Categorie),y=Total,data=mydata,geom="histogram")


Error in storage.mode(test) <- "logical" :
        object "y" not found

any hint about this...

thx, St?phane.


========================================================
the real data frame

                                                    Categorie     Part 
Total  chr1  chr2    pl
1                                     Amino acid biosynthesis   common  
3.03  4.55  1.68  0.00
2          Purines, pyrimidines, nucleosides, and nucleotides   common  
1.65  2.37  1.06  0.00
3                      Fatty acid and phospholipid metabolism   common  
1.52  1.77  1.55  0.00
4  Biosynthesis of cofactors, prosthetic groups, and carriers   common  
2.85  4.68  1.02  0.00
5                             Central intermediary metabolism   common  
3.40  3.19  4.57  0.00
6                                           Energy metabolism   common 
11.81 12.49 13.87  0.17
7                              Transport and binding proteins   common 
10.51 13.56  7.85  4.27
8                                              DNA metabolism   common  
1.95  2.81  0.98  1.03
9                                               Transcription   common  
2.08  3.13  1.06  0.34
10                                          Protein synthesis   common  
2.51  4.58  0.27  0.00
11                                               Protein fate   common  
2.23  3.26  1.20  0.68
12                                       Regulatory functions   common  
7.63  7.30  9.88  0.68
13                                        Signal transduction   common  
1.88  2.06  2.13  0.00
14                                              Cell envelope   common  
2.76  3.41  2.53  0.17
15                                         Cellular processes   common  
7.21  7.90  7.71  1.54
16              Mobile and extrachromosomal element functions   common  
1.08  0.22  0.40  8.38
17                                           Unknown function   common 
20.75 22.45 22.38  5.30
18                                    Amino acid biosynthesis specific  
0.35  0.16  0.71  0.00
19         Purines, pyrimidines, nucleosides, and nucleotides specific  
0.17  0.06  0.35  0.00
20                     Fatty acid and phospholipid metabolism specific  
0.08  0.09  0.09  0.00
21 Biosynthesis of cofactors, prosthetic groups, and carriers specific  
0.18  0.19  0.22  0.00
22                            Central intermediary metabolism specific  
0.42  0.09  0.98  0.00
23                                          Energy metabolism specific  
2.05  0.70  3.90  2.22
24                             Transport and binding proteins specific  
1.98  0.85  3.24  3.25
25                                             DNA metabolism specific  
0.63  0.22  0.22  4.44
26                                              Transcription specific  
0.17  0.06  0.22  0.51
27                                          Protein synthesis specific  
0.20  0.03  0.49  0.00
28                                               Protein fate specific  
0.30  0.32  0.31  0.17
29                                       Regulatory functions specific  
1.58  0.66  2.79  1.88
30                                        Signal transduction specific  
0.27  0.06  0.62  0.00
31                                              Cell envelope specific  
0.83  0.63  1.33  0.00
32                                         Cellular processes specific  
1.38  0.38  1.95  4.62
33              Mobile and extrachromosomal element functions specific  
3.56  1.14  0.44 28.72
34                                           Unknown function specific 
11.63  6.17 16.00 24.27



Domenico Vistocco wrote:
> Perhaps this could be useful:
> > x=scan()
> 11.81 10.51  1.95  2.08  2.51  2.05  1.98  0.63  0.17  0.20
> 12.49 13.56 2.81  3.13  4.58  0.70  0.85  0.22  0.06  0.03
>
> > x=matrix(x,5,4,byrow=T)
> > rownames(x)=paste("comp",1:5,sep="")
> > colnames(x)=paste("c",1:4,sep="")
>
> > library(ggplot2)
> > dfm=melt(x)
> > qplot(as.factor(x=X1),y=value,geom="histogram",data=dfm,fill=X2)
>
> domenico vistocco
>
> St?phane CRUVEILLER wrote:
>> Dear R-Users,
>>
>> I would like to know whether it is possible to draw several
>> stacked barplots (i.e. side by side on the same sheet)...
>>
>>
>> my data look like :
>>
>>                             Cond1  Cond1' Cond2   Cond2'
>> Compartment 1    11,81    2,05    12,49    0,70   Compartment 2     
>> 10,51    1,98    13,56    0,85
>> Compartment 3     1,95    0,63    2,81    0,22  Compartment 4     
>> 2,08    0,17    3,13    0,06   Compartment 5     2,51    0,20    
>> 4,58    0,03
>>
>> ps: Cond1' values should be stacked on Cond1, Cond2' on Cond 2 and so 
>> on... and series 1 and 2
>> should be side by side for each compartement....
>>
>> Thanks for any help.
>>
>> St?phane.
>>
>>   
>

-- 
"La science a certes quelques magnifiques r?ussites ? son actif mais
? tout prendre, je pr?f?re de loin ?tre heureux plut?t qu'avoir raison." 
D. Adams
-- 
AGC website <http://www.genoscope.cns.fr/agc>
	St?phane CRUVEILLER Ph. D.
Genoscope - Centre National de S?quencage
Atelier de G?nomique Comparative
2, Rue Gaston Cremieux CP 5706
91057 Evry Cedex - France
Phone: +33 (0)1 60 87 84 58
Fax: +33 (0)1 60 87 25 14
scruveil at genoscope.cns.fr


From vistocco at unicas.it  Tue Dec  4 17:42:37 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Tue, 04 Dec 2007 17:42:37 +0100
Subject: [R] Inserting a subsequence between values of a vector
In-Reply-To: <OF1013D776.0A0AE339-ONC12573A7.0056EE4B-C12573A7.0056EE4E@wsr.ac.at>
References: <OF1013D776.0A0AE339-ONC12573A7.0056EE4B-C12573A7.0056EE4E@wsr.ac.at>
Message-ID: <4755837D.1060108@unicas.it>

library(R.utils)
pos=which(diff(x)==1)+1
insert(x,ats=pos,rep(list(rep(0,3)),length(pos)))

domenico vistocco

Serguei Kaniovski wrote:
> Hallo,
>
> suppose I have a vector:
>
> x <- c(1,1,1,2,2,3,3,3,3,3,4)
>
> How can I generate a vector/sequence in which a fixed number of zeroes (say
> 3) is inserted between the consecutive values, so I get
>
> 1,1,1,0,0,0,2,2,0,0,0,3,3,3,3,3,0,0,0,4
>
> thanks a lot,
> Serguei
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From gunter.berton at gene.com  Tue Dec  4 17:46:29 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Tue, 4 Dec 2007 08:46:29 -0800
Subject: [R] Is R portable?
In-Reply-To: <47557A3F.9070502@psych.uib.no>
References: <47557A3F.9070502@psych.uib.no>
Message-ID: <002b01c83695$3884e680$3a0b2c0a@gne.windows.gene.com>

On Windows anyway, R can be located in any directory including one on a
flash drive). R can also be set up to make no use of the registry (again --
Windows only), so AFAICS the answer is yes, and it's trivial to do. I would
be surprised if this were not true in other OS's, too. R is just an
executable with supporting libraries that can be located and run from
anywhere. Of course, various configuration details (file locations, language
environment, graphics options,...) must be set to agree with the particular
computer hardware and software on which the flash drive runs, but that is
inevitable (if R was told it should use a cyrillic character set for
Russian, it won't automatically switch to French when the flash drive is
stuck into a French computer).

If I am wrong about any of this, **PLEASE CORRECT**

Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Tom Backer Johnsen
Sent: Tuesday, December 04, 2007 8:03 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Is R portable?

Recently I came across an interesting web site: 
http://portableapps.com/.  The idea is simple, this is software that 
is possible to install and run on some type of USB memory, a stick or 
one of these hard disks.  I can think of a number of situations where 
this could be handy.  In addition memory sticks are getting cheaper 
and more powerful by the day.

So:  Is it possible to run R off one of these sticks?

I am also informed that it is possible to run Latex in this manner.

Tom

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From roland.rproject at gmail.com  Tue Dec  4 18:02:44 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Tue, 04 Dec 2007 12:02:44 -0500
Subject: [R] Is R portable?
In-Reply-To: <47557A3F.9070502@psych.uib.no>
References: <47557A3F.9070502@psych.uib.no>
Message-ID: <47558834.8000900@gmail.com>

Hi Tom,

did you check the R for Windows FAQ?

http://cran.r-project.org/bin/windows/base/rw-FAQ.html#Can-I-run-R-from-a-CD-or-USB-drive_003f

Hope this helps,
Roland


Tom Backer Johnsen wrote:
> Recently I came across an interesting web site: 
> http://portableapps.com/.  The idea is simple, this is software that 
> is possible to install and run on some type of USB memory, a stick or 
> one of these hard disks.  I can think of a number of situations where 
> this could be handy.  In addition memory sticks are getting cheaper 
> and more powerful by the day.
> 
> So:  Is it possible to run R off one of these sticks?
> 
> I am also informed that it is possible to run Latex in this manner.
> 
> Tom
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From nrouraonline at gmail.com  Tue Dec  4 18:14:23 2007
From: nrouraonline at gmail.com (=?UTF-8?Q?N=C3=BAria_Roura?=)
Date: Tue, 4 Dec 2007 09:14:23 -0800 (PST)
Subject: [R] How can I use Adehabitat to obtain an .asc object with the
 predicted values of a niche model?
Message-ID: <14155047.post@talk.nabble.com>


Dear all,

I'm using the package adehabitat in R to import several .asc files
(=matrix), 
and also create a kasc object (=dataframe) with all of them.

The main idea is to use this kasc object to map the predicted values of
climate-matching model for an overall area. However, I don't know how to
proceed: Do I have to project the model (already stored in R) onto the kasc
object directly, or convert the kasc object to a data frame where each row
refers to a pixel?

On the other hand, I would also like to know how to obtain the XY
coordinates for each row of the kasc object. 

Thank you very much. Cheers,

N?ria Roura

-- 
View this message in context: http://www.nabble.com/How-can-I-use-Adehabitat-to-obtain-an-.asc-object-with-the-predicted-values-of-a-niche-model--tf4944431.html#a14155047
Sent from the R help mailing list archive at Nabble.com.


From csardi at rmki.kfki.hu  Tue Dec  4 18:17:12 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Tue, 4 Dec 2007 18:17:12 +0100
Subject: [R] Is R portable?
In-Reply-To: <002b01c83695$3884e680$3a0b2c0a@gne.windows.gene.com>
References: <47557A3F.9070502@psych.uib.no>
	<002b01c83695$3884e680$3a0b2c0a@gne.windows.gene.com>
Message-ID: <20071204171712.GB10692@localdomain>

Yes, it is indeed true for other systems as well, although
some configuration problems might arise, at least on Linux. 

It is also true that there are several small Linux distributions 
which easily fit into a flash drive, and then you can boot from the 
flash drive. I used to use SLAX, this is module based, ie. you can
choose which packages to install into the flash drive, and it has an R
package, although R version 2.2, so it is outdated. But i'm sure
there are more options, eg. Damn Small Linux is popular, i believe it
is desktop oriented, so might not contain R by default.

I used to install my whole Linux system into a small (ie. notebook)
USB harddisk, this is very comfortable, you just boot from the
harddisk and wehereever you are you get the same system (assuming you
can find a PC, but usually this is not a problem). It was a bit
slower, but much more comfortable than bringing a notebook with me
all the time. Until i accidentally kicked down the harddisk from the
desk...

Gabor

On Tue, Dec 04, 2007 at 08:46:29AM -0800, Bert Gunter wrote:
> On Windows anyway, R can be located in any directory including one on a
> flash drive). R can also be set up to make no use of the registry (again --
> Windows only), so AFAICS the answer is yes, and it's trivial to do. I would
> be surprised if this were not true in other OS's, too. R is just an
> executable with supporting libraries that can be located and run from
> anywhere. Of course, various configuration details (file locations, language
> environment, graphics options,...) must be set to agree with the particular
> computer hardware and software on which the flash drive runs, but that is
> inevitable (if R was told it should use a cyrillic character set for
> Russian, it won't automatically switch to French when the flash drive is
> stuck into a French computer).
> 
> If I am wrong about any of this, **PLEASE CORRECT**
> 
> Bert Gunter
> Genentech Nonclinical Statistics
> 
> 
> -----Original Message-----
> From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
> Behalf Of Tom Backer Johnsen
> Sent: Tuesday, December 04, 2007 8:03 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Is R portable?
> 
> Recently I came across an interesting web site: 
> http://portableapps.com/.  The idea is simple, this is software that 
> is possible to install and run on some type of USB memory, a stick or 
> one of these hard disks.  I can think of a number of situations where 
> this could be handy.  In addition memory sticks are getting cheaper 
> and more powerful by the day.
> 
> So:  Is it possible to run R off one of these sticks?
> 
> I am also informed that it is possible to run Latex in this manner.
> 
> Tom
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From ssefick at gmail.com  Tue Dec  4 18:17:59 2007
From: ssefick at gmail.com (stephen sefick)
Date: Tue, 4 Dec 2007 12:17:59 -0500
Subject: [R] time series
Message-ID: <c502a9e10712040917y6bb49038k5472d9b897aa3149@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/c24094a7/attachment.pl 

From jrkrideau at yahoo.ca  Tue Dec  4 18:19:42 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 4 Dec 2007 12:19:42 -0500 (EST)
Subject: [R] Multiple stacked barplots on the same graph?
In-Reply-To: <47557529.3050806@unicas.it>
Message-ID: <470371.39045.qm@web32805.mail.mud.yahoo.com>


--- Domenico Vistocco <vistocco at unicas.it> wrote:

> Perhaps this could be useful:
>  > x=scan()
> 11.81 10.51  1.95  2.08  2.51  2.05  1.98  0.63 
> 0.17  0.20
> 12.49 13.56 2.81  3.13  4.58  0.70  0.85  0.22  0.06
>  0.03
> 
>  > x=matrix(x,5,4,byrow=T)
>  > rownames(x)=paste("comp",1:5,sep="")
>  > colnames(x)=paste("c",1:4,sep="")
> 
>  > library(ggplot2)
>  > dfm=melt(x)
>  >
>
qplot(as.factor(x=X1),y=value,geom="histogram",data=dfm,fill=X2)
> 
> domenico vistocco

Very nice but bit garish. :)

What about a dotchart instead?

 dotchart(x, labels=rownames(x),col=c(1:4), pch=16)



> 
> St???phane CRUVEILLER wrote:
> > Dear R-Users,
> >
> > I would like to know whether it is possible to
> draw several
> > stacked barplots (i.e. side by side on the same
> sheet)...
> >
> >
> > my data look like :
> >
> >                             Cond1  Cond1' Cond2  
> Cond2'
> > Compartment 1    11,81    2,05    12,49    0,70   
> > Compartment 2     10,51    1,98    13,56    0,85
> > Compartment 3     1,95    0,63    2,81    0,22  
> > Compartment 4     2,08    0,17    3,13    0,06   
> > Compartment 5     2,51    0,20    4,58    0,03
> >
> > ps: Cond1' values should be stacked on Cond1,
> Cond2' on Cond 2 and so 
> > on... and series 1 and 2
> > should be side by side for each compartement....
> >
> > Thanks for any help.
> >
> > St???phane.
> >
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From jrkrideau at yahoo.ca  Tue Dec  4 18:22:36 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 4 Dec 2007 12:22:36 -0500 (EST)
Subject: [R] Is R portable?
In-Reply-To: <47558834.8000900@gmail.com>
Message-ID: <281383.37749.qm@web32815.mail.mud.yahoo.com>

I simply installed R onto a USB stick, downloaded my
normal packages to it and it works fine under Windows.
 

--- Roland Rau <roland.rproject at gmail.com> wrote:

> Hi Tom,
> 
> did you check the R for Windows FAQ?
> 
http://cran.r-project.org/bin/windows/base/rw-FAQ.html#Can-I-run-R-from-a-CD-or-USB-drive_003f
> 
> Hope this helps,
> Roland
> 
> 
> Tom Backer Johnsen wrote:
> > Recently I came across an interesting web site: 
> > http://portableapps.com/.  The idea is simple,
> this is software that 
> > is possible to install and run on some type of USB
> memory, a stick or 
> > one of these hard disks.  I can think of a number
> of situations where 
> > this could be handy.  In addition memory sticks
> are getting cheaper 
> > and more powerful by the day.
> > 
> > So:  Is it possible to run R off one of these
> sticks?
> > 
> > I am also informed that it is possible to run
> Latex in this manner.
> > 
> > Tom
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From jombart at biomserv.univ-lyon1.fr  Tue Dec  4 18:37:31 2007
From: jombart at biomserv.univ-lyon1.fr (Thibaut Jombart)
Date: Tue, 04 Dec 2007 18:37:31 +0100
Subject: [R] How can I use Adehabitat to obtain an .asc object with the
 predicted values of a niche model?
In-Reply-To: <14155047.post@talk.nabble.com>
References: <14155047.post@talk.nabble.com>
Message-ID: <4755905B.3060100@biomserv.univ-lyon1.fr>

N?ria Roura wrote:

>Dear all,
>I'm using the package adehabitat in R to import several .asc files(=matrix), and also create a kasc object (=dataframe) with all of them.
>The main idea is to use this kasc object to map the predicted values ofclimate-matching model for an overall area. However, I don't know how toproceed: Do I have to project the model (already stored in R) onto the kascobject directly, or convert the kasc object to a data frame where each rowrefers to a pixel?
>On the other hand, I would also like to know how to obtain the XYcoordinates for each row of the kasc object. 
>Thank you very much. Cheers,
>N?ria Roura
>-- View this message in context: http://www.nabble.com/How-can-I-use-Adehabitat-to-obtain-an-.asc-object-with-the-predicted-values-of-a-niche-model--tf4944431.html#a14155047Sent from the R help mailing list archive at Nabble.com.
>______________________________________________R-help at r-project.org mailing listhttps://stat.ethz.ch/mailman/listinfo/r-helpPLEASE do read the posting guide http://www.R-project.org/posting-guide.htmland provide commented, minimal, self-contained, reproducible code.
>
>
>  
>
Dear Nuria,

The mailing list devoted to adehabitat may be a better place for your 
post. See:

http://www.faunalia.com/cgi-bin/mailman/listinfo/animov

Regards,

Thibaut.
-- 
######################################
Thibaut JOMBART
CNRS UMR 5558 - Laboratoire de Biom?trie et Biologie Evolutive
Universite Lyon 1
43 bd du 11 novembre 1918
69622 Villeurbanne Cedex
T?l. : 04.72.43.29.35
Fax : 04.72.43.13.88
jombart at biomserv.univ-lyon1.fr
http://lbbe.univ-lyon1.fr/-Jombart-Thibaut-.html?lang=en
http://pbil.univ-lyon1.fr/software/adegenet/


From lisas at salford-systems.com  Tue Dec  4 18:37:29 2007
From: lisas at salford-systems.com (Lisa Solomon)
Date: Tue, 4 Dec 2007 09:37:29 -0800
Subject: [R] Free Online: Data Mining Intro for Beginners, Vendor-Neutral,
	December 13
Message-ID: <Q1pRQjNVRyhOVyVaIC8pMTEyMDQ3ODc1MA@sspc-lisa>

ONLINE VENDOR NEUTRAL INTRO TO DATA MINING FOR ABSOLUTE BEGINNERS
(no charge)

A non-technical data mining introduction for beginners
December 13, 2007
US and European Timezones:
To register: http://salford.webex.com

This one-hour webinar is a perfect place to start if you are new to data mining and have little-to-no background in statistics or machine learning. 

In one hour, we will discuss:

**Data basics: what kind of data is required for data mining and predictive analytics; In what format must the data be; what steps are necessary to prepare data appropriately 

**What kinds of questions can we answer with data mining

**How data mining models work: the inputs, the outputs, and the nature of the predictive mechanism 

**Evaluation criteria: how predictive models can be assessed and their value measured 

**Specific background knowledge to prepare you to begin a data mining project.

To register: http://salford.webex.com

Contact me if the December 13th date/time is inconvenient and you wish to be put on our webinar notification list.

Sincerely,
Lisa Solomon
lisas at salford-systems.com


From wang0174 at umn.edu  Tue Dec  4 18:42:49 2007
From: wang0174 at umn.edu (shelley)
Date: Tue, 4 Dec 2007 11:42:49 -0600
Subject: [R] cannot install R-2.6.1 on Mac OS X 10.4.9 PowerPC G5
Message-ID: <0C8C3AF3-C104-45A2-A2EA-9F3D896B0D86@umn.edu>

Hello:

I tried to install the latest R2.6.1 on my Mac OS X machines. I have  
no problems installing it on my Mac OS X Tigr machine. But when I  
tried to install it on other Mac OS X 10.4.9 PowerPCs, I could not  
have the GNU Fortran and/or Tcl/Tk libraries installed. I always got  
the error message : There were errors installing the software, please  
try installing again.

Does somebody have any idea what was going wrong? I had X11 installed  
on the machines, do I need to install other softwares before  
installing R?

Thanks!
-Shelley


From backer at psych.uib.no  Tue Dec  4 18:49:53 2007
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Tue, 04 Dec 2007 18:49:53 +0100
Subject: [R] Is R portable?
In-Reply-To: <47558834.8000900@gmail.com>
References: <47557A3F.9070502@psych.uib.no> <47558834.8000900@gmail.com>
Message-ID: <47559341.7070503@psych.uib.no>

Roland Rau wrote:
> Hi Tom,
> 
> did you check the R for Windows FAQ?
> 
> http://cran.r-project.org/bin/windows/base/rw-FAQ.html#Can-I-run-R-from-a-CD-or-USB-drive_003f 

Puh.  My apologies.  I should have done so before I asked the 
question.  Sorry.

Tom

> Hope this helps,
> Roland
> 
> 
> Tom Backer Johnsen wrote:
>> Recently I came across an interesting web site: 
>> http://portableapps.com/.  The idea is simple, this is software that 
>> is possible to install and run on some type of USB memory, a stick or 
>> one of these hard disks.  I can think of a number of situations where 
>> this could be handy.  In addition memory sticks are getting cheaper 
>> and more powerful by the day.
>>
>> So:  Is it possible to run R off one of these sticks?
>>
>> I am also informed that it is possible to run Latex in this manner.
>>
>> Tom
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 


-- 
+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+


From vdemart1 at tin.it  Tue Dec  4 18:52:41 2007
From: vdemart1 at tin.it (vittorio)
Date: Tue, 4 Dec 2007 18:52:41 +0100
Subject: [R] Plotting monthly timeseries with an x-axis in "time format"
In-Reply-To: <971536df0712031334g768234afx60d51dd18c3db0c8@mail.gmail.com>
References: <200712031811.47949.vdemart1@tin.it>
	<200712032216.13133.vdemart1@tin.it>
	<971536df0712031334g768234afx60d51dd18c3db0c8@mail.gmail.com>
Message-ID: <200712041852.41797.vdemart1@tin.it>

You're right! It was kmail under kde to distort your message.
It works.
Thanks a lot
Vittorio
Il Monday 03 December 2007 22:34:48 Gabor Grothendieck ha scritto:
> Maybe your email software corrupted it somehow.  Often
> email software will cause weird line wrappings, for example.
> Or maybe you have an old version of R or zoo.  I am running zoo
> 1.4.0 and "R version 2.6.0 Patched (2007-10-08 r43124)"
>
> I just located my post in the archives
> https://stat.ethz.ch/pipermail/r-help/2007-December/147481.html
>
> and copied it from there pasting it into a fresh R session and
> it worked.
>
> On Dec 3, 2007 4:16 PM, vittorio <vdemart1 at tin.it> wrote:
> > Unfortunately something doesn't work:
> > > tab <- ts(cbind(A = c(79.47, 89.13, 84.86, 75.68, 72.82, 78.87, 93.46,
> >
> > + 78.18, 82.46, 77.25, 80.95, 84.39, 81.7, 74.76, 65.29, 60.3,
> > + 66.59, 73.19, 92.39, 65.76, 77.45, 74.22, 101.36, 100.01), B = c(77.95,
> > + 76.73, 51.2, 51.86, 51.29, 49.45, 53.88, 47.96, 55.07, 45.34,
> > + 37.07, 37.53, 47.79, 37.5, 30.35, 37.78, 34.13, 39.14, 39.89,
> > + 35.46, 36.54, 38.39, 47.33, 45.34)), start = c(2006, 1), freq = 12)
> >
> > > library(zoo)
> > > pnl.xaxis <- function(...) {
> >
> > + lines(...)
> > Error: unexpected input in:
> > "pnl.xaxis <- function(...) {
> > "
> >
> > What's wrong with it?
> > Ciao Vittorio
> >
> > Il Monday 03 December 2007 20:40:16 Gabor Grothendieck ha scritto:
> > > This can be done with plot.zoo and a panel function:
> > >
> > > tab <- ts(cbind(A = c(79.47, 89.13, 84.86, 75.68, 72.82, 78.87, 93.46,
> > > 78.18, 82.46, 77.25, 80.95, 84.39, 81.7, 74.76, 65.29, 60.3,
> > > 66.59, 73.19, 92.39, 65.76, 77.45, 74.22, 101.36, 100.01), B = c(77.95,
> > > 76.73, 51.2, 51.86, 51.29, 49.45, 53.88, 47.96, 55.07, 45.34,
> > > 37.07, 37.53, 47.79, 37.5, 30.35, 37.78, 34.13, 39.14, 39.89,
> > > 35.46, 36.54, 38.39, 47.33, 45.34)), start = c(2006, 1), freq = 12)
> > >
> > > library(zoo)
> > > pnl.xaxis <- function(...) {
> > >      lines(...)
> > >      panel.number <- parent.frame()$panel.number
> > >      nser <- parent.frame()$nser
> > >      # if bottom panel
> > >      if (!length(panel.number) || panel.number == nser) {
> > >            tt <- list(...)[[1]]
> > >            ym <- as.yearmon(tt)
> > >            mon <- as.numeric(format(ym, "%m"))
> > >            yy <- format(ym, "%y")
> > >            mm <- substring(month.abb[mon], 1, 1)
> > >            axis(1, tt[mon == 1], yy[mon == 1], cex.axis = 0.7)
> > >            axis(1, tt[mon > 1], mm[mon > 1], cex.axis = 0.5, tcl =
> > > -0.3) }
> > > }
> > > plot(as.zoo(tab), panel = pnl.xaxis, xaxt = "n")
> > >
> > > On Dec 3, 2007 12:11 PM, vittorio <vdemart1 at tin.it> wrote:
> > > > I have the following timeseries "tab"
> > > > =====================================
> > > >
> > > > > str(tab)
> > > >
> > > >  mts [1:23, 1:2] 79.5 89.1 84.9 75.7 72.8 ...
> > > >  - attr(*, "dimnames")=List of 2
> > > >  ..$ : NULL
> > > >  ..$ : chr [1:2] "Ipex...I" "Omel...E"
> > > >  - attr(*, "tsp")= num [1:3] 2006 2008   12
> > > >  - attr(*, "class")= chr [1:2] "mts" "ts"
> > > >
> > > > > tab
> > > >
> > > >         Ipex...I Omel...E
> > > > Jan 2006    79.47    77.95
> > > > Feb 2006    89.13    76.73
> > > > Mar 2006    84.86    51.20
> > > > Apr 2006    75.68    51.86
> > > > May 2006    72.82    51.29
> > > > Jun 2006    78.87    49.45
> > > > Jul 2006    93.46    53.88
> > > > Aug 2006    78.18    47.96
> > > > Sep 2006    82.46    55.07
> > > > Oct 2006    77.25    45.34
> > > > Nov 2006    80.95    37.07
> > > > Dec 2006    84.39    37.53
> > > > Jan 2007    81.70    47.79
> > > > Feb 2007    74.76    37.50
> > > > Mar 2007    65.29    30.35
> > > > Apr 2007    60.30    37.78
> > > > May 2007    66.59    34.13
> > > > Jun 2007    73.19    39.14
> > > > Jul 2007    92.39    39.89
> > > > Aug 2007    65.76    35.46
> > > > Sep 2007    77.45    36.54
> > > > Oct 2007    74.22    38.39
> > > > Nov 2007   101.36    47.33
> > > > Dec 2007   100.01  45.34
> > > > ===============================
> > > >
> > > > Plotting tab with a simple "plot(tab,plot.type="single")" I'm
> > > > obtaining a graph  with the x axis in an orrible decimal format so
> > > > that,e.g., Jan 2006 is 2006.0 and Nov 2006 is 2006.8(33)!
> > > >
> > > >  Instead I would like to see the x-axis in a more human-readable
> > > > format, for instance, 12 tics for each year and a label at the
> > > > beginning of each quarter of the year: 2006.1, 2006.4,2006.7.
> > > >  - OR -
> > > > more elegantly, I would like to have the 12 tics with the month
> > > > shortened labels: Jan, Feb, etc. and below, say June, one label for
> > > > the year.
> > > >
> > > > Please help.
> > > >
> > > > Ciao
> > > > Vittorio
> > > >


From ripley at stats.ox.ac.uk  Tue Dec  4 18:57:24 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 4 Dec 2007 17:57:24 +0000 (GMT)
Subject: [R] Is R portable?
In-Reply-To: <281383.37749.qm@web32815.mail.mud.yahoo.com>
References: <281383.37749.qm@web32815.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0712041737260.9274@gannet.stats.ox.ac.uk>

On Tue, 4 Dec 2007, John Kane wrote:

> I simply installed R onto a USB stick, downloaded my
> normal packages to it and it works fine under Windows.

Yes, on Windows, but

1) There are other OSes,

2) This didn't just happen: it needed some careful design, including some 
caching to make it run fast from a USB disk.


Unix-alike ports of R are not completely portable, as the path to R_HOME 
is encapsulated in the R and Rscript front ends.  So if on, say, Linux you 
want to plug in a USB disc then it will only work if you installed R to 
that USB disk mounted at the same location in the file system, or are 
prepared to edit the copies of the R script (which had therefore better be 
mounted read-write).

The standard MacOS build has standard paths encapsulated in many places.

>
>
> --- Roland Rau <roland.rproject at gmail.com> wrote:
>
>> Hi Tom,
>>
>> did you check the R for Windows FAQ?
>>
> http://cran.r-project.org/bin/windows/base/rw-FAQ.html#Can-I-run-R-from-a-CD-or-USB-drive_003f
>>
>> Hope this helps,
>> Roland
>>
>>
>> Tom Backer Johnsen wrote:
>>> Recently I came across an interesting web site:
>>> http://portableapps.com/.  The idea is simple,
>> this is software that
>>> is possible to install and run on some type of USB
>> memory, a stick or
>>> one of these hard disks.  I can think of a number
>> of situations where
>>> this could be handy.  In addition memory sticks
>> are getting cheaper
>>> and more powerful by the day.
>>>
>>> So:  Is it possible to run R off one of these
>> sticks?
>>>
>>> I am also informed that it is possible to run
>> Latex in this manner.
>>>
>>> Tom
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained,
>> reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained,
>> reproducible code.
>>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Roy.Mendelssohn at noaa.gov  Tue Dec  4 18:57:47 2007
From: Roy.Mendelssohn at noaa.gov (Roy Mendelssohn)
Date: Tue, 04 Dec 2007 09:57:47 -0800
Subject: [R] cannot install R-2.6.1 on Mac OS X 10.4.9 PowerPC G5
In-Reply-To: <0C8C3AF3-C104-45A2-A2EA-9F3D896B0D86@umn.edu>
References: <0C8C3AF3-C104-45A2-A2EA-9F3D896B0D86@umn.edu>
Message-ID: <6D488CDC-6401-432C-9EEF-E7A8B861F636@noaa.gov>

Hi Shelley:

Look at:

https://stat.ethz.ch/pipermail/r-devel/2007-December/047601.html

-Roy M.


On Dec 4, 2007, at 9:42 AM, shelley wrote:

> Hello:
>
> I tried to install the latest R2.6.1 on my Mac OS X machines. I have
> no problems installing it on my Mac OS X Tigr machine. But when I
> tried to install it on other Mac OS X 10.4.9 PowerPCs, I could not
> have the GNU Fortran and/or Tcl/Tk libraries installed. I always got
> the error message : There were errors installing the software, please
> try installing again.
>
> Does somebody have any idea what was going wrong? I had X11 installed
> on the machines, do I need to install other softwares before
> installing R?
>
> Thanks!
> -Shelley
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S.  
Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division	
Southwest Fisheries Science Center
1352 Lighthouse Avenue
Pacific Grove, CA 93950-2097

e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
voice: (831)-648-9029
fax: (831)-648-8440
www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."


From Roy.Mendelssohn at noaa.gov  Tue Dec  4 19:11:26 2007
From: Roy.Mendelssohn at noaa.gov (Roy Mendelssohn)
Date: Tue, 04 Dec 2007 10:11:26 -0800
Subject: [R] cannot install R-2.6.1 on Mac OS X 10.4.9 PowerPC G5
In-Reply-To: <6D488CDC-6401-432C-9EEF-E7A8B861F636@noaa.gov>
References: <0C8C3AF3-C104-45A2-A2EA-9F3D896B0D86@umn.edu>
	<6D488CDC-6401-432C-9EEF-E7A8B861F636@noaa.gov>
Message-ID: <9FA6768E-2C48-41FD-8416-8F1FE9C26CE6@noaa.gov>

Sorry, I copied the wrong link.   Look at:

https://stat.ethz.ch/pipermail/r-devel/2007-December/047605.html

-Roy M.

On Dec 4, 2007, at 9:57 AM, Roy Mendelssohn wrote:

> Hi Shelley:
>
> Look at:
>
> https://stat.ethz.ch/pipermail/r-devel/2007-December/047601.html
>
> -Roy M.
>
>
> On Dec 4, 2007, at 9:42 AM, shelley wrote:
>
>> Hello:
>>
>> I tried to install the latest R2.6.1 on my Mac OS X machines. I have
>> no problems installing it on my Mac OS X Tigr machine. But when I
>> tried to install it on other Mac OS X 10.4.9 PowerPCs, I could not
>> have the GNU Fortran and/or Tcl/Tk libraries installed. I always got
>> the error message : There were errors installing the software, please
>> try installing again.
>>
>> Does somebody have any idea what was going wrong? I had X11 installed
>> on the machines, do I need to install other softwares before
>> installing R?
>>
>> Thanks!
>> -Shelley
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> **********************
> "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division	
> Southwest Fisheries Science Center
> 1352 Lighthouse Avenue
> Pacific Grove, CA 93950-2097
>
> e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
> voice: (831)-648-9029
> fax: (831)-648-8440
> www: http://www.pfeg.noaa.gov/
>
> "Old age and treachery will overcome youth and skill."
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S.  
Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division	
Southwest Fisheries Science Center
1352 Lighthouse Avenue
Pacific Grove, CA 93950-2097

e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
voice: (831)-648-9029
fax: (831)-648-8440
www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."


From h.wickham at gmail.com  Tue Dec  4 19:14:37 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 4 Dec 2007 12:14:37 -0600
Subject: [R] Multiple stacked barplots on the same graph?
In-Reply-To: <470371.39045.qm@web32805.mail.mud.yahoo.com>
References: <47557529.3050806@unicas.it>
	<470371.39045.qm@web32805.mail.mud.yahoo.com>
Message-ID: <f8e6ff050712041014m772f429fo76c0e193b82b08df@mail.gmail.com>

On Dec 4, 2007 11:19 AM, John Kane <jrkrideau at yahoo.ca> wrote:
>
> --- Domenico Vistocco <vistocco at unicas.it> wrote:
>
> > Perhaps this could be useful:
> >  > x=scan()
> > 11.81 10.51  1.95  2.08  2.51  2.05  1.98  0.63
> > 0.17  0.20
> > 12.49 13.56 2.81  3.13  4.58  0.70  0.85  0.22  0.06
> >  0.03
> >
> >  > x=matrix(x,5,4,byrow=T)
> >  > rownames(x)=paste("comp",1:5,sep="")
> >  > colnames(x)=paste("c",1:4,sep="")
> >
> >  > library(ggplot2)
> >  > dfm=melt(x)
> >  >
> >
> qplot(as.factor(x=X1),y=value,geom="histogram",data=dfm,fill=X2)
> >
> > domenico vistocco
>
> Very nice but bit garish. :)
>
> What about a dotchart instead?
>
>  dotchart(x, labels=rownames(x),col=c(1:4), pch=16)

which would be (roughly equivalent to)

qplot(X2, value, data=dfm, colour=X2, facets = . ~ X1)

in ggplot.

Hadley

-- 
http://had.co.nz/


From h.wickham at gmail.com  Tue Dec  4 19:15:51 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 4 Dec 2007 12:15:51 -0600
Subject: [R] Multiple stacked barplots on the same graph?
In-Reply-To: <475581AD.9030000@genoscope.cns.fr>
References: <47555E3C.9030301@genoscope.cns.fr> <47557529.3050806@unicas.it>
	<475581AD.9030000@genoscope.cns.fr>
Message-ID: <f8e6ff050712041015u2cc3aa75x50f343649efcadae@mail.gmail.com>

On Dec 4, 2007 10:34 AM, St?phane CRUVEILLER <scruveil at genoscope.cns.fr> wrote:
> Hi,
>
> I tried this method but it seems that there is something wrong with my
> data frame:
>
>
> when I type in:
>
>  > qplot(x=as.factor(Categorie),y=Total,data=mydata)
>
> It displays a graph with 2 points in each category...
> but if  I add the parameter geom="histogram"
>
>  > qplot(x=as.factor(Categorie),y=Total,data=mydata,geom="histogram")
>
>
> Error in storage.mode(test) <- "logical" :
>         object "y" not found
>
> any hint about this...

Could you copy and paste the output of dput(mydata) ?

(And I'd probably write the plot call as: qplot(Categorie, Total,
data=mydata, geom="bar"), since it is a bar plot, not a histogram)

-- 
http://had.co.nz/


From nrouraonline at gmail.com  Tue Dec  4 19:24:26 2007
From: nrouraonline at gmail.com (=?UTF-8?Q?N=C3=BAria_Roura?=)
Date: Tue, 4 Dec 2007 10:24:26 -0800 (PST)
Subject: [R] How can I use Adehabitat to obtain an .asc object with the
 predicted values of a niche model?
In-Reply-To: <4755905B.3060100@biomserv.univ-lyon1.fr>
References: <14155047.post@talk.nabble.com>
	<4755905B.3060100@biomserv.univ-lyon1.fr>
Message-ID: <14156533.post@talk.nabble.com>


Dear Thibaut,

Thank you very much. I will post the message in that website. 

Cheers,

N?ria



Thibaut Jombart wrote:
> 
> N?ria Roura wrote:
> 
>>Dear all,
>>I'm using the package adehabitat in R to import several .asc
files(=matrix), and also create a kasc object (=dataframe) with all of them.
>>The main idea is to use this kasc object to map the predicted values
ofclimate-matching model for an overall area. However, I don't know how
toproceed: Do I have to project the model (already stored in R) onto the
kascobject directly, or convert the kasc object to a data frame where each
rowrefers to a pixel?
>>On the other hand, I would also like to know how to obtain the
XYcoordinates for each row of the kasc object. 
>>Thank you very much. Cheers,
>>N?ria Roura
>>-- View this message in context:
http://www.nabble.com/How-can-I-use-Adehabitat-to-obtain-an-.asc-object-with-the-predicted-values-of-a-niche-model--tf4944431.html#a14155047Sent
from the R help mailing list archive at Nabble.com.
>>______________________________________________R-help at r-project.org mailing
listhttps://stat.ethz.ch/mailman/listinfo/r-helpPLEASE do read the posting
guide http://www.R-project.org/posting-guide.htmland provide commented,
minimal, self-contained, reproducible code.
>>
>>
>>  
>>
> Dear Nuria,
> 
> The mailing list devoted to adehabitat may be a better place for your 
> post. See:
> 
> http://www.faunalia.com/cgi-bin/mailman/listinfo/animov
> 
> Regards,
> 
> Thibaut.
> -- 
> ######################################
> Thibaut JOMBART
> CNRS UMR 5558 - Laboratoire de Biom?trie et Biologie Evolutive
> Universite Lyon 1
> 43 bd du 11 novembre 1918
> 69622 Villeurbanne Cedex
> T?l. : 04.72.43.29.35
> Fax : 04.72.43.13.88
> jombart at biomserv.univ-lyon1.fr
> http://lbbe.univ-lyon1.fr/-Jombart-Thibaut-.html?lang=en
> http://pbil.univ-lyon1.fr/software/adegenet/
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/How-can-I-use-Adehabitat-to-obtain-an-.asc-object-with-the-predicted-values-of-a-niche-model--tf4944431.html#a14156533
Sent from the R help mailing list archive at Nabble.com.


From h.wickham at gmail.com  Tue Dec  4 19:26:29 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 4 Dec 2007 12:26:29 -0600
Subject: [R] color palette from red to blue passing white
In-Reply-To: <18261.5726.555747.583528@stat.math.ethz.ch>
References: <644e1f320712031752k27ac3b14l9b5a4e6e075b799e@mail.gmail.com>
	<Pine.LNX.4.44.0712040504230.6948-100000@disco.wu-wien.ac.at>
	<18261.5726.555747.583528@stat.math.ethz.ch>
Message-ID: <f8e6ff050712041026g1fa1fe12wcd794622370c66e4@mail.gmail.com>

> Hmm,  I would have recommended
>
>   colorRampPalette(c('dark red','white','dark blue'),
>                    space = "Lab")
>
> where the 'space = "Lab"' part also makes sure that a
> "perceptually-based" space rather than RGB is used.
>
> I think the functions colorRamp() and (even more)
> colorRampPalette()  are very nice, part of "standard R" and
> still not known and used enough.

Well I use them in ggplot2 :)  Unfortunately I've noticed that space =
"Lab" is much slower than space = "RGB" which is why I don't use Lab
space as a default.  And it also ignores alpha values, which is a bit
of a pain.

> Note that they are based on 'convertColor()' and other color
> space functionality in R all of which deserve more usage
> in my oppinion and also in my own code ! ;-)

I find colour manipulation generally painful - all the really hard
stuff is there (i.e. conversion between colour spaces), but convenient
functions are lacking.   For example, I have this alpha function in
ggplot:

alpha <- function(colour, alpha) {
  col <- col2rgb(colour, TRUE) / 255
  col[4, ] <- rep(alpha, length(colour))

  new_col <- rgb(col[1,], col[2,], col[3,], col[4,])
  new_col[is.na(colour)] <- NA
  new_col
}

which seems like a lot of work for a simple task.  The fact that
col2rgb and rgb aren't symmetric is frustrating, especially since one
outputs values between 0 and 255 and the other (by default) accepts
inputs between 0 and 1.

> Package 'vcd' (and others) use package 'colorspace',
> and I have wondered in the past if these color space computations
> should not be merged into to standard R (package 'grDevices').
> But that's really a topic for another thread, on R-devel, not R-help..

It would definitely be nice if all colour space manipulations were
merged into a single package, with a consistent interface.  I would
happily contribute to such a project, since I do a lot of colour
manipulation in ggplot.

Hadley

-- 
http://had.co.nz/


From wang0174 at umn.edu  Tue Dec  4 19:21:57 2007
From: wang0174 at umn.edu (shelley)
Date: Tue, 4 Dec 2007 12:21:57 -0600
Subject: [R] cannot install R-2.6.1 on Mac OS X 10.4.9 PowerPC G5
In-Reply-To: <9FA6768E-2C48-41FD-8416-8F1FE9C26CE6@noaa.gov>
References: <0C8C3AF3-C104-45A2-A2EA-9F3D896B0D86@umn.edu>
	<6D488CDC-6401-432C-9EEF-E7A8B861F636@noaa.gov>
	<9FA6768E-2C48-41FD-8416-8F1FE9C26CE6@noaa.gov>
Message-ID: <E996EEB2-3795-488C-BA32-143E48B471A4@umn.edu>

Hi Roy:

Thank you for your quick response! I checked the softlink and I did  
see: /usr/local/ -> /private/var/automount/usr/local

 From the post I read: "I'm not quite sure what to do with this".  
What does it mean? It means there is no way to install the Tcl/Tk  
libraries on my powerPCs? I want to use it.

-Shelley


On Dec 4, 2007, at 12:11 PM, Roy Mendelssohn wrote:

> Sorry, I copied the wrong link.   Look at:
>
> https://stat.ethz.ch/pipermail/r-devel/2007-December/047605.html
>
> -Roy M.
>
> On Dec 4, 2007, at 9:57 AM, Roy Mendelssohn wrote:
>
>> Hi Shelley:
>>
>> Look at:
>>
>> https://stat.ethz.ch/pipermail/r-devel/2007-December/047601.html
>>
>> -Roy M.
>>
>>
>> On Dec 4, 2007, at 9:42 AM, shelley wrote:
>>
>>> Hello:
>>>
>>> I tried to install the latest R2.6.1 on my Mac OS X machines. I have
>>> no problems installing it on my Mac OS X Tigr machine. But when I
>>> tried to install it on other Mac OS X 10.4.9 PowerPCs, I could not
>>> have the GNU Fortran and/or Tcl/Tk libraries installed. I always got
>>> the error message : There were errors installing the software,  
>>> please
>>> try installing again.
>>>
>>> Does somebody have any idea what was going wrong? I had X11  
>>> installed
>>> on the machines, do I need to install other softwares before
>>> installing R?
>>>
>>> Thanks!
>>> -Shelley
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> **********************
>> "The contents of this message do not reflect any position of the U.S.
>> Government or NOAA."
>> **********************
>> Roy Mendelssohn
>> Supervisory Operations Research Analyst
>> NOAA/NMFS
>> Environmental Research Division	
>> Southwest Fisheries Science Center
>> 1352 Lighthouse Avenue
>> Pacific Grove, CA 93950-2097
>>
>> e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
>> voice: (831)-648-9029
>> fax: (831)-648-8440
>> www: http://www.pfeg.noaa.gov/
>>
>> "Old age and treachery will overcome youth and skill."
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> **********************
> "The contents of this message do not reflect any position of the  
> U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division	
> Southwest Fisheries Science Center
> 1352 Lighthouse Avenue
> Pacific Grove, CA 93950-2097
>
> e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
> voice: (831)-648-9029
> fax: (831)-648-8440
> www: http://www.pfeg.noaa.gov/
>
> "Old age and treachery will overcome youth and skill."
>
>
>


From daniel at umd.edu  Tue Dec  4 19:30:36 2007
From: daniel at umd.edu (Daniel Malter)
Date: Tue, 4 Dec 2007 13:30:36 -0500
Subject: [R] How can I use the rho value in the cor.test() summary?
In-Reply-To: <475510D7.2010103@biostat.ku.dk>
Message-ID: <200712041829.CIL37122@md2.mail.umd.edu>

My apologies, I was wrong with z$p.value. That is not what you wanted. I
should not write email at 3:30 :) But I think z$statistic as suggested by
Peter is not it either. You said you want the rho. The code for it is
z$estimate, assuming that you used method="spearman" to get a rho . Please
look below. It is othervariable3 that you want. So to get it (hopefully)
fully right once:

x=c(1,2,3,4,5,6,7,8,9)
y=c(3,5,4,6,7,8,8,7,10)

z=cor.test(x,y,method="spearman")

z
othervariable=z$p.value
othervariable2=z$statistic
othervariable3=z$estimate

You can get help for the function by typing ?cor.test . This shows you the
available values returned by cor.test that you can easily assign to other
variables.


-------------------------
cuncta stricte discussurus
-------------------------

-----Urspr?ngliche Nachricht-----
Von: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Gesendet: Tuesday, December 04, 2007 3:33 AM
An: Daniel Malter
Cc: r-help at r-project.org
Betreff: Re: [R] How can I use the rho value in the cor.test() summary?

Daniel Malter wrote:
> x=c(1,2,3,4,5,6,7,8,9)
> y=c(3,5,4,6,7,8,8,7,10)
>
> z=cor.test(x,y)
>
> othervariable=z$p.value
>   
, more likely.

> Cheers,
> Daniel
>
> -------------------------
> cuncta stricte discussurus
> -------------------------
>
> -----Urspr?ngliche Nachricht-----
> Von: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] Im Auftrag von Hayes
> Gesendet: Tuesday, December 04, 2007 2:11 AM
> An: r-help at r-project.org
> Betreff: [R] How can I use the rho value in the cor.test() summary?
>
>
> I want to give the "rho" value below to another variable.How ?
>
>   
>> Spearman's rank correlation rho
>>
>>
>>
>> data:  a[, 3] and a[, 2]
>>
>> S = 22, p-value = 0.001174
>>
>> alternative hypothesis: true rho is not equal to 0
>>
>> sample estimates:
>>
>>       rho
>>
>> 0.8666667
>>     
> --
> View this message in context:
> http://www.nabble.com/How-can-I-use-the-rho-value-in-the-cor.test%28%2
> 9-summ
> ary--tf4941501.html#a14145737
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Roy.Mendelssohn at noaa.gov  Tue Dec  4 20:03:51 2007
From: Roy.Mendelssohn at noaa.gov (Roy Mendelssohn)
Date: Tue, 04 Dec 2007 11:03:51 -0800
Subject: [R] cannot install R-2.6.1 on Mac OS X 10.4.9 PowerPC G5
In-Reply-To: <E996EEB2-3795-488C-BA32-143E48B471A4@umn.edu>
References: <0C8C3AF3-C104-45A2-A2EA-9F3D896B0D86@umn.edu>
	<6D488CDC-6401-432C-9EEF-E7A8B861F636@noaa.gov>
	<9FA6768E-2C48-41FD-8416-8F1FE9C26CE6@noaa.gov>
	<E996EEB2-3795-488C-BA32-143E48B471A4@umn.edu>
Message-ID: <CC6B3600-31CC-414F-8A7F-EFB34B322E82@noaa.gov>

Look to see if you have a link in /usr/local that links /usr/local/ib  
to  "/usr/local/lib 1".  it is that which is causing the problems.   
In the link below, Simon gives 2 commands that remove the link and  
renames the library location.  Then the install works.

-Roy

On Dec 4, 2007, at 10:21 AM, shelley wrote:

> Hi Roy:
>
> Thank you for your quick response! I checked the softlink and I did
> see: /usr/local/ -> /private/var/automount/usr/local
>
>  From the post I read: "I'm not quite sure what to do with this".
> What does it mean? It means there is no way to install the Tcl/Tk
> libraries on my powerPCs? I want to use it.
>
> -Shelley
>
>
> On Dec 4, 2007, at 12:11 PM, Roy Mendelssohn wrote:
>
>> Sorry, I copied the wrong link.   Look at:
>>
>> https://stat.ethz.ch/pipermail/r-devel/2007-December/047605.html
>>
>> -Roy M.
>>
>> On Dec 4, 2007, at 9:57 AM, Roy Mendelssohn wrote:
>>
>>> Hi Shelley:
>>>
>>> Look at:
>>>
>>> https://stat.ethz.ch/pipermail/r-devel/2007-December/047601.html
>>>
>>> -Roy M.
>>>
>>>
>>> On Dec 4, 2007, at 9:42 AM, shelley wrote:
>>>
>>>> Hello:
>>>>
>>>> I tried to install the latest R2.6.1 on my Mac OS X machines. I  
>>>> have
>>>> no problems installing it on my Mac OS X Tigr machine. But when I
>>>> tried to install it on other Mac OS X 10.4.9 PowerPCs, I could not
>>>> have the GNU Fortran and/or Tcl/Tk libraries installed. I always  
>>>> got
>>>> the error message : There were errors installing the software,
>>>> please
>>>> try installing again.
>>>>
>>>> Does somebody have any idea what was going wrong? I had X11
>>>> installed
>>>> on the machines, do I need to install other softwares before
>>>> installing R?
>>>>
>>>> Thanks!
>>>> -Shelley
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>> guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> **********************
>>> "The contents of this message do not reflect any position of the  
>>> U.S.
>>> Government or NOAA."
>>> **********************
>>> Roy Mendelssohn
>>> Supervisory Operations Research Analyst
>>> NOAA/NMFS
>>> Environmental Research Division	
>>> Southwest Fisheries Science Center
>>> 1352 Lighthouse Avenue
>>> Pacific Grove, CA 93950-2097
>>>
>>> e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
>>> voice: (831)-648-9029
>>> fax: (831)-648-8440
>>> www: http://www.pfeg.noaa.gov/
>>>
>>> "Old age and treachery will overcome youth and skill."
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> **********************
>> "The contents of this message do not reflect any position of the
>> U.S. Government or NOAA."
>> **********************
>> Roy Mendelssohn
>> Supervisory Operations Research Analyst
>> NOAA/NMFS
>> Environmental Research Division	
>> Southwest Fisheries Science Center
>> 1352 Lighthouse Avenue
>> Pacific Grove, CA 93950-2097
>>
>> e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
>> voice: (831)-648-9029
>> fax: (831)-648-8440
>> www: http://www.pfeg.noaa.gov/
>>
>> "Old age and treachery will overcome youth and skill."
>>
>>
>>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S.  
Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division	
Southwest Fisheries Science Center
1352 Lighthouse Avenue
Pacific Grove, CA 93950-2097

e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
voice: (831)-648-9029
fax: (831)-648-8440
www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."


From wang0174 at umn.edu  Tue Dec  4 20:23:45 2007
From: wang0174 at umn.edu (shelley)
Date: Tue, 4 Dec 2007 13:23:45 -0600
Subject: [R] cannot install R-2.6.1 on Mac OS X 10.4.9 PowerPC G5
In-Reply-To: <CC6B3600-31CC-414F-8A7F-EFB34B322E82@noaa.gov>
References: <0C8C3AF3-C104-45A2-A2EA-9F3D896B0D86@umn.edu>
	<6D488CDC-6401-432C-9EEF-E7A8B861F636@noaa.gov>
	<9FA6768E-2C48-41FD-8416-8F1FE9C26CE6@noaa.gov>
	<E996EEB2-3795-488C-BA32-143E48B471A4@umn.edu>
	<CC6B3600-31CC-414F-8A7F-EFB34B322E82@noaa.gov>
Message-ID: <D6197833-3A1A-4945-9D1D-1A3C129609C3@umn.edu>

Hi Roy:

I don't know where Simon found the "/usr/local/lib 1". I checked  
everywhere but could not find it. There is a softlink links "/usr/ 
bin/ to "/private/var/automount/usr/local", but no softlink links "/ 
usr/bin/local" to other place.

###########
lrwxr-xr-x     1 root  wheel     27 Dec  3 12:18 local -> /automount/ 
static/usr/local
drwxr-xr-x    2 swlocal  lab      68 Jan 16  2004 lib
###########

-Shelley

On Dec 4, 2007, at 1:03 PM, Roy Mendelssohn wrote:

> Look to see if you have a link in /usr/local that links /usr/local/ 
> ib to  "/usr/local/lib 1".  it is that which is causing the  
> problems.  In the link below, Simon gives 2 commands that remove  
> the link and renames the library location.  Then the install works.
>
> -Roy
>
> On Dec 4, 2007, at 10:21 AM, shelley wrote:
>
>> Hi Roy:
>>
>> Thank you for your quick response! I checked the softlink and I did
>> see: /usr/local/ -> /private/var/automount/usr/local
>>
>>  From the post I read: "I'm not quite sure what to do with this".
>> What does it mean? It means there is no way to install the Tcl/Tk
>> libraries on my powerPCs? I want to use it.
>>
>> -Shelley
>>
>>
>> On Dec 4, 2007, at 12:11 PM, Roy Mendelssohn wrote:
>>
>>> Sorry, I copied the wrong link.   Look at:
>>>
>>> https://stat.ethz.ch/pipermail/r-devel/2007-December/047605.html
>>>
>>> -Roy M.
>>>
>>> On Dec 4, 2007, at 9:57 AM, Roy Mendelssohn wrote:
>>>
>>>> Hi Shelley:
>>>>
>>>> Look at:
>>>>
>>>> https://stat.ethz.ch/pipermail/r-devel/2007-December/047601.html
>>>>
>>>> -Roy M.
>>>>
>>>>
>>>> On Dec 4, 2007, at 9:42 AM, shelley wrote:
>>>>
>>>>> Hello:
>>>>>
>>>>> I tried to install the latest R2.6.1 on my Mac OS X machines. I  
>>>>> have
>>>>> no problems installing it on my Mac OS X Tigr machine. But when I
>>>>> tried to install it on other Mac OS X 10.4.9 PowerPCs, I could not
>>>>> have the GNU Fortran and/or Tcl/Tk libraries installed. I  
>>>>> always got
>>>>> the error message : There were errors installing the software,
>>>>> please
>>>>> try installing again.
>>>>>
>>>>> Does somebody have any idea what was going wrong? I had X11
>>>>> installed
>>>>> on the machines, do I need to install other softwares before
>>>>> installing R?
>>>>>
>>>>> Thanks!
>>>>> -Shelley
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>>> guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>> **********************
>>>> "The contents of this message do not reflect any position of the  
>>>> U.S.
>>>> Government or NOAA."
>>>> **********************
>>>> Roy Mendelssohn
>>>> Supervisory Operations Research Analyst
>>>> NOAA/NMFS
>>>> Environmental Research Division	
>>>> Southwest Fisheries Science Center
>>>> 1352 Lighthouse Avenue
>>>> Pacific Grove, CA 93950-2097
>>>>
>>>> e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
>>>> voice: (831)-648-9029
>>>> fax: (831)-648-8440
>>>> www: http://www.pfeg.noaa.gov/
>>>>
>>>> "Old age and treachery will overcome youth and skill."
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>> guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> **********************
>>> "The contents of this message do not reflect any position of the
>>> U.S. Government or NOAA."
>>> **********************
>>> Roy Mendelssohn
>>> Supervisory Operations Research Analyst
>>> NOAA/NMFS
>>> Environmental Research Division	
>>> Southwest Fisheries Science Center
>>> 1352 Lighthouse Avenue
>>> Pacific Grove, CA 93950-2097
>>>
>>> e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
>>> voice: (831)-648-9029
>>> fax: (831)-648-8440
>>> www: http://www.pfeg.noaa.gov/
>>>
>>> "Old age and treachery will overcome youth and skill."
>>>
>>>
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> **********************
> "The contents of this message do not reflect any position of the  
> U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division	
> Southwest Fisheries Science Center
> 1352 Lighthouse Avenue
> Pacific Grove, CA 93950-2097
>
> e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
> voice: (831)-648-9029
> fax: (831)-648-8440
> www: http://www.pfeg.noaa.gov/
>
> "Old age and treachery will overcome youth and skill."
>
>
>


From bolker at ufl.edu  Tue Dec  4 20:57:09 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 4 Dec 2007 11:57:09 -0800 (PST)
Subject: [R] comparison of two vectors
In-Reply-To: <4755221A.8010700@stepputtis.net>
References: <14129032.post@talk.nabble.com> <14129523.post@talk.nabble.com>
	<14130713.post@talk.nabble.com> <14140387.post@talk.nabble.com>
	<4755221A.8010700@stepputtis.net>
Message-ID: <14158136.post@talk.nabble.com>




Daniel Stepputtis wrote:
> 
> Dear Ben,
> I was searching for the same problem. Thank you very much, it helped me a
> lot and I will use it quite often!
> 
> In addition to the problem given by tintin_et_milou. I have to compare a
> two pairs of vectors.
> 
> I.e. I have two datasets each with latitude and longitude (which defines
> the geographical position of data points.)
> As you might imagine, it is meaningful to take into account both latitude
> and longitude, when searching for the nearest data point from the otehr
> dataset.
> 
> Do you have any idea how to do this efficiently (actually I used loops ;-(
> ?
> Best regards
> Daniel
> 
> 
> Ben Bolker schrieb:
>> 
>> distfun <- function(x1,x2) { (x1-x2)^2 }
>> 
>> outer(m1[,1],y,distfun)
> 
>>   cheers
>>     Ben
> 

  Maybe (untested)

  distfun <- function(i,j) { (x1[i]-x2[j])^2 + (y1[i]-y2[j])^2) }
outer(i,j,distfun)

  ?
-- 
View this message in context: http://www.nabble.com/comparison-of-two-vectors-tf4936213.html#a14158136
Sent from the R help mailing list archive at Nabble.com.


From kzembowe at jhuccp.org  Tue Dec  4 21:15:07 2007
From: kzembowe at jhuccp.org (Zembower, Kevin)
Date: Tue, 4 Dec 2007 15:15:07 -0500
Subject: [R] Learning to do randomized block design analysis
Message-ID: <2E8AE992B157C0409B18D0225D0B476306CD91FB@XCH-VN01.sph.ad.jhsph.edu>

We just studied randomized block design analysis in my statistics class,
and I'm trying to learn how to do them in R. I'm trying to duplicate a
case study example from my textbook [1]:

> # Case Study 13.2.1, page 778
> cd <- c(8, 11, 9, 16, 24)
> dp <- c(2, 1, 12, 11, 19)
> lm <- c(-2, 0, 6, 2, 11)
>  table <- data.frame(Block=LETTERS[1:5], "Score changes"=c(cd, dp,
lm), Therapy=rep(c("Contact Desensitisztion", "Demonstration
Participation", "Live Modeling"), each=5))
> table
   Block Score.changes                     Therapy
1      A             8     Contact Desensitisztion
2      B            11     Contact Desensitisztion
3      C             9     Contact Desensitisztion
4      D            16     Contact Desensitisztion
5      E            24     Contact Desensitisztion
6      A             2 Demonstration Participation
7      B             1 Demonstration Participation
8      C            12 Demonstration Participation
9      D            11 Demonstration Participation
10     E            19 Demonstration Participation
11     A            -2               Live Modeling
12     B             0               Live Modeling
13     C             6               Live Modeling
14     D             2               Live Modeling
15     E            11               Live Modeling
> model.aov <- aov(Score.changes ~ Therapy + Error(Block), data=table)
> summary(model.aov)

Error: Block
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals  4  438.0   109.5               

Error: Within
          Df Sum Sq Mean Sq F value   Pr(>F)   
Therapy    2 260.93  130.47  15.259 0.001861 **
Residuals  8  68.40    8.55                    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>

I don't understand why R doesn't output a value for F and Pr for the
Error (Block) dimension, as my textbook shows 12.807 and 0.0015
respectively. All the other numbers match. Can these two values be
recovered? Also, my text shows a total line which R omits. Is this
because it's not particularly useful?

Thanks for your suggestions and advice. Also, if I'm executing this type
of problem in R inefficiently, I'd appreciate suggestions.

-Kevin

[1] An Introduction to Mathematical Statistics and Its Applications,
Larsen and Marx, fourth edition.

Kevin Zembower
Internet Services Group manager
Center for Communication Programs
Bloomberg School of Public Health
Johns Hopkins University
111 Market Place, Suite 310
Baltimore, Maryland  21202
410-659-6139 


From tura at centroin.com.br  Tue Dec  4 21:26:56 2007
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Tue, 4 Dec 2007 19:26:56 -0100
Subject: [R] Is R portable?
In-Reply-To: <281383.37749.qm@web32815.mail.mud.yahoo.com>
References: <47558834.8000900@gmail.com>
	<281383.37749.qm@web32815.mail.mud.yahoo.com>
Message-ID: <20071204202417.M50422@centroin.com.br>


Well, yesterday I put a linux version of R 2.6.0 in a USB stick of 2Gb and it
runs very well...

Bernardo Rangel Tura, MD,MPH,Phd
National Cardiology Institute



---------- Original Message -----------
From: John Kane <jrkrideau at yahoo.ca>
To: Roland Rau <roland.rproject at gmail.com>, Tom Backer Johnsen
<backer at psych.uib.no>Cc: 
Sent: Tue, 4 Dec 2007 12:22:36 -0500 (EST)
Subject: Re: [R] Is R portable?

> I simply installed R onto a USB stick, downloaded my
> normal packages to it and it works fine under Windows.
> 
> --- Roland Rau <roland.rproject at gmail.com> wrote:
> 
> > Hi Tom,
> > 
> > did you check the R for Windows FAQ?
> > 
> http://cran.r-project.org/bin/windows/base/rw-FAQ.html#Can-I-run-R-
> from-a-CD-or-USB-drive_003f
> > 
> > Hope this helps,
> > Roland
> > 
> > 
> > Tom Backer Johnsen wrote:
> > > Recently I came across an interesting web site: 
> > > http://portableapps.com/.  The idea is simple,
> > this is software that 
> > > is possible to install and run on some type of USB
> > memory, a stick or 
> > > one of these hard disks.  I can think of a number
> > of situations where 
> > > this could be handy.  In addition memory sticks
> > are getting cheaper 
> > > and more powerful by the day.
> > > 
> > > So:  Is it possible to run R off one of these
> > sticks?
> > > 
> > > I am also informed that it is possible to run
> > Latex in this manner.
> > > 
> > > Tom


From scionforbai at gmail.com  Tue Dec  4 21:38:01 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Tue, 4 Dec 2007 21:38:01 +0100
Subject: [R] 2/3d interpolation from a regular grid to another regular
	grid
In-Reply-To: <17EF7ABB-B4F8-4279-B9D0-620C02C91E84@gmail.com>
References: <17EF7ABB-B4F8-4279-B9D0-620C02C91E84@gmail.com>
Message-ID: <e9ee1f0a0712041238w4ff935fbg80f8f26895e02277@mail.gmail.com>

> - krigging in package fields, which also requires irregular spaced data

That kriging requires irregularly spaced data sounds new to me ;) It
cannot be, you misread something (I feel free to say that even if I
never used that package).
It can be tricky doing kriging, though, if you're not comfortable with
a little bit of geostatistics. You have to infer a variogram model for
each data set; you possibly run into non-stationarity or anisotropy,
which are indeed very well treated (maybe at best) by kriging in one
of its forms, but ... it takes more than this list to help you then;
basically kriging requires modelling, so it is often very difficult to
set up an automatic procedure. I can reccomend kriging if the spatial
variability of your data (compared to grid refinement) is quite
important.

In other simple cases, a wheighted mean using the (squared) inverse of
the distance as wheight and a spherical neighbourhood could be the
simpliest way to perform the interpolation.


From knoblauch at lyon.inserm.fr  Tue Dec  4 21:55:57 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Tue, 4 Dec 2007 20:55:57 +0000 (UTC)
Subject: [R] color palette from red to blue passing white
References: <644e1f320712031752k27ac3b14l9b5a4e6e075b799e@mail.gmail.com>
	<Pine.LNX.4.44.0712040504230.6948-100000@disco.wu-wien.ac.at>
	<18261.5726.555747.583528@stat.math.ethz.ch>
	<f8e6ff050712041026g1fa1fe12wcd794622370c66e4@mail.gmail.com>
Message-ID: <loom.20071204T204557-594@post.gmane.org>

hadley wickham <h.wickham <at> gmail.com> writes:
<<<pruned>>>
>For example, I have this alpha function in
> ggplot:
> alpha <- function(colour, alpha) {
>   col <- col2rgb(colour, TRUE) / 255
>   col[4, ] <- rep(alpha, length(colour))
>   new_col <- rgb(col[1,], col[2,], col[3,], col[4,])
>   new_col[is.na(colour)] <- NA
>   new_col
> }
> 
> which seems like a lot of work for a simple task.  The fact that
> col2rgb and rgb aren't symmetric is frustrating, especially since one
> outputs values between 0 and 255 and the other (by default) accepts
> inputs between 0 and 1.
> > Package 'vcd' (and others) use package 'colorspace',
> > and I have wondered in the past if these color space computations
> > should not be merged into to standard R (package 'grDevices').
> > But that's really a topic for another thread, on R-devel, not R-help..
> It would definitely be nice if all colour space manipulations were
> merged into a single package, with a consistent interface.  I would
> happily contribute to such a project, since I do a lot of colour
> manipulation in ggplot.
> 
> Hadley
> 
It's a good thought.
There is some thought going into standardizing, if 
you look at

http://developer.R-project.org/sRGB-RFC.html

and by the way, for a previous post, it's inaccurate to call Lab
"perceptual", although many people do.  It's based on discrimination data
in an attempt to make a uniform space for small color differences, 
but it is only approximately so.

best,

Ken

PS, apologies that gmane is making me prune some quoted text


From backer at psych.uib.no  Tue Dec  4 23:25:23 2007
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Tue, 04 Dec 2007 23:25:23 +0100
Subject: [R] Is R portable?
In-Reply-To: <Pine.LNX.4.64.0712041737260.9274@gannet.stats.ox.ac.uk>
References: <281383.37749.qm@web32815.mail.mud.yahoo.com>
	<Pine.LNX.4.64.0712041737260.9274@gannet.stats.ox.ac.uk>
Message-ID: <4755D3D3.3080700@psych.uib.no>

Prof Brian Ripley wrote:
> On Tue, 4 Dec 2007, John Kane wrote:
> 
>> I simply installed R onto a USB stick, downloaded my
>> normal packages to it and it works fine under Windows.
> 
> Yes, on Windows, but
> 
> 1) There are other OSes,
> 
> 2) This didn't just happen: it needed some careful design, including 
> some caching to make it run fast from a USB disk.

Nice to discover good planning.  Am I then correct in my
understanding: Installing R under Windows does not require any
registry entries, the installation is essentially to unpack the
necessary files in the correct directories?

Tom
> 
> 
> Unix-alike ports of R are not completely portable, as the path to R_HOME 
> is encapsulated in the R and Rscript front ends.  So if on, say, Linux 
> you want to plug in a USB disc then it will only work if you installed R 
> to that USB disk mounted at the same location in the file system, or are 
> prepared to edit the copies of the R script (which had therefore better 
> be mounted read-write).
> 
> The standard MacOS build has standard paths encapsulated in many places.
> 
>>
>>
>> --- Roland Rau <roland.rproject at gmail.com> wrote:
>>
>>> Hi Tom,
>>>
>>> did you check the R for Windows FAQ?
>>>
>> http://cran.r-project.org/bin/windows/base/rw-FAQ.html#Can-I-run-R-from-a-CD-or-USB-drive_003f 
>>
>>>
>>> Hope this helps,
>>> Roland
>>>
>>>
>>> Tom Backer Johnsen wrote:
>>>> Recently I came across an interesting web site:
>>>> http://portableapps.com/.  The idea is simple,
>>> this is software that
>>>> is possible to install and run on some type of USB
>>> memory, a stick or
>>>> one of these hard disks.  I can think of a number
>>> of situations where
>>>> this could be handy.  In addition memory sticks
>>> are getting cheaper
>>>> and more powerful by the day.
>>>>
>>>> So:  Is it possible to run R off one of these
>>> sticks?
>>>>
>>>> I am also informed that it is possible to run
>>> Latex in this manner.
>>>>
>>>> Tom
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained,
>>> reproducible code.
>>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained,
>>> reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 


-- 
+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+


From tlumley at u.washington.edu  Tue Dec  4 23:28:30 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 4 Dec 2007 14:28:30 -0800 (PST)
Subject: [R] weighted Cox proportional hazards regression
In-Reply-To: <fb240dd50712040500h6e28785fh24ce1933955f71d8@mail.gmail.com>
References: <fb240dd50712040500h6e28785fh24ce1933955f71d8@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0712041348170.11780@homer24.u.washington.edu>

On Tue, 4 Dec 2007, Scott Bartell wrote:

> I'm getting unexpected results from the coxph function when using
> weights from counter-matching.  For example, the following code
> produces a parameter estimate of -1.59 where I expect 0.63:

You can get the answer you want with

coxph(Surv(pseudotime, cc)~x+strata(riskset)+offset(log(wt)), data=d2, robust=TRUE)

which is how countermatching usually seems to be done, and is what the 
original paper by Langholz & Borgan recommends.

I think it's right that weight=wt doesn't do the same thing.  The weights 
are not simple inverse-probability sampling weights, because the sampling 
units in this design are pairs, not individuals.  If we assume the 
distribution of x is the same across risk sets (which looks approximately 
true in your data) then the sampling weight for a pair is proportional to 
the number of eligible controls: ie, just your control weight.  Using 
these as weights for the pairs in the weight= argument I get 0.585 as the 
hazard ratio, reasonably close to your 0.63 given that this is a different 
estimator and given the assumptions.

 	-thomas


> d2 = structure(list(x = c(1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
> 1, 0, 0, 1, 0, 1, 0, 1, 0, 1), wt = c(5, 42, 40, 4, 43, 4, 42,
> 4, 44, 5, 38, 4, 39, 4, 4, 37, 40, 4, 44, 5, 45, 5, 44, 5), riskset = c(1L,
> 1L, 4L, 4L, 6L, 6L, 12L, 12L, 13L, 13L, 19L, 19L, 23L, 23L, 31L,
> 31L, 42L, 42L, 45L, 45L, 70L, 70L, 93L, 93L), cc = c(1, 0, 1,
> 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0
> ), pseudotime = rep(1,24)), .Names = c("x", "wt", "riskset",
> "cc", "pseudotime"), class = "data.frame", row.names=1:24)
>
> coxph( Surv(pseudotime, cc) ~ x + strata(riskset), weights=wt,
> robust=T, method="breslow",data=d2)
>
> I'm expecting a value of about 0.63 to 0.64 based on the data source
> (simulated) and the following hand-coded MLE:
>
> negloglik = function(beta,dat) {
>  dat$wexb = dat$wt * exp(dat$x * beta)
>  agged = aggregate(dat$wexb,list(riskset=dat$riskset),sum)
>  names(agged)[2] = "denom"
>  dat = merge(dat[dat$cc==1,],agged,by="riskset")
>  -sum(log(dat$wexb)-log(dat$denom))
>  }
> nlm(negloglik,0,hessian=T,dat=d2)
>
> Am I misunderstanding the meaning of case weights in the coxph
> function?  The help file is pretty terse.
>
> Scott Bartell, PhD
> Assistant Professor
> Department of Epidemiology
> University of California, Irvine
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From taekyunk at gmail.com  Tue Dec  4 23:46:03 2007
From: taekyunk at gmail.com (T.K.)
Date: Tue, 4 Dec 2007 14:46:03 -0800
Subject: [R] Learning to do randomized block design analysis
In-Reply-To: <2E8AE992B157C0409B18D0225D0B476306CD91FB@XCH-VN01.sph.ad.jhsph.edu>
References: <2E8AE992B157C0409B18D0225D0B476306CD91FB@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <36923f1d0712041446v539fdb1eof377f7739e8a6d7d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/c274d7d7/attachment.pl 

From murdoch at stats.uwo.ca  Tue Dec  4 23:54:53 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 04 Dec 2007 17:54:53 -0500
Subject: [R] Is R portable?
In-Reply-To: <4755D3D3.3080700@psych.uib.no>
References: <281383.37749.qm@web32815.mail.mud.yahoo.com>	<Pine.LNX.4.64.0712041737260.9274@gannet.stats.ox.ac.uk>
	<4755D3D3.3080700@psych.uib.no>
Message-ID: <4755DABD.7010109@stats.uwo.ca>

On 04/12/2007 5:25 PM, Tom Backer Johnsen wrote:
> Prof Brian Ripley wrote:
>> On Tue, 4 Dec 2007, John Kane wrote:
>>
>>> I simply installed R onto a USB stick, downloaded my
>>> normal packages to it and it works fine under Windows.
>> Yes, on Windows, but
>>
>> 1) There are other OSes,
>>
>> 2) This didn't just happen: it needed some careful design, including 
>> some caching to make it run fast from a USB disk.
> 
> Nice to discover good planning.  Am I then correct in my
> understanding: Installing R under Windows does not require any
> registry entries, the installation is essentially to unpack the
> necessary files in the correct directories?

Yes.  It does record some information in the registry, but that's purely 
optional.  The only problem you're likely to run into is using external 
software with R, that might look in the registry to find where R was 
installed.

Duncan Murdoch


From gunter.berton at gene.com  Wed Dec  5 00:06:18 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Tue, 4 Dec 2007 15:06:18 -0800
Subject: [R] Learning to do randomized block design analysis
In-Reply-To: <36923f1d0712041446v539fdb1eof377f7739e8a6d7d@mail.gmail.com>
References: <2E8AE992B157C0409B18D0225D0B476306CD91FB@XCH-VN01.sph.ad.jhsph.edu>
	<36923f1d0712041446v539fdb1eof377f7739e8a6d7d@mail.gmail.com>
Message-ID: <00ae01c836ca$479a5710$3a0b2c0a@gne.windows.gene.com>

Let's be careful here. aov() treats block as a **random** error component of
variance.  lm() treats block as a **fixed effect**. That's a different
kettle of fish. Perhaps both Kevin and the authors of his textbook need to
read up on fixed versus random effects and what they mean -- and what sorts
of tests make sense for each.


Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of T.K.
Sent: Tuesday, December 04, 2007 2:46 PM
To: Zembower, Kevin
Cc: r-help at r-project.org
Subject: Re: [R] Learning to do randomized block design analysis

This seems to work.
The trick is to use 'lm' instead of 'aov'.

> model.aov <- lm(Score.changes ~ factor(Therapy) + factor(Block),
data=table)
> anova(model.aov)
Analysis of Variance Table

Response: Score.changes
                Df Sum Sq Mean Sq F value   Pr(>F)
factor(Therapy)  2 260.93  130.47  15.259 0.001861 **
factor(Block)    4 438.00  109.50  12.807 0.001484 **
Residuals        8  68.40    8.55
---
Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1



On Dec 4, 2007 12:15 PM, Zembower, Kevin <kzembowe at jhuccp.org> wrote:

> We just studied randomized block design analysis in my statistics class,
> and I'm trying to learn how to do them in R. I'm trying to duplicate a
> case study example from my textbook [1]:
>
> > # Case Study 13.2.1, page 778
> > cd <- c(8, 11, 9, 16, 24)
> > dp <- c(2, 1, 12, 11, 19)
> > lm <- c(-2, 0, 6, 2, 11)
> >  table <- data.frame(Block=LETTERS[1:5], "Score changes"=c(cd, dp,
> lm), Therapy=rep(c("Contact Desensitisztion", "Demonstration
> Participation", "Live Modeling"), each=5))
> > table
>   Block Score.changes                     Therapy
> 1      A             8     Contact Desensitisztion
> 2      B            11     Contact Desensitisztion
> 3      C             9     Contact Desensitisztion
> 4      D            16     Contact Desensitisztion
> 5      E            24     Contact Desensitisztion
> 6      A             2 Demonstration Participation
> 7      B             1 Demonstration Participation
> 8      C            12 Demonstration Participation
> 9      D            11 Demonstration Participation
> 10     E            19 Demonstration Participation
> 11     A            -2               Live Modeling
> 12     B             0               Live Modeling
> 13     C             6               Live Modeling
> 14     D             2               Live Modeling
> 15     E            11               Live Modeling
> > model.aov <- aov(Score.changes ~ Therapy + Error(Block), data=table)
> > summary(model.aov)
>
> Error: Block
>          Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  4  438.0   109.5
>
> Error: Within
>          Df Sum Sq Mean Sq F value   Pr(>F)
> Therapy    2 260.93  130.47  15.259 0.001861 **
> Residuals  8  68.40    8.55
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
>
> I don't understand why R doesn't output a value for F and Pr for the
> Error (Block) dimension, as my textbook shows 12.807 and 0.0015
> respectively. All the other numbers match. Can these two values be
> recovered? Also, my text shows a total line which R omits. Is this
> because it's not particularly useful?
>
> Thanks for your suggestions and advice. Also, if I'm executing this type
> of problem in R inefficiently, I'd appreciate suggestions.
>
> -Kevin
>
> [1] An Introduction to Mathematical Statistics and Its Applications,
> Larsen and Marx, fourth edition.
>
> Kevin Zembower
> Internet Services Group manager
> Center for Communication Programs
> Bloomberg School of Public Health
> Johns Hopkins University
> 111 Market Place, Suite 310
> Baltimore, Maryland  21202
> 410-659-6139
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
======================================
T.K. (Tae-kyun) Kim
Ph.D. student
Department of Marketing
Marshall School of Business
University of Southern California
======================================

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From taekyunk at gmail.com  Wed Dec  5 00:35:12 2007
From: taekyunk at gmail.com (T.K.)
Date: Tue, 4 Dec 2007 15:35:12 -0800
Subject: [R] Learning to do randomized block design analysis
In-Reply-To: <2E8AE992B157C0409B18D0225D0B476306CD91FB@XCH-VN01.sph.ad.jhsph.edu>
References: <2E8AE992B157C0409B18D0225D0B476306CD91FB@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <36923f1d0712041535j70ebfc14x5f93716e93afa83c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/ac03efe8/attachment.pl 

From michael_bibo at health.qld.gov.au  Wed Dec  5 00:50:08 2007
From: michael_bibo at health.qld.gov.au (Michael Bibo)
Date: Tue, 4 Dec 2007 23:50:08 +0000 (UTC)
Subject: [R] Is R portable?
References: <47557A3F.9070502@psych.uib.no>
Message-ID: <loom.20071204T234458-535@post.gmane.org>

Tom Backer Johnsen <backer <at> psych.uib.no> writes:

> 
> I am also informed that it is possible to run Latex in this manner.
> 

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/107419.html  refers to 

http://at-aka.blogspot.com/2006/06/portable-emacs-22050-on-usb.html

which can give you a portable emacs + auctex installation for windows.

Michael
Research Officer,
Queensland Health


From g.abraham at ms.unimelb.edu.au  Wed Dec  5 00:58:15 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Wed, 05 Dec 2007 10:58:15 +1100
Subject: [R] predict error for survreg with natural splines
Message-ID: <4755E997.4020907@ms.unimelb.edu.au>

Hi,

The following error looks like a bug to me but perhaps someone can shed 
light on it:

 > library(splines)
 > library(survival)
 > s <- survreg(Surv(futime, fustat) ~ ns(age, knots=c(50, 60)), 
data=ovarian)
 > n <- data.frame(age=rep(mean(ovarian$age), 10))
 > predict(s, newdata=n)
Error in qr.default(t(const)) :
   NA/NaN/Inf in foreign function call (arg 1)

Thanks,
Gad



 > sessionInfo()
R version 2.6.1 (2007-11-26)
i486-pc-linux-gnu

locale:
LC_CTYPE=en_AU.UTF-8;LC_NUMERIC=C;LC_TIME=en_AU.UTF-8;
LC_COLLATE=en_AU.UTF-8;LC_MONETARY=en_AU.UTF-8;LC_MESSAGES=en_AU.UTF-8;
LC_PAPER=en_AU.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;
LC_MEASUREMENT=en_AU.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] splines   stats     graphics  grDevices utils     datasets  methods
[8] base

other attached packages:
[1] survival_2.34

loaded via a namespace (and not attached):
[1] rcompgen_0.1-17



-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From m_olshansky at yahoo.com  Wed Dec  5 01:31:48 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Tue, 4 Dec 2007 16:31:48 -0800 (PST)
Subject: [R] predict error for survreg with natural splines
In-Reply-To: <4755E997.4020907@ms.unimelb.edu.au>
Message-ID: <144993.91851.qm@web32214.mail.mud.yahoo.com>

Hi Gad,

The problem is with ns:

> x <- ns(rnorm(100), knots=c(50, 60))
Error in qr.default(t(const)) : 
  NA/NaN/Inf in foreign function call (arg 1)

but the following is OK:

> x <- ns(rnorm(100))
> dim(x)
[1] 100   1

Regards,

Moshe.

--- Gad Abraham <g.abraham at ms.unimelb.edu.au> wrote:

> Hi,
> 
> The following error looks like a bug to me but
> perhaps someone can shed 
> light on it:
> 
>  > library(splines)
>  > library(survival)
>  > s <- survreg(Surv(futime, fustat) ~ ns(age,
> knots=c(50, 60)), 
> data=ovarian)
>  > n <- data.frame(age=rep(mean(ovarian$age), 10))
>  > predict(s, newdata=n)
> Error in qr.default(t(const)) :
>    NA/NaN/Inf in foreign function call (arg 1)
> 
> Thanks,
> Gad
> 
> 
> 
>  > sessionInfo()
> R version 2.6.1 (2007-11-26)
> i486-pc-linux-gnu
> 
> locale:
>
LC_CTYPE=en_AU.UTF-8;LC_NUMERIC=C;LC_TIME=en_AU.UTF-8;
>
LC_COLLATE=en_AU.UTF-8;LC_MONETARY=en_AU.UTF-8;LC_MESSAGES=en_AU.UTF-8;
>
LC_PAPER=en_AU.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;
> LC_MEASUREMENT=en_AU.UTF-8;LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] splines   stats     graphics  grDevices utils   
>  datasets  methods
> [8] base
> 
> other attached packages:
> [1] survival_2.34
> 
> loaded via a namespace (and not attached):
> [1] rcompgen_0.1-17
> 
> 
> 
> -- 
> Gad Abraham
> Department of Mathematics and Statistics
> The University of Melbourne
> Parkville 3010, Victoria, Australia
> email: g.abraham at ms.unimelb.edu.au
> web: http://www.ms.unimelb.edu.au/~gabraham
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From maj at stats.waikato.ac.nz  Wed Dec  5 01:35:32 2007
From: maj at stats.waikato.ac.nz (maj at stats.waikato.ac.nz)
Date: Wed, 5 Dec 2007 13:35:32 +1300 (NZDT)
Subject: [R] Dimension of a vector
Message-ID: <4433.203.173.149.245.1196814932.squirrel@webmail.scms.waikato.ac.nz>

Consider the following:
> A <- 1:10
> A
 [1]  1  2  3  4  5  6  7  8  9 10
> dim(A)
NULL
> dim(A) <- c(2,5)
> A
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    3    5    7    9
[2,]    2    4    6    8   10
> dim(A)
[1] 2 5
> dim(A) <- 10
> A
 [1]  1  2  3  4  5  6  7  8  9 10
> dim(A)
[1] 10

Would it not make sense to have dim(A) = length(A) for all vectors?

Murray
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From cberry at tajo.ucsd.edu  Wed Dec  5 01:36:49 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 4 Dec 2007 16:36:49 -0800
Subject: [R] predict error for survreg with natural splines
In-Reply-To: <4755E997.4020907@ms.unimelb.edu.au>
References: <4755E997.4020907@ms.unimelb.edu.au>
Message-ID: <Pine.LNX.4.64.0712041632470.19680@tajo.ucsd.edu>



I think coxph() leaves safe prediction off by default. You need to turn it 
on.

See

 	?predict.coxph

Chuck

On Wed, 5 Dec 2007, Gad Abraham wrote:

> Hi,
>
> The following error looks like a bug to me but perhaps someone can shed
> light on it:
>
> > library(splines)
> > library(survival)
> > s <- survreg(Surv(futime, fustat) ~ ns(age, knots=c(50, 60)),
> data=ovarian)
> > n <- data.frame(age=rep(mean(ovarian$age), 10))
> > predict(s, newdata=n)
> Error in qr.default(t(const)) :
>   NA/NaN/Inf in foreign function call (arg 1)
>
> Thanks,
> Gad
>
>
>
> > sessionInfo()
> R version 2.6.1 (2007-11-26)
> i486-pc-linux-gnu
>
> locale:
> LC_CTYPE=en_AU.UTF-8;LC_NUMERIC=C;LC_TIME=en_AU.UTF-8;
> LC_COLLATE=en_AU.UTF-8;LC_MONETARY=en_AU.UTF-8;LC_MESSAGES=en_AU.UTF-8;
> LC_PAPER=en_AU.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;
> LC_MEASUREMENT=en_AU.UTF-8;LC_IDENTIFICATION=C
>
> attached base packages:
> [1] splines   stats     graphics  grDevices utils     datasets  methods
> [8] base
>
> other attached packages:
> [1] survival_2.34
>
> loaded via a namespace (and not attached):
> [1] rcompgen_0.1-17
>
>
>
> -- 
> Gad Abraham
> Department of Mathematics and Statistics
> The University of Melbourne
> Parkville 3010, Victoria, Australia
> email: g.abraham at ms.unimelb.edu.au
> web: http://www.ms.unimelb.edu.au/~gabraham
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From ggrothendieck at gmail.com  Wed Dec  5 01:42:08 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 4 Dec 2007 19:42:08 -0500
Subject: [R] Dimension of a vector
In-Reply-To: <4433.203.173.149.245.1196814932.squirrel@webmail.scms.waikato.ac.nz>
References: <4433.203.173.149.245.1196814932.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <971536df0712041642v4acbfe00i24745dc50cd6bd31@mail.gmail.com>

In R, vectors don't have dimensions, arrays do.

> x <- c(1, 4, 5)
> class(x)
[1] "numeric"
> y <- array(x)
> class(y)
[1] "array"
> dim(x)
NULL
> dim(y)
[1] 3

On Dec 4, 2007 7:35 PM,  <maj at stats.waikato.ac.nz> wrote:
> Consider the following:
> > A <- 1:10
> > A
>  [1]  1  2  3  4  5  6  7  8  9 10
> > dim(A)
> NULL
> > dim(A) <- c(2,5)
> > A
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    1    3    5    7    9
> [2,]    2    4    6    8   10
> > dim(A)
> [1] 2 5
> > dim(A) <- 10
> > A
>  [1]  1  2  3  4  5  6  7  8  9 10
> > dim(A)
> [1] 10
>
> Would it not make sense to have dim(A) = length(A) for all vectors?
>
> Murray
> --
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cberry at tajo.ucsd.edu  Wed Dec  5 01:54:26 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 4 Dec 2007 16:54:26 -0800
Subject: [R] predict error for survreg with natural splines
In-Reply-To: <Pine.LNX.4.64.0712041632470.19680@tajo.ucsd.edu>
References: <4755E997.4020907@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0712041632470.19680@tajo.ucsd.edu>
Message-ID: <Pine.LNX.4.64.0712041644190.19680@tajo.ucsd.edu>

On Tue, 4 Dec 2007, Charles C. Berry wrote:

>
>
> I think coxph() leaves safe prediction off by default. You need to turn it 
> on.


Sorry for the misdirection, I see now that you are describing survreg() 
not coxph().

But even coxph() seems to barf on this.

A possible workaround:

> n <- data.frame(age=c(65,rep(mean(ovarian$age)), 10))
> predict(s, newdata=n)[-1]
[1] -0.4182318 -1.9911592
>

Chuck


>
> See
>
> 	?predict.coxph
>
> Chuck
>
> On Wed, 5 Dec 2007, Gad Abraham wrote:
>
>>  Hi,
>>
>>  The following error looks like a bug to me but perhaps someone can shed
>>  light on it:
>> 
>> >  library(splines)
>> >  library(survival)
>> >  s <- survreg(Surv(futime, fustat) ~ ns(age, knots=c(50, 60)),
>>  data=ovarian)
>> >  n <- data.frame(age=rep(mean(ovarian$age), 10))
>> >  predict(s, newdata=n)
>>  Error in qr.default(t(const)) :
>>    NA/NaN/Inf in foreign function call (arg 1)
>>
>>  Thanks,
>>  Gad
>> 
>> 
>> 
>> >  sessionInfo()
>>  R version 2.6.1 (2007-11-26)
>>  i486-pc-linux-gnu
>>
>>  locale:
>>  LC_CTYPE=en_AU.UTF-8;LC_NUMERIC=C;LC_TIME=en_AU.UTF-8;
>>  LC_COLLATE=en_AU.UTF-8;LC_MONETARY=en_AU.UTF-8;LC_MESSAGES=en_AU.UTF-8;
>>  LC_PAPER=en_AU.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;
>>  LC_MEASUREMENT=en_AU.UTF-8;LC_IDENTIFICATION=C
>>
>>  attached base packages:
>>  [1] splines   stats     graphics  grDevices utils     datasets  methods
>>  [8] base
>>
>>  other attached packages:
>>  [1] survival_2.34
>>
>>  loaded via a namespace (and not attached):
>>  [1] rcompgen_0.1-17
>> 
>> 
>>
>>  --
>>  Gad Abraham
>>  Department of Mathematics and Statistics
>>  The University of Melbourne
>>  Parkville 3010, Victoria, Australia
>>  email: g.abraham at ms.unimelb.edu.au
>>  web: http://www.ms.unimelb.edu.au/~gabraham
>>
>>  ______________________________________________
>>  R-help at r-project.org mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-help
>>  PLEASE do read the posting guide
>>  http://www.R-project.org/posting-guide.html
>>  and provide commented, minimal, self-contained, reproducible code.
>> 
>
> Charles C. Berry                            (858) 534-2098
>                                            Dept of Family/Preventive 
> Medicine
> E mailto:cberry at tajo.ucsd.edu	            UC San Diego
> http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901
>
>
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From cberry at tajo.ucsd.edu  Wed Dec  5 02:03:01 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 4 Dec 2007 17:03:01 -0800
Subject: [R] Another correction WAS: Re: predict error for survreg with
 natural splines
In-Reply-To: <Pine.LNX.4.64.0712041644190.19680@tajo.ucsd.edu>
References: <4755E997.4020907@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0712041632470.19680@tajo.ucsd.edu>
	<Pine.LNX.4.64.0712041644190.19680@tajo.ucsd.edu>
Message-ID: <Pine.LNX.4.64.0712041656480.19680@tajo.ucsd.edu>

On Tue, 4 Dec 2007, Charles C. Berry wrote:

> On Tue, 4 Dec 2007, Charles C. Berry wrote:
>
>> 
>>
>>  I think coxph() leaves safe prediction off by default. You need to turn it
>>  on.
>
>
> Sorry for the misdirection, I see now that you are describing survreg() not 
> coxph().
>
> But even coxph() seems to barf on this.
>
> A possible workaround:
>
>>  n <- data.frame(age=c(65,rep(mean(ovarian$age)), 10))
>>  predict(s, newdata=n)[-1]
> [1] -0.4182318 -1.9911592

This was wrong. It is not invariant to the choice of the discarded value.

I guess it is time to study up on safe prediction. It is obvious I am 
missing something.

Sorry again,

Chuck

>> 
>
> Chuck
>
>
>>
>>  See
>>
>>   ?predict.coxph
>>
>>  Chuck
>>
>>  On Wed, 5 Dec 2007, Gad Abraham wrote:
>> 
>> >   Hi,
>> > 
>> >   The following error looks like a bug to me but perhaps someone can shed
>> >   light on it:
>> > 
>> > >   library(splines)
>> > >   library(survival)
>> > >   s <- survreg(Surv(futime, fustat) ~ ns(age, knots=c(50, 60)),
>> >   data=ovarian)
>> > >   n <- data.frame(age=rep(mean(ovarian$age), 10))
>> > >   predict(s, newdata=n)
>> >   Error in qr.default(t(const)) :
>> >     NA/NaN/Inf in foreign function call (arg 1)
>> > 
>> >   Thanks,
>> >   Gad
>> > 
>> > 
>> > 
>> > >   sessionInfo()
>> >   R version 2.6.1 (2007-11-26)
>> >   i486-pc-linux-gnu
>> > 
>> >   locale:
>> >   LC_CTYPE=en_AU.UTF-8;LC_NUMERIC=C;LC_TIME=en_AU.UTF-8;
>> >   LC_COLLATE=en_AU.UTF-8;LC_MONETARY=en_AU.UTF-8;LC_MESSAGES=en_AU.UTF-8;
>> >   LC_PAPER=en_AU.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;
>> >   LC_MEASUREMENT=en_AU.UTF-8;LC_IDENTIFICATION=C
>> > 
>> >   attached base packages:
>> >   [1] splines   stats     graphics  grDevices utils     datasets  methods
>> >   [8] base
>> > 
>> >   other attached packages:
>> >   [1] survival_2.34
>> > 
>> >   loaded via a namespace (and not attached):
>> >   [1] rcompgen_0.1-17
>> > 
>> > 
>> > 
>> >   --
>> >   Gad Abraham
>> >   Department of Mathematics and Statistics
>> >   The University of Melbourne
>> >   Parkville 3010, Victoria, Australia
>> >   email: g.abraham at ms.unimelb.edu.au
>> >   web: http://www.ms.unimelb.edu.au/~gabraham
>> > 
>> >   ______________________________________________
>> >   R-help at r-project.org mailing list
>> >   https://stat.ethz.ch/mailman/listinfo/r-help
>> >   PLEASE do read the posting guide
>> >   http://www.R-project.org/posting-guide.html
>> >   and provide commented, minimal, self-contained, reproducible code.
>> > 
>>
>>  Charles C. Berry                            (858) 534-2098
>>                                             Dept of Family/Preventive
>>  Medicine
>>  E mailto:cberry at tajo.ucsd.edu	            UC San Diego
>>  http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901
>> 
>> 
>> 
>
> Charles C. Berry                            (858) 534-2098
>                                            Dept of Family/Preventive 
> Medicine
> E mailto:cberry at tajo.ucsd.edu	            UC San Diego
> http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901
>
>
>
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From bolker at ufl.edu  Wed Dec  5 02:16:17 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 4 Dec 2007 17:16:17 -0800 (PST)
Subject: [R] Learning to do randomized block design analysis
In-Reply-To: <00ae01c836ca$479a5710$3a0b2c0a@gne.windows.gene.com>
References: <2E8AE992B157C0409B18D0225D0B476306CD91FB@XCH-VN01.sph.ad.jhsph.edu>
	<36923f1d0712041446v539fdb1eof377f7739e8a6d7d@mail.gmail.com>
	<00ae01c836ca$479a5710$3a0b2c0a@gne.windows.gene.com>
Message-ID: <14163277.post@talk.nabble.com>




Bert Gunter wrote:
> 
> Let's be careful here. aov() treats block as a **random** error component
> of
> variance.  lm() treats block as a **fixed effect**. That's a different
> kettle of fish. Perhaps both Kevin and the authors of his textbook need to
> read up on fixed versus random effects and what they mean -- and what
> sorts
> of tests make sense for each.
> 
> 

  Note that Kevin didn't send the last message, "T.K." did.
My copies of ISwR and MASS are either at work or loaned
out, so please forgive confusion in the following ... but ...

# Case Study 13.2.1, page 778
cd <- c(8, 11, 9, 16, 24)
dp <- c(2, 1, 12, 11, 19)
lm <- c(-2, 0, 6, 2, 11)
table <- data.frame(Block=LETTERS[1:5],
                    "Score changes"=c(cd, dp,
                      lm), Therapy=rep(c("Contact Desensitization",
                             "Demonstration Participation",
                             "Live Modeling"), each=5)) 

model.aov <- aov(Score.changes ~ Therapy + Error(Block), data=table) 
summary(model.aov)

m1 = summary(model.aov)
str(m1)  ## looking at the guts

## not particularly friendly! but this will do it.
## the first element of the list is the block error info
## the second is the within-block info

blockmeansq = m1[["Error: Block"]][[1]][["Mean Sq"]]
errmeansq =   m1[["Error: Within"]][[1]][["Mean Sq"]][2]

fval = blockmeansq/errmeansq ## 109.5/8.55
blockdf = m1[["Error: Block"]][[1]][["Df"]] ## 4
errdf =   m1[["Error: Within"]][[1]][["Df"]][2] ## 8
pf(fval,df1=blockdf,df2=errdf,lower.tail=FALSE)

  In this particular case, the fixed-effect model and the RCB
design will give the same p-values:

bad.aov <- aov(Score.changes ~ Therapy + Block, data=table) 
summary(bad.aov)

BUT THIS IS NOT TO BE RELIED UPON IN GENERAL!

You can also use lme [nlme package]  or lmer [lme4 package],
which can do much more complicated models.


library(nlme)
m2 = lme(Score.changes ~ Therapy, random = ~1|Block, data=table)
anova(m2)

detach("package:nlme")
library(lme4)
m3 = lmer(Score.changes ~ Therapy+(1|Block), data=table)
anova(m3)

-- 
View this message in context: http://www.nabble.com/Learning-to-do-randomized-block-design-analysis-tf4945409.html#a14163277
Sent from the R help mailing list archive at Nabble.com.


From maj at stats.waikato.ac.nz  Wed Dec  5 02:18:23 2007
From: maj at stats.waikato.ac.nz (maj at stats.waikato.ac.nz)
Date: Wed, 5 Dec 2007 14:18:23 +1300 (NZDT)
Subject: [R] Calculating large determinants
Message-ID: <4455.203.173.149.245.1196817503.squirrel@webmail.scms.waikato.ac.nz>

I apologise for not including a reproducible example with this query but I
hope that I can make things clear without one.

I am fitting some finite mixture models to data. Each mixture component
has p parameters (p=29 in my application) and there are q components to
the mixture. The number of data points is n ~ 1500.

I need to select a good q and I have been considering model selection
methods suggested in Chapter 6 of
@BOOK{mp01,
  author    = {McLachlan, G. J. and Peel, D.},
  title     = {Finite Mixture Models},
  publisher = {Wiley},
  address   = {New York},
  year      = {2001}
}

One of these methods involves an "empirical information matrix" which is
the matrix of products of parameter scores at the observation level
evaluated at the MLE and summed over all observations. For example a
six-component mixture will have 6 - 1 + 29*6 = 179 parameters. So for
observation i I form the 179 by 179 matrix of products of scores and sum
these up over all 1500-odd observations.

Actually it is the log of the determinant of the resultant matrix that I
really need rather than the matrix itself. I am seeking advice on what may
be the best way to evaluate this log(det()). I have been encountering
problems using
determinant(SS,logarithm=TRUE)

and   eigen(SS,only.values = TRUE)$values

shows some negative eigenvalues.

Advice will be gratefully received!

Murray Jorgensen


From bolker at ufl.edu  Wed Dec  5 02:32:45 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 4 Dec 2007 17:32:45 -0800 (PST)
Subject: [R] Rating R Helpers
In-Reply-To: <s7533dcf.025@tedmail.lgc.co.uk>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
	<s7533dcf.025@tedmail.lgc.co.uk>
Message-ID: <14163486.post@talk.nabble.com>




S Ellison wrote:
> 
> Package review is a nice idea. But you raise a worrying point.
> Are any of the 'downright dangerous' packages on CRAN?
> If so, er... why?
> 
> 
>>>> <Bill.Venables at csiro.au> 12/01/07 7:21 AM >>>
>>I think the need for this is rather urgent, in fact.  Most packages are
>>very good, but I regret to say some are pretty inefficient and others
>>downright dangerous.
> 
> 

Presumably because the primary requirement for packages being
accepted on CRAN is that they pass "R CMD check".  This is a fine
minimum standard -- it means that packages will definitely install --
but there's nothing to stop anyone posting a package full of
statistical nonsense to CRAN, as far as I know.   I'm _not_ suggesting
that R-core should take up this challenge, but this is where ratings
come in.

  Ben Bolker

-- 
View this message in context: http://www.nabble.com/Rating-R-Helpers-tf4925550.html#a14163486
Sent from the R help mailing list archive at Nabble.com.


From bolker at ufl.edu  Wed Dec  5 02:36:07 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 4 Dec 2007 17:36:07 -0800 (PST)
Subject: [R] Rating R Helpers
In-Reply-To: <475336DE.91DF.00CB.0@grecc.umaryland.edu>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
	<B998A44C8986644EA8029CFE6396A924D89A7C@exqld2-bne.nexus.csiro.au>
	<475336DE.91DF.00CB.0@grecc.umaryland.edu>
Message-ID: <14163487.post@talk.nabble.com>




John Sorkin wrote:
> 
> I believe we need to know the following about packages:
> (1) Does the package do what it purports to do, i.e. are the results
> valid?
> (2) Have the results generated by the package been validate against some
> other statistical package, or hand-worked example?
> (3) Are the methods used in the soundly based?
> (4) Does the package documentation refer to referred papers or textbooks?
> (5) In addition to the principle result, does the package return ancillary
> values that allow for proper interpretation of the main result, (e.g. lm
> gives estimates of the betas and their SEs, but also generates
> residuals)?.
> (6) Is the package easy to use, i.e. do the parameters used when invoking
> the package chosen so as to allow the package to be flexible?
> (7) Are the error messages produced by the package helpful?
> (8) Does the package conform to standards of R coding and good programming
> principles in general?
> (9) Does the package interact will with the larger R environment, e.g.
> does it have a plot method etc.?
> (10) Is the package well documented internally, i.e. is the code easy to
> follow, are the comments in the code adequate?
> (11) Is the package well documented externally, i.e. through man pages and
> perhaps other documentation (e.g. MASS and its associated textbook)?
> 
> 

Numbers 1 to 3 are critical.  The rest would be very nice to know (and
should be part of a rating
system), but in the end are more likely to lead to frustration than outright
errors ... (i.e., you'll
find out soon enough if a package is poorly documented, then you just won't
use it).

-- 
View this message in context: http://www.nabble.com/Rating-R-Helpers-tf4925550.html#a14163487
Sent from the R help mailing list archive at Nabble.com.


From alamj at uwaterloo.ca  Wed Dec  5 03:11:08 2007
From: alamj at uwaterloo.ca (Jahrul Alam)
Date: Wed, 5 Dec 2007 08:11:08 +0600
Subject: [R] Installing hdf5
Message-ID: <d1ce528b0712041811n5f7833d8r9efd4ffa7cf7ffb0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/1009bb8b/attachment.pl 

From taekyunk at gmail.com  Wed Dec  5 03:16:53 2007
From: taekyunk at gmail.com (T.K.)
Date: Tue, 4 Dec 2007 18:16:53 -0800
Subject: [R] Learning to do randomized block design analysis
In-Reply-To: <14163277.post@talk.nabble.com>
References: <2E8AE992B157C0409B18D0225D0B476306CD91FB@XCH-VN01.sph.ad.jhsph.edu>
	<36923f1d0712041446v539fdb1eof377f7739e8a6d7d@mail.gmail.com>
	<00ae01c836ca$479a5710$3a0b2c0a@gne.windows.gene.com>
	<14163277.post@talk.nabble.com>
Message-ID: <36923f1d0712041816s35d884bat512773c83d4fa940@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/cf2c08aa/attachment.pl 

From cberry at tajo.ucsd.edu  Wed Dec  5 04:37:00 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 4 Dec 2007 19:37:00 -0800
Subject: [R] predict error for survreg with natural splines
In-Reply-To: <4755E997.4020907@ms.unimelb.edu.au>
References: <4755E997.4020907@ms.unimelb.edu.au>
Message-ID: <Pine.LNX.4.64.0712041925400.20006@tajo.ucsd.edu>

On Wed, 5 Dec 2007, Gad Abraham wrote:

> Hi,
>
> The following error looks like a bug to me but perhaps someone can shed
> light on it:
>
> > library(splines)
> > library(survival)
> > s <- survreg(Surv(futime, fustat) ~ ns(age, knots=c(50, 60)),
> data=ovarian)
> > n <- data.frame(age=rep(mean(ovarian$age), 10))
> > predict(s, newdata=n)
> Error in qr.default(t(const)) :
>   NA/NaN/Inf in foreign function call (arg 1)
>
> Thanks,
> Gad

Gad,

I think I have it now.

survreg does not automatically place the boundary knots in its $terms 
component.

You can force this by hand:


> range(ovarian$age)
[1] 38.8932 74.5041
> s <- survreg(Surv(futime, fustat) ~ ns(age, knots=c(50, 60),Boundary.knots=c(38.8932, 74.5041)),data=ovarian)
> predict(s,newdata=data.frame(age=mean(ovarian$age)))
       [,1]
1 1119.448
> all.preds <- predict(s)
> all.preds.2 <- sapply(ovarian$age,function(x) predict(s,newdata=data.frame(age=x)))
> all.equal(all.preds,all.preds.2)
[1] TRUE
>


Hard coding the boundary knots as above will assure that predict will work 
even if the data.frame used in making the fit is unavailable.

HTH,

Chuck




>
>
>
> > sessionInfo()
> R version 2.6.1 (2007-11-26)
> i486-pc-linux-gnu
>
> locale:
> LC_CTYPE=en_AU.UTF-8;LC_NUMERIC=C;LC_TIME=en_AU.UTF-8;
> LC_COLLATE=en_AU.UTF-8;LC_MONETARY=en_AU.UTF-8;LC_MESSAGES=en_AU.UTF-8;
> LC_PAPER=en_AU.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;
> LC_MEASUREMENT=en_AU.UTF-8;LC_IDENTIFICATION=C
>
> attached base packages:
> [1] splines   stats     graphics  grDevices utils     datasets  methods
> [8] base
>
> other attached packages:
> [1] survival_2.34
>
> loaded via a namespace (and not attached):
> [1] rcompgen_0.1-17
>
>
>
> -- 
> Gad Abraham
> Department of Mathematics and Statistics
> The University of Melbourne
> Parkville 3010, Victoria, Australia
> email: g.abraham at ms.unimelb.edu.au
> web: http://www.ms.unimelb.edu.au/~gabraham
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From tim.calkins at gmail.com  Wed Dec  5 04:39:45 2007
From: tim.calkins at gmail.com (Tim Calkins)
Date: Wed, 5 Dec 2007 14:39:45 +1100
Subject: [R] converting factors to dummy variables
Message-ID: <ca5a2f2e0712041939p4739d879m9b51ab21c08d5e30@mail.gmail.com>

Hi all -

I'm trying to find a way to create dummy variables from factors in a
regression.  I have been using biglm along the lines of

ff <- log(Price) ~ factor(Colour):factor(Store) +
factor(DummyVar):factor(Colour):factor(Store)

lm1 <- biglm(ff, data=my.dataset)

but because there are lots of colours (>100) and lots of stores
(>250), I run it to memory problems.  Now, not every store sells every
colour and so it should be possible to create the matrix of factor
variables myself and greatly reduce the size of the problem.  it seems
that lm / biglm use all combinations of factor levels when used in
factor(Colour):factor(Store) so by creating my own matrix of factor
variables i should be able to reduce the size of the problem
considerably.

If i have a data frame
>my.dataset <- data.frame(Price=1:12, Colour= c('red','blue','green'),
Store=c('a', 'b', 'c', 'a', 'c', 'd', 'e', 'e', 'e', 'e', 'b', 'e'),
DummyVar = sort(rep(c(0,1),6)) )

i want to create a data frame with the dummy vars that looks like

red:a	red:e	blue:b	blue:c	blue:e	green:c	green:d	green:e
1	0	0	0	0	0	0	0
0	0	1	0	0	0	0	0
0	0	0	0	0	1	0	0
1	0	0	0	0	0	0	0
0	0	0	1	0	0	0	0
0	0	0	0	0	0	1	0
0	1	0	0	0	0	0	0
0	0	0	0	1	0	0	0
0	0	0	0	0	0	0	1
0	1	0	0	0	0	0	0
0	0	1	0	0	0	0	0
0	0	0	0	0	0	0	1

any ideas would be appreciated.


-- 
Tim Calkins
0406 753 997


From zhuanshi.he at gmail.com  Wed Dec  5 04:55:09 2007
From: zhuanshi.he at gmail.com (Zhuanshi He)
Date: Tue, 4 Dec 2007 22:55:09 -0500
Subject: [R] Installing hdf5
In-Reply-To: <d1ce528b0712041811n5f7833d8r9efd4ffa7cf7ffb0@mail.gmail.com>
References: <d1ce528b0712041811n5f7833d8r9efd4ffa7cf7ffb0@mail.gmail.com>
Message-ID: <2a8feb1c0712041955k648bd918ueec70d5580db6fe3@mail.gmail.com>

I can install hdf5 smoothly. I am using R2.5.0 on WinXP-32bit machine.


>> configure: error: Can't find HDF5

It looks u r using x86_64 machine. Maybe u need install hdf5 from source code.
I have been install hdf5 on both CentOS and Fedora, no error messages.


ZHUANSHI



On 12/4/07, Jahrul Alam <alamj at uwaterloo.ca> wrote:
>
> Hello,
>
> I want to install hdf5 libraries for R and I get the same error as posted
> below whether I try install.packages("hdf5") from R command prompt or the
> command line installation below. I would appreciate if any one could help me
> on this.
>
>
> [alamj at thor hdf5]$ ./configure --with-hdf5=/bluejay/apps/HDF5-1.6.6/x86_64
> checking for gcc... gcc
> checking for C compiler default output... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for executable suffix...
> checking for object suffix... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for library containing inflate... -lz
> checking for library containing H5open... no
> configure: error: Can't find HDF5
>
> Thanks.
>
> Jahrul
>
>
>
> --
> Jahrul Alam, PhD
> Post-doctoral researcher
> Dept of Earth and Environmental Sciences
> University of Waterloo
> Waterloo, ON, Canada
> http://www.math.mcmaster.ca/~alamj/
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


-- 
Zhuanshi He / Z. He (PhD)
Waterloo Centre for Atmospheric Sciences (WCAS)
Department of Earth and Environmental Sciences
Phy Bldg, Rm 2022
University of Waterloo,
Waterloo, ON N2L 3G1
Canada
Tel: +1-519-888-4567 ext 38053        FAX: +1-519-746-0435


From vancamp at sonic.net  Wed Dec  5 05:24:56 2007
From: vancamp at sonic.net (Warren Van Camp)
Date: Tue, 4 Dec 2007 20:24:56 -0800 (PST)
Subject: [R] R and TeraData
In-Reply-To: <47166114.5090208@kutsyy.com>
References: <47166114.5090208@kutsyy.com>
Message-ID: <14160501.post@talk.nabble.com>



Vadim Kutsyy wrote:
> 
> 
> Does anyone know a way to connect from R on Linux box to TeraData 
> server?  I can use ODBC connection on Windows box, but with amount of 
> data I need (and prefer) to use large Linux box.
> 
> Thanks,
> Vadim
> 
> 

You indicate that the ODBC connection to Teradata on Windows is working for
you.  This may be a newbie question, but I'm not getting beyond an
apparently successful connection to the Teradata...

> odbcGetInfo(odw)
             DBMS_Name               DBMS_Ver        Driver_ODBC_Ver 
            "Teradata"          "06.02.0205  V2R"             "03.52" 
          Data_Source_Name        Driver_Name             Driver_Ver 
             "ODW"                "TDATA32.DLL"          " 3.05.00.04" 
           ODBC_Ver                   Server_Name 
          "03.52.0000"               "********" 
> getSqlTypeInfo()
                               double integer    character      logical
MySQL                          double integer varchar(255)   varchar(5)
ACCESS                         DOUBLE INTEGER VARCHAR(255)   varchar(5)
Microsoft.SQL.Server            float     int varchar(255)   varchar(5)
PostgreSQL                     float8    int4 varchar(255)   varchar(5)
Oracle               double precision integer varchar(255) varchar(255)
SQLite                         double integer varchar(255)   varchar(5)
EXCEL                          NUMBER  NUMBER VARCHAR(255)      LOGICAL
DBASE                         Numeric Numeric    Char(254)      Logical

> getSqlTypeInfo("TDATA32.DLL")
NULL

> odw_driver <- odbcGetInfo(odw)
> getSqlTypeInfo(odw_driver)
Error in typesR2DBMS[[driver]] : no such index at level 1

> sqlTables(odw)
Error in iconv(data[[i]], from = enc) : invalid 'from' argument
In addition: Warning message:
closing unused RODBC handle 1 

> sqlQuery(odw,"select top 10 * from odw_prs_v.pool")
Error in iconv(query, to = enc) : invalid 'to' argument


Apparently there are some data conversion errors... do data types need to be
established first?  Any other ideas what's missing?

Thanks,
Warren.
-- 
View this message in context: http://www.nabble.com/R-and-TeraData-tf4642728.html#a14160501
Sent from the R help mailing list archive at Nabble.com.


From cberry at tajo.ucsd.edu  Wed Dec  5 05:26:39 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 4 Dec 2007 20:26:39 -0800
Subject: [R] converting factors to dummy variables
In-Reply-To: <ca5a2f2e0712041939p4739d879m9b51ab21c08d5e30@mail.gmail.com>
References: <ca5a2f2e0712041939p4739d879m9b51ab21c08d5e30@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0712042015340.20249@tajo.ucsd.edu>

On Wed, 5 Dec 2007, Tim Calkins wrote:

> Hi all -
>
> I'm trying to find a way to create dummy variables from factors in a
> regression.  I have been using biglm along the lines of
>
> ff <- log(Price) ~ factor(Colour):factor(Store) +
> factor(DummyVar):factor(Colour):factor(Store)
>
> lm1 <- biglm(ff, data=my.dataset)
>
> but because there are lots of colours (>100) and lots of stores
> (>250), I run it to memory problems.  Now, not every store sells every
> colour and so it should be possible to create the matrix of factor
> variables myself and greatly reduce the size of the problem.  it seems
> that lm / biglm use all combinations of factor levels when used in
> factor(Colour):factor(Store) so by creating my own matrix of factor
> variables i should be able to reduce the size of the problem
> considerably.
>
> If i have a data frame
>> my.dataset <- data.frame(Price=1:12, Colour= c('red','blue','green'),
> Store=c('a', 'b', 'c', 'a', 'c', 'd', 'e', 'e', 'e', 'e', 'b', 'e'),
> DummyVar = sort(rep(c(0,1),6)) )
>
> i want to create a data frame with the dummy vars that looks like
>
> red:a	red:e	blue:b	blue:c	blue:e	green:c	green:d	green:e
> 1	0	0	0	0	0	0	0
> 0	0	1	0	0	0	0	0
> 0	0	0	0	0	1	0	0
> 1	0	0	0	0	0	0	0
> 0	0	0	1	0	0	0	0
> 0	0	0	0	0	0	1	0
> 0	1	0	0	0	0	0	0
> 0	0	0	0	1	0	0	0
> 0	0	0	0	0	0	0	1
> 0	1	0	0	0	0	0	0
> 0	0	1	0	0	0	0	0
> 0	0	0	0	0	0	0	1
>
> any ideas would be appreciated.


Use

mat <- model.matrix( ~ClrStr-1,
 	transform( my.dataset, ClrStr =
 		factor( paste(Colour,Store,sep=":") ) ) )

then pretty up the colnames() and re-order columns if order matters.

----

However, if DummyVar is a categorical variable, you could just compute 
means on the appropriate subsets by maintaining a table of sums and 
totals. Then in a second pass through the data get the residual sums of 
squares. If the data are already in a database, it might make sense to do 
these operations there and import the results to R for further massaging.


HTH,

Chuck

>
>
> -- 
> Tim Calkins
> 0406 753 997
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From p_connolly at slingshot.co.nz  Wed Dec  5 06:49:29 2007
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Wed, 5 Dec 2007 18:49:29 +1300
Subject: [R] Rating R Helpers
In-Reply-To: <14163486.post@talk.nabble.com>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
	<s7533dcf.025@tedmail.lgc.co.uk> <14163486.post@talk.nabble.com>
Message-ID: <20071205054929.GI6584@slingshot.co.nz>

On Tue, 04-Dec-2007 at 05:32PM -0800, Ben Bolker wrote:

|> 
|> 
|> 
|> S Ellison wrote:
|> > 
|> > Package review is a nice idea. But you raise a worrying point.
|> > Are any of the 'downright dangerous' packages on CRAN?
|> > If so, er... why?
|> > 
|> > 
|> >>>> <Bill.Venables at csiro.au> 12/01/07 7:21 AM >>>
|> >>I think the need for this is rather urgent, in fact.  Most packages are
|> >>very good, but I regret to say some are pretty inefficient and others
|> >>downright dangerous.
|> > 
|> > 
|> 
|> Presumably because the primary requirement for packages being
|> accepted on CRAN is that they pass "R CMD check".  This is a fine
|> minimum standard -- it means that packages will definitely install --

That's not quite true.  Package BRugs will go halfway through the
installation before a Linux user is given the information that it will
not work with Linux.  The automated way the packages are listed
doesn't manage to collect that bit of information (and that's nothing
anyone should be ashamed of).  

Somewhere for adding information such as that could help avoid the
need for many people finding that out for themselves.

best

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From zhangyanshen at gmail.com  Wed Dec  5 06:58:13 2007
From: zhangyanshen at gmail.com (environ)
Date: Tue, 4 Dec 2007 21:58:13 -0800 (PST)
Subject: [R] pointwise nonparametric estimation of odds-ratios of continuous
 predictors
Message-ID: <14165792.post@talk.nabble.com>


Dear listers,

I am doing an time-series analysis on the relationship fo daily air
pollutants and mortality using generalized additive models. I have two
questions as followed:

Question 1:
How can I get the relative risk estimates from the smoothing spines ? I have
found a paper which focused on this problem. The paper is "np.OR: an s-plus
function for pointwise nonparametric estimation of odds ratios of continuous
predictors". Please see the attached files.

Question 2: 
Is there some functions in package mgcv ? If not, can I use the method
described in the above reference?

Thanks a lot in advance

Yanshen Zhang
http://www.nabble.com/file/p14165792/an%2Bs-plus%2Bfunction%2Bfor%2Bpointwise%2Bnonparametric%2Bestimation%2Bof%2Bodds-ratios%2Bof%2Bcontinuous%2Bpredictors.pdf
an+s-plus+function+for+pointwise+nonparametric+estimation+of+odds-ratios+of+continuous+predictors.pdf 
http://www.nabble.com/file/p14165792/an%2Bs-plus%2Bfunction%2Bfor%2Bpointwise%2Bnonparametric%2Bestimation%2Bof%2Bodds-ratios%2Bof%2Bcontinuous%2Bpredictors.pdf
an+s-plus+function+for+pointwise+nonparametric+estimation+of+odds-ratios+of+continuous+predictors.pdf 
-- 
View this message in context: http://www.nabble.com/pointwise-nonparametric-estimation-of-odds-ratios-of-continuous-predictors-tf4947653.html#a14165792
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Wed Dec  5 07:20:32 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Dec 2007 06:20:32 +0000 (GMT)
Subject: [R] Dimension of a vector
In-Reply-To: <4433.203.173.149.245.1196814932.squirrel@webmail.scms.waikato.ac.nz>
References: <4433.203.173.149.245.1196814932.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <Pine.LNX.4.64.0712050616050.17931@gannet.stats.ox.ac.uk>

On Wed, 5 Dec 2007, maj at stats.waikato.ac.nz wrote:

> Consider the following:
>> A <- 1:10
>> A
> [1]  1  2  3  4  5  6  7  8  9 10
>> dim(A)
> NULL
>> dim(A) <- c(2,5)
>> A
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    1    3    5    7    9
> [2,]    2    4    6    8   10
>> dim(A)
> [1] 2 5
>> dim(A) <- 10
>> A
> [1]  1  2  3  4  5  6  7  8  9 10
>> dim(A)
> [1] 10
>
> Would it not make sense to have dim(A) = length(A) for all vectors?

No.  A one-dimensional array and a vector are not the same thing.
There are subtle differences, such as what names() means (see ?names).

That a 1D array and a vector _print_ in the same way does occasionally 
lead to confusion, but then you also cannot tell from your printout that A 
has type "integer" and not "double".

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Wed Dec  5 07:22:13 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Dec 2007 01:22:13 -0500
Subject: [R] Rating R Helpers
In-Reply-To: <20071205054929.GI6584@slingshot.co.nz>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
	<s7533dcf.025@tedmail.lgc.co.uk> <14163486.post@talk.nabble.com>
	<20071205054929.GI6584@slingshot.co.nz>
Message-ID: <971536df0712042222p125bb67fo3e174fab48d70da1@mail.gmail.com>

On Dec 5, 2007 12:49 AM, Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
> On Tue, 04-Dec-2007 at 05:32PM -0800, Ben Bolker wrote:
>
> |>
> |>
> |>
> |> S Ellison wrote:
> |> >
> |> > Package review is a nice idea. But you raise a worrying point.
> |> > Are any of the 'downright dangerous' packages on CRAN?
> |> > If so, er... why?
> |> >
> |> >
> |> >>>> <Bill.Venables at csiro.au> 12/01/07 7:21 AM >>>
> |> >>I think the need for this is rather urgent, in fact.  Most packages are
> |> >>very good, but I regret to say some are pretty inefficient and others
> |> >>downright dangerous.
> |> >
> |> >
> |>
> |> Presumably because the primary requirement for packages being
> |> accepted on CRAN is that they pass "R CMD check".  This is a fine
> |> minimum standard -- it means that packages will definitely install --
>
> That's not quite true.  Package BRugs will go halfway through the
> installation before a Linux user is given the information that it will
> not work with Linux.  The automated way the packages are listed
> doesn't manage to collect that bit of information (and that's nothing
> anyone should be ashamed of).
>
> Somewhere for adding information such as that could help avoid the
> need for many people finding that out for themselves.
>

The bioconductor packages have the DESCRIPTION file's SystemRequirements
field listed on the net so you can know what they are prior to
downloading the file.
For CRAN packages this information seems not to be shown on the net.


From scruveil at genoscope.cns.fr  Wed Dec  5 09:02:30 2007
From: scruveil at genoscope.cns.fr (=?ISO-8859-1?Q?St=E9phane_CRUVEILLER?=)
Date: Wed, 05 Dec 2007 09:02:30 +0100
Subject: [R] Multiple stacked barplots on the same graph?
In-Reply-To: <f8e6ff050712041015u2cc3aa75x50f343649efcadae@mail.gmail.com>
References: <47555E3C.9030301@genoscope.cns.fr> <47557529.3050806@unicas.it>	
	<475581AD.9030000@genoscope.cns.fr>
	<f8e6ff050712041015u2cc3aa75x50f343649efcadae@mail.gmail.com>
Message-ID: <47565B16.9010607@genoscope.cns.fr>

Hi,

the same error message is displayed with geom="bar" as parameter.
here is the output of dput:

 > dput(mydata)
structure(list(Categorie = structure(c(1L, 12L, 8L, 2L, 5L, 7L,
16L, 6L, 15L, 11L, 10L, 13L, 14L, 3L, 4L, 9L, 17L, 1L, 12L, 8L,
2L, 5L, 7L, 16L, 6L, 15L, 11L, 10L, 13L, 14L, 3L, 4L, 9L, 17L
), .Label = c("Amino acid biosynthesis", "Biosynthesis of cofactors, 
prosthetic groups, and carriers",
"Cell envelope", "Cellular processes", "Central intermediary metabolism",
"DNA metabolism", "Energy metabolism", "Fatty acid and phospholipid 
metabolism",
"Mobile and extrachromosomal element functions", "Protein fate",
"Protein synthesis", "Purines, pyrimidines, nucleosides, and nucleotides",
"Regulatory functions", "Signal transduction", "Transcription",
"Transport and binding proteins", "Unknown function"), class = "factor"),
    Part = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("common",
    "specific"), class = "factor"), Total = c(3.03, 1.65, 1.52,
    2.85, 3.4, 11.81, 10.51, 1.95, 2.08, 2.51, 2.23, 7.63, 1.88,
    2.76, 7.21, 1.08, 20.75, 0.35, 0.17, 0.08, 0.18, 0.42, 2.05,
    1.98, 0.63, 0.17, 0.2, 0.3, 1.58, 0.27, 0.83, 1.38, 3.56,
    11.63), chr1 = c(4.55, 2.37, 1.77, 4.68, 3.19, 12.49, 13.56,
    2.81, 3.13, 4.58, 3.26, 7.3, 2.06, 3.41, 7.9, 0.22, 22.45,
    0.16, 0.06, 0.09, 0.19, 0.09, 0.7, 0.85, 0.22, 0.06, 0.03,
    0.32, 0.66, 0.06, 0.63, 0.38, 1.14, 6.17), chr2 = c(1.68,
    1.06, 1.55, 1.02, 4.57, 13.87, 7.85, 0.98, 1.06, 0.27, 1.2,
    9.88, 2.13, 2.53, 7.71, 0.4, 22.38, 0.71, 0.35, 0.09, 0.22,
    0.98, 3.9, 3.24, 0.22, 0.22, 0.49, 0.31, 2.79, 0.62, 1.33,
    1.95, 0.44, 16), pl = c(0, 0, 0, 0, 0, 0.17, 4.27, 1.03,
    0.34, 0, 0.68, 0.68, 0, 0.17, 1.54, 8.38, 5.3, 0, 0, 0, 0,
    0, 2.22, 3.25, 4.44, 0.51, 0, 0.17, 1.88, 0, 0, 4.62, 28.72,
    24.27)), .Names = c("Categorie", "Part", "Total", "chr1",
"chr2", "pl"), class = "data.frame", row.names = c(NA, -34L))


thx,

St?phane.

hadley wickham wrote:
> On Dec 4, 2007 10:34 AM, St?phane CRUVEILLER <scruveil at genoscope.cns.fr> wrote:
>   
>> Hi,
>>
>> I tried this method but it seems that there is something wrong with my
>> data frame:
>>
>>
>> when I type in:
>>
>>  > qplot(x=as.factor(Categorie),y=Total,data=mydata)
>>
>> It displays a graph with 2 points in each category...
>> but if  I add the parameter geom="histogram"
>>
>>  > qplot(x=as.factor(Categorie),y=Total,data=mydata,geom="histogram")
>>
>>
>> Error in storage.mode(test) <- "logical" :
>>         object "y" not found
>>
>> any hint about this...
>>     
>
> Could you copy and paste the output of dput(mydata) ?
>
> (And I'd probably write the plot call as: qplot(Categorie, Total,
> data=mydata, geom="bar"), since it is a bar plot, not a histogram)
>
>   

-- 
"La science a certes quelques magnifiques r?ussites ? son actif mais
? tout prendre, je pr?f?re de loin ?tre heureux plut?t qu'avoir raison." 
D. Adams
-- 
AGC website <http://www.genoscope.cns.fr/agc>
	St?phane CRUVEILLER Ph. D.
Genoscope - Centre National de S?quencage
Atelier de G?nomique Comparative
2, Rue Gaston Cremieux CP 5706
91057 Evry Cedex - France
Phone: +33 (0)1 60 87 84 58
Fax: +33 (0)1 60 87 25 14
scruveil at genoscope.cns.fr


From stian at mail.rockefeller.edu  Tue Dec  4 18:57:55 2007
From: stian at mail.rockefeller.edu (Suyan Tian)
Date: Tue, 4 Dec 2007 12:57:55 -0500
Subject: [R] bootstrapping on the growth curve
In-Reply-To: <470371.39045.qm@web32805.mail.mud.yahoo.com>
References: <470371.39045.qm@web32805.mail.mud.yahoo.com>
Message-ID: <20A8EE5F-B450-4FF3-9ABB-3AF946466820@rockefeller.edu>

Hi, I am trying to get 95% CI s around a quantile growth curve, but  
my result looks strange to me.

Here is the function I defined myself

boot.qregress<-function(mat1, group,  quantile, int, seed.1){

     boot.fit<-NULL
     set.seed(seed.1)
     for (i in 1:int){

     index<-sample((unique(mat1$Subject[mat1$Group==group])), length 
(unique(mat1$Subject[mat1$Group==group])), replace=TRUE)

     #make the bootstrapping dataset
     mat.junk<-NULL
     for (j in 1: length(index)){

         mat.junk<-rbind(mat.junk, mat1[mat1$Subject==index[j], ])	
     	}

    boot.fit<-cbind(boot.fit, cobs(mat.junk$Day, mat.junk$Weight,   
constraint="none",  degree=2, tau=quantile, lambda=-1)$fitted)	
     	
    	}

    boot.fit 	

}


The curves I made from the bootstrapping is attached, I don't  
understand why for a group, the 5% curve drops suddenly around time  
130. I am thinking about missingness since before 130 there are 50  
patients, but after day 130 there are only 40 patients for this group.

Any suggestions on the R-code (especially about how to do the  
bootstrapping for the growth curves) or why the drops happened would  
be appreciated.

Thanks a lot,

Suyan
?     	



From dieterbest_2000 at yahoo.com  Wed Dec  5 00:39:19 2007
From: dieterbest_2000 at yahoo.com (Dieter Best)
Date: Tue, 4 Dec 2007 15:39:19 -0800 (PST)
Subject: [R] installing RSPerl on windows
Message-ID: <593939.44455.qm@web38410.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071204/bf1d969d/attachment.pl 

From seanpor at acm.org  Wed Dec  5 09:56:07 2007
From: seanpor at acm.org (seanpor)
Date: Wed, 5 Dec 2007 00:56:07 -0800 (PST)
Subject: [R] Rating R Helpers
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E012DE9CD@dc1ex01.air.org>
Message-ID: <14167605.post@talk.nabble.com>


Good morning,

I read (and occasionally post to) R-Help through nabble.com
(http://www.nabble.com/R-f13819.html) and there is a rating system there by
post rather than by author.

Message boards (e.g. www.boards.ie amongst many others) note on each message
how many posts that a person has made since they joined the board.

Best Regards,
Sean


Doran, Harold wrote:
> 
> Since R is open source and help may come from varied levels of
> experience on R-Help, I wonder if it might be helpful to construct a
> method that can be used to "rate" those who provide help on this list.
> 
> This is something that is done on other comp lists, like
> http://www.experts-exchange.com/.
> 
> I think some of the reasons for this are pretty transparent, but I
> suppose one reason is that one could decide to implement the advise of
> those with "superior" or "expert" levels. In other words, you can trust
> the advice of someone who is more experienced more than someone who is
> not. Currently, there is no way to discern who on this list is really an
> R expert and who is not. Of course, there is R core, but most people
> don't actually know who these people are (at least I surmise that to be
> true).
> 
> If this is potentially useful, maybe one way to begin the development of
> such ratings is to allow the original poster to "rate" the level of help
> from those who responded. Maybe something like a very simple
> questionnaire on a likert-like scale that the original poster would
> respond to upon receiving help which would lead to the accumulation of
> points for the responders. Higher points would result in higher levels
> of expertise (e.g., novice, ..., wizaRd).
> 
> Just a random thought. What do others think?
> 
> Harold
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Rating-R-Helpers-tf4925550.html#a14167605
Sent from the R help mailing list archive at Nabble.com.


From maechler at stat.math.ethz.ch  Wed Dec  5 10:42:05 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 5 Dec 2007 10:42:05 +0100
Subject: [R] Calculating large determinants
In-Reply-To: <4455.203.173.149.245.1196817503.squirrel@webmail.scms.waikato.ac.nz>
References: <4455.203.173.149.245.1196817503.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <18262.29293.511387.218323@stat.math.ethz.ch>

>>>>> "MJ" == maj  <maj at stats.waikato.ac.nz>
>>>>>     on Wed, 5 Dec 2007 14:18:23 +1300 (NZDT) writes:

    MJ> I apologise for not including a reproducible example
    MJ> with this query but I hope that I can make things clear
    MJ> without one.

    MJ> I am fitting some finite mixture models to data. Each
    MJ> mixture component has p parameters (p=29 in my
    MJ> application) and there are q components to the
    MJ> mixture. The number of data points is n ~ 1500.

    MJ> I need to select a good q and I have been considering model selection
    MJ> methods suggested in Chapter 6 of
    MJ> @BOOK{mp01,
    MJ> author    = {McLachlan, G. J. and Peel, D.},
    MJ> title     = {Finite Mixture Models},
    MJ> publisher = {Wiley},
    MJ> address   = {New York},
    MJ> year      = {2001}
    MJ> }

    MJ> One of these methods involves an "empirical information
    MJ> matrix" which is the matrix of products of parameter
    MJ> scores at the observation level evaluated at the MLE and
    MJ> summed over all observations. For example a six-component
    MJ> mixture will have 6 - 1 + 29*6 = 179 parameters. So for
    MJ> observation i I form the 179 by 179 matrix of products of
    MJ> scores and sum these up over all 1500-odd observations.

    MJ> Actually it is the log of the determinant of the resultant matrix that I
    MJ> really need rather than the matrix itself. I am seeking advice on what may
    MJ> be the best way to evaluate this log(det()). I have been encountering
    MJ> problems using
    MJ> determinant(SS,logarithm=TRUE)

    MJ> and   eigen(SS,only.values = TRUE)$values

    MJ> shows some negative eigenvalues.

which is a problem?
In that case I guess your problem is that you want to estimate a
positive definite matrix S but your estimate S^ is not quite
positive definite.

Function posdefify() in CRAN package "sfsmisc" provides an old
cheap solution to this problem,
where  nearPD() in package 'Matrix' (based on a donation from
Jens Oehlschlaegel) provides a more sophisticated algorithm for
this problem.

If you really only need the eigenvalues of the "corrected"
matrix, you might want to abbreviate the nearPD() function by
just returning the final 'd' vector of eigenvalues.

    MJ> Advice will be gratefully received!

I'll be glad to hear if and how you'd use one of these two functions.

Martin Maechler, ETH Zurich

    MJ>    Murray Jorgensen


From hwborchers at gmail.com  Wed Dec  5 10:44:52 2007
From: hwborchers at gmail.com (Hans W. Borchers)
Date: Wed, 5 Dec 2007 09:44:52 +0000 (UTC)
Subject: [R] Plotting error bars in xy-direction
Message-ID: <loom.20071205T094247-719@post.gmane.org>

Dear R-help,

I am looking for a function that will plot error bars in x- or y-direction (or 
both), the same as the Gnuplot function 'plot' can achieve with:

    plot "file.dat" with xyerrorbars,...

Rsite-searching led me to the functions 'errbar' and 'plotCI' in the Hmisc, 
gregmisc, and plotrix packages. As I understand the descriptions and examples, 
none of these functions provides horizontal error bars.

Looking into 'errbar' and using segments, I wrote a small function for myself 
adding these kinds of error bars to existing plots. I would still be interested 
to know what the standard R solution is.

Regards,  Hans Werner


From serge.de.gosson.de.varennes at forsakringskassan.se  Wed Dec  5 11:25:41 2007
From: serge.de.gosson.de.varennes at forsakringskassan.se (de Gosson de Varennes Serge (4100))
Date: Wed, 5 Dec 2007 11:25:41 +0100
Subject: [R] Quadratic programming
Message-ID: <D0B620C63B10AC41BF1DBD4184C9C50003415F7D@S00MAIL003.ads.sfa.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/1401d793/attachment.pl 

From Serguei.Kaniovski at wifo.ac.at  Wed Dec  5 11:31:33 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Wed, 5 Dec 2007 11:31:33 +0100
Subject: [R] Information criteria for kmeans
Message-ID: <OF77A7965B.9015C1B2-ONC12573A8.0039D22C-C12573A8.0039D22F@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/53042339/attachment.pl 

From wwwhsd at gmail.com  Wed Dec  5 11:34:18 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 5 Dec 2007 08:34:18 -0200
Subject: [R] converting factors to dummy variables
In-Reply-To: <ca5a2f2e0712041939p4739d879m9b51ab21c08d5e30@mail.gmail.com>
References: <ca5a2f2e0712041939p4739d879m9b51ab21c08d5e30@mail.gmail.com>
Message-ID: <da79af330712050234m51c467d0jff0d7fe7b3e05fe2@mail.gmail.com>

Try this also:

table(cbind.data.frame(Price=my.dataset$Price,
Colour=paste(my.dataset$Colour, my.dataset$Store, sep=":")))

On 05/12/2007, Tim Calkins <tim.calkins at gmail.com> wrote:
> Hi all -
>
> I'm trying to find a way to create dummy variables from factors in a
> regression.  I have been using biglm along the lines of
>
> ff <- log(Price) ~ factor(Colour):factor(Store) +
> factor(DummyVar):factor(Colour):factor(Store)
>
> lm1 <- biglm(ff, data=my.dataset)
>
> but because there are lots of colours (>100) and lots of stores
> (>250), I run it to memory problems.  Now, not every store sells every
> colour and so it should be possible to create the matrix of factor
> variables myself and greatly reduce the size of the problem.  it seems
> that lm / biglm use all combinations of factor levels when used in
> factor(Colour):factor(Store) so by creating my own matrix of factor
> variables i should be able to reduce the size of the problem
> considerably.
>
> If i have a data frame
> >my.dataset <- data.frame(Price=1:12, Colour= c('red','blue','green'),
> Store=c('a', 'b', 'c', 'a', 'c', 'd', 'e', 'e', 'e', 'e', 'b', 'e'),
> DummyVar = sort(rep(c(0,1),6)) )
>
> i want to create a data frame with the dummy vars that looks like
>
> red:a   red:e   blue:b  blue:c  blue:e  green:c green:d green:e
> 1       0       0       0       0       0       0       0
> 0       0       1       0       0       0       0       0
> 0       0       0       0       0       1       0       0
> 1       0       0       0       0       0       0       0
> 0       0       0       1       0       0       0       0
> 0       0       0       0       0       0       1       0
> 0       1       0       0       0       0       0       0
> 0       0       0       0       1       0       0       0
> 0       0       0       0       0       0       0       1
> 0       1       0       0       0       0       0       0
> 0       0       1       0       0       0       0       0
> 0       0       0       0       0       0       0       1
>
> any ideas would be appreciated.
>
>
> --
> Tim Calkins
> 0406 753 997
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From bernarduse1 at yahoo.fr  Wed Dec  5 11:34:18 2007
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Wed, 5 Dec 2007 11:34:18 +0100 (CET)
Subject: [R] lme output
Message-ID: <491892.82141.qm@web23406.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/803322a8/attachment.pl 

From jo.irisson at gmail.com  Wed Dec  5 11:44:31 2007
From: jo.irisson at gmail.com (jiho)
Date: Wed, 5 Dec 2007 11:44:31 +0100
Subject: [R] 2/3d interpolation from a regular grid to another regular
	grid
In-Reply-To: <e9ee1f0a0712041238w4ff935fbg80f8f26895e02277@mail.gmail.com>
References: <17EF7ABB-B4F8-4279-B9D0-620C02C91E84@gmail.com>
	<e9ee1f0a0712041238w4ff935fbg80f8f26895e02277@mail.gmail.com>
Message-ID: <85D2CAEE-0DAC-4451-A7D2-DC45969F1E14@gmail.com>

On 2007-December-04  , at 21:38 , Scionforbai wrote:
>> - krigging in package fields, which also requires irregular spaced  
>> data
>
> That kriging requires irregularly spaced data sounds new to me ;) It
> cannot be, you misread something (I feel free to say that even if I
> never used that package).

Of Krigging I only know the name and general intent so I gladly line  
up to your opinion.
I just read the description in ?Krig in the package fields which says:
" Fits a surface to irregularly spaced data. "
But there are probably other Krigging methods I overlooked.

> It can be tricky doing kriging, though, if you're not comfortable with
> a little bit of geostatistics. You have to infer a variogram model for
> each data set; you possibly run into non-stationarity or anisotropy,
> which are indeed very well treated (maybe at best) by kriging in one
> of its forms, but ... it takes more than this list to help you then;
> basically kriging requires modelling, so it is often very difficult to
> set up an automatic procedure. I can reccomend kriging if the spatial
> variability of your data (compared to grid refinement) is quite
> important.

This was the impression I had too: that Krigging is an art in itself  
and that it requires you to know much about your data. My problem is  
simpler: the variability is not very large between grid points (it is  
oceanic current velocity data so it is highly auto-correlated  
spatially) and I can get grids fine enough for variability to be low  
anyway. So it is really purely numerical.

> In other simple cases, a wheighted mean using the (squared) inverse of
> the distance as wheight and a spherical neighbourhood could be the
> simpliest way to perform the interpolation.

Yes, that would be largely enough for me. I had C routines for 2D  
polynomial interpolation of a similar cases and low order polynomes  
gave good results. I just hoped that R had that already coded  
somewhere in an handy and generic function rather than having to  
recode it myself in a probably highly specialized and not reusable  
manner.

Thank you very much for you answer and if someone knows a function  
doing what is described above, that would be terrific.

JiHO
---
http://jo.irisson.free.fr/


From michael.watson at bbsrc.ac.uk  Wed Dec  5 12:14:37 2007
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 5 Dec 2007 11:14:37 -0000
Subject: [R] Is R portable?
References: <281383.37749.qm@web32815.mail.mud.yahoo.com>	<Pine.LNX.4.64.0712041737260.9274@gannet.stats.ox.ac.uk>
	<4755D3D3.3080700@psych.uib.no> <4755DABD.7010109@stats.uwo.ca>
Message-ID: <8975119BCD0AC5419D61A9CF1A923E9504AA228B@iahce2ksrv1.iah.bbsrc.ac.uk>

I opened this hoping someone had installed R on windows mobile or simbian.... :(

________________________________

From: r-help-bounces at r-project.org on behalf of Duncan Murdoch
Sent: Tue 04/12/2007 10:54 PM
To: Tom Backer Johnsen
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Is R portable?



On 04/12/2007 5:25 PM, Tom Backer Johnsen wrote:
> Prof Brian Ripley wrote:
>> On Tue, 4 Dec 2007, John Kane wrote:
>>
>>> I simply installed R onto a USB stick, downloaded my
>>> normal packages to it and it works fine under Windows.
>> Yes, on Windows, but
>>
>> 1) There are other OSes,
>>
>> 2) This didn't just happen: it needed some careful design, including
>> some caching to make it run fast from a USB disk.
>
> Nice to discover good planning.  Am I then correct in my
> understanding: Installing R under Windows does not require any
> registry entries, the installation is essentially to unpack the
> necessary files in the correct directories?

Yes.  It does record some information in the registry, but that's purely
optional.  The only problem you're likely to run into is using external
software with R, that might look in the registry to find where R was
installed.

Duncan Murdoch

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Wed Dec  5 12:24:38 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Dec 2007 11:24:38 +0000 (GMT)
Subject: [R] Information criteria for kmeans
In-Reply-To: <OF77A7965B.9015C1B2-ONC12573A8.0039D22C-C12573A8.0039D22F@wsr.ac.at>
References: <OF77A7965B.9015C1B2-ONC12573A8.0039D22C-C12573A8.0039D22F@wsr.ac.at>
Message-ID: <Pine.LNX.4.64.0712051057430.31849@gannet.stats.ox.ac.uk>

This is not primarily an R question: if you tell us how you want to define 
it, we may be able to help you compute it.  I presume you are talking 
about Schwarz (1978), which is not billed as an 'information criterion'.

AFAIK, all Gideon Schwarz did was to define a criterion for linear 
regression.  People have applied it to other situations with a vector 
space of parameters.  However in many clustering methods (including 
kmeans, and as for example in classification trees) there is also a 
combinatorial part of the fit: you optimize over both the cluster centres 
and the allocation of units to clusters.  It does not come close to the 
Schwarz framework.

Nor does clustering fit into Akaike (1973, 1974)'s information framework.

There is discussion in Banfield & Raftery (1993) of a Schwarz-like 
criterion for clustering, but with a rather different derivation and I 
don't think it should be attributed to Schwarz.


On Wed, 5 Dec 2007, Serguei Kaniovski wrote:

>
> Hello,
>
> how is, for example, the Schwarz criterion is defined for kmeans? It should
> be something like:
>
> k <- 2
> vars <- 4
> nobs <- 100
>
> dat <- rbind(matrix(rnorm(nobs, sd = 0.3), ncol = vars),
>           matrix(rnorm(nobs, mean = 1, sd = 0.3), ncol = vars))
>
> colnames(dat) <- paste("var",1:4)
>
> (cl <- kmeans(dat, k))
>
> schwarz <- sum(cl$withinss)+ vars*k*log(nobs)
>
> Thanks for your help,
> Serguei
> ________________________________________
> Austrian Institute of Economic Research (WIFO)
>
> P.O.Box 91                          Tel.: +43-1-7982601-231
> 1103 Vienna, Austria        Fax: +43-1-7989386
>
> Mail: Serguei.Kaniovski at wifo.ac.at
> http://www.wifo.ac.at/Serguei.Kaniovski
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From D.J.Baker at soton.ac.uk  Wed Dec  5 12:29:15 2007
From: D.J.Baker at soton.ac.uk (Baker D.J.)
Date: Wed, 5 Dec 2007 11:29:15 +0000
Subject: [R] Help installing taskPR
Message-ID: <E117AD245282BA48ABE1E16E9712070C0C5A100817@UOS-CL-EX7-L3.soton.ac.uk>

Hello,

I'd like to take a look at how the taskPR package performs on our computational cluster, and this morning I've spend some time trying to do a test install of this package on one of our linux machines. Unfortunately I've been unsuccessful in completing the install, and I wondered if someone could please advise me.

As I've noted I working on a linux machine, and it's running RHEL 4.0. I've build the lam package (we're very familiar with all the mpi libraries and so that is not an issue), and then I attempted to install taskPR...

A). I added the lam binaries to my command search PATH
B). Defined CC=mpicc
C). Started R, and tried the install using
install.packages(c("taskPR_0.2.7.tar.gz"), repos=NULL, configure.args= c(taskPR="--with-mpi='/usr/local/lam-7.1.4'"))

The configure/install fails, and I've appended the output below. I can understand why it's failing, however I don't understand how to fix the problem. Could anyone please shead any light on this issue.

Thanks & Regards,

David Baker.

> install.packages(c("taskPR_0.2.7.tar.gz"), repos=NULL, configure.args= c(taskPR="--with-mpi='/usr/local/lam-7.1.4'"))
* Installing *source* package 'taskPR' ...
creating cache ./config.cache
checking for gcc... mpicc
checking whether the C compiler (mpicc  ) works... yes
checking whether the C compiler (mpicc  ) is a cross-compiler... no
checking whether we are using GNU C... yes
checking whether mpicc accepts -g... yes
checking how to run the C preprocessor... mpicc -E
checking for mpi.h... yes
checking for pthread_detach in -lpthread... yes
checking for MPI_Comm_spawn... yes
checking whether mpicc -Wall is understood... yes
updating cache ./config.cache
creating ./config.status
creating src/Makevars
** libs
** arch -
gcc -std=gnu99 -I/usr/local/R-2.5.0/lib64/R/include -I/usr/local/R-2.5.0/lib64/R/include  -I/usr/local/include   -g -O2  -Wall  -DPARALLEL_R_PACKAGE -fpic  -g -O2 -c spawnhelper.c -o spawnhelper.o
spawnhelper.c:30:17: mpi.h: No such file or directory
spawnhelper.c: In function `main':
spawnhelper.c:41: warning: implicit declaration of function `MPI_Init'
spawnhelper.c:41: error: `MPI_SUCCESS' undeclared (first use in this function)
spawnhelper.c:41: error: (Each undeclared identifier is reported only once
spawnhelper.c:41: error: for each function it appears in.)
spawnhelper.c:46: warning: implicit declaration of function `MPI_Comm_rank'
spawnhelper.c:46: error: `MPI_COMM_WORLD' undeclared (first use in this function)
spawnhelper.c:47: warning: implicit declaration of function `MPI_Comm_size'
spawnhelper.c:49: warning: implicit declaration of function `MPI_Bcast'
spawnhelper.c:49: error: `MPI_INT' undeclared (first use in this function)
spawnhelper.c:75: error: `MPI_CHAR' undeclared (first use in this function)
make: *** [spawnhelper.o] Error 1
ERROR: compilation failed for package 'taskPR'
** Removing '/usr/local/R-2.5.0/lib64/R/library/taskPR'


From jcsantos at student.dei.uc.pt  Wed Dec  5 12:36:05 2007
From: jcsantos at student.dei.uc.pt (Joao Santos)
Date: Wed, 5 Dec 2007 03:36:05 -0800 (PST)
Subject: [R] seasonal time serie with missing values
In-Reply-To: <c59bb5710711270833u3913f2a5y2d00a358e9e228fe@mail.gmail.com>
References: <13609649.post@talk.nabble.com> <13973330.post@talk.nabble.com>
	<fad888a10711270820v67fa631brd57d6cbef44900b5@mail.gmail.com>
	<c59bb5710711270833u3913f2a5y2d00a358e9e228fe@mail.gmail.com>
Message-ID: <14170004.post@talk.nabble.com>


Hello All,

I don't know if help, but I find a package written in Java and .NET that
have a implementation of tramo/seats.
The link http://www.nbb.be/app/dqrd/index.htm

If I have more information I will post it.

Thanks for the reply s, they really help!

Jo?o Santos
 


CCordeiro wrote:
> 
> Thanks a lot for the information J
> Best regards,
> Clara
> 
> 
> 2007/11/27, John C Frain <frainj at gmail.com>:
>>
>> The TRAMO/SEATS suite of programs can interpolate missing values in
>> seasonal time series.  A stand alone version of the software and
>> various papers are available from
>> http://www.bde.es/servicio/software/softwaree.htm.  TRAMO/SEATS is
>> integrated into the econometric package GRETL which interfaces well
>> with R.  (I do not know if you can use the interpolation routines in
>> TRAMO/SEATS from within GRETL but it may be worth trying.
>> You may also be able to call TRAMO/SEATS from within R.
>>
>> Best Regards
>>
>> John Frain.
>>
>> On 27/11/2007, CCordeiro <cmhcordei at gmail.com> wrote:
>> >
>> > Hi
>> > I am interested in this issue also. I have some papers concerning this
>> and
>> > if you want it let me know.
>> >
>> > Best regards,
>> > Clara Cordeiro
>> >
>> >
>> > Joao Santos wrote:
>> > >
>> > > Hello All,
>> > >
>> > > I trying to find some way to fill in missing values in a seasonal
>> time
>>
>> > > series. All the function that I find until now, don't have any
>> reference
>> > > to seasonal data and the output is very different of what I looking
>> for.
>> > > I also searched the forum but this problem don't have many
>> information
>> or
>> > > people asking.
>> > >
>> > > Could someone indicate some links or packages related to this
>> question?
>> > >
>> > >
>> > > Thanks in advance,
>> > >
>> > > Jo?o Santos
>> > >
>> > >
>> > >
>> >
>> > --
>> > View this message in context:
>> http://www.nabble.com/seasonal-time-serie-with-missing-values-tf4758955.html#a13973330
>>
>> > Sent from the R help mailing list archive at
>> Nabble.com<http://nabble.com/>
>> .
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>
>> --
>> John C Frain
>> Trinity College Dublin
>> Dublin 2
>> Ireland
>> www.tcd.ie/Economics/staff/frainj/home.html
>> mailto:frainj at tcd.ie
>> mailto:frainj at gmail.com
>>
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/seasonal-time-serie-with-missing-values-tf4758955.html#a14170004
Sent from the R help mailing list archive at Nabble.com.


From landronimirc at gmail.com  Wed Dec  5 13:02:36 2007
From: landronimirc at gmail.com (Liviu Andronic)
Date: Wed, 5 Dec 2007 13:02:36 +0100
Subject: [R] alternatives to latex() or xtable() ?
Message-ID: <68b1e2610712050402s21370c39o4b89b51f7dd3b2c9@mail.gmail.com>

Hello everyone,

I have several problems with exporting to LaTeX the output of
numSummary() from the abind package.

> numSummary(finance[,"Cash_flow"], statistics=c("mean", "sd", "quantiles"))
     mean       sd      0%   25%   50%      75%    100%  n NA
 188070.9 414771.9 -426804 26743 53866 150975.5 1871500 54  4

> xtable(numSummary(finance[,"Cash_flow"], statistics=c("mean", "sd", "quantiles")))
Error in UseMethod("xtable") : no applicable method for "xtable"

> latex(numSummary(finance[,"Cash_flow"], statistics=c("mean", "sd", "quantiles")))

The latter gives me much more verbose error messages (please see the
attached log). Checking the temporary files, I found the tex code
generated by latex() and tried it in LyX, but the document fails to
compile.

Could anyone please suggest either (1) alternative commands to latex()
or xtable() or (2) a way to tweak the latter commands so that these
work with numSummary()?

Thanks in advance,
Liviu

From ndoye_p at hotmail.com  Wed Dec  5 13:37:49 2007
From: ndoye_p at hotmail.com (Ndoye Souleymane)
Date: Wed, 5 Dec 2007 12:37:49 +0000
Subject: [R] confidence intervals for y predicted in non
	linear	regression
In-Reply-To: <1196780877.47556d4d7feca@webmail.univ-lyon1.fr>
References: <9BD63C7EDBFC234DB982DF636397395E06C957@oceanoastur.gi.ieo.es>
	<1196780877.47556d4d7feca@webmail.univ-lyon1.fr>
Message-ID: <BAY116-W30921566FA9910AB1B1283996E0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/02028e98/attachment.pl 

From berwin at maths.uwa.edu.au  Wed Dec  5 13:37:43 2007
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Wed, 5 Dec 2007 20:37:43 +0800
Subject: [R] Quadratic programming
In-Reply-To: <D0B620C63B10AC41BF1DBD4184C9C50003415F7D@S00MAIL003.ads.sfa.se>
References: <D0B620C63B10AC41BF1DBD4184C9C50003415F7D@S00MAIL003.ads.sfa.se>
Message-ID: <20071205203743.46103c37@absentia>

G'day Serge,

On Wed, 5 Dec 2007 11:25:41 +0100
"de Gosson de Varennes Serge (4100)"
<serge.de.gosson.de.varennes at forsakringskassan.se> wrote:

> I am using the quadprog package and its solve.QP routine to solve and
> quadratic programming problem with inconsistent constraints, which
> obviously doesn't work since the constraint matrix doesn't have full
> rank. 

I guess it will help to fix some terminology first.  In my book,
"inconsistent constraints" are constraints that cannot be fulfilled
simultaneously, e.g. something like "x_1 <= 3" and "x_1 >= 5" for an
obvious example.  Thus, a problem with inconsistent constraints cannot
be solved, regardless of the rank of the constraint matrix.  (Anyway,
that matrix is typically not square, so would be be talking about full
column rank or full row rank?)

Of course, it can happen that the constraints are consistent but that 
there are some redundancy in the specified constraints, e.g. a simply
case would be "x_1 >= 0", "x_2 >= 0" and "x_1+x_2 >= 0"; if the first
two constraints are fulfilled, then the last one is automatically
fulfilled too. In my experience, it can happen that solve.QP comes to
the conclusion that a constraint that ought to be fulfilled, given the
constraints that have already been enforced, is deemed to be violated
and to be inconsistent with the constraints already enforced. In that
case solve.QP stops, rather misleadingly, with the message that the
constraints are inconsistent.  

I guess the package should be worked over by someone with a better
understanding of the kind of fudges that do not come back to bite and of
finite precision arithmetic than the original author's appreciation of
such issues when the code was written. ;-))

> A way to solve this is to "perturb" the objective function and
> the constraints with a parameter that changes at each iteration (so
> you can dismiss it), but that's where it gets tricky! Solve.QP
> doesn't seem to admitt constant terms, it need Dmat (a matrix) and
> dvec (a vector) as defined in the package description. Now, some
> might object that a constant is a vector but the problem looks like
> this
> 
> Min f(x) = (1/2)x^t Q x + D^t x + d

It is a bit unclear to me what you call the constant term.  Is it `d'?
In that case, it does not perturb the constraints and it is irrelevant
for the minimizer of f(x); also for the minimizer of f(x) under linear
constraints.  Regardless of d, the solution is always the same.  I do
not know of any quadratic programming solver that allows `d' as input,
probably because it is irrelevant for determining the solution of the
problem.

> Can anyone help me, PLEASEEEEEEE?

In my experience, rescaling the problem might help, i.e. use Q* = Q/2
and D*=D/2 instead of the original Q and D; but do not forget to
rescale the constraints accordingly.  

Or you might want to try another quadratic program solver in R, e.g.
ipop() in package kernlab.

Hope this helps.

Best wishes,

	Berwin

=========================== Full address =============================
Berwin A Turlach                            Tel.: +65 6516 4416 (secr)
Dept of Statistics and Applied Probability        +65 6516 6650 (self)
Faculty of Science                          FAX : +65 6872 3919       
National University of Singapore     
6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
Singapore 117546                    http://www.stat.nus.edu.sg/~statba


From florencio.gonzalez at GI.IEO.ES  Wed Dec  5 13:40:14 2007
From: florencio.gonzalez at GI.IEO.ES (=?iso-8859-1?Q?Florencio_Gonz=E1lez?=)
Date: Wed, 5 Dec 2007 13:40:14 +0100 
Subject: [R] confidence intervals for y predicted in non linearregression
Message-ID: <9BD63C7EDBFC234DB982DF636397395E06C9C7@oceanoastur.gi.ieo.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/0fb6b955/attachment.pl 

From landronimirc at gmail.com  Wed Dec  5 13:49:54 2007
From: landronimirc at gmail.com (Liviu Andronic)
Date: Wed, 5 Dec 2007 13:49:54 +0100
Subject: [R] alternatives to latex() or xtable() ?
In-Reply-To: <3CBFCFB1FEFFA841BA83ADF2F2A9C6FA048AFC@mango-data1.Mango.local>
References: <68b1e2610712050402s21370c39o4b89b51f7dd3b2c9@mail.gmail.com>
	<3CBFCFB1FEFFA841BA83ADF2F2A9C6FA048AFC@mango-data1.Mango.local>
Message-ID: <68b1e2610712050449o51596944s98c20fcec364b561@mail.gmail.com>

On 12/5/07, Romain Francois <rfrancois at mango-solutions.com> wrote:
>
>
>
> Hello,
>
>  My guess is that you are actually talking about the numSummary function in
> Rcmdr, not in abind. In that case, you can look how the structure of the
> output is like:
>
>  > str( numSummary( iris[,1:4] ) )
>  List of 4
>   $ type      : num 3
>   $ table     : num [1:4, 1:7] 5.843 3.057 3.758 1.199 0.828 ...
>    ..- attr(*, "dimnames")=List of 2
>    .. ..$ : chr [1:4] "Sepal.Length" "Sepal.Width" "Petal.Length"
> "Petal.Width"
>    .. ..$ : chr [1:7] "mean" "sd" "0%" "25%" ...
>   $ statistics: chr [1:3] "mean" "sd" "quantiles"
>   $ n         : Named num [1:4] 150 150 150 150
>    ..- attr(*, "names")= chr [1:4] "Sepal.Length" "Sepal.Width"
> "Petal.Length" "Petal.Width"
>   - attr(*, "class")= chr "numSummary"
>
>  and then use the table element from it:
>
>  > xtable( numSummary( iris[,1:4] )$table )
>  % latex table generated in R 2.6.0 by xtable 1.5-2 package
>  % Wed Dec 05 12:16:44 2007
>  \begin{table}[ht]
>  \begin{center}
>  \begin{tabular}{rrrrrrrr}
>    \hline
>   & mean & sd & 0\% & 25\% & 50\% & 75\% & 100\% \\
>    \hline
>  Sepal.Length & 5.84 & 0.83 & 4.30 & 5.10 & 5.80 & 6.40 & 7.90 \\
>    Sepal.Width & 3.06 & 0.44 & 2.00 & 2.80 & 3.00 & 3.30 & 4.40 \\
>    Petal.Length & 3.76 & 1.77 & 1.00 & 1.60 & 4.35 & 5.10 & 6.90 \\
>    Petal.Width & 1.20 & 0.76 & 0.10 & 0.30 & 1.30 & 1.80 & 2.50 \\
>     \hline
>  \end{tabular}
>  \end{center}
>  \end{table}
>
>  Otherwise, you can define your own xtable.numSummary function that would
> wrap this up. (This one does not do everything as it does not take into
> account the groups argument of numSummary, so you might want to do something
> else if you have used it, ...)
>
>  > xtable.numSummary <- function( x, ...){
>  +  out <- cbind( x$table, n = x$n )
>  +  xtable( out, ... )
>  + }
>  >  xtable( numSummary( iris[,1:4] ) )
>  % latex table generated in R 2.6.0 by xtable 1.5-2 package
>  % Wed Dec 05 12:20:13 2007
>  \begin{table}[ht]
>  \begin{center}
>  \begin{tabular}{rrrrrrrrr}
>    \hline
>   & mean & sd & 0\% & 25\% & 50\% & 75\% & 100\% & n \\
>    \hline
>  Sepal.Length & 5.84 & 0.83 & 4.30 & 5.10 & 5.80 & 6.40 & 7.90 & 150.00 \\
>    Sepal.Width & 3.06 & 0.44 & 2.00 & 2.80 & 3.00 & 3.30 & 4.40 & 150.00 \\
>    Petal.Length & 3.76 & 1.77 & 1.00 & 1.60 & 4.35 & 5.10 & 6.90 & 150.00 \\
>    Petal.Width & 1.20 & 0.76 & 0.10 & 0.30 & 1.30 & 1.80 & 2.50 & 150.00 \\
>     \hline
>  \end{tabular}
>  \end{center}
>  \end{table}
>
>  Hope this helps,
>  Romain

It helped. Thanks.
Liviu


From ggrothendieck at gmail.com  Wed Dec  5 13:52:00 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Dec 2007 07:52:00 -0500
Subject: [R] confidence intervals for y predicted in non linear
	regression
In-Reply-To: <BAY116-W30921566FA9910AB1B1283996E0@phx.gbl>
References: <9BD63C7EDBFC234DB982DF636397395E06C957@oceanoastur.gi.ieo.es>
	<1196780877.47556d4d7feca@webmail.univ-lyon1.fr>
	<BAY116-W30921566FA9910AB1B1283996E0@phx.gbl>
Message-ID: <971536df0712050452y3c40ab1h49570dfe8c1d6431@mail.gmail.com>

I don't think this is referring to the nls2 package on CRAN but
rather something else.

On Dec 5, 2007 7:37 AM, Ndoye Souleymane <ndoye_p at hotmail.com> wrote:
>
> Hi, Salut,
>
> You should use the package nsl2 (only for Linux distribution)
> Vous pouvez utiliser le package nls2 (Linux seulement)
>
> Regards,
>
> Souleymane> Date: Tue, 4 Dec 2007 16:07:57 +0100> From: bady at univ-lyon1.fr> To: florencio.gonzalez at GI.IEO.ES> CC: r-help at stat.math.ethz.ch> Subject: Re: [R] confidence intervals for y predicted in non linear regression> > hi, hi all,> > you can consult these links:> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/43008.html> https://stat.ethz.ch/pipermail/r-help/2004-October/058703.html> > hope this help> > > pierre> > > Selon Florencio Gonz?lez <florencio.gonzalez at GI.IEO.ES>:> > >> > Hi, I?m trying to plot a nonlinear regresion with the confidence bands for> > the curve obtained, similar to what nlintool or nlpredci functions in Matlab> > does, but I no figure how to. In nls the option is there but not implemented> > yet.> >> > Is there a plan to implement the in a relative near future?> >> > Thanks in advance, Florencio> >> >> >> > La informaci?n contenida en este e-mail y sus ficheros adjuntos es totalmente> > confidencial y no deber?a ser usado si no fuera usted alguno de los> > destinatarios. Si ha recibido este e-mail por error, por favor avise al> > remitente y b?rrelo de su buz?n o de cualquier otro medio de almacenamiento.> > This email is confidential and should not be used by anyone who is not the> > original intended recipient. If you have received this e-mail in error> > please inform the sender and delete it from your mailbox or any other> > storage mechanism.> > [[alternative HTML version deleted]]> >> >> > ______________________________________________> R-help at r-project.org mailing list> https://stat.ethz.ch/mailman/listinfo/r-help> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html> and provide commented, minimal, self-contained, reproducible code.
> _________________________________________________________________
> Vous ?tes plut?t Desperate ou LOST ? Personnalisez votre PC avec votre [[replacing trailing spam]]
>
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From blacertain at gmail.com  Wed Dec  5 14:05:10 2007
From: blacertain at gmail.com (bernardo lagos alvarez)
Date: Wed, 5 Dec 2007 10:05:10 -0300
Subject: [R] rgdal for R.2.4.0?
Message-ID: <b28bd39c0712050505v62781e39g6e505a4209cd7a7d@mail.gmail.com>

Hi,
	
Know anyone where to find the package rgdal for R.2.4.0?

I would appreciate advice very much.


Best wishes,

Bernardo Lagos
Department of Statistics
University of Concepci?n


From ripley at stats.ox.ac.uk  Wed Dec  5 14:09:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Dec 2007 13:09:44 +0000 (GMT)
Subject: [R] confidence intervals for y predicted in non linear
	regression
In-Reply-To: <BAY116-W30921566FA9910AB1B1283996E0@phx.gbl>
References: <9BD63C7EDBFC234DB982DF636397395E06C957@oceanoastur.gi.ieo.es>
	<1196780877.47556d4d7feca@webmail.univ-lyon1.fr>
	<BAY116-W30921566FA9910AB1B1283996E0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0712051300270.25629@gannet.stats.ox.ac.uk>

You mean the package nls2 at

http://w3.jouy.inra.fr/unites/miaj/public/AB/nls2/install.html

and not the unfortunately named nls2 that has just appeared on CRAN?

The first is not really a 'package' in the R sense.


On Wed, 5 Dec 2007, Ndoye Souleymane wrote:

>
> Hi, Salut,
>
> You should use the package nsl2 (only for Linux distribution)
> Vous pouvez utiliser le package nls2 (Linux seulement)
>
> Regards,
>
> Souleymane> Date: Tue, 4 Dec 2007 16:07:57 +0100> From: 
> bady at univ-lyon1.fr> To: florencio.gonzalez at GI.IEO.ES> CC: 
> r-help at stat.math.ethz.ch> Subject: Re: [R] confidence intervals for y 
> predicted in non linear regression> > hi, hi all,> > you can consult 
> these links:> 
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/43008.html> 
> https://stat.ethz.ch/pipermail/r-help/2004-October/058703.html> > hope 
> this help> > > pierre> > > Selon Florencio Gonz?lez 
> <florencio.gonzalez at GI.IEO.ES>:> > >> > Hi, I?m trying to plot a 
> nonlinear regresion with the confidence bands for> > the curve obtained, 
> similar to what nlintool or nlpredci functions in Matlab> > does, but I 
> no figure how to. In nls the option is there but not implemented> > 
> yet.> >> > Is there a plan to implement the in a relative near future?> 
> >> > Thanks in advance, Florencio> >> >> >> > La informaci?n contenida 
> en este e-mail y sus ficheros adjuntos es totalmente> > confidencial y 
> no deber?a ser usado si no fuera usted alguno de los> > destinatarios. 
> Si ha recibido este e-mail por error, por favor avise al> > remitente y 
> b?rrelo de su buz?n o de cualquier otro medio de almacenamiento.> > This 
> email is confidential and should not be used by anyone who is not the> > 
> original intended recipient. If you have received this e-mail in error> 
> > please inform the sender and delete it from your mailbox or any other> 
> > storage mechanism.> > [[alternative HTML version deleted]]> >> >> >

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Wed Dec  5 14:14:33 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Dec 2007 13:14:33 +0000 (GMT)
Subject: [R] rgdal for R.2.4.0?
In-Reply-To: <b28bd39c0712050505v62781e39g6e505a4209cd7a7d@mail.gmail.com>
References: <b28bd39c0712050505v62781e39g6e505a4209cd7a7d@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0712051310020.25629@gannet.stats.ox.ac.uk>

On Wed, 5 Dec 2007, bernardo lagos alvarez wrote:

> Hi,
>
> Know anyone where to find the package rgdal for R.2.4.0?

On CRAN: the current version has

Package: rgdal
Title: Bindings for the Geospatial Data Abstraction Library
Version: 0.5-20
Date: 2007-11-07
Depends: R (>= 2.3.0), methods, sp

Or were you looking for a binary version for an unstated platform?  (A 
Windows binary is there and should be available via the Rgui menus.)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Dec  5 14:16:37 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Dec 2007 13:16:37 +0000 (GMT)
Subject: [R] Help installing taskPR
In-Reply-To: <E117AD245282BA48ABE1E16E9712070C0C5A100817@UOS-CL-EX7-L3.soton.ac.uk>
References: <E117AD245282BA48ABE1E16E9712070C0C5A100817@UOS-CL-EX7-L3.soton.ac.uk>
Message-ID: <Pine.LNX.4.64.0712051151540.9512@gannet.stats.ox.ac.uk>

I suggest you ask the maintainer (as does the posting guide).

Red Hat headers for mpi are in a different layout from the sources, and 
recent versions of mpi from earlier ones.  But the path to mpi.h (which I 
guess is /usr/local/lam-7.1.4/include in your setup) is missing.  You 
might try adding it to the packages' PKG_CPPFLAGS or to C_INCLUDE_PATH.

I suspect you will then have problems with the libs.  One I have is that 
the lam libs built from sources need -lutil, and Rmpi and taskPR do not 
allow for that.

There are a lot of other problems with this package: one is that it 
includes a long-outdated (>2.5 years) copy of the private R header Defh.h 
that is incompatible with the current Rinternals.h

As a result the package is basically skipped (a fake install is used) in 
the CRAN checks and I am pretty sure it is not going to work with recent 
versions of R without very extensive changes.

On Wed, 5 Dec 2007, Baker D.J. wrote:

> Hello,
>
> I'd like to take a look at how the taskPR package performs on our computational cluster, and this morning I've spend some time trying to do a test install of this package on one of our linux machines. Unfortunately I've been unsuccessful in completing the install, and I wondered if someone could please advise me.
>
> As I've noted I working on a linux machine, and it's running RHEL 4.0. I've build the lam package (we're very familiar with all the mpi libraries and so that is not an issue), and then I attempted to install taskPR...
>
> A). I added the lam binaries to my command search PATH
> B). Defined CC=mpicc
> C). Started R, and tried the install using
> install.packages(c("taskPR_0.2.7.tar.gz"), repos=NULL, configure.args= c(taskPR="--with-mpi='/usr/local/lam-7.1.4'"))
>
> The configure/install fails, and I've appended the output below. I can understand why it's failing, however I don't understand how to fix the problem. Could anyone please shead any light on this issue.
>
> Thanks & Regards,
>
> David Baker.
>
>> install.packages(c("taskPR_0.2.7.tar.gz"), repos=NULL, configure.args= c(taskPR="--with-mpi='/usr/local/lam-7.1.4'"))
> * Installing *source* package 'taskPR' ...
> creating cache ./config.cache
> checking for gcc... mpicc
> checking whether the C compiler (mpicc  ) works... yes
> checking whether the C compiler (mpicc  ) is a cross-compiler... no
> checking whether we are using GNU C... yes
> checking whether mpicc accepts -g... yes
> checking how to run the C preprocessor... mpicc -E
> checking for mpi.h... yes
> checking for pthread_detach in -lpthread... yes
> checking for MPI_Comm_spawn... yes
> checking whether mpicc -Wall is understood... yes
> updating cache ./config.cache
> creating ./config.status
> creating src/Makevars
> ** libs
> ** arch -
> gcc -std=gnu99 -I/usr/local/R-2.5.0/lib64/R/include -I/usr/local/R-2.5.0/lib64/R/include  -I/usr/local/include   -g -O2  -Wall  -DPARALLEL_R_PACKAGE -fpic  -g -O2 -c spawnhelper.c -o spawnhelper.o
> spawnhelper.c:30:17: mpi.h: No such file or directory
> spawnhelper.c: In function `main':
> spawnhelper.c:41: warning: implicit declaration of function `MPI_Init'
> spawnhelper.c:41: error: `MPI_SUCCESS' undeclared (first use in this function)
> spawnhelper.c:41: error: (Each undeclared identifier is reported only once
> spawnhelper.c:41: error: for each function it appears in.)
> spawnhelper.c:46: warning: implicit declaration of function `MPI_Comm_rank'
> spawnhelper.c:46: error: `MPI_COMM_WORLD' undeclared (first use in this function)
> spawnhelper.c:47: warning: implicit declaration of function `MPI_Comm_size'
> spawnhelper.c:49: warning: implicit declaration of function `MPI_Bcast'
> spawnhelper.c:49: error: `MPI_INT' undeclared (first use in this function)
> spawnhelper.c:75: error: `MPI_CHAR' undeclared (first use in this function)
> make: *** [spawnhelper.o] Error 1
> ERROR: compilation failed for package 'taskPR'
> ** Removing '/usr/local/R-2.5.0/lib64/R/library/taskPR'
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From therneau at mayo.edu  Wed Dec  5 14:22:21 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Wed, 5 Dec 2007 07:22:21 -0600 (CST)
Subject: [R] weighted Cox proportional hazards regression
Message-ID: <200712051322.lB5DMLF06756@hsrnfs-101.mayo.edu>

> I'm getting unexpected results from the coxph function when using
> weights from counter-matching.  For example, the following code
> produces a parameter estimate of -1.59 where I expect 0.63:

  I agree with Thomas' answer wrt using offset instead of weight.  One way to 
understand this is to look at the score equation for the Cox model, which is
  	sum over the deaths of (x[i] - xbar[i])
x[i] is the covariate vector of the ith death
xbar[i] is the average of all the subjects who were at risk at the time of the 
ith death.

  In situations where one samples selected controls, the score equation will be 
correct if one fixes up xbar so that it is an estimate of the population mean 
(all those in the population that were at risk for a death) rather than being 
the mean of just those in the sample.  Use of an offset statement allows one to 
reweight xbar without changing the rest of the score equation.  It's kind of a 
trick, see Therneau and Li, Lifetime Data Analysis, 1999, p99-112 for a simple 
example of how it works.  Langholz and Borgan give details on exactly how to 
correctly reweight using some old results from sampling theory - it is just a 
little bit more subtle than one would guess, but not too different from the 
obvious.
  
  	Terry Therneau


From landronimirc at gmail.com  Wed Dec  5 14:42:49 2007
From: landronimirc at gmail.com (Liviu Andronic)
Date: Wed, 5 Dec 2007 14:42:49 +0100
Subject: [R] Export to LaTeX using xtable() - Control the digits to the
	right of the separator [solved]
Message-ID: <68b1e2610712050542g5ca52840x6b9ee9c0be6028c@mail.gmail.com>

Hello everyone,

The thread title speaks for itself. Here's the code that worked for me:

> numSummary(finance[,"Employees"], statistics=c("mean", "sd", "quantiles"))
     mean       sd   0%  25%  50%  75%   100%  n NA
 11492.92 29373.14 1777 3040 4267 6553 179774 53  5

> str(numSummary(finance[,"Employees"], statistics=c("mean", "sd", "quantiles")))
List of 5
 $ type      : num 3
 $ table     : num [1, 1:7] 11493 29373  1777  3040  4267 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr ""
  .. ..$ : chr [1:7] "mean" "sd" "0%" "25%" ...
 $ statistics: chr [1:3] "mean" "sd" "quantiles"
 $ n         : Named num 53
  ..- attr(*, "names")= chr "data"
 $ NAs       : Named num 5
  ..- attr(*, "names")= chr "data"
 - attr(*, "class")= chr "numSummary"

> xtable(numSummary(finance[,"Employees"], statistics=c("mean", "sd", "quantiles"))$table, digit = c(0,0,2,2,2,0,0,0))
% latex table generated in R 2.6.1 by xtable 1.5-2 package
% Wed Dec  5 14:37:51 2007
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrrrrrr}
  \hline
 & mean & sd & 0\% & 25\% & 50\% & 75\% & 100\% \\
  \hline
1 & 11493 & 29373.14 & 1777.00 & 3040.00 & 4267 & 6553 & 179774 \\
   \hline
\end{tabular}
\end{center}
\end{table}

Regards,
Liviu

---------- Forwarded message ----------
From: Romain Francois <rfrancois at mango-solutions.com>
Date: Dec 5, 2007 2:10 PM
Subject: RE: [R] alternatives to latex() or xtable() ?
To: Liviu Andronic <landronimirc at gmail.com>


You need to look at the digits argument of xtable that would allow you
to control this i think.

 >  xtable( numSummary( iris[,1:4] ) , digit = c( 0, 0, 2,2,2,2,2,2,0) )
 % latex table generated in R 2.6.0 by xtable 1.5-2 package
 % Wed Dec 05 13:07:47 2007
 \begin{table}[ht]
 \begin{center}
 \begin{tabular}{rrrrrrrrr}
   \hline
  & mean & sd & 0\% & 25\% & 50\% & 75\% & 100\% & n \\
   \hline
 Sepal.Length & 6 & 0.83 & 4.30 & 5.10 & 5.80 & 6.40 & 7.90 & 150 \\
   Sepal.Width & 3 & 0.44 & 2.00 & 2.80 & 3.00 & 3.30 & 4.40 & 150 \\
   Petal.Length & 4 & 1.77 & 1.00 & 1.60 & 4.35 & 5.10 & 6.90 & 150 \\
   Petal.Width & 1 & 0.76 & 0.10 & 0.30 & 1.30 & 1.80 & 2.50 & 150 \\
    \hline
 \end{tabular}
 \end{center}
 \end{table}



 -----Original Message-----
 From: Liviu Andronic [mailto:landronimirc at gmail.com]
 Sent: Wed 05/12/2007 13:07
 To: Romain Francois
 Subject: Re: [R] alternatives to latex() or xtable() ?

 I have not yet understood how to set the number of displayed digits
 after the period (not sure how to express correctly in English) in the
 exported TeX code. For example, I would like to make all numbers
 display as integers. Or, I would like to have 123.00 numbers display
 as integers and the rest 123.212(3) display as 123.21. Do you know how
 this is done within R? (I understand that I can perfectly do this
 manually in the TeX code).

 Thanks in advance,
 Liviu


From yonstats at gmail.com  Wed Dec  5 15:07:13 2007
From: yonstats at gmail.com (Yoni Schamroth)
Date: Wed, 5 Dec 2007 16:07:13 +0200
Subject: [R] Mixed Integer Non Linear Programming.
In-Reply-To: <fd6ff0b80712050605lf8b42fal88482e566b71d6c6@mail.gmail.com>
References: <fd6ff0b80712050605lf8b42fal88482e566b71d6c6@mail.gmail.com>
Message-ID: <fd6ff0b80712050607hb1dbf7bhd604fe7a963b2154@mail.gmail.com>

Hi,

I am attempting to solve a Mixed Integer Non Linear Program using R.
I am aware of the GLPK package that solved mixed integer linear
programs, and the Rdonlp2 package that solve linear programs.
Is there any package that that solves the MINLP case?

Any info or pointers would be greatly appreciated.

Jonathan.


From maechler at stat.math.ethz.ch  Tue Dec  4 10:15:45 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 4 Dec 2007 10:15:45 +0100
Subject: [R] about color palettes, colorRamp etc
In-Reply-To: <fj1l8m$jqd$1@ger.gmane.org>
References: <5032046e0711301409q6b0b10b6u34463c0cc3314c01@mail.gmail.com>
	<644e1f320711301608r284235abpd4a535deb0e54921@mail.gmail.com>
	<5032046e0711301658w7589fedax64ded2da1d7e2fd@mail.gmail.com>
	<644e1f320711301725q4e3f2a1bs3b00ce0bd873aac8@mail.gmail.com>
	<5032046e0711301734o2bdd53a5p3da1276bc68b28ba@mail.gmail.com>
	<fj1l8m$jqd$1@ger.gmane.org>
Message-ID: <18261.6849.552348.138200@stat.math.ethz.ch>

       [if you get this twice: it seems to have not made it through, yesterday]

>>>>> "Earl" == Earl F Glynn <efg at stowers-institute.org>
>>>>>     on Mon, 3 Dec 2007 13:26:11 -0600 writes:

    Earl> "affy snp" <affysnp at gmail.com> wrote in message
    Earl> news:5032046e0711301734o2bdd53a5p3da1276bc68b28ba at mail.gmail.com...
    >> For example, it should go from very red---red---less
    >> red---dark----green---very green coinciding with the
    >> descending order of values, just like the very left panel
    >> shown in
    >> http://www.bme.unc.edu/research/Bioinformatics.FunctionalGenomics.html

    Earl> This looks like the MatLab palette that's in
    Earl> tim.colors:

    Earl> library(fields)  # tim.colors:  Matlab-like color palette

    Earl> N <- 100
    Earl> par(lend="square")
    Earl> plot(rep(1,N), type="h", col=tim.colors(N), lwd=6, ylim=c(0,1))

Well, the R help page  ?colorRamp

in its 'examples' section
has an example of this Matlab-lik color scheme, calling them
'jet.colors', easily constructed with the nice
colorRampPalette() function [I've just posted about to R-help as well].

Please say
       
       example(colorRamp)
in R
and slowly watch the output, and I expect you will never ever
want to use the horrible "Matlab-like" color palette again..

Regards,
Martin Maechler, ETH Zurich


    Earl> efg

    Earl> Earl F. Glynn
    Earl> Scientific Programmer
    Earl> Stowers Institute for Medical Research


From Oldeland at gmx.de  Wed Dec  5 15:28:09 2007
From: Oldeland at gmx.de (Jens Oldeland)
Date: Wed, 05 Dec 2007 15:28:09 +0100
Subject: [R] Significance of clarkevans in spatstat
Message-ID: <4756B579.50701@gmx.de>

Dear R-Users,

I was wondering if there is a way to test the significance of the 
clarkevans statistic in spatstat package?
I did not find any related function or the related values to calculate 
it by hand.
does someone has any ideas?

thank you,
Jens

-- 
+++++++++++++++++++++++++++++++++++++++++
Dipl.Biol. Jens Oldeland
University of Hamburg
Biocentre Klein Flottbek and Botanical Garden
Ohnhorststr. 18
22609 Hamburg,
Germany

Tel:    0049-(0)40-42816-407
Fax:    0049-(0)40-42816-543
Mail: 	Oldeland at botanik.uni-hamburg.de
        Jens.Oldeland at DLR.de  (for attachments > 2mb!!)
http://www.biologie.uni-hamburg.de/bzf/fbda005/fbda005.htm
+++++++++++++++++++++++++++++++++++++++++


From rvaradhan at jhmi.edu  Wed Dec  5 15:30:56 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 5 Dec 2007 09:30:56 -0500
Subject: [R] Calculating large determinants
In-Reply-To: <18262.29293.511387.218323@stat.math.ethz.ch>
References: <4455.203.173.149.245.1196817503.squirrel@webmail.scms.waikato.ac.nz>
	<18262.29293.511387.218323@stat.math.ethz.ch>
Message-ID: <000301c8374b$7281e750$7c94100a@win.ad.jhu.edu>

Hi Murray,

A likely reason for the observed information matrix not to be positive
definite is the inaccuracies in the numerical estimation of the scores.  You
might want to try more accurate methods (e.g. Richardson extrapolation) for
approximating numerical derivatives, such as available in the package
"numDeriv".   This would likely alleviate your problem.  

Please disregard my advice, if you are using analytical, closed-form
expressions for the scores.  In this case, you might try the approaches
suggested by Martin.

Ravi.


----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Martin Maechler
Sent: Wednesday, December 05, 2007 4:42 AM
To: maj at stats.waikato.ac.nz
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Calculating large determinants

>>>>> "MJ" == maj  <maj at stats.waikato.ac.nz>
>>>>>     on Wed, 5 Dec 2007 14:18:23 +1300 (NZDT) writes:

    MJ> I apologise for not including a reproducible example
    MJ> with this query but I hope that I can make things clear
    MJ> without one.

    MJ> I am fitting some finite mixture models to data. Each
    MJ> mixture component has p parameters (p=29 in my
    MJ> application) and there are q components to the
    MJ> mixture. The number of data points is n ~ 1500.

    MJ> I need to select a good q and I have been considering model
selection
    MJ> methods suggested in Chapter 6 of
    MJ> @BOOK{mp01,
    MJ> author    = {McLachlan, G. J. and Peel, D.},
    MJ> title     = {Finite Mixture Models},
    MJ> publisher = {Wiley},
    MJ> address   = {New York},
    MJ> year      = {2001}
    MJ> }

    MJ> One of these methods involves an "empirical information
    MJ> matrix" which is the matrix of products of parameter
    MJ> scores at the observation level evaluated at the MLE and
    MJ> summed over all observations. For example a six-component
    MJ> mixture will have 6 - 1 + 29*6 = 179 parameters. So for
    MJ> observation i I form the 179 by 179 matrix of products of
    MJ> scores and sum these up over all 1500-odd observations.

    MJ> Actually it is the log of the determinant of the resultant matrix
that I
    MJ> really need rather than the matrix itself. I am seeking advice on
what may
    MJ> be the best way to evaluate this log(det()). I have been
encountering
    MJ> problems using
    MJ> determinant(SS,logarithm=TRUE)

    MJ> and   eigen(SS,only.values = TRUE)$values

    MJ> shows some negative eigenvalues.

which is a problem?
In that case I guess your problem is that you want to estimate a
positive definite matrix S but your estimate S^ is not quite
positive definite.

Function posdefify() in CRAN package "sfsmisc" provides an old
cheap solution to this problem,
where  nearPD() in package 'Matrix' (based on a donation from
Jens Oehlschlaegel) provides a more sophisticated algorithm for
this problem.

If you really only need the eigenvalues of the "corrected"
matrix, you might want to abbreviate the nearPD() function by
just returning the final 'd' vector of eigenvalues.

    MJ> Advice will be gratefully received!

I'll be glad to hear if and how you'd use one of these two functions.

Martin Maechler, ETH Zurich

    MJ>    Murray Jorgensen

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From vistocco at unicas.it  Wed Dec  5 15:33:45 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Wed, 05 Dec 2007 15:33:45 +0100
Subject: [R] Multiple stacked barplots on the same graph?
In-Reply-To: <47565B16.9010607@genoscope.cns.fr>
References: <47555E3C.9030301@genoscope.cns.fr> <47557529.3050806@unicas.it>	
	<475581AD.9030000@genoscope.cns.fr>
	<f8e6ff050712041015u2cc3aa75x50f343649efcadae@mail.gmail.com>
	<47565B16.9010607@genoscope.cns.fr>
Message-ID: <4756B6C9.2000406@unicas.it>

This command works:

qplot(x=Categorie,y=Total,data=mydata,geom="bar",fill=Part)

for your data.

domenico vistocco

St?phane CRUVEILLER wrote:
> Hi,
>
> the same error message is displayed with geom="bar" as parameter.
> here is the output of dput:
>
> > dput(mydata)
> structure(list(Categorie = structure(c(1L, 12L, 8L, 2L, 5L, 7L,
> 16L, 6L, 15L, 11L, 10L, 13L, 14L, 3L, 4L, 9L, 17L, 1L, 12L, 8L,
> 2L, 5L, 7L, 16L, 6L, 15L, 11L, 10L, 13L, 14L, 3L, 4L, 9L, 17L
> ), .Label = c("Amino acid biosynthesis", "Biosynthesis of cofactors, 
> prosthetic groups, and carriers",
> "Cell envelope", "Cellular processes", "Central intermediary metabolism",
> "DNA metabolism", "Energy metabolism", "Fatty acid and phospholipid 
> metabolism",
> "Mobile and extrachromosomal element functions", "Protein fate",
> "Protein synthesis", "Purines, pyrimidines, nucleosides, and 
> nucleotides",
> "Regulatory functions", "Signal transduction", "Transcription",
> "Transport and binding proteins", "Unknown function"), class = "factor"),
>    Part = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("common",
>    "specific"), class = "factor"), Total = c(3.03, 1.65, 1.52,
>    2.85, 3.4, 11.81, 10.51, 1.95, 2.08, 2.51, 2.23, 7.63, 1.88,
>    2.76, 7.21, 1.08, 20.75, 0.35, 0.17, 0.08, 0.18, 0.42, 2.05,
>    1.98, 0.63, 0.17, 0.2, 0.3, 1.58, 0.27, 0.83, 1.38, 3.56,
>    11.63), chr1 = c(4.55, 2.37, 1.77, 4.68, 3.19, 12.49, 13.56,
>    2.81, 3.13, 4.58, 3.26, 7.3, 2.06, 3.41, 7.9, 0.22, 22.45,
>    0.16, 0.06, 0.09, 0.19, 0.09, 0.7, 0.85, 0.22, 0.06, 0.03,
>    0.32, 0.66, 0.06, 0.63, 0.38, 1.14, 6.17), chr2 = c(1.68,
>    1.06, 1.55, 1.02, 4.57, 13.87, 7.85, 0.98, 1.06, 0.27, 1.2,
>    9.88, 2.13, 2.53, 7.71, 0.4, 22.38, 0.71, 0.35, 0.09, 0.22,
>    0.98, 3.9, 3.24, 0.22, 0.22, 0.49, 0.31, 2.79, 0.62, 1.33,
>    1.95, 0.44, 16), pl = c(0, 0, 0, 0, 0, 0.17, 4.27, 1.03,
>    0.34, 0, 0.68, 0.68, 0, 0.17, 1.54, 8.38, 5.3, 0, 0, 0, 0,
>    0, 2.22, 3.25, 4.44, 0.51, 0, 0.17, 1.88, 0, 0, 4.62, 28.72,
>    24.27)), .Names = c("Categorie", "Part", "Total", "chr1",
> "chr2", "pl"), class = "data.frame", row.names = c(NA, -34L))
>
>
> thx,
>
> St?phane.
>
> hadley wickham wrote:
>> On Dec 4, 2007 10:34 AM, St?phane CRUVEILLER 
>> <scruveil at genoscope.cns.fr> wrote:
>>  
>>> Hi,
>>>
>>> I tried this method but it seems that there is something wrong with my
>>> data frame:
>>>
>>>
>>> when I type in:
>>>
>>>  > qplot(x=as.factor(Categorie),y=Total,data=mydata)
>>>
>>> It displays a graph with 2 points in each category...
>>> but if  I add the parameter geom="histogram"
>>>
>>>  > qplot(x=as.factor(Categorie),y=Total,data=mydata,geom="histogram")
>>>
>>>
>>> Error in storage.mode(test) <- "logical" :
>>>         object "y" not found
>>>
>>> any hint about this...
>>>     
>>
>> Could you copy and paste the output of dput(mydata) ?
>>
>> (And I'd probably write the plot call as: qplot(Categorie, Total,
>> data=mydata, geom="bar"), since it is a bar plot, not a histogram)
>>
>>   
>


From remisorama at gmail.com  Wed Dec  5 15:45:39 2007
From: remisorama at gmail.com (remi sorama)
Date: Wed, 5 Dec 2007 16:45:39 +0200
Subject: [R] how to estimate ARCH?
Message-ID: <bba82c9f0712050645pec46694v4f33bfe5dd8feb55@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/aafb413d/attachment.pl 

From attenka at utu.fi  Wed Dec  5 15:49:54 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Wed, 05 Dec 2007 16:49:54 +0200
Subject: [R] Asymmetrically dependent variables to 2D-map?
Message-ID: <f6c8aae9fd4.4756d6b2@utu.fi>

Hello,

I'm searching for a method which maps variables of this kind of table, see below, to 2-dimensional space, like in multidimensional scaling. However, this table is asymmetric: for example, variable T1 affects T2 more than T2 affects T1(0.41 vs. 0.21). 

> DEPTABLE
     T1    T2     T3    T4
T1 0.00 0.41 0.24 1.18
T2 0.21 0.00 0.46 0.12
T3 0.80 0.89 0.00 0.20
T4 0.09 1.04 0.17 0.00

Any suggestions? Something like gplot+mds+weighted arrays?

Atte Tenkanen
University of Turku, Finland


From ottorino-luca.pantani at unifi.it  Wed Dec  5 16:16:50 2007
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Wed, 05 Dec 2007 16:16:50 +0100
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
Message-ID: <4756C0E2.1060902@unifi.it>

Dear R-users.
I eventually bought myself a new computer with the following 
characteristics:

Processor AMD ATHLON 64 DUAL CORE 4000+ (socket AM2)
Mother board ASR SK-AM2 2
Ram Corsair Value 1 GB DDR2 800 Mhz
Hard Disk WESTERN DIGITAL 160 GB SATA2 8MB

I'm a newcomer to the Linux world.
I started using it (Ubuntu 7.10 at work and FC4 on laptop) on a regular 
basis on May.
I must say I'm quite comfortable with it, even if I have to re-learn a 
lot of things.  But this is not a problem, I will improve my knowledge 
with time.

My main problem now, is that I installed Ubuntu 7.10 Gutsy Gibbon on the 
new one amd64.

To install R on it i followed the directions found here
http://help.nceas.ucsb.edu/index.php/Installing_R_on_Ubuntu

but unfortunately it did not work.

After reading some posts on the R-SIG-debian list, such as

https://stat.ethz.ch/pipermail/r-sig-debian/2007-October/000253.html

I immediately realize that an amd64 is not the right processor to make 
life easy.

Therefore I would like to know from you, how can I solve this problem:
Should I install the i386 version of R ?
Should I install another flavour of Linux ?
Which one ?
Fedora Core 7 ?
Debian ?

Thanks a lot, for any suggestion

-- 
Ottorino-Luca Pantani, Universit? di Firenze
Dip. Scienza del Suolo e Nutrizione della Pianta
P.zle Cascine 28 50144 Firenze Italia
Tel 39 055 3288 202 (348 lab) Fax 39 055 333 273 
OLPantani at unifi.it  http://www4.unifi.it/dssnp/


From h.wickham at gmail.com  Wed Dec  5 16:31:44 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 5 Dec 2007 09:31:44 -0600
Subject: [R] Multiple stacked barplots on the same graph?
In-Reply-To: <4756B6C9.2000406@unicas.it>
References: <47555E3C.9030301@genoscope.cns.fr> <47557529.3050806@unicas.it>
	<475581AD.9030000@genoscope.cns.fr>
	<f8e6ff050712041015u2cc3aa75x50f343649efcadae@mail.gmail.com>
	<47565B16.9010607@genoscope.cns.fr> <4756B6C9.2000406@unicas.it>
Message-ID: <f8e6ff050712050731g1148b272w1acd901bc029509d@mail.gmail.com>

And qplot(x=Categorie,y=Total,data=mydata,geom="bar",fill=Part) + coord_flip()

makes it a bit easier to read the labels.

Hadley

On Dec 5, 2007 8:33 AM, Domenico Vistocco <vistocco at unicas.it> wrote:
> This command works:
>
> qplot(x=Categorie,y=Total,data=mydata,geom="bar",fill=Part)
>
> for your data.
>
> domenico vistocco
>
> St?phane CRUVEILLER wrote:
>
> > Hi,
> >
> > the same error message is displayed with geom="bar" as parameter.
> > here is the output of dput:
> >
> > > dput(mydata)
> > structure(list(Categorie = structure(c(1L, 12L, 8L, 2L, 5L, 7L,
> > 16L, 6L, 15L, 11L, 10L, 13L, 14L, 3L, 4L, 9L, 17L, 1L, 12L, 8L,
> > 2L, 5L, 7L, 16L, 6L, 15L, 11L, 10L, 13L, 14L, 3L, 4L, 9L, 17L
> > ), .Label = c("Amino acid biosynthesis", "Biosynthesis of cofactors,
> > prosthetic groups, and carriers",
> > "Cell envelope", "Cellular processes", "Central intermediary metabolism",
> > "DNA metabolism", "Energy metabolism", "Fatty acid and phospholipid
> > metabolism",
> > "Mobile and extrachromosomal element functions", "Protein fate",
> > "Protein synthesis", "Purines, pyrimidines, nucleosides, and
> > nucleotides",
> > "Regulatory functions", "Signal transduction", "Transcription",
> > "Transport and binding proteins", "Unknown function"), class = "factor"),
> >    Part = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> >    1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> >    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("common",
> >    "specific"), class = "factor"), Total = c(3.03, 1.65, 1.52,
> >    2.85, 3.4, 11.81, 10.51, 1.95, 2.08, 2.51, 2.23, 7.63, 1.88,
> >    2.76, 7.21, 1.08, 20.75, 0.35, 0.17, 0.08, 0.18, 0.42, 2.05,
> >    1.98, 0.63, 0.17, 0.2, 0.3, 1.58, 0.27, 0.83, 1.38, 3.56,
> >    11.63), chr1 = c(4.55, 2.37, 1.77, 4.68, 3.19, 12.49, 13.56,
> >    2.81, 3.13, 4.58, 3.26, 7.3, 2.06, 3.41, 7.9, 0.22, 22.45,
> >    0.16, 0.06, 0.09, 0.19, 0.09, 0.7, 0.85, 0.22, 0.06, 0.03,
> >    0.32, 0.66, 0.06, 0.63, 0.38, 1.14, 6.17), chr2 = c(1.68,
> >    1.06, 1.55, 1.02, 4.57, 13.87, 7.85, 0.98, 1.06, 0.27, 1.2,
> >    9.88, 2.13, 2.53, 7.71, 0.4, 22.38, 0.71, 0.35, 0.09, 0.22,
> >    0.98, 3.9, 3.24, 0.22, 0.22, 0.49, 0.31, 2.79, 0.62, 1.33,
> >    1.95, 0.44, 16), pl = c(0, 0, 0, 0, 0, 0.17, 4.27, 1.03,
> >    0.34, 0, 0.68, 0.68, 0, 0.17, 1.54, 8.38, 5.3, 0, 0, 0, 0,
> >    0, 2.22, 3.25, 4.44, 0.51, 0, 0.17, 1.88, 0, 0, 4.62, 28.72,
> >    24.27)), .Names = c("Categorie", "Part", "Total", "chr1",
> > "chr2", "pl"), class = "data.frame", row.names = c(NA, -34L))
> >
> >
> > thx,
> >
> > St?phane.
> >
> > hadley wickham wrote:
> >> On Dec 4, 2007 10:34 AM, St?phane CRUVEILLER
> >> <scruveil at genoscope.cns.fr> wrote:
> >>
> >>> Hi,
> >>>
> >>> I tried this method but it seems that there is something wrong with my
> >>> data frame:
> >>>
> >>>
> >>> when I type in:
> >>>
> >>>  > qplot(x=as.factor(Categorie),y=Total,data=mydata)
> >>>
> >>> It displays a graph with 2 points in each category...
> >>> but if  I add the parameter geom="histogram"
> >>>
> >>>  > qplot(x=as.factor(Categorie),y=Total,data=mydata,geom="histogram")
> >>>
> >>>
> >>> Error in storage.mode(test) <- "logical" :
> >>>         object "y" not found
> >>>
> >>> any hint about this...
> >>>
> >>
> >> Could you copy and paste the output of dput(mydata) ?
> >>
> >> (And I'd probably write the plot call as: qplot(Categorie, Total,
> >> data=mydata, geom="bar"), since it is a bar plot, not a histogram)
> >>
> >>
> >
>
>



-- 
http://had.co.nz/


From jombart at biomserv.univ-lyon1.fr  Wed Dec  5 16:37:08 2007
From: jombart at biomserv.univ-lyon1.fr (Thibaut Jombart)
Date: Wed, 05 Dec 2007 16:37:08 +0100
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <4756C0E2.1060902@unifi.it>
References: <4756C0E2.1060902@unifi.it>
Message-ID: <4756C5A4.3080908@biomserv.univ-lyon1.fr>

8rino-Luca Pantani wrote:

>Dear R-users.
>I eventually bought myself a new computer with the following 
>characteristics:
>
>Processor AMD ATHLON 64 DUAL CORE 4000+ (socket AM2)
>Mother board ASR SK-AM2 2
>Ram Corsair Value 1 GB DDR2 800 Mhz
>Hard Disk WESTERN DIGITAL 160 GB SATA2 8MB
>
>I'm a newcomer to the Linux world.
>I started using it (Ubuntu 7.10 at work and FC4 on laptop) on a regular 
>basis on May.
>I must say I'm quite comfortable with it, even if I have to re-learn a 
>lot of things.  But this is not a problem, I will improve my knowledge 
>with time.
>
>My main problem now, is that I installed Ubuntu 7.10 Gutsy Gibbon on the 
>new one amd64.
>
>To install R on it i followed the directions found here
>http://help.nceas.ucsb.edu/index.php/Installing_R_on_Ubuntu
>
>but unfortunately it did not work.
>
>After reading some posts on the R-SIG-debian list, such as
>
>https://stat.ethz.ch/pipermail/r-sig-debian/2007-October/000253.html
>
>I immediately realize that an amd64 is not the right processor to make 
>life easy.
>
>Therefore I would like to know from you, how can I solve this problem:
>Should I install the i386 version of R ?
>Should I install another flavour of Linux ?
>Which one ?
>Fedora Core 7 ?
>Debian ?
>
>Thanks a lot, for any suggestion
>
>  
>
Hi,

I've got an Athlon 64bits 3000+ processor and Ubuntu LTS (dapper) 
installed (64 bits version) on my laptop. I do not have any problem to 
install R from the sources, as long as the correct 
libraries/compilers/etc. are installed. But it is no pain if you just 
follow what the configure script tells you (and use apt-get to install 
missing packages). I guess a common mistake is to forget to install 
"-dev" versions of packages, which sometimes contain required headers. 
However, you should not have troubles installing R on different R 
distributions, 64bits or not.

Hope this help.

Thibaut, 64bit-linux-Ruser and still alive.

-- 
######################################
Thibaut JOMBART
CNRS UMR 5558 - Laboratoire de Biom?trie et Biologie Evolutive
Universite Lyon 1
43 bd du 11 novembre 1918
69622 Villeurbanne Cedex
T?l. : 04.72.43.29.35
Fax : 04.72.43.13.88
jombart at biomserv.univ-lyon1.fr
http://lbbe.univ-lyon1.fr/-Jombart-Thibaut-.html?lang=en
http://pbil.univ-lyon1.fr/software/adegenet/


From scionforbai at gmail.com  Wed Dec  5 16:47:34 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Wed, 5 Dec 2007 16:47:34 +0100
Subject: [R] 2/3d interpolation from a regular grid to another regular
	grid
In-Reply-To: <85D2CAEE-0DAC-4451-A7D2-DC45969F1E14@gmail.com>
References: <17EF7ABB-B4F8-4279-B9D0-620C02C91E84@gmail.com>
	<e9ee1f0a0712041238w4ff935fbg80f8f26895e02277@mail.gmail.com>
	<85D2CAEE-0DAC-4451-A7D2-DC45969F1E14@gmail.com>
Message-ID: <e9ee1f0a0712050747u256186caj7a3a237f9aded948@mail.gmail.com>

> I just read the description in ?Krig in the package fields which says:
> " Fits a surface to irregularly spaced data. "

Yes, that is the most general case. Regular data location is a subset
of irregular. Anyway, kriging, just one g, after the name of Danie
Krige, the south african statistician who first applied such method
for minig survey.

> My problem is simpler
...
> So it is really purely numerical.
...
> I just hoped that R had that already coded ...

Of course R has ... ;) If your grids are really as simple as the
example you posted above, and you have a really little variability,
all you need is a "moving average", the arithmetic mean of the two
nearest points belonging to grid1 and grid2 respectively. I assume
that your regularly shaped grids are values stored in matrix objects.

The functions comes from the "diff.default" code (downloading the R
source code, I assure, is worth):

my.interp <- function(x, lag = 1)
{
    r <- unclass(x)  # don't want class-specific subset methods
    i1 <- -1:-lag
    r <- (r[i1] + r[-length(r):-(length(r)-lag+1)])/2
    class(r) <- oldClass(x)
    return(r)
}

Finally,

g1 <- apply(grid1val,1,my.interp)
g2 <- apply(grid2val,2,my.interp)

give the interpolations on gridFinal, provided that all gridFinal
points are within the grid1 and grid2 ones.

If you want the mean from 4 points, you apply once more with lag=3,
cbind/rbind to the result columns/rows o NAs, and you calculate the
mean of the points of the two matrixes.
This is the simplest (and quickest) moving average that you can do.
For more complicated examples, and for 3d, you have to go a little
further, but the principle holds.

ScionForbai


From bolker at ufl.edu  Wed Dec  5 16:54:16 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 5 Dec 2007 07:54:16 -0800 (PST)
Subject: [R] Plotting error bars in xy-direction
In-Reply-To: <loom.20071205T094247-719@post.gmane.org>
References: <loom.20071205T094247-719@post.gmane.org>
Message-ID: <14174151.post@talk.nabble.com>




Hans W Borchers wrote:
> 
> Dear R-help,
> 
> I am looking for a function that will plot error bars in x- or y-direction
> (or 
> both), the same as the Gnuplot function 'plot' can achieve with:
> 
>     plot "file.dat" with xyerrorbars,...
> 
> Rsite-searching led me to the functions 'errbar' and 'plotCI' in the
> Hmisc, 
> gregmisc, and plotrix packages. As I understand the descriptions and
> examples, 
> none of these functions provides horizontal error bars.
> 
> Looking into 'errbar' and using segments, I wrote a small function for
> myself 
> adding these kinds of error bars to existing plots. I would still be
> interested 
> to know what the standard R solution is.
> 
> Regards,  Hans Werner
> 
> 

plotCI from plotrix will do horizontal error bars --
from ?plotCI:

  err: The direction of error bars: "x" for horizontal, "y" for
          vertical ("xy" would be nice but is not implemented yet;
          don't know quite how everything would be specified.  See
          examples for composing a plot with simultaneous horizontal
          and vertical error bars)

  Ben Bolker

  
-- 
View this message in context: http://www.nabble.com/Plotting-error-bars-in-xy-direction-tf4948535.html#a14174151
Sent from the R help mailing list archive at Nabble.com.


From david at incogen.com  Wed Dec  5 17:16:19 2007
From: david at incogen.com (David Coppit)
Date: Wed, 5 Dec 2007 08:16:19 -0800
Subject: [R] Java parser for R data file?
Message-ID: <C37C3903.56%david@incogen.com>

Hi everyone,

Has anyone written a parser in Java for either the ASCII or binary format
produced by save()? I need to parse a single large 2D array that is
structured like this:

list(
  "32609_1" = c(-9549.39231289146, -9574.07159324482, ... ),
  "32610_2" = c(-6369.12526971635, -6403.99620977124, ... ),
  "32618_2" = c(-2138.29095689061, -2057.9229403233, ... ),
        ...
)

Or, given that I'm dealing with just a single array, would it be better to
roll my own I/O using write.table or write.matrix from the MASS package?

Thanks,
David


From scionforbai at gmail.com  Wed Dec  5 17:17:58 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Wed, 5 Dec 2007 17:17:58 +0100
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <4756C0E2.1060902@unifi.it>
References: <4756C0E2.1060902@unifi.it>
Message-ID: <e9ee1f0a0712050817y3e10df06w20dbd8916769f235@mail.gmail.com>

> but unfortunately it did not work.

What did not work? Provide some information more...
By the way, isn't 'gfortran' the new GNU fortran compiler which
replaced 'g77'? Or not on Ubuntu?

> I immediately realize that an amd64 is not the right processor to make
> life easy.

??? Example of bad extrapolation ;)

> Should I install the i386 version of R ?

I assume you are talking about installing from source. You installed
the i386 version of Ubuntu? Then yes. Else no.

But rather let apt-get do it for you. Just install the binary provided
by the Ubuntu community. It is the best way to get things working and
avoid problems.

> Should I install another flavour of Linux ?

It depends. Ubuntu is good to start, and has the widest users base;
Archlinux my best choice (but you need to be already somewhat
advanced).

ScionForbai


From rvaradhan at jhmi.edu  Wed Dec  5 17:21:54 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 5 Dec 2007 11:21:54 -0500
Subject: [R] Calculating large determinants
In-Reply-To: <18262.29293.511387.218323@stat.math.ethz.ch>
References: <4455.203.173.149.245.1196817503.squirrel@webmail.scms.waikato.ac.nz>
	<18262.29293.511387.218323@stat.math.ethz.ch>
Message-ID: <000001c8375a$f394aad0$7c94100a@win.ad.jhu.edu>

Hi Murray,

A likely reason for the observed information matrix not to be positive
definite is the inaccuracies in the numerical estimation of the scores.  You
might want to try more accurate methods (e.g. Richardson extrapolation) for
approximating numerical derivatives, such as available in the package
"numDeriv".   This would likely alleviate your problem.  

Please disregard my advice, if you are using analytical, closed-form
expressions for the scores.  In this case, you might try the approaches
suggested by Martin.

It is also likely that your likelihood is quite flat around the MLE, which,
of course, means that you don't have enough information in your data to
separate the mixtures and estimate 179 parameters.  Have you tried
estimating with fewer mixture components and then working your way up?
Also, with this many parameters and relatively few data points, can you
ensure that you have a "global" maximum?  


Ravi.


----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Martin Maechler
Sent: Wednesday, December 05, 2007 4:42 AM
To: maj at stats.waikato.ac.nz
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Calculating large determinants

>>>>> "MJ" == maj  <maj at stats.waikato.ac.nz>
>>>>>     on Wed, 5 Dec 2007 14:18:23 +1300 (NZDT) writes:

    MJ> I apologise for not including a reproducible example
    MJ> with this query but I hope that I can make things clear
    MJ> without one.

    MJ> I am fitting some finite mixture models to data. Each
    MJ> mixture component has p parameters (p=29 in my
    MJ> application) and there are q components to the
    MJ> mixture. The number of data points is n ~ 1500.

    MJ> I need to select a good q and I have been considering model
selection
    MJ> methods suggested in Chapter 6 of
    MJ> @BOOK{mp01,
    MJ> author    = {McLachlan, G. J. and Peel, D.},
    MJ> title     = {Finite Mixture Models},
    MJ> publisher = {Wiley},
    MJ> address   = {New York},
    MJ> year      = {2001}
    MJ> }

    MJ> One of these methods involves an "empirical information
    MJ> matrix" which is the matrix of products of parameter
    MJ> scores at the observation level evaluated at the MLE and
    MJ> summed over all observations. For example a six-component
    MJ> mixture will have 6 - 1 + 29*6 = 179 parameters. So for
    MJ> observation i I form the 179 by 179 matrix of products of
    MJ> scores and sum these up over all 1500-odd observations.

    MJ> Actually it is the log of the determinant of the resultant matrix
that I
    MJ> really need rather than the matrix itself. I am seeking advice on
what may
    MJ> be the best way to evaluate this log(det()). I have been
encountering
    MJ> problems using
    MJ> determinant(SS,logarithm=TRUE)

    MJ> and   eigen(SS,only.values = TRUE)$values

    MJ> shows some negative eigenvalues.

which is a problem?
In that case I guess your problem is that you want to estimate a
positive definite matrix S but your estimate S^ is not quite
positive definite.

Function posdefify() in CRAN package "sfsmisc" provides an old
cheap solution to this problem,
where  nearPD() in package 'Matrix' (based on a donation from
Jens Oehlschlaegel) provides a more sophisticated algorithm for
this problem.

If you really only need the eigenvalues of the "corrected"
matrix, you might want to abbreviate the nearPD() function by
just returning the final 'd' vector of eigenvalues.

    MJ> Advice will be gratefully received!

I'll be glad to hear if and how you'd use one of these two functions.

Martin Maechler, ETH Zurich

    MJ>    Murray Jorgensen

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ljubomir at sfsu.edu  Wed Dec  5 17:39:42 2007
From: ljubomir at sfsu.edu (Ljubomir J. Buturovic)
Date: Wed, 5 Dec 2007 08:39:42 -0800
Subject: [R]  Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <4756C0E2.1060902@unifi.it>
References: <4756C0E2.1060902@unifi.it>
Message-ID: <18262.54350.143549.665409@localhost.localdomain>


Hi Ottorino,

I have been using R on 64-bit Ubuntu for about a year without
problems, both Intel and AMD CPUs. Installing and using several
packages (e1071, svmpath, survival) also works. However, I had to
install R from source:

$ gunzip -c R-2.6.1.tar.gz | tar xvf -
$ cd R-2.6.1
$ ./configure --enable-R-shlib; make; make pdf
# make install; make install-pdf

Notice that `make install' has to be run as root.

I am using Feisty Fawn (Ubuntu 7.04), although I doubt that makes a
difference.

Hope this helps,

Ljubomir

8rino-Luca Pantani writes:
 > Dear R-users.
 > I eventually bought myself a new computer with the following 
 > characteristics:
 > 
 > Processor AMD ATHLON 64 DUAL CORE 4000+ (socket AM2)
 > Mother board ASR SK-AM2 2
 > Ram Corsair Value 1 GB DDR2 800 Mhz
 > Hard Disk WESTERN DIGITAL 160 GB SATA2 8MB
 > 
 > I'm a newcomer to the Linux world.
 > I started using it (Ubuntu 7.10 at work and FC4 on laptop) on a regular 
 > basis on May.
 > I must say I'm quite comfortable with it, even if I have to re-learn a 
 > lot of things.  But this is not a problem, I will improve my knowledge 
 > with time.
 > 
 > My main problem now, is that I installed Ubuntu 7.10 Gutsy Gibbon on the 
 > new one amd64.
 > 
 > To install R on it i followed the directions found here
 > http://help.nceas.ucsb.edu/index.php/Installing_R_on_Ubuntu
 > 
 > but unfortunately it did not work.
 > 
 > After reading some posts on the R-SIG-debian list, such as
 > 
 > https://stat.ethz.ch/pipermail/r-sig-debian/2007-October/000253.html
 > 
 > I immediately realize that an amd64 is not the right processor to make 
 > life easy.
 > 
 > Therefore I would like to know from you, how can I solve this problem:
 > Should I install the i386 version of R ?
 > Should I install another flavour of Linux ?
 > Which one ?
 > Fedora Core 7 ?
 > Debian ?
 > 
 > Thanks a lot, for any suggestion
 > 
 > -- 
 > Ottorino-Luca Pantani, Universit? di Firenze
 > Dip. Scienza del Suolo e Nutrizione della Pianta
 > P.zle Cascine 28 50144 Firenze Italia
 > Tel 39 055 3288 202 (348 lab) Fax 39 055 333 273 
 > OLPantani at unifi.it  http://www4.unifi.it/dssnp/
 > 
 > ______________________________________________
 > R-help at r-project.org mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


From ranjan.bagchi at frotz.com  Wed Dec  5 18:03:33 2007
From: ranjan.bagchi at frotz.com (Ranjan Bagchi)
Date: Wed, 5 Dec 2007 09:03:33 -0800 (PST)
Subject: [R] newbie lapply question
Message-ID: <Pine.LNX.4.63.0712050901120.21474@manjula.frotz.bogus>

Hi --

I just noticed the following (R 2.6.1 on OSX)

> lapply(c(as.Date('2007-01-01')), I)
[[1]]
[1] 13514

This is a bit surprising.. Why does lapply unclass the object?  Sorry for 
such a basic question, I don't seem able to produce the right google keywords.

Ranjan


From P.Dalgaard at biostat.ku.dk  Wed Dec  5 18:11:40 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 05 Dec 2007 18:11:40 +0100
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <4756C0E2.1060902@unifi.it>
References: <4756C0E2.1060902@unifi.it>
Message-ID: <4756DBCC.6030602@biostat.ku.dk>

8rino-Luca Pantani wrote:
> Dear R-users.
> I eventually bought myself a new computer with the following 
> characteristics:
>
> Processor AMD ATHLON 64 DUAL CORE 4000+ (socket AM2)
> Mother board ASR SK-AM2 2
> Ram Corsair Value 1 GB DDR2 800 Mhz
> Hard Disk WESTERN DIGITAL 160 GB SATA2 8MB
>
> I'm a newcomer to the Linux world.
> I started using it (Ubuntu 7.10 at work and FC4 on laptop) on a regular 
> basis on May.
> I must say I'm quite comfortable with it, even if I have to re-learn a 
> lot of things.  But this is not a problem, I will improve my knowledge 
> with time.
>
> My main problem now, is that I installed Ubuntu 7.10 Gutsy Gibbon on the 
> new one amd64.
>
> To install R on it i followed the directions found here
> http://help.nceas.ucsb.edu/index.php/Installing_R_on_Ubuntu
>
> but unfortunately it did not work.
>
> After reading some posts on the R-SIG-debian list, such as
>
> https://stat.ethz.ch/pipermail/r-sig-debian/2007-October/000253.html
>
> I immediately realize that an amd64 is not the right processor to make 
> life easy.
>
> Therefore I would like to know from you, how can I solve this problem:
> Should I install the i386 version of R ?
> Should I install another flavour of Linux ?
> Which one ?
> Fedora Core 7 ?
> Debian ?
>
> Thanks a lot, for any suggestion
>
>   
Amd64 architecture should not be a major issue for R on any of the major
platforms, as far as I know. I have Fedora 7 (soon-ish F8) on the "big"
machine back home (dual Opteron) and that one never had any major issues
with either of source builds or the "official" RPMs. The main (only?)
thing that still tends to bite people on 64 bit is browser plugins,
notably Java.

In general, I've been happy with Fedora, although its desire to update
itself constantly does require a good 'Net connection.

My SUSE desktop at work is also 64 bit and happy to deal with R (in fact
the official release builds are made on it). The KDE desktop has a few
annoying misfeatures (to me), though, and you need a little special
setup to include Detlefs repository as an install source.

One oddity about Ubuntu is that there are no CRAN builds for 64bit.
Presumably, the Debian packages work, or you can get the build script
and make your own build. This is not really within my domain, though.


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Wed Dec  5 18:15:31 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Dec 2007 17:15:31 +0000 (GMT)
Subject: [R] Java parser for R data file?
In-Reply-To: <C37C3903.56%david@incogen.com>
References: <C37C3903.56%david@incogen.com>
Message-ID: <Pine.LNX.4.64.0712051712040.12631@gannet.stats.ox.ac.uk>

On Wed, 5 Dec 2007, David Coppit wrote:

> Hi everyone,
>
> Has anyone written a parser in Java for either the ASCII or binary format
> produced by save()? I need to parse a single large 2D array that is
> structured like this:
>
> list(
>  "32609_1" = c(-9549.39231289146, -9574.07159324482, ... ),
>  "32610_2" = c(-6369.12526971635, -6403.99620977124, ... ),
>  "32618_2" = c(-2138.29095689061, -2057.9229403233, ... ),
>        ...
> )
>
> Or, given that I'm dealing with just a single array, would it be better to
> roll my own I/O using write.table or write.matrix from the MASS package?

It would be much easier.  The save() format is far more complex than you 
need.  However, I would use writeBin() to write a binary file and read 
that in in Java, avoiding the binary -> ASCII -> binary conversion.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Dec  5 18:19:40 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Dec 2007 17:19:40 +0000 (GMT)
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <18262.54350.143549.665409@localhost.localdomain>
References: <4756C0E2.1060902@unifi.it>
	<18262.54350.143549.665409@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0712051655180.12631@gannet.stats.ox.ac.uk>

Note that Ottorino has only 1GB of RAM installed, which makes a 64-bit 
version of R somewhat moot.  See chapter 8 of

http://cran.r-project.org/doc/manuals/R-admin.html

I would install a i386 version of R on x86_64 Linux unless I had 2Gb or 
more of RAM.  I don't know how easily that works on Ubuntu these days, but 
I would try it.


On Wed, 5 Dec 2007, Ljubomir J. Buturovic wrote:

>
> Hi Ottorino,
>
> I have been using R on 64-bit Ubuntu for about a year without
> problems, both Intel and AMD CPUs. Installing and using several
> packages (e1071, svmpath, survival) also works. However, I had to
> install R from source:
>
> $ gunzip -c R-2.6.1.tar.gz | tar xvf -
> $ cd R-2.6.1
> $ ./configure --enable-R-shlib; make; make pdf
> # make install; make install-pdf
>
> Notice that `make install' has to be run as root.
>
> I am using Feisty Fawn (Ubuntu 7.04), although I doubt that makes a
> difference.
>
> Hope this helps,
>
> Ljubomir
>
> 8rino-Luca Pantani writes:
> > Dear R-users.
> > I eventually bought myself a new computer with the following
> > characteristics:
> >
> > Processor AMD ATHLON 64 DUAL CORE 4000+ (socket AM2)
> > Mother board ASR SK-AM2 2
> > Ram Corsair Value 1 GB DDR2 800 Mhz
> > Hard Disk WESTERN DIGITAL 160 GB SATA2 8MB
> >
> > I'm a newcomer to the Linux world.
> > I started using it (Ubuntu 7.10 at work and FC4 on laptop) on a regular
> > basis on May.
> > I must say I'm quite comfortable with it, even if I have to re-learn a
> > lot of things.  But this is not a problem, I will improve my knowledge
> > with time.
> >
> > My main problem now, is that I installed Ubuntu 7.10 Gutsy Gibbon on the
> > new one amd64.
> >
> > To install R on it i followed the directions found here
> > http://help.nceas.ucsb.edu/index.php/Installing_R_on_Ubuntu
> >
> > but unfortunately it did not work.
> >
> > After reading some posts on the R-SIG-debian list, such as
> >
> > https://stat.ethz.ch/pipermail/r-sig-debian/2007-October/000253.html
> >
> > I immediately realize that an amd64 is not the right processor to make
> > life easy.
> >
> > Therefore I would like to know from you, how can I solve this problem:
> > Should I install the i386 version of R ?
> > Should I install another flavour of Linux ?
> > Which one ?
> > Fedora Core 7 ?
> > Debian ?
> >
> > Thanks a lot, for any suggestion
> >
> > --
> > Ottorino-Luca Pantani, Universit? di Firenze
> > Dip. Scienza del Suolo e Nutrizione della Pianta
> > P.zle Cascine 28 50144 Firenze Italia
> > Tel 39 055 3288 202 (348 lab) Fax 39 055 333 273
> > OLPantani at unifi.it  http://www4.unifi.it/dssnp/

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Wed Dec  5 18:24:43 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Dec 2007 17:24:43 +0000 (GMT)
Subject: [R] newbie lapply question
In-Reply-To: <Pine.LNX.4.63.0712050901120.21474@manjula.frotz.bogus>
References: <Pine.LNX.4.63.0712050901120.21474@manjula.frotz.bogus>
Message-ID: <Pine.LNX.4.64.0712051716020.12631@gannet.stats.ox.ac.uk>

On Wed, 5 Dec 2007, Ranjan Bagchi wrote:

> Hi --
>
> I just noticed the following (R 2.6.1 on OSX)
>
>> lapply(c(as.Date('2007-01-01')), I)
> [[1]]
> [1] 13514
>
> This is a bit surprising.. Why does lapply unclass the object?  Sorry for
> such a basic question, I don't seem able to produce the right google keywords.

Did you not read the help page?:

Arguments:

        X: a vector (atomic or list) or an expressions vector.  Other
           objects (including classed objects) will be coerced by
           'as.list'.

and

> as.list(c(as.Date('2007-01-01')))
[[1]]
[1] 13514

BTW, the c() is redundant here: you are concatenating one item only.

As to why as.list() removes the class, read its help page which tells you.

Perhaps if you told us what you are trying to achieve we might be able to 
help you achieve it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ranjan.bagchi at frotz.com  Wed Dec  5 18:29:28 2007
From: ranjan.bagchi at frotz.com (Ranjan Bagchi)
Date: Wed, 5 Dec 2007 09:29:28 -0800 (PST)
Subject: [R] newbie lapply question
In-Reply-To: <Pine.LNX.4.64.0712051716020.12631@gannet.stats.ox.ac.uk>
References: <Pine.LNX.4.63.0712050901120.21474@manjula.frotz.bogus>
	<Pine.LNX.4.64.0712051716020.12631@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.63.0712050926340.21474@manjula.frotz.bogus>



On Wed, 5 Dec 2007, Prof Brian Ripley wrote:
>[...]

Thanks I'll read it more carefully.

> Perhaps if you told us what you are trying to achieve we might be able to 
> help you achieve it.
>

I have a function which takes a date as an argument.  I've tested it, and 
I'd like to run it over a range of dates.  So I'm looking at apply- or 
map- type functions.

> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>
>


From maechler at stat.math.ethz.ch  Wed Dec  5 18:42:40 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 5 Dec 2007 18:42:40 +0100
Subject: [R] R function for percentrank
In-Reply-To: <1196537601.2980.15.camel@Bellerophon.localdomain>
References: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
	<243517.68093.qm@web32807.mail.mud.yahoo.com>
	<65cc7bdf0712010951p451a993i70da89f285d801de@mail.gmail.com>
	<Xns99F989B3A3057dNOTwinscomcast@80.91.229.13>
	<Xns99F98B11F90B6dNOTwinscomcast@80.91.229.13>
	<1196537601.2980.15.camel@Bellerophon.localdomain>
Message-ID: <18262.58128.974202.67709@stat.math.ethz.ch>

I'm coming late to this, but this *does* need a correction
just for the archives !

>>>>> "MS" == Marc Schwartz <marc_schwartz at comcast.net>
>>>>>     on Sat, 01 Dec 2007 13:33:21 -0600 writes:

    MS> On Sat, 2007-12-01 at 18:40 +0000, David Winsemius wrote:
    >> David Winsemius <dwinsemius at comcast.net> wrote in 
    >> news:Xns99F989B3A3057dNOTwinscomcast at 80.91.229.13:
    >> 
    >> > "tom soyer" <tom.soyer at gmail.com> wrote in
    >> > news:65cc7bdf0712010951p451a993i70da89f285d801de at mail.gmail.com: 
    >> > 
    >> >> John,
    >> >> 
    >> >> The Excel's percentrank function works like this: if one has a number,
    >> >> x for example, and one wants to know the percentile of this number in
    >> >> a given data set, dataset, one would type =percentrank(dataset,x) in
    >> >> Excel to calculate the percentile. So for example, if the data set is
    >> >> c(1:10), and one wants to know the percentile of 2.5 in the data set,
    >> >> then using the percentrank function one would get 0.166, i.e., 2.5 is
    >> >> in the 16.6th percentile. 
    >> >> 
    >> >> I am not sure how to program this function in R. I couldn't find it as
    >> >> a built-in function in R either. It seems to be an obvious choice for
    >> >> a built-in function. I am very surprised, but maybe we both missed it.
    >> >  
    >> > My nomination for a function with a similar result would be ecdf(), the 
    >> > empirical cumulative distribution function. It is of class "function" 
    >> so 
    >> > efforts to index ecdf(.)[.] failed for me.

I think you did not understand ecdf() !!!
It *returns* a function,
that you can then apply to old (or new) data; see below

    MS> You can use ls.str() to look into the function environment:

    >> ls.str(environment(ecdf(x)))
    MS> f :  num 0
    MS> method :  int 2
    MS> n :  int 25
    MS> x :  num [1:25] -2.215 -1.989 -0.836 -0.820 -0.626 ...
    MS> y :  num [1:25] 0.04 0.08 0.12 0.16 0.2 0.24 0.28 0.32 0.36 0.4 ...
    MS> yleft :  num 0
    MS> yright :  num 1



    MS> You can then use get() or mget() within the function environment to
    MS> return the requisite values. Something along the lines of the following
    MS> within the function percentrank():

    MS> percentrank <- function(x, val)
    MS> {
    MS> env.x <- environment(ecdf(x))
    MS> res <- mget(c("x", "y"), env.x)
    MS> Ind <- which(sapply(seq(length(res$x)),
    MS> function(i) isTRUE(all.equal(res$x[i], val))))
    MS> res$y[Ind]
    MS> }

sorry Marc, but "Yuck !!"

- this  percentrank() only works when you apply it to original x[i] values
- only works for 'val' of length 1
- is a complicated hack

and absolutely unneeded  (see below)

    MS> Thus:

    MS> set.seed(1)
    MS> x <- rnorm(25)

    >> x
    MS> [1] -0.62645381  0.18364332 -0.83562861  1.59528080  0.32950777
    MS> [6] -0.82046838  0.48742905  0.73832471  0.57578135 -0.30538839
    MS> [11]  1.51178117  0.38984324 -0.62124058 -2.21469989  1.12493092
    MS> [16] -0.04493361 -0.01619026  0.94383621  0.82122120  0.59390132
    MS> [21]  0.91897737  0.78213630  0.07456498 -1.98935170  0.61982575


    >> percentrank(x, 0.48742905)
    MS> [1] 0.56

[gives 0.52 in my version of R ]

Well, that is *THE SAME*  as using  ecdf() the way you 
should have used it :

  ecdf(x)(0.48742905)

{in two lines, that is

  mypercR <- ecdf(x)
  mypercR(0.48742905)

 which maybe easier to understand, if you have never used the
 nice concept that underlies all of

 approxfun(), splinefun() or ecdf()
}

You can also use

  ecdf(x)(x)

and indeed check that it is identical to the convoluted
percentrank() function above :

> ecdf(x)(0.48742905)
[1] 0.52
> ecdf(x)(x)
 [1] 0.20 0.44 0.12 1.00 0.48 0.16 0.56 0.72 0.60 0.28 0.96 0.52 0.24 0.04 0.92
[16] 0.32 0.36 0.88 0.80 0.64 0.84 0.76 0.40 0.08 0.68
> all(ecdf(x)(x) == sapply(x, function(v) percentrank(x,v)))
[1] TRUE
> 


Regards (and apologies for my apparent indignation ;-)
by the author of ecdf() ,

Martin Maechler, ETH Zurich  


    MS> One other approach, which returns the values and their respective rank
    MS> percentiles is:

     >> cumsum(prop.table(table(x)))
    
    [...... snip ........]


From P.Dalgaard at biostat.ku.dk  Wed Dec  5 18:52:09 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 05 Dec 2007 18:52:09 +0100
Subject: [R] confidence intervals for y predicted in non
	linear	regression
In-Reply-To: <Pine.LNX.4.64.0712051300270.25629@gannet.stats.ox.ac.uk>
References: <9BD63C7EDBFC234DB982DF636397395E06C957@oceanoastur.gi.ieo.es>	<1196780877.47556d4d7feca@webmail.univ-lyon1.fr>	<BAY116-W30921566FA9910AB1B1283996E0@phx.gbl>
	<Pine.LNX.4.64.0712051300270.25629@gannet.stats.ox.ac.uk>
Message-ID: <4756E549.1050605@biostat.ku.dk>

Prof Brian Ripley wrote:
> You mean the package nls2 at
>
> http://w3.jouy.inra.fr/unites/miaj/public/AB/nls2/install.html
>
> and not the unfortunately named nls2 that has just appeared on CRAN?
>
> The first is not really a 'package' in the R sense.
>
Actually, it is one (sort of), but it is broken. The instructions say
that you can use R CMD INSTALL to install in R < 2.0.0 (!) With a
current R, you can try but it dies:

No man pages found in package 'nls2'
** building package indices ...
Warning in file(file, "r", encoding = encoding) :
  cannot open file '../R/init.R', reason 'No such file or directory'
Error in file(file, "r", encoding = encoding) : unable to open connection
Calls: <Anonymous> ... switch -> sys.source -> eval -> eval -> source ->
file
Execution halted
ERROR: installing package indices failed
** Removing '/home/bs/pd/Rlibrary/nls2'

...and there were some odd goings-on at the start as well.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From simone.gabbriellini at gmail.com  Wed Dec  5 18:54:36 2007
From: simone.gabbriellini at gmail.com (Simone Gabbriellini)
Date: Wed, 5 Dec 2007 18:54:36 +0100
Subject: [R] how to interpolate a plot with a logistic curve
Message-ID: <E2978D5F-BED0-44A7-9196-E5A9E074BB96@gmail.com>

hello,

I have this simple question. This is my dataset

	size
1	57
2	97
3	105
4	123
5	136
6	153
7	173
8	180
9	193
10	202
11	213
12	219
13	224
14	224
15	248
16	367
17	496
18	568
19	618
20	670
21	719
22	774
23	810
24	814
25	823

I plot it with:

plot(generalstats[,1], type="b", xlab="Mesi", ylab="Numero di  
vertici", main="");

and try to interpolate with a linear regression with

abline(lm(generalstats[, 
1 
]~ 
c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)),  
lty=3, col="red");

how to interpolate the data with a logistic curve? I cannot find the -  
I suppose easy - solution..

thank you,
Simone


From dylan.beaudette at gmail.com  Wed Dec  5 19:25:06 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Wed, 5 Dec 2007 10:25:06 -0800
Subject: [R] how to interpolate a plot with a logistic curve
In-Reply-To: <E2978D5F-BED0-44A7-9196-E5A9E074BB96@gmail.com>
References: <E2978D5F-BED0-44A7-9196-E5A9E074BB96@gmail.com>
Message-ID: <200712051025.06325.dylan.beaudette@gmail.com>

On Wednesday 05 December 2007, Simone Gabbriellini wrote:
> hello,
>
> I have this simple question. This is my dataset
>
> 	size
> 1	57
> 2	97
> 3	105
> 4	123
> 5	136
> 6	153
> 7	173
> 8	180
> 9	193
> 10	202
> 11	213
> 12	219
> 13	224
> 14	224
> 15	248
> 16	367
> 17	496
> 18	568
> 19	618
> 20	670
> 21	719
> 22	774
> 23	810
> 24	814
> 25	823
>
> I plot it with:
>
> plot(generalstats[,1], type="b", xlab="Mesi", ylab="Numero di
> vertici", main="");
>
> and try to interpolate with a linear regression with
>
> abline(lm(generalstats[,
> 1
> ]~
> c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)),
> lty=3, col="red");
>
> how to interpolate the data with a logistic curve? I cannot find the -
> I suppose easy - solution..
>
> thank you,
> Simone
>

try:

glm(formula, data, family=binomial())

require(Design)
lrm()


Cheers,

-- 
Dylan Beaudette
Soil Resource Laboratory
http://casoilresource.lawr.ucdavis.edu/
University of California at Davis
530.754.7341


From ripley at stats.ox.ac.uk  Wed Dec  5 19:21:24 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Dec 2007 18:21:24 +0000 (GMT)
Subject: [R] confidence intervals for y predicted in non linear
	regression
In-Reply-To: <4756E549.1050605@biostat.ku.dk>
References: <9BD63C7EDBFC234DB982DF636397395E06C957@oceanoastur.gi.ieo.es>
	<1196780877.47556d4d7feca@webmail.univ-lyon1.fr>
	<BAY116-W30921566FA9910AB1B1283996E0@phx.gbl>
	<Pine.LNX.4.64.0712051300270.25629@gannet.stats.ox.ac.uk>
	<4756E549.1050605@biostat.ku.dk>
Message-ID: <Pine.LNX.4.64.0712051818540.15112@gannet.stats.ox.ac.uk>

On Wed, 5 Dec 2007, Peter Dalgaard wrote:

> Prof Brian Ripley wrote:
>> You mean the package nls2 at
>>
>> http://w3.jouy.inra.fr/unites/miaj/public/AB/nls2/install.html
>>
>> and not the unfortunately named nls2 that has just appeared on CRAN?
>>
>> The first is not really a 'package' in the R sense.
>>
> Actually, it is one (sort of), but it is broken. The instructions say

In other words, it is 'not really' a package!  You can install it 
following the instructions it contains, into the current workspace.

> that you can use R CMD INSTALL to install in R < 2.0.0 (!) With a
> current R, you can try but it dies:
>
> No man pages found in package 'nls2'
> ** building package indices ...
> Warning in file(file, "r", encoding = encoding) :
>  cannot open file '../R/init.R', reason 'No such file or directory'
> Error in file(file, "r", encoding = encoding) : unable to open connection
> Calls: <Anonymous> ... switch -> sys.source -> eval -> eval -> source ->
> file
> Execution halted
> ERROR: installing package indices failed
> ** Removing '/home/bs/pd/Rlibrary/nls2'
>
> ...and there were some odd goings-on at the start as well.
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From P.Dalgaard at biostat.ku.dk  Wed Dec  5 19:26:43 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 05 Dec 2007 19:26:43 +0100
Subject: [R] Is R portable?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E9504AA228B@iahce2ksrv1.iah.bbsrc.ac.uk>
References: <281383.37749.qm@web32815.mail.mud.yahoo.com>	<Pine.LNX.4.64.0712041737260.9274@gannet.stats.ox.ac.uk>	<4755D3D3.3080700@psych.uib.no>
	<4755DABD.7010109@stats.uwo.ca>
	<8975119BCD0AC5419D61A9CF1A923E9504AA228B@iahce2ksrv1.iah.bbsrc.ac.uk>
Message-ID: <4756ED63.4060305@biostat.ku.dk>

michael watson (IAH-C) wrote:
> I opened this hoping someone had installed R on windows mobile or simbian.... :(
>   
I think that's Symbian with a 'y'.

The toolchain availability tends to get in the way. Linux-based gadgets
could prove easier. I do wonder from time to time whether there really
is a market for R on cellphones...
> ________________________________
>
> From: r-help-bounces at r-project.org on behalf of Duncan Murdoch
> Sent: Tue 04/12/2007 10:54 PM
> To: Tom Backer Johnsen
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Is R portable?
>
>
>
> On 04/12/2007 5:25 PM, Tom Backer Johnsen wrote:
>   
>> Prof Brian Ripley wrote:
>>     
>>> On Tue, 4 Dec 2007, John Kane wrote:
>>>
>>>       
>>>> I simply installed R onto a USB stick, downloaded my
>>>> normal packages to it and it works fine under Windows.
>>>>         
>>> Yes, on Windows, but
>>>
>>> 1) There are other OSes,
>>>
>>> 2) This didn't just happen: it needed some careful design, including
>>> some caching to make it run fast from a USB disk.
>>>       
>> Nice to discover good planning.  Am I then correct in my
>> understanding: Installing R under Windows does not require any
>> registry entries, the installation is essentially to unpack the
>> necessary files in the correct directories?
>>     
>
> Yes.  It does record some information in the registry, but that's purely
> optional.  The only problem you're likely to run into is using external
> software with R, that might look in the registry to find where R was
> installed.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From marc_schwartz at comcast.net  Wed Dec  5 19:43:50 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 05 Dec 2007 12:43:50 -0600
Subject: [R] R function for percentrank
In-Reply-To: <18262.58128.974202.67709@stat.math.ethz.ch>
References: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
	<243517.68093.qm@web32807.mail.mud.yahoo.com>
	<65cc7bdf0712010951p451a993i70da89f285d801de@mail.gmail.com>
	<Xns99F989B3A3057dNOTwinscomcast@80.91.229.13>
	<Xns99F98B11F90B6dNOTwinscomcast@80.91.229.13>
	<1196537601.2980.15.camel@Bellerophon.localdomain>
	<18262.58128.974202.67709@stat.math.ethz.ch>
Message-ID: <1196880230.2945.26.camel@Bellerophon.localdomain>


On Wed, 2007-12-05 at 18:42 +0100, Martin Maechler wrote:
> I'm coming late to this, but this *does* need a correction
> just for the archives !
> 
> >>>>> "MS" == Marc Schwartz <marc_schwartz at comcast.net>
> >>>>>     on Sat, 01 Dec 2007 13:33:21 -0600 writes:
> 
>     MS> On Sat, 2007-12-01 at 18:40 +0000, David Winsemius wrote:
>     >> David Winsemius <dwinsemius at comcast.net> wrote in 
>     >> news:Xns99F989B3A3057dNOTwinscomcast at 80.91.229.13:
>     >> 
>     >> > "tom soyer" <tom.soyer at gmail.com> wrote in
>     >> > news:65cc7bdf0712010951p451a993i70da89f285d801de at mail.gmail.com: 
>     >> > 
>     >> >> John,
>     >> >> 
>     >> >> The Excel's percentrank function works like this: if one has a number,
>     >> >> x for example, and one wants to know the percentile of this number in
>     >> >> a given data set, dataset, one would type =percentrank(dataset,x) in
>     >> >> Excel to calculate the percentile. So for example, if the data set is
>     >> >> c(1:10), and one wants to know the percentile of 2.5 in the data set,
>     >> >> then using the percentrank function one would get 0.166, i.e., 2.5 is
>     >> >> in the 16.6th percentile. 
>     >> >> 
>     >> >> I am not sure how to program this function in R. I couldn't find it as
>     >> >> a built-in function in R either. It seems to be an obvious choice for
>     >> >> a built-in function. I am very surprised, but maybe we both missed it.
>     >> >  
>     >> > My nomination for a function with a similar result would be ecdf(), the 
>     >> > empirical cumulative distribution function. It is of class "function" 
>     >> so 
>     >> > efforts to index ecdf(.)[.] failed for me.
> 
> I think you did not understand ecdf() !!!
> It *returns* a function,
> that you can then apply to old (or new) data; see below
> 
>     MS> You can use ls.str() to look into the function environment:
> 
>     >> ls.str(environment(ecdf(x)))
>     MS> f :  num 0
>     MS> method :  int 2
>     MS> n :  int 25
>     MS> x :  num [1:25] -2.215 -1.989 -0.836 -0.820 -0.626 ...
>     MS> y :  num [1:25] 0.04 0.08 0.12 0.16 0.2 0.24 0.28 0.32 0.36 0.4 ...
>     MS> yleft :  num 0
>     MS> yright :  num 1
> 
> 
> 
>     MS> You can then use get() or mget() within the function environment to
>     MS> return the requisite values. Something along the lines of the following
>     MS> within the function percentrank():
> 
>     MS> percentrank <- function(x, val)
>     MS> {
>     MS> env.x <- environment(ecdf(x))
>     MS> res <- mget(c("x", "y"), env.x)
>     MS> Ind <- which(sapply(seq(length(res$x)),
>     MS> function(i) isTRUE(all.equal(res$x[i], val))))
>     MS> res$y[Ind]
>     MS> }
> 
> sorry Marc, but "Yuck !!"
> 
> - this  percentrank() only works when you apply it to original x[i] values
> - only works for 'val' of length 1
> - is a complicated hack
> 
> and absolutely unneeded  (see below)
> 
>     MS> Thus:
> 
>     MS> set.seed(1)
>     MS> x <- rnorm(25)
> 
>     >> x
>     MS> [1] -0.62645381  0.18364332 -0.83562861  1.59528080  0.32950777
>     MS> [6] -0.82046838  0.48742905  0.73832471  0.57578135 -0.30538839
>     MS> [11]  1.51178117  0.38984324 -0.62124058 -2.21469989  1.12493092
>     MS> [16] -0.04493361 -0.01619026  0.94383621  0.82122120  0.59390132
>     MS> [21]  0.91897737  0.78213630  0.07456498 -1.98935170  0.61982575
> 
> 
>     >> percentrank(x, 0.48742905)
>     MS> [1] 0.56
> 
> [gives 0.52 in my version of R ]
> 
> Well, that is *THE SAME*  as using  ecdf() the way you 
> should have used it :
> 
>   ecdf(x)(0.48742905)
> 
> {in two lines, that is
> 
>   mypercR <- ecdf(x)
>   mypercR(0.48742905)
> 
>  which maybe easier to understand, if you have never used the
>  nice concept that underlies all of
> 
>  approxfun(), splinefun() or ecdf()
> }
> 
> You can also use
> 
>   ecdf(x)(x)
> 
> and indeed check that it is identical to the convoluted
> percentrank() function above :
> 
> > ecdf(x)(0.48742905)
> [1] 0.52
> > ecdf(x)(x)
>  [1] 0.20 0.44 0.12 1.00 0.48 0.16 0.56 0.72 0.60 0.28 0.96 0.52 0.24 0.04 0.92
> [16] 0.32 0.36 0.88 0.80 0.64 0.84 0.76 0.40 0.08 0.68
> > all(ecdf(x)(x) == sapply(x, function(v) percentrank(x,v)))
> [1] TRUE
> > 
> 
> 
> Regards (and apologies for my apparent indignation ;-)
> by the author of ecdf() ,
> 
> Martin Maechler, ETH Zurich  

Martin,

Thanks for the corrections. In hindsight, now seeing the intended use of
ecdf() in the fashion you describe above, it is now clear that my
approach in response to David's query was un-needed and "over the top".
"Yuck" is quite appropriate... :-)

As I was going through this "exercise", it did seem overly complicated,
given R's usual elegant philosophy about such things. I suppose if I had
looked at the source for plot.stepfun(), it would have been more evident
as to how the y values are acquired.

In reviewing the examples in ?ecdf, I think that an example using
something along the lines of the discussion here more explicitly, would
be helpful. It is not crystal clear from the examples, that one can use
ecdf() in this fashion, though the use of "12 * Fn(tt)" hints at it.

Perhaps:

##-- Simple didactical  ecdf  example:
x <- rnorm(12)
Fn <- ecdf(x)
Fn
Fn(x)  # returns the percentiles for x
...


Thanks again Martin and no offense taken...  :-)

Regards,

Marc


From P.Dalgaard at biostat.ku.dk  Wed Dec  5 20:04:09 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 05 Dec 2007 20:04:09 +0100
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <Pine.LNX.4.64.0712051655180.12631@gannet.stats.ox.ac.uk>
References: <4756C0E2.1060902@unifi.it>	<18262.54350.143549.665409@localhost.localdomain>
	<Pine.LNX.4.64.0712051655180.12631@gannet.stats.ox.ac.uk>
Message-ID: <4756F629.70703@biostat.ku.dk>

Prof Brian Ripley wrote:
> Note that Ottorino has only 1GB of RAM installed, which makes a 64-bit
> version of R somewhat moot.  See chapter 8 of
>
> http://cran.r-project.org/doc/manuals/R-admin.html
>
Only somewhat. The Opteron actually has 1GB too (Hey, it was bought in
2004!  And the main point was to see whether 64 bit builds would work at
all) but 16GB of swap. So large data sets will fit but be processed slowly.
> I would install a i386 version of R on x86_64 Linux unless I had 2Gb
> or more of RAM.  I don't know how easily that works on Ubuntu these
> days, but I would try it.
It's not like the 64 bit build feels slow for basic usage, though. I
don't think you need to bother with mixing architectures unless you have
applications where it really matters (CPU intensive, but below 32-bit
addressing limitations). Buying more RAM is much to be preferred.

Now what kind of RAM does my Opteron board take... ?



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From b.rowlingson at lancaster.ac.uk  Wed Dec  5 19:57:58 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 05 Dec 2007 18:57:58 +0000
Subject: [R] Is R portable?
In-Reply-To: <4756ED63.4060305@biostat.ku.dk>
References: <281383.37749.qm@web32815.mail.mud.yahoo.com>	<Pine.LNX.4.64.0712041737260.9274@gannet.stats.ox.ac.uk>	<4755D3D3.3080700@psych.uib.no>	<4755DABD.7010109@stats.uwo.ca>	<8975119BCD0AC5419D61A9CF1A923E9504AA228B@iahce2ksrv1.iah.bbsrc.ac.uk>
	<4756ED63.4060305@biostat.ku.dk>
Message-ID: <4756F4B6.7070306@lancaster.ac.uk>

Peter Dalgaard wrote:

> The toolchain availability tends to get in the way. Linux-based gadgets
> could prove easier. I do wonder from time to time whether there really
> is a market for R on cellphones...

  As soon as someone writes library(ringtone) there might be :)

  And I think you'd have to turn off predictive text. Can someone with a 
mobile/cellphone tell me what 'hist(runif(100))' comes up as? [1]

Barry

[1] No, I haven't got one.


From edd at debian.org  Wed Dec  5 20:16:11 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 5 Dec 2007 13:16:11 -0600
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <4756DBCC.6030602@biostat.ku.dk>
References: <4756C0E2.1060902@unifi.it> <4756DBCC.6030602@biostat.ku.dk>
Message-ID: <20071205191611.GA15290@eddelbuettel.com>

On Wed, Dec 05, 2007 at 06:11:40PM +0100, Peter Dalgaard wrote:
> One oddity about Ubuntu is that there are no CRAN builds for 64bit.

Volunteers would be welcomed with open arms.

Dirk,

-- 
Three out of two people have difficulties with fractions.


From bolker at ufl.edu  Wed Dec  5 20:19:10 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 5 Dec 2007 11:19:10 -0800 (PST)
Subject: [R] Displaying numerics to full double precision
In-Reply-To: <14175334.post@talk.nabble.com>
References: <14175334.post@talk.nabble.com>
Message-ID: <14178707.post@talk.nabble.com>




Jeff Delmerico wrote:
> 
> I'm working on a shared library of C functions for use with R, and I want
> to create a matrix in R and pass it to the C routines.  I know R computes
> and supposedly stores numerics in double precision, but when I create a
> matrix of random numerics using rnorm(), the values are displayed in
> single precision, and also exported in single precision when I pass them
> out to my C routines.  An example is below:
> 
>> a <- matrix(rnorm(16, mean=10, sd=4), nrow=4)
>> a
>           [,1]      [,2]      [,3]      [,4]
> [1,] 14.907606 17.572872 19.708977  9.809943
> [2,]  9.322041 13.624452  7.745254  7.596176
> [3,] 10.642408  6.151546  9.937434  6.913875
> [4,] 14.617647  5.577073  8.217559 12.115465
>> storage.mode(a)
> [1] "double"
> 
> Does anyone know if there is a way to change the display or storage
> settings so that the values will be displayed to their full precision?  
> Or does rnorm only produce values to single precision? 
> 
> Any assistance would be greatly appreciated.
> 
> Thanks,
> Jeff Delmerico
> 

options("digits") # 7
options(digits=x)

  I may be mistaken, but I think the values are indeed exported
as double precision -- the issue here is just a display setting.

  Ben Bolker


-- 
View this message in context: http://www.nabble.com/Displaying-numerics-to-full-double-precision-tf4950807.html#a14178707
Sent from the R help mailing list archive at Nabble.com.


From p_connolly at slingshot.co.nz  Wed Dec  5 20:25:54 2007
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Thu, 6 Dec 2007 08:25:54 +1300
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <4756DBCC.6030602@biostat.ku.dk>
References: <4756C0E2.1060902@unifi.it> <4756DBCC.6030602@biostat.ku.dk>
Message-ID: <20071205192554.GK6584@slingshot.co.nz>

On Wed, 05-Dec-2007 at 06:11PM +0100, Peter Dalgaard wrote:

[....]

|> One oddity about Ubuntu is that there are no CRAN builds for 64bit.
|> Presumably, the Debian packages work, or you can get the build
|> script and make your own build. This is not really within my
|> domain, though.

I've always used rpms (or debs for Debian-type OSes) but I install R
from source which is very easy to do.  Adding R packages with
install.packages() is also extremely easy.  If you have the 64bit OS,
it will compile R as 64 bit (enless you make some modifications to the
standard configuration).

One downside of that is that you'd be unable to use packages that have
only 32 bit versions.  One such is ASReml-R but if you never intend to
use such things, the only other consideration I can think of is the
relatively small amount of memory.  No great benefits of 64 bit
without lots of memory, but a few downsides.

HTH

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From sabaric at auburn.edu  Wed Dec  5 20:30:34 2007
From: sabaric at auburn.edu (Richard Saba)
Date: Wed, 5 Dec 2007 13:30:34 -0600
Subject: [R] Working with "ts" objects
Message-ID: <001b01c83775$4ef6fd50$ece4f7f0$@edu>

I am relatively new to R and object oriented programming. I have relied on
SAS for most of my data analysis.  I teach an introductory undergraduate
forecasting course using the Diebold text and I am considering using R in
addition to SAS and Eviews in the course. I work primarily with univariate
or multivariate time series data. I am having a great deal of difficulty
understanding and working with "ts" objects particularly when it comes to
referencing variables in plot commands or in formulas. The confusion is
amplified when certain procedures (lm for example) coerce the "ts" object
into a data.frame before application with the results that the output is
stored in a data.frame object. 
For example the two sets of code below replicate examples from chapter 2 and
6 in the text. In the first set of code if I were to replace
"anscombe<-read.table(fname, header=TRUE)" with
"anscombe<-ts(read.table(fname, header=TRUE))" the plot() commands would
generate errors. The objects "x1", "y1" ...  would not be recognized. In
this case I would have to reference the specific column in the anscombe data
set. If I would have constructed the data set from several different data
sets using the ts.intersect() function (see second code below)the problem
becomes even more involved and keeping track of which columns are associated
with which variables can be rather daunting. All I wanted was to plot actual
vs. predicted values of "hstarts" and the residuals from the model. 

Given the difficulties I have encountered I know my students will have
similar problems. Is there a source other than the basic R manuals that I
can consult and recommend to my students that will help get a handle on
working with time series objects? I found the Shumway "Time series analysis
and its applications with R Examples" website very helpful but many
practical questions involving manipulation of time series data still remain.
Any help will be appreciated.
Thanks,

Richard Saba
Department of Economics
Auburn University
Email:  sabaric at auburn.edu
Phone:  334 844-2922
        



anscombe<-read.table(fname, header=TRUE)
names(anscombe)<-c("x1","y1","x2","y2","x3","y3","x4","y4")  
reg1<-lm(y1~1 + x1, data=anscombe)
reg2<-lm(y2~1 + x2, data=anscombe)
reg3<-lm(y3~1 + x3, data=anscombe)
reg4<-lm(y4~1 + x4, data=anscombe)
summary(reg1)
summary(reg2)
summary(reg3)       
summary(reg4)
par(mfrow=c(2,2))
plot(x1,y1)
abline(reg1)
plot(x2,y2)
abline(reg2)
plot(x3,y3)
abline(reg3)
plot(x4,y4)
abline(reg4)

..........................................................................
fname<-file.choose()
tab6.1<-ts(read.table(fname, header=TRUE),frequency=12,start=c(1946,1))
month<-cycle(tab6.1)
year<-floor(time(tab6.1))
dat1<-ts.intersect(year,month,tab6.1)
dat2<-window(dat1,start=c(1946,1),end=c(1993,12)) 
reg1<-lm(tab6.1~1+factor(month),data=dat2, na.action=NULL)
summary(reg1)       
hstarts<-dat2[,3]                 
plot1<-ts.intersect(hstarts,reg1$fitted.value,reg1$resid)
plot.ts(plot1[,1])
lines(plot1[,2], col="red")
plot.ts(plot[,3], ylab="Residuals")


From gustaf.granath at ebc.uu.se  Wed Dec  5 20:31:43 2007
From: gustaf.granath at ebc.uu.se (Gustaf Granath)
Date: Wed, 05 Dec 2007 20:31:43 +0100
Subject: [R] Interpretation of 'Intercept' in a 2-way factorial lm
Message-ID: <20071205203143.p8xlokf00s4sk48o@webmail5.uu.se>

Hi all,

I hope this question is not too trivial. I can't find an explanation
anywhere (Stats and R books, R-archives) so now I have to turn to the R-list.

Question:

If you have a factorial design with two factors (say A and B with two
levels each). What does the intercept coefficient with
treatment.contrasts represent??

Here is an example without interaction where A has two levels A1 and
A2, and B has two levels B1 and B2. So R takes as a baseline A1 and B1.

coef( summary ( lm ( fruit ~ A + B, data = test)))

                Estimate   Std. Error  t value       Pr(>|t|)
(Intercept)   2.716667   0.5484828   4.953058   7.879890e-04
A2            6.266667   0.6333333   9.894737   3.907437e-06
B2            5.166667   0.6333333   8.157895   1.892846e-05

I understand that the mean of A2 is +6.3 more than A1, and
that B2 is 5.2 more than B1.

So the question is: Is the intercept A1 and B1 combined as one mean
("the baseline")? or is it something else? Does this number actually
tell me anything
useful (2.716)??

What does the model (y = intercept  + ??) look like then? I can't understand
how both factors (A and B) can have the same intercept?

Thanks in advance!!

Gustaf Granath

Dept of Plant Ecology
Uppsala University, Sweden


From Ted.Harding at manchester.ac.uk  Wed Dec  5 20:32:46 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Wed, 05 Dec 2007 19:32:46 -0000 (GMT)
Subject: [R] Is R portable?
In-Reply-To: <4756F4B6.7070306@lancaster.ac.uk>
Message-ID: <XFMail.071205193246.Ted.Harding@manchester.ac.uk>

On 05-Dec-07 18:57:58, Barry Rowlingson wrote:
> Peter Dalgaard wrote:
> 
>> The toolchain availability tends to get in the way. Linux-based
>> gadgets could prove easier. I do wonder from time to time whether
>> there really is a market for R on cellphones...
> 
> As soon as someone writes library(ringtone) there might be :)
> 
> And I think you'd have to turn off predictive text. Can someone
> with a mobile/cellphone tell me what 'hist(runif(100))' comes up as?
> [1]
> 
> Barry
> 
> [1] No, I haven't got one.

I have a very old cellphone whose display, when I switch it on,
looks very much like what one would expect from 'hist(runif(100))'.

I'm not using it any more.

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Dec-07                                       Time: 19:32:42
------------------------------ XFMail ------------------------------


From kmm at csusb.edu  Wed Dec  5 20:38:17 2007
From: kmm at csusb.edu (Kevin Middleton)
Date: Wed, 05 Dec 2007 11:38:17 -0800
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <20071205191611.GA15290@eddelbuettel.com>
References: <4756C0E2.1060902@unifi.it> <4756DBCC.6030602@biostat.ku.dk>
	<20071205191611.GA15290@eddelbuettel.com>
Message-ID: <E0780CF2-6F60-4557-A2D0-D29A071F5F4B@csusb.edu>

>> One oddity about Ubuntu is that there are no CRAN builds for 64bit.
>
> Volunteers would be welcomed with open arms.
>

I don't think I know enough to volunteer (but I am open to suggestion  
off list), but I had no difficulty installing R from source on a 64  
bit Ubuntu machine (7.10, with, I believe, 1 GB of RAM) following the  
instructions in R-admin. I just kept installing necessary libraries  
until ./configure completed without errors. Then make, make check,  
and make install ran without a hitch.

Kevin

-------------------------------------------------
Kevin M. Middleton
Department of Biology
California State University San Bernardino
5500 University Parkway
San Bernardino CA 92507


From jeffdelmerico at yahoo.com  Wed Dec  5 17:39:41 2007
From: jeffdelmerico at yahoo.com (Jeff Delmerico)
Date: Wed, 5 Dec 2007 08:39:41 -0800 (PST)
Subject: [R]  Displaying numerics to full double precision
Message-ID: <14175334.post@talk.nabble.com>


I'm working on a shared library of C functions for use with R, and I want to
create a matrix in R and pass it to the C routines.  I know R computes and
supposedly stores numerics in double precision, but when I create a matrix
of random numerics using rnorm(), the values are displayed in single
precision, and also exported in single precision when I pass them out to my
C routines.  An example is below:

> a <- matrix(rnorm(16, mean=10, sd=4), nrow=4)
> a
          [,1]      [,2]      [,3]      [,4]
[1,] 14.907606 17.572872 19.708977  9.809943
[2,]  9.322041 13.624452  7.745254  7.596176
[3,] 10.642408  6.151546  9.937434  6.913875
[4,] 14.617647  5.577073  8.217559 12.115465
> storage.mode(a)
[1] "double"

Does anyone know if there is a way to change the display or storage settings
so that the values will be displayed to their full precision?   Or does
rnorm only produce values to single precision? 

Any assistance would be greatly appreciated.

Thanks,
Jeff Delmerico
-- 
View this message in context: http://www.nabble.com/Displaying-numerics-to-full-double-precision-tf4950807.html#a14175334
Sent from the R help mailing list archive at Nabble.com.


From amitpatel_ak at yahoo.co.uk  Wed Dec  5 18:33:34 2007
From: amitpatel_ak at yahoo.co.uk (Amit Patel)
Date: Wed, 5 Dec 2007 17:33:34 +0000 (GMT)
Subject: [R] Dealing with NA's in a data matrix
Message-ID: <786988.22594.qm@web25011.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/24a04fa4/attachment.pl 

From alexander.f.moreno at gmail.com  Wed Dec  5 19:05:00 2007
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Wed, 5 Dec 2007 12:05:00 -0600
Subject: [R] kalman filter random walk
Message-ID: <3303a4570712051005l3199f0f3y3a67af1796f8351f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/32621e6c/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Wed Dec  5 20:41:29 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 05 Dec 2007 20:41:29 +0100
Subject: [R] Interpretation of 'Intercept' in a 2-way factorial lm
In-Reply-To: <20071205203143.p8xlokf00s4sk48o@webmail5.uu.se>
References: <20071205203143.p8xlokf00s4sk48o@webmail5.uu.se>
Message-ID: <4756FEE9.2090509@biostat.ku.dk>

Gustaf Granath wrote:
> Hi all,
>
> I hope this question is not too trivial. I can't find an explanation
> anywhere (Stats and R books, R-archives) so now I have to turn to the R-list.
>
> Question:
>
> If you have a factorial design with two factors (say A and B with two
> levels each). What does the intercept coefficient with
> treatment.contrasts represent??
>
> Here is an example without interaction where A has two levels A1 and
> A2, and B has two levels B1 and B2. So R takes as a baseline A1 and B1.
>
> coef( summary ( lm ( fruit ~ A + B, data = test)))
>
>                 Estimate   Std. Error  t value       Pr(>|t|)
> (Intercept)   2.716667   0.5484828   4.953058   7.879890e-04
> A2            6.266667   0.6333333   9.894737   3.907437e-06
> B2            5.166667   0.6333333   8.157895   1.892846e-05
>
> I understand that the mean of A2 is +6.3 more than A1, and
> that B2 is 5.2 more than B1.
>
> So the question is: Is the intercept A1 and B1 combined as one mean
> ("the baseline")? or is it something else? Does this number actually
> tell me anything
> useful (2.716)??
>
> What does the model (y = intercept  + ??) look like then? I can't understand
> how both factors (A and B) can have the same intercept?
>
>   
Consider an AxB crosstable of (fitted) means. Upper left corner is
intercept , add A2, B2, or both to get the other three cells.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From wwwhsd at gmail.com  Wed Dec  5 20:51:16 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 5 Dec 2007 17:51:16 -0200
Subject: [R] Dealing with NA's in a data matrix
In-Reply-To: <786988.22594.qm@web25011.mail.ukl.yahoo.com>
References: <786988.22594.qm@web25011.mail.ukl.yahoo.com>
Message-ID: <da79af330712051151k7a13b326g705a4a13647b4e99@mail.gmail.com>

 x[is.na(x)] <- 0

On 05/12/2007, Amit Patel <amitpatel_ak at yahoo.co.uk> wrote:
> Hi I have a matrix with NA value that I would like to convert these to a value of 0.
> any suggestions
>
> Kind Regards
> Amit Patel
>
>
>
>
>       ___________________________________________________________
>
> ttp://uk.promotions.yahoo.com/forgood/
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From Roy.Mendelssohn at noaa.gov  Wed Dec  5 20:52:33 2007
From: Roy.Mendelssohn at noaa.gov (Roy Mendelssohn)
Date: Wed, 05 Dec 2007 11:52:33 -0800
Subject: [R] kalman filter random walk
In-Reply-To: <3303a4570712051005l3199f0f3y3a67af1796f8351f@mail.gmail.com>
References: <3303a4570712051005l3199f0f3y3a67af1796f8351f@mail.gmail.com>
Message-ID: <90483E81-ADB6-4AB5-9158-29F859E29039@noaa.gov>

sspir or ldm.

-Roy M.


On Dec 5, 2007, at 10:05 AM, Alexander Moreno wrote:

> Hi,
>
> I'm trying to use the kalman filter to estimate the variable drift  
> of a
> random walk, given that I have a vector of time series data.   
> Anyone have
> any thoughts on how to do this in R?
>
> Thanks,
> Alex
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S.  
Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division	
Southwest Fisheries Science Center
1352 Lighthouse Avenue
Pacific Grove, CA 93950-2097

e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
voice: (831)-648-9029
fax: (831)-648-8440
www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."


From vistocco at unicas.it  Wed Dec  5 21:29:41 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Wed, 05 Dec 2007 21:29:41 +0100
Subject: [R] newbie lapply question
In-Reply-To: <Pine.LNX.4.63.0712050926340.21474@manjula.frotz.bogus>
References: <Pine.LNX.4.63.0712050901120.21474@manjula.frotz.bogus>	<Pine.LNX.4.64.0712051716020.12631@gannet.stats.ox.ac.uk>
	<Pine.LNX.4.63.0712050926340.21474@manjula.frotz.bogus>
Message-ID: <47570A35.8090702@unicas.it>

I am not sure to understand your problem, but it seems to me that you 
can use directly the function on the range of the dates:

 > x=as.Date(c('2007-01-01','2007-01-02'))
 > fff=function(x){y=x+1;return(y)}
 > fff(x)
[1] "2007-01-02" "2007-01-03"
 > class(fff(x))
[1] "Date"

Perhaps your function use a different input (not a vector of dates but a 
dataframe)?

domenico vistocco

Ranjan Bagchi wrote:
> On Wed, 5 Dec 2007, Prof Brian Ripley wrote:
>   
>> [...]
>>     
>
> Thanks I'll read it more carefully.
>
>   
>> Perhaps if you told us what you are trying to achieve we might be able to 
>> help you achieve it.
>>
>>     
>
> I have a function which takes a date as an argument.  I've tested it, and 
> I'd like to run it over a range of dates.  So I'm looking at apply- or 
> map- type functions.
>
>   
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>>
>>
>>     
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From bolker at ufl.edu  Wed Dec  5 21:35:44 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 5 Dec 2007 12:35:44 -0800 (PST)
Subject: [R] Displaying numerics to full double precision
In-Reply-To: <14179534.post@talk.nabble.com>
References: <14175334.post@talk.nabble.com> <14178707.post@talk.nabble.com>
	<14179534.post@talk.nabble.com>
Message-ID: <14179985.post@talk.nabble.com>




Jeff Delmerico wrote:
> 
> Thanks Ben, that fixed the display within R.  However, even after changing
> the display settings, the matrix elements still appear to be exported in
> single precision.  The matrix object is being passed into my C routines as
> an SEXP Numeric type, and somewhere along the way, some of the digits are
> getting lost.  
> Here's the relevant bit of my C code:
> 
> SEXP
> divideMatrix(SEXP matrix_in, SEXP sub_height, SEXP sub_width, SEXP fileS)
> ...
> if ( isMatrix(matrix_in) && isNumeric(matrix_in) )
> {
> 	/* Use R macros to convert from SEXP to C types */
> 	matrix = REAL(matrix_in);
> 	height = INTEGER(GET_DIM(matrix_in))[0];
> 	width = INTEGER(GET_DIM(matrix_in))[1];
> 	subW = INTEGER_VALUE(sub_width);
> 	subH = INTEGER_VALUE(sub_height);
>         ...
> }
> 
> Am I using the wrong macro to convert into a double in C?  Any ideas?
> 
> Thanks,
> Jeff Delmerico
> 
> 
> Ben Bolker wrote:
>> 
>> 
>> 
>> Jeff Delmerico wrote:
>>> 
>>> I'm working on a shared library of C functions for use with R, and I
>>> want to create a matrix in R and pass it to the C routines.  I know R
>>> computes and supposedly stores numerics in double precision, but when I
>>> create a matrix of random numerics using rnorm(), the values are
>>> displayed in single precision, and also exported in single precision
>>> when I pass them out to my C routines.  An example is below:
>>> 
>>>> a <- matrix(rnorm(16, mean=10, sd=4), nrow=4)
>>>> a
>>>           [,1]      [,2]      [,3]      [,4]
>>> [1,] 14.907606 17.572872 19.708977  9.809943
>>> [2,]  9.322041 13.624452  7.745254  7.596176
>>> [3,] 10.642408  6.151546  9.937434  6.913875
>>> [4,] 14.617647  5.577073  8.217559 12.115465
>>>> storage.mode(a)
>>> [1] "double"
>>> 
>>> Does anyone know if there is a way to change the display or storage
>>> settings so that the values will be displayed to their full precision?  
>>> Or does rnorm only produce values to single precision? 
>>> 
>>> Any assistance would be greatly appreciated.
>>> 
>>> Thanks,
>>> Jeff Delmerico
>>> 
>> 
>> options("digits") # 7
>> options(digits=x)
>> 
>>   I may be mistaken, but I think the values are indeed exported
>> as double precision -- the issue here is just a display setting.
>> 
>>   Ben Bolker
>> 
>> 
>> 
> 
> 

 I'm not sure.
  I do know  that Rinternals.h has

#define REAL(x)         ((double *) DATAPTR(x))

  so that doesn't seem to be the problem ...

  Ben


-- 
View this message in context: http://www.nabble.com/Displaying-numerics-to-full-double-precision-tf4950807.html#a14179985
Sent from the R help mailing list archive at Nabble.com.


From corey.sparks at utsa.edu  Wed Dec  5 21:39:26 2007
From: corey.sparks at utsa.edu (Corey Sparks)
Date: Wed, 5 Dec 2007 14:39:26 -0600
Subject: [R] coxme frailty model standard errors?
Message-ID: <2768A5B569B1D54EA47861B9A05422E10227ED5B@jade1604.UTSARR.NET>

Hello,
I am running R 2.6.1 on windows xp 
I am trying to fit a cox proportional hazard model with a shared
Gaussian frailty term using coxme
My model is specified as:

nofit1<-coxme(Surv(Age,cen1new)~ Sex+bo2+bo3,random=~1|isl,data=mydat)

With x1-x3 being dummy variables, and isl being the community level
variable with 4 levels.

Does anyone know if there is a way to get the standard error for the
random effect, like in nofit1$var?  I would like to know if my random
effect is worth writing home about.

Any help would be most appreciated
Corey Sparks

I can get the following output
nofit1<-coxme(Surv(Age,cen1new)~ Sex+bo2+bo3,random=~1|isl, data=no1901)
nofit1
Cox mixed-effects model fit by maximum likelihood
  Data: no1901 
  n=959 (2313 observations deleted due to missingness)
  Iterations= 3 69 
                    NULL Integrated Penalized
Log-likelihood -600.0795  -581.1718 -577.9682

  Penalized loglik: chisq= 44.22 on 5.61 degrees of freedom, p= 4.3e-08 
 Integrated loglik: chisq= 37.82 on 4 degrees of freedom, p= 1.2e-07 

Fixed effects: Surv(Age, cen1new) ~ Sex + bo2 + bo3 
         coef exp(coef)  se(coef)    z      p
Sex 0.2269214  1.254731 0.2151837 1.05 0.2900
bo2 0.5046991  1.656487 0.2510523 2.01 0.0440
bo3 1.0606144  2.888145 0.2726000 3.89 0.0001

Random effects: ~1 | isl 
                isl
Variance: 0.3876189



Corey Sparks
Assistant Professor
Department of Demography and Organization Studies
University of Texas-San Antonio
One UTSA Circle
San Antonio TX 78249
Phone: 210 458 6858
corey.sparks at utsa.edu


From daniel at umd.edu  Wed Dec  5 22:02:48 2007
From: daniel at umd.edu (Daniel Malter)
Date: Wed, 5 Dec 2007 16:02:48 -0500
Subject: [R] Interpretation of 'Intercept' in a 2-way factorial lm
In-Reply-To: <20071205203143.p8xlokf00s4sk48o@webmail5.uu.se>
Message-ID: <200712052102.CIM47190@md2.mail.umd.edu>

You estimate a model with the Factors A or B either present (1) or not
present (0) and with an intercept. Thus you would predict:

For both A and B not present: Intercept
For A only present: Intercept+coef(A)
For B only preseent: Intercept+coef(B)
For both present: Intercept+coef(A)+coef(B).

Again, you would interpret the intercept as the value of "fruit" when A and
B are not present (or inactive). If the intercept is not meaningful in your
setting and you just want to know if both groups differ, then you want to
use function aov I guess. What is your "fruit" variable? I would also
suggest to visually inspect your data. That always helps :) The code is also
down below.

Look at the following example in which 4 x 10 Ys are drawn randomly from
normal distributions with equal variance but different means. The first ten
observations have both A and B not present (i.e. 0) as specified in the
vectors "a" and "b". The mean of these observations where A and B are zero
is 1 as specified in y1=rnorm(10, -> 1 <-,1). As you will see if you run
this code, the estimated Intercept is 1.0512 which is close to 1 (the true
mean). As you see (just confirming what was said above), this is the average
of the baseline (or reference group if you will) when both A and B are
absent.

y1=rnorm(10,1,1)
y2=rnorm(10,2,1)
y3=rnorm(10,3,1)
y4=rnorm(10,4,1)

a=c(rep(0,20),rep(1,20))
b=c(rep(0,10),rep(1,10),rep(0,10),rep(1,10))

y=c(y1,y2,y3,y4)

data=data.frame(cbind(y,a,b))

####Plot####

interaction.plot(a,b,y)

####Models####

summary(lm(y~factor(a)+factor(b),data=data)

####Compare this to####

summary(aov(y~factor(a)+factor(b),data=data)

Cheers,
Daniel 


-------------------------
cuncta stricte discussurus
-------------------------

-----Urspr?ngliche Nachricht-----
Von: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Im
Auftrag von Gustaf Granath
Gesendet: Wednesday, December 05, 2007 2:32 PM
An: r-help at r-project.org
Betreff: [R] Interpretation of 'Intercept' in a 2-way factorial lm

Hi all,

I hope this question is not too trivial. I can't find an explanation
anywhere (Stats and R books, R-archives) so now I have to turn to the
R-list.

Question:

If you have a factorial design with two factors (say A and B with two levels
each). What does the intercept coefficient with treatment.contrasts
represent??

Here is an example without interaction where A has two levels A1 and A2, and
B has two levels B1 and B2. So R takes as a baseline A1 and B1.

coef( summary ( lm ( fruit ~ A + B, data = test)))

                Estimate   Std. Error  t value       Pr(>|t|)
(Intercept)   2.716667   0.5484828   4.953058   7.879890e-04
A2            6.266667   0.6333333   9.894737   3.907437e-06
B2            5.166667   0.6333333   8.157895   1.892846e-05

I understand that the mean of A2 is +6.3 more than A1, and that B2 is 5.2
more than B1.

So the question is: Is the intercept A1 and B1 combined as one mean ("the
baseline")? or is it something else? Does this number actually tell me
anything useful (2.716)??

What does the model (y = intercept  + ??) look like then? I can't understand
how both factors (A and B) can have the same intercept?

Thanks in advance!!

Gustaf Granath

Dept of Plant Ecology
Uppsala University, Sweden

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From GPetris at uark.edu  Wed Dec  5 22:04:01 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Wed, 5 Dec 2007 15:04:01 -0600 (CST)
Subject: [R] kalman filter random walk
In-Reply-To: <3303a4570712051005l3199f0f3y3a67af1796f8351f@mail.gmail.com>
	(message from Alexander Moreno on Wed, 05 Dec 2007 12:05:00 -0600)
References: <3303a4570712051005l3199f0f3y3a67af1796f8351f@mail.gmail.com>
Message-ID: <200712052104.lB5L41ci014735@definetti.ddns.uark.edu>


You may want to look at package dlm.

Giovanni

> Date: Wed, 05 Dec 2007 12:05:00 -0600
> From: Alexander Moreno <alexander.f.moreno at gmail.com>
> Sender: r-help-bounces at r-project.org
> Precedence: list
> 
> Hi,
> 
> I'm trying to use the kalman filter to estimate the variable drift of a
> random walk, given that I have a vector of time series data.  Anyone have
> any thoughts on how to do this in R?
> 
> Thanks,
> Alex
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 

Giovanni Petris  <GPetris at uark.edu>
Department of Mathematical Sciences
University of Arkansas - Fayetteville, AR 72701
Ph: (479) 575-6324, 575-8630 (fax)
http://definetti.uark.edu/~gpetris/


From landronimirc at gmail.com  Wed Dec  5 22:43:34 2007
From: landronimirc at gmail.com (Liviu Andronic)
Date: Wed, 5 Dec 2007 22:43:34 +0100
Subject: [R] Displaying numerics to full double precision
In-Reply-To: <14175334.post@talk.nabble.com>
References: <14175334.post@talk.nabble.com>
Message-ID: <68b1e2610712051343n20392a11mb6868101de2f7db2@mail.gmail.com>

On 12/5/07, Jeff Delmerico <jeffdelmerico at yahoo.com> wrote:
> Does anyone know if there is a way to change the display or storage settings
> so that the values will be displayed to their full precision?   Or does
> rnorm only produce values to single precision?
>
> Any assistance would be greatly appreciated.

As far as I know (beware, I'm a novice), internally R stores to and
uses full precision. The display of the data, however, is controlled
by "digits". You'd need to put, say, options(digits=7) in your
Rprofile.site (if it doesn't exist, creat it in the R etc/ folder).
You might also be interested by "scipen". Check ?options.

Regards,
Liviu


From hedbag at gmail.com  Wed Dec  5 23:05:45 2007
From: hedbag at gmail.com (Thomas Allen)
Date: Thu, 6 Dec 2007 11:05:45 +1300
Subject: [R] File based configuration
Message-ID: <2b2890520712051405h55b2f45fs2374265c661c9a75@mail.gmail.com>

I'm wanting to run R scripts non-interactively as part of a
"technology independent" framework.
I want control over the behaviour of these processes by specifying
various global variables in a "configuration file" that would be
passed as a command line argument.

I'm wondering if you know of any R support for configuration file
formats. (i.e. any functions that would read a configuration file of
some common format)

For example:
-The .properties configuration format for java seems to be quite
popular, would I have to read it in by writing some kind of java
extension to R?
-An XML configuration format could also be possible, but it's overkill
for my needs.

Any help would be greatly appreciated


From taekyunk at gmail.com  Wed Dec  5 23:16:37 2007
From: taekyunk at gmail.com (T.K.)
Date: Wed, 5 Dec 2007 14:16:37 -0800
Subject: [R] Learning to do randomized block design analysis
In-Reply-To: <36923f1d0712041816s35d884bat512773c83d4fa940@mail.gmail.com>
References: <2E8AE992B157C0409B18D0225D0B476306CD91FB@XCH-VN01.sph.ad.jhsph.edu>
	<36923f1d0712041446v539fdb1eof377f7739e8a6d7d@mail.gmail.com>
	<00ae01c836ca$479a5710$3a0b2c0a@gne.windows.gene.com>
	<14163277.post@talk.nabble.com>
	<36923f1d0712041816s35d884bat512773c83d4fa940@mail.gmail.com>
Message-ID: <36923f1d0712051416v2ddec85ewb82699ade7d73bb6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/563f91a4/attachment.pl 

From corey.sparks at utsa.edu  Wed Dec  5 23:22:04 2007
From: corey.sparks at utsa.edu (Corey Sparks)
Date: Wed, 5 Dec 2007 16:22:04 -0600
Subject: [R] coxme frailty model standard errors?
Message-ID: <2768A5B569B1D54EA47861B9A05422E10227ED5D@jade1604.UTSARR.NET>

Hello,
I am running R 2.6.1 on windows xp
I am trying to fit a cox proportional hazard model with a shared
Gaussian frailty term using coxme My model is specified as:

nofit1<-coxme(Surv(Age,cen1new)~ Sex+bo2+bo3,random=~1|isl,data=mydat)

With x1-x3 being dummy variables, and isl being the community level
variable with 4 levels.

Does anyone know if there is a way to get the standard error for the
random effect, like in nofit1$var?  I would like to know if my random
effect is worth writing home about.

Any help would be most appreciated
Corey Sparks

I can get the following output
nofit1<-coxme(Surv(Age,cen1new)~ Sex+bo2+bo3,random=~1|isl, data=no1901)
nofit1 Cox mixed-effects model fit by maximum likelihood
  Data: no1901
  n=959 (2313 observations deleted due to missingness)
  Iterations= 3 69 
                    NULL Integrated Penalized Log-likelihood -600.0795
-581.1718 -577.9682

  Penalized loglik: chisq= 44.22 on 5.61 degrees of freedom, p= 4.3e-08
Integrated loglik: chisq= 37.82 on 4 degrees of freedom, p= 1.2e-07 

Fixed effects: Surv(Age, cen1new) ~ Sex + bo2 + bo3 
         coef exp(coef)  se(coef)    z      p
Sex 0.2269214  1.254731 0.2151837 1.05 0.2900
bo2 0.5046991  1.656487 0.2510523 2.01 0.0440
bo3 1.0606144  2.888145 0.2726000 3.89 0.0001

Random effects: ~1 | isl 
                isl
Variance: 0.3876189

Corey Sparks
Assistant Professor
Department of Demography and Organization Studies
University of Texas-San Antonio
One UTSA Circle
San Antonio TX 78249
Phone: 210 458 6858
corey.sparks at utsa.edu


From b3i4old02 at sneakemail.com  Wed Dec  5 23:25:02 2007
From: b3i4old02 at sneakemail.com (Michael Hoffman)
Date: Wed, 05 Dec 2007 22:25:02 +0000
Subject: [R] Java parser for R data file?
In-Reply-To: <C37C3903.56%david@incogen.com>
References: <C37C3903.56%david@incogen.com>
Message-ID: <fj78fb$i3d$1@ger.gmane.org>

David Coppit wrote:
> Hi everyone,
> 
> Has anyone written a parser in Java for either the ASCII or binary format
> produced by save()?

You might want to consider using the hdf5 package to save the array in 
HDF5 format. There are HDF5 libraries for Java as well 
<http://hdf.ncsa.uiuc.edu/hdf-java-html/>. I have never used them, but 
it works quite well for transferring data between R and Python.


From jo.irisson at gmail.com  Wed Dec  5 23:45:38 2007
From: jo.irisson at gmail.com (jiho)
Date: Wed, 5 Dec 2007 23:45:38 +0100
Subject: [R] 2/3d interpolation from a regular grid to another regular
	grid
In-Reply-To: <e9ee1f0a0712050747u256186caj7a3a237f9aded948@mail.gmail.com>
References: <17EF7ABB-B4F8-4279-B9D0-620C02C91E84@gmail.com>
	<e9ee1f0a0712041238w4ff935fbg80f8f26895e02277@mail.gmail.com>
	<85D2CAEE-0DAC-4451-A7D2-DC45969F1E14@gmail.com>
	<e9ee1f0a0712050747u256186caj7a3a237f9aded948@mail.gmail.com>
Message-ID: <57D5F796-CC04-4CAC-BA83-E41FDBB6F5DD@gmail.com>

On 2007-December-05  , at 16:47 , Scionforbai wrote:
>> I just read the description in ?Krig in the package fields which  
>> says:
>> " Fits a surface to irregularly spaced data. "
>
> Yes, that is the most general case. Regular data location is a subset
> of irregular. Anyway, kriging, just one g, after the name of Danie
> Krige, the south african statistician who first applied such method
> for minig survey.

ooops. sorry about the typo.

>> My problem is simpler
> ...
>> So it is really purely numerical.
> ...
>> I just hoped that R had that already coded ...
>
> Of course R has ... ;) If your grids are really as simple as the
> example you posted above, and you have a really little variability,
> all you need is a "moving average", the arithmetic mean of the two
> nearest points belonging to grid1 and grid2 respectively. I assume
> that your regularly shaped grids are values stored in matrix objects.
>
> The functions comes from the "diff.default" code (downloading the R
> source code, I assure, is worth):

I can imagine it is indeed. I use the source of packages functions  
very often.

> my.interp <- function(x, lag = 1)
> {
>     r <- unclass(x)  # don't want class-specific subset methods
>     i1 <- -1:-lag
>     r <- (r[i1] + r[-length(r):-(length(r)-lag+1)])/2
>     class(r) <- oldClass(x)
>     return(r)
> }
>
> Finally,
>
> g1 <- apply(grid1val,1,my.interp)
> g2 <- apply(grid2val,2,my.interp)
>
> give the interpolations on gridFinal, provided that all gridFinal
> points are within the grid1 and grid2 ones.
>
> If you want the mean from 4 points, you apply once more with lag=3,
> cbind/rbind to the result columns/rows o NAs, and you calculate the
> mean of the points of the two matrixes.
> This is the simplest (and quickest) moving average that you can do.
> For more complicated examples, and for 3d, you have to go a little
> further, but the principle holds.

Thanks very much. I'll test this soon (and it looks like the vector  
operation might even be directly translatable in Fortran which is  
nice since I'll need to do it in Fortran too).

Thanks again.

JiHO
---
http://jo.irisson.free.fr/


From engrav at u.washington.edu  Thu Dec  6 00:16:55 2007
From: engrav at u.washington.edu (Loren Engrav)
Date: Wed, 05 Dec 2007 15:16:55 -0800
Subject: [R] Junk or not Junk ???
In-Reply-To: <4754C51A.4010109@stats.uwo.ca>
Message-ID: <C37C7167.156AD%engrav@u.washington.edu>

Thank you

As per advice from several R users I have set

r-project.org, stat.math.ethz.ch,fhcrc.org, stat.ethz.ch, math.ethz.ch,
hypatia.math.ethz.ch

 all to be "safe domains"

But still some R emails go to Junk and require to be found manually

I have explored the issue with Univ Wash computing to no avail

Is this just how it is or have I still missed the "fix" to keep R emails out
of junk?

Thank you

Loren Engrav
Univ Wash
Seattle



> From: Duncan Murdoch <murdoch at stats.uwo.ca>
> Date: Mon, 03 Dec 2007 22:10:18 -0500
> To: Loren Engrav <engrav at u.washington.edu>
> Cc: RHelp <r-help at r-project.org>
> Subject: Re: [R] Junk or not Junk
> 
> On 03/12/2007 8:56 PM, Loren Engrav wrote:
>> So a message from
>> 
>> Benilton Carvalho <bcarvalh at jhsph.edu> (sent by
>> r-help-bounces at r-project.org)
>> 
>>  arrives and goes in the Junk Mail even tho I have set @r-project.org to not
>> be junk
>> 
>> Why does this go in Junk mail if @r-project.org is defined as not junk?
> 
> Why are you asking us about how you have your mail filters set up?
> 
> If you didn't set them up yourself, you should find out from your local
> admin who did, and ask them.
> 
> Duncan Murdoch


From charpent at bacbuc.dyndns.org  Thu Dec  6 00:31:29 2007
From: charpent at bacbuc.dyndns.org (Emmanuel Charpentier)
Date: Thu, 06 Dec 2007 00:31:29 +0100
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <Pine.LNX.4.64.0712051655180.12631@gannet.stats.ox.ac.uk>
References: <4756C0E2.1060902@unifi.it>	<18262.54350.143549.665409@localhost.localdomain>
	<Pine.LNX.4.64.0712051655180.12631@gannet.stats.ox.ac.uk>
Message-ID: <fj7cci$ufe$1@ger.gmane.org>

Prof Brian Ripley a ?crit :
> Note that Ottorino has only 1GB of RAM installed, which makes a 64-bit
> version of R somewhat moot.  See chapter 8 of
> 
> http://cran.r-project.org/doc/manuals/R-admin.html

Thank you for this reminder|tip ! I didn't re-read this document since
... oh, my  ... very early (1.x ?) versions. At which time my favorite
peeve against R was the fixed memory allocation scheme.

I would have thought that 64 bits machines could take advantage of a
wider bus and (marginally ?) faster instructions to balance larger
pointers. Am I mistaken ?

Thank again !

					Emmanuel Charpentier


From jfox at mcmaster.ca  Thu Dec  6 00:59:20 2007
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 5 Dec 2007 18:59:20 -0500
Subject: [R] nearest correlation to polychoric
In-Reply-To: <20070713184214.82160@gmx.net>
Message-ID: <6bpm1d$503of7@toip4.srvr.bell.ca>

Dear Jens,

I've submitted a new version (0.7-4) of the polycor package to CRAN. The
hetcor() function now uses your nearcor() in sfsmisc to make the returned
correlation matrix positive-definite if it is not already.

I know that quite some time has elapsed since you raised this issue, and I
apologize for taking so long to deal with it. (I've also kept track of your
suggestions for the sem package, and will respond to them when I next make
substantial modifications to the package -- though not in the near future.)

Thank you,
 John

--------------------------------
John Fox, Professor
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: "Jens Oehlschl?gel" [mailto:oehl_list at gmx.de] 
> Sent: Friday, July 13, 2007 2:42 PM
> To: dimitris.rizopoulos at med.kuleuven.be; 
> maechler at stat.math.ethz.ch; jfox at mcmaster.ca
> Cc: r-help at hypatia.math.ethz.ch
> Subject: RE: [R] nearest correlation to polychoric
> 
> Dimitris,
> 
> Thanks a lot for the quick response with the pointer to 
> posdefify. Using its logic as an afterburner to the algorithm 
> of Higham seems to work.
> 
> Martin,
> 
> > Jens, could you make your code (mentioned below) available 
> to the community, or even donate to be included as a new 
> method of posdefify() ?
> 
> Nice opportunity to give-back. Below is the R code for 
> nearcor and .Rd help file. A quite natural place for nearcor 
> would be John Fox' package polycor, what do you think?
> 
> John?
> 
> Best regards
> 
> 
> Jens Oehlschl?gel
> 
> 
> # Copyright (2007) Jens Oehlschl?gel
> # GPL licence, no warranty, use at your own risk
> 
> #! \name{nearcor}
> #! \alias{nearcor}
> #! \title{ function to find the nearest proper correlation 
> matrix given an improper one } #! \description{
> #!   This function smooths a improper correlation matrix as 
> it can result from \code{\link{cor}} with 
> \code{use="pairwise.complete.obs"} or \code{\link[polycor]{hetcor}}.
> #! }
> #! \usage{
> #! nearcor(R, eig.tol = 1e-06, conv.tol = 1e-07, posd.tol = 
> 1e-08, maxits = 100, verbose = FALSE) #! } #! \arguments{
> #!   \item{R}{ a square symmetric approximate correlation matrix }
> #!   \item{eig.tol}{ defines relative positiveness of 
> eigenvalues compared to largest, default=1.0e-6 }
> #!   \item{conv.tol}{ convergence tolerance for algorithm, 
> default=1.0e-7  }
> #!   \item{posd.tol}{ tolerance for enforcing positive 
> definiteness, default=1.0e-8 }
> #!   \item{maxits}{ maximum number of iterations allowed }
> #!   \item{verbose}{ set to TRUE to verbose convergence }
> #! }
> #! \details{
> #!   This implements the algorithm of Higham (2002), then 
> forces symmetry, then forces positive definiteness using code 
> from \code{\link[sfsmisc]{posdefify}}.
> #!   This implementation does not make use of direct LAPACK 
> access for tuning purposes as in the MATLAB code of Lucas (2001).
> #!   The algorithm of Knol DL and ten Berge (1989) (not 
> implemented here) is more general in (1) that it allows 
> contraints to fix some rows (and columns) of the matrix and 
> (2) to force the smallest eigenvalue to have a certain value.
> #! }
> #! \value{
> #!   A LIST, with components
> #!   \item{cor}{resulting correlation matrix}
> #!   \item{fnorm}{Froebenius norm of difference of input and output}
> #!   \item{iterations}{number of iterations used}
> #!   \item{converged}{logical}
> #! }
> #! \references{
> #!        Knol, DL and ten Berge, JMF (1989). Least-squares 
> approximation of an improper correlation matrix by a proper 
> one.  Psychometrika, 54, 53-61.
> #!   \cr  Higham (2002). Computing the nearest correlation 
> matrix - a problem from finance, IMA Journal of Numerical 
> Analysis, 22, 329-343.
> #!   \cr  Lucas (2001). Computing nearest covariance and 
> correlation matrices. A thesis submitted to the University of 
> Manchester for the degree of Master of Science in the Faculty 
> of Science and Engeneering.
> #! }
> #! \author{ Jens Oehlschl?gel }
> #! \seealso{ \code{\link[polycor]{hetcor}}, 
> \code{\link{eigen}}, \code{\link[sfsmisc]{posdefify}} } #! \examples{
> #!   cat("pr is the example matrix used in Knol DL, ten Berge 
> (1989)\n")
> #!   pr <- structure(c(1, 0.477, 0.644, 0.478, 0.651, 0.826, 
> 0.477, 1, 0.516,
> #!   0.233, 0.682, 0.75, 0.644, 0.516, 1, 0.599, 0.581, 0.742, 0.478,
> #!   0.233, 0.599, 1, 0.741, 0.8, 0.651, 0.682, 0.581, 0.741, 
> 1, 0.798,
> #!   0.826, 0.75, 0.742, 0.8, 0.798, 1), .Dim = c(6, 6))
> #!
> #!   nr <- nearcor(pr)$cor
> #!   plot(pr[lower.tri(pr)],nr[lower.tri(nr)])
> #!   round(cbind(eigen(pr)$values, eigen(nr)$values), 8)
> #!
> #!   cat("The following will fail:\n")
> #!   try(factanal(cov=pr, factors=2))
> #!   cat("and this should work\n")
> #!   try(factanal(cov=nr, factors=2))
> #!
> #!   \dontrun{
> #!     library(polycor)
> #!
> #!     n <- 400
> #!     x <- rnorm(n)
> #!     y <- rnorm(n)
> #!
> #!     x1 <- (x + rnorm(n))/2
> #!     x2 <- (x + rnorm(n))/2
> #!     x3 <- (x + rnorm(n))/2
> #!     x4 <- (x + rnorm(n))/2
> #!
> #!     y1 <- (y + rnorm(n))/2
> #!     y2 <- (y + rnorm(n))/2
> #!     y3 <- (y + rnorm(n))/2
> #!     y4 <- (y + rnorm(n))/2
> #!
> #!     dat <- data.frame(x1, x2, x3, x4, y1, y2, y3, y4)
> #!
> #!     x1 <- ordered(as.integer(x1 > 0))
> #!     x2 <- ordered(as.integer(x2 > 0))
> #!     x3 <- ordered(as.integer(x3 > 1))
> #!     x4 <- ordered(as.integer(x4 > -1))
> #!
> #!     y1 <- ordered(as.integer(y1 > 0))
> #!     y2 <- ordered(as.integer(y2 > 0))
> #!     y3 <- ordered(as.integer(y3 > 1))
> #!     y4 <- ordered(as.integer(y4 > -1))
> #!
> #!     odat <- data.frame(x1, x2, x3, x4, y1, y2, y3, y4)
> #!
> #!     xcor <- cor(dat)
> #!     pcor <- cor(odat)
> #!     hcor <- hetcor(odat, ML=TRUE, std.err=FALSE)$correlations
> #!     ncor <- nearcor(hcor)$cor
> #!
> #!     try(factanal(covmat=xcor, factors=2, n.obs=n))
> #!     try(factanal(covmat=pcor, factors=2, n.obs=n))
> #!     try(factanal(covmat=hcor, factors=2, n.obs=n))
> #!     try(factanal(covmat=ncor, factors=2, n.obs=n))
> #!   }
> #! }
> #! \keyword{algebra}
> #! \keyword{array}
> 
> nearcor <- function(  # Computes the nearest correlation 
> matrix to an approximate correlation matrix, i.e. not 
> positive semidefinite.
>   R                   # n-by-n approx correlation matrix
> , eig.tol   = 1.0e-6  # defines relative positiveness of 
> eigenvalues compared to largest
> , conv.tol  = 1.0e-7  # convergence tolerance for algorithm , 
> posd.tol  = 1.0e-8  # tolerance for enforcing positive definiteness
> , maxits    = 100     # maximum number of iterations allowed
> , verbose   = FALSE   # set to TRUE to verbose convergence
> 
>                       # RETURNS list of class nearcor with 
> components cor, iterations, converged ){
>   if (!(is.numeric(R) && is.matrix(R) && identical(R,t(R))))
>     stop('Error: Input matrix R must be square and symmetric')
> 
>   # Inf norm
>   inorm <- function(x)max(rowSums(abs(x)))
>   # Froebenius norm
>   fnorm <- function(x)sqrt(sum(diag(t(x) %*% x)))
> 
>   n <- ncol(R)
>   U <- matrix(0, n, n)
>   Y <- R
>   iter <- 0
> 
>   while (TRUE){
>       T <- Y - U
> 
>       # PROJECT ONTO PSD MATRICES
>       e <- eigen(Y, symmetric=TRUE)
>       Q <- e$vectors
>       d <- e$values
>       D <- diag(d)
> 
>       # create mask from relative positive eigenvalues
>       p <- (d>eig.tol*d[1]);
> 
>       # use p mask to only compute 'positive' part
>       X <- Q[,p,drop=FALSE] %*% D[p,p,drop=FALSE] %*% 
> t(Q[,p,drop=FALSE])
> 
>       # UPDATE DYKSTRA'S CORRECTION
>       U <- X - T
> 
>       # PROJECT ONTO UNIT DIAG MATRICES
>       X <- (X + t(X))/2
>       diag(X) <- 1
> 
>       conv <- inorm(Y-X) / inorm(Y)
>       iter <- iter + 1
>       if (verbose)
>         cat("iter=", iter, "  conv=", conv, "\n", sep="")
> 
>       if (conv <= conv.tol){
>         converged <- TRUE
>         break
>       }else if (iter==maxits){
>         warning(paste("nearcor did not converge in", iter, 
> "iterations"))
>         converged <- FALSE
>         break
>       }
>       Y <- X
>   }
>   X <- (X + t(X))/2
>   # begin from posdefify(sfsmisc)
>   e <- eigen(X, symmetric = TRUE)
>   d <- e$values
>   Eps <- posd.tol * abs(d[1])
>   if (d[n] < Eps) {
>       d[d < Eps] <- Eps
>       Q <- e$vectors
>       o.diag <- diag(X)
>       X <- Q %*% (d * t(Q))
>       D <- sqrt(pmax(Eps, o.diag)/diag(X))
>       X[] <- D * X * rep(D, each = n)
>   }
>   # end from posdefify(sfsmisc)
>   # force symmetry
>   X <- (X + t(X))/2
>   diag(X) <- 1
>   ret <- list(cor=X, fnorm=fnorm(R-X), iterations=iter, 
> converged=converged)
>   class(ret) <- "nearcor"
>   ret
> }
> 
> --
> Ist Ihr Browser Vista-kompatibel? Jetzt die neuesten 
> Browser-Versionen downloaden: http://www.gmx.net/de/go/browser
> 


From gregor.gorjanc at bfro.uni-lj.si  Thu Dec  6 01:06:00 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 6 Dec 2007 00:06:00 +0000 (UTC)
Subject: [R] Dealing with NA's in a data matrix
References: <786988.22594.qm@web25011.mail.ukl.yahoo.com>
	<da79af330712051151k7a13b326g705a4a13647b4e99@mail.gmail.com>
Message-ID: <loom.20071206T000350-315@post.gmane.org>

Henrique Dallazuanna <wwwhsd <at> gmail.com> writes:
>  x[is.na(x)] <- 0
> 
> On 05/12/2007, Amit Patel <amitpatel_ak <at> yahoo.co.uk> wrote:
> > Hi I have a matrix with NA value that I would like to convert these to a
value of 0.
> > any suggestions

also

library(gdata)
x <- matrix(rnorm(16), nrow=4, ncol=4)
x[1, 1] <- NA
NAToUnknown(x, unknown=0)

Gregor


From g.abraham at ms.unimelb.edu.au  Thu Dec  6 01:07:48 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Thu, 06 Dec 2007 11:07:48 +1100
Subject: [R] predict error for survreg with natural splines
In-Reply-To: <Pine.LNX.4.64.0712041925400.20006@tajo.ucsd.edu>
References: <4755E997.4020907@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0712041925400.20006@tajo.ucsd.edu>
Message-ID: <47573D54.3020901@ms.unimelb.edu.au>

Charles C. Berry wrote:
> On Wed, 5 Dec 2007, Gad Abraham wrote:
> 
>> Hi,
>>
>> The following error looks like a bug to me but perhaps someone can shed
>> light on it:
>>
>> > library(splines)
>> > library(survival)
>> > s <- survreg(Surv(futime, fustat) ~ ns(age, knots=c(50, 60)),
>> data=ovarian)
>> > n <- data.frame(age=rep(mean(ovarian$age), 10))
>> > predict(s, newdata=n)
>> Error in qr.default(t(const)) :
>>   NA/NaN/Inf in foreign function call (arg 1)
>>
>> Thanks,
>> Gad
> 
> Gad,
> 
> I think I have it now.
> 
> survreg does not automatically place the boundary knots in its $terms 
> component.
> 
> You can force this by hand:

Thanks Chuck and Moshe, manually setting the boundary fixes the problem.

Cheers,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From Michael_Bibo at health.qld.gov.au  Thu Dec  6 01:18:13 2007
From: Michael_Bibo at health.qld.gov.au (Michael Bibo)
Date: Thu, 06 Dec 2007 10:18:13 +1000
Subject: [R] HTML help search in R 2.6.0 v 2.6.1
Message-ID: <s757cc74.089@gwiaext.health.qld.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/6fe345d8/attachment.pl 

From affysnp at gmail.com  Thu Dec  6 01:20:33 2007
From: affysnp at gmail.com (affy snp)
Date: Wed, 5 Dec 2007 19:20:33 -0500
Subject: [R] hclust in heatmap.2
Message-ID: <5032046e0712051620l4ea3e458o7807d53a039cac16@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/10c31a81/attachment.pl 

From ggrothendieck at gmail.com  Thu Dec  6 01:29:35 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Dec 2007 19:29:35 -0500
Subject: [R] File based configuration
In-Reply-To: <2b2890520712051405h55b2f45fs2374265c661c9a75@mail.gmail.com>
References: <2b2890520712051405h55b2f45fs2374265c661c9a75@mail.gmail.com>
Message-ID: <971536df0712051629t7fdaf194iaf0ab42476425281@mail.gmail.com>

See the following (the email seems to have wrapped 2 lines onto one
at one point but it should be obvious):

http://tolstoy.newcastle.edu.au/R/e2/help/07/06/18853.html

On Dec 5, 2007 5:05 PM, Thomas Allen <hedbag at gmail.com> wrote:
> I'm wanting to run R scripts non-interactively as part of a
> "technology independent" framework.
> I want control over the behaviour of these processes by specifying
> various global variables in a "configuration file" that would be
> passed as a command line argument.
>
> I'm wondering if you know of any R support for configuration file
> formats. (i.e. any functions that would read a configuration file of
> some common format)
>
> For example:
> -The .properties configuration format for java seems to be quite
> popular, would I have to read it in by writing some kind of java
> extension to R?
> -An XML configuration format could also be possible, but it's overkill
> for my needs.
>
> Any help would be greatly appreciated
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tfitzhugh at TNC.ORG  Thu Dec  6 01:42:04 2007
From: tfitzhugh at TNC.ORG (Tom Fitzhugh)
Date: Wed, 5 Dec 2007 19:42:04 -0500
Subject: [R] correlation coefficient from qq plot
Message-ID: <EDE5250EC60782409CCD2A210403E7D128CDBE@mail04.TNC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/3edfee3b/attachment.pl 

From tfitzhugh at TNC.ORG  Thu Dec  6 01:53:58 2007
From: tfitzhugh at TNC.ORG (Tom Fitzhugh)
Date: Wed, 5 Dec 2007 19:53:58 -0500
Subject: [R] prediction R-squared
Message-ID: <EDE5250EC60782409CCD2A210403E7D128CDC6@mail04.TNC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/1cb621da/attachment.pl 

From p.dalgaard at biostat.ku.dk  Thu Dec  6 02:19:47 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 06 Dec 2007 02:19:47 +0100
Subject: [R] HTML help search in R 2.6.0 v 2.6.1
In-Reply-To: <s757cc74.089@gwiaext.health.qld.gov.au>
References: <s757cc74.089@gwiaext.health.qld.gov.au>
Message-ID: <47574E33.5020603@biostat.ku.dk>

Michael Bibo wrote:
> I am running R on a corporate Windows XP SP2 machine on which I do not
> have 
> administrator privileges or access to most settings in Control Panel. 
> R is 
> installed from my limited user account.  The version of the JVM I have
>
> installed is perhaps best described as antique:
>  
>   
>> system(paste("java -version"),show.output.on.console=T)
>>     
> java version "1.4.1"
> Java(TM) 2 Runtime Environment, Standard Edition (build 1.4.1-b21)
> Java HotSpot(TM) Client VM (build 1.4.1-b21, mixed mode)
>  
> The HTML help search applet has worked for all versions of R up to and
>
> including 2.6.0, but in 2.6.1 the java applet is not initialised
> ("Applet 
> SearchEngine notinited").  I have checked Appendix D of the admin & 
> installation manual, and the test java applet referred to does not
> load, 
> but the web page says that java 1.4.2 is required.  Other java applets
> do run 
> in the browser, including R 2.6.0 HTML help search, so I presume java
> is 
> enabled. 
>  
> Has something changed from 2.6.0 to 2.6.1 that may require JVM > 1.4.1?
>  If 
> so, I can use that information to request an upgrade of my JVM.
>  
>
>   
Hmm, could be. They got rebuilt on my system and committted at some 
point in the 2.6.1 run-in. and that system has 1.5.0. I did check that 
things still worked, but I didn't think the version would matter. The 
actual Java code is unchanged, so you could copy the .class files over 
from 2.6.0 (& let us know if it works)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From michael_bibo at health.qld.gov.au  Thu Dec  6 02:37:48 2007
From: michael_bibo at health.qld.gov.au (Michael Bibo)
Date: Thu, 6 Dec 2007 01:37:48 +0000 (UTC)
Subject: [R] HTML help search in R 2.6.0 v 2.6.1
References: <s757cc74.089@gwiaext.health.qld.gov.au>
	<47574E33.5020603@biostat.ku.dk>
Message-ID: <loom.20071206T013345-210@post.gmane.org>

Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:


> >  
> > Has something changed from 2.6.0 to 2.6.1 that may require JVM > 1.4.1?
> >  If 
> > so, I can use that information to request an upgrade of my JVM.
> >  
> >
> >   
> Hmm, could be. They got rebuilt on my system and committted at some 
> point in the 2.6.1 run-in. and that system has 1.5.0. I did check that 
> things still worked, but I didn't think the version would matter. The 
> actual Java code is unchanged, so you could copy the .class files over 
> from 2.6.0 (& let us know if it works)
> 
Excellent!  That worked.  Thanks, Peter.
If this is going to be a continuing issue for future versions, would the best 
advice be to upgrade JVM anyway, at least to 1.5.0?

Michael


From leffgh at 163.com  Thu Dec  6 03:06:54 2007
From: leffgh at 163.com (Bin Yue)
Date: Wed, 5 Dec 2007 18:06:54 -0800 (PST)
Subject: [R]  logistic regression using "glm",which "y" is set to be "1"
Message-ID: <14185060.post@talk.nabble.com>


Dear friends :
    using the "glm" function and setting family=binomial, I got a list of
coefficients.
The coefficients reflect the effects  of predicted variables on the
probability of the response to be "1".
My response variable consists of  "A" and "D" . I don't know which level of
the response was set to be 1.
is the first element of the response set to be 1?
   Thank all in advance.
   Regards,

-----
Best regards,
Bin Yue

*************
student for a Master program in South Botanical Garden , CAS

-- 
View this message in context: http://www.nabble.com/logistic-regression-using-%22glm%22%2Cwhich-%22y%22-is-set-to-be-%221%22-tf4953617.html#a14185060
Sent from the R help mailing list archive at Nabble.com.


From kubovy at virginia.edu  Thu Dec  6 03:13:21 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 5 Dec 2007 21:13:21 -0500
Subject: [R] Using expression in Hmisc Key()
Message-ID: <55B194DA-5A84-4BCD-8869-C55D47FDB695@virginia.edu>

Dear r-helpers,

How do I tell xYplot() and Key() that I want the labels in italic?

I tried this:

pdf(file = "khw.pdf", width = 8, height = 8)
with(subset(khw, length < 1.6), xYplot(lo ~ length|sub, groups = v,
aspect="xy", pch = c(1, 2, 16), col = c(2, 4, 9),
xlab=expression(frac(abs(bolditalic(v) ), abs(bolditalic(a)))),
ylab = grid::textGrob(expression(paste(log, ~~frac(italic(p) 
(italic(v)), italic(p)(italic(a))))) ),
panel=function(x,y,...){
	panel.xYplot(x, y, ...)
	panel.lmline(x, y, type = "l")
	}))
Key(x = 0.667, y = 0.833, other = list(title = expression(italic(v)),  
cex.title = 1,
	labels = c(expression(italic(b)), expression(italic(c)),  
expression(italic(d)))))
dev.off()

This works only for the title.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From marc_schwartz at comcast.net  Thu Dec  6 03:47:53 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 05 Dec 2007 20:47:53 -0600
Subject: [R] logistic regression using "glm",which "y" is set to be "1"
In-Reply-To: <14185060.post@talk.nabble.com>
References: <14185060.post@talk.nabble.com>
Message-ID: <1196909273.2962.12.camel@Bellerophon.localdomain>


On Wed, 2007-12-05 at 18:06 -0800, Bin Yue wrote:
> Dear friends :
>     using the "glm" function and setting family=binomial, I got a list of
> coefficients.
> The coefficients reflect the effects  of predicted variables on the
> probability of the response to be "1".
> My response variable consists of  "A" and "D" . I don't know which level of
> the response was set to be 1.
> is the first element of the response set to be 1?
>    Thank all in advance.
>    Regards,
> 
> -----
> Best regards,
> Bin Yue


As per the Details section of ?glm:

For binomial and quasibinomial families the response can also be
specified as a factor (when the first level denotes failure and all
others success) ...


So use:

  levels(response.variable)

and that will give you the factor levels, where the first level is 0 and
the second level is 1. 

If you work in a typical English based locale with default alpha based
level ordering, it will likely be A (Alive?) is 0 and D (Dead?) is 1.

HTH,

Marc Schwartz


From leffgh at 163.com  Thu Dec  6 04:41:37 2007
From: leffgh at 163.com (Bin Yue)
Date: Wed, 5 Dec 2007 19:41:37 -0800 (PST)
Subject: [R] logistic regression using "glm",which "y" is set to be "1"
In-Reply-To: <1196909273.2962.12.camel@Bellerophon.localdomain>
References: <14185060.post@talk.nabble.com>
	<1196909273.2962.12.camel@Bellerophon.localdomain>
Message-ID: <14185819.post@talk.nabble.com>


Dear Marc Schwartz:
 When I ask R2.6.0 for windows, the information it gives does not contain
much about family=binomial .
 You said that there is a detail section of "?glm". I want to read it
thoroughly. Could  you tell me where and how I can find the detail section
of "?glm".
   Thank you very much .
   Best regards,
 Bin Yue
  
     

Marc Schwartz wrote:
> 
> 
> On Wed, 2007-12-05 at 18:06 -0800, Bin Yue wrote:
>> Dear friends :
>>     using the "glm" function and setting family=binomial, I got a list of
>> coefficients.
>> The coefficients reflect the effects  of predicted variables on the
>> probability of the response to be "1".
>> My response variable consists of  "A" and "D" . I don't know which level
>> of
>> the response was set to be 1.
>> is the first element of the response set to be 1?
>>    Thank all in advance.
>>    Regards,
>> 
>> -----
>> Best regards,
>> Bin Yue
> 
> 
> As per the Details section of ?glm:
> 
> For binomial and quasibinomial families the response can also be
> specified as a factor (when the first level denotes failure and all
> others success) ...
> 
> 
> So use:
> 
>   levels(response.variable)
> 
> and that will give you the factor levels, where the first level is 0 and
> the second level is 1. 
> 
> If you work in a typical English based locale with default alpha based
> level ordering, it will likely be A (Alive?) is 0 and D (Dead?) is 1.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 


-----
Best regards,
Bin Yue

*************
student for a Master program in South Botanical Garden , CAS

-- 
View this message in context: http://www.nabble.com/logistic-regression-using-%22glm%22%2Cwhich-%22y%22-is-set-to-be-%221%22-tf4953617.html#a14185819
Sent from the R help mailing list archive at Nabble.com.


From helprhelp at gmail.com  Thu Dec  6 04:54:30 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 5 Dec 2007 22:54:30 -0500
Subject: [R] logistic regression using "glm",which "y" is set to be "1"
In-Reply-To: <14185819.post@talk.nabble.com>
References: <14185060.post@talk.nabble.com>
	<1196909273.2962.12.camel@Bellerophon.localdomain>
	<14185819.post@talk.nabble.com>
Message-ID: <cdf817830712051954y717e778dn4d7d4faa99b36f61@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/4e5741a3/attachment.pl 

From sabunime at gmail.com  Thu Dec  6 05:03:51 2007
From: sabunime at gmail.com (Saeed Abu Nimeh)
Date: Wed, 05 Dec 2007 22:03:51 -0600
Subject: [R] snow package on multi core unix box
Message-ID: <475774A7.1020003@gmail.com>

Is the rmpi package (or rpvm) needed to exploit multiple cores on a
single unix box using the snow package. The documentation of the package
does not provide info about setting up a single machine with multiple
cores. Also, if how effective is it to run a bayesian simulation on
parallel (or distributed) processors using the snow package.
Thanks,
Saeed


From ggrothendieck at gmail.com  Thu Dec  6 05:07:37 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Dec 2007 23:07:37 -0500
Subject: [R] Working with "ts" objects
In-Reply-To: <001b01c83775$4ef6fd50$ece4f7f0$@edu>
References: <001b01c83775$4ef6fd50$ece4f7f0$@edu>
Message-ID: <971536df0712052007j73fe6623ndbdbbf61782357f1@mail.gmail.com>

anscombe is built into R already so you don't need to read it in.
An intercept is the default in lm so you don't have to specify it.

opar <- par(mfrow = c(2,2))
plot(y1 ~ x1, anscombe)
reg <- lm(y1 ~ x1, anscombe)
reg
abline(reg)
...etc...
par(opar)

Note that plot(anscombe[1:2]) and lm(anscombe[2:1]) also work.

read.table returns a data frame whereas ts requires a vector
or matrix so none of your ts code will work.  as.matrix(DF)
or data.matrix(DF) will convert data frame DF to a matrix.


On Dec 5, 2007 2:30 PM, Richard Saba <sabaric at auburn.edu> wrote:
> I am relatively new to R and object oriented programming. I have relied on
> SAS for most of my data analysis.  I teach an introductory undergraduate
> forecasting course using the Diebold text and I am considering using R in
> addition to SAS and Eviews in the course. I work primarily with univariate
> or multivariate time series data. I am having a great deal of difficulty
> understanding and working with "ts" objects particularly when it comes to
> referencing variables in plot commands or in formulas. The confusion is
> amplified when certain procedures (lm for example) coerce the "ts" object
> into a data.frame before application with the results that the output is
> stored in a data.frame object.
> For example the two sets of code below replicate examples from chapter 2 and
> 6 in the text. In the first set of code if I were to replace
> "anscombe<-read.table(fname, header=TRUE)" with
> "anscombe<-ts(read.table(fname, header=TRUE))" the plot() commands would
> generate errors. The objects "x1", "y1" ...  would not be recognized. In
> this case I would have to reference the specific column in the anscombe data
> set. If I would have constructed the data set from several different data
> sets using the ts.intersect() function (see second code below)the problem
> becomes even more involved and keeping track of which columns are associated
> with which variables can be rather daunting. All I wanted was to plot actual
> vs. predicted values of "hstarts" and the residuals from the model.
>
> Given the difficulties I have encountered I know my students will have
> similar problems. Is there a source other than the basic R manuals that I
> can consult and recommend to my students that will help get a handle on
> working with time series objects? I found the Shumway "Time series analysis
> and its applications with R Examples" website very helpful but many
> practical questions involving manipulation of time series data still remain.
> Any help will be appreciated.
> Thanks,
>
> Richard Saba
> Department of Economics
> Auburn University
> Email:  sabaric at auburn.edu
> Phone:  334 844-2922
>
>
>
>
> anscombe<-read.table(fname, header=TRUE)
> names(anscombe)<-c("x1","y1","x2","y2","x3","y3","x4","y4")
> reg1<-lm(y1~1 + x1, data=anscombe)
> reg2<-lm(y2~1 + x2, data=anscombe)
> reg3<-lm(y3~1 + x3, data=anscombe)
> reg4<-lm(y4~1 + x4, data=anscombe)
> summary(reg1)
> summary(reg2)
> summary(reg3)
> summary(reg4)
> par(mfrow=c(2,2))
> plot(x1,y1)
> abline(reg1)
> plot(x2,y2)
> abline(reg2)
> plot(x3,y3)
> abline(reg3)
> plot(x4,y4)
> abline(reg4)
>
> ..........................................................................
> fname<-file.choose()
> tab6.1<-ts(read.table(fname, header=TRUE),frequency=12,start=c(1946,1))
> month<-cycle(tab6.1)
> year<-floor(time(tab6.1))
> dat1<-ts.intersect(year,month,tab6.1)
> dat2<-window(dat1,start=c(1946,1),end=c(1993,12))
> reg1<-lm(tab6.1~1+factor(month),data=dat2, na.action=NULL)
> summary(reg1)
> hstarts<-dat2[,3]
> plot1<-ts.intersect(hstarts,reg1$fitted.value,reg1$resid)
> plot.ts(plot1[,1])
> lines(plot1[,2], col="red")
> plot.ts(plot[,3], ylab="Residuals")
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Brendan.Power at dpi.qld.gov.au  Thu Dec  6 05:47:06 2007
From: Brendan.Power at dpi.qld.gov.au (Power, Brendan D (Toowoomba))
Date: Thu, 6 Dec 2007 14:47:06 +1000
Subject: [R] Segmented regression
Message-ID: <200712060447.lB64l7I6005602@dpi-gw1.dpi.qld.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/041543a1/attachment.pl 

From ashoka.polpitiya at gmail.com  Thu Dec  6 06:27:42 2007
From: ashoka.polpitiya at gmail.com (Ashoka Polpitiya)
Date: Wed, 5 Dec 2007 21:27:42 -0800
Subject: [R] hclust in heatmap.2
In-Reply-To: <5032046e0712051620l4ea3e458o7807d53a039cac16@mail.gmail.com>
References: <5032046e0712051620l4ea3e458o7807d53a039cac16@mail.gmail.com>
Message-ID: <fb74d7d50712052127p2dfadfe7h5fde8292d9218b75@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071205/cc0a5311/attachment.pl 

From leffgh at 163.com  Thu Dec  6 07:33:47 2007
From: leffgh at 163.com (Bin Yue)
Date: Wed, 5 Dec 2007 22:33:47 -0800 (PST)
Subject: [R] logistic regression using "glm",which "y" is set to be "1"
In-Reply-To: <cdf817830712051954y717e778dn4d7d4faa99b36f61@mail.gmail.com>
References: <14185060.post@talk.nabble.com>
	<1196909273.2962.12.camel@Bellerophon.localdomain>
	<14185819.post@talk.nabble.com>
	<cdf817830712051954y717e778dn4d7d4faa99b36f61@mail.gmail.com>
Message-ID: <14187112.post@talk.nabble.com>


 Dear all:
     By comparing glmresult$y and model.response(model.frame(glmresult)),  I
have found out which one is 
set to be "TRUE" and which "FALSE".But it seems that to fit a logistic
regression , logit (or logistic) transformation has to be done before
regression.
     Does anybody know how to obtain the transformation result ? It is hard
to settle down before knowing the actual process R works . I have read some
books and the "?glm" help file , but what they told me was not sufficient.
   Best wishes ,
 Bin Yue


Weiwei Shi wrote:
> 
> Dear Bin:
> you type
> ?glm
> in R console and you will find the Detail section of help file for glm
> 
> i pasted it for you too
> 
> Details
> 
> A typical predictor has the form response ~ terms where response is the
> (numeric) response vector and terms is a series of terms which specifies a
> linear predictor for response. For binomialand quasibinomial families the
> response can also be specified as a
> factor<file:///Library/Frameworks/R.framework/Versions/2.6/Resources/library/base/html/factor.html>
> (when
> the first level denotes failure and all others success) or as a two-column
> matrix with the columns giving the numbers of successes and failures. A
> terms specification of the form first + second indicates all the terms in
> first together with all the terms in second with duplicates removed. The
> terms in the formula will be re-ordered so that main effects come first,
> followed by the interactions, all second-order, all third-order and so on:
> to avoid this pass a terms object as the formula.
> 
> A specification of the form first:second indicates the the set of terms
> obtained by taking the interactions of all terms in first with all terms
> in
> second. The specification first*second indicates the *cross* of first and
> second. This is the same as first + second + first:second.
> 
> glm.fit is the workhorse function.
> 
> If more than one of etastart, start and mustart is specified, the first in
> the list will be used. It is often advisable to supply starting values for
> a
> quasi<file:///Library/Frameworks/R.framework/Versions/2.6/Resources/library/stats/html/family.html>
> family,
> and also for families with unusual links such as gaussian("log").
> 
> All of weights, subset, offset, etastart and mustart are evaluated in the
> same way as variables in formula, that is first in data and then in the
> environment of formula.
> 
> 
> 
> On Dec 5, 2007 10:41 PM, Bin Yue <leffgh at 163.com> wrote:
> 
>>
>> Dear Marc Schwartz:
>>  When I ask R2.6.0 for windows, the information it gives does not contain
>> much about family=binomial .
>>  You said that there is a detail section of "?glm". I want to read it
>> thoroughly. Could  you tell me where and how I can find the detail
>> section
>> of "?glm".
>>   Thank you very much .
>>   Best regards,
>>  Bin Yue
>>
>>
>>
>> Marc Schwartz wrote:
>> >
>> >
>> > On Wed, 2007-12-05 at 18:06 -0800, Bin Yue wrote:
>> >> Dear friends :
>> >>     using the "glm" function and setting family=binomial, I got a list
>> of
>> >> coefficients.
>> >> The coefficients reflect the effects  of predicted variables on the
>> >> probability of the response to be "1".
>> >> My response variable consists of  "A" and "D" . I don't know which
>> level
>> >> of
>> >> the response was set to be 1.
>> >> is the first element of the response set to be 1?
>> >>    Thank all in advance.
>> >>    Regards,
>> >>
>> >> -----
>> >> Best regards,
>> >> Bin Yue
>> >
>> >
>> > As per the Details section of ?glm:
>> >
>> > For binomial and quasibinomial families the response can also be
>> > specified as a factor (when the first level denotes failure and all
>> > others success) ...
>> >
>> >
>> > So use:
>> >
>> >   levels(response.variable)
>> >
>> > and that will give you the factor levels, where the first level is 0
>> and
>> > the second level is 1.
>> >
>> > If you work in a typical English based locale with default alpha based
>> > level ordering, it will likely be A (Alive?) is 0 and D (Dead?) is 1.
>> >
>> > HTH,
>> >
>> > Marc Schwartz
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>>
>>
>> -----
>> Best regards,
>> Bin Yue
>>
>> *************
>> student for a Master program in South Botanical Garden , CAS
>>
>> --
>> View this message in context:
>> http://www.nabble.com/logistic-regression-using-%22glm%22%2Cwhich-%22y%22-is-set-to-be-%221%22-tf4953617.html#a14185819
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> 
> -- 
> Weiwei Shi, Ph.D
> Research Scientist
> GeneGO, Inc.
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 


-----
Best regards,
Bin Yue

*************
student for a Master program in South Botanical Garden , CAS

-- 
View this message in context: http://www.nabble.com/logistic-regression-using-%22glm%22%2Cwhich-%22y%22-is-set-to-be-%221%22-tf4953617.html#a14187112
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Thu Dec  6 08:07:09 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 6 Dec 2007 07:07:09 +0000 (GMT)
Subject: [R] Which Linux OS on Athlon amd64, to comfortably run R?
In-Reply-To: <fj7cci$ufe$1@ger.gmane.org>
References: <4756C0E2.1060902@unifi.it>
	<18262.54350.143549.665409@localhost.localdomain>
	<Pine.LNX.4.64.0712051655180.12631@gannet.stats.ox.ac.uk>
	<fj7cci$ufe$1@ger.gmane.org>
Message-ID: <Pine.LNX.4.64.0712060623340.24848@gannet.stats.ox.ac.uk>

On Thu, 6 Dec 2007, Emmanuel Charpentier wrote:

> Prof Brian Ripley a ?crit :
>> Note that Ottorino has only 1GB of RAM installed, which makes a 64-bit
>> version of R somewhat moot.  See chapter 8 of
>>
>> http://cran.r-project.org/doc/manuals/R-admin.html
>
> Thank you for this reminder|tip ! I didn't re-read this document since
> ... oh, my  ... very early (1.x ?) versions. At which time my favorite
> peeve against R was the fixed memory allocation scheme.
>
> I would have thought that 64 bits machines could take advantage of a
> wider bus and (marginally ?) faster instructions to balance larger
> pointers. Am I mistaken ?

Yes, it is more complex than that.  If you run 32-bit instructions on a 
x86_64, the physical bus is the same as when you run 64-bit instructions. 
The larger code usually means the CPU caches spill more often, and some 
64-bit chips have more 32-bit than 64-bit registers which allows better 
scheduling.

The R-admin manual reports on some empirical testing.  But when you have 
limited RAM the larger code and data for a 64-bit build will cause more 
swapping and that is likely to dominate performance issues on large 
problems.

Note that the comparisons depend on both the chip and the OS: it seems 
that on Mac OS 10.5 on a Core 2 Duo the 64-bit version is faster (on small 
examples).  The original enquiry was about 'amd64 linux', but I've checked 
Intel Core 2 Duo as well: on my box 64-bit builds are faster than 32-bit 
ones, whereas the reverse is true for Opterons.  So it seems that the 
architectural differences of Core 2 Duo vs AMD64 do affect the issue.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From dieter.menne at menne-biomed.de  Thu Dec  6 08:17:38 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 6 Dec 2007 07:17:38 +0000 (UTC)
Subject: [R] Using expression in Hmisc Key()
References: <55B194DA-5A84-4BCD-8869-C55D47FDB695@virginia.edu>
Message-ID: <loom.20071206T071522-217@post.gmane.org>

Michael Kubovy <kubovy <at> virginia.edu> writes:

> 
> Dear r-helpers,
> 
> How do I tell xYplot() and Key() that I want the labels in italic?
> 
....
> Key(x = 0.667, y = 0.833, other = list(title = expression(italic(v)),  
> cex.title = 1,
> 	labels = c(expression(italic(b)), expression(italic(c)),  
> expression(italic(d)))))
> dev.off()

Michael, 

I have submit a similar case last week to the Bug tracker. Maybe you can raise
that "enhancement request" to "defect"

http://biostat.mc.vanderbilt.edu/trac/Hmisc/ticket/21

Dieter

-----------
The Key function generated by some plot commands should have a ... parameter.
Otherwise, the ... in rlegend is useless, and it would be nice to be able to
suppress the box, for example.

Key = function (x = NULL, y = NULL, lev = c("No Fail", "Fail"), pch = c(16, 1))
{ .. part omitted

    rlegend(x, y, legend = lev, pch = pch, ...) invisible()

}


From fasidfas at yahoo.com  Thu Dec  6 09:00:29 2007
From: fasidfas at yahoo.com (faisal afzal siddiqui)
Date: Thu, 6 Dec 2007 00:00:29 -0800 (PST)
Subject: [R] Conjoint Analysis in R??
Message-ID: <895090.36783.qm@web53305.mail.re2.yahoo.com>

Pls advise how I can use R in conjoint analysis??

regds
Faisal Afzal Siddiqui
Karachi, Pakistan


      ____________________________________________________________________________________
Looking for last minute shopping deals?


From ritz at life.ku.dk  Thu Dec  6 09:00:43 2007
From: ritz at life.ku.dk (Christian Ritz)
Date: Thu, 06 Dec 2007 09:00:43 +0100
Subject: [R] how to interpolate a plot with a logistic curve
In-Reply-To: <E2978D5F-BED0-44A7-9196-E5A9E074BB96@gmail.com>
References: <E2978D5F-BED0-44A7-9196-E5A9E074BB96@gmail.com>
Message-ID: <4757AC2B.1020609@life.ku.dk>

Dear Simone,

you can use the package 'drc' to fit a logistic model, that is a non-linear regression 
model based on the equation c+(d-c)/(1+exp(b(x-e))), to your data (below named 'simone'):


## Fitting a 4-parameter logistic model (also called Boltzmann model)
simone.m1 <- drm(size~x, data=simone, fct=B.4())

## Plotting the data together with the fitted curve
plot(simone.m1, log="")


The fit is not great due to the bend.


Christian


From oezbek at inf.fu-berlin.de  Thu Dec  6 09:04:09 2007
From: oezbek at inf.fu-berlin.de (Christopher Oezbek)
Date: Thu, 06 Dec 2007 09:04:09 +0100
Subject: [R] Using panel.densityplot with stripplot
Message-ID: <op.t2wl071ymknj4h@thimphu.pcpool.mi.fu-berlin.de>

Hi Lattice-Experts/Hi Deepayan,
   I have been searching the archives for an answer to this, but am finally  
giving up:

I am plotting stripplots above each other using

stripplot(type ~ date, data = email)

which looks exactely as I want (type is a factor with 8 levels). I  
addition I would now like to display for each stripplot a density curve.

 From examples I would have thought I would need to do:

stripplot(type ~ date2, data = email,
           panel = function(x, y, ...) {
             panel.stripplot(x, y, ...)
             panel.densityplot(x, y, ...)
           })

This works as far as the stripplot part is concerned. But

a.) panel.densityplot does not take a y argument.

b.) even if I drop y, no density curve shows up (I guess I am missing  
correct dargs here)

I have been toying with

- using panel.superpose but guess that this is not the right approach,  
since I do not have a grouping variable (and I have not succeeded in  
letting "~ date2 | type" and "~date, groups=email$type" look the way I  
want).

- using densityplot instead of stripplot, but then panel.stripplot  
complains that argument y is missing. :-(

Many thanks for any advice/ideas!
   Christopher

-- 
Christopher Oezbek
Arbeitsgruppe Software Engineering
Institut f?r Informatik
Freie Universit?t Berlin
Takustr. 9, 14195 Berlin, Germany
+49 30 838 75242, Raum 008
http://www.inf.fu-berlin.de/~oezbek/


From deepayan.sarkar at gmail.com  Thu Dec  6 09:20:20 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 6 Dec 2007 00:20:20 -0800
Subject: [R] Using panel.densityplot with stripplot
In-Reply-To: <op.t2wl071ymknj4h@thimphu.pcpool.mi.fu-berlin.de>
References: <op.t2wl071ymknj4h@thimphu.pcpool.mi.fu-berlin.de>
Message-ID: <eb555e660712060020n169ebd6fmf50f6356bf23d385@mail.gmail.com>

On 12/6/07, Christopher Oezbek <oezbek at inf.fu-berlin.de> wrote:
> Hi Lattice-Experts/Hi Deepayan,
>    I have been searching the archives for an answer to this, but am finally
> giving up:
>
> I am plotting stripplots above each other using
>
> stripplot(type ~ date, data = email)
>
> which looks exactely as I want (type is a factor with 8 levels). I
> addition I would now like to display for each stripplot a density curve.
>
>  From examples I would have thought I would need to do:
>
> stripplot(type ~ date2, data = email,
>            panel = function(x, y, ...) {
>              panel.stripplot(x, y, ...)
>              panel.densityplot(x, y, ...)
>            })

What examples? panel.stripplot and panel.densityplot have entirely
different expectations about what the y-range of the panel is going to
be, and they cannot be mixed.

Sounds like you want something like panel.violin (?panel.violin has an example).

-Deepayan


From vistocco at unicas.it  Thu Dec  6 09:56:15 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Thu, 06 Dec 2007 09:56:15 +0100
Subject: [R] correlation coefficient from qq plot
In-Reply-To: <EDE5250EC60782409CCD2A210403E7D128CDBE@mail04.TNC.ORG>
References: <EDE5250EC60782409CCD2A210403E7D128CDBE@mail04.TNC.ORG>
Message-ID: <4757B92F.7000308@unicas.it>

You could use the qqnorm function to obtain the correlation, as:

 > qqp=qqnorm(rstudent(regrname))
 > cor(qqp$x,qqp$y)

If you do not want see the plot (as the qq.plot is richer):
 > qqp=qqnorm(rstudent(regrname), plot.it=F)

domenico vistocco

Tom Fitzhugh wrote:
> Hi,
>  
> I am trying to figure out how to get the correlation coefficient for a
> QQ plot (residual plot).  So to be more precise, I am creating the plot
> like this:
>  
> qq.plot(rstudent(regrname), main = rformula, col=1) 
>  
> But want to also access (or compute) the correlation coefficient for
> that plot.
>  
> Thanks,  
>  
> Tom
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From johannes_graumann at web.de  Thu Dec  6 09:54:23 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 06 Dec 2007 09:54:23 +0100
Subject: [R] R CMD Build feature searches or requests
Message-ID: <fj8dbj$950$1@ger.gmane.org>

Hello,

I'm missing two features in "R CMD build":
1) Easy building of Windows/zip packaged package version alongside the
*nix-style *.tar.gz.
Right now I'm doing a scripted version of 
        R CMD build <PACKAGE>
        R CMD INSTALL <PACKAGE>
        mkdir tmp
        cp -r /usr/local/lib/R/site-library/<PACKAGE> tmp/<PACKAGE>
        cd tmp
        zip -r <PACKAGE>_<EXTRACTEDVERSION>.zip <PACKAGE> 
        mv *.zip ..
        cd ..
        rm -rf tmp
I was wondering whether it wouldn't be helpfull to others maintaining
packages not requiring genuine cross-compilation (only containing R code)
to deal with this via an option to "R CMD build". Something
like '-zip-package' might do it ...
2) My scripted solution right now also automatically increments version
numbers and adjusts dates in <PACKAGE>/man/<PACKAGE>-package.Rd and
<PACKAGE>/DESCRIPTION, ensuring progressing and continuous package naming.
Would be nice to have an "R CMD build"-option to take care of that too ...

Please let me know what you think or where to find the functionality in case
I overlooked it.

Thanks, Joh


From kubovy at virginia.edu  Thu Dec  6 10:12:27 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 6 Dec 2007 04:12:27 -0500
Subject: [R] Using expression in Hmisc Key()
In-Reply-To: <loom.20071206T071522-217@post.gmane.org>
References: <55B194DA-5A84-4BCD-8869-C55D47FDB695@virginia.edu>
	<loom.20071206T071522-217@post.gmane.org>
Message-ID: <184E1205-7AAD-4CEF-A0FC-083B6D6E7BCC@virginia.edu>

Hi Dieter,

I actually solved *my* problem:

Key(x = 0.667, y = 0.833, lev = c(expression(italic(b)),  
expression(italic(c)), expression(italic(d))),
	other = list(title = expression(italic(v)), cex.title = 1))

I was able to figure this out only by looking at the code for Key(),  
because I hadn't come across the 'lev' argument in the documentation  
(in fact Key does not appear in the list of functions of Hmisc; I  
discovered its usefulness by consulting the examples for xYplot()). I  
would have expected this argument to be called 'labels' (see  
help(labels)), (or at least 'levels'):

function (x = NULL, y = NULL, lev = c("b", "c", "d"), cex = c(0.7,  
0.7, 0.7), col = c(2, 4, 9), font = c(1, 1, 1), pch = c(1, 2, 16),  
other = NULL)
{
?
}
?

Note that the NULL setting of the parameter may be useful for  
programming, but it is not useful to anyone reading the code. How is  
one to proceed after having read help(NULL)?

At the same time, I have no grounds for complaining about the  
documentation for Key(): Frank Harrell is giving away his work. His  
examples are extensive, and his functions always work as advertised.  
Sometimes they also work as not advertised.

This is one of the cases where I wish there were a function that  
"exercised" all the arguments of a graphics function by visualizing  
the effect of changing  two or three levels of each argument (one by  
one, of course). This might have the side effect of allowing authors  
to shorten documentation. I have in mind adding a method to each  
graphics function, and having the author specify the two or three  
instructive levels of each argument. See the exhaustive exploration by  
J.R. Lobry, A.B. Dufour & D. Chessel of the graphical parameters  
accessible by par(), at
http://pbil.univ-lyon1.fr/R/fichestd/tdr75.pdf

On Dec 6, 2007, at 2:17 AM, Dieter Menne wrote:

> Michael Kubovy <kubovy <at> virginia.edu> writes:
>
>>
>> Dear r-helpers,
>>
>> How do I tell xYplot() and Key() that I want the labels in italic?
>>
> ....
>> Key(x = 0.667, y = 0.833, other = list(title = expression(italic(v)),
>> cex.title = 1,
>> 	labels = c(expression(italic(b)), expression(italic(c)),
>> expression(italic(d)))))
>> dev.off()
>
> Michael,
>
> I have submit a similar case last week to the Bug tracker. Maybe you  
> can raise
> that "enhancement request" to "defect"
>
> http://biostat.mc.vanderbilt.edu/trac/Hmisc/ticket/21
>
> Dieter
>
> -----------
> The Key function generated by some plot commands should have a ...  
> parameter.
> Otherwise, the ... in rlegend is useless, and it would be nice to be  
> able to
> suppress the box, for example.
>
> Key = function (x = NULL, y = NULL, lev = c("No Fail", "Fail"), pch  
> = c(16, 1))
> { .. part omitted
>
>    rlegend(x, y, legend = lev, pch = pch, ...) invisible()
>
> }

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From oezbek at inf.fu-berlin.de  Thu Dec  6 10:15:17 2007
From: oezbek at inf.fu-berlin.de (Christopher Oezbek)
Date: Thu, 06 Dec 2007 10:15:17 +0100
Subject: [R] Using panel.densityplot with stripplot
In-Reply-To: <eb555e660712060020n169ebd6fmf50f6356bf23d385@mail.gmail.com>
References: <op.t2wl071ymknj4h@thimphu.pcpool.mi.fu-berlin.de>
	<eb555e660712060020n169ebd6fmf50f6356bf23d385@mail.gmail.com>
Message-ID: <op.t2wpbrwhmknj4h@thimphu.pcpool.mi.fu-berlin.de>

Hi Deepayan!
   thank you! panel.violin with the following modification works as I want:

grid.polyline(x = dx.list[[i]], y = dy.list[[i]], ...

instead of

grid.polygon(x = c(dx.list[[i]], rev(dx.list[[i]])),
                 y = c(dy.list[[i]], -rev(dy.list[[i]])), ...

>>  From examples I would have thought I would need to do...
>
> What examples?

The examples for writing your own panel functions. I was not aware that  
there were different expectations regarding arguments.

> panel.stripplot and panel.densityplot have entirely
> different expectations about what the y-range of the panel is going to
> be, and they cannot be mixed.

But is there a technical reason for making this distinction? It seemed to  
me that most panel functions can deal with x and y parameters and that  
panel.densityplot could make use of the same mechanisms as panel.violin to  
subdivide based on factor y.

Again many thanks for the fast response,
   Christopher

-- 
Christopher Oezbek
Arbeitsgruppe Software Engineering
Institut f?r Informatik
Freie Universit?t Berlin
Takustr. 9, 14195 Berlin, Germany
+49 30 838 75242, Raum 008
http://www.inf.fu-berlin.de/~oezbek/


From petr.pikal at precheza.cz  Thu Dec  6 10:52:18 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Thu, 6 Dec 2007 10:52:18 +0100
Subject: [R] colour coded points in biplot
Message-ID: <OFC179B06D.1019233A-ONC12573A9.00331B1B-C12573A9.00363A6A@precheza.cz>

Dear all

I tried to make a biplot with color coded labels but I was not successful. 
Searching archives I found that it is probably not so simple.
I found
http://tolstoy.newcastle.edu.au/R/help/05/01/10661.html

which recommends eqscplot

eqscplot(fit$score[,1], fit$score[,2], pch = 20, col=c(rep(2,10), 
rep(3,40)), cex=2)

http://tolstoy.newcastle.edu.au/R/help/05/01/10661.html

or I can use plot itself, but then I was not able to find how to add 
arrows.

The closest I get is

fit<-(princomp(USArrests, cor = TRUE))
biplot(fit , xlabs=rep("", 50))
points(fit$score[,1], fit$score[,2], pch = 20, col=c(rep(2,10), 
rep(3,40)), cex=2)

and use some suitable scaling before calling points. 

My question is if there is some other easier approach to get color coded 
points in biplot?

Petr Pikal
petr.pikal at precheza.cz


From vmuggeo at dssm.unipa.it  Thu Dec  6 10:54:36 2007
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Thu, 06 Dec 2007 10:54:36 +0100
Subject: [R] Segmented regression
In-Reply-To: <200712060447.lB64l7I6005602@dpi-gw1.dpi.qld.gov.au>
References: <200712060447.lB64l7I6005602@dpi-gw1.dpi.qld.gov.au>
Message-ID: <4757C6DC.1090202@dssm.unipa.it>

Dear Brendan,
I am not sure to understand your code..

It seems to me that your are interested in fitting a one-breakpoint 
segmented relationship in each level of your grouping variable

If this is the case, the correct code is below.
In order to fit a segmented relationship in each group you have to 
define the relevant variable before fitting, and to constrain the last 
slope to be zero you have to consider the `minus' variable..(I discuss 
these points in the submitted Rnews article..If you are interested, let 
me know off list).

If my code does not fix your problem, please let me know,

Best,
vito

##--define the group-specific segmented variable:
X<-model.matrix(~0+factor(group),data=df)*df$tt
df$tt.KV<-X[,1] #KV
df$tt.KW<-X[,2]   #KW
df$tt.WC<-X[,3]   #WC

##-fit the unconstrained model
olm<-lm(y~group+tt.KV+tt.KW+tt.WC,data=df)
os<-segmented(olm,seg.Z=~tt.KV+tt.KW+tt.WC,psi=list(tt.KV=350, 
tt.KW=500, tt.WC=350))
#have a look to results:
with(df,plot(tt,y))
with(subset(df,group=="RKW"),points(tt,y,col=2))
with(subset(df,group=="RKV"),points(tt,y,col=3))
points(df$tt[df$group=="RWC"],fitted(os)[df$group=="RWC"],pch=20)
points(df$tt[df$group=="RKW"],fitted(os)[df$group=="RKW"],pch=20,col=2)
points(df$tt[df$group=="RKV"],fitted(os)[df$group=="RKV"],pch=20,col=3)


#constrain the last slope in group KW
tt.KW.minus<- -df$tt.KW
olm1<-lm(y~group+tt.KV+tt.WC,data=df)
os1<-segmented(olm1,seg.Z=~tt.KV+tt.KW.minus+tt.WC,psi=list(tt.KV=350, 
tt.KW.minus=-500, tt.WC=350))
#check..:-)
slope(os1)

with(df,plot(tt,y))
with(subset(df,group=="RKW"),points(tt,y,col=2))
with(subset(df,group=="RKV"),points(tt,y,col=3))
points(df$tt[df$group=="RWC"],fitted(os1)[df$group=="RWC"],pch=20)
points(df$tt[df$group=="RKW"],fitted(os1)[df$group=="RKW"],pch=20,col=2)
points(df$tt[df$group=="RKV"],fitted(os1)[df$group=="RKV"],pch=20,col=3)






Power, Brendan D (Toowoomba) ha scritto:
> Hello all,
> 
> I have 3 time series (tt) that I've fitted segmented regression models
> to, with 3 breakpoints that are common to all, using code below
> (requires segmented package). However I wish to specifiy a zero
> coefficient, a priori, for the last segment of the KW series (green)
> only. Is this possible to do with segmented? If not, could someone point
> in a direction?
> 
> The final goal is to compare breakpoint sets for differences from those
> derived from other data.
> 
> Thanks in advance,
> 
> Brendan.
> 
> 
> library(segmented)
> df<-data.frame(y=c(0.12,0.12,0.11,0.19,0.27,0.28,0.35,0.38,0.46,0.51,0.5
> 8,0.59,0.60,0.57,0.64,0.68,0.72,0.73,0.78,0.84,0.85,0.83,0.86,0.88,0.88,
> 0.95,0.95,0.93,0.92,0.97,0.86,1.00,0.85,0.97,0.90,1.02,0.95,0.54,0.53,0.
> 50,0.60,0.70,0.74,0.78,0.82,0.88,0.83,1.00,0.85,0.96,0.84,0.86,0.82,0.86
> ,0.84,0.84,0.84,0.77,0.69,0.61,0.67,0.73,0.65,0.55,0.58,0.56,0.60,0.50,0
> .50,0.42,0.43,0.44,0.42,0.40,0.51,0.60,0.63,0.71,0.74,0.82,0.82,0.85,0.8
> 9,0.91,0.87,0.91,0.93,0.95,0.95,0.97,1.00,0.96,0.90,0.86,0.91,0.94,0.96,
> 0.88,0.88,0.88,0.92,0.82,0.85),
>  
> tt=c(141.6,141.6,141.6,183.2,212.8,227.0,242.4,271.5,297.4,312.3,331.4,3
> 42.4,346.3,356.6,371.6,408.8,408.8,419.5,434.4,464.5,492.6,521.7,550.5,5
> 50.3,565.4,588.0,602.9,623.7,639.6,647.9,672.6,680.6,709.7,709.7,750.2,7
> 50.2,750.2,141.6,141.6,141.6,183.2,212.8,227.0,242.4,271.5,297.4,312.3,3
> 31.4,342.4,346.3,356.6,371.6,408.8,408.8,419.5,434.4,464.5,492.6,521.7,5
> 50.5,550.3,565.4,588.0,602.9,623.7,639.6,647.9,672.6,680.6,709.7,709.7,1
> 41.6,141.6,141.6,183.2,212.8,227.0,242.4,271.5,297.4,312.3,331.4,342.4,3
> 46.3,356.6,371.6,408.8,408.8,419.5,434.4,464.5,492.6,521.7,550.5,550.3,5
> 65.4,588.0,602.9,623.7,639.6,647.9,672.6,709.7),
>            group=c(rep("RKW",37),rep("RWC",34),rep("RKV",32)))
> init.bp <- c(297.4,639.6,680.6)
> lm.1 <- lm(y~tt+group,data=df)
> seg.1 <- segmented(lm.1, seg.Z=~tt, psi=list(tt=init.bp))
> 
>>  version
>                _                           
> platform       i386-pc-mingw32             
> arch           i386                        
> os             mingw32                     
> system         i386, mingw32               
> status                                     
> major          2                           
> minor          6.0                         
> year           2007                        
> month          10                          
> day            03                          
> svn rev        43063                       
> language       R                           
> version.string R version 2.6.0 (2007-10-03)
> 
> 
> 
> ********************************DISCLAIMER**************...{{dropped:15}}
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612


From amitpatel_ak at yahoo.co.uk  Thu Dec  6 11:00:15 2007
From: amitpatel_ak at yahoo.co.uk (Amit Patel)
Date: Thu, 6 Dec 2007 10:00:15 +0000 (GMT)
Subject: [R] Dealing with NA's in a data matrix
Message-ID: <373051.50133.qm@web25014.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/64f6120e/attachment.pl 

From johannes_graumann at web.de  Thu Dec  6 11:18:24 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 06 Dec 2007 11:18:24 +0100
Subject: [R] Building package - tab delimited example data issue
Message-ID: <fj8i94$q54$1@ger.gmane.org>

Hello,

I'm trying to integrate example data in the shape of a tab delimited ASCII
file into my package and therefore dropped it into the data subdirectory.
The build works out just fine, but when I attempt to install I get:

** building package indices ...
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines,
na.strings,  :
  line 1 did not have 500 elements
Calls: <Anonymous> ... <Anonymous> -> switch -> assign -> read.table -> scan
Execution halted
ERROR: installing package indices failed
** Removing '/usr/local/lib/R/site-library/MaxQuantUtils'
** Restoring previous '/usr/local/lib/R/site-library/MaxQuantUtils'

Accordingly the check delivers:

...
* checking whether package 'MaxQuantUtils' can be installed ... ERROR

Can anyone tell me what I'm doing wrong? build/install witout the ASCII file
works just fine.

Joh


From vistocco at unicas.it  Thu Dec  6 11:23:02 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Thu, 06 Dec 2007 11:23:02 +0100
Subject: [R] Conjoint Analysis in R??
In-Reply-To: <895090.36783.qm@web53305.mail.re2.yahoo.com>
References: <895090.36783.qm@web53305.mail.re2.yahoo.com>
Message-ID: <4757CD86.3020309@unicas.it>

http://tolstoy.newcastle.edu.au/R/help/05/06/6104.html
http://tolstoy.newcastle.edu.au/R/help/05/06/6103.html

domenico

faisal afzal siddiqui wrote:
> Pls advise how I can use R in conjoint analysis??
>
> regds
> Faisal Afzal Siddiqui
> Karachi, Pakistan
>
>
>       ____________________________________________________________________________________
> Looking for last minute shopping deals?
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From paulandpen at optusnet.com.au  Thu Dec  6 11:32:12 2007
From: paulandpen at optusnet.com.au (paulandpen)
Date: Thu, 6 Dec 2007 20:32:12 +1000
Subject: [R] Conjoint Analysis in R??
References: <895090.36783.qm@web53305.mail.re2.yahoo.com>
Message-ID: <007a01c837f3$43de93a0$8255eddc@superpaul>

Faisal,

can you elaborate further on your conjoint design....

there is bayesm which offers a hierarchical bayes approach to analysing 
choice data

MLogit available through zelig (see below)

http://gking.harvard.edu/zelig/docs/index.html

MNP as a standalone package for the probit model

thanks Paul


----- Original Message ----- 
From: "faisal afzal siddiqui" <fasidfas at yahoo.com>
To: "R Help" <r-help at stat.math.ethz.ch>
Sent: Thursday, December 06, 2007 6:00 PM
Subject: [R] Conjoint Analysis in R??


> Pls advise how I can use R in conjoint analysis??
>
> regds
> Faisal Afzal Siddiqui
> Karachi, Pakistan
>
>
> 
> ____________________________________________________________________________________
> Looking for last minute shopping deals?
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From baron at psych.upenn.edu  Thu Dec  6 11:47:11 2007
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 6 Dec 2007 05:47:11 -0500
Subject: [R] Conjoint Analysis in R??
In-Reply-To: <4757CD86.3020309@unicas.it>
References: <895090.36783.qm@web53305.mail.re2.yahoo.com>
	<4757CD86.3020309@unicas.it>
Message-ID: <20071206104711.GA631@psych.upenn.edu>

Another possibility is ace {acepack}.  See 

Gurmankin Levy, A., & Baron, J. (2005). How bad is a 10% chance of
losing a toe? Judgments of probabilistic conditions by doctors and
laypeople. Memory and Cognition, 33, 1399-1406.

for a published example.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From P.Dalgaard at biostat.ku.dk  Thu Dec  6 11:52:46 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 06 Dec 2007 11:52:46 +0100
Subject: [R] Building package - tab delimited example data issue
In-Reply-To: <fj8i94$q54$1@ger.gmane.org>
References: <fj8i94$q54$1@ger.gmane.org>
Message-ID: <4757D47E.8060505@biostat.ku.dk>

Johannes Graumann wrote:
> Hello,
>
> I'm trying to integrate example data in the shape of a tab delimited ASCII
> file into my package and therefore dropped it into the data subdirectory.
> The build works out just fine, but when I attempt to install I get:
>
> ** building package indices ...
> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines,
> na.strings,  :
>   line 1 did not have 500 elements
> Calls: <Anonymous> ... <Anonymous> -> switch -> assign -> read.table -> scan
> Execution halted
> ERROR: installing package indices failed
> ** Removing '/usr/local/lib/R/site-library/MaxQuantUtils'
> ** Restoring previous '/usr/local/lib/R/site-library/MaxQuantUtils'
>
> Accordingly the check delivers:
>
> ...
> * checking whether package 'MaxQuantUtils' can be installed ... ERROR
>
> Can anyone tell me what I'm doing wrong? build/install witout the ASCII file
> works just fine.
>
> Joh
>
>   
If you had looked at help(data), you would have found a list of which
file formats it supports and how they are read. Hint: TAB-delimited
files are not among them. *Whitespace* separated files work, using
read.table(filename, header=TRUE), but that is not a superset of
TAB-delimited data if there are empty fields.

A nice trick is to figure out how to read the data from the command line
and drop the relevant code into a mydata.R file (assuming that the
actual data file is mydata.txt). This gets executed when the data is
loaded (by data(mydata) or when building the lazyload database) because
.R files have priority over .txt.

This is quite general and allows a nice way of incorporating data
management while retaining the original data source:

>more ISwR/data/stroke.R

stroke <-  read.csv2("stroke.csv", na.strings=".")
names(stroke) <- tolower(names(stroke))
stroke <-  within(stroke,{
    sex <- factor(sex,levels=0:1,labels=c("Female","Male"))
    dgn <- factor(dgn)
    coma <- factor(coma, levels=0:1, labels=c("No","Yes"))
    minf <- factor(minf, levels=0:1, labels=c("No","Yes"))
    diab <- factor(diab, levels=0:1, labels=c("No","Yes"))
    han <- factor(han, levels=0:1, labels=c("No","Yes"))
    died <- as.Date(died, format="%d.%m.%Y")
    dstr <- as.Date(dstr,format="%d.%m.%Y")
    dead <- !is.na(died) & died < as.Date("1996-01-01")
    died[!dead] <- NA
})

>head ISwR/data/stroke.csv

SEX;DIED;DSTR;AGE;DGN;COMA;DIAB;MINF;HAN
1;7.01.1991;2.01.1991;76;INF;0;0;1;0
1;.;3.01.1991;58;INF;0;0;0;0
1;2.06.1991;8.01.1991;74;INF;0;0;1;1
0;13.01.1991;11.01.1991;77;ICH;0;1;0;1
0;23.01.1996;13.01.1991;76;INF;0;1;0;1
1;13.01.1991;13.01.1991;48;ICH;1;0;0;1
0;1.12.1993;14.01.1991;81;INF;0;0;0;1
1;12.12.1991;14.01.1991;53;INF;0;0;1;1
0;.;15.01.1991;73;ID;0;0;0;1



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From maechler at stat.math.ethz.ch  Thu Dec  6 12:11:46 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 6 Dec 2007 12:11:46 +0100
Subject: [R] R function for percentrank
In-Reply-To: <1196880230.2945.26.camel@Bellerophon.localdomain>
References: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
	<243517.68093.qm@web32807.mail.mud.yahoo.com>
	<65cc7bdf0712010951p451a993i70da89f285d801de@mail.gmail.com>
	<Xns99F989B3A3057dNOTwinscomcast@80.91.229.13>
	<Xns99F98B11F90B6dNOTwinscomcast@80.91.229.13>
	<1196537601.2980.15.camel@Bellerophon.localdomain>
	<18262.58128.974202.67709@stat.math.ethz.ch>
	<1196880230.2945.26.camel@Bellerophon.localdomain>
Message-ID: <18263.55538.53978.535345@stat.math.ethz.ch>

>>>>> "MS" == Marc Schwartz <marc_schwartz at comcast.net>
>>>>>     on Wed, 05 Dec 2007 12:43:50 -0600 writes:

[............]

    MS> Martin,

    MS> Thanks for the corrections. In hindsight, now seeing the intended use of
    MS> ecdf() in the fashion you describe above, it is now clear that my
    MS> approach in response to David's query was un-needed and "over the top".
    MS> "Yuck" is quite appropriate... :-)

    MS> As I was going through this "exercise", it did seem overly complicated,
    MS> given R's usual elegant philosophy about such things. I suppose if I had
    MS> looked at the source for plot.stepfun(), it would have been more evident
    MS> as to how the y values are acquired.

    MS> In reviewing the examples in ?ecdf, I think that an example using
    MS> something along the lines of the discussion here more explicitly, would
    MS> be helpful. It is not crystal clear from the examples, that one can use
    MS> ecdf() in this fashion, though the use of "12 * Fn(tt)" hints at it.

    MS> Perhaps:

    MS> ##-- Simple didactical  ecdf  example:
    MS> x <- rnorm(12)
    MS> Fn <- ecdf(x)
    MS> Fn
    MS> Fn(x)  # returns the percentiles for x
    MS> ...

Thank you, Marc for the above proposal, to make the examples
more "crystal clear" :-) 
I've now amended the R-devel version of  help(ecdf) accordingly.

    MS> Thanks again Martin and no offense taken...  :-)

I'm glad. You're welcome!
Martin


From berwin at maths.uwa.edu.au  Thu Dec  6 12:19:41 2007
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Thu, 6 Dec 2007 19:19:41 +0800
Subject: [R] Building package - tab delimited example data issue
In-Reply-To: <4757D47E.8060505@biostat.ku.dk>
References: <fj8i94$q54$1@ger.gmane.org>
	<4757D47E.8060505@biostat.ku.dk>
Message-ID: <20071206191941.5ceffb71@berwin-nus1>

G'day Peter,

On Thu, 06 Dec 2007 11:52:46 +0100
Peter Dalgaard <P.Dalgaard at biostat.ku.dk> wrote:
  
> If you had looked at help(data), you would have found a list of which
> file formats it supports and how they are read. Hint: TAB-delimited
> files are not among them. [...]

On the other hand, "Writing R Extensions" has stated since a long time
(and still does):

The @file{data} subdirectory is for additional data files the package
makes available for loading using @code{data()}.  Currently, data files
can have one of three types as indicated by their extension: plain R
code (@file{.R} or @file{.r}), tables (@file{.tab}, @file{.txt}, or
@file{.csv}), or @code{save()} images (@file{.RData} or @file{.rda}).

Now in my book, .csv files contain comma separated values, .tab files
contain values separated by TABs and .txt files are "pure" text files,
presumably values separated by any kind of white space. 

Thus, I think that the expectation that TAB-delimited file formats
should work is not unreasonable; I was long time ago bitten by this
too. Then I realised that the phrase "one of the three types" should
probably be interpreted as implying that .tab, .txt and .csv files are
all of the same type and, apparently, should contain values separated
by whitespace.  I admit that I never tested whether .csv files would
lead to the same problems as TAB delimited .tab files. Rather, I decided
in the end that the safest option, i.e. to avoid misleading file
extensions, would be to use .rda files in the future.

Cheers,

	Berwin

=========================== Full address =============================
Berwin A Turlach                            Tel.: +65 6515 4416 (secr)
Dept of Statistics and Applied Probability        +65 6515 6650 (self)
Faculty of Science                          FAX : +65 6872 3919       
National University of Singapore     
6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
Singapore 117546                    http://www.stat.nus.edu.sg/~statba


From vistocco at unicas.it  Thu Dec  6 11:27:26 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Thu, 06 Dec 2007 11:27:26 +0100
Subject: [R] colour coded points in biplot
In-Reply-To: <OFC179B06D.1019233A-ONC12573A9.00331B1B-C12573A9.00363A6A@precheza.cz>
References: <OFC179B06D.1019233A-ONC12573A9.00331B1B-C12573A9.00363A6A@precheza.cz>
Message-ID: <4757CE8E.1060600@unicas.it>

If you are using correspondence analysis you could see the plot.ca 
function in the ca library.

Petr PIKAL wrote:
> Dear all
>
> I tried to make a biplot with color coded labels but I was not successful. 
> Searching archives I found that it is probably not so simple.
> I found
> http://tolstoy.newcastle.edu.au/R/help/05/01/10661.html
>
> which recommends eqscplot
>
> eqscplot(fit$score[,1], fit$score[,2], pch = 20, col=c(rep(2,10), 
> rep(3,40)), cex=2)
>
> http://tolstoy.newcastle.edu.au/R/help/05/01/10661.html
>
> or I can use plot itself, but then I was not able to find how to add 
> arrows.
>
> The closest I get is
>
> fit<-(princomp(USArrests, cor = TRUE))
> biplot(fit , xlabs=rep("", 50))
> points(fit$score[,1], fit$score[,2], pch = 20, col=c(rep(2,10), 
> rep(3,40)), cex=2)
>
> and use some suitable scaling before calling points. 
>
> My question is if there is some other easier approach to get color coded 
> points in biplot?
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From simone.gabbriellini at gmail.com  Thu Dec  6 12:29:51 2007
From: simone.gabbriellini at gmail.com (Simone Gabbriellini)
Date: Thu, 6 Dec 2007 12:29:51 +0100
Subject: [R] how to interpolate a plot with a logistic curve
In-Reply-To: <4757AC2B.1020609@life.ku.dk>
References: <E2978D5F-BED0-44A7-9196-E5A9E074BB96@gmail.com>
	<4757AC2B.1020609@life.ku.dk>
Message-ID: <AD0EC73E-BFBB-4BDB-87B9-C9DC35F9DE12@gmail.com>

Dear Christian,

it works great, and it is simple to understand, thank you very much.

just a detail, is it possible to plot just the fitted curve and not  
the data? it's because I would like to have data in black and the  
fitted curve in red.. I don't know how to pick up the single parts, if  
I simply wrote

> plot(simone.m1, log="", col="red")

everything became red..

thank in advance,
Simone


Il giorno 06/dic/07, alle ore 09:00, Christian Ritz ha scritto:

> Dear Simone,
>
> you can use the package 'drc' to fit a logistic model, that is a non- 
> linear regression model based on the equation c+(d-c)/(1+exp(b(x- 
> e))), to your data (below named 'simone'):
>
>
> ## Fitting a 4-parameter logistic model (also called Boltzmann model)
> simone.m1 <- drm(size~x, data=simone, fct=B.4())
>
> ## Plotting the data together with the fitted curve
> plot(simone.m1, log="")
>
>
> The fit is not great due to the bend.
>
>
> Christian
>


From simone.gabbriellini at gmail.com  Thu Dec  6 12:45:02 2007
From: simone.gabbriellini at gmail.com (Simone Gabbriellini)
Date: Thu, 6 Dec 2007 12:45:02 +0100
Subject: [R] how to interpolate a plot with a logistic curve
References: <AD0EC73E-BFBB-4BDB-87B9-C9DC35F9DE12@gmail.com>
Message-ID: <CA36EA02-CFC6-44A4-A858-E03B80E2558F@gmail.com>

Dear Christian,

it works great, and it is simple to understand, thank you very much.

just a detail, is it possible to plot just the fitted curve and not  
the data? it's because I would like to have data in black and the  
fitted curve in red.. I don't know how to pick up the single parts, if  
I simply wrote

> plot(simone.m1, log="", col="red")

everything became red..

thank in advance,
Simone


Il giorno 06/dic/07, alle ore 09:00, Christian Ritz ha scritto:

> Dear Simone,
>
> you can use the package 'drc' to fit a logistic model, that is a non- 
> linear regression model based on the equation c+(d-c)/(1+exp(b(x- 
> e))), to your data (below named 'simone'):
>
>
> ## Fitting a 4-parameter logistic model (also called Boltzmann model)
> simone.m1 <- drm(size~x, data=simone, fct=B.4())
>
> ## Plotting the data together with the fitted curve
> plot(simone.m1, log="")
>
>
> The fit is not great due to the bend.
>
>
> Christian
>


From johannes_graumann at web.de  Thu Dec  6 13:03:20 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 6 Dec 2007 13:03:20 +0100
Subject: [R] Building package - tab delimited example data issue
In-Reply-To: <4757D47E.8060505@biostat.ku.dk>
References: <fj8i94$q54$1@ger.gmane.org> <4757D47E.8060505@biostat.ku.dk>
Message-ID: <200712061303.20781.johannes_graumann@web.de>

On Thursday 06 December 2007 11:52:46 Peter Dalgaard wrote:
> Johannes Graumann wrote:
> > Hello,
> >
> > I'm trying to integrate example data in the shape of a tab delimited
> > ASCII file into my package and therefore dropped it into the data
> > subdirectory. The build works out just fine, but when I attempt to
> > install I get:
> >
> > ** building package indices ...
> > Error in scan(file, what, nmax, sep, dec, quote, skip, nlines,
> > na.strings,  :
> >   line 1 did not have 500 elements
> > Calls: <Anonymous> ... <Anonymous> -> switch -> assign -> read.table ->
> > scan Execution halted
> > ERROR: installing package indices failed
> > ** Removing '/usr/local/lib/R/site-library/MaxQuantUtils'
> > ** Restoring previous '/usr/local/lib/R/site-library/MaxQuantUtils'
> >
> > Accordingly the check delivers:
> >
> > ...
> > * checking whether package 'MaxQuantUtils' can be installed ... ERROR
> >
> > Can anyone tell me what I'm doing wrong? build/install witout the ASCII
> > file works just fine.
> >
> > Joh
>
> If you had looked at help(data), you would have found a list of which
> file formats it supports and how they are read. Hint: TAB-delimited
> files are not among them. *Whitespace* separated files work, using
> read.table(filename, header=TRUE), but that is not a superset of
> TAB-delimited data if there are empty fields.
>
> A nice trick is to figure out how to read the data from the command line
> and drop the relevant code into a mydata.R file (assuming that the
> actual data file is mydata.txt). This gets executed when the data is
> loaded (by data(mydata) or when building the lazyload database) because
> .R files have priority over .txt.
>
> This is quite general and allows a nice way of incorporating data
>
> management while retaining the original data source:
> >more ISwR/data/stroke.R
>
> stroke <-  read.csv2("stroke.csv", na.strings=".")
> names(stroke) <- tolower(names(stroke))
> stroke <-  within(stroke,{
>     sex <- factor(sex,levels=0:1,labels=c("Female","Male"))
>     dgn <- factor(dgn)
>     coma <- factor(coma, levels=0:1, labels=c("No","Yes"))
>     minf <- factor(minf, levels=0:1, labels=c("No","Yes"))
>     diab <- factor(diab, levels=0:1, labels=c("No","Yes"))
>     han <- factor(han, levels=0:1, labels=c("No","Yes"))
>     died <- as.Date(died, format="%d.%m.%Y")
>     dstr <- as.Date(dstr,format="%d.%m.%Y")
>     dead <- !is.na(died) & died < as.Date("1996-01-01")
>     died[!dead] <- NA
> })
>
> >head ISwR/data/stroke.csv
>
> SEX;DIED;DSTR;AGE;DGN;COMA;DIAB;MINF;HAN
> 1;7.01.1991;2.01.1991;76;INF;0;0;1;0
> 1;.;3.01.1991;58;INF;0;0;0;0
> 1;2.06.1991;8.01.1991;74;INF;0;0;1;1
> 0;13.01.1991;11.01.1991;77;ICH;0;1;0;1
> 0;23.01.1996;13.01.1991;76;INF;0;1;0;1
> 1;13.01.1991;13.01.1991;48;ICH;1;0;0;1
> 0;1.12.1993;14.01.1991;81;INF;0;0;0;1
> 1;12.12.1991;14.01.1991;53;INF;0;0;1;1
> 0;.;15.01.1991;73;ID;0;0;0;1

Thanks for your help. Very insightfull and your version of "RTFM" was not to 
harsh either ;0)
Part of what I want to achieve with the inclusion of the file is to be able to 
showcase a read-in function for the particular data type. Is there a slick 
way - sticking to your example - to reference the 'stroke.csv' directly?
I'd like to put in the example of some function.Rd something analogous to
	# Use function to read in file:
	result <- function(<link to 'stroke.csv' in installed ISwR package>)
Without having to resort to accepting the example as "No Run".

Thanks for your help, Joh
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 827 bytes
Desc: This is a digitally signed message part.
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20071206/9c4b5910/attachment.bin 

From Soren.Hojsgaard at agrsci.dk  Thu Dec  6 13:04:56 2007
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 6 Dec 2007 13:04:56 +0100
Subject: [R] $ operator is invalid for atomic vectors,
	returning NULL - what is the right thing to do then?
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC0562F999@DJFPOST01.djf.agrsci.dk>

Dear all,
Starting from a recent version of R, the $ became "unusable" on atomic vectors, e.g.
> x <- c(a=1,b=2)
> x$a
NULL
Warning message:
In x$a : $ operator is invalid for atomic vectors, returning NULL
 
I can of course do
> x['a']
- but that requires more typing (5 characters rather than 2). 
 
Apologies if I've missed a an announcement regarding this, but
1) Is there an alternative to the ['a'] and
2) Why was this change made?
 
Regards
S?ren
 


From info at aghmed.fsnet.co.uk  Thu Dec  6 13:17:58 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Thu, 06 Dec 2007 12:17:58 +0000
Subject: [R] Nine questions about methods and generics
Message-ID: <Zen-1J0Fg4-0005As-ON@heisenberg.zen.co.uk>

I have a series of question about methods and generics.
The questions are interspersed in some text which explains
what I want to do, how it works now and why I do not
understand well why it works. Questions are written
Q1 and so on up to Q9 and always start a new line.

The concrete problem is this: it has become customary
after a meta-analysis to quote various statistics
to give a picture of the degree of heterogeneity.
Initially I wrote a function which calculated these given
the heterogeneity chi-squared Q and the number of studies k.
It then occurred to me that it would be convenient to
have functions which take the objects returned by
the various functions in the package rmeta (available
from CRAN) and perform the calculation on them.
So after plagiarising some ideas from rmeta
I wrote

hetero <- function(x, ...) {
    UseMethod("hetero")
}

Q1 - have I created a generic called hetero? If so
what is the correct idiom: created, declared, ...?
Q2 - if I go class(hetero) it returns "function" rather
than "generic". Is that what I should expect?

I then called my original function hetero.default

hetero.default <- function(Q = NULL, k = NULL, conflevel = 0.95) {
# some code left out here which computes values in the list res
    class(res) <- "meta.hetero"
    res
}

Q3 was it a good idea to have it return an object of class
"meta.hetero" or would it have been better to call it "hetero"

now if I go
hetero(14.4, 24)
I get what I expected

I then provide a new function

hetero.meta.summaries <- function(obj, ...) {
# some code to calculate Q and k
    res <- hetero.default(Q, k, ...)
    res
}
which also seems to do what I wanted. There are also
similar functions hetero.meta.MH and hetero.meta.DSL
to operate on the other classes of object returned in rmeta.

The problem is that I do not really understand why it all works
and suspect it could easily stop working.

At this point I decided that if all else fails I should read the
documentation. The manual pages (the things you get
with ?UseMethod) describe what the functions do but
do not give me a series of steps to salvation.
I found S Programming (V&R) much clearer now I knew where to
look but I think V&R underestimate the depths of my
ignorance.

Q4 V&R seems to suggest that I should have called the first
argument to hetero, hetero.default and hetero.meta.summaries
the same (possibly x or object) yet it seems to work.
Q5 Does it matter?
If it does matter which should I choose?

I also decide to look at R-extensions which seems
to suggest that I should be using NextMethod and
also has dire warnings about having different argument
lists to the various methods.
I found R-extensions much the least helpful
of the three sources I tried.

Q6 should I be using NextMethod?
Q7 if what I have created are methods which are
displayed by methods("hetero") why is it UseMethod not
usemethod? (I note that ?Methods tells me about something
different from ?methods.)
Q8 I have used a ... argument for hetero.meta.summaries
but not for hetero.default. Was that wise? R-extensions
has some stern things to say here which I do not really
understand.
Q9 is there somewhere else I could have looked
bearing in mind my comments above about the
manual pages, V&R and R-extensions? Or shall
I just keep reading V&R until the penny drops?

For the record:

S Programming (V&R) means
@BOOK{venables00,
   author = {Venables, W N and Ripley, B D},
   year = 2000,
   title = {S programming},
   publisher = {Springer-Verlag},
   address = {New York},
   keywords = {statistics general; software}
}
The heterogeneity statistics are explained in
@ARTICLE{higgins02,
   author = {Higgins, J P T and Thompson, S G},
   year = 2002,
   title = {Quantifying heterogeneity in a meta--analysis},
   journal = {Statistics in Medicine},
   volume = 21,
   pages = {1539--1558},
   keywords = {meta-analysis, heterogeneity}
}

I am using 2.6.1 under XP Professional.
My knowledge of computer science is frozen at about 1975.



Michael Dewey
http://www.aghmed.fsnet.co.uk


From ripley at stats.ox.ac.uk  Thu Dec  6 13:22:39 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 6 Dec 2007 12:22:39 +0000 (GMT)
Subject: [R] $ operator is invalid for atomic vectors,
 returning NULL - what is the right thing to do then?
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0562F999@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC0562F999@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.64.0712061208290.21084@gannet.stats.ox.ac.uk>

On Thu, 6 Dec 2007, S?ren H?jsgaard wrote:

> Dear all,
> Starting from a recent version of R,

>From R 2.5.0, not so recent.

> the $ became "unusable" on atomic vectors, e.g.
>> x <- c(a=1,b=2)
>> x$a
> NULL
> Warning message:
> In x$a : $ operator is invalid for atomic vectors, returning NULL
> I can of course do
>> x['a']
> - but that requires more typing (5 characters rather than 2).
>
> Apologies if I've missed a an announcement regarding this, but
> 1) Is there an alternative to the ['a'] and
> 2) Why was this change made?

It has always returned NULL on atomic vectors: see the help page.
E.g. in R 2.0.0 from 2004:

> x <- c(a=1,b=2)
> x$a
NULL
> x['a']
a
1

The warning was added three versions of R ago: the announcement is in the 
NEWS file for 2.5.0.

USER-VISIBLE CHANGES

     o   Using $ on an atomic vector now raises a warning, as does use
         on an S4 class for which a method has not been defined.

I think you have exemplified the answer to your question '2)': because 
users misunderstood what it did.

In R 2.7.0 this will be an error, since package programmers did not seem 
to be heeding the warnings.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Ted.Harding at manchester.ac.uk  Thu Dec  6 13:39:53 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Thu, 06 Dec 2007 12:39:53 -0000 (GMT)
Subject: [R] $ operator is invalid for atomic vectors, returning NULL
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0562F999@DJFPOST01.djf.agrsci.dk>
Message-ID: <XFMail.071206123953.Ted.Harding@manchester.ac.uk>

On 06-Dec-07 12:04:56, S?ren H?jsgaard wrote:
> Dear all,
> Starting from a recent version of R, the $ became "unusable"
> on atomic vectors, e.g.
>> x <- c(a=1,b=2)
>> x$a
> NULL
> Warning message:
> In x$a : $ operator is invalid for atomic vectors, returning NULL
>  
> I can of course do
>> x['a']
> - but that requires more typing (5 characters rather than 2). 
>  
> Apologies if I've missed a an announcement regarding this, but
> 1) Is there an alternative to the ['a'] and
> 2) Why was this change made?

While Brian has answered your query from a more technical
point of view, there's an implication you should consider.

x$a always (as far as I know)[*] has been "unusable", in that
it returns NULL, and not what you expected to get. The only
thing that's (relatively) new is that you now get a warning.

[*] Certainly as early as R-1.6.2 (Jan 2003), as I've just checked.

Previously, you were not getting a warning.

So, if you were using the likes of x$a in code, and getting
no warnings, possibly your code was generating results
derived from NULL values rather than from the values you
expected it to be using. Perhaps you should check back!

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Dec-07                                       Time: 12:39:49
------------------------------ XFMail ------------------------------


From ggrothendieck at gmail.com  Thu Dec  6 13:40:10 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 6 Dec 2007 07:40:10 -0500
Subject: [R] R function for percentrank
In-Reply-To: <18263.55538.53978.535345@stat.math.ethz.ch>
References: <65cc7bdf0712010337o5844e1a3sfcb6984f93946267@mail.gmail.com>
	<243517.68093.qm@web32807.mail.mud.yahoo.com>
	<65cc7bdf0712010951p451a993i70da89f285d801de@mail.gmail.com>
	<Xns99F989B3A3057dNOTwinscomcast@80.91.229.13>
	<Xns99F98B11F90B6dNOTwinscomcast@80.91.229.13>
	<1196537601.2980.15.camel@Bellerophon.localdomain>
	<18262.58128.974202.67709@stat.math.ethz.ch>
	<1196880230.2945.26.camel@Bellerophon.localdomain>
	<18263.55538.53978.535345@stat.math.ethz.ch>
Message-ID: <971536df0712060440n15500205t410eab4d8d9975b2@mail.gmail.com>

On Dec 6, 2007 6:11 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "MS" == Marc Schwartz <marc_schwartz at comcast.net>
> >>>>>     on Wed, 05 Dec 2007 12:43:50 -0600 writes:
>
> [............]
>
>    MS> Martin,
>
>    MS> Thanks for the corrections. In hindsight, now seeing the intended use of
>    MS> ecdf() in the fashion you describe above, it is now clear that my
>    MS> approach in response to David's query was un-needed and "over the top".
>    MS> "Yuck" is quite appropriate... :-)
>
>    MS> As I was going through this "exercise", it did seem overly complicated,
>    MS> given R's usual elegant philosophy about such things. I suppose if I had
>    MS> looked at the source for plot.stepfun(), it would have been more evident
>    MS> as to how the y values are acquired.
>
>    MS> In reviewing the examples in ?ecdf, I think that an example using
>    MS> something along the lines of the discussion here more explicitly, would
>    MS> be helpful. It is not crystal clear from the examples, that one can use
>    MS> ecdf() in this fashion, though the use of "12 * Fn(tt)" hints at it.
>
>    MS> Perhaps:
>
>    MS> ##-- Simple didactical  ecdf  example:
>    MS> x <- rnorm(12)
>    MS> Fn <- ecdf(x)
>    MS> Fn
>    MS> Fn(x)  # returns the percentiles for x
>    MS> ...
>
> Thank you, Marc for the above proposal, to make the examples
> more "crystal clear" :-)
> I've now amended the R-devel version of  help(ecdf) accordingly.

Since the above does not actually reproduce percentrank in the case
of ties, the change that would facilitate this particularly would be to add
a ties = "excelpercentrank" to approx.  See my solution earlier in this
thread.


From P.Dalgaard at biostat.ku.dk  Thu Dec  6 13:47:26 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 06 Dec 2007 13:47:26 +0100
Subject: [R] Building package - tab delimited example data issue
In-Reply-To: <20071206191941.5ceffb71@berwin-nus1>
References: <fj8i94$q54$1@ger.gmane.org>	<4757D47E.8060505@biostat.ku.dk>
	<20071206191941.5ceffb71@berwin-nus1>
Message-ID: <4757EF5E.6040904@biostat.ku.dk>

Berwin A Turlach wrote:
> G'day Peter,
>
> On Thu, 06 Dec 2007 11:52:46 +0100
> Peter Dalgaard <P.Dalgaard at biostat.ku.dk> wrote:
>   
>   
>> If you had looked at help(data), you would have found a list of which
>> file formats it supports and how they are read. Hint: TAB-delimited
>> files are not among them. [...]
>>     
>
> On the other hand, "Writing R Extensions" has stated since a long time
> (and still does):
>
> The @file{data} subdirectory is for additional data files the package
> makes available for loading using @code{data()}.  Currently, data files
> can have one of three types as indicated by their extension: plain R
> code (@file{.R} or @file{.r}), tables (@file{.tab}, @file{.txt}, or
> @file{.csv}), or @code{save()} images (@file{.RData} or @file{.rda}).
>
> Now in my book, .csv files contain comma separated values, .tab files
> contain values separated by TABs and .txt files are "pure" text files,
> presumably values separated by any kind of white space. 
>
> Thus, I think that the expectation that TAB-delimited file formats
> should work is not unreasonable; I was long time ago bitten by this
> too. Then I realised that the phrase "one of the three types" should
> probably be interpreted as implying that .tab, .txt and .csv files are
> all of the same type and, apparently, should contain values separated
> by whitespace.  I admit that I never tested whether .csv files would
> lead to the same problems as TAB delimited .tab files. Rather, I decided
> in the end that the safest option, i.e. to avoid misleading file
> extensions, would be to use .rda files in the future.
>
>   
Now had you lived in the Western world ... (Hey, what's that? New
address!) ... then you would have known better than to have any trust in
file extensions. At the time "they" apparently figured that the .CSV
standard was so good that it was even better to have two of them (double
standards are twice as good, right?), depending on whether you were in
England or in Denmark, I lost faith completely. (In this country you can
export to a text file with SAS and then NOT read it with SPSS and vice
versa on the same Windows machine).

Actually, R is a bit perverse about .csv too since it expects
_semicolon_  field separator, but not the  comma decimal separator which
usually accompanies it. The reason for this is lost in the mists of time
-- the datasets in current versions of R do not include any .csv files.
There are, however, six .tab files, three of which are not
tab-separated, but I don't actually think there was ever a standard to
the effect that they should be (.tab just means that it is a _table_).

So, you really need to read the help page for data, which does have the 
exact info. The passage you cite from the manual could do with a
rephrasing, although it probably isn't technically incorrect. As it
stands, it reminds me a bit of the old Monty Python sketch:

"Our *three* weapons are fear, surprise, and ruthless efficiency...and
an almost fanatical devotion to the Pope.... Our *four*...no...
*Amongst* our weapons.... Amongst our weaponry...are such elements as
fear, surprise.... I'll come in again"

(There really are 3 data TYPES, but 4 FORMATS and, er, diverse EXTENSIONS)



--  
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From vmuggeo at dssm.unipa.it  Thu Dec  6 14:01:29 2007
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Thu, 06 Dec 2007 14:01:29 +0100
Subject: [R] differences in using source() or console
Message-ID: <4757F2A9.8040300@dssm.unipa.it>

Dear all,
Is there *any* reason explaining what I describe below?
I have the following line

myfun(x)

If I type them directly in R (or copy/past), it works..

However if I type in R 2.6.1

 > source("code.R") ##code.R includes the above line
Error in inherits(x, "data.frame") : object "d" not found

namely myfun() does not work correctly.
In particular the non-working line inside myfun() is

update(eval(x$call$obj), data=d) #d is created in myfun()

My question is: why the problem occurs just via source()ing???

many thanks,
vito


====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612


From iacopetti at fastpiu.it  Thu Dec  6 15:18:48 2007
From: iacopetti at fastpiu.it (Roberto Iacopetti)
Date: Thu, 6 Dec 2007 06:18:48 -0800 (PST)
Subject: [R]  R2HTML  how to pair graphic.png and table
Message-ID: <14193218.post@talk.nabble.com>


Dear list,
i have this problem:
how to pair a graphic.png and a table in R2HTML ?

The better showing of a mutiple analysis is sometimes to mate graphic and
table

Can anyone help me in this task ??

In the example below graphisc and table are subsequent and not pair..
 

directory=getwd()
myfile<-file.path(directory,"testHTML.html")    
HTMLoutput=file.path(directory,"testHTML.html")
graf="graf.png"
png(file.path(directory,graf))
plot(c(1:12))
dev.off()
tab<-as.matrix(c(1:12))
HTMLInsertGraph(graf,tab,file=HTMLoutput,caption="Esempio di grafico")
browseURL(myfile)

Thanks in advance

Roberto Iacopetti

-- 
View this message in context: http://www.nabble.com/R2HTML--how-to-pair-graphic.png-and-table-tf4956321.html#a14193218
Sent from the R help mailing list archive at Nabble.com.


From hoontaechung at gmail.com  Thu Dec  6 15:25:10 2007
From: hoontaechung at gmail.com (=?EUC-KR?B?waQgxcLIxg==?=)
Date: Thu, 6 Dec 2007 23:25:10 +0900
Subject: [R] 64-bit R question in Leopard
Message-ID: <DBCED950-C20F-47BD-8857-8B830FF8FFEA@gmail.com>

Hi, All;

I've compiled and installed successfully "presumably" 64-bit R on  
Leopard.
But when I tried to run R, I got the following error:

/usr/bin/R: line 179: /Library/Frameworks/R.framework/Resources/etc/ 
i386/ldpaths: No such file or directory

Here is the configuration I used:

CONFIG_SHELL=/bin/bash ../R-devel/configure r_arch=x86_64 LDFLAGS="-L/ 
usr/local/lib64 -L/usr/X11R6/lib" CC='gcc-4.2 -g -O3' CXX='g++-4.2 -g - 
O3' F77='gfortran -g -O3' FC='gfortran -g -O3' --with-blas='-framework  
vecLib' --with-lapack

Thanks in advance

Tae-Hoon Chung


From jmacdon at med.umich.edu  Thu Dec  6 15:21:44 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Thu, 06 Dec 2007 09:21:44 -0500
Subject: [R] hclust in heatmap.2
In-Reply-To: <5032046e0712051620l4ea3e458o7807d53a039cac16@mail.gmail.com>
References: <5032046e0712051620l4ea3e458o7807d53a039cac16@mail.gmail.com>
Message-ID: <47580578.2090106@med.umich.edu>



affy snp wrote:
> Dear list,
> 
> I am using heatmap.2(x) to draw a heatmap. Ideally, I want to the matrix
> x clustered only by columns and keep the original order of rows unchanged.
> Is there a way to do that in heatmap.2()?
> 
> Thanks a lot! Any suggestions will be appreciated!

 From the help page:

Arguments:

        x: numeric matrix of the values to be plotted.

     Rowv: determines if and how the _row_ dendrogram should be
           reordered.  By default, it is TRUE, which implies dendrogram
           is computed and reordered based on row means. If NULL or
           FALSE, then no dendrogram is computed and reordering is done.
           If a 'dendrogram', then it is used "as-is", ie without any
           reordering. If a vector of integers, then dendrogram is
           computed and reordered based on the order of the vector.
> 
> Best,
>       Allen
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


From johannes_graumann at web.de  Thu Dec  6 15:37:19 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 06 Dec 2007 15:37:19 +0100
Subject: [R] Building package - tab delimited example data issue
References: <fj8i94$q54$1@ger.gmane.org> <4757D47E.8060505@biostat.ku.dk>
	<200712061303.20781.johannes_graumann@web.de>
Message-ID: <fj91ei$ilk$1@ger.gmane.org>

Johannes Graumann wrote:

> On Thursday 06 December 2007 11:52:46 Peter Dalgaard wrote:
>> Johannes Graumann wrote:
>> > Hello,
>> >
>> > I'm trying to integrate example data in the shape of a tab delimited
>> > ASCII file into my package and therefore dropped it into the data
>> > subdirectory. The build works out just fine, but when I attempt to
>> > install I get:
>> >
>> > ** building package indices ...
>> > Error in scan(file, what, nmax, sep, dec, quote, skip, nlines,
>> > na.strings,  :
>> >   line 1 did not have 500 elements
>> > Calls: <Anonymous> ... <Anonymous> -> switch -> assign -> read.table ->
>> > scan Execution halted
>> > ERROR: installing package indices failed
>> > ** Removing '/usr/local/lib/R/site-library/MaxQuantUtils'
>> > ** Restoring previous '/usr/local/lib/R/site-library/MaxQuantUtils'
>> >
>> > Accordingly the check delivers:
>> >
>> > ...
>> > * checking whether package 'MaxQuantUtils' can be installed ... ERROR
>> >
>> > Can anyone tell me what I'm doing wrong? build/install witout the ASCII
>> > file works just fine.
>> >
>> > Joh
>>
>> If you had looked at help(data), you would have found a list of which
>> file formats it supports and how they are read. Hint: TAB-delimited
>> files are not among them. *Whitespace* separated files work, using
>> read.table(filename, header=TRUE), but that is not a superset of
>> TAB-delimited data if there are empty fields.
>>
>> A nice trick is to figure out how to read the data from the command line
>> and drop the relevant code into a mydata.R file (assuming that the
>> actual data file is mydata.txt). This gets executed when the data is
>> loaded (by data(mydata) or when building the lazyload database) because
>> .R files have priority over .txt.
>>
>> This is quite general and allows a nice way of incorporating data
>>
>> management while retaining the original data source:
>> >more ISwR/data/stroke.R
>>
>> stroke <-  read.csv2("stroke.csv", na.strings=".")
>> names(stroke) <- tolower(names(stroke))
>> stroke <-  within(stroke,{
>>     sex <- factor(sex,levels=0:1,labels=c("Female","Male"))
>>     dgn <- factor(dgn)
>>     coma <- factor(coma, levels=0:1, labels=c("No","Yes"))
>>     minf <- factor(minf, levels=0:1, labels=c("No","Yes"))
>>     diab <- factor(diab, levels=0:1, labels=c("No","Yes"))
>>     han <- factor(han, levels=0:1, labels=c("No","Yes"))
>>     died <- as.Date(died, format="%d.%m.%Y")
>>     dstr <- as.Date(dstr,format="%d.%m.%Y")
>>     dead <- !is.na(died) & died < as.Date("1996-01-01")
>>     died[!dead] <- NA
>> })
>>
>> >head ISwR/data/stroke.csv
>>
>> SEX;DIED;DSTR;AGE;DGN;COMA;DIAB;MINF;HAN
>> 1;7.01.1991;2.01.1991;76;INF;0;0;1;0
>> 1;.;3.01.1991;58;INF;0;0;0;0
>> 1;2.06.1991;8.01.1991;74;INF;0;0;1;1
>> 0;13.01.1991;11.01.1991;77;ICH;0;1;0;1
>> 0;23.01.1996;13.01.1991;76;INF;0;1;0;1
>> 1;13.01.1991;13.01.1991;48;ICH;1;0;0;1
>> 0;1.12.1993;14.01.1991;81;INF;0;0;0;1
>> 1;12.12.1991;14.01.1991;53;INF;0;0;1;1
>> 0;.;15.01.1991;73;ID;0;0;0;1
> 
> Thanks for your help. Very insightfull and your version of "RTFM" was not
> to harsh either ;0)
> Part of what I want to achieve with the inclusion of the file is to be
> able to showcase a read-in function for the particular data type. Is there
> a slick way - sticking to your example - to reference the 'stroke.csv'
> directly? I'd like to put in the example of some function.Rd something
> analogous to # Use function to read in file:
> result <- function(<link to 'stroke.csv' in installed ISwR package>)
> Without having to resort to accepting the example as "No Run".

Answering to myself and staying with the same example:
        system.file("data/stroke.csv",package="ISwR")
allows direct access to the example file (name).

Joh


From ericlecoutre at gmail.com  Thu Dec  6 15:46:57 2007
From: ericlecoutre at gmail.com (Eric Lecoutre)
Date: Thu, 6 Dec 2007 15:46:57 +0100
Subject: [R] R2HTML how to pair graphic.png and table
In-Reply-To: <14193218.post@talk.nabble.com>
References: <14193218.post@talk.nabble.com>
Message-ID: <5d897a2f0712060646h70061b72k62ce7eb23333c5fe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/12fa1864/attachment.pl 

From murdoch at stats.uwo.ca  Thu Dec  6 15:51:38 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 06 Dec 2007 09:51:38 -0500
Subject: [R] $ operator is invalid for atomic vectors,
 returning NULL - what is the right thing to do then?
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0562F999@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC0562F999@DJFPOST01.djf.agrsci.dk>
Message-ID: <47580C7A.6040500@stats.uwo.ca>

On 12/6/2007 7:04 AM, S?ren H?jsgaard wrote:
> Dear all,
> Starting from a recent version of R, the $ became "unusable" on atomic vectors, e.g.
>> x <- c(a=1,b=2)
>> x$a
> NULL
> Warning message:
> In x$a : $ operator is invalid for atomic vectors, returning NULL
>  
> I can of course do
>> x['a']
> - but that requires more typing (5 characters rather than 2). 
>  
> Apologies if I've missed a an announcement regarding this, but
> 1) Is there an alternative to the ['a'] and
> 2) Why was this change made?

There's an announcement in the NEWS file for 2.6.0

DEPRECATED & DEFUNCT

     o	$ on an atomic vector now gives a warning that it is 'invalid'.
	It remains deprecated, but may be removed in R >= 2.7.0.

and an update in R-devel saying that in fact it has been made defunct there.

In answer to your questions:

1) I don't think so, other than x[1].
2) I don't remember the details, other than it was to avoid confusion. 
Maybe someone else can summarize the discussion?

Duncan Murdoch


From vistocco at unicas.it  Thu Dec  6 15:59:00 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Thu, 06 Dec 2007 15:59:00 +0100
Subject: [R] R2HTML  how to pair graphic.png and table
In-Reply-To: <14193218.post@talk.nabble.com>
References: <14193218.post@talk.nabble.com>
Message-ID: <47580E34.1080407@unicas.it>

You could use a table with one row and two columns:

HTML("<TABLE><TD>",file=HTMLoutput)
HTML(tab,file=HTMLoutput)
HTML("</TD><TD>",file=HTMLoutput)
HTMLInsertGraph(graf,file=HTMLoutput,caption="Esempio di grafico")
HTML("</TD></TABLE>",file=HTMLoutput)

domenico

PS:
You could create a function if this is a common operation:

tableGraph=function(tab2Html,graph2Html,fileHtml){
    HTML("<TABLE><TD>",file=fileHtml)
    HTML(tab2Html,file=HTMLoutput)
    HTML("</TD><TD>",file=HTMLoutput)
    HTMLInsertGraph(graph2Html,file=fileHtml,caption="Esempio di grafico")
    HTML("</TD></TABLE>",file=fileHtml)
}

and then:
tableGraph(tab,graf,HTMLoutput)


Roberto Iacopetti wrote:
> Dear list,
> i have this problem:
> how to pair a graphic.png and a table in R2HTML ?
>
> The better showing of a mutiple analysis is sometimes to mate graphic and
> table
>
> Can anyone help me in this task ??
>
> In the example below graphisc and table are subsequent and not pair..
>  
>
> directory=getwd()
> myfile<-file.path(directory,"testHTML.html")    
> HTMLoutput=file.path(directory,"testHTML.html")
> graf="graf.png"
> png(file.path(directory,graf))
> plot(c(1:12))
> dev.off()
> tab<-as.matrix(c(1:12))
> HTMLInsertGraph(graf,tab,file=HTMLoutput,caption="Esempio di grafico")
> browseURL(myfile)
>
> Thanks in advance
>
> Roberto Iacopetti
>
>


From P.Dalgaard at biostat.ku.dk  Thu Dec  6 16:03:48 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 06 Dec 2007 16:03:48 +0100
Subject: [R] Building package - tab delimited example data issue
In-Reply-To: <fj91ei$ilk$1@ger.gmane.org>
References: <fj8i94$q54$1@ger.gmane.org>
	<4757D47E.8060505@biostat.ku.dk>	<200712061303.20781.johannes_graumann@web.de>
	<fj91ei$ilk$1@ger.gmane.org>
Message-ID: <47580F54.1000506@biostat.ku.dk>

Johannes Graumann wrote:
> Johannes Graumann wrote:
>
>   
>> On Thursday 06 December 2007 11:52:46 Peter Dalgaard wrote:
>>     
>>> Johannes Graumann wrote:
>>>       
>>>> Hello,
>>>>
>>>> I'm trying to integrate example data in the shape of a tab delimited
>>>> ASCII file into my package and therefore dropped it into the data
>>>> subdirectory. The build works out just fine, but when I attempt to
>>>> install I get:
>>>>
>>>> ** building package indices ...
>>>> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines,
>>>> na.strings,  :
>>>>   line 1 did not have 500 elements
>>>> Calls: <Anonymous> ... <Anonymous> -> switch -> assign -> read.table ->
>>>> scan Execution halted
>>>> ERROR: installing package indices failed
>>>> ** Removing '/usr/local/lib/R/site-library/MaxQuantUtils'
>>>> ** Restoring previous '/usr/local/lib/R/site-library/MaxQuantUtils'
>>>>
>>>> Accordingly the check delivers:
>>>>
>>>> ...
>>>> * checking whether package 'MaxQuantUtils' can be installed ... ERROR
>>>>
>>>> Can anyone tell me what I'm doing wrong? build/install witout the ASCII
>>>> file works just fine.
>>>>
>>>> Joh
>>>>         
>>> If you had looked at help(data), you would have found a list of which
>>> file formats it supports and how they are read. Hint: TAB-delimited
>>> files are not among them. *Whitespace* separated files work, using
>>> read.table(filename, header=TRUE), but that is not a superset of
>>> TAB-delimited data if there are empty fields.
>>>
>>> A nice trick is to figure out how to read the data from the command line
>>> and drop the relevant code into a mydata.R file (assuming that the
>>> actual data file is mydata.txt). This gets executed when the data is
>>> loaded (by data(mydata) or when building the lazyload database) because
>>> .R files have priority over .txt.
>>>
>>> This is quite general and allows a nice way of incorporating data
>>>
>>> management while retaining the original data source:
>>>       
>>>> more ISwR/data/stroke.R
>>>>         
>>> stroke <-  read.csv2("stroke.csv", na.strings=".")
>>> names(stroke) <- tolower(names(stroke))
>>> stroke <-  within(stroke,{
>>>     sex <- factor(sex,levels=0:1,labels=c("Female","Male"))
>>>     dgn <- factor(dgn)
>>>     coma <- factor(coma, levels=0:1, labels=c("No","Yes"))
>>>     minf <- factor(minf, levels=0:1, labels=c("No","Yes"))
>>>     diab <- factor(diab, levels=0:1, labels=c("No","Yes"))
>>>     han <- factor(han, levels=0:1, labels=c("No","Yes"))
>>>     died <- as.Date(died, format="%d.%m.%Y")
>>>     dstr <- as.Date(dstr,format="%d.%m.%Y")
>>>     dead <- !is.na(died) & died < as.Date("1996-01-01")
>>>     died[!dead] <- NA
>>> })
>>>
>>>       
>>>> head ISwR/data/stroke.csv
>>>>         
>>> SEX;DIED;DSTR;AGE;DGN;COMA;DIAB;MINF;HAN
>>> 1;7.01.1991;2.01.1991;76;INF;0;0;1;0
>>> 1;.;3.01.1991;58;INF;0;0;0;0
>>> 1;2.06.1991;8.01.1991;74;INF;0;0;1;1
>>> 0;13.01.1991;11.01.1991;77;ICH;0;1;0;1
>>> 0;23.01.1996;13.01.1991;76;INF;0;1;0;1
>>> 1;13.01.1991;13.01.1991;48;ICH;1;0;0;1
>>> 0;1.12.1993;14.01.1991;81;INF;0;0;0;1
>>> 1;12.12.1991;14.01.1991;53;INF;0;0;1;1
>>> 0;.;15.01.1991;73;ID;0;0;0;1
>>>       
>> Thanks for your help. Very insightfull and your version of "RTFM" was not
>> to harsh either ;0)
>> Part of what I want to achieve with the inclusion of the file is to be
>> able to showcase a read-in function for the particular data type. Is there
>> a slick way - sticking to your example - to reference the 'stroke.csv'
>> directly? I'd like to put in the example of some function.Rd something
>> analogous to # Use function to read in file:
>> result <- function(<link to 'stroke.csv' in installed ISwR package>)
>> Without having to resort to accepting the example as "No Run".
>>     
>
> Answering to myself and staying with the same example:
>         system.file("data/stroke.csv",package="ISwR")
> allows direct access to the example file (name).
>
>   
Yes, but...

This works right until you turn on LazyData for your package, then you
end up with only

00Index  Rdata.rdb  Rdata.rds  Rdata.rdx

in the data directory. Use the "inst" source subdir for files you want
to have installed explicitly.

Also, in principle, it is

system.file("data", "stroke.csv", package="ISwR")


although platforms that do not understand "/" as the path separator are
rare nowadays.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From dhewitt at vims.edu  Thu Dec  6 16:13:12 2007
From: dhewitt at vims.edu (David Hewitt)
Date: Thu, 6 Dec 2007 07:13:12 -0800 (PST)
Subject: [R] Junk or not Junk ???
In-Reply-To: <C37C7167.156AD%engrav@u.washington.edu>
References: <C379F3E1.15549%engrav@u.washington.edu>
	<4754C51A.4010109@stats.uwo.ca>
	<C37C7167.156AD%engrav@u.washington.edu>
Message-ID: <14193897.post@talk.nabble.com>



Loren Engrav wrote:
> 
> Thank you
> 
> As per advice from several R users I have set
> 
> r-project.org, stat.math.ethz.ch,fhcrc.org, stat.ethz.ch, math.ethz.ch,
> hypatia.math.ethz.ch
> 
>  all to be "safe domains"
> 
> But still some R emails go to Junk and require to be found manually
> 
> I have explored the issue with Univ Wash computing to no avail
> 
> Is this just how it is or have I still missed the "fix" to keep R emails
> out
> of junk?
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

I suspect most people that stay on this list read most posts through a news
reader... you might consider doing the same. I can't even imagine keeping up
with the daily deluge of individual emails.

http://www.nabble.com/R-help-f13820.html

http://news.gmane.org/gmane.comp.lang.r.general


-----
David Hewitt
Virginia Institute of Marine Science
http://www.vims.edu/fish/students/dhewitt/
-- 
View this message in context: http://www.nabble.com/Junk-or-not-Junk-tf4940701.html#a14193897
Sent from the R help mailing list archive at Nabble.com.


From nyggus at gmail.com  Thu Dec  6 16:16:49 2007
From: nyggus at gmail.com (Marcin Kozak)
Date: Thu, 6 Dec 2007 16:16:49 +0100
Subject: [R] Vertical text in a plot
Message-ID: <5c79da7a0712060716j3bfabdd7v7bad975de9c1b953@mail.gmail.com>

Hi,

Consider this simple plot:
> plot(1:25,runif(25,0,1),ylab="First Y-axis label",xaxt="n")

I want to add an additional axis as
> axis(4,at=seq(0.2,1,.2), labels=1:5)

I have no idea how to add now the title of the new axis as "Second
Y-axis label". I want this text to be vertically directed from bottom
to top. I can't find the function in text() to write vertically. Any
ideas?

Thanks,
Marcin


From P.Dalgaard at biostat.ku.dk  Thu Dec  6 16:23:33 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 06 Dec 2007 16:23:33 +0100
Subject: [R] Vertical text in a plot
In-Reply-To: <5c79da7a0712060716j3bfabdd7v7bad975de9c1b953@mail.gmail.com>
References: <5c79da7a0712060716j3bfabdd7v7bad975de9c1b953@mail.gmail.com>
Message-ID: <475813F5.6000902@biostat.ku.dk>

Marcin Kozak wrote:
> Hi,
>
> Consider this simple plot:
>   
>> plot(1:25,runif(25,0,1),ylab="First Y-axis label",xaxt="n")
>>     
>
> I want to add an additional axis as
>   
>> axis(4,at=seq(0.2,1,.2), labels=1:5)
>>     
>
> I have no idea how to add now the title of the new axis as "Second
> Y-axis label". I want this text to be vertically directed from bottom
> to top. I can't find the function in text() to write vertically. Any
> ideas?
>
>   
mtext() is your friend....

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From hoontaechung at gmail.com  Thu Dec  6 16:31:27 2007
From: hoontaechung at gmail.com (=?EUC-KR?B?waQgxcLIxg==?=)
Date: Fri, 7 Dec 2007 00:31:27 +0900
Subject: [R] 64-bit R compiling problem on Leopard
Message-ID: <B7FD8FE3-DB83-441E-B122-CC9C159C75AE@gmail.com>

Hi, All;

I've compiled and installed successfully "presumably" 64-bit R on  
Leopard.
But when I tried to run R, I got the following error:

/usr/bin/R: line 179: /Library/Frameworks/R.framework/Resources/etc/ 
i386/ldpaths: No such file or directory

Here is the configuration I used:

CONFIG_SHELL=/bin/bash ../R-devel/configure r_arch=x86_64 LDFLAGS="-L/ 
usr/local/lib64 -L/usr/X11R6/lib" CC='gcc-4.2 -g -O3' CXX='g++-4.2 -g - 
O3' F77='gfortran -g -O3' FC='gfortran -g -O3' --with-blas='-framework  
vecLib' --with-lapack

Thanks in advance

Tae-Hoon Chung

Korea Centers for Disease Control & Prevention (KCDC)
Korea National Institute of Health (KNIH)
Center for Genome Sciences, Biobank for Health Sciences

194 Tongil-ro, Eunpyoung-gu, Seoul, 122-701, Korea

Tel		82-2-380-2252
Fax		82-2-354-1078
Mobile	82-10-8011-1036


From abfriedman at gmail.com  Thu Dec  6 16:28:28 2007
From: abfriedman at gmail.com (A Friedman)
Date: Thu, 6 Dec 2007 10:28:28 -0500
Subject: [R] alternatives to latex() or xtable()
Message-ID: <1bed33b00712060728j5e35b3bana8824a5357bd91a9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/f6f33750/attachment.pl 

From johannes_graumann at web.de  Thu Dec  6 16:34:46 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 06 Dec 2007 16:34:46 +0100
Subject: [R] Building package - tab delimited example data issue
References: <fj8i94$q54$1@ger.gmane.org>
	<4757D47E.8060505@biostat.ku.dk>	<200712061303.20781.johannes_graumann@web.de>
	<fj91ei$ilk$1@ger.gmane.org> <47580F54.1000506@biostat.ku.dk>
Message-ID: <fj94qa$7n7$1@ger.gmane.org>

Peter Dalgaard wrote:

>> Answering to myself and staying with the same example:
>>         system.file("data/stroke.csv",package="ISwR")
>> allows direct access to the example file (name).
>>
>>   
> Yes, but...
> 
> This works right until you turn on LazyData for your package, then you
> end up with only
> 
> 00Index  Rdata.rdb  Rdata.rds  Rdata.rdx
> in the data directory. 
How would you do it in that case?


> Use the "inst" source subdir for files you want 
> to have installed explicitly.
Well: you provided the example ;0) - sort of ...

> 
> Also, in principle, it is
> 
> system.file("data", "stroke.csv", package="ISwR")
> 
> 
> although platforms that do not understand "/" as the path separator are
> rare nowadays.
Thanks for that hint!

Joh


From michael.allerhand at ed.ac.uk  Thu Dec  6 16:37:54 2007
From: michael.allerhand at ed.ac.uk (MikeHA)
Date: Thu, 6 Dec 2007 07:37:54 -0800 (PST)
Subject: [R] Sweave problem in Windows
In-Reply-To: <bfc676680709262229rdad555yeb9c5387f706102d@mail.gmail.com>
References: <bfc676680709262229rdad555yeb9c5387f706102d@mail.gmail.com>
Message-ID: <14194509.post@talk.nabble.com>




huang min wrote:
> 
> Hi,
> 
> I have searched the lists but still can not solve the problem. I am using
> a
> windows machine. After I sweave some Rnw file, I got a tex file. However,
> the tex file can not be compiled. I know the problem is in the line
> \usepackage{C:/PROGRA~1/R/R-25~1.1/share/texmf/Sweave} and I need to
> modify
> this line to
> \usepackage{Sweave}.
> 
> I hope there can be some automatic way to do this instead of changing this
> line in the tex file whenever I modify something. Thank you.
> 
> Huang
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

Simple workaround is to include the \usepackage{Sweave} in your .Rnw file,
after the \documentclass like this:

\documentclass[a4paper]{article}
\usepackage{Sweave}

Make sure you have all the .sty files you need in the same folder as your
.tex file.

cheers,  Mike

-- 
View this message in context: http://www.nabble.com/Sweave-problem-in-Windows-tf4527254.html#a14194509
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Thu Dec  6 16:42:18 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 6 Dec 2007 10:42:18 -0500
Subject: [R] Vertical text in a plot
In-Reply-To: <5c79da7a0712060716j3bfabdd7v7bad975de9c1b953@mail.gmail.com>
References: <5c79da7a0712060716j3bfabdd7v7bad975de9c1b953@mail.gmail.com>
Message-ID: <971536df0712060742m15f5f8a3p70ee4757232770c0@mail.gmail.com>

There is an example in the example section of

library(zoo)
?plot.zoo

starting at:
 ## plot with left and right axes


On Dec 6, 2007 10:16 AM, Marcin Kozak <nyggus at gmail.com> wrote:
> Hi,
>
> Consider this simple plot:
> > plot(1:25,runif(25,0,1),ylab="First Y-axis label",xaxt="n")
>
> I want to add an additional axis as
> > axis(4,at=seq(0.2,1,.2), labels=1:5)
>
> I have no idea how to add now the title of the new axis as "Second
> Y-axis label". I want this text to be vertically directed from bottom
> to top. I can't find the function in text() to write vertically. Any
> ideas?
>
> Thanks,
> Marcin
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From iacopetti at fastpiu.it  Thu Dec  6 17:00:56 2007
From: iacopetti at fastpiu.it (Roberto Iacopetti)
Date: Thu, 6 Dec 2007 08:00:56 -0800 (PST)
Subject: [R] R2HTML how to pair graphic.png and table
In-Reply-To: <5d897a2f0712060646h70061b72k62ce7eb23333c5fe@mail.gmail.com>
References: <14193218.post@talk.nabble.com>
	<5d897a2f0712060646h70061b72k62ce7eb23333c5fe@mail.gmail.com>
Message-ID: <14194905.post@talk.nabble.com>


Eric,

your code work well for my need,
i'm not skillful in html environment, but your functions in R2HTML give
simple many
output requirements

thanks

Roberto




Eric Lecoutre wrote:
> 
> Hi Roberto,
> 
> here is a way that presumes you know some (basic) HTML tags:
> 
> library(R2HTML)
> directory=getwd()
> myfile<-file.path(directory,"testHTML.html")
> HTMLoutput=file.path(directory,"testHTML.html")
> graf="graf.png"
> png(file.path(directory,graf))
> plot(c(1:12))
> dev.off()
> tab<-as.matrix(c(1:12))
> 
> cat("<table border=0><td width=50%>",file=HTMLoutput, append=TRUE)
>   HTMLInsertGraph(graf,file=HTMLoutput,caption="Esempio di grafico")
> cat("</td><td width=50%>",file=HTMLoutput, append=TRUE)
> HTML(tab,file=HTMLoutput)
> cat("</td></table>",file=HTMLoutput, append=TRUE)
> browseURL(myfile)
> 
> 
> I had already though I should include a function like layout for plots,
> i will have a look at that when some time is available.
> 
> Best,
> 
> Eric
> 
> 
> 
> 
> 2007/12/6, Roberto Iacopetti <iacopetti at fastpiu.it>:
>>
>>
>> Dear list,
>> i have this problem:
>> how to pair a graphic.png and a table in R2HTML ?
>>
>> The better showing of a mutiple analysis is sometimes to mate graphic and
>> table
>>
>> Can anyone help me in this task ??
>>
>> In the example below graphisc and table are subsequent and not pair..
>>
>>
>> directory=getwd()
>> myfile<-file.path(directory,"testHTML.html")
>> HTMLoutput=file.path(directory,"testHTML.html")
>> graf="graf.png"
>> png(file.path(directory,graf))
>> plot(c(1:12))
>> dev.off()
>> tab<-as.matrix(c(1:12))
>> HTMLInsertGraph(graf,tab,file=HTMLoutput,caption="Esempio di grafico")
>> browseURL(myfile)
>>
>> Thanks in advance
>>
>> Roberto Iacopetti
>>
>> --
>> View this message in context:
>> http://www.nabble.com/R2HTML--how-to-pair-graphic.png-and-table-tf4956321.html#a14193218
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> 
> -- 
> Eric Lecoutre
> Consultant - Business & Decision
> Business Intelligence & Customer Intelligence
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/R2HTML--how-to-pair-graphic.png-and-table-tf4956321.html#a14194905
Sent from the R help mailing list archive at Nabble.com.


From tkremund98 at hotmail.com  Thu Dec  6 17:04:25 2007
From: tkremund98 at hotmail.com (Todd Remund)
Date: Thu, 6 Dec 2007 09:04:25 -0700
Subject: [R] Frequency and Phase Spectrograms
Message-ID: <BAY121-W57799914A2B54F5FAB264D46F0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/89f2c691/attachment.pl 

From affysnp at gmail.com  Thu Dec  6 17:06:53 2007
From: affysnp at gmail.com (affy snp)
Date: Thu, 6 Dec 2007 11:06:53 -0500
Subject: [R] hclust in heatmap.2
In-Reply-To: <fb74d7d50712052127p2dfadfe7h5fde8292d9218b75@mail.gmail.com>
References: <5032046e0712051620l4ea3e458o7807d53a039cac16@mail.gmail.com>
	<fb74d7d50712052127p2dfadfe7h5fde8292d9218b75@mail.gmail.com>
Message-ID: <5032046e0712060806p482393e5v8aec6d23d101e543@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/86c4fa2a/attachment.pl 

From affysnp at gmail.com  Thu Dec  6 17:07:10 2007
From: affysnp at gmail.com (affy snp)
Date: Thu, 6 Dec 2007 11:07:10 -0500
Subject: [R] hclust in heatmap.2
In-Reply-To: <47580578.2090106@med.umich.edu>
References: <5032046e0712051620l4ea3e458o7807d53a039cac16@mail.gmail.com>
	<47580578.2090106@med.umich.edu>
Message-ID: <5032046e0712060807i56dc44f7nb4479e4a1c3cac48@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/08c15748/attachment.pl 

From gangchen at mail.nih.gov  Thu Dec  6 17:20:20 2007
From: gangchen at mail.nih.gov (Chen, Gang (NIH/NIMH) [C])
Date: Thu, 6 Dec 2007 11:20:20 -0500
Subject: [R] Any package for deconvolution?
Message-ID: <C2EA610984C9514BA9879086B622D0A908C8E6@nihcesmlbx2.nih.gov>

I want to run deconvolution of a time series by an impulse or point-spread function through Wiener filter, regularized filter, Lucy-Richardson method, or any other approaches. I searched the CRAN website and the mailing list archive, but could not find any package for such a deconvolution analysis. Does anybody know an existing R function for deconvolution?
 
TIA,
Gang


From sabunime at gmail.com  Thu Dec  6 17:23:03 2007
From: sabunime at gmail.com (Saeed Abu Nimeh)
Date: Thu, 6 Dec 2007 10:23:03 -0600
Subject: [R] R on a multi core unix box
Message-ID: <e8231c250712060823h62fb3178ib4a73ad1a21796fc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/153316ac/attachment.pl 

From Richard.Cotton at hsl.gov.uk  Thu Dec  6 17:46:04 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Thu, 6 Dec 2007 16:46:04 +0000
Subject: [R] Any package for deconvolution?
In-Reply-To: <C2EA610984C9514BA9879086B622D0A908C8E6@hsl.gov.uk>
Message-ID: <OFD6B14A8E.7AF74F13-ON802573A9.005BC863-802573A9.005C1BF7@hsl.gov.uk>

The RTisean package has wiener filter functions (wiener1 and wiener2).

Regards,
Richie.

Mathematical Sciences Unit
HSL




"Chen, Gang (NIH/NIMH) [C]" <gangchen at mail.nih.gov> 
Sent by: r-help-bounces at r-project.org
06/12/2007 16:20

To
<r-help at r-project.org>
cc

Subject
[R] Any package for deconvolution?






I want to run deconvolution of a time series by an impulse or point-spread 
function through Wiener filter, regularized filter, Lucy-Richardson 
method, or any other approaches. I searched the CRAN website and the 
mailing list archive, but could not find any package for such a 
deconvolution analysis. Does anybody know an existing R function for 
deconvolution?
 
TIA,
Gang

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From sheck at ucar.edu  Thu Dec  6 17:52:07 2007
From: sheck at ucar.edu (Sherri Heck)
Date: Thu, 06 Dec 2007 09:52:07 -0700
Subject: [R] creating conditional means
In-Reply-To: <971536df0712011621n12b19eaeidab396f7729ac891@mail.gmail.com>
References: <4751CAAD.2040102@ucar.edu>	
	<971536df0712011344n5dc99ec0tbf542842edf5ee9@mail.gmail.com>	
	<4751DE6D.4070605@ucar.edu>
	<971536df0712011621n12b19eaeidab396f7729ac891@mail.gmail.com>
Message-ID: <475828B7.9000903@ucar.edu>

hi gabor,

i was able to get your suggestion to work.  i have been going through 
the R help tools to figure out what each step actually does because i 
have something similar but hours 2,5,8,11,14,17 and 20 are missing.  i 
haven't had any luck.  each "mean value" that is calculated is the 
same.  i keep getting the following error:

"> DF<- read.table(textConnection(Lines), header = TRUE)
Error in read.table(textConnection(Lines), header = TRUE) :
        duplicate 'row.names' are not allowed
 >   aggregate(DF[2:4],
+    with(DF, data.frame(Year, Qtr = (Month - 3) %/% 3 + 1, Hour)),
+    mean)    #skip=hour[2,5,8,11,14]
Error in data.frame(Year, Qtr = (Month - 3)%/%3 + 1, Hour) :
        object "Year" not found
"

i am not clear why in "aggregate(DF[#:#]" that we are subsetting other 
variables besides co2.  i have been trying to just subset co2 without 
success though.
your original suggestion is below and a snippet of my data set is below 
that. if you have any ideas  or if you know of a help page that i may 
not have found yet that would be great (i've been using the "aggregate" 
help pages mostly.

thanks for your help-

s.heck



Lines <- "Year Month Hour co2 num1 num2
 2006   11    0 383.3709   28   28
 2006   11    1 383.3709   28   28
 2006   11    2 383.3709   28   28
 2006   11    3 383.3709   28   28
 2006   11    4 383.3709   28   28
 2006   11    5 383.3709   28   28
 2006   11    6 383.3709   28   28
 2006   11    7 383.3709   28   28
 2006   11    8 383.3709   28   28
 2006   11    9 383.3709   27   27
 2006   11   10 383.3709   28   28
"
DF <- read.table(textConnection(Lines), header = TRUE)
aggregate(DF[4:6],
   with(DF, data.frame(Year, Qtr = (Month - 1) %/% 3 + 1, Hour)),
   mean)			#skip=hour[2,5,8,11,14,17,20]???


 



Year Month Hour co2
2005    1    0    386.1600708
2005    1    1    386.823056
2005    1    3    387.1335939
2005    1    4    387.0681103
2005    1    6    387.4750983
2005    1    7    388.3398313
2005    1    9    388.7545317
2005    1    10    388.0844451
2005    1    12    386.7929627
2005    1    13    385.5569521
2005    1    15    384.5523752
2005    1    16    385.0246721
2005    1    18    385.8646669
2005    1    19    386.2182493
2005    1    21    386.4820756
2005    1    22    386.6606276
2005    2    0    386.6791667
2005    2    1    386.6597544
2005    2    3    386.5725303
2005    2    4    387.0638611
2005    2    6    387.9293508
2005    2    7    388.3778991
2005    2    9    388.3721947
2005    2    10    387.8324642
2005    2    12    386.8404892
2005    2    13    385.6770345
2005    2    15    384.4798484
2005    2    16    384.6214677
2005    2    18    384.3044105
2005    2    19    383.3018709
2005    2    21    382.5837339
2005    2    22    382.2658036

Gabor Grothendieck wrote:
> Just adjust the formula for Qtr appropriately if your quarters
> are not Jan/Feb/Mar, Apr/May/Jun, Jul/Aug/Sep, Oct/Nov/Dec
> as I assumed.
>
> On Dec 1, 2007 5:21 PM, Sherri Heck <sheck at ucar.edu> wrote:
>   
>> Hi Gabor,
>>
>> Thank you for your help.  I think I need to clarify a bit more.  I am
>> trying to say
>>
>> average all 2pms for months march + april + may (for example). I hope this is clearer.
>>
>> here's a larger subset of my data set:
>>
>> year, month, hour, co2(ppm), num1,num2
>>
>> 2006 1 0 384.2055 14 14
>> 2006 1 1 384.0304 14 14
>> 2006 1 2 383.9672 14 14
>> 2006 1 3 383.8452 14 14
>> 2006 1 4 383.8594 14 14
>> 2006 1 5 383.7318 14 14
>> 2006 1 6 383.6439 14 14
>> 2006 1 7 383.7019 14 14
>> 2006 1 8 383.7487 14 14
>> 2006 1 9 383.8376 14 14
>> 2006 1 10 383.8684 14 14
>> 2006 1 11 383.8301 14 14
>> 2006 1 12 383.8058 14 14
>> 2006 1 13 383.9419 14 14
>> 2006 1 14 383.7876 14 14
>> 2006 1 15 383.7744 14 14
>> 2006 1 16 383.8566 14 14
>> 2006 1 17 384.1014 14 14
>> 2006 1 18 384.1312 14 14
>> 2006 1 19 384.1551 14 14
>> 2006 1 20 384.099 14 14
>> 2006 1 21 384.1408 14 14
>> 2006 1 22 384.3637 14 14
>> 2006 1 23 384.1491 14 14
>> 2006 2 0 384.7082 27 27
>> 2006 2 1 384.6139 27 27
>> 2006 2 2 384.7453 26 26
>> 2006 2 3 384.9224 28 28
>> 2006 2 4 384.8581 28 28
>> 2006 2 5 384.9208 28 28
>> 2006 2 6 384.9086 28 28
>> 2006 2 7 384.837 28 28
>> 2006 2 8 384.6163 27 27
>> 2006 2 9 384.7406 28 28
>> 2006 2 10 384.7468 28 28
>> 2006 2 11 384.6992 28 28
>> 2006 2 12 384.6388 28 28
>> 2006 2 13 384.6346 28 28
>> 2006 2 14 384.6037 28 28
>> 2006 2 15 384.5295 28 28
>> 2006 2 16 384.5654 28 28
>> 2006 2 17 384.6466 28 28
>> 2006 2 18 384.6344 28 28
>> 2006 2 19 384.5911 28 28
>> 2006 2 20 384.6084 28 28
>> 2006 2 21 384.6318 28 28
>> 2006 2 22 384.6181 27 27
>> 2006 2 23 384.6087 27 27
>>
>>
>> thanks you again for your assistance-
>>
>> s.heck
>>
>>
>>
>> Gabor Grothendieck wrote:
>>     
>>> Try aggregate:
>>>
>>>
>>> Lines <- "Year Month Hour co2 num1 num2
>>>  2006   11    0 383.3709   28   28
>>>  2006   11    1 383.3709   28   28
>>>  2006   11    2 383.3709   28   28
>>>  2006   11    3 383.3709   28   28
>>>  2006   11    4 383.3709   28   28
>>>  2006   11    5 383.3709   28   28
>>>  2006   11    6 383.3709   28   28
>>>  2006   11    7 383.3709   28   28
>>>  2006   11    8 383.3709   28   28
>>>  2006   11    9 383.3709   27   27
>>>  2006   11   10 383.3709   28   28
>>> "
>>> DF <- read.table(textConnection(Lines), header = TRUE)
>>> aggregate(DF[4:6],
>>>    with(DF, data.frame(Year, Qtr = (Month - 1) %/% 3 + 1, Hour)),
>>>    mean)
>>>
>>> On Dec 1, 2007 3:57 PM, Sherri Heck <sheck at ucar.edu> wrote:
>>>
>>>       
>>>> Hi all-
>>>>
>>>> I have a dataset (year, month, hour, co2(ppm), num1,num2)
>>>>
>>>>
>>>> [49,] 2006   11    0 383.3709   28   28
>>>> [50,] 2006   11    1 383.3709   28   28
>>>> [51,] 2006   11    2 383.3709   28   28
>>>> [52,] 2006   11    3 383.3709   28   28
>>>> [53,] 2006   11    4 383.3709   28   28
>>>> [54,] 2006   11    5 383.3709   28   28
>>>> [55,] 2006   11    6 383.3709   28   28
>>>> [56,] 2006   11    7 383.3709   28   28
>>>> [57,] 2006   11    8 383.3709   28   28
>>>> [58,] 2006   11    9 383.3709   27   27
>>>> [59,] 2006   11   10 383.3709   28   28
>>>>
>>>> that repeats in this style for each month.  I would like to compute the
>>>> mean for each hour in three month intervals.
>>>> i.e.  average all 2pms for each day for months march, april and may. and
>>>> then do this for each hour interval.
>>>> i have been messing around with 'for loops' but can't seem to get the
>>>> output I want.
>>>>
>>>> thanks in advance for any help-
>>>>
>>>> s.heck
>>>> CU, Boulder
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>>


From deepayan.sarkar at gmail.com  Thu Dec  6 17:53:52 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 6 Dec 2007 08:53:52 -0800
Subject: [R] Using panel.densityplot with stripplot
In-Reply-To: <op.t2wpbrwhmknj4h@thimphu.pcpool.mi.fu-berlin.de>
References: <op.t2wl071ymknj4h@thimphu.pcpool.mi.fu-berlin.de>
	<eb555e660712060020n169ebd6fmf50f6356bf23d385@mail.gmail.com>
	<op.t2wpbrwhmknj4h@thimphu.pcpool.mi.fu-berlin.de>
Message-ID: <eb555e660712060853w6d978770ye4cc0331ae092e1c@mail.gmail.com>

On 12/6/07, Christopher Oezbek <oezbek at inf.fu-berlin.de> wrote:

> But is there a technical reason for making this distinction? It seemed to
> me that most panel functions can deal with x and y parameters and that
> panel.densityplot could make use of the same mechanisms as panel.violin to
> subdivide based on factor y.

The technical reason is a fundamental part of the Trellis design. The
prepanel function determines a bounding box as part of the initial
calculations. Knowing this in advance allows the panel display and the
axis rendering to be completely separated.

Note that the panel.violin plot cannot easily say anything about the
actual heights of the densities, only their shape. The y-axis there
annotates the sample groups.

-Deepayan


From therneau at mayo.edu  Thu Dec  6 17:58:03 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Thu, 6 Dec 2007 10:58:03 -0600 (CST)
Subject: [R] coxme frailty model standard errors?
Message-ID: <200712061658.lB6Gw3F09575@hsrnfs-101.mayo.edu>

--- begin included message
I am running R 2.6.1 on windows xp
I am trying to fit a cox proportional hazard model with a shared
Gaussian frailty term using coxme My model is specified as:

nofit1<-coxme(Surv(Age,cen1new)~ Sex+bo2+bo3,random=~1|isl,data=mydat)

With x1-x3 being dummy variables, and isl being the community level
variable with 4 levels.

Does anyone know if there is a way to get the standard error for the
random effect, like in nofit1$var?  I would like to know if my random
effect is worth writing home about.

-- end included message

  Computation of the se of the random effect turns out to be very hard, and 
worse it isn't worth much when you have done so.  For both these reasons coxme 
doesn't even try (and it's not on the list of things to add).
  
   First, you can do a likelihood ratio test by comparing the fit to a coxph 
model that does not have the random effect.   Make sure that the null 
loglikelihood for the two fits are the same though!  (For instance, one obs was 
missing the "isl" variable above, so the two fits have differnt n).  

     fit1<-  coxme(Surv(Age,cen1new)~ Sex+bo2+bo3,random=~1|isl, data=mydat)  
     fit2<- coxph(Surv(Age,cen1new)~ Sex+bo2+bo3, data=mydat)
     fit1$loglik[1:2] - fit2$loglik	
     
The first number printed should be 0, twice the second is distributed as a chisq 
on "number of random effects" degrees of freedom.  (Not quite.  The chisq test 
is actually conservative since the random effect is constrained to be >=0.  But 
it is close, and I'll let someone else work out the details)

   Second, you can print a profile likelihood confidence interval.  You had a 
random effects variance of about .4, so make a guess that the confidence 
interval is somewhere between 0 and 2.
     vtemp <- seq(0.0001, 2, length=20) 
     ltemp <- 0*vtemp
     for (i in 1:length(vtemp)) {
     	tfit <- coxme(Surv(Age,cen1new)~ Sex+bo2+bo3,random=~1|isl, data=mydat,
     		    variance= vtemp[i])
        ltemp <- 2 * diff(tfit$loglik[1:2])
        }
     plot(vtemp, ltemp, xlab='Variance', ylab="LR test")
     abline(h= 2*diff(fit1$loglik[1:2]) - 3.84)
     
   The sequence of fits each have a fixed value for the variance of the random 
effect; the plot shows the profile likelihood.  The profile is often very 
asymmetric (which is why the se of the random effect isn't worth much).  The 
intersection of the profile with a line 3.84 units down (chisq on 1df) is the 
profile based 95% confidence interval. 
   
   	Terry Therneau


From rcl7820 at warnell.uga.edu  Thu Dec  6 18:02:25 2007
From: rcl7820 at warnell.uga.edu (TLowe)
Date: Thu, 6 Dec 2007 09:02:25 -0800 (PST)
Subject: [R]  Help rewriting looping structure?
Message-ID: <14196412.post@talk.nabble.com>


Hey Folks,

Could somebody help me rewrite the following code?

I am looping through all records across 5 fields to calculate the cumulative
percentage of each record (relative to each individual field).

Is there a way to rewrite it so I don't have to loop through each individual
record?

##### tdat is my data frame
##### j is my field index
##### k is my record index
##### tsum is the sum of all values in field j
##### tmp is a vector containing the values in field j
##### tdat[k,paste("cpct,j,sep="")] creates new fields "cpct1",...,"cpct5" 


for(j in 1:5) {
  tsum<- sum(tdat[,j]);
  for(k in 1:nrow(tdat)) {
    td<- tdat[k,j];
    tmp<-tdat[,j];
##### sum values <= to current value and divide by the total sum
    tdat[k,paste("cpct,j,sep="")]<- sum(tmp[tmp <= td]) / tsum;
  }
}


Thanks,
TLowe
-- 
View this message in context: http://www.nabble.com/Help-rewriting-looping-structure--tf4957267.html#a14196412
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Thu Dec  6 18:05:37 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 6 Dec 2007 12:05:37 -0500
Subject: [R] creating conditional means
In-Reply-To: <475828B7.9000903@ucar.edu>
References: <4751CAAD.2040102@ucar.edu>
	<971536df0712011344n5dc99ec0tbf542842edf5ee9@mail.gmail.com>
	<4751DE6D.4070605@ucar.edu>
	<971536df0712011621n12b19eaeidab396f7729ac891@mail.gmail.com>
	<475828B7.9000903@ucar.edu>
Message-ID: <971536df0712060905g1b315de4h2a71ca2b056395fc@mail.gmail.com>

The error message says you have duplicate row names and that
is not allowed.  Make sure you have the same number of elements
on each line of data as in the header.  If you have one more on each line
than on the header then the first data item on each line will be regarded
as the row name.  See ?count.fields

The rest of your message is not clear.


On Dec 6, 2007 11:52 AM, Sherri Heck <sheck at ucar.edu> wrote:
> hi gabor,
>
> i was able to get your suggestion to work.  i have been going through
> the R help tools to figure out what each step actually does because i
> have something similar but hours 2,5,8,11,14,17 and 20 are missing.  i
> haven't had any luck.  each "mean value" that is calculated is the
> same.  i keep getting the following error:
>
> "> DF<- read.table(textConnection(Lines), header = TRUE)
> Error in read.table(textConnection(Lines), header = TRUE) :
>        duplicate 'row.names' are not allowed
>  >   aggregate(DF[2:4],
> +    with(DF, data.frame(Year, Qtr = (Month - 3) %/% 3 + 1, Hour)),
> +    mean)    #skip=hour[2,5,8,11,14]
> Error in data.frame(Year, Qtr = (Month - 3)%/%3 + 1, Hour) :
>        object "Year" not found
> "
>
> i am not clear why in "aggregate(DF[#:#]" that we are subsetting other
> variables besides co2.  i have been trying to just subset co2 without
> success though.
> your original suggestion is below and a snippet of my data set is below
> that. if you have any ideas  or if you know of a help page that i may
> not have found yet that would be great (i've been using the "aggregate"
> help pages mostly.
>
> thanks for your help-
>
> s.heck
>
>
>
> Lines <- "Year Month Hour co2 num1 num2
>  2006   11    0 383.3709   28   28
>  2006   11    1 383.3709   28   28
>  2006   11    2 383.3709   28   28
>  2006   11    3 383.3709   28   28
>  2006   11    4 383.3709   28   28
>  2006   11    5 383.3709   28   28
>  2006   11    6 383.3709   28   28
>  2006   11    7 383.3709   28   28
>  2006   11    8 383.3709   28   28
>  2006   11    9 383.3709   27   27
>  2006   11   10 383.3709   28   28
> "
> DF <- read.table(textConnection(Lines), header = TRUE)
> aggregate(DF[4:6],
>   with(DF, data.frame(Year, Qtr = (Month - 1) %/% 3 + 1, Hour)),
>   mean)                        #skip=hour[2,5,8,11,14,17,20]???
>
>
>
>
>
>
> Year Month Hour co2
> 2005    1    0    386.1600708
> 2005    1    1    386.823056
> 2005    1    3    387.1335939
> 2005    1    4    387.0681103
> 2005    1    6    387.4750983
> 2005    1    7    388.3398313
> 2005    1    9    388.7545317
> 2005    1    10    388.0844451
> 2005    1    12    386.7929627
> 2005    1    13    385.5569521
> 2005    1    15    384.5523752
> 2005    1    16    385.0246721
> 2005    1    18    385.8646669
> 2005    1    19    386.2182493
> 2005    1    21    386.4820756
> 2005    1    22    386.6606276
> 2005    2    0    386.6791667
> 2005    2    1    386.6597544
> 2005    2    3    386.5725303
> 2005    2    4    387.0638611
> 2005    2    6    387.9293508
> 2005    2    7    388.3778991
> 2005    2    9    388.3721947
> 2005    2    10    387.8324642
> 2005    2    12    386.8404892
> 2005    2    13    385.6770345
> 2005    2    15    384.4798484
> 2005    2    16    384.6214677
> 2005    2    18    384.3044105
> 2005    2    19    383.3018709
> 2005    2    21    382.5837339
> 2005    2    22    382.2658036
>
>
> Gabor Grothendieck wrote:
> > Just adjust the formula for Qtr appropriately if your quarters
> > are not Jan/Feb/Mar, Apr/May/Jun, Jul/Aug/Sep, Oct/Nov/Dec
> > as I assumed.
> >
> > On Dec 1, 2007 5:21 PM, Sherri Heck <sheck at ucar.edu> wrote:
> >
> >> Hi Gabor,
> >>
> >> Thank you for your help.  I think I need to clarify a bit more.  I am
> >> trying to say
> >>
> >> average all 2pms for months march + april + may (for example). I hope this is clearer.
> >>
> >> here's a larger subset of my data set:
> >>
> >> year, month, hour, co2(ppm), num1,num2
> >>
> >> 2006 1 0 384.2055 14 14
> >> 2006 1 1 384.0304 14 14
> >> 2006 1 2 383.9672 14 14
> >> 2006 1 3 383.8452 14 14
> >> 2006 1 4 383.8594 14 14
> >> 2006 1 5 383.7318 14 14
> >> 2006 1 6 383.6439 14 14
> >> 2006 1 7 383.7019 14 14
> >> 2006 1 8 383.7487 14 14
> >> 2006 1 9 383.8376 14 14
> >> 2006 1 10 383.8684 14 14
> >> 2006 1 11 383.8301 14 14
> >> 2006 1 12 383.8058 14 14
> >> 2006 1 13 383.9419 14 14
> >> 2006 1 14 383.7876 14 14
> >> 2006 1 15 383.7744 14 14
> >> 2006 1 16 383.8566 14 14
> >> 2006 1 17 384.1014 14 14
> >> 2006 1 18 384.1312 14 14
> >> 2006 1 19 384.1551 14 14
> >> 2006 1 20 384.099 14 14
> >> 2006 1 21 384.1408 14 14
> >> 2006 1 22 384.3637 14 14
> >> 2006 1 23 384.1491 14 14
> >> 2006 2 0 384.7082 27 27
> >> 2006 2 1 384.6139 27 27
> >> 2006 2 2 384.7453 26 26
> >> 2006 2 3 384.9224 28 28
> >> 2006 2 4 384.8581 28 28
> >> 2006 2 5 384.9208 28 28
> >> 2006 2 6 384.9086 28 28
> >> 2006 2 7 384.837 28 28
> >> 2006 2 8 384.6163 27 27
> >> 2006 2 9 384.7406 28 28
> >> 2006 2 10 384.7468 28 28
> >> 2006 2 11 384.6992 28 28
> >> 2006 2 12 384.6388 28 28
> >> 2006 2 13 384.6346 28 28
> >> 2006 2 14 384.6037 28 28
> >> 2006 2 15 384.5295 28 28
> >> 2006 2 16 384.5654 28 28
> >> 2006 2 17 384.6466 28 28
> >> 2006 2 18 384.6344 28 28
> >> 2006 2 19 384.5911 28 28
> >> 2006 2 20 384.6084 28 28
> >> 2006 2 21 384.6318 28 28
> >> 2006 2 22 384.6181 27 27
> >> 2006 2 23 384.6087 27 27
> >>
> >>
> >> thanks you again for your assistance-
> >>
> >> s.heck
> >>
> >>
> >>
> >> Gabor Grothendieck wrote:
> >>
> >>> Try aggregate:
> >>>
> >>>
> >>> Lines <- "Year Month Hour co2 num1 num2
> >>>  2006   11    0 383.3709   28   28
> >>>  2006   11    1 383.3709   28   28
> >>>  2006   11    2 383.3709   28   28
> >>>  2006   11    3 383.3709   28   28
> >>>  2006   11    4 383.3709   28   28
> >>>  2006   11    5 383.3709   28   28
> >>>  2006   11    6 383.3709   28   28
> >>>  2006   11    7 383.3709   28   28
> >>>  2006   11    8 383.3709   28   28
> >>>  2006   11    9 383.3709   27   27
> >>>  2006   11   10 383.3709   28   28
> >>> "
> >>> DF <- read.table(textConnection(Lines), header = TRUE)
> >>> aggregate(DF[4:6],
> >>>    with(DF, data.frame(Year, Qtr = (Month - 1) %/% 3 + 1, Hour)),
> >>>    mean)
> >>>
> >>> On Dec 1, 2007 3:57 PM, Sherri Heck <sheck at ucar.edu> wrote:
> >>>
> >>>
> >>>> Hi all-
> >>>>
> >>>> I have a dataset (year, month, hour, co2(ppm), num1,num2)
> >>>>
> >>>>
> >>>> [49,] 2006   11    0 383.3709   28   28
> >>>> [50,] 2006   11    1 383.3709   28   28
> >>>> [51,] 2006   11    2 383.3709   28   28
> >>>> [52,] 2006   11    3 383.3709   28   28
> >>>> [53,] 2006   11    4 383.3709   28   28
> >>>> [54,] 2006   11    5 383.3709   28   28
> >>>> [55,] 2006   11    6 383.3709   28   28
> >>>> [56,] 2006   11    7 383.3709   28   28
> >>>> [57,] 2006   11    8 383.3709   28   28
> >>>> [58,] 2006   11    9 383.3709   27   27
> >>>> [59,] 2006   11   10 383.3709   28   28
> >>>>
> >>>> that repeats in this style for each month.  I would like to compute the
> >>>> mean for each hour in three month intervals.
> >>>> i.e.  average all 2pms for each day for months march, april and may. and
> >>>> then do this for each hour interval.
> >>>> i have been messing around with 'for loops' but can't seem to get the
> >>>> output I want.
> >>>>
> >>>> thanks in advance for any help-
> >>>>
> >>>> s.heck
> >>>> CU, Boulder
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>>
> >>>>
>


From timh at insightful.com  Thu Dec  6 18:08:20 2007
From: timh at insightful.com (Tim Hesterberg)
Date: Thu, 06 Dec 2007 09:08:20 -0800
Subject: [R] Bootstrap Correlation Coefficient with Moving
	Block	Bootstrap
In-Reply-To: <164703.2350.qm@web30604.mail.mud.yahoo.com> (message from
	Andreas Klein on Thu, 29 Nov 2007 16:22:24 +0100 (CET))
References: <164703.2350.qm@web30604.mail.mud.yahoo.com>
Message-ID: <ufxyf69a3.fsf@insightful.com>

It sounds like you should sample x and y together using the
block bootstrap.  If you have the usual situation, x and y in columns
and observations in rows, then sample blocks of rows.

Even though observations in y are independent, you would take
advantage of that only for bootstrapping statistics that depend only
on y.

The answer to your second question is the same as the first - sample
blocks of observations, keeping x and y together.

Tim Hesterberg					

>Hello.
>
>I have got two problems in bootstrapping from
>dependent data sets.
>
>Given two time-series x and y. Both consisting of n
>observations with x consisting of dependent and y
>consisting of independent observations over time. Also
>assume, that the optimal block-length l is given.
>
>To obtain my bootstrap sample, I have to draw
>pairwise, but there is the problem of dependence of
>the x-observations and so if I draw the third
>observation of y, I cannot simply draw the third
>observation of x (to retain the serial correlation
>structure between x and y), because I devided x into
>blocks of length l and I have to draw blocks, then I
>draw from x.
>
>1.
>How can I compute a bootstrap sample of the
>correlation coefficient between x and y with respect
>to the dependence in time-series of x?
>
>2.
>How does it look like, if x and y both consist of
>dependent observations?
>
>
>
>I hope you can help me. I got really stuck with this
>problem.
>
>Sincerly
>Klein.


From thomas.pujol at yahoo.com  Thu Dec  6 18:10:23 2007
From: thomas.pujol at yahoo.com (Thomas Pujol)
Date: Thu, 6 Dec 2007 09:10:23 -0800 (PST)
Subject: [R] using "eval(parse(text)) "  , gsub(pattern, replacement, x) ,
	to process "code" within a loop/custom function
Message-ID: <346709.39944.qm@web59302.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/af09443f/attachment.pl 

From Greg.Snow at intermountainmail.org  Thu Dec  6 18:18:51 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 6 Dec 2007 10:18:51 -0700
Subject: [R] Using expression in Hmisc Key()
In-Reply-To: <184E1205-7AAD-4CEF-A0FC-083B6D6E7BCC@virginia.edu>
References: <55B194DA-5A84-4BCD-8869-C55D47FDB695@virginia.edu><loom.20071206T071522-217@post.gmane.org>
	<184E1205-7AAD-4CEF-A0FC-083B6D6E7BCC@virginia.edu>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBD624C9@LP-EXCHVS07.CO.IHC.COM>

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Michael Kubovy
> Sent: Thursday, December 06, 2007 2:12 AM
> To: Dieter Menne
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Using expression in Hmisc Key()
> 
[snip]
> This is one of the cases where I wish there were a function 
> that "exercised" all the arguments of a graphics function by 
> visualizing the effect of changing  two or three levels of 
> each argument (one by one, of course). This might have the 
> side effect of allowing authors to shorten documentation. I 
> have in mind adding a method to each graphics function, and 
> having the author specify the two or three instructive levels 
> of each argument. See the exhaustive exploration by J.R. 
> Lobry, A.B. Dufour & D. Chessel of the graphical parameters 
> accessible by par(), at http://pbil.univ-lyon1.fr/R/fichestd/tdr75.pdf

There are some tools that already exist to help with creating these
types of visualization tools.  The playwith package appears to be one
such set of tools (I have not played with it much, so don't expect
details from me).

The tkrplot package is another one (I am more familiar with this one).
The slider function (relax and TeachingDemos package) and sliderv
function (TeachingDemos) also help here.  Try running the example for
the sliderv function and move the sliders around to see the effects.
(one of these days I am going to rewrite many of the demos in
TeachingDemos to use tkrplot)

Yes, it would be nice to have a general function that could look at the
arguments of a function, but this is not an easy task, for example, how
many arguments can you pass to the plot function?  Also which arguments
should use slider bars and which text widgets? (radio buttons are a bit
easier).  One option (that I think you are possibly suggesting above) is
to have each writer of a function include some type of metadata that a
general function could query and use to make decisions about what type
of control to use for that option including possible ranges of values.
I'll think some more about this approach.  I have plans to add some more
graphical demos to the TeachingDemos package that would do what you want
with subsets of the par arguments (there are too many to do all at once,
but one function could let you modify plotting type, cex, lwd, lty, col,
and similar options, it gets tricky when mar and mai both modify the
same thing in different ways and both take vectors rather than a single
number).

Some functions have arguments that depend on each other, making the GUI
for these cases is a bit more dificult in that you need to build in
those relationships (see the vis.gamma function in TeachingDemos for one
example, the rate and scale arguments are 2 different ways of specifying
the same thing).

One option (I would be willing to put this into TeachingDemos, or it may
want its own package at some point) is to write a general function that
takes a plotting expression/function as the first argument and a list as
the 2nd argument.  The list would include information on which type of
widget (slider, radio buttons, entry, ...) to use, what the range of
values are, and which argument to the plotting function they correspond
to.  The function would then create a gui (using tkrplot would be my
preference) that would modify those values.  Any function author that
wants users to be able to use this with their function just needs to
provide the list and possibly a not run example in the docs (and we can
create several predone lists in the package for common plotting
functions and subsets of "par" arguments).

I'm adding this to my list of bus projects to think about.

Any other thoughts,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 


From gangchen at mail.nih.gov  Thu Dec  6 18:22:29 2007
From: gangchen at mail.nih.gov (Chen, Gang (NIH/NIMH) [C])
Date: Thu, 6 Dec 2007 12:22:29 -0500
Subject: [R] Any package for deconvolution?
References: <OFD6B14A8E.7AF74F13-ON802573A9.005BC863-802573A9.005C1BF7@hsl.gov.uk>
Message-ID: <C2EA610984C9514BA9879086B622D0A908C8EB@nihcesmlbx2.nih.gov>

Thanks a lot for the quick pointer, Richie. I will take a close look of the RTisean package.
 
Gang

________________________________

From: Richard.Cotton at hsl.gov.uk [mailto:Richard.Cotton at hsl.gov.uk]
Sent: Thu 12/6/2007 11:46 AM
To: Chen, Gang (NIH/NIMH) [C]
Cc: r-help at r-project.org
Subject: Re: [R] Any package for deconvolution?



The RTisean package has wiener filter functions (wiener1 and wiener2).

Regards,
Richie.

Mathematical Sciences Unit
HSL




"Chen, Gang (NIH/NIMH) [C]" <gangchen at mail.nih.gov>
Sent by: r-help-bounces at r-project.org
06/12/2007 16:20

To
<r-help at r-project.org>
cc

Subject
[R] Any package for deconvolution?






I want to run deconvolution of a time series by an impulse or point-spread
function through Wiener filter, regularized filter, Lucy-Richardson
method, or any other approaches. I searched the CRAN website and the
mailing list archive, but could not find any package for such a
deconvolution analysis. Does anybody know an existing R function for
deconvolution?

TIA,
Gang

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:21}}


From murdoch at stats.uwo.ca  Thu Dec  6 19:11:15 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 06 Dec 2007 13:11:15 -0500
Subject: [R] differences in using source() or console
In-Reply-To: <4757F2A9.8040300@dssm.unipa.it>
References: <4757F2A9.8040300@dssm.unipa.it>
Message-ID: <47583B43.5020804@stats.uwo.ca>

On 12/6/2007 8:01 AM, vito muggeo wrote:
> Dear all,
> Is there *any* reason explaining what I describe below?

Probably, but you're unlikely to get a correct explanation if you don't 
give us code to reproduce the problem.

 From the look of things, your real example is quite complex, which is 
probably why you didn't post it.  But the very act of simplifying it to 
something that is reasonable to post is quite likely to reveal to you 
what the problem is.

Duncan Murdoch

> I have the following line
> 
> myfun(x)
> 
> If I type them directly in R (or copy/past), it works..
> 
> However if I type in R 2.6.1
> 
>  > source("code.R") ##code.R includes the above line
> Error in inherits(x, "data.frame") : object "d" not found
> 
> namely myfun() does not work correctly.
> In particular the non-working line inside myfun() is
> 
> update(eval(x$call$obj), data=d) #d is created in myfun()
> 
> My question is: why the problem occurs just via source()ing???
> 
> many thanks,
> vito
> 
> 
> ====================================
> Vito M.R. Muggeo
> Dip.to Sc Statist e Matem `Vianelli'
> Universit? di Palermo
> viale delle Scienze, edificio 13
> 90128 Palermo - ITALY
> tel: 091 6626240
> fax: 091 485726/485612
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Thu Dec  6 19:11:53 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 06 Dec 2007 12:11:53 -0600
Subject: [R] logistic regression using "glm",which "y" is set to be "1"
In-Reply-To: <14187112.post@talk.nabble.com>
References: <14185060.post@talk.nabble.com>
	<1196909273.2962.12.camel@Bellerophon.localdomain>
	<14185819.post@talk.nabble.com>
	<cdf817830712051954y717e778dn4d7d4faa99b36f61@mail.gmail.com>
	<14187112.post@talk.nabble.com>
Message-ID: <1196964713.2955.71.camel@Bellerophon.localdomain>


On Wed, 2007-12-05 at 22:33 -0800, Bin Yue wrote:
> Dear all:
>      By comparing glmresult$y and model.response(model.frame(glmresult)),  I
> have found out which one is 
> set to be "TRUE" and which "FALSE".But it seems that to fit a logistic
> regression , logit (or logistic) transformation has to be done before
> regression.
>      Does anybody know how to obtain the transformation result ? It is hard
> to settle down before knowing the actual process R works . I have read some
> books and the "?glm" help file , but what they told me was not sufficient.
>    Best wishes ,
>  Bin Yue

Bin,

I may be mis-interpreting your follow up query, but here goes:

You have presumably created a logistic regression model. The resultant
model object is called 'glmresult'.

If you use:

  fitted(glmresult)

it will return the fitted predicted values on a probability scale (0 -
1) for the original set of data that you used.

You can also use:

  predict(glmresult, type = "response")

The advantage of using predict.glm() is that you can apply the model
against new data.


If you want the linear predicted values on a log-odds scale, you can
use:

  glmresult$linear.predictors

or more easily:

  predict(glmresult)

See ?fitted and ?predict.glm for more information.


Let's use an example from ?infert:

model1 <- glm(case ~ spontaneous+induced, data=infert,family=binomial())

# Summary of fitted values on a probability scale
> summary(fitted(model1))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.1534  0.1534  0.2949  0.3347  0.3750  0.7511 


# Same
> summary(predict(model1, type = "response"))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.1534  0.1534  0.2949  0.3347  0.3750  0.7511 


# Get log-odds scale values
> summary(model1$linear.predictors)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.7080 -1.7080 -0.8716 -0.7781 -0.5107  1.1050


# Same
> summary(predict(model1))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.7080 -1.7080 -0.8716 -0.7781 -0.5107  1.1050


If we wanted to do the log-odds scale to probability scale transform
manually, we could do:

> summary(exp(predict(model1)) / (1 + exp(predict(model1))))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.1534  0.1534  0.2949  0.3347  0.3750  0.7511 

Look familiar?

I would urge you to read through An Introduction To R, which is
available with your R installation or via the R web site under
Documentation. In addition, there are various books listed on the R web
site regarding model building and related subject matter. Which you
choose can be a matter of taste, but two I recommend would be:

William N. Venables and Brian D. Ripley. Modern Applied Statistics with
S. Fourth Edition. Springer, New York, 2002. ISBN 0-387-95457-0

Frank E. Harrell. Regression Modeling Strategies, with Applications to
Linear Models, Survival Analysis and Logistic Regression. Springer,
2001. ISBN 0-387-95232-2

HTH,

Marc Schwartz


From iverson at biostat.wisc.edu  Thu Dec  6 19:20:19 2007
From: iverson at biostat.wisc.edu (Erik Iverson)
Date: Thu, 06 Dec 2007 12:20:19 -0600
Subject: [R] Help rewriting looping structure?
In-Reply-To: <14196412.post@talk.nabble.com>
References: <14196412.post@talk.nabble.com>
Message-ID: <47583D63.9000705@biostat.wisc.edu>

How about this example?

## sample data frame with two columns
df <- data.frame(x = abs(rnorm(20)), y=abs(rnorm(20,2)))

## create new variables in df with an lapply call
df[c("cpctx","cptcty")] <- lapply(df, function(x) cumsum(x)/sum(x))

A possible improvement would be to construct the new column names in
the data frame automatically.

Best,
Erik
TLowe wrote:
> Hey Folks,
> 
> Could somebody help me rewrite the following code?
> 
> I am looping through all records across 5 fields to calculate the cumulative
> percentage of each record (relative to each individual field).
> 
> Is there a way to rewrite it so I don't have to loop through each individual
> record?
> 
> ##### tdat is my data frame
> ##### j is my field index
> ##### k is my record index
> ##### tsum is the sum of all values in field j
> ##### tmp is a vector containing the values in field j
> ##### tdat[k,paste("cpct,j,sep="")] creates new fields "cpct1",...,"cpct5" 
> 
> 
> for(j in 1:5) {
>   tsum<- sum(tdat[,j]);
>   for(k in 1:nrow(tdat)) {
>     td<- tdat[k,j];
>     tmp<-tdat[,j];
> ##### sum values <= to current value and divide by the total sum
>     tdat[k,paste("cpct,j,sep="")]<- sum(tmp[tmp <= td]) / tsum;
>   }
> }
> 
> 
> Thanks,
> TLowe


From jholtman at gmail.com  Thu Dec  6 19:23:28 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 6 Dec 2007 10:23:28 -0800
Subject: [R] Help rewriting looping structure?
In-Reply-To: <14196412.post@talk.nabble.com>
References: <14196412.post@talk.nabble.com>
Message-ID: <644e1f320712061023q6939f90ay7192e125eb4c7b39@mail.gmail.com>

Is this basically what you want to do? (Please include commented,
minimal, self-contained, reproducible code so we don't have to guess
at what you want)

> x <- data.frame(a=runif(10), b=runif(10))
# do for one column
> cumsum(x$a)/sum(x$a)
 [1] 0.05892073 0.08129611 0.11067218 0.28640268 0.28969826 0.44477544
0.55195101 0.76500220 0.85234025
[10] 1.00000000
>

If this is the case, you can extend it.

On Dec 6, 2007 9:02 AM, TLowe <rcl7820 at warnell.uga.edu> wrote:
>
> Hey Folks,
>
> Could somebody help me rewrite the following code?
>
> I am looping through all records across 5 fields to calculate the cumulative
> percentage of each record (relative to each individual field).
>
> Is there a way to rewrite it so I don't have to loop through each individual
> record?
>
> ##### tdat is my data frame
> ##### j is my field index
> ##### k is my record index
> ##### tsum is the sum of all values in field j
> ##### tmp is a vector containing the values in field j
> ##### tdat[k,paste("cpct,j,sep="")] creates new fields "cpct1",...,"cpct5"
>
>
> for(j in 1:5) {
>  tsum<- sum(tdat[,j]);
>  for(k in 1:nrow(tdat)) {
>    td<- tdat[k,j];
>    tmp<-tdat[,j];
> ##### sum values <= to current value and divide by the total sum
>    tdat[k,paste("cpct,j,sep="")]<- sum(tmp[tmp <= td]) / tsum;
>  }
> }
>
>
> Thanks,
> TLowe
> --
> View this message in context: http://www.nabble.com/Help-rewriting-looping-structure--tf4957267.html#a14196412
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From rcl7820 at warnell.uga.edu  Thu Dec  6 19:28:10 2007
From: rcl7820 at warnell.uga.edu (TLowe)
Date: Thu, 6 Dec 2007 10:28:10 -0800 (PST)
Subject: [R] Help rewriting looping structure?
In-Reply-To: <14196412.post@talk.nabble.com>
References: <14196412.post@talk.nabble.com>
Message-ID: <14198294.post@talk.nabble.com>


Thank you all.  That's exactly what I was looking for.



TLowe wrote:
> 
> Hey Folks,
> 
> Could somebody help me rewrite the following code?
> 
> I am looping through all records across 5 fields to calculate the
> cumulative
> percentage of each record (relative to each individual field).
> 
> Is there a way to rewrite it so I don't have to loop through each
> individual
> record?
> 
> ##### tdat is my data frame
> ##### j is my field index
> ##### k is my record index
> ##### tsum is the sum of all values in field j
> ##### tmp is a vector containing the values in field j
> ##### tdat[k,paste("cpct,j,sep="")] creates new fields "cpct1",...,"cpct5" 
> 
> 
> for(j in 1:5) {
>   tsum<- sum(tdat[,j]);
>   for(k in 1:nrow(tdat)) {
>     td<- tdat[k,j];
>     tmp<-tdat[,j];
> ##### sum values <= to current value and divide by the total sum
>     tdat[k,paste("cpct,j,sep="")]<- sum(tmp[tmp <= td]) / tsum;
>   }
> }
> 
> 
> Thanks,
> TLowe
> 

-- 
View this message in context: http://www.nabble.com/Help-rewriting-looping-structure--tf4957267.html#a14198294
Sent from the R help mailing list archive at Nabble.com.


From minya.pu at gmail.com  Thu Dec  6 19:47:30 2007
From: minya.pu at gmail.com (Minya Pu)
Date: Thu, 6 Dec 2007 10:47:30 -0800
Subject: [R] a question on stepAIC
Message-ID: <c1352e4a0712061047p4caa62e4od1c0e92ef529702@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/cdff3375/attachment.pl 

From dieter.menne at menne-biomed.de  Thu Dec  6 19:34:19 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 6 Dec 2007 18:34:19 +0000 (UTC)
Subject: [R] Frequency and Phase Spectrograms
References: <BAY121-W57799914A2B54F5FAB264D46F0@phx.gbl>
Message-ID: <loom.20071206T182601-560@post.gmane.org>

Todd Remund <tkremund98 <at> hotmail.com> writes:

> I know that there is a function, (spectro3D), that produces the Power
Spectrogram.  Are there R functions
> that produce the Frequency Spectrogram and the Phase Spectrogram?  Thank you
for your time.

fft in stats gives you all you need, possibly combined with some time window in
package signal, which also has some higher level stuff. Do you really want to
interpret the "frequency spectrum" (whatever it is) with real, imaginary part
and signs?

The docs of spectro3D gives makes me cringe:

>>Following Heisenberg uncertainty principle, the short-term Fourier transform
cannot be precised in both time and frequency. >>

I know that's written in quite a few books, but that upside-down thinking has
been a source of confusion for generations of students and professors (of
biology, to say).

Dieter


From oman at mscc.huji.ac.il  Thu Dec  6 17:19:32 2007
From: oman at mscc.huji.ac.il (Samuel Oman)
Date: Thu, 06 Dec 2007 18:19:32 +0200
Subject: [R] generalized linear model with mixed effects
Message-ID: <47582114.8060200@mscc.huji.ac.il>

Hi,

    I need to fit a general generalized linear model, for observations 
on a response Y which is Gamma-distributed and observed in clusters.  So, if

    E(Y) = mu,

then for a suitable link function f, f(mu) is a (linear) function of 
both fixed and random effects.  Are there R packages or functions which 
fit these models, using either MLE or estimating-equation approaches?

Many thanks,
Sam

-- 
Professor Samuel D. Oman
Department of Statistics
Hebrew University
Mount Scopus
Jerusalem
91905 Israel

tel: 	+ 972 2 5883 442
fax:	+ 972 2 5883 549


From snowch at coralms.com  Thu Dec  6 13:51:07 2007
From: snowch at coralms.com (christopher snow)
Date: Thu, 06 Dec 2007 12:51:07 +0000
Subject: [R] relationship between two factors
Message-ID: <4757F03B.60701@coralms.com>

I have a dataset with two variables that are factors:

1) Decision Making Satisfaction (DMS), values = A - Completely, B - 
Mostly, C - Partly, D - Not at all
2) IT Satisfaction values (ITS), values = A - Completely, B - Mostly, C 
- Partly, D - Not at all

I would like to produce a table (matrix) and a chart of the factors, 
with counts at the cross sections:

         A   B   C   D
A   
B       counts
C
D

How can I do this in R?

Many thanks,

Chris

-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.


From funnymoody999 at yahoo.com  Thu Dec  6 18:45:24 2007
From: funnymoody999 at yahoo.com (mogra)
Date: Thu, 6 Dec 2007 09:45:24 -0800 (PST)
Subject: [R]  Randomizing one column in the dataMatrix
Message-ID: <14197423.post@talk.nabble.com>


I have huge data file, and I would like randomize just one column at a time ,
is there any easy way?

Thanks a lot.
-- 
View this message in context: http://www.nabble.com/Randomizing-one-column-in-the-dataMatrix-tf4957535.html#a14197423
Sent from the R help mailing list archive at Nabble.com.


From jeffdelmerico at yahoo.com  Wed Dec  5 21:16:25 2007
From: jeffdelmerico at yahoo.com (Jeff Delmerico)
Date: Wed, 5 Dec 2007 12:16:25 -0800 (PST)
Subject: [R] Displaying numerics to full double precision
In-Reply-To: <14178707.post@talk.nabble.com>
References: <14175334.post@talk.nabble.com> <14178707.post@talk.nabble.com>
Message-ID: <14179534.post@talk.nabble.com>


Thanks Ben, that fixed the display within R.  However, even after changing
the display settings, the matrix elements still appear to be exported in
single precision.  The matrix object is being passed into my C routines as
an SEXP Numeric type, and somewhere along the way, some of the digits are
getting lost.  
Here's the relevant bit of my C code:

SEXP
divideMatrix(SEXP matrix_in, SEXP sub_height, SEXP sub_width, SEXP fileS)
...
if ( isMatrix(matrix_in) && isNumeric(matrix_in) )
{
	/* Use R macros to convert from SEXP to C types */
	matrix = REAL(matrix_in);
	height = INTEGER(GET_DIM(matrix_in))[0];
	width = INTEGER(GET_DIM(matrix_in))[1];
	subW = INTEGER_VALUE(sub_width);
	subH = INTEGER_VALUE(sub_height);
        ...
}

Am I using the wrong macro to convert into a double in C?  Any ideas?

Thanks,
Jeff Delmerico


Ben Bolker wrote:
> 
> 
> 
> Jeff Delmerico wrote:
>> 
>> I'm working on a shared library of C functions for use with R, and I want
>> to create a matrix in R and pass it to the C routines.  I know R computes
>> and supposedly stores numerics in double precision, but when I create a
>> matrix of random numerics using rnorm(), the values are displayed in
>> single precision, and also exported in single precision when I pass them
>> out to my C routines.  An example is below:
>> 
>>> a <- matrix(rnorm(16, mean=10, sd=4), nrow=4)
>>> a
>>           [,1]      [,2]      [,3]      [,4]
>> [1,] 14.907606 17.572872 19.708977  9.809943
>> [2,]  9.322041 13.624452  7.745254  7.596176
>> [3,] 10.642408  6.151546  9.937434  6.913875
>> [4,] 14.617647  5.577073  8.217559 12.115465
>>> storage.mode(a)
>> [1] "double"
>> 
>> Does anyone know if there is a way to change the display or storage
>> settings so that the values will be displayed to their full precision?  
>> Or does rnorm only produce values to single precision? 
>> 
>> Any assistance would be greatly appreciated.
>> 
>> Thanks,
>> Jeff Delmerico
>> 
> 
> options("digits") # 7
> options(digits=x)
> 
>   I may be mistaken, but I think the values are indeed exported
> as double precision -- the issue here is just a display setting.
> 
>   Ben Bolker
> 
> 
> 

-- 
View this message in context: http://www.nabble.com/Displaying-numerics-to-full-double-precision-tf4950807.html#a14179534
Sent from the R help mailing list archive at Nabble.com.


From fempa at yahoo.com  Thu Dec  6 16:16:20 2007
From: fempa at yahoo.com (Svempa)
Date: Thu, 6 Dec 2007 07:16:20 -0800 (PST)
Subject: [R]  Fitting large titles in a plot
Message-ID: <14193900.post@talk.nabble.com>


I want to fit a fairly long main title for a plot, supposedly by changing row
after a while. As for now it starts way outside the picture margin at the
left and continues way out right passed the right margins.

>plot(A,main="This is my really long title and it's so long that I can see
just about half of it.")

Any suggestions? Shouldn't be that hard.

-- 
View this message in context: http://www.nabble.com/Fitting-large-titles-in-a-plot-tf4956510.html#a14193900
Sent from the R help mailing list archive at Nabble.com.


From price_ja at hotmail.com  Thu Dec  6 18:28:06 2007
From: price_ja at hotmail.com (Jim Price)
Date: Thu, 6 Dec 2007 09:28:06 -0800 (PST)
Subject: [R] Fitting large titles in a plot
In-Reply-To: <14193900.post@talk.nabble.com>
References: <14193900.post@talk.nabble.com>
Message-ID: <14196971.post@talk.nabble.com>


I wrote a little utility function for exactly this reason, which I use with
long titles. You may want to add calls to par to adjust the upper margin if
you are using raw graphical functionality (plot et al) - but lattice adjusts
the upper margin automatically so you wouldn't need to add anything else.


PrettyString <- function(theString, maxLength, collapse = "\n")
{
 	words <- unlist(strsplit(theString, " "))
 	wordLengths <- unlist(lapply(strsplit(words, ""), length))

 	if(max(wordLengths) > maxLength) 
		stop("maxChar must be increased due to string length")

 	count = wordLengths[1]
 	results = vector()
 	currentLine = words[1]

 	for(i in 2:length(words))
 	{
  		if((count + wordLengths[i] + 1) > maxLength)
  		{
   			results = c(results, currentLine)
   			currentLine = words[i]
   			count = wordLengths[i]
  		}
  		else
  		{
   			currentLine = paste(currentLine, words[i])
   			count = count + wordLengths[i] + 1
  		}
 	}
 	if(length(currentLine))
		results <- c(results, currentLine)

 	paste(results, collapse = collapse)
}


Knowing the R list, someone can probably reduce this function to 2 lines of
code.
Jim



Svempa wrote:
> 
> I want to fit a fairly long main title for a plot, supposedly by changing
> row after a while. As for now it starts way outside the picture margin at
> the left and continues way out right passed the right margins.
> 
>>plot(A,main="This is my really long title and it's so long that I can see
just about half of it.")
> 
> Any suggestions? Shouldn't be that hard.
> 
> 

-- 
View this message in context: http://www.nabble.com/Fitting-large-titles-in-a-plot-tf4956510.html#a14196971
Sent from the R help mailing list archive at Nabble.com.


From qianpland at yahoo.com  Thu Dec  6 18:27:37 2007
From: qianpland at yahoo.com (qian z)
Date: Thu, 6 Dec 2007 09:27:37 -0800 (PST)
Subject: [R] merge in function
Message-ID: <802470.99065.qm@web57311.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/2f1c7b9d/attachment.pl 

From dieter.menne at menne-biomed.de  Thu Dec  6 19:22:59 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 6 Dec 2007 18:22:59 +0000 (UTC)
Subject: [R] Using expression in Hmisc Key()
References: <55B194DA-5A84-4BCD-8869-C55D47FDB695@virginia.edu>
	<loom.20071206T071522-217@post.gmane.org>
	<184E1205-7AAD-4CEF-A0FC-083B6D6E7BCC@virginia.edu>
Message-ID: <loom.20071206T181411-946@post.gmane.org>

Michael Kubovy <kubovy <at> virginia.edu> writes:

> 
> This is one of the cases where I wish there were a function that  
> "exercised" all the arguments of a graphics function by visualizing  
> the effect of changing  two or three levels of each argument (one by  
> one, of course). This might have the side effect of allowing authors  
> to shorten documentation. I have in mind adding a method to each  
> graphics function, and having the author specify the two or three  
> instructive levels of each argument. See the exhaustive exploration by  
> J.R. Lobry, A.B. Dufour & D. Chessel of the graphical parameters  
> accessible by par(), at


> http://pbil.univ-lyon1.fr/R/fichestd/tdr75.pdf

That looks like a nice source for classical graphics, but I fear that it will
have a limited readience because of the language (Disclaimer: I am not working
for Time Magazine, my French is acceptable, and we drink their wine. ).

Even reading such stuff in German is difficult for me, because of the required 
switching between computer terms and translation.

For lattice, show.settings() is a good starter. Too bad it's miles from showing
the complete set of options. No student around looking for the first steps to 
fame?

Dieter


From jrkrideau at yahoo.ca  Thu Dec  6 19:54:45 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 6 Dec 2007 13:54:45 -0500 (EST)
Subject: [R] Vertical text in a plot
In-Reply-To: <5c79da7a0712060716j3bfabdd7v7bad975de9c1b953@mail.gmail.com>
Message-ID: <993862.11703.qm@web32805.mail.mud.yahoo.com>

op <- par(mar=(c(5, 4, 4, 4) + 0.1)) 
plot(1:25,runif(25,0,1),ylab="First Y-axis
label",xaxt="n")
axis(4,at=seq(0.2,1,.2), labels=1:5)
mtext("Second  Y-axis label",side=4, line=2)
par(op) 
--- Marcin Kozak <nyggus at gmail.com> wrote:

> Hi,
> 
> Consider this simple plot:
> > plot(1:25,runif(25,0,1),ylab="First Y-axis
> label",xaxt="n")
> 
> I want to add an additional axis as
> > axis(4,at=seq(0.2,1,.2), labels=1:5)
> 
> I have no idea how to add now the title of the new
> axis as "Second
> Y-axis label". I want this text to be vertically
> directed from bottom
> to top. I can't find the function in text() to write
> vertically. Any
> ideas?
> 
> Thanks,
> Marcin
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From gerard.tromp at sanger.med.wayne.edu  Thu Dec  6 20:00:15 2007
From: gerard.tromp at sanger.med.wayne.edu (Gerard Tromp)
Date: Thu, 6 Dec 2007 14:00:15 -0500
Subject: [R] lm.influence under R2.6.1
Message-ID: <HGEMKEDILMPOCJBJOGBDIEPKDAAA.gerard.tromp@sanger.med.wayne.edu>

Greetings!

Recently when I tried to use lm.influence I get the following error:
Error in .Fortran("lminfl", model$qr$qr, n, n, k, as.integer(do.coef),  :
  Fortran symbol name "lminfl" not in DLL for package "base"

This occurs on both Linux and Windows platforms (details below).

Searching the mail lists and other sources indicates that the fortran code
for lminfluence (lminfl.f) was moved in the source code tree suggesting it
should be found in package stats. Not clear to me why the "hardcoded" path
is package base.
The offending line in the R code for lm.influence is:
res <- .Fortran("lminfl", model$qr$qr, n, n, k, as.integer(do.coef),

There were no other messages from people experiencing errors with
lm.influence on the mailing list.

Can someone else confirm that they are experiencing the same problem?

Also, as a workaround, is there a way of forcing R to look in stats for the
fortran symbol (similar to using the double and triple colon for addressing
specific namespaces)?


On Linux platform:
=================
sessionInfo()
R version 2.6.1 (2007-11-26)
x86_64-redhat-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8
;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAM
E=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION
=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] SMPracticals_1.1-1 ellipse_0.3-5

loaded via a namespace (and not attached):
[1] rcompgen_0.1-17 tools_2.6.1

On Windows platform:
===================
R version 2.6.1 (2007-11-26)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] SMPracticals_1.1-1 ellipse_0.3-5

loaded via a namespace (and not attached):
[1] tools_2.6.1



Gerard
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Gerard Tromp, Ph.D
Center for Molecular Medicine and Genetics      Vox:     313.577.8773
   & Department of Neurology                    fax:     313.577.5218
Wayne State University School of Medicine
3309 Scott Hall
540 East Canfield Ave.
Detroit, MI 48201

        e-mail:                     gerard.tromp at sanger.med.wayne.edu
           web: http://www.genetics.wayne.edu/gtromp/gtromp-home.html


From jason.law at BES.CI.PORTLAND.OR.US  Thu Dec  6 20:03:31 2007
From: jason.law at BES.CI.PORTLAND.OR.US (Law, Jason)
Date: Thu, 6 Dec 2007 11:03:31 -0800 
Subject: [R] Help rewriting looping structure?
Message-ID: <59F44D74269BD711898900B0D0491AD80B15F804@miranda.bes.city>

This will give you the percents in the same order as your original data (as
this is what your original code did)

apply(tdat, 2,
function(x) {
	o <- order(x)
	oldo <- order(o)
	prc <- cumsum(x[o]) / sum(x)
	prc[oldo]
})

Jason Law
Statistician
City of Portland, Bureau of Environmental Services
Water Pollution Control Laboratory
6543 N Burlington Avenue
Portland, OR 97203 -5452
jlaw at bes.ci.portland.or.us



-----Original Message-----
From: r-help-bounces at r-project.org
[mailto:r-help-bounces at r-project.org]On Behalf Of TLowe
Sent: Thursday, December 06, 2007 10:28 AM
To: r-help at r-project.org
Subject: Re: [R] Help rewriting looping structure?



Thank you all.  That's exactly what I was looking for.



TLowe wrote:
> 
> Hey Folks,
> 
> Could somebody help me rewrite the following code?
> 
> I am looping through all records across 5 fields to calculate the
> cumulative
> percentage of each record (relative to each individual field).
> 
> Is there a way to rewrite it so I don't have to loop through each
> individual
> record?
> 
> ##### tdat is my data frame
> ##### j is my field index
> ##### k is my record index
> ##### tsum is the sum of all values in field j
> ##### tmp is a vector containing the values in field j
> ##### tdat[k,paste("cpct,j,sep="")] creates new fields "cpct1",...,"cpct5"

> 
> 
> for(j in 1:5) {
>   tsum<- sum(tdat[,j]);
>   for(k in 1:nrow(tdat)) {
>     td<- tdat[k,j];
>     tmp<-tdat[,j];
> ##### sum values <= to current value and divide by the total sum
>     tdat[k,paste("cpct,j,sep="")]<- sum(tmp[tmp <= td]) / tsum;
>   }
> }
> 
> 
> Thanks,
> TLowe
> 

-- 
View this message in context:
http://www.nabble.com/Help-rewriting-looping-structure--tf4957267.html#a1419
8294
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gunter.berton at gene.com  Thu Dec  6 20:01:48 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 6 Dec 2007 11:01:48 -0800
Subject: [R] Fitting large titles in a plot
In-Reply-To: <14193900.post@talk.nabble.com>
References: <14193900.post@talk.nabble.com>
Message-ID: <007701c8383a$73e168b0$3a0b2c0a@gne.windows.gene.com>


Try This:

plot(A,main=paste("This is my really long title and","\n","it's so long that
I can see
just about half of it.", sep = " "))

-- Bert Gunter
Genentech


From Greg.Snow at intermountainmail.org  Thu Dec  6 20:02:16 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 6 Dec 2007 12:02:16 -0700
Subject: [R] Fitting large titles in a plot
In-Reply-To: <14193900.post@talk.nabble.com>
References: <14193900.post@talk.nabble.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBD624F2@LP-EXCHVS07.CO.IHC.COM>

Try inserting \n into the title where you would like it to start on a
new line, e.g.:

plot(A,main="This is my really long title\nand it's so long\nthat I can
see just about half of it.")

You may need to give yourself more room in the margin for multi-line
titles (see ?par and the mar entry) and using the title function (rather
than in the plot) will let you specify the position of the title (line
argument).

Hope this helps,


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Svempa
> Sent: Thursday, December 06, 2007 8:16 AM
> To: r-help at r-project.org
> Subject: [R] Fitting large titles in a plot
> 
> 
> I want to fit a fairly long main title for a plot, supposedly 
> by changing row after a while. As for now it starts way 
> outside the picture margin at the left and continues way out 
> right passed the right margins.
> 
> >plot(A,main="This is my really long title and it's so long 
> that I can 
> >see
> just about half of it.")
> 
> Any suggestions? Shouldn't be that hard.
> 
> --
> View this message in context: 
> http://www.nabble.com/Fitting-large-titles-in-a-plot-tf4956510
> .html#a14193900
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From jrkrideau at yahoo.ca  Thu Dec  6 20:03:04 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 6 Dec 2007 14:03:04 -0500 (EST)
Subject: [R] Fitting large titles in a plot
In-Reply-To: <14193900.post@talk.nabble.com>
Message-ID: <529018.10449.qm@web32811.mail.mud.yahoo.com>

\n to start a new line 

plot(1:10,main="This is my really long title \n and
it's so long that I can see \n just about half of
it.")

--- Svempa <fempa at yahoo.com> wrote:

> 
> I want to fit a fairly long main title for a plot,
> supposedly by changing row
> after a while. As for now it starts way outside the
> picture margin at the
> left and continues way out right passed the right
> margins.
> 
> >plot(A,main="This is my really long title and it's
> so long that I can see
> just about half of it.")
> 
> Any suggestions? Shouldn't be that hard.
> 
> -- 
> View this message in context:
>
http://www.nabble.com/Fitting-large-titles-in-a-plot-tf4956510.html#a14193900
> Sent from the R help mailing list archive at
> Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From alison.waller at utoronto.ca  Thu Dec  6 20:03:28 2007
From: alison.waller at utoronto.ca (alison waller)
Date: Thu, 6 Dec 2007 14:03:28 -0500
Subject: [R] finding most highly transcribed genes - ranking,
	sorting and subsets?
Message-ID: <002901c8383a$afae3f30$0a02a8c0@AWALL>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/c9749cff/attachment.pl 

From marc_schwartz at comcast.net  Thu Dec  6 20:04:27 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 06 Dec 2007 13:04:27 -0600
Subject: [R] Fitting large titles in a plot
In-Reply-To: <14193900.post@talk.nabble.com>
References: <14193900.post@talk.nabble.com>
Message-ID: <1196967867.2955.77.camel@Bellerophon.localdomain>


On Thu, 2007-12-06 at 07:16 -0800, Svempa wrote:
> I want to fit a fairly long main title for a plot, supposedly by changing row
> after a while. As for now it starts way outside the picture margin at the
> left and continues way out right passed the right margins.
> 
> >plot(A,main="This is my really long title and it's so long that I can see
> just about half of it.")
> 
> Any suggestions? Shouldn't be that hard.

You can insert newline characters ('\n') in the title:


plot(A, main = "This is my really long title\nand it's so long that I
can see\njust about half of it.")


The likelihood is that you will need to alter the vertical position of
the title to accommodate the line wrapping.

To do that, you can either use mtext() in lieu of the 'main' argument,
or consider adjusting par("mgp"), the first value of which is the
position of the plot title. See ?mtext and ?par.

You might also want to look at ?strwrap for a more general way of
wrapping long lines of text.

HTH,

Marc Schwartz


From Greg.Snow at intermountainmail.org  Thu Dec  6 20:05:02 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 6 Dec 2007 12:05:02 -0700
Subject: [R] relationship between two factors
In-Reply-To: <4757F03B.60701@coralms.com>
References: <4757F03B.60701@coralms.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBD624F3@LP-EXCHVS07.CO.IHC.COM>

The 'table' function will give you the simple counts.  Plotting a table
with the 'plot' function gives common charts for this.  The 'CrossTable'
function in the gmodels package creates the table along with additional
information.  There are a lot of other functions for creating/working
with tables depending on what you are trying to do.

Hope this helps, 

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of christopher snow
> Sent: Thursday, December 06, 2007 5:51 AM
> To: r-help at r-project.org
> Subject: [R] relationship between two factors
> 
> I have a dataset with two variables that are factors:
> 
> 1) Decision Making Satisfaction (DMS), values = A - 
> Completely, B - Mostly, C - Partly, D - Not at all
> 2) IT Satisfaction values (ITS), values = A - Completely, B - 
> Mostly, C
> - Partly, D - Not at all
> 
> I would like to produce a table (matrix) and a chart of the 
> factors, with counts at the cross sections:
> 
>          A   B   C   D
> A   
> B       counts
> C
> D
> 
> How can I do this in R?
> 
> Many thanks,
> 
> Chris
> 
> --
> This message has been scanned for viruses and dangerous 
> content by MailScanner, and is believed to be clean.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From jholtman at gmail.com  Thu Dec  6 20:07:44 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 6 Dec 2007 11:07:44 -0800
Subject: [R] Fitting large titles in a plot
In-Reply-To: <14193900.post@talk.nabble.com>
References: <14193900.post@talk.nabble.com>
Message-ID: <644e1f320712061107j404e6fe0jd6fb4e30d76fa9bd@mail.gmail.com>

try this:


plot(0, main=paste(strwrap("This is my really long title and it's so
long that I can see just about half of it.", width=50),
collapse="\n"))

On Dec 6, 2007 7:16 AM, Svempa <fempa at yahoo.com> wrote:
>
> I want to fit a fairly long main title for a plot, supposedly by changing row
> after a while. As for now it starts way outside the picture margin at the
> left and continues way out right passed the right margins.
>
> >plot(A,main="This is my really long title and it's so long that I can see
> just about half of it.")
>
> Any suggestions? Shouldn't be that hard.
>
> --
> View this message in context: http://www.nabble.com/Fitting-large-titles-in-a-plot-tf4956510.html#a14193900
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From marc_schwartz at comcast.net  Thu Dec  6 20:08:49 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 06 Dec 2007 13:08:49 -0600
Subject: [R] relationship between two factors
In-Reply-To: <4757F03B.60701@coralms.com>
References: <4757F03B.60701@coralms.com>
Message-ID: <1196968129.2955.83.camel@Bellerophon.localdomain>


On Thu, 2007-12-06 at 12:51 +0000, christopher snow wrote:
> I have a dataset with two variables that are factors:
> 
> 1) Decision Making Satisfaction (DMS), values = A - Completely, B - 
> Mostly, C - Partly, D - Not at all
> 2) IT Satisfaction values (ITS), values = A - Completely, B - Mostly, C 
> - Partly, D - Not at all
> 
> I would like to produce a table (matrix) and a chart of the factors, 
> with counts at the cross sections:
> 
>          A   B   C   D
> A   
> B       counts
> C
> D
> 
> How can I do this in R?
> 
> Many thanks,
> 
> Chris


See ?table, for example:

  table(DMS, ITS)

You did not indicate the type of chart you want to create, but some
possibilities, using base graphics, would be barplot(), dotchart() and
mosaicplot(). The latter is in the vcd package on CRAN.

HTH,

Marc Schwartz


From david at incogen.com  Thu Dec  6 20:12:43 2007
From: david at incogen.com (David Coppit)
Date: Thu, 6 Dec 2007 11:12:43 -0800
Subject: [R] Java parser for R data file?
In-Reply-To: <Pine.LNX.4.64.0712051712040.12631@gannet.stats.ox.ac.uk>
Message-ID: <C37DB3DB.DD%david@incogen.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/cebde253/attachment.pl 

From marc_schwartz at comcast.net  Thu Dec  6 20:14:34 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 06 Dec 2007 13:14:34 -0600
Subject: [R] merge in function
In-Reply-To: <802470.99065.qm@web57311.mail.re1.yahoo.com>
References: <802470.99065.qm@web57311.mail.re1.yahoo.com>
Message-ID: <1196968474.2955.87.camel@Bellerophon.localdomain>


On Thu, 2007-12-06 at 09:27 -0800, qian z wrote:
> I used merge() in a function, but it doesn't return correct data frame. 
>    
>   add.name <- function(data, x)
>    
>   {
>    
>   ...
>   ...
>    
>   newfile <- merge(data, resid, by =0, all.x=TRUE, all.y= FALSE)
>   newfile
>    
>    
>   }

You are going to need to give us more information. 

It is not clear what you mean by not returning the correct data frame.
Why is it not correct? 

Where and what is 'resid'?  

What is 'x' and how is it used in the function?  

What is 'by = 0'? The 'by' argument values need to be quoted.

Please provide a small reproducible example as per the Posting Guide.

HTH,

Marc Schwartz


From jrkrideau at yahoo.ca  Thu Dec  6 20:17:48 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 6 Dec 2007 14:17:48 -0500 (EST)
Subject: [R] relationship between two factors
In-Reply-To: <4757F03B.60701@coralms.com>
Message-ID: <768758.67745.qm@web32803.mail.mud.yahoo.com>

?table should work 

table (DMS, ITS)

I am not clear on what kind of chart you want.

will 
plot(DMS, ITS) do what you want?


--- christopher snow <snowch at coralms.com> wrote:

> I have a dataset with two variables that are
> factors:
> 
> 1) Decision Making Satisfaction (DMS), values = A -
> Completely, B - 
> Mostly, C - Partly, D - Not at all
> 2) IT Satisfaction values (ITS), values = A -
> Completely, B - Mostly, C 
> - Partly, D - Not at all
> 
> I would like to produce a table (matrix) and a chart
> of the factors, 
> with counts at the cross sections:
> 
>          A   B   C   D
> A   
> B       counts
> C
> D
> 
> How can I do this in R?
> 
> Many thanks,
> 
> Chris



      Looking for the perfect gift? Give the gift of Flickr!


From marc_schwartz at comcast.net  Thu Dec  6 20:30:30 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 06 Dec 2007 13:30:30 -0600
Subject: [R] Fitting large titles in a plot
In-Reply-To: <14196971.post@talk.nabble.com>
References: <14193900.post@talk.nabble.com>  <14196971.post@talk.nabble.com>
Message-ID: <1196969430.2955.92.camel@Bellerophon.localdomain>


On Thu, 2007-12-06 at 09:28 -0800, Jim Price wrote:
> I wrote a little utility function for exactly this reason, which I use with
> long titles. You may want to add calls to par to adjust the upper margin if
> you are using raw graphical functionality (plot et al) - but lattice adjusts
> the upper margin automatically so you wouldn't need to add anything else.
> 
> 
> PrettyString <- function(theString, maxLength, collapse = "\n")
> {
>  	words <- unlist(strsplit(theString, " "))
>  	wordLengths <- unlist(lapply(strsplit(words, ""), length))
> 
>  	if(max(wordLengths) > maxLength) 
> 		stop("maxChar must be increased due to string length")
> 
>  	count = wordLengths[1]
>  	results = vector()
>  	currentLine = words[1]
> 
>  	for(i in 2:length(words))
>  	{
>   		if((count + wordLengths[i] + 1) > maxLength)
>   		{
>    			results = c(results, currentLine)
>    			currentLine = words[i]
>    			count = wordLengths[i]
>   		}
>   		else
>   		{
>    			currentLine = paste(currentLine, words[i])
>    			count = count + wordLengths[i] + 1
>   		}
>  	}
>  	if(length(currentLine))
> 		results <- c(results, currentLine)
> 
>  	paste(results, collapse = collapse)
> }
> 
> 
> Knowing the R list, someone can probably reduce this function to 2 lines of
> code.
> Jim

Would you believe one line?

  paste(strwrap(theString, width = maxLength), collapse = "\n")

;-)

See ?strwrap as I noted previously.

HTH,

Marc Schwartz


From marc_schwartz at comcast.net  Thu Dec  6 20:39:44 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 06 Dec 2007 13:39:44 -0600
Subject: [R] Randomizing one column in the dataMatrix
In-Reply-To: <14197423.post@talk.nabble.com>
References: <14197423.post@talk.nabble.com>
Message-ID: <1196969984.2955.100.camel@Bellerophon.localdomain>


On Thu, 2007-12-06 at 09:45 -0800, mogra wrote:
> I have huge data file, and I would like randomize just one column at a time ,
> is there any easy way?
> 
> Thanks a lot.

If you just want to randomly <hint> sample <\hint> from a single column,
_independent of the other columns_, you can use:

  DF$Column[sample(nrow(DF))]

where DF$Column is the column of interest in your data frame 'DF'.

If you want to randomly sample entire rows from 'DF', you can use:

  DF[sample(nrow(DF)), ]

See ?sample for more information.

HTH,

Marc Schwartz


From wwwhsd at gmail.com  Thu Dec  6 20:44:49 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 6 Dec 2007 17:44:49 -0200
Subject: [R] relationship between two factors
In-Reply-To: <4757F03B.60701@coralms.com>
References: <4757F03B.60701@coralms.com>
Message-ID: <da79af330712061144m738a9bd6ob8ecb2e89e8ff2c3@mail.gmail.com>

Try this:

df <- data.frame(DMS=factor(rep(LETTERS[1:4], 10)),
ITS=factor(rep(LETTERS[1:4], 10)))
table(df)
plot(table(df))

-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


On 06/12/2007, christopher snow <snowch at coralms.com> wrote:
> I have a dataset with two variables that are factors:
>
> 1) Decision Making Satisfaction (DMS), values = A - Completely, B -
> Mostly, C - Partly, D - Not at all
> 2) IT Satisfaction values (ITS), values = A - Completely, B - Mostly, C
> - Partly, D - Not at all
>
> I would like to produce a table (matrix) and a chart of the factors,
> with counts at the cross sections:
>
>          A   B   C   D
> A
> B       counts
> C
> D
>
> How can I do this in R?
>
> Many thanks,
>
> Chris
>
> --
> This message has been scanned for viruses and
> dangerous content by MailScanner, and is
> believed to be clean.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Thu Dec  6 21:00:42 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 06 Dec 2007 14:00:42 -0600
Subject: [R] lm.influence under R2.6.1
In-Reply-To: <HGEMKEDILMPOCJBJOGBDIEPKDAAA.gerard.tromp@sanger.med.wayne.edu>
References: <HGEMKEDILMPOCJBJOGBDIEPKDAAA.gerard.tromp@sanger.med.wayne.edu>
Message-ID: <1196971242.2955.107.camel@Bellerophon.localdomain>


On Thu, 2007-12-06 at 14:00 -0500, Gerard Tromp wrote:
> Greetings!
> 
> Recently when I tried to use lm.influence I get the following error:
> Error in .Fortran("lminfl", model$qr$qr, n, n, k, as.integer(do.coef),  :
>   Fortran symbol name "lminfl" not in DLL for package "base"
> 
> This occurs on both Linux and Windows platforms (details below).

I presume that the Linux message does not refer to DLLs? :-)

> Searching the mail lists and other sources indicates that the fortran code
> for lminfluence (lminfl.f) was moved in the source code tree suggesting it
> should be found in package stats. Not clear to me why the "hardcoded" path
> is package base.
> The offending line in the R code for lm.influence is:
> res <- .Fortran("lminfl", model$qr$qr, n, n, k, as.integer(do.coef),
> 
> There were no other messages from people experiencing errors with
> lm.influence on the mailing list.
> 
> Can someone else confirm that they are experiencing the same problem?
> 
> Also, as a workaround, is there a way of forcing R to look in stats for the
> fortran symbol (similar to using the double and triple colon for addressing
> specific namespaces)?

<snip>

I can run example(lm.influence) and example(influence.measures) without
issue on:

  R version 2.6.1 Patched (2007-11-26 r43541)

on an F8 Linux system.

I don't see anything immediately obvious in the NEWS file for 2.6.1
patched to suggest that something has changed here.

Can you provide a reproducible example here and/or try to upgrade to
patched?

I would also try to run:

  R --vanilla

from the command line to provide a clean environment to be sure that
none of the other packages that you are loading are causing conflicts.

HTH,

Marc Schwartz


From minya.pu at gmail.com  Thu Dec  6 21:10:29 2007
From: minya.pu at gmail.com (Minya Pu)
Date: Thu, 6 Dec 2007 12:10:29 -0800
Subject: [R] a question on stepAIC
In-Reply-To: <c1352e4a0712061047p4caa62e4od1c0e92ef529702@mail.gmail.com>
References: <c1352e4a0712061047p4caa62e4od1c0e92ef529702@mail.gmail.com>
Message-ID: <c1352e4a0712061210k66f177b5v518669dbfd6e5bd6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/72c141ef/attachment.pl 

From demirtas at uic.edu  Thu Dec  6 21:18:05 2007
From: demirtas at uic.edu (HAKAN DEMIRTAS)
Date: Thu, 6 Dec 2007 14:18:05 -0600
Subject: [R] correlated data
Message-ID: <000c01c83845$1bf5eda0$9d3ff880@demirtasxp157>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/64493f43/attachment.pl 

From spencer.graves at pdf.com  Thu Dec  6 21:18:27 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 06 Dec 2007 12:18:27 -0800
Subject: [R] S3 and S4 clash
Message-ID: <47585913.5070206@pdf.com>

Hello: 

      How can I work around the conflict between the S3 and S4 
illustrated in the example below?  I'm developing a package that 
requires a function in 'stats4', but when 'stats4' is attached, it 
breaks my AIC function.  I could give my AIC function another name so it 
no longer uses the generic dispatch, but I wonder if there is another way. 

      Thanks,
      Spencer Graves
################################
bar.tmp <- structure(1, class = "bar")
AIC.bar <- function(object, ..., k=2) {
  3
}

 > AIC(bar.tmp)
[1] 3
 > library(stats4)
 > AIC(bar.tmp)
Error in UseMethod("logLik") : no applicable method for "logLik"
 > detach("package:stats4")
 > AIC(bar.tmp)
[1] 3
 > objects()
[1] "AIC.bar" "bar.tmp"
 > sessionInfo()
R version 2.6.1 (2007-11-26)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
[1] stats4_2.6.1


From sue at xlsolutions-corp.com  Thu Dec  6 21:25:28 2007
From: sue at xlsolutions-corp.com (sue at xlsolutions-corp.com)
Date: Thu, 06 Dec 2007 13:25:28 -0700
Subject: [R] Course***January 2008 *** San Francisco & New York City ***
	R/S-Plus Advanced Programming by XLSolutions Corp
Message-ID: <20071206132527.aa8924c5d28ca71e2a043bb294e795eb.26ae24b8bf.wbe@email.secureserver.net>

Happy Holidays Folks!

XLSolutions Corporation has scheduled 2 great R/Splus Advanced
programming courses. More course can be viewed on our website.


R/Splus Advanced Programming*****  San Francisco ****   January 21-22,
2008
R/Splus Advanced Programming*****  New York City ****   January 24-25,
2008


Please email for earlybird rates: Payments due after the class.


R/Splus Advanced Programming  Course Outline:

Day 1 

 - Overview of R/S fundamentals: Syntax and Semantics  
 - Class and Inheritance  
 - Concepts, Construction and good use of Language Objects  
 - Coercion and Efficiency  
 - Object-oriented Programming in R and S-Plus  
 - Taking advantage of fast objects and fast functions  
 - Advanced Manipulation tools: Parse, Deparse, Substitute, etc.  
 - How to fully take advantage of Vectorization  
 - Generic and Method Functions; S4 (S-Plus 6), etc.  
 - Search path, Databases and Frames Visibility (S-plus)  

Day 2 

 - Working with Large Objects  
 - Handling Properly Recursion and Iterative Calculations  
 - Managing loops; For (S-Plus) and for() loops  
 - Consequences of Lazy Evaluation  
 - Efficient Code Practices for Large Computations  
 - Memory Management and Resource Monitoring  
 - Writing R and S-plus Functions to call Compiled Code  
 - Writing and Debugging Compiled Code for S-plus and R system  
 - Connecting R to External Data Sources  
 - Macros in R  
 - Understanding the Structure of Model fitting Functions in R and
S-plus  
 - Designing and Packaging Efficiently a new model Function  

Payment due AFTER the class
Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/courselist.htm
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. 

Register now to secure your seat!

Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com


From marcia.rocha at gmail.com  Thu Dec  6 21:26:46 2007
From: marcia.rocha at gmail.com (marciarr)
Date: Thu, 6 Dec 2007 12:26:46 -0800 (PST)
Subject: [R]  simple problems
Message-ID: <14200401.post@talk.nabble.com>


Hello R users,
I have been looking through Help files and Nabble list for the answers for
these simple questions, but it seems to be fruitless.
1- in a data frame with two columns, x and y, how do I get the corresponding
value of x to, let's say, the minimum value of the y column (min (data$y)) ?
2- how do I solve a simple equation? Considering the equation y= exp(-x)^12,
I would like to find the values of x for,  for example, y=0.01, so
exp(-x)^12=0.01. How do I do that using R?
I know those a probably very, very simple questions, but for which I do not
seem to find the answer.
Thank you very much,
Marcia
-- 
View this message in context: http://www.nabble.com/simple-problems-tf4958407.html#a14200401
Sent from the R help mailing list archive at Nabble.com.


From price_ja at hotmail.com  Thu Dec  6 22:08:58 2007
From: price_ja at hotmail.com (Jim Price)
Date: Thu, 6 Dec 2007 13:08:58 -0800 (PST)
Subject: [R] Fitting large titles in a plot
In-Reply-To: <1196969430.2955.92.camel@Bellerophon.localdomain>
References: <14193900.post@talk.nabble.com> <14196971.post@talk.nabble.com>
	<1196969430.2955.92.camel@Bellerophon.localdomain>
Message-ID: <14201050.post@talk.nabble.com>


I have learned something new - thanks for the strwrap info.

The problem with posting from Nabble is that by the time your post actually
gets to the list (2 hours after you posted it in this case) and you've
written some line like "Knowing the R list, someone can probably reduce this
function to 2 lines of code" half a dozen people have already shown exactly
how you can do that, making me look totally incompetent.

Ah well, such is life :)

Jim.


Marc Schwartz wrote:
> 
> 
> On Thu, 2007-12-06 at 09:28 -0800, Jim Price wrote:
>> I wrote a little utility function for exactly this reason, which I use
>> with
>> long titles. You may want to add calls to par to adjust the upper margin
>> if
>> you are using raw graphical functionality (plot et al) - but lattice
>> adjusts
>> the upper margin automatically so you wouldn't need to add anything else.
>> 
>> 
>> PrettyString <- function(theString, maxLength, collapse = "\n")
>> {
>>  	words <- unlist(strsplit(theString, " "))
>>  	wordLengths <- unlist(lapply(strsplit(words, ""), length))
>> 
>>  	if(max(wordLengths) > maxLength) 
>> 		stop("maxChar must be increased due to string length")
>> 
>>  	count = wordLengths[1]
>>  	results = vector()
>>  	currentLine = words[1]
>> 
>>  	for(i in 2:length(words))
>>  	{
>>   		if((count + wordLengths[i] + 1) > maxLength)
>>   		{
>>    			results = c(results, currentLine)
>>    			currentLine = words[i]
>>    			count = wordLengths[i]
>>   		}
>>   		else
>>   		{
>>    			currentLine = paste(currentLine, words[i])
>>    			count = count + wordLengths[i] + 1
>>   		}
>>  	}
>>  	if(length(currentLine))
>> 		results <- c(results, currentLine)
>> 
>>  	paste(results, collapse = collapse)
>> }
>> 
>> 
>> Knowing the R list, someone can probably reduce this function to 2 lines
>> of
>> code.
>> Jim
> 
> Would you believe one line?
> 
>   paste(strwrap(theString, width = maxLength), collapse = "\n")
> 
> ;-)
> 
> See ?strwrap as I noted previously.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Fitting-large-titles-in-a-plot-tf4956510.html#a14201050
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Thu Dec  6 22:22:22 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 6 Dec 2007 21:22:22 +0000 (GMT)
Subject: [R] S3 and S4 clash
In-Reply-To: <47585913.5070206@pdf.com>
References: <47585913.5070206@pdf.com>
Message-ID: <Pine.LNX.4.64.0712062112280.30805@gannet.stats.ox.ac.uk>

I'd say that was pretty clearly a bug in stats4 (which as I recall was 
needed to get around the scoping awkwardnesses of S4).

But could you not write a logLik method for your class?  E.g.

> logLik.bar <- function(object, ...) structure(pi, class="logLik", df=1)
> AIC(bar.tmp)
[1] -4.283185
> library(stats4)
> AIC(bar.tmp)
[1] -4.283185


On Thu, 6 Dec 2007, Spencer Graves wrote:

> Hello:
>
>      How can I work around the conflict between the S3 and S4
> illustrated in the example below?  I'm developing a package that
> requires a function in 'stats4', but when 'stats4' is attached, it
> breaks my AIC function.  I could give my AIC function another name so it
> no longer uses the generic dispatch, but I wonder if there is another way.
>
>      Thanks,
>      Spencer Graves
> ################################
> bar.tmp <- structure(1, class = "bar")
> AIC.bar <- function(object, ..., k=2) {
>  3
> }
>
> > AIC(bar.tmp)
> [1] 3
> > library(stats4)
> > AIC(bar.tmp)
> Error in UseMethod("logLik") : no applicable method for "logLik"
> > detach("package:stats4")
> > AIC(bar.tmp)
> [1] 3
> > objects()
> [1] "AIC.bar" "bar.tmp"
> > sessionInfo()
> R version 2.6.1 (2007-11-26)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] stats4_2.6.1
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From kchine at ebi.ac.uk  Thu Dec  6 22:25:41 2007
From: kchine at ebi.ac.uk (kchine at ebi.ac.uk)
Date: Thu, 6 Dec 2007 21:25:41 -0000 (GMT)
Subject: [R] Java parser for R data file?
Message-ID: <44671.80.189.98.44.1196976341.squirrel@webmail.ebi.ac.uk>

Dear David,

You may also consider using the biocep project' tools and frameworks. they
provide an advanced  bridge that allow you to exchange between R and Java
any standard R Object and any mapped S4 object.
the object extracted to Java (an RList for you data) can be serialized to
a file (saved as a java object). you can then read the serialized object
very easily from java and this is obviously  much faster than parsing and
more elegant than a "proprietary" serialization/deserialization .

the easiest way to achieve this is to create the Serialized RList via the
Virtual R Workbench (Universal IDE for R).
you can run the R workbench on any kind of OS via the biocep sources:
http://www.ebi.ac.uk/microarray-srv/frontendapp/BIOCEP_README.txt

or just use this link to get the software installed on your machine if you
are using Mac OS or windows
http://www.ebi.ac.uk/microarray-srv/frontendapp/rworkbench.jnlp

create your data via the workbench R Console (you may also use the script
editor or just read your list from a file)
go to the menu "Java", choose "Save R/Java Object to local file"
select the name you've given to your list and choose a file name.

to retrieve your list in java, all you need is to add the RJB.jar to your
class path and use this simple code:

RList l=(RList)new ObjectInputStream(new
FileInputStream("J:/list.ser")).readObject();

and that's it.
this works for much more complex R objects (ExpressionSet..)

best wishes,

Karim


>
> On 12/5/07 12:15 PM, "Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote:
>
> On Wed, 5 Dec 2007, David Coppit wrote:
>
>> Or, given that I'm dealing with just a single array, would it be better
>> to
>> roll my own I/O using write.table or write.matrix from the MASS package?
>
> It would be much easier.  The save() format is far more complex than you
> need.  However, I would use writeBin() to write a binary file and read
> that in in Java, avoiding the binary -> ASCII -> binary conversion.
>
> Thanks for the suggestion-writeBin works quite well. For posterity, here's
> what I did:
>
> On the R side:
>
> # Assumes that there are no special values in the tofList, such as
> NA_REAL,
> # R_PosInf, R_NegInf, ISNAN, R_FINITE. See the "R Data Import/Export"
> manual.
> saveListAsBinary <- function( tofList, filename )
> {
>   outConn <- file( filename, "wb" );
>
>   for (m in 1:length(tofList)) {
>     writeBin(names(tofList)[[m]], outConn);
>     writeBin(length(tofList[[m]]), outConn, size = 4, endian = "big");
>     writeBin(tofList[[m]], outConn, size = 4, endian = "big");
>   }
>
>   close(outConn);
> }
>
> saveListAsBinary(myList, "outfile.RDat");
>
> On the Java side:
>
>   public static void read_R_Output(String filename, ArrayList<String>
> names,
>       ArrayList<ArrayList<Float>> data)
>   {
>     try {
>       DataInputStream dataInputStream = new DataInputStream(
>         new BufferedInputStream(new FileInputStream(filename)));
>
>       boolean endOfFile = false;
>
>       while (!endOfFile) {
>         try {
>           StringBuffer sb = new StringBuffer();
>
>           byte c;
>           while ((c = dataInputStream.readByte()) != 0)
>             sb.append((char)c);
>
>           names.add(new String(sb));
>
>           int cols = dataInputStream.readInt();
>
>           ArrayList<Float> row = new ArrayList<Float>(cols);
>
>           for (int i = 0; i < cols; i++)
>             row.add(dataInputStream.readFloat());
>
>           data.add(row);
>         } catch (EOFException e) {
>           endOfFile = true;
>         }
>       }
>
>       dataInputStream.close();
>     } catch (Exception e) {
>       e.printStackTrace();
>     }
>   }
>
> Regards,
> David
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ebekkers at few.eur.nl  Thu Dec  6 22:45:07 2007
From: ebekkers at few.eur.nl (Eddy H. G. Bekkers)
Date: Thu, 6 Dec 2007 22:45:07 +0100
Subject: [R] Integral implicit function
Message-ID: <20071206224507660.00000003820@ae-030530>

Hi,

Could somebody help me with the following. I want to calculate the integral over an implicit function. I thought to integrate over a function depending on uniroot. In previous topics I found a thread about finding the root of an integral. And that works. But the other way around, does not work. Does R support this?

I included the following example. The function in the example is very easy and can be solved explicitly, but when it does not work for such an easy function it will certainly not work for a more difficult function. First the root of an integral (which works) and then the integral of a function dependent on uniroot:

# Calculating the root of an integral

a<- function(x,y)
    {x-y}

b<- function(y)
    {integrate(a,lower=1,upper=2,y=y)$value}

d<- uniroot(b,c(0,10))$root

print(d)

# Calculating the integral of a function dependent on uniroot

e<- function(u,v)
    {u-v}

f<- function(v)
    {uniroot(e,c(0,10),v=v)$root}

g<- integrate(f,lower=1,upper=2)$value

print(g)

Does anyone have suggestions how to proceed? By the way, the implicit function I am targeting does have a unique solution, it is only not explicitly solvable, i.e. in the example above, you cannot solve u as a function of v explicitly, so as to substitute it in the integrand. 


Thanks a lot in advance for your help,

Best regards,

Eddy Bekkers
Department of Economics
Erasmus University Rotterdam


From jmontema at kent.edu  Thu Dec  6 22:49:07 2007
From: jmontema at kent.edu (Justin Montemarano)
Date: Thu, 06 Dec 2007 16:49:07 -0500
Subject: [R] coxme() random effect syntax
Message-ID: <47586E53.1080606@kent.edu>

Hello:

I would like to run a Cox proportional hazards regression on crayfish 
dislodgement at different water velocities by crayfish size class and 
substrate (rock) type. Additionally, there is a covariate variable, rock 
movement that may be influencing crayfish dislodgment. So...

I have crayfish size class (CFSZCL) and substrate type (SUBSZ) as fixed 
factors influencing the dislodgment of crayfish at different water 
velocities (SLIPVEL). Thus, I am currently using the call:

coxph(Surv(SLIPVEL, FREEDOM) ~ CFSZCL + SUBSZ + CFSZCL:SUBSZ, data = 
data.table)

However, I would like to add rock movement (BEDLOAD) as a random 
co-variate, which is not possible with the coxph() function. Is it 
possible to do so with coxme() in the kinship package? If so, what is 
the proper syntax?

Thanks for any help.

Justin Montemarano


From vistocco at unicas.it  Thu Dec  6 22:49:22 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Thu, 06 Dec 2007 22:49:22 +0100
Subject: [R] simple problems
In-Reply-To: <14200401.post@talk.nabble.com>
References: <14200401.post@talk.nabble.com>
Message-ID: <47586E62.9060702@unicas.it>

marciarr wrote:
> Hello R users,
> I have been looking through Help files and Nabble list for the answers for
> these simple questions, but it seems to be fruitless.
> 1- in a data frame with two columns, x and y, how do I get the corresponding
> value of x to, let's say, the minimum value of the y column (min (data$y)) ?
>   
data$x2[data$x1==min(data$x1)]
> 2- how do I solve a simple equation? Considering the equation y= exp(-x)^12,
> I would like to find the values of x for,  for example, y=0.01, so
> exp(-x)^12=0.01. How do I do that using R?
>   
I know there is solve for linear equations, but I do not know if there is
a generic solver for other equations.
You could implement it using a numerical approach (perhaps it is yet in some
package).

domenico


From tan at cooper.edu  Thu Dec  6 22:59:12 2007
From: tan at cooper.edu (tan at cooper.edu)
Date: Thu, 6 Dec 2007 16:59:12 -0500 (EST)
Subject: [R] Cross Validation of lda() from MASS package
Message-ID: <50483.70.111.90.93.1196978352.squirrel@webmail.cooper.edu>

Is the cross validation procedure implemented in lda() from the MASS
package internal or external?

Thanks,

Bijun Tan
Cooper Union


From ramin.1981 at gmail.com  Thu Dec  6 23:05:38 2007
From: ramin.1981 at gmail.com (ramin.1981 at gmail.com)
Date: Thu, 06 Dec 2007 14:05:38 -0800
Subject: [R] Testing Two Categorical Variable
Message-ID: <21073409.1618231196978738587.JavaMail.nabble@isper.nabble.com>

The chi-square does not need your two categorical variables to have equal levels, nor limitation for the number of levels.

The Chi-square procedure is as follow:
?^2=?_(All Cells)??(Observed-Expected)?^2/Expected

Expected Cell= E_ij=n((i^th RowTotal)/n)((j^th RowTotal)/n)

Degree of Freedom=df= (row-1)(Col-1)

This way should not give you any errors if your calculations are all correct. I usually use SAS for calculations like this. Below is a sample code I wrote to test whether US_State and Blood type are independent. You can modify it for your data and should give you no error.

data bloodtype;
input bloodtype$ state$ count@@;
datalines;
A FL 122 B FL 117
AB FL 19 O FL 244
A IA 1781 B IA 351
AB IA 289 O IA 3301
A MO 353 B MO 269
AB MO 60 O MO 713
;
proc freq data=bloodtype;
tables bloodtype*state
/ cellchi2 chisq expected norow nocol nopercent;
weight count;
quit;


Best
Ramin


From ramin.1981 at gmail.com  Thu Dec  6 23:05:48 2007
From: ramin.1981 at gmail.com (ramin.1981 at gmail.com)
Date: Thu, 06 Dec 2007 14:05:48 -0800
Subject: [R] Testing Two Categorical Variable
Message-ID: <13492672.1618261196978748568.JavaMail.nabble@isper.nabble.com>

The chi-square does not need your two categorical variables to have equal levels, nor limitation for the number of levels.

The Chi-square procedure is as follow:
?^2=?_(All Cells)??(Observed-Expected)?^2/Expected

Expected Cell= E_ij=n((i^th RowTotal)/n)((j^th RowTotal)/n)

Degree of Freedom=df= (row-1)(Col-1)

This way should not give you any errors if your calculations are all correct. I usually use SAS for calculations like this. Below is a sample code I wrote to test whether US_State and Blood type are independent. You can modify it for your data and should give you no error.

data bloodtype;
input bloodtype$ state$ count@@;
datalines;
A FL 122 B FL 117
AB FL 19 O FL 244
A IA 1781 B IA 351
AB IA 289 O IA 3301
A MO 353 B MO 269
AB MO 60 O MO 713
;
proc freq data=bloodtype;
tables bloodtype*state
/ cellchi2 chisq expected norow nocol nopercent;
weight count;
quit;


Best
Ramin


From affysnp at gmail.com  Thu Dec  6 22:11:48 2007
From: affysnp at gmail.com (affy snp)
Date: Thu, 6 Dec 2007 16:11:48 -0500
Subject: [R] row lables in heatmap.2()
Message-ID: <5032046e0712061311q2276d6dcj74ac79135100d82e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/78fee57d/attachment.pl 

From ramin.1981 at gmail.com  Thu Dec  6 23:09:24 2007
From: ramin.1981 at gmail.com (Ramin Shamshiri)
Date: Thu, 6 Dec 2007 14:09:24 -0800 (PST)
Subject: [R] testing independence of categorical variables
In-Reply-To: <ab02bb240711220316q25e0bbd6rd2de31610c245422@mail.gmail.com>
References: <ab02bb240711220316q25e0bbd6rd2de31610c245422@mail.gmail.com>
Message-ID: <14202348.post@talk.nabble.com>


The chi-square does not need your two categorical variables to have equal
levels, nor limitation for the number of levels.

The Chi-square procedure is as follow:
?^2=?_(All Cells)??(Observed-Expected)?^2/Expected

Expected Cell= E_ij=n((i^th RowTotal)/n)((j^th RowTotal)/n)

Degree of Freedom=df= (row-1)(Col-1)

This way should not give you any errors if your calculations are all
correct. I usually use SAS for calculations like this. Below is a sample
code I wrote to test whether US_State and Blood type are independent. You
can modify it for your data and should give you no error.

data bloodtype;
input bloodtype$ state$ count@@;
datalines;
A FL 122 B FL 117
AB FL 19 O FL 244
A IA 1781 B IA 351
AB IA 289 O IA 3301
A MO 353 B MO 269
AB MO 60 O MO 713
;
proc freq data=bloodtype;
tables bloodtype*state
/ cellchi2 chisq expected norow nocol nopercent;
weight count;
quit;


Best
Ramin
Gainesville



Shoaaib Mehmood wrote:
> 
> hi,
> 
> is there a way of calculating of measuring dependence between two
> categorical variables. i tried using the chi square test to test for
> independence but i got error saying that the lengths of the two
> vectors don't match. Suppose X and Y are two factors. X has 5 levels
> and Y has 7 levels. This is what i tried doing
> 
>>temp<-chisq.test(x,y)
> 
> but got error "the lengths of the two vectors don't match". any help
> will be appreciated
> -- 
> Regards,
> Rana Shoaaib Mehmood
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/testing-independence-of-categorical-variables-tf4855773.html#a14202348
Sent from the R help mailing list archive at Nabble.com.


From shitao at hotmail.com  Thu Dec  6 23:38:36 2007
From: shitao at hotmail.com (Tao Shi)
Date: Thu, 6 Dec 2007 22:38:36 +0000
Subject: [R] updating a helper function in a R package
Message-ID: <BAY120-W36138F3288572FAA2EA1EAC76F0@phx.gbl>


Hi list,

Sorry for the vague title, but here is the scenario.  

I?m writing an R package, let?s say, ?pkg1?, which contains 3 functions: f1, f2, f3.  f2 and f3 are helper functions for f1, i.e. f1 calls f2 which in turn calls f3.

f1 <- function(?) {
	?.
	f2()
	?
}

f2 <- function(?){
	?
	f3(?)
	?
}

f3 <- function(...){
       ....
}

Then, I wrote a new version of f3 and I want to test it.  With the old version of ?pkg1? already loaded into my R session, I tried just copy-and-paste the new ?f3? to R console and hope f1 will pick the new ?f3? up.  It obviously didn?t work.  I know it?s b/c the new f3 and old f3 are in different environments and when f1 is called, only old f3 is used.  Then I tried to change the environment of new f3 to the same as the old f3's by calling:

environment(f3) <- environment(pkg1:::f3)

but it wasn't working either.

So,  
1)	Could somebody help me to put all these into perspectives?
2)	Is there an easier way to update f3 without rebuilding the package? (by that I mean, writing the new version of f3 in a way that I only need to copy-and-paste to R console and I?m good to go.  I know it?s kind of stupid but I?m curious to know) 

I'm using R-2.5.1, on WinXP.

Many thanks,

?Tao


_________________________________________________________________


07

From ggrothendieck at gmail.com  Thu Dec  6 23:40:04 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 6 Dec 2007 17:40:04 -0500
Subject: [R] simple problems
In-Reply-To: <14200401.post@talk.nabble.com>
References: <14200401.post@talk.nabble.com>
Message-ID: <971536df0712061440u18614138wcfde3768c4c58c9@mail.gmail.com>

On Dec 6, 2007 3:26 PM, marciarr <marcia.rocha at gmail.com> wrote:
>
> Hello R users,
> I have been looking through Help files and Nabble list for the answers for
> these simple questions, but it seems to be fruitless.
> 1- in a data frame with two columns, x and y, how do I get the corresponding
> value of x to, let's say, the minimum value of the y column (min (data$y)) ?

In the builtin Formaldehyde data frame find the value of carb (first column)
which corresponds to the least value of optden (second column):

with(Formaldehyde, carb[which.min(optden)])

> 2- how do I solve a simple equation? Considering the equation y= exp(-x)^12,
> I would like to find the values of x for,  for example, y=0.01, so
> exp(-x)^12=0.01. How do I do that using R?
> I know those a probably very, very simple questions, but for which I do not
> seem to find the answer.

Search between 0 and 1 for the root of the indicated function:

uniroot(function(x) 0.1 - exp(-x)^12, 0:1)


From ggrothendieck at gmail.com  Thu Dec  6 23:44:02 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 6 Dec 2007 17:44:02 -0500
Subject: [R] updating a helper function in a R package
In-Reply-To: <BAY120-W36138F3288572FAA2EA1EAC76F0@phx.gbl>
References: <BAY120-W36138F3288572FAA2EA1EAC76F0@phx.gbl>
Message-ID: <971536df0712061444p729188bfte073b00cfa2e138e@mail.gmail.com>

If you are using namespaces see:

?assignInNamespace

On Dec 6, 2007 5:38 PM, Tao Shi <shitao at hotmail.com> wrote:
>
> Hi list,
>
> Sorry for the vague title, but here is the scenario.
>
> I'm writing an R package, let's say, 'pkg1', which contains 3 functions: f1, f2, f3.  f2 and f3 are helper functions for f1, i.e. f1 calls f2 which in turn calls f3.
>
> f1 <- function(?) {
>        ?.
>        f2()
>        ?
> }
>
> f2 <- function(?){
>        ?
>        f3(?)
>        ?
> }
>
> f3 <- function(...){
>       ....
> }
>
> Then, I wrote a new version of f3 and I want to test it.  With the old version of 'pkg1' already loaded into my R session, I tried just copy-and-paste the new 'f3' to R console and hope f1 will pick the new 'f3' up.  It obviously didn't work.  I know it's b/c the new f3 and old f3 are in different environments and when f1 is called, only old f3 is used.  Then I tried to change the environment of new f3 to the same as the old f3's by calling:
>
> environment(f3) <- environment(pkg1:::f3)
>
> but it wasn't working either.
>
> So,
> 1)      Could somebody help me to put all these into perspectives?
> 2)      Is there an easier way to update f3 without rebuilding the package? (by that I mean, writing the new version of f3 in a way that I only need to copy-and-paste to R console and I'm good to go.  I know it's kind of stupid but I'm curious to know)
>
> I'm using R-2.5.1, on WinXP.
>
> Many thanks,
>
> ?Tao
>
>
> _________________________________________________________________
>
>
> 07
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From davidrees1 at yahoo.co.uk  Fri Dec  7 00:30:14 2007
From: davidrees1 at yahoo.co.uk (David Rees)
Date: Thu, 6 Dec 2007 23:30:14 +0000 (GMT)
Subject: [R]  How can I plot this graph
Message-ID: <446399.56442.qm@web25911.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071206/f9f75084/attachment.pl 

From charpent at bacbuc.dyndns.org  Fri Dec  7 00:13:23 2007
From: charpent at bacbuc.dyndns.org (Emmanuel Charpentier)
Date: Fri, 07 Dec 2007 00:13:23 +0100
Subject: [R] simple problems
In-Reply-To: <971536df0712061440u18614138wcfde3768c4c58c9@mail.gmail.com>
References: <14200401.post@talk.nabble.com>
	<971536df0712061440u18614138wcfde3768c4c58c9@mail.gmail.com>
Message-ID: <fj9vmk$c03$1@ger.gmane.org>

Gabor Grothendieck a ?crit :
> On Dec 6, 2007 3:26 PM, marciarr <marcia.rocha at gmail.com> wrote:

[ Snip.... ]

>> 2- how do I solve a simple equation? Considering the equation y= exp(-x)^12,
>> I would like to find the values of x for,  for example, y=0.01, so
>> exp(-x)^12=0.01. How do I do that using R?
>> I know those a probably very, very simple questions, but for which I do not
>> seem to find the answer.
> 
> Search between 0 and 1 for the root of the indicated function:
> 
> uniroot(function(x) 0.1 - exp(-x)^12, 0:1)

I beg your pardon ?

I'd rather use high-school algebra/analysis :
log(exp(-x)^12)=log(0.01)
12log(exp(-x)=log(0.01)
-12x=log(0.01)
x=-log(0.01)/12=log(100)/12

Rushing for a sophisticated numerical tool without thinking for an
explicit solution is easy, fast, lazy (I do that every day ...), but
deprives you of the process of understanding the problem.

Which was probably the point of this (probable) homework...

					Emmanuel Charpentier


From charpent at bacbuc.dyndns.org  Fri Dec  7 00:00:21 2007
From: charpent at bacbuc.dyndns.org (Emmanuel Charpentier)
Date: Fri, 07 Dec 2007 00:00:21 +0100
Subject: [R] using "eval(parse(text)) "  , gsub(pattern, replacement, x) ,
 to process "code" within a loop/custom function
In-Reply-To: <346709.39944.qm@web59302.mail.re1.yahoo.com>
References: <346709.39944.qm@web59302.mail.re1.yahoo.com>
Message-ID: <fj9uu5$9n4$1@ger.gmane.org>

Thomas Pujol a ?crit :
> R-help users,
>   Thanks in advance for any assistance ... I truly appreciate your expertise.  I searched help and could not figure this out, and think you can probably offer some helpful tips. I apologize if I missed something, which I'm sure I probably did. 
>    
>   I have data for many "samples". (e.g. 1950, 1951, 1952, etc.)
> 
>   For each "sample", I have many data-frames. (e.g. temp.1952, births.1952, gdp.1952, etc.)
> 
>   (Because the data is rather "large" (and for other reasons), I have chosen to store the data as individual files, as opposed to a list of data frames.)
>    
>   I wish to write a function that enables me to "run" any of many custom "functions/processes" on each sample of data.
> 
>   I currently accomplish this by using a custom function that uses:
> "eval(parse(t=text.i2)) ", and "gsub(pat, rep, x)" (this changes the "sample number" for each line of text I submit to "eval(parse(t=text.i2))" ).
> 
>   Is there a better/preferred/more flexible way to do this?

Beware : what follows is the advice of someone used to use RDBMS and SQL
to work with data ; as anyone should know, everything is a nail to a man
with a hammer. Caveat emptor...

Unless I misunderstand you, you are trying to treat piecewise a large
dataset made of a large number of reasonably-sized independent chunks.

What you're trying to do seems to me a bit reinventing SAS macro
language. What's the point ?

IMNSHO, "large" datasets that are used only piecewise are much better
handled in a real database (RDBMS), queried at runtime via, for example,
Brian Ripley's RODBC.

In your example, I'd create a table births with all your data + the
relevant year. Out of the top of my mind :

# Do that ONCE in the lifetime of your data : a RDBMS is probably more
# apt than R dataframes for this kind of management

library(RODBC)
channel<-odbcConnect(WhateverYouHaveToUseForYourFavoriteDBMS)

sqlSave(channel, tablename="Births",
        rbind(cbind(data.frame(Year=rep(1952,nrow(births.1952))),
                    births.1952),
              cbind(data.frame(Year=rep(1953,nrow(births.1953))),
                    births.1953),
# ... ^W^Y ad nauseam ...
))

rm(births.1951, births.1952, ...) # get back breathing space

Beware : certain data types may be tricky to save ! I got bitten by
Dates recently... See RODBC documentation, your DBMS documentation and
the "R Data Import/Export guide"...

At analysis time, you may use the result of the relevant query exactly
as one of your dataframes. instead of :
foo(... data=birth.1952, ...)
type :
foo(... data=sqlQuery(channel,"select * from \"Births\" where
\"Year\"=1952;", ...) # Syntax illustrating talking to a "picky" DBMS...

Furthermore, the variable "Year" bears your "d" information. Problem
(dis)solved.

You may loop (or even sapply()...) at will on d :
for(year in 1952:1978) {
  query<-sprintf("select * from \"Births\" where \"Year\"=%d;",year)
  foo(... data=sqlQuery(channel,query), ...)
  ...
}

If you already use a DBMS with some connection to R (via RODBC or
otherwise), use that. If not, sqlite is a very lightweight library that
enables you to use a (very considerable) subset of SQL92 to manipulate
your data.

I understand that some people of this list have undertaken the creation
of a sqlite-based package dedicated to this kind of large data management.

HTH,

					Emmanuel Charpentier


From ggrothendieck at gmail.com  Fri Dec  7 00:42:06 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 6 Dec 2007 18:42:06 -0500
Subject: [R] simple problems
In-Reply-To: <fj9vmk$c03$1@ger.gmane.org>
References: <14200401.post@talk.nabble.com>
	<971536df0712061440u18614138wcfde3768c4c58c9@mail.gmail.com>
	<fj9vmk$c03$1@ger.gmane.org>
Message-ID: <971536df0712061542w7d4cddbekde1a1a5d0c074df6@mail.gmail.com>

On Dec 6, 2007 6:13 PM, Emmanuel Charpentier <charpent at bacbuc.dyndns.org> wrote:
> Gabor Grothendieck a ?crit :
> > On Dec 6, 2007 3:26 PM, marciarr <marcia.rocha at gmail.com> wrote:
>
> [ Snip.... ]
>
> >> 2- how do I solve a simple equation? Considering the equation y= exp(-x)^12,
> >> I would like to find the values of x for,  for example, y=0.01, so
> >> exp(-x)^12=0.01. How do I do that using R?
> >> I know those a probably very, very simple questions, but for which I do not
> >> seem to find the answer.
> >
> > Search between 0 and 1 for the root of the indicated function:
> >
> > uniroot(function(x) 0.1 - exp(-x)^12, 0:1)
>
> I beg your pardon ?
>
> I'd rather use high-school algebra/analysis :
> log(exp(-x)^12)=log(0.01)
> 12log(exp(-x)=log(0.01)
> -12x=log(0.01)
> x=-log(0.01)/12=log(100)/12
>
> Rushing for a sophisticated numerical tool without thinking for an
> explicit solution is easy, fast, lazy (I do that every day ...), but
> deprives you of the process of understanding the problem.
>

The posting instructions ask for minimal code and its likely that the
poster did not literally mean that that was the eventual problem.


From luke at stat.uiowa.edu  Fri Dec  7 00:42:11 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Thu, 6 Dec 2007 17:42:11 -0600 (CST)
Subject: [R] R on a multi core unix box
In-Reply-To: <e8231c250712060823h62fb3178ib4a73ad1a21796fc@mail.gmail.com>
References: <e8231c250712060823h62fb3178ib4a73ad1a21796fc@mail.gmail.com>
Message-ID: <Pine.OSX.4.64.0712061740020.469@luke-tierneys-computer-3.local>

You can use the socket implementation of snow, with

     library(snow)
     cl <- makeCluster(rep("localhost", 2), type="SOCK")

to start up a cluster of 2 R processes.

luke

On Thu, 6 Dec 2007, Saeed Abu Nimeh wrote:

> Hi,
> I installed the snow package on a unix box that has multiple cores. To be
> able to exploit the multiple cores (on one pc) do I still need to install
> the rmpi package (or rpvm). Another question, if i run a bayesian simulation
> on the multiple core after setting them up correctly (using snow), would you
> think there will be a noticeable speedup gain.
> Thanks,
> Saeed
> ---
>
> linux centos
> 4 dual core processors
> 32 gb ram
> R (2.6.0)
> snow 0.29
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From rdbriscoe at ucdavis.edu  Fri Dec  7 01:14:17 2007
From: rdbriscoe at ucdavis.edu (Ryan Briscoe Runquist)
Date: Thu, 6 Dec 2007 16:14:17 -0800 (PST)
Subject: [R] AIC v. extractAIC
Message-ID: <200712070014.lB70EH2t021662@citheronia.ucdavis.edu>


Hello,

I am using a simple linear model and I would like to get an AIC value.  I
came across both AIC() and extractAIC() and I am not sure which is best to
use.  I assumed that I should use AIC for a glm and extractAIC() for lm,
but if I run my model in glm the AIC value is the same if I use AIC() on an
lm object.  What might be going on?  Did I interpret these functions
incorrectly?

Thanks,
Ryan


~~~~~~~~~~~~~~~~~~
Ryan D. Briscoe Runquist
Population Biology Graduate Group
University of California, Davis
rdbriscoe at ucdavis.edu


From engrav at u.washington.edu  Fri Dec  7 01:29:56 2007
From: engrav at u.washington.edu (Loren Engrav)
Date: Thu, 06 Dec 2007 16:29:56 -0800
Subject: [R] Junk or not Junk ???
In-Reply-To: <14193897.post@talk.nabble.com>
Message-ID: <C37DD404.1578E%engrav@u.washington.edu>

As for news readers
I found R and R.mac and R.Bio on the sites you recommend, thank you very
much, they would avoid the individual emails, but then I would have to go
look at them, which might be Ok

Deluge? Well, there are from R and Bio and R-Mac every morning 30 or 35, and
10-15 more during the daytime, and ~50 deletes is painful

But then every morning one or two are useful so...

Still would be fun to understand why some R are junk and some are not

For example
Today received two emails from the same person, one junk and the other not

The Not Junk then went
from sweep.unicas.it
to phil2.ethz.ch 
to hypatia.math.ethz.ch

The Junk went 
from sweep.unicas.it directly to
to hypatia.math.ethz.ch

Otherwise the same, so why one junk and the other not?

Thank you

Engrav
Univ Washington
Seattle


> From: David Hewitt <dhewitt at vims.edu>
> Date: Thu, 6 Dec 2007 07:13:12 -0800 (PST)
> To: <r-help at r-project.org>
> Subject: Re: [R] Junk or not Junk ???
> 
> 
> 
> Loren Engrav wrote:
>> 
>> Thank you
>> 
>> As per advice from several R users I have set
>> 
>> r-project.org, stat.math.ethz.ch,fhcrc.org, stat.ethz.ch, math.ethz.ch,
>> hypatia.math.ethz.ch
>> 
>>  all to be "safe domains"
>> 
>> But still some R emails go to Junk and require to be found manually
>> 
>> I have explored the issue with Univ Wash computing to no avail
>> 
>> Is this just how it is or have I still missed the "fix" to keep R emails
>> out
>> of junk?
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> I suspect most people that stay on this list read most posts through a news
> reader... you might consider doing the same. I can't even imagine keeping up
> with the daily deluge of individual emails.
> 
> http://www.nabble.com/R-help-f13820.html
> 
> http://news.gmane.org/gmane.comp.lang.r.general
> 
> 
> -----
> David Hewitt
> Virginia Institute of Marine Science
> http://www.vims.edu/fish/students/dhewitt/
> -- 
> View this message in context:
> http://www.nabble.com/Junk-or-not-Junk-tf4940701.html#a14193897
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From hoontaechung at gmail.com  Fri Dec  7 03:06:09 2007
From: hoontaechung at gmail.com (=?EUC-KR?B?waQgxcLIxg==?=)
Date: Fri, 7 Dec 2007 11:06:09 +0900
Subject: [R] R help mailing system configuration change?
Message-ID: <CAEBF111-283C-46A0-983D-76DD5D1E0F07@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: ?????? ?? ????????.
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/bfb7bf39/attachment.pl 

From g.abraham at ms.unimelb.edu.au  Fri Dec  7 03:10:41 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Fri, 07 Dec 2007 13:10:41 +1100
Subject: [R] Make natural splines constant outside boundary
Message-ID: <4758ABA1.3040708@ms.unimelb.edu.au>

Hi,

I'm using natural cubic splines from splines::ns() in survival 
regression (regressing inter-arrival times of patients to a queue on 
queue size). The queue size fluctuates between 3600 and 3900.

I would like to be able to run predict.survreg() for sizes <3600 and 
 >3900 by assuming that the rate for <3600 is the same as for 3600 and 
that for >4000 it's the same as for 4000 (i.e., keep the splines cubic 
within the boundaries but make them constant outside the boundaries).

(By default, natural splines will be linear outside the boundaries, 
which is very bad here because the predictions grow without bound. 
Setting the boundary knots to extreme values such as (0, 10000) doesn't 
help, because there are no intermediate knots to constrain the spline 
and you get a very large cubic curve between these boundaries and the 
observed sizes.)

Any suggestions?

Thanks,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From Brendan.Power at dpi.qld.gov.au  Fri Dec  7 03:35:12 2007
From: Brendan.Power at dpi.qld.gov.au (Power, Brendan D (Toowoomba))
Date: Fri, 7 Dec 2007 12:35:12 +1000
Subject: [R] Segmented regression
Message-ID: <200712070235.lB72ZEI6012426@dpi-gw1.dpi.qld.gov.au>

Hello Vito,

Thanks for your reply and apologies for not being clearer. 

I'd like to fit a three-segmented relationship to each level but have only 3 unique breakpoints. The result would be 9 slopes, one of which would be zero.

I achieved this by finding the 3 breakpoint with:

init.bp <- c(297.4,639.6,680.6)
lm.1 <- lm(y~tt+group,data=df)
seg.1 <- segmented(lm.1, seg.Z=~tt, psi=list(tt=init.bp))

The starting values of init.bp came from a grid search and are that which minimised residuals.

I then used these breakpoints to get the 9 coefficients (which I omitted from original email for brevity):

df.KW <- df[df$group=="KW",]
lm.KW <- lm(y~tt,data=df.KW)
seg.KW <- segmented(lm.KW, seg.Z=~tt, psi=list(tt=seg.1$psi[,"Est."]),control = list(it.max = 0))

And similarly for the other 2 levels. This was done just for plotting purposes, my main interest is in the identification of the breakpoints.

Btw I'd appreciate a copy of your rnews article.

Thanks for you help,

Brendan.


-----Original Message-----
From: vito muggeo [mailto:vmuggeo at dssm.unipa.it] 
Sent: Thursday, 6 December 2007 7:55 PM
To: Power, Brendan D (Toowoomba)
Cc: r-help at r-project.org
Subject: Re: [R] Segmented regression

Dear Brendan,
I am not sure to understand your code..

It seems to me that your are interested in fitting a one-breakpoint segmented relationship in each level of your grouping variable

If this is the case, the correct code is below.
In order to fit a segmented relationship in each group you have to define the relevant variable before fitting, and to constrain the last slope to be zero you have to consider the `minus' variable..(I discuss these points in the submitted Rnews article..If you are interested, let me know off list).

If my code does not fix your problem, please let me know,

Best,
vito

##--define the group-specific segmented variable:
X<-model.matrix(~0+factor(group),data=df)*df$tt
df$tt.KV<-X[,1] #KV
df$tt.KW<-X[,2]   #KW
df$tt.WC<-X[,3]   #WC

##-fit the unconstrained model
olm<-lm(y~group+tt.KV+tt.KW+tt.WC,data=df)
os<-segmented(olm,seg.Z=~tt.KV+tt.KW+tt.WC,psi=list(tt.KV=350,
tt.KW=500, tt.WC=350))
#have a look to results:
with(df,plot(tt,y))
with(subset(df,group=="RKW"),points(tt,y,col=2))
with(subset(df,group=="RKV"),points(tt,y,col=3))
points(df$tt[df$group=="RWC"],fitted(os)[df$group=="RWC"],pch=20)
points(df$tt[df$group=="RKW"],fitted(os)[df$group=="RKW"],pch=20,col=2)
points(df$tt[df$group=="RKV"],fitted(os)[df$group=="RKV"],pch=20,col=3)


#constrain the last slope in group KW
tt.KW.minus<- -df$tt.KW
olm1<-lm(y~group+tt.KV+tt.WC,data=df)
os1<-segmented(olm1,seg.Z=~tt.KV+tt.KW.minus+tt.WC,psi=list(tt.KV=350, 
tt.KW.minus=-500, tt.WC=350))
#check..:-)
slope(os1)

with(df,plot(tt,y))
with(subset(df,group=="RKW"),points(tt,y,col=2))
with(subset(df,group=="RKV"),points(tt,y,col=3))
points(df$tt[df$group=="RWC"],fitted(os1)[df$group=="RWC"],pch=20)
points(df$tt[df$group=="RKW"],fitted(os1)[df$group=="RKW"],pch=20,col=2)
points(df$tt[df$group=="RKV"],fitted(os1)[df$group=="RKV"],pch=20,col=3)






Power, Brendan D (Toowoomba) ha scritto:
> Hello all,
> 
> I have 3 time series (tt) that I've fitted segmented regression models
> to, with 3 breakpoints that are common to all, using code below
> (requires segmented package). However I wish to specifiy a zero
> coefficient, a priori, for the last segment of the KW series (green)
> only. Is this possible to do with segmented? If not, could someone point
> in a direction?
> 
> The final goal is to compare breakpoint sets for differences from those
> derived from other data.
> 
> Thanks in advance,
> 
> Brendan.
> 
> 
> library(segmented)
> df<-data.frame(y=c(0.12,0.12,0.11,0.19,0.27,0.28,0.35,0.38,0.46,0.51,0.5
> 8,0.59,0.60,0.57,0.64,0.68,0.72,0.73,0.78,0.84,0.85,0.83,0.86,0.88,0.88,
> 0.95,0.95,0.93,0.92,0.97,0.86,1.00,0.85,0.97,0.90,1.02,0.95,0.54,0.53,0.
> 50,0.60,0.70,0.74,0.78,0.82,0.88,0.83,1.00,0.85,0.96,0.84,0.86,0.82,0.86
> ,0.84,0.84,0.84,0.77,0.69,0.61,0.67,0.73,0.65,0.55,0.58,0.56,0.60,0.50,0
> .50,0.42,0.43,0.44,0.42,0.40,0.51,0.60,0.63,0.71,0.74,0.82,0.82,0.85,0.8
> 9,0.91,0.87,0.91,0.93,0.95,0.95,0.97,1.00,0.96,0.90,0.86,0.91,0.94,0.96,
> 0.88,0.88,0.88,0.92,0.82,0.85),
>  
> tt=c(141.6,141.6,141.6,183.2,212.8,227.0,242.4,271.5,297.4,312.3,331.4,3
> 42.4,346.3,356.6,371.6,408.8,408.8,419.5,434.4,464.5,492.6,521.7,550.5,5
> 50.3,565.4,588.0,602.9,623.7,639.6,647.9,672.6,680.6,709.7,709.7,750.2,7
> 50.2,750.2,141.6,141.6,141.6,183.2,212.8,227.0,242.4,271.5,297.4,312.3,3
> 31.4,342.4,346.3,356.6,371.6,408.8,408.8,419.5,434.4,464.5,492.6,521.7,5
> 50.5,550.3,565.4,588.0,602.9,623.7,639.6,647.9,672.6,680.6,709.7,709.7,1
> 41.6,141.6,141.6,183.2,212.8,227.0,242.4,271.5,297.4,312.3,331.4,342.4,3
> 46.3,356.6,371.6,408.8,408.8,419.5,434.4,464.5,492.6,521.7,550.5,550.3,5
> 65.4,588.0,602.9,623.7,639.6,647.9,672.6,709.7),
>            group=c(rep("RKW",37),rep("RWC",34),rep("RKV",32)))
> init.bp <- c(297.4,639.6,680.6)
> lm.1 <- lm(y~tt+group,data=df)
> seg.1 <- segmented(lm.1, seg.Z=~tt, psi=list(tt=init.bp))
> 
>>  version
>                _                           
> platform       i386-pc-mingw32             
> arch           i386                        
> os             mingw32                     
> system         i386, mingw32               
> status                                     
> major          2                           
> minor          6.0                         
> year           2007                        
> month          10                          
> day            03                          
> svn rev        43063                       
> language       R                           
> version.string R version 2.6.0 (2007-10-03)
> 
> 
> 
> ********************************DISCLAIMER**************...{{dropped:15}}
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612
====================================

********************************DISCLAIMER****************************
The information contained in the above e-mail message or messages 
(which includes any attachments) is confidential and may be legally 
privileged.  It is intended only for the use of the person or entity 
to which it is addressed.  If you are not the addressee any form of 
disclosure, copying, modification, distribution or any action taken 
or omitted in reliance on the information is unauthorised.  Opinions 
contained in the message(s) do not necessarily reflect the opinions 
of the Queensland Government and its authorities.  If you received 
this communication in error, please notify the sender immediately and 
delete it from your computer system network.



From dwinsemius at comcast.net  Fri Dec  7 03:42:00 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 7 Dec 2007 02:42:00 +0000 (UTC)
Subject: [R] Building package - tab delimited example data issue
References: <20071206191941.5ceffb71@berwin-nus1>
	<4757EF5E.6040904@biostat.ku.dk>
Message-ID: <Xns99FEDCC43A841dNOTwinscomcast@80.91.229.13>

Peter Dalgaard <P.Dalgaard at biostat.ku.dk> wrote in
news:4757EF5E.6040904 at biostat.ku.dk: 

> The passage you cite from the manual could do with a
> rephrasing, although it probably isn't technically incorrect. As it
> stands, it reminds me a bit of the old Monty Python sketch:
>
> "Our *three* weapons are fear, surprise, and ruthless efficiency...and
> an almost fanatical devotion to the Pope.... Our *four*...no...
> *Amongst* our weapons.... Amongst our weaponry...are such elements as
> fear, surprise.... I'll come in again"
>
> (There really are 3 data TYPES, but 4 FORMATS and, er, diverse
> EXTENSIONS) 
> 

Is there a place where I can file a claim for a new keyboard? Mine has beer 
all over it.

-- 
David Winsemius


From bolker at ufl.edu  Fri Dec  7 04:11:25 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 6 Dec 2007 19:11:25 -0800 (PST)
Subject: [R] AIC v. extractAIC
In-Reply-To: <200712070014.lB70EH2t021662@citheronia.ucdavis.edu>
References: <200712070014.lB70EH2t021662@citheronia.ucdavis.edu>
Message-ID: <14206069.post@talk.nabble.com>




Ryan Briscoe Runquist wrote:
> 
> 
> Hello,
> 
> I am using a simple linear model and I would like to get an AIC value.  I
> came across both AIC() and extractAIC() and I am not sure which is best to
> use.  I assumed that I should use AIC for a glm and extractAIC() for lm,
> but if I run my model in glm the AIC value is the same if I use AIC() on
> an
> lm object.  What might be going on?  Did I interpret these functions
> incorrectly?
> 
> Thanks,
> Ryan
> 
> 

  The documentation for these functions does explain what's going on,
albeit fairly tersely.  The bottom line is that you should be OK with either
one, provided that you stick to one or the other.

set.seed(1001)
x = runif(100)
y = 1 + 2*x+3*x^2+rnorm(100,sd=0.02)

lm1 = lm(y~x)
lm2 = lm(y~x+I(x^2))

## different
AIC(lm1,lm2)
extractAIC(lm1)
extractAIC(lm2)

## the same
AIC(lm1)-AIC(lm2)
extractAIC(lm1)[2]-extractAIC(lm2)[2]

lm3 = glm(y~x)
lm4 = glm(y~x+I(x^2))

## the same
AIC(lm4)
extractAIC(lm4)

-- 
View this message in context: http://www.nabble.com/AIC-v.-extractAIC-tf4959483.html#a14206069
Sent from the R help mailing list archive at Nabble.com.


From hb at stat.berkeley.edu  Fri Dec  7 06:26:27 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Thu, 6 Dec 2007 21:26:27 -0800
Subject: [R] Junk or not Junk ???
In-Reply-To: <C37DD404.1578E%engrav@u.washington.edu>
References: <14193897.post@talk.nabble.com>
	<C37DD404.1578E%engrav@u.washington.edu>
Message-ID: <59d7961d0712062126g23ce2f5g556ef28ee29b3cc4@mail.gmail.com>

On 06/12/2007, Loren Engrav <engrav at u.washington.edu> wrote:
> As for news readers
> I found R and R.mac and R.Bio on the sites you recommend, thank you very
> much, they would avoid the individual emails, but then I would have to go
> look at them, which might be Ok
>
> Deluge? Well, there are from R and Bio and R-Mac every morning 30 or 35, and
> 10-15 more during the daytime, and ~50 deletes is painful
>
> But then every morning one or two are useful so...

This what you want to you email filtering for.  Create a (sub)folder
named "r-help", setup an email filter that sends all message that has
a subject starting with "[R] " to that folder.  That way they will not
clutter up your inbox, but you can still browser the r-help messages.

You haven't told us your email client, but pretty much any client I
know of supports this.  I use gmail as my client and there it is very
simple.  This way reading message is no different from reading them
via a news reader.

>
> Still would be fun to understand why some R are junk and some are not
>
> For example
> Today received two emails from the same person, one junk and the other not
>
> The Not Junk then went
> from sweep.unicas.it
> to phil2.ethz.ch
> to hypatia.math.ethz.ch
>
> The Junk went
> from sweep.unicas.it directly to
> to hypatia.math.ethz.ch
>
> Otherwise the same, so why one junk and the other not?

FYI, I very very rarely get false positives (from the r-help lists)
and hardly any spam for that sake (thanks!) and I've been on the list
for a long time.  I couldn't find a single one during the last 30 days
in my gmail spam box.

It is impossible to tell why some of your message are falsely
classified as spam without know what your email client is.  Some
clients have there own build in spam filtering that you can train by
pressing "This is spam/This is not spam", whereas others rely on their
email provider to analyze all messages and add a spam score in the
email header and then you can set up the client to filter those out
without much local analysis.  The latter is common at universities.

As already been suggested, it is more likely that this something that
you email provider/sys adm should be able to help you out with.  To me
it sounds unlikely that there is something "wrong" with the R messages
or that R mail server is at fault.

Hope this helps

/Henrik



>
> Thank you
>
> Engrav
> Univ Washington
> Seattle
>
>
> > From: David Hewitt <dhewitt at vims.edu>
> > Date: Thu, 6 Dec 2007 07:13:12 -0800 (PST)
> > To: <r-help at r-project.org>
> > Subject: Re: [R] Junk or not Junk ???
> >
> >
> >
> > Loren Engrav wrote:
> >>
> >> Thank you
> >>
> >> As per advice from several R users I have set
> >>
> >> r-project.org, stat.math.ethz.ch,fhcrc.org, stat.ethz.ch, math.ethz.ch,
> >> hypatia.math.ethz.ch
> >>
> >>  all to be "safe domains"
> >>
> >> But still some R emails go to Junk and require to be found manually
> >>
> >> I have explored the issue with Univ Wash computing to no avail
> >>
> >> Is this just how it is or have I still missed the "fix" to keep R emails
> >> out
> >> of junk?
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >
> > I suspect most people that stay on this list read most posts through a news
> > reader... you might consider doing the same. I can't even imagine keeping up
> > with the daily deluge of individual emails.
> >
> > http://www.nabble.com/R-help-f13820.html
> >
> > http://news.gmane.org/gmane.comp.lang.r.general
> >
> >
> > -----
> > David Hewitt
> > Virginia Institute of Marine Science
> > http://www.vims.edu/fish/students/dhewitt/
> > --
> > View this message in context:
> > http://www.nabble.com/Junk-or-not-Junk-tf4940701.html#a14193897
> > Sent from the R help mailing list archive at Nabble.com.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Fri Dec  7 07:07:22 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Dec 2007 01:07:22 -0500
Subject: [R] How can I plot this graph
In-Reply-To: <446399.56442.qm@web25911.mail.ukl.yahoo.com>
References: <446399.56442.qm@web25911.mail.ukl.yahoo.com>
Message-ID: <971536df0712062207ge5c8a45i568ff6a564474028@mail.gmail.com>

Try this:

matplot(xxx[,1], xxx[2:4], type = "l")
with(xxx, {
	segments(x, z1, x, z2)
	points(x, z1)
	points(x, z2)
})

Omit the two points commands if you don't want circles at the ends of
the segments.

On Dec 6, 2007 6:30 PM, David Rees <davidrees1 at yahoo.co.uk> wrote:
> Hi,
>
> I am having trouble plotting the graph I need given the follow kind of data
>
> > xxx <- data.frame(
>               "x"=c(1,2,3,4,5),
>               "y1"=c(2,4,3,5,6),
>               "y2"=c(3,4,6,3,1),
>               "y3"=c(1,3,5,7,3),
>               "z1"=c(1,NA,3,5,NA),
>               "z2"=c(2,NA,4,6,NA) )
> > xxx
>  x y1 y2 y3 z1 z2
> 1 1  2  3  1  1  2
> 2 2  4  4  3 NA NA
> 3 3  3  6  5  3  4
> 4 4  5  3  7  5  6
> 5 5  6  1  3 NA NA
>
> What I need is the following
>
> - One graph
> - y1, y2, y3 plotted as seperate lines vs x
> - z1, z2 give the range of the value of z, and need to be plotted as error bars or confidence intervals vs x
>
> Many thanks for any help,
> Regards,
> David
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From stephenc at ics.mq.edu.au  Fri Dec  7 07:24:05 2007
From: stephenc at ics.mq.edu.au (stephenc at ics.mq.edu.au)
Date: Fri, 7 Dec 2007 17:24:05 +1100 (EST)
Subject: [R] duplicate row.names are not allowed
Message-ID: <1129.60.241.49.44.1197008645.squirrel@webmail.ics.mq.edu.au>

I am using read.table and keep on getting this message.

The function is confusing my first column with a row.names column.  I have
checked the table carefully using excel and it seems quite symetric and
with a name at the top of each column.

This is what I am using:

form1 = read.table("c:/horses/form.tbl", sep="\t", header = T)

and this is the top few rows of the table:

fields.result	fields.age	fields.sex	fields.barrier	horse.winnings	
horse.form.ran	horse.form.win	horse.form.place	horse.form.at.this.track.ran	horse.form.at.this.track.win	horse.form.at.this.track.place	horse.form.at.this.distance.ran	horse.form.at.this.distance.win	horse.form.at.this.distance.place	horse.form.with.this.jockey.ran	horse.form.with.this.jockey.win	horse.form.with.this.jockey.place	horse.form.with.this.going.ran	horse.form.with.this.going.win	horse.form.with.this.going.place	rider.rides.last.3	rider.places.last.3	trainer.rides.last.3	trainer.places.last.3
4	4	G	3	50	5	0	0	0	0	0	0	0	0	0	0	0	1	0	0	0	0	36	0.166666667
2	5	G	5	3250	9	0	0.333333333	0	0	0	0	0	0	5	0	0.4	3	0	0.333333333	35	0.114285714	7	0.285714286
3	4	M	4	2075	11	0	0.090909091	0	0	0	0	0	0	0	0	0	1	0	0	0	0	13	0.307692308
5	5	M	2	0	2	0	0	0	0	0	0	0	0	1	0	0	1	0	0	54	0.185185185	2	0
1	4	M	7	1800	4	0	0.5	0	0	0	0	0	0	0	0	0	2	0	0	0	0	90	0.366666667

Can anyone suggest whats wrong or a workaround.

Thanks

Stephen


From ggrothendieck at gmail.com  Fri Dec  7 07:33:21 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Dec 2007 01:33:21 -0500
Subject: [R] duplicate row.names are not allowed
In-Reply-To: <1129.60.241.49.44.1197008645.squirrel@webmail.ics.mq.edu.au>
References: <1129.60.241.49.44.1197008645.squirrel@webmail.ics.mq.edu.au>
Message-ID: <971536df0712062233k6aab5836t18b0e24770842c7a@mail.gmail.com>

The message seems to be messed up but if your data has one more column
than the header then it will assume the first column is the row names.
Use the R count.fields function to diagnose this.


On Dec 7, 2007 1:24 AM,  <stephenc at ics.mq.edu.au> wrote:
> I am using read.table and keep on getting this message.
>
> The function is confusing my first column with a row.names column.  I have
> checked the table carefully using excel and it seems quite symetric and
> with a name at the top of each column.
>
> This is what I am using:
>
> form1 = read.table("c:/horses/form.tbl", sep="\t", header = T)
>
> and this is the top few rows of the table:
>
> fields.result   fields.age      fields.sex      fields.barrier  horse.winnings
> horse.form.ran  horse.form.win  horse.form.place        horse.form.at.this.track.ran    horse.form.at.this.track.win    horse.form.at.this.track.place  horse.form.at.this.distance.ran horse.form.at.this.distance.win horse.form.at.this.distance.place       horse.form.with.this.jockey.ran horse.form.with.this.jockey.win horse.form.with.this.jockey.place       horse.form.with.this.going.ran  horse.form.with.this.going.win  horse.form.with.this.going.place        rider.rides.last.3      rider.places.last.3     trainer.rides.last.3    trainer.places.last.3
> 4       4       G       3       50      5       0       0       0       0       0       0       0       0       0       0       0       1       0       0       0       0       36      0.166666667
> 2       5       G       5       3250    9       0       0.333333333     0       0       0       0       0       0       5       0       0.4     3       0       0.333333333     35      0.114285714     7       0.285714286
> 3       4       M       4       2075    11      0       0.090909091     0       0       0       0       0       0       0       0       0       1       0       0       0       0       13      0.307692308
> 5       5       M       2       0       2       0       0       0       0       0       0       0       0       1       0       0       1       0       0       54      0.185185185     2       0
> 1       4       M       7       1800    4       0       0.5     0       0       0       0       0       0       0       0       0       2       0       0       0       0       90      0.366666667
>
> Can anyone suggest whats wrong or a workaround.
>
> Thanks
>
> Stephen
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From leffgh at 163.com  Fri Dec  7 08:17:52 2007
From: leffgh at 163.com (Bin Yue)
Date: Thu, 6 Dec 2007 23:17:52 -0800 (PST)
Subject: [R]  what actually a logistic regression fits
Message-ID: <14207920.post@talk.nabble.com>


Dear all:
    I did the following because I was not sure what a logistic regression
fits ."small.glm" is a glm fit , setting the family to be binomial.

>fitted(small.glm)->p
>log(p/(1-p))->left   
> for(i in 1:length(coef(small.glm))){
+  coef(small.glm)[i]*model.matrix(small.glm)[,i]->res[,i]
+ }
> apply(res,1,sum)->right
> plot(left,right)  #I got a straight line whose slope was about 1
Therefore ,I am almost sure that what a logistic regression fits is
log(p/(1-p))~b0+b1*x.
Is that right? 
Regards,

-----
Best regards,
Bin Yue

*************
student for a Master program in South Botanical Garden , CAS

-- 
View this message in context: http://www.nabble.com/what-actually-a-logistic-regression-fits-tf4960618.html#a14207920
Sent from the R help mailing list archive at Nabble.com.


From petr.pikal at precheza.cz  Fri Dec  7 08:23:37 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Fri, 7 Dec 2007 08:23:37 +0100
Subject: [R] Odp:  duplicate row.names are not allowed
In-Reply-To: <1129.60.241.49.44.1197008645.squirrel@webmail.ics.mq.edu.au>
Message-ID: <OFC336A2A7.50BCC932-ONC12573AA.00282D7A-C12573AA.00289855@precheza.cz>

Hi

No error message? It seems to me that it is OK. There is of course limited 
width for printout on screen (see GUI preferences/console columns), and 
therefore everything which exceeds specified limits is printed on second 
and subsequent row.

try

dim(form1) or str(form1) for evaluation of your read data frame.

Regards
Petr

r-help-bounces at r-project.org napsal dne 07.12.2007 07:24:05:

> I am using read.table and keep on getting this message.
> 
> The function is confusing my first column with a row.names column.  I 
have
> checked the table carefully using excel and it seems quite symetric and
> with a name at the top of each column.
> 
> This is what I am using:
> 
> form1 = read.table("c:/horses/form.tbl", sep="\t", header = T)
> 
> and this is the top few rows of the table:
> 
> fields.result   fields.age   fields.sex   fields.barrier horse.winnings  

> horse.form.ran   horse.form.win   horse.form.place 
horse.form.at.this.track.
> ran   horse.form.at.this.track.win   horse.form.at.this.track.place 
horse.
> form.at.this.distance.ran   horse.form.at.this.distance.win 
horse.form.at.
> this.distance.place   horse.form.with.this.jockey.ran 
horse.form.with.this.
> jockey.win   horse.form.with.this.jockey.place 
horse.form.with.this.going.
> ran   horse.form.with.this.going.win   horse.form.with.this.going.place  

> rider.rides.last.3   rider.places.last.3   trainer.rides.last.3 
trainer.places.last.3
> 4   4   G   3   50   5   0   0   0   0   0   0   0   0   0   0   0   1 0 
  0
> 0   0   36   0.166666667
> 2   5   G   5   3250   9   0   0.333333333   0   0   0   0   0   0   5 0 
 
> 0.4   3   0   0.333333333   35   0.114285714   7   0.285714286
> 3   4   M   4   2075   11   0   0.090909091   0   0   0   0   0   0   0  
0 
> 0   1   0   0   0   0   13   0.307692308
> 5   5   M   2   0   2   0   0   0   0   0   0   0   0   1   0   0   1 0  
0 
> 54   0.185185185   2   0
> 1   4   M   7   1800   4   0   0.5   0   0   0   0   0   0   0   0   0 2 
  0
> 0   0   0   90   0.366666667
> 
> Can anyone suggest whats wrong or a workaround.
> 
> Thanks
> 
> Stephen
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From troutman23 at gmail.com  Fri Dec  7 08:26:29 2007
From: troutman23 at gmail.com (Phil taylor)
Date: Fri, 7 Dec 2007 18:26:29 +1100
Subject: [R] pvclust warning message
Message-ID: <116bbb3c0712062326h59609327n8acc3155d5dae092@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/0407c49e/attachment.pl 

From dieter.menne at menne-biomed.de  Fri Dec  7 08:40:30 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 7 Dec 2007 07:40:30 +0000 (UTC)
Subject: [R] R help mailing system configuration change?
References: <CAEBF111-283C-46A0-983D-76DD5D1E0F07@gmail.com>
Message-ID: <loom.20071207T073152-670@post.gmane.org>

? ?? <hoontaechung <at> gmail.com> writes:

> I got a reply for my previous several postings saying that I was  
> spamming the r-help mailing list.
> I am very sorry to all subscribers if I did that.
> But I've been reposting my message to the mailing list several times  
> because I didn't know whether my help post was actually posted or not.
> I remember from my previous experiences that, when I post a message, I  
> can see my own posting myself.
> But this time, I didn't see my own message so I thought my message got  
> dropped for some reasons.
> Was there any change in r-help mailing system configuration?
> 
> Thanks in advance,
> 
> Tae-Hoon Chung
> 
> Korea Centers for Disease Control & Prevention (KCDC)
> Korea National Institute of Health (KNIH)
> Center for Genome Sciences, Biobank for Health Sciences
> 
> 194 Tongil-ro, Eunpyoung-gu, Seoul, 122-701, Korea
> 

I have to second that, the same for me. The only way to access this list is via
gmane, otherwise orbitl.com (a Ceylon-based anti-spamming list) will jump in. We
have a dynamic address, so this list seems ban whole ranges.

I have tried to remove me from orbitl; it may have worked or not, because of the
dynamic address I do not always track. I tried to contact the webmaster at ETH
twice, but got no response. 

This is an annoyance; I know that I should contact orbitl every time, but since
that organization is a mess, it better would be removed from the ETH
anti-spamming list.

Dieter


From petr.pikal at precheza.cz  Fri Dec  7 08:46:20 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Fri, 7 Dec 2007 08:46:20 +0100
Subject: [R] testing independence of categorical variables
In-Reply-To: <14202348.post@talk.nabble.com>
Message-ID: <OFA8B40C90.79D9F859-ONC12573AA.0029493A-C12573AA.002AACC3@precheza.cz>

Hi

Well, R does exactly what it says. From help page.

"Otherwise, x and y must be vectors or factors of the same length"

I do not know SAS but I presume that

> tables bloodtype*state

gives you something like

tab <- table(bloodtype, state)

and

chisq.test(tab)

shall give you the expected result. You can also do directly 
chisq.test(bloodtype, state). But what you cannot do is to test vectors 
unequal **lengths**, and that is what he did. I beleve that you can not do 
it in SAS either.
 
 x<-sample(letters[1:3], 10, replace=T)
 x
 [1] "c" "a" "c" "c" "a" "c" "a" "c" "a" "a"
 y<-sample(1:5, 20, replace=T)
> y
 [1] 2 5 1 1 2 5 2 3 1 5 5 5 1 5 5 3 2 2 5 1
> chisq.test(x,y)
Error in chisq.test(x, y) : 'x' and 'y' must have the same length
 x<-sample(letters[1:3], 20, replace=T)

> chisq.test(x,y)

        Pearson's Chi-squared test

data:  x and y 
X-squared = 4.7937, df = 6, p-value = 0.5705

Warning message:
In chisq.test(x, y) : Chi-squared approximation may be incorrect
>

Regards
Petr


r-help-bounces at r-project.org napsal dne 06.12.2007 23:09:24:

> 
> The chi-square does not need your two categorical variables to have 
equal
> levels, nor limitation for the number of levels.
> 
> The Chi-square procedure is as follow:
> ?^2=?_(All Cells)??(Observed-Expected)?^2/Expected
> 
> Expected Cell= E_ij=n((i^th RowTotal)/n)((j^th RowTotal)/n)
> 
> Degree of Freedom=df= (row-1)(Col-1)
> 
> This way should not give you any errors if your calculations are all
> correct. I usually use SAS for calculations like this. Below is a sample
> code I wrote to test whether US_State and Blood type are independent. 
You
> can modify it for your data and should give you no error.
> 
> data bloodtype;
> input bloodtype$ state$ count@@;
> datalines;
> A FL 122 B FL 117
> AB FL 19 O FL 244
> A IA 1781 B IA 351
> AB IA 289 O IA 3301
> A MO 353 B MO 269
> AB MO 60 O MO 713
> ;
> proc freq data=bloodtype;
> tables bloodtype*state
> / cellchi2 chisq expected norow nocol nopercent;
> weight count;
> quit;
> 
> 
> Best
> Ramin
> Gainesville
> 
> 
> 
> Shoaaib Mehmood wrote:
> > 
> > hi,
> > 
> > is there a way of calculating of measuring dependence between two
> > categorical variables. i tried using the chi square test to test for
> > independence but i got error saying that the lengths of the two
> > vectors don't match. Suppose X and Y are two factors. X has 5 levels
> > and Y has 7 levels. This is what i tried doing
> > 
> >>temp<-chisq.test(x,y)
> > 
> > but got error "the lengths of the two vectors don't match". any help
> > will be appreciated
> > -- 
> > Regards,
> > Rana Shoaaib Mehmood
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> 
> -- 
> View this message in context: 
http://www.nabble.com/testing-independence-of-
> categorical-variables-tf4855773.html#a14202348
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From leffgh at 163.com  Fri Dec  7 08:55:23 2007
From: leffgh at 163.com (Bin Yue)
Date: Thu, 6 Dec 2007 23:55:23 -0800 (PST)
Subject: [R] paradox about the degree of freedom in a logistic regression
 model
Message-ID: <14208306.post@talk.nabble.com>


 Dear all:
   "predict.glm" provides an example to perform logistic regression when the
response variable is a tow-columned  matrix. I find some paradox about the
degree of freedom  .
 > summary(budworm.lg)

Call:
glm(formula = SF ~ sex * ldose, family = binomial)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.39849  -0.32094  -0.07592   0.38220   1.10375  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -2.9935     0.5527  -5.416 6.09e-08 ***
sexM          0.1750     0.7783   0.225    0.822    
ldose         0.9060     0.1671   5.422 5.89e-08 ***
sexM:ldose    0.3529     0.2700   1.307    0.191    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 124.8756  on 11  degrees of freedom
Residual deviance:   4.9937  on  8  degrees of freedom
AIC: 43.104

Number of Fisher Scoring iterations: 4

This is the data set used in regression:
  numdead numalive sex ldose
1        1       19   M     0
2        4       16   M     1
3        9       11   M     2
4       13        7   M     3
5       18        2   M     4
6       20        0   M     5
7        0       20   F     0
8        2       18   F     1
9        6       14   F     2
10      10       10   F     3
11      12        8   F     4
12      16        4   F     5

     The degree of freedom is 8. Each row in the example is thought to be
one observation. If  I extend it to be a three column data.frame, the first
denoting the whether the individual is alive , the secode denoting the sex,
and the third "ldose",there will be 12*20=240 observations. 
     Since my data set is one of the second type , I wish to know whether
the form of data set affects the result of regression ,such as the degree of
freedom.
   Dose anybody have any idea about this? Thank all who read this message.
   Regards,
   Bin Yue

-----
Best regards,
Bin Yue

*************
student for a Master program in South Botanical Garden , CAS

-- 
View this message in context: http://www.nabble.com/paradox-about-the-degree-of-freedom-in-a-logistic-regression-model-tf4960753.html#a14208306
Sent from the R help mailing list archive at Nabble.com.


From ottorino-luca.pantani at unifi.it  Fri Dec  7 08:56:40 2007
From: ottorino-luca.pantani at unifi.it (Ottortino-Luca Pantani)
Date: Fri, 07 Dec 2007 08:56:40 +0100
Subject: [R] Recommended textbooks for R?
In-Reply-To: <mn.e1ca7d7bb3ad1ad5.83239@exitcheck.net>
References: <mn.e1ca7d7bb3ad1ad5.83239@exitcheck.net>
Message-ID: <4758FCB8.7000203@unifi.it>

Max ha scritto:
> Hi everyone!
>
> I've recently begun to learn R for my job as the IT department suffers 
> from lack of funding for new software. I was talking to the guy in 
> charge of Requisitions and have found out the budget for books is in 
> great shape.
>
> So, I'm curious what books people know of that have R examples and are 
> good for:
>
> 1.) Uni and Multivariate Time Series Analysis/Forecasting
>
> 2.) GLMs (at this point it looks like I'll be focusing on nominal and 
> ordinal regression models)
>
> 3.) Survival Models
>
> 4.) Multiple Regression
>
> Any suggestions would be awesome. :)
>
> thanks,
>
> -Max
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   
I would add

R graphics by Paul Murrell

http://www.amazon.com/Graphics-Computer-Science-Data-Analysis/dp/158488486X
may be a little difficult at the beginning, but very useful.



-- 
Ottorino-Luca Pantani, Universit? di Firenze
Dip. Scienza del Suolo e Nutrizione della Pianta
P.zle Cascine 28 50144 Firenze Italia
Tel 39 055 3288 202 (348 lab) Fax 39 055 333 273 
OLPantani at unifi.it  http://www4.unifi.it/dssnp/


From zhangchicool at gmail.com  Fri Dec  7 08:58:13 2007
From: zhangchicool at gmail.com (kexinz)
Date: Thu, 6 Dec 2007 23:58:13 -0800 (PST)
Subject: [R]  how to generate uniformly distributed random integers
Message-ID: <14208376.post@talk.nabble.com>


   I'm a beginner of R. 
   I can use runif() to generate uniformly distributed numbers, but I don't
know which function can generate uniformly distributed random integers, or
what kind of method?
   Thanks!
-- 
View this message in context: http://www.nabble.com/how-to-generate-uniformly-distributed-random-integers-tf4960778.html#a14208376
Sent from the R help mailing list archive at Nabble.com.


From zhangchicool at gmail.com  Fri Dec  7 08:59:16 2007
From: zhangchicool at gmail.com (kexinz)
Date: Thu, 6 Dec 2007 23:59:16 -0800 (PST)
Subject: [R]  how to generate uniformly distributed random integers
Message-ID: <14208376.post@talk.nabble.com>


   I'm a beginner of R. 
   I can use runif() to generate uniformly distributed numbers, but I don't
know which function can generate uniformly distributed random integers, or
what kind of method do?
   Thanks!
-- 
View this message in context: http://www.nabble.com/how-to-generate-uniformly-distributed-random-integers-tf4960778.html#a14208376
Sent from the R help mailing list archive at Nabble.com.


From jared.oconnell at gmail.com  Fri Dec  7 09:07:04 2007
From: jared.oconnell at gmail.com (Jared O'Connell)
Date: Fri, 7 Dec 2007 09:07:04 +0100
Subject: [R] how to generate uniformly distributed random integers
In-Reply-To: <14208376.post@talk.nabble.com>
References: <14208376.post@talk.nabble.com>
Message-ID: <8c464e8f0712070007t56d1d135x715f5dbfb696a3a4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/a08205bc/attachment.pl 

From r.nieuwenhuis at student.ru.nl  Fri Dec  7 09:32:57 2007
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Fri, 7 Dec 2007 09:32:57 +0100
Subject: [R] Same regression per sub-group: apply?
Message-ID: <598AEDDB-9170-497F-A0E9-D5AAAFDAC8F5@student.ru.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/5a918982/attachment.pl 

From mehault at iim.csic.es  Fri Dec  7 10:04:35 2007
From: mehault at iim.csic.es (Sonia Mehault)
Date: Fri, 7 Dec 2007 10:04:35 +0100
Subject: [R] Grouping by interval
Message-ID: <000801c838b0$301a7920$2c29a8c0@pcsonia>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/9101ed00/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Fri Dec  7 10:03:21 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 7 Dec 2007 10:03:21 +0100
Subject: [R] Same regression per sub-group: apply?
References: <598AEDDB-9170-497F-A0E9-D5AAAFDAC8F5@student.ru.nl>
Message-ID: <000601c838b0$040d28f0$0540210a@www.domain>

try something like this (untested):

dataCountry <- split(data, data$COUNTRY)
model.per.country <- lapply(dataCountry, function (x) {
    glm(dependent.var ~ FEMALE + AGE + EDUCLIN, family = binomial, 
data = x)
})


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rense Nieuwenhuis" <r.nieuwenhuis at student.ru.nl>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, December 07, 2007 9:32 AM
Subject: [R] Same regression per sub-group: apply?


> Dear helpers,
>
> I've come up with what is probably a simple problem, but I cannot
> find the solution. I have a data-set containing survey-data from
> several countries. What I want to do is to perform some regression
> analyses, for each country separately. The question is, how to do
> this nicely (thus without repeating the same syntax with another
> `subset' argument).
>
> I thought of the following:
>
> model.per.country <- tapply(data, data$COUNTRY, function(x) glm
> (dependent.var ~ FEMALE + AGE + EDUCLIN + (), family=binomial,
> data=capital))
>
> But this does not work. What goes wrong, I think, is that the
> dependent variable is clustered according to `Country', but not so
> for the predictors. The error message I received:
>
> Error in tapply(dat, dat$COUNTRY, function(x) glm(participate ~
> FEMALE +  :
> arguments must have same length
> >
>
>
> Could you please help me solving this little problem?
>
> Thanks in advance:
>
> Rense Nieuwenhuis
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From jim at bitwrit.com.au  Fri Dec  7 10:09:44 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Fri, 07 Dec 2007 20:09:44 +1100
Subject: [R] How can I plot this graph
In-Reply-To: <446399.56442.qm@web25911.mail.ukl.yahoo.com>
References: <446399.56442.qm@web25911.mail.ukl.yahoo.com>
Message-ID: <47590DD8.5080601@bitwrit.com.au>

David Rees wrote:
> ...
> What I need is the following
> 
> - One graph 
> - y1, y2, y3 plotted as seperate lines vs x
> - z1, z2 give the range of the value of z, and need to be plotted as error bars or confidence intervals vs x
> 
This doesn't look right to me, but it might give you a start:

library(plotrix)
brkdn.plot(c("y1","y2","y3"),NA,"x",xxx,
  dispbar=FALSE,col=2:4,ylim=c(-3,10))
dispbars(xxx$x,xxx$y1,xxx$z1,col=2)
dispbars(xxx$x-0.12,xxx$y2,xxx$z2,col=3)

Jim


From rfrancois at mango-solutions.com  Fri Dec  7 10:16:59 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Fri, 07 Dec 2007 09:16:59 +0000
Subject: [R] Same regression per sub-group: apply?
In-Reply-To: <000601c838b0$040d28f0$0540210a@www.domain>
References: <598AEDDB-9170-497F-A0E9-D5AAAFDAC8F5@student.ru.nl>
	<000601c838b0$040d28f0$0540210a@www.domain>
Message-ID: <47590F8B.7050602@mango-solutions.com>

What about ?by, something like this (still untested):

model.per.country <- by( data, data$COUNTRY, function (x) {
    glm(dependent.var ~ FEMALE + AGE + EDUCLIN, family = binomial, data = x)
})

Or

model.per.country <- by( data, data$COUNTRY, 
  glm , dependent.var ~ FEMALE + AGE + EDUCLIN, family = binomial )


Cheers,

Romain

Dimitris Rizopoulos wrote:
> try something like this (untested):
>
> dataCountry <- split(data, data$COUNTRY)
> model.per.country <- lapply(dataCountry, function (x) {
>     glm(dependent.var ~ FEMALE + AGE + EDUCLIN, family = binomial, 
> data = x)
> })
>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
> ----- Original Message ----- 
> From: "Rense Nieuwenhuis" <r.nieuwenhuis at student.ru.nl>
> To: <r-help at stat.math.ethz.ch>
> Sent: Friday, December 07, 2007 9:32 AM
> Subject: [R] Same regression per sub-group: apply?
>
>
>   
>> Dear helpers,
>>
>> I've come up with what is probably a simple problem, but I cannot
>> find the solution. I have a data-set containing survey-data from
>> several countries. What I want to do is to perform some regression
>> analyses, for each country separately. The question is, how to do
>> this nicely (thus without repeating the same syntax with another
>> `subset' argument).
>>
>> I thought of the following:
>>
>> model.per.country <- tapply(data, data$COUNTRY, function(x) glm
>> (dependent.var ~ FEMALE + AGE + EDUCLIN + (), family=binomial,
>> data=capital))
>>
>> But this does not work. What goes wrong, I think, is that the
>> dependent variable is clustered according to `Country', but not so
>> for the predictors. The error message I received:
>>
>> Error in tapply(dat, dat$COUNTRY, function(x) glm(participate ~
>> FEMALE +  :
>> arguments must have same length
>>     
>> Could you please help me solving this little problem?
>>
>> Thanks in advance:
>>
>> Rense Nieuwenhuis
>>
>>
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   


-- 
Mango Solutions
data analysis that delivers

Tel:  +44(0) 1249 467 467
Fax:  +44(0) 1249 467 468
Mob:  +44(0) 7813 526 123


From jarekj at amu.edu.pl  Fri Dec  7 11:03:27 2007
From: jarekj at amu.edu.pl (=?UTF-8?B?SmFyb3PFgmF3IEphc2lld2ljeg==?=)
Date: Fri, 07 Dec 2007 11:03:27 +0100
Subject: [R] Grouping by interval
In-Reply-To: <000801c838b0$301a7920$2c29a8c0@pcsonia>
References: <000801c838b0$301a7920$2c29a8c0@pcsonia>
Message-ID: <47591A6F.4080402@amu.edu.pl>

Sonia Mehault pisze:
> Hello,
>
> I have a dataframe of say 20 lines with one line per individual. I want to group these 20 individuals
> by length class (eg. of 5cm) and get the mean value of all the other variables (eg VarA and VarB) for each length class
>
> My dataframe is as follow:
>
> Length <- 10:30
> VarA <- seq(1000,1200,10)
> VarB <- seq(500,700,10)
> Data <- cbind(Length,VarA,VarB)
>
>
> And I want to get something like:
>
>
> Length Class      Mean VarA       Mean VarB
> [10-15[               1020                520
> [15-20[               1070                570
> [20-25[               1120                620
> [25-30]               1175                675
>
>
> Would you have any suggestions how to do that ?
> Many thanks.
>
>
> Sonia.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help w r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   
Sonia!


use cut fuction.

Create two wectos with intervals and lables acording documentation. Very 
simple and fast solution.
Jarek


From wwwhsd at gmail.com  Fri Dec  7 11:13:20 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 7 Dec 2007 08:13:20 -0200
Subject: [R] Grouping by interval
In-Reply-To: <000801c838b0$301a7920$2c29a8c0@pcsonia>
References: <000801c838b0$301a7920$2c29a8c0@pcsonia>
Message-ID: <da79af330712070213x39ffc4c2tb5f73e79365f3b16@mail.gmail.com>

Try this:
Vec <- cut(Data[,1], breaks=c(10,15,20,25,30), include=T, right=F)
Data <- data.frame(Data, Vec)
aggregate(Data[,2:3], list(Data$Vec), mean)



On 07/12/2007, Sonia Mehault <mehault at iim.csic.es> wrote:
> Hello,
>
> I have a dataframe of say 20 lines with one line per individual. I want to group these 20 individuals
> by length class (eg. of 5cm) and get the mean value of all the other variables (eg VarA and VarB) for each length class
>
> My dataframe is as follow:
>
> Length <- 10:30
> VarA <- seq(1000,1200,10)
> VarB <- seq(500,700,10)
> Data <- cbind(Length,VarA,VarB)
>
>
> And I want to get something like:
>
>
> Length Class      Mean VarA       Mean VarB
> [10-15[               1020                520
> [15-20[               1070                570
> [20-25[               1120                620
> [25-30]               1175                675
>
>
> Would you have any suggestions how to do that ?
> Many thanks.
>
>
> Sonia.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From Andrej.Blejec at nib.si  Fri Dec  7 11:13:19 2007
From: Andrej.Blejec at nib.si (Andrej Blejec)
Date: Fri, 7 Dec 2007 11:13:19 +0100
Subject: [R] Grouping by interval
In-Reply-To: <000801c838b0$301a7920$2c29a8c0@pcsonia>
Message-ID: <949B5DA4A23A044A98917931F2495BB98B6C63@SRVLJ01.nib.sql>

Try this:

Length <- 10:30 
VarA <- seq(1000,1200,10) 
VarB <- seq(500,700,10) 
Data <- cbind(Length,VarA,VarB)
# create a factor of desired Length classes and aggregate
classLength=cut(Length,c(10,15,20,25,30),include.lowest=TRUE) 
aggregate(cbind(VarA,VarB),by=list(LengthClass=classLength),FUN=mean)


LengthClass VarA VarB
1 [10,15] 1025 525
2 (15,20] 1080 580
3 (20,25] 1130 630
4 (25,30] 1180 680

--
Andrej Blejec
National Institute of Biology
Vecna pot 111 POB 141
SI-1000 Ljubljana
SLOVENIA

e-mail: andrej.blejec at nib.si
URL: http://ablejec.nib.si 
tel: + 386 1 423 33 88
fax: + 386 1 241 29 80
--------------------------
Organizer of
Applied Statistics 2008 conference
http://ablejec.nib.si/AS2008 

 


> -----Original Message-----
> From: r-help-bounces at r-project.org
[mailto:r-help-bounces at r-project.org]
> On Behalf Of Sonia Mehault
> Sent: Friday, December 07, 2007 10:05 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Grouping by interval
> 
> Hello,
> 
> I have a dataframe of say 20 lines with one line per individual. I
want to
> group these 20 individuals
> by length class (eg. of 5cm) and get the mean value of all the other
> variables (eg VarA and VarB) for each length class
> 
> My dataframe is as follow:
> 
> Length <- 10:30
> VarA <- seq(1000,1200,10)
> VarB <- seq(500,700,10)
> Data <- cbind(Length,VarA,VarB)
> 
> 
> And I want to get something like:
> 
> 
> Length Class      Mean VarA       Mean VarB
> [10-15[               1020                520
> [15-20[               1070                570
> [20-25[               1120                620
> [25-30]               1175                675
> 
> 
> Would you have any suggestions how to do that ?
> Many thanks.
> 
> 
> Sonia.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Fri Dec  7 11:27:57 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Fri, 7 Dec 2007 11:27:57 +0100
Subject: [R] Odp:  Grouping by interval
In-Reply-To: <000801c838b0$301a7920$2c29a8c0@pcsonia>
Message-ID: <OFC4790114.23832B4A-ONC12573AA.0037A9A3-C12573AA.003978D7@precheza.cz>

Hi

r-help-bounces at r-project.org napsal dne 07.12.2007 10:04:35:

> Hello,
> 
> I have a dataframe of say 20 lines with one line per individual. I want 
to 
> group these 20 individuals
> by length class (eg. of 5cm) and get the mean value of all the other 
variables
> (eg VarA and VarB) for each length class
> 
> My dataframe is as follow:
> 
> Length <- 10:30
> VarA <- seq(1000,1200,10)
> VarB <- seq(500,700,10)
> Data <- cbind(Length,VarA,VarB)
> 
> 
> And I want to get something like:
> 
> 
> Length Class      Mean VarA       Mean VarB
> [10-15[               1020                520
> [15-20[               1070                570
> [20-25[               1120                620
> [25-30]               1175                675
> 
> 
> Would you have any suggestions how to do that ?
> Many thanks.

Cut and aggregate

fac <- cut(Data[,1], seq(5,30,5))

> aggregate(Data[,-1],list(fac), mean)
  Group.1 VarA VarB
1  (5,10] 1000  500
2 (10,15] 1030  530
3 (15,20] 1080  580
4 (20,25] 1130  630
5 (25,30] 1180  680

is close, but you need to add first level in fac into the second.

lev<-levels(fac)
levels(fac)<-lev[c(2,2,3,4,5)]

> aggregate(Data[,-1],list(fac), mean)
  Group.1 VarA VarB
1 (10,15] 1025  525
2 (15,20] 1080  580
3 (20,25] 1130  630
4 (25,30] 1180  680

Not sure if it is the most elegant solution but it works.

Regards
Petr



> 
> 
> Sonia.
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rlevy at ucsd.edu  Fri Dec  7 11:28:44 2007
From: rlevy at ucsd.edu (Roger Levy)
Date: Fri, 07 Dec 2007 11:28:44 +0100
Subject: [R] if/else for plot/lines?
Message-ID: <4759205C.4060905@ucsd.edu>

I'm interested in writing a function that constructs a new plot on the
current graphics device if no plot exists there yet, but adds lines to
the existing plot if a plot is already there.  How can I do this?  It
seems to me that the exists() function might be co-opted to do this, but
it's not obvious how.

Many thanks,

Roger

-- 

Roger Levy                      Email: rlevy at ucsd.edu
Assistant Professor             Phone: 858-534-7219
Department of Linguistics       Fax:   858-534-4789
UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy


From wwwhsd at gmail.com  Fri Dec  7 11:33:57 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 7 Dec 2007 08:33:57 -0200
Subject: [R] if/else for plot/lines?
In-Reply-To: <4759205C.4060905@ucsd.edu>
References: <4759205C.4060905@ucsd.edu>
Message-ID: <da79af330712070233xeb105e3se76ed7596867ab02@mail.gmail.com>

Try this:

if(length(dev.list()) == 0) plot(rnorm(100), type="l") else (lines(rnorm(100)))


On 07/12/2007, Roger Levy <rlevy at ucsd.edu> wrote:
> I'm interested in writing a function that constructs a new plot on the
> current graphics device if no plot exists there yet, but adds lines to
> the existing plot if a plot is already there.  How can I do this?  It
> seems to me that the exists() function might be co-opted to do this, but
> it's not obvious how.
>
> Many thanks,
>
> Roger
>
> --
>
> Roger Levy                      Email: rlevy at ucsd.edu
> Assistant Professor             Phone: 858-534-7219
> Department of Linguistics       Fax:   858-534-4789
> UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From Ted.Harding at manchester.ac.uk  Fri Dec  7 11:41:02 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Fri, 07 Dec 2007 10:41:02 -0000 (GMT)
Subject: [R] R help mailing system configuration change?
In-Reply-To: <loom.20071207T073152-670@post.gmane.org>
Message-ID: <XFMail.071207104102.Ted.Harding@manchester.ac.uk>

I have to express sympathy (with comments, below) with both
these posters!

On 07-Dec-07 07:40:30, Dieter Menne wrote:
> ?_? ?????? <hoontaechung <at> gmail.com> writes:
>> I got a reply for my previous several postings saying that
>> I was spamming the r-help mailing list.
>> I am very sorry to all subscribers if I did that.
>> But I've been reposting my message to the mailing list several
>> times because I didn't know whether my help post was actually
>> posted or not.
>> I remember from my previous experiences that, when I post a message,
>> I can see my own posting myself.
>> But this time, I didn't see my own message so I thought my message
>> got dropped for some reasons.
>> Was there any change in r-help mailing system configuration?
>> 
>> Thanks in advance,
>> 
>> Tae-Hoon Chung

It can sometimes be the case that a message which you have
posted takes a long time to be sent to you by the R list,
while it may have been distributed to many other list
readers quite quickly. At times I have experienced delays
of up to 3 hours (though normally it is within say 15 minutes).

One way to check whether your posting has reached the list
is to check in the R-help archives at:

  https://stat.ethz.ch/pipermail/r-help

Select the most recent month and "View by Date". Your message
should appear near the end of this archive within a few minutes
of being accepted by the R-help list server.

But there is always also the possibility eperienced by Dieter:

> I have to second that, the same for me. The only way to access
> this list is via gmane, otherwise orbitl.com (a Ceylon-based
> anti-spamming list) will jump in. We have a dynamic address,
> so this list seems ban whole ranges.
> 
> I have tried to remove me from orbitl; it may have worked or not,
> because of the dynamic address I do not always track. I tried to
> contact the webmaster at ETH twice, but got no response. 
> 
> This is an annoyance; I know that I should contact orbitl every time,
> but since that organization is a mess, it better would be removed
> from the ETH anti-spamming list.
> 
> Dieter

This sort of thing can paralyse innocent users.

A couple of years ago, when I was using dial-up from home on
the BT Openworld ISP service (which allocated a dynamic IP
address on connection), I found at one point that I was
unable to send email to any UK academic institution whatever!

The error message was:
  550 host is listed in rbl-plus.mail-abuse.ja.net
The reason was that JA.NET (the UK Joint Academic Network)
had subscribed to a blacklisting service which included
every known dynamic IP address and of course the IP addresses
which BT gave me (the "host" in the above message) were
included.

For a while I worked round this because I had a log-in
account on a machine at Manchester University, so I could
dial-up, log in, and mail from there. That host was
acceptable ...

But then I dumped BT (and not just for that reason) and
switched ISP to Zen.co.uk, who gave me a permanent fixed
IP address. This evaded that particular problem.

But then I was hit by SORBS (e.g. https://www.us.sorbs.net )
which had blacklisted *every* IP address owned by Zen,
dynamic or static, apparently on the grounds that some of
these had (allegedly) been used to send spam and SORBS had
received complaints. Many institutions at the time used SORBS.

SORBS had (and still has) the interesting rule:

  "Third and finally, if you are really not a spammer,
   or you are truly reformed, de-listing is relatively
   easy, and you can choose one of two options:

    * Donate US$50 to a charity or trust approved by,
      and not connected with, SORBS for each spam
      received related to the listing. This is referred
      to as the SORBS 'fine'.
 
    * Wait for a period of 1 year for each spam received
      related to the listing (e.g. if 3 spams were received,
      wait 3 years)."

Clearly this made it out of the question for an ISP to get
themselves removed from the SORBS list, since it could amount
to many 1000s of $$! (And then there's the next time ... ).

The situation, as I understand it, was resolved when the
institutions stopped using SORBS, and I have had no such
trouble since.

I conclude from experiences like this that institutions
have a responsibility to treat bona-fide users fairly.
This means in particular avoiding "automated" blacklisting
of totally innocent people who have had the misfortune
to get on a blacklist through no fault of their own.

And this can, in turn, mean taking a close look at the
blacklisting services they consult, in order to ensure
that making use of them will not penalise people unfairly.

It's all very well for people working within institutions
(whose IP addresses will generally be "clean") to be unaware
of this kind of issue. But people connecting from outside
will be penalised unless care is taken to be fair to all.

Best wishes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Dec-07                                       Time: 10:40:59
------------------------------ XFMail ------------------------------


From ligges at statistik.uni-dortmund.de  Fri Dec  7 12:03:14 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 07 Dec 2007 12:03:14 +0100
Subject: [R] pvclust warning message
In-Reply-To: <116bbb3c0712062326h59609327n8acc3155d5dae092@mail.gmail.com>
References: <116bbb3c0712062326h59609327n8acc3155d5dae092@mail.gmail.com>
Message-ID: <47592872.301@statistik.uni-dortmund.de>



Phil taylor wrote:
> Hi all
> 
> I am trying to perform the follwing:
> 
> fit<-pvclust(wq, method.hclust="ward", method.dist="euclidean")


What is wq? Which version of R? WHich version of the pvclust package (I 
guess we are talking about that one?)? We also need a reproducible 
example, as the posting guide suggests.

Uwe Ligges


> but get a strange error message that I just cant figure out.
> 
> Has anyone come across this? Any help would be most appricieated
> 
> 
> Error in hclust(distance, method = method.hclust) :
> NA/NaN/Inf in foreign function call (arg 11)
> In addition: Warning message:
> NAs introduced by coercion in: as.double.default(x)
> 
> 
> Many thanks,
> 
> Phil
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murali.menon at uk.abnamro.com  Fri Dec  7 12:06:07 2007
From: murali.menon at uk.abnamro.com (murali.menon at uk.abnamro.com)
Date: Fri, 7 Dec 2007 11:06:07 +0000
Subject: [R] manipulation of list of matrices
Message-ID: <OF111880A1.BB6DBDD5-ONC12573AA.003AEB7E-802573AA.003D17A3@abnamro.com>

Folks,

I have a list of correlation matrices of equities, say, and I need to 
manipulate each matrix: I weight each equity by its median absolute 
correlation against all the other equities, such that the equity with the 
least such correlation gets the highest weight. Something like min(over 
all average correlations) / average correlation of equity

So I do the following:

> a <- lapply(1:1000, function(n) {b <- matrix(runif(45*45, min = -1, max 
= 1), ncol = 45); diag(b) <- 1; b})

I write the following function:

constructCorrelationWeights <- function(rollCorr, FUN = median)
{
    corWt <- sapply(rollCorr,
                    function(corMat)
                    {
                        wt <- apply(corMat, 1, function(x) FUN(abs(x)))
                        min(wt) / wt
                    })
    t(corWt)
}

> system.time(corWt <- constructCorrelationWeights(a))
[1] 33.42 18.03 79.55

Is there a more efficient and cleverer way to accomplish this? 

Many thanks,

Murali


---------------------------------------------------------------------------
This message (including any attachments) is confidential...{{dropped:3}}


From ligges at statistik.uni-dortmund.de  Fri Dec  7 12:13:00 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 07 Dec 2007 12:13:00 +0100
Subject: [R] correlated data
In-Reply-To: <000c01c83845$1bf5eda0$9d3ff880@demirtasxp157>
References: <000c01c83845$1bf5eda0$9d3ff880@demirtasxp157>
Message-ID: <47592ABC.5080206@statistik.uni-dortmund.de>



HAKAN DEMIRTAS wrote:
> Hi,
> 
> Is there an R library that has the same functionalities of Splus7.0+ library correlatedData?

If you mean an R *package* and tell us what functionalities are 
available in "Splus7.0+ library correlatedData", we might be able to help.

Uwe Ligges


> I'd appreciate any input.
> 
> Hakan Demirtas
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Fri Dec  7 12:26:58 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 7 Dec 2007 11:26:58 +0000 (GMT)
Subject: [R] if/else for plot/lines?
In-Reply-To: <4759205C.4060905@ucsd.edu>
References: <4759205C.4060905@ucsd.edu>
Message-ID: <Pine.LNX.4.64.0712071115370.20883@gannet.stats.ox.ac.uk>

On Fri, 7 Dec 2007, Roger Levy wrote:

> I'm interested in writing a function that constructs a new plot on the
> current graphics device if no plot exists there yet, but adds lines to
> the existing plot if a plot is already there.  How can I do this?  It
> seems to me that the exists() function might be co-opted to do this, but
> it's not obvious how.

exists() will not help with graphics devices, whose state is not stored in 
R objects that exists() can test for.

Note that it is rare for a graphics device to be open and not contain a 
plot.  But in those circumstances par("usr") will be c(0,1,0,1) which 
would be unusual after a plot.

A simple way to test if a non-null device is active is dev.cur() > 1.

However, I doubt if you want to add lines to any old plot that happens to 
be on the device, and there is no general way to tell if the existing 
plot is suitable (it need not be the last plot made, for example).
So I can only see this goal as achievable within a constrained set of 
circumstances.

(A further complication is that a graphics device can display either base 
or grid graphics, and you can't add base graphics to a grid plot or v.v.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Fri Dec  7 12:35:33 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 07 Dec 2007 06:35:33 -0500
Subject: [R] Junk or not Junk ???
In-Reply-To: <59d7961d0712062126g23ce2f5g556ef28ee29b3cc4@mail.gmail.com>
References: <14193897.post@talk.nabble.com>	<C37DD404.1578E%engrav@u.washington.edu>
	<59d7961d0712062126g23ce2f5g556ef28ee29b3cc4@mail.gmail.com>
Message-ID: <47593005.8010507@stats.uwo.ca>

On 07/12/2007 12:26 AM, Henrik Bengtsson wrote:
> On 06/12/2007, Loren Engrav <engrav at u.washington.edu> wrote:
>> As for news readers
>> I found R and R.mac and R.Bio on the sites you recommend, thank you very
>> much, they would avoid the individual emails, but then I would have to go
>> look at them, which might be Ok
>>
>> Deluge? Well, there are from R and Bio and R-Mac every morning 30 or 35, and
>> 10-15 more during the daytime, and ~50 deletes is painful
>>
>> But then every morning one or two are useful so...
> 
> This what you want to you email filtering for.  Create a (sub)folder
> named "r-help", setup an email filter that sends all message that has
> a subject starting with "[R] " to that folder.  That way they will not
> clutter up your inbox, but you can still browser the r-help messages.

That test will occasionally misclassify, because some private replies 
might keep the [R] in the subject.  A more reliable test, if you can 
work with the undisplayed message headers, is to look for "r-help" in 
the List-Id: header.

Duncan Murdoch
> 
> You haven't told us your email client, but pretty much any client I
> know of supports this.  I use gmail as my client and there it is very
> simple.  This way reading message is no different from reading them
> via a news reader.
> 
>> Still would be fun to understand why some R are junk and some are not
>>
>> For example
>> Today received two emails from the same person, one junk and the other not
>>
>> The Not Junk then went
>> from sweep.unicas.it
>> to phil2.ethz.ch
>> to hypatia.math.ethz.ch
>>
>> The Junk went
>> from sweep.unicas.it directly to
>> to hypatia.math.ethz.ch
>>
>> Otherwise the same, so why one junk and the other not?
> 
> FYI, I very very rarely get false positives (from the r-help lists)
> and hardly any spam for that sake (thanks!) and I've been on the list
> for a long time.  I couldn't find a single one during the last 30 days
> in my gmail spam box.
> 
> It is impossible to tell why some of your message are falsely
> classified as spam without know what your email client is.  Some
> clients have there own build in spam filtering that you can train by
> pressing "This is spam/This is not spam", whereas others rely on their
> email provider to analyze all messages and add a spam score in the
> email header and then you can set up the client to filter those out
> without much local analysis.  The latter is common at universities.
> 
> As already been suggested, it is more likely that this something that
> you email provider/sys adm should be able to help you out with.  To me
> it sounds unlikely that there is something "wrong" with the R messages
> or that R mail server is at fault.
> 
> Hope this helps
> 
> /Henrik
> 
> 
> 
>> Thank you
>>
>> Engrav
>> Univ Washington
>> Seattle
>>
>>
>>> From: David Hewitt <dhewitt at vims.edu>
>>> Date: Thu, 6 Dec 2007 07:13:12 -0800 (PST)
>>> To: <r-help at r-project.org>
>>> Subject: Re: [R] Junk or not Junk ???
>>>
>>>
>>>
>>> Loren Engrav wrote:
>>>> Thank you
>>>>
>>>> As per advice from several R users I have set
>>>>
>>>> r-project.org, stat.math.ethz.ch,fhcrc.org, stat.ethz.ch, math.ethz.ch,
>>>> hypatia.math.ethz.ch
>>>>
>>>>  all to be "safe domains"
>>>>
>>>> But still some R emails go to Junk and require to be found manually
>>>>
>>>> I have explored the issue with Univ Wash computing to no avail
>>>>
>>>> Is this just how it is or have I still missed the "fix" to keep R emails
>>>> out
>>>> of junk?
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>> I suspect most people that stay on this list read most posts through a news
>>> reader... you might consider doing the same. I can't even imagine keeping up
>>> with the daily deluge of individual emails.
>>>
>>> http://www.nabble.com/R-help-f13820.html
>>>
>>> http://news.gmane.org/gmane.comp.lang.r.general
>>>
>>>
>>> -----
>>> David Hewitt
>>> Virginia Institute of Marine Science
>>> http://www.vims.edu/fish/students/dhewitt/
>>> --
>>> View this message in context:
>>> http://www.nabble.com/Junk-or-not-Junk-tf4940701.html#a14193897
>>> Sent from the R help mailing list archive at Nabble.com.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Fri Dec  7 13:41:32 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Dec 2007 07:41:32 -0500
Subject: [R] Same regression per sub-group: apply?
In-Reply-To: <598AEDDB-9170-497F-A0E9-D5AAAFDAC8F5@student.ru.nl>
References: <598AEDDB-9170-497F-A0E9-D5AAAFDAC8F5@student.ru.nl>
Message-ID: <971536df0712070441i78dba3ccv6f32f95532e53b40@mail.gmail.com>

See:

https://stat.ethz.ch/pipermail/r-help/2007-May/132866.html

On Dec 7, 2007 3:32 AM, Rense Nieuwenhuis <r.nieuwenhuis at student.ru.nl> wrote:
> Dear helpers,
>
> I've come up with what is probably a simple problem, but I cannot
> find the solution. I have a data-set containing survey-data from
> several countries. What I want to do is to perform some regression
> analyses, for each country separately. The question is, how to do
> this nicely (thus without repeating the same syntax with another
> `subset' argument).
>
> I thought of the following:
>
> model.per.country <- tapply(data, data$COUNTRY, function(x) glm
> (dependent.var ~ FEMALE + AGE + EDUCLIN + (), family=binomial,
> data=capital))
>
> But this does not work. What goes wrong, I think, is that the
> dependent variable is clustered according to `Country', but not so
> for the predictors. The error message I received:
>
> Error in tapply(dat, dat$COUNTRY, function(x) glm(participate ~
> FEMALE +  :
>        arguments must have same length
>  >
>
>
> Could you please help me solving this little problem?
>
> Thanks in advance:
>
> Rense Nieuwenhuis
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From funnymoody999 at yahoo.com  Thu Dec  6 23:19:55 2007
From: funnymoody999 at yahoo.com (mogra)
Date: Thu, 6 Dec 2007 14:19:55 -0800 (PST)
Subject: [R] Re ad File : Header with special character
Message-ID: <14202399.post@talk.nabble.com>


Hi,

I have a data file which has column names : 

a 
a,b 
a->b

R converst a,b to  a.b
a->b to a..b

Is there any way to handle this ? 

Thanks a lot.


-- 
View this message in context: http://www.nabble.com/Read-File-%3A-Header-with-special-character-tf4958972.html#a14202399
Sent from the R help mailing list archive at Nabble.com.


From xwan at cs.dal.ca  Fri Dec  7 08:15:05 2007
From: xwan at cs.dal.ca (xwan at cs.dal.ca)
Date: Fri, 7 Dec 2007 03:15:05 -0400 (AST)
Subject: [R] plot dendrogram generated by hc (mclust package)
In-Reply-To: <mailman.18512.1197010859.4289.r-help@r-project.org>
References: <mailman.18512.1197010859.4289.r-help@r-project.org>
Message-ID: <4925.68.145.126.117.1197011705.squirrel@68.145.126.117>

Anyone knows how to plot the dendrogram generated by the hierarchical
clustering function hc from mclust package?

Xiaomeng Wan
Dalhousie University


From wvancamp at ebay.com  Tue Dec  4 22:59:36 2007
From: wvancamp at ebay.com (Warren Van Camp)
Date: Tue, 4 Dec 2007 13:59:36 -0800 (PST)
Subject: [R] R and TeraData
In-Reply-To: <47166114.5090208@kutsyy.com>
References: <47166114.5090208@kutsyy.com>
Message-ID: <14160501.post@talk.nabble.com>



Vadim Kutsyy wrote:
> 
> 
> Does anyone know a way to connect from R on Linux box to TeraData 
> server?  I can use ODBC connection on Windows box, but with amount of 
> data I need (and prefer) to use large Linux box.
> 
> Thanks,
> Vadim
> 
> 

You indicate that the ODBC connection to Teradata on Windows is working for
you.  This may be a newbie question, but I'm not getting beyond an
apparently successful connection to the Teradata...

> odbcGetInfo(odw)
             DBMS_Name               DBMS_Ver        Driver_ODBC_Ver 
            "Teradata"          "06.02.0205  V2R"             "03.52" 
          Data_Source_Name        Driver_Name             Driver_Ver 
             "ODW"                "TDATA32.DLL"          " 3.05.00.04" 
           ODBC_Ver                   Server_Name 
          "03.52.0000"               "********" 
> getSqlTypeInfo()
                               double integer    character      logical
MySQL                          double integer varchar(255)   varchar(5)
ACCESS                         DOUBLE INTEGER VARCHAR(255)   varchar(5)
Microsoft.SQL.Server            float     int varchar(255)   varchar(5)
PostgreSQL                     float8    int4 varchar(255)   varchar(5)
Oracle               double precision integer varchar(255) varchar(255)
SQLite                         double integer varchar(255)   varchar(5)
EXCEL                          NUMBER  NUMBER VARCHAR(255)      LOGICAL
DBASE                         Numeric Numeric    Char(254)      Logical

> getSqlTypeInfo("TDATA32.DLL")
NULL

> odw_driver <- odbcGetInfo(odw)
> getSqlTypeInfo(odw_driver)
Error in typesR2DBMS[[driver]] : no such index at level 1

> sqlTables(odw)
Error in iconv(data[[i]], from = enc) : invalid 'from' argument
In addition: Warning message:
closing unused RODBC handle 1 

> sqlQuery(odw,"select top 10 * from odw_prs_v.pool")
Error in iconv(query, to = enc) : invalid 'to' argument


Apparently there are some data conversion errors... do data types need to be
established first?  Any other ideas what's missing?

Thanks,
Warren.
-- 
View this message in context: http://www.nabble.com/R-and-TeraData-tf4642728.html#a14160501
Sent from the R help mailing list archive at Nabble.com.


From t.zumbrunn at unibas.ch  Fri Dec  7 14:36:55 2007
From: t.zumbrunn at unibas.ch (Thomas Zumbrunn)
Date: Fri, 7 Dec 2007 14:36:55 +0100
Subject: [R] x11() and Xinerama settings
Message-ID: <200712071436.56046.t.zumbrunn@unibas.ch>

Hello

Since I changed my X11 settings to a Xinerama setup, calls to x11() result in 
windows with a width half the size of the one specified by the corresponding 
parameter. Furthermore, x axis labels are overlapping. Other devices (e.g. 
pdf()), are not affected by these distortions.

Did anybody have the same problem, and if so, how did you solve it? ( I'm 
using openSUSE 10.3 on a x86 architecture.)

Thomas Zumbrunn


-- 
Thomas Zumbrunn
Institute of Zoology, University of Basel
Vesalgasse 1, CH-4051 Basel, Switzerland
Tel +41 (0)61 267 03 71
GPG: 1024D/8E0AD1CE


From murisoares at yahoo.com  Fri Dec  7 14:41:47 2007
From: murisoares at yahoo.com (Muri Soares)
Date: Fri, 7 Dec 2007 05:41:47 -0800 (PST)
Subject: [R] Adding a subset to a glm messes up factors?
Message-ID: <699802.80226.qm@web50912.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/6e6afe22/attachment.pl 

From bcarvalh at jhsph.edu  Fri Dec  7 14:43:22 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Fri, 7 Dec 2007 08:43:22 -0500
Subject: [R] Grouping by interval
In-Reply-To: <000801c838b0$301a7920$2c29a8c0@pcsonia>
References: <000801c838b0$301a7920$2c29a8c0@pcsonia>
Message-ID: <FA98B443-ACD8-4026-A005-D2F05295E23A@jhsph.edu>

Check the cut() function.
b

On Dec 7, 2007, at 4:04 AM, "Sonia Mehault" <mehault at iim.csic.es> wrote:

> Hello,
>
> I have a dataframe of say 20 lines with one line per individual. I  
> want to group these 20 individuals
> by length class (eg. of 5cm) and get the mean value of all the other  
> variables (eg VarA and VarB) for each length class
>
> My dataframe is as follow:
>
> Length <- 10:30
> VarA <- seq(1000,1200,10)
> VarB <- seq(500,700,10)
> Data <- cbind(Length,VarA,VarB)
>
>
> And I want to get something like:
>
>
> Length Class      Mean VarA       Mean VarB
> [10-15[               1020                520
> [15-20[               1070                570
> [20-25[               1120                620
> [25-30]               1175                675
>
>
> Would you have any suggestions how to do that ?
> Many thanks.
>
>
> Sonia.
>
>    [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Fri Dec  7 15:03:28 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 7 Dec 2007 14:03:28 +0000 (GMT)
Subject: [R] Adding a subset to a glm messes up factors?
In-Reply-To: <699802.80226.qm@web50912.mail.re2.yahoo.com>
References: <699802.80226.qm@web50912.mail.re2.yahoo.com>
Message-ID: <Pine.LNX.4.64.0712071350220.9652@gannet.stats.ox.ac.uk>

First, 'subset' is an argument to glm(), but for some reason you did not 
use it.  Your subject line is quite misleading, and had it been the more 
accurate

 	Adding a 'data' argument to glm messes up factors?

you might have realised the problem.

Second, your models are fitted to different datasets: the first to objects 
in your workspace, and the second to columns of data.all. Since you have 
not (as we asked) given a reproducible example we cannot know what those 
differences are, but differences in the datasets will be the key.

Third, the best way to fit linear models is lm(), not 
glm(family=gaussian).


On Fri, 7 Dec 2007, Muri Soares wrote:

> I have a problem with running a glm using a subset of my data. Whenever 
> I choose a subset, in the summary the factors arent shown (as if the 
> variable was a continuous variable). If I dont use subsets then all the 
> factors are shown. I have copied the output from summary for both cases.
>
> Thanks for the help,
> Muri
>
>> model<-glm(log(cpue)~year,family=gaussian)
> Call:
> glm(formula = log(cpue) ~ year, family = gaussian)
>
> Deviance Residuals:
>    Min       1Q   Median       3Q      Max
> -2.0962  -0.5851  -0.1241   0.4805   3.9236
>
> Coefficients:
>            Estimate Std. Error t value Pr(>|t|)
> (Intercept)   0.8899     0.1844   4.825 1.42e-06 ***
> year1990     -0.6107     0.1925  -3.173  0.00152 **
> year1991     -1.7466     0.1902  -9.184  < 2e-16 ***
> year1992     -1.4061     0.1864  -7.544 5.07e-14 ***
> year1993     -1.4069     0.1860  -7.565 4.31e-14 ***
> ...
>
>> model<-glm(log(cpue)~year,family=gaussian,subset(data.all,species=="n")
> Call:
> glm(formula = log(cpue) ~ year, family = gaussian, data = subset(data.all,
>    species == "n"))
>
> Deviance Residuals:
>     Min        1Q    Median        3Q       Max
> -1.64577  -0.61671  -0.08972   0.55792   2.73737
>
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) 32.446570  10.076895   3.220  0.00135 **
> year        -0.016345   0.005037  -3.245  0.00123 **
> ---

> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


PLEASE do!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From P.Dalgaard at biostat.ku.dk  Fri Dec  7 15:14:52 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 07 Dec 2007 15:14:52 +0100
Subject: [R] paradox about the degree of freedom in a logistic
 regression model
In-Reply-To: <14208306.post@talk.nabble.com>
References: <14208306.post@talk.nabble.com>
Message-ID: <4759555C.6010205@biostat.ku.dk>

Bin Yue wrote:
>  Dear all:
>    "predict.glm" provides an example to perform logistic regression when the
> response variable is a tow-columned  matrix. I find some paradox about the
> degree of freedom  .
>  > summary(budworm.lg)
>
> Call:
> glm(formula = SF ~ sex * ldose, family = binomial)
>
> Deviance Residuals: 
>      Min        1Q    Median        3Q       Max  
> -1.39849  -0.32094  -0.07592   0.38220   1.10375  
>
> Coefficients:
>             Estimate Std. Error z value Pr(>|z|)    
> (Intercept)  -2.9935     0.5527  -5.416 6.09e-08 ***
> sexM          0.1750     0.7783   0.225    0.822    
> ldose         0.9060     0.1671   5.422 5.89e-08 ***
> sexM:ldose    0.3529     0.2700   1.307    0.191    
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
>
> (Dispersion parameter for binomial family taken to be 1)
>
>     Null deviance: 124.8756  on 11  degrees of freedom
> Residual deviance:   4.9937  on  8  degrees of freedom
> AIC: 43.104
>
> Number of Fisher Scoring iterations: 4
>
> This is the data set used in regression:
>   numdead numalive sex ldose
> 1        1       19   M     0
> 2        4       16   M     1
> 3        9       11   M     2
> 4       13        7   M     3
> 5       18        2   M     4
> 6       20        0   M     5
> 7        0       20   F     0
> 8        2       18   F     1
> 9        6       14   F     2
> 10      10       10   F     3
> 11      12        8   F     4
> 12      16        4   F     5
>
>      The degree of freedom is 8. Each row in the example is thought to be
> one observation. If  I extend it to be a three column data.frame, the first
> denoting the whether the individual is alive , the secode denoting the sex,
> and the third "ldose",there will be 12*20=240 observations. 
>      Since my data set is one of the second type , I wish to know whether
> the form of data set affects the result of regression ,such as the degree of
> freedom.
>    Dose anybody have any idea about this? Thank all who read this message.
>    Regards,
>    Bin Yue
>
>   
Yes. Never use the deviance in binary logistic regression. Only use
differences in deviance between models, each of which satisfy
requirements for asymptotic theory (in your case, you could compare your
model with that described by sex*factor(ldose)). Another striking
example is this

y <- rbinom(1000, prob=.5, size=1)
summary(glm(y~-1,binomial))

now try it with different data

y <- rbinom(1000, prob=.01, size=1)
summary(glm(y~-1,binomial))

and think about it. Then consider the same thing with y~1.

As Brian keeps telling me, there IS a sense in which the residual
deviances make sense in such cases, but it is not as a means of testing
the model adequacy.

> -----
> Best regards,
> Bin Yue
>
> *************
> student for a Master program in South Botanical Garden , CAS
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Fri Dec  7 15:24:18 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 7 Dec 2007 14:24:18 +0000 (GMT)
Subject: [R] R and TeraData
In-Reply-To: <14160501.post@talk.nabble.com>
References: <47166114.5090208@kutsyy.com> <14160501.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0712071405221.9652@gannet.stats.ox.ac.uk>

I think that your first step should be to read the documentation of 
whatever software you are using here (you didn't say).

You are getting iconv errors because you set an invalid encoding.  You 
haven't shown us all the steps you used, and in one of those not shown you 
will find your error.  If you don't understand what they are needed for, 
at least at first accept the default arguments for things like encodings.

You will find people more willing to help you if you give them credit for 
their work: you are using a contributed package here without giving *any* 
credit, not even its name.


On Tue, 4 Dec 2007, Warren Van Camp wrote:

>
>
> Vadim Kutsyy wrote:
>>
>>
>> Does anyone know a way to connect from R on Linux box to TeraData
>> server?  I can use ODBC connection on Windows box, but with amount of
>> data I need (and prefer) to use large Linux box.
>>
>> Thanks,
>> Vadim
>>
>>
>
> You indicate that the ODBC connection to Teradata on Windows is working for
> you.  This may be a newbie question, but I'm not getting beyond an
> apparently successful connection to the Teradata...
>
>> odbcGetInfo(odw)
>             DBMS_Name               DBMS_Ver        Driver_ODBC_Ver
>            "Teradata"          "06.02.0205  V2R"             "03.52"
>          Data_Source_Name        Driver_Name             Driver_Ver
>             "ODW"                "TDATA32.DLL"          " 3.05.00.04"
>           ODBC_Ver                   Server_Name
>          "03.52.0000"               "********"
>> getSqlTypeInfo()
>                               double integer    character      logical
> MySQL                          double integer varchar(255)   varchar(5)
> ACCESS                         DOUBLE INTEGER VARCHAR(255)   varchar(5)
> Microsoft.SQL.Server            float     int varchar(255)   varchar(5)
> PostgreSQL                     float8    int4 varchar(255)   varchar(5)
> Oracle               double precision integer varchar(255) varchar(255)
> SQLite                         double integer varchar(255)   varchar(5)
> EXCEL                          NUMBER  NUMBER VARCHAR(255)      LOGICAL
> DBASE                         Numeric Numeric    Char(254)      Logical
>
>> getSqlTypeInfo("TDATA32.DLL")
> NULL
>
>> odw_driver <- odbcGetInfo(odw)
>> getSqlTypeInfo(odw_driver)
> Error in typesR2DBMS[[driver]] : no such index at level 1
>
>> sqlTables(odw)
> Error in iconv(data[[i]], from = enc) : invalid 'from' argument
> In addition: Warning message:
> closing unused RODBC handle 1
>
>> sqlQuery(odw,"select top 10 * from odw_prs_v.pool")
> Error in iconv(query, to = enc) : invalid 'to' argument
>
>
> Apparently there are some data conversion errors... do data types need to be
> established first?  Any other ideas what's missing?
>
> Thanks,
> Warren.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jrkrideau at yahoo.ca  Fri Dec  7 15:49:17 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 7 Dec 2007 09:49:17 -0500 (EST)
Subject: [R] merge in function
In-Reply-To: <802470.99065.qm@web57311.mail.re1.yahoo.com>
Message-ID: <564409.71831.qm@web32805.mail.mud.yahoo.com>

I think we need some actual sample code .  Also by= 0
seems a bit funny.
--- qian z <qianpland at yahoo.com> wrote:

> I used merge() in a function, but it doesn't return
> correct data frame. 
>    
>   add.name <- function(data, x)
>    
>   {
>    
>   ...
>   ...
>    
>   newfile <- merge(data, resid, by =0, all.x=TRUE,
> all.y= FALSE)
>   newfile
>    
>    
>   }
>


From robk at statmethods.net  Fri Dec  7 15:58:32 2007
From: robk at statmethods.net (Rob Kabacoff)
Date: Fri, 7 Dec 2007 09:58:32 -0500
Subject: [R] Studentized maximum modulus distribution
Message-ID: <0238e763db7f465086feb792286a0b8a@mail.infosaic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/b024b946/attachment.pl 

From otter at otter-rsch.com  Sat Dec  8 01:15:23 2007
From: otter at otter-rsch.com (dave fournier)
Date: Fri, 07 Dec 2007 16:15:23 -0800
Subject: [R]  coxme frailty model standard errors?
Message-ID: <4759E21B.2080901@otter-rsch.com>

While it may be true that for coxme models the "standard errors"
are not very good approximations, it is always useful to have them
to compare with other diagnostics such as likelihood ratios and
profile likelihoods.  It is interesting to hear that with the currently
used methodology

   "Computation of the se of the random effect turns out to be very hard"

because if you simply use AD Model Builders Random Effects module to
formulate the model you will get the standard errors calculated for free
without any more effort.

    Cheers,

      Dave

-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com


From thomas.pujol at yahoo.com  Fri Dec  7 16:12:53 2007
From: thomas.pujol at yahoo.com (Thomas Pujol)
Date: Fri, 7 Dec 2007 07:12:53 -0800 (PST)
Subject: [R] using "eval(parse(text)) " , gsub(pattern, replacement, x) ,
	to process "code" within a loop/custom function
Message-ID: <802678.95169.qm@web59315.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/fda8cc0a/attachment.pl 

From iacopetti at fastpiu.it  Fri Dec  7 16:37:36 2007
From: iacopetti at fastpiu.it (Roberto Iacopetti)
Date: Fri, 7 Dec 2007 07:37:36 -0800 (PST)
Subject: [R]  SQLiteDF SQLITE ERROR after attach
Message-ID: <14214520.post@talk.nabble.com>


Dear list,

i have installed SQLiteDF (and SQlite 3.5.3 too)

After the suggested dataframe generation  
   
    iris.sdf <- sqlite.data.frame(iris)

i have close R and than restart it for verify the correct load of dataframe
:

the file data1 is saved in the usual folder .SQLiteDF

after the command :
attachSdf("c:/R/Report/.SQLiteDF/data1.db","iris.sdf")
R give this error
SQLITE ERROR (line 295 at sqlite_workspace.c): column internal_name is not
unique
SQLITE ERROR (line 463 at sqlite_workspace.c): near "data1": syntax error

if i try a new solution (after delete all the .SQLite folder)
iris.sdf <- sqlite.data.frame(iris)
iris.sdf1<-inameSdf(iris.sdf) 

      attachSdf(".SQLiteDF/iris.db","iris.sdf")

OR  attachSdf(".SQLiteDF/iris.db","iris.sdf1")

the message is the same
Errore in attachSdf(".SQLiteDF/iris.db", "iris.sdf") : 
        .SQLiteDF/iris.db is not a valid SDF.


perhaps anyone know thi problem

thanks in advance

Roberto Iacopetti










-- 
View this message in context: http://www.nabble.com/SQLiteDF-SQLITE-ERROR-after-attach-tf4962680.html#a14214520
Sent from the R help mailing list archive at Nabble.com.


From jholtman at gmail.com  Fri Dec  7 16:45:15 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 7 Dec 2007 07:45:15 -0800
Subject: [R] Re ad File : Header with special character
In-Reply-To: <14202399.post@talk.nabble.com>
References: <14202399.post@talk.nabble.com>
Message-ID: <644e1f320712070745n61b2e611v5bc3eee4d5f54294@mail.gmail.com>

?make.names

Check out what R does with names that are not syntaxically correct.

On Dec 6, 2007 2:19 PM, mogra <funnymoody999 at yahoo.com> wrote:
>
> Hi,
>
> I have a data file which has column names :
>
> a
> a,b
> a->b
>
> R converst a,b to  a.b
> a->b to a..b
>
> Is there any way to handle this ?
>
> Thanks a lot.
>
>
> --
> View this message in context: http://www.nabble.com/Read-File-%3A-Header-with-special-character-tf4958972.html#a14202399
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From h.wickham at gmail.com  Fri Dec  7 17:15:03 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 7 Dec 2007 10:15:03 -0600
Subject: [R] if/else for plot/lines?
In-Reply-To: <4759205C.4060905@ucsd.edu>
References: <4759205C.4060905@ucsd.edu>
Message-ID: <f8e6ff050712070815n3f1c35car3688ff9ee7393652@mail.gmail.com>

On Dec 7, 2007 4:28 AM, Roger Levy <rlevy at ucsd.edu> wrote:
> I'm interested in writing a function that constructs a new plot on the
> current graphics device if no plot exists there yet, but adds lines to
> the existing plot if a plot is already there.  How can I do this?  It
> seems to me that the exists() function might be co-opted to do this, but
> it's not obvious how.

You might want to look at the ggplot2 package,
http://had.co.nz/ggplot2/, which exposes plots as objects that can be
easily manipulated.

Hadley

-- 
http://had.co.nz/


From alison.waller at utoronto.ca  Fri Dec  7 17:17:08 2007
From: alison.waller at utoronto.ca (alison waller)
Date: Fri, 7 Dec 2007 11:17:08 -0500
Subject: [R] finding most highly transcribed genes - ranking,
	sorting and subsets?
In-Reply-To: <6phve7b8rfj.fsf@gopher4.fhcrc.org>
Message-ID: <003c01c838ec$9d797840$7400a8c0@AWALL>

Thanks so much Martin,

This method is definitely more straightforward.  And you are right I don't
think I was doing anything wrong before. However, I thought that rank, would
rank the highest 1st, however after looking at the results using your
methods, I realized it ranks the lowest number 1.  So I modified it for
rank>18500.  And now I'm getting 300 rows for which the intensity is
consistenly high.

However, I am still laking some information.  For the results I can get a
matrix of 300 rows and the corresponding intensities (from m) or rank (from
h), but what I really want is the name of the original row, which
corresponds to a specific spot on the array).

I did msubset<-m[hrows,] and as mentioned I just get the rows numbered
1-300, while I want to essentially pickout the 300 rows from the original
19,000 rows maintaing the original row designation as it corresponds to a
specific gene.

Thanks again for any suggestions,

Alison

-----Original Message-----
From: Martin Morgan [mailto:mtmorgan at fhcrc.org] 
Sent: Thursday, December 06, 2007 4:06 PM
To: alison waller
Subject: Re: [R] finding most highly transcribed genes - ranking, sorting
and subsets?

Hi Alison --

I'm not sure where your problem is coming from, but R can help you to
more efficiently do your task. Skipping the bioc terminology and data
structures, you have a matrix

> m <- matrix(runif(100000), ncol=10)

you'd like to determine the rank of values in each column

> r <- apply(m, 2, rank)

identfiy those with high rank

> h <- r < 500

and find the rows for which the rank is always high

> hrows <- apply(h, 1, all)

you can then use hrows to subset your original matrix (m[hrows,]) or
otherwise, e.g., how many rows with high rank

> sum(hrows)
[1] 0

or perhaps the distribution of the number of columns in which high
ranking genes occur.

> table(apply(h, 1, sum))

   0    1    2    3    4 
5996 3132  765  100    7 

Martin

"alison waller" <alison.waller at utoronto.ca> writes:

> Hello,
>
>  
>
> I am not only interested in finding out which genes are the most highly
up-
> or down-regulated (which I have done using the linear models and Bayesian
> statistics in Limma), but I also want to know which genes are consistently
> highly transcribed (ie. they have a high intensity in the channel of
> interest eg. Cy5 or Cy3 across the set of experiments).  I might have
missed
> a straight forward way to do this, or a valuable function, but I've been
> using my own methods and going around in circles.
>
>  
>
> So far I've normalized within and between arrays, then returned the RG
> values using RG<-RG.MA, then I ranked each R and G values for each array
as
> below.
>
> rankRG<-RG
>
> rankRG$R[,1]<-rank(rankRG$R[,1])
>
> rankRG$R[,2]<-rank(rankRG$R[,2]) .. and so on for 6 columns(ie. arrays, as
> well as the G's)
>
>  
>
> then I thought I could pull out a subset of rankRG using something like;
>
> topRG<-rankRG
>
> topRG$R<-subset(topRG$R,topRG$R[,1]<500&topRG$R[,2]<500&topRG$R[,5]<500)
>
>  
>
> However, this just returned me a matrix with one row of $R (the ranks were
> <500 for columns 1,2, and 5 and greater than 500 for 3,4,and 6).  However,
I
> can't believe that there is only one gene that is in the top 500 for R
> intensitiy among those three arrays.
>
>  
>
> Am I doing something wrong?  Can someone think of a better way of doing
> this?
>
>  
>
> Thanks
>
>  
>
> Alison
>
>  
>
>  
>
> ******************************************
> Alison S. Waller  M.A.Sc.
> Doctoral Candidate
> awaller at chem-eng.utoronto.ca
> 416-978-4222 (lab)
> Department of Chemical Engineering
> Wallberg Building
> 200 College st.
> Toronto, ON
> M5S 3E5
>
>   
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr. Martin Morgan, PhD
Computational Biology Shared Resource Director
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M2 B169
Phone: (206) 667-2793


From yangfan1 at msu.edu  Fri Dec  7 17:22:59 2007
From: yangfan1 at msu.edu (Fan Yang)
Date: Fri, 7 Dec 2007 11:22:59 -0500
Subject: [R] Data import error: duplicate "row.names"
Message-ID: <34B0E2B3-7DEB-41E7-9CD8-8EE9146102C8@msu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/37a9087c/attachment.pl 

From dwinsemius at comcast.net  Fri Dec  7 17:26:02 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 7 Dec 2007 16:26:02 +0000 (UTC)
Subject: [R] relationship between two factors
References: <4757F03B.60701@coralms.com>
	<da79af330712061144m738a9bd6ob8ecb2e89e8ff2c3@mail.gmail.com>
Message-ID: <Xns99FF74566E7C9dNOTwinscomcast@80.91.229.13>

"Henrique Dallazuanna" <wwwhsd at gmail.com> wrote in 
news:da79af330712061144m738a9bd6ob8ecb2e89e8ff2c3 at mail.gmail.com:

> Try this:
> 
> df <- data.frame(DMS=factor(rep(LETTERS[1:4], 10)),
> ITS=factor(rep(LETTERS[1:4], 10)))
> table(df)
> plot(table(df))
> 

That gave me simply 10's on the diagonals of the table and the plot was 
not very satisfying. I suggest this alternate example:

tbl<-r2dtable(1,c=4*c(15,12,8,5),r=4*c(20,10,5,5))
tbl
#[[1]]
#     [,1] [,2] [,3] [,4]
#[1,]   27   23   21    9
#[2,]   18   11    5    6
#[3,]    8    6    2    4
#[4,]    7    8    4    1

# > class(tbl)
# [1] "list"
ITS=factor(rep(LETTERS[1:4]))
DMS=factor(rep(LETTERS[1:4]))
df<-data.frame(counts=unlist(tbl),expand.grid(DMS,ITS))
names(df)<-c("counts","DMS",   "ITS")
df
(df.tbl<-xtabs(counts~DMS+ITS,df))
#       ITS
#DMS  A  B  C  D
#      A 27 23 21  9
#      B 18 11  5  6
#      C  8  6  2  4
#      D  7  8  4  1
plot(df.tbl)

-- 
David Winsemius


From jluo.rhelp at gmail.com  Fri Dec  7 17:36:14 2007
From: jluo.rhelp at gmail.com (Jack Luo)
Date: Fri, 7 Dec 2007 11:36:14 -0500
Subject: [R] low level plotting question on R
Message-ID: <124ea520712070836td8189f1vd47a733dae2daa24@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/b339337a/attachment.pl 

From loic.joffre at gmail.com  Fri Dec  7 17:47:04 2007
From: loic.joffre at gmail.com (tintin_et_milou)
Date: Fri, 7 Dec 2007 08:47:04 -0800 (PST)
Subject: [R] Data import error: duplicate "row.names"
In-Reply-To: <34B0E2B3-7DEB-41E7-9CD8-8EE9146102C8@msu.edu>
References: <34B0E2B3-7DEB-41E7-9CD8-8EE9146102C8@msu.edu>
Message-ID: <14215832.post@talk.nabble.com>


I had the same problem one time. It was because I did a copy paste under
excel that is to say i put some data from an other file at the following of
the file, and R thought it was duplicate row.names. So i did two exports on
R and after i did a rbind... I don't know if you did a copy paste like i did
but the problem can be that. I did not find an other solution and i think
other easier way are possible.

Lo?c


Fan Yang-4 wrote:
> 
> Hi,
> 
> I am trying to import a tab delimited file (converted from .xls file) by
>  >Test<-read.table("/Users/....txt", header=T, row.names=1)
> 
> The command has always worked for me, but now I have been getting the  
> error message saying that "duplicate 'row.names' are not allowed.  I  
> have checked my original files and all names were unique.  I have  
> also tried to change the sample names (as 1, 2, 3,..., 13), it was  
> still returning the same error message "duplicate 'row.names' are not  
> allowed" to me.  I have tried using "row.names=NULL", but it returned  
> the column names as well.
> 
> Does anyone know what the problem was?  Thanks
> 
> Fan
> 
> Fan Yang
> Industrial Microbiology
> 2209 Biomedical Phys Sci
> Michigan State University
> Phone: (517)355-6463 Ext 1588
> Email: yangfan1 at msu.edu
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Data-import-error%3A-duplicate-%22row.names%22-tf4963044.html#a14215832
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Fri Dec  7 17:53:10 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Dec 2007 11:53:10 -0500
Subject: [R] Data import error: duplicate "row.names"
In-Reply-To: <34B0E2B3-7DEB-41E7-9CD8-8EE9146102C8@msu.edu>
References: <34B0E2B3-7DEB-41E7-9CD8-8EE9146102C8@msu.edu>
Message-ID: <971536df0712070853p415c82aay2beb2251a76d57bb@mail.gmail.com>

Read your file in without row names and check for duplicates:

DF <- read.table(myfile, skip = 1, header = FALSE)
myfile[duplicated(DF[[1]]), ]  # list rows with duplicated column 1




On Dec 7, 2007 11:22 AM, Fan Yang <yangfan1 at msu.edu> wrote:
> Hi,
>
> I am trying to import a tab delimited file (converted from .xls file) by
>  >Test<-read.table("/Users/....txt", header=T, row.names=1)
>
> The command has always worked for me, but now I have been getting the
> error message saying that "duplicate 'row.names' are not allowed.  I
> have checked my original files and all names were unique.  I have
> also tried to change the sample names (as 1, 2, 3,..., 13), it was
> still returning the same error message "duplicate 'row.names' are not
> allowed" to me.  I have tried using "row.names=NULL", but it returned
> the column names as well.
>
> Does anyone know what the problem was?  Thanks
>
> Fan
>
> Fan Yang
> Industrial Microbiology
> 2209 Biomedical Phys Sci
> Michigan State University
> Phone: (517)355-6463 Ext 1588
> Email: yangfan1 at msu.edu
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From demirtas at uic.edu  Fri Dec  7 18:02:15 2007
From: demirtas at uic.edu (HAKAN DEMIRTAS)
Date: Fri, 7 Dec 2007 11:02:15 -0600
Subject: [R] correlated data
References: <000c01c83845$1bf5eda0$9d3ff880@demirtasxp157>
	<47592ABC.5080206@statistik.uni-dortmund.de>
Message-ID: <000901c838f2$ecb33df0$9d3ff880@demirtasxp157>



Hi,

Thanks for your reply. I am looking for an R package that has the same 
functionality of the glme() function within the correlatedData library in 
SPlus 6.2 or above. This generic function fits a generalized linear 
mixed-effects model in the formulation described in Breslow and Clayton 
(1993) but allowing for nested random effects. The within-group errors are 
allowed to be correlated and/or have additional heteroscedastic patterns. 
Regards, Hakan Demirtas


From roland.rproject at gmail.com  Fri Dec  7 18:33:43 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Fri, 07 Dec 2007 12:33:43 -0500
Subject: [R] low level plotting question on R
In-Reply-To: <124ea520712070836td8189f1vd47a733dae2daa24@mail.gmail.com>
References: <124ea520712070836td8189f1vd47a733dae2daa24@mail.gmail.com>
Message-ID: <475983F7.9080108@gmail.com>

Hi Jack,

Jack Luo wrote:
> Dear List,
> 
> I am trying to modify the xlab and ylab for a current figure that was
> plotted by a package, I searched through the low level plotting command and
> they do not seem to contain how to do this (the only way is to use xlab,
> ylab as arguments in "plot" command, which I can not do since the plot is
> plotted using some other package, not by my own script). Is there any
> command for doing this? In addition, the package is from CRAN (named Pamr),
> is there any way that I can modify the function used in the package?

I think it would be useful if you are a bit more specific by telling us 
which function you were using.
I assume now that you used
library(pamr)
pamr.geneplot(...)

Is this correct?
If you check the package description, you will see that you are allowed 
to make changes to the software (GPL 2.0).
Now, have a look at
pamr.geneplot

Pretty much in the end, you will find the plotting command. What I did 
now was to basically slightly modify the function by adding two 
arguments for the labels of the x-axis and the y-axis to the function 
definition. As you will see with the provided example, you can make your 
own labels for the x-axis and y-axis now.

I hope this helps,
Roland


pamr.geneplot.modif <- function(fit, data, threshold, xlabel="new xlab",
                                 ylabel="new ylab") {
   # Slightly modified function of pamr.geneplot from package pamr by
   # Trevor Hastie, Robert Tibshirani, Balasubramanian Narasimhan,
   # and Gilbert Chu
   require(pamr)
   par(pch = 1, col = 1)
   geneid <- data$geneid
   if (is.null(geneid)) {
     geneid <- as.character(1:nrow(data$x))
   }
   if (is.null(fit$newy)) {
     y <- factor(data$y[fit$sample.subset])
   }
   else {
     y <- factor(fit$newy[fit$sample.subset])
   }
   x <- data$x[fit$gene.subset, fit$sample.subset]
   geneid <- geneid[fit$gene.subset]
   nc <- length(unique(y))
   aa <- pamr.predict(fit, x, threshold = threshold, type = "nonzero")
   cen <- pamr.predict(fit, x, threshold = threshold, type = "cen")
   d <- (cen - fit$centroid.overall)[aa, ]/fit$sd[aa]
   oo <- order(-apply(abs(d), 1, max))
   aa <- aa[oo]
   ngenes <- length(aa)
   o <- order(y)
   xx <- x[aa, o]
   geneid <- geneid[aa]
   nc <- length(unique(y))
   nn <- c(0, cumsum(table(y)))
   nrow <- trunc(sqrt(ngenes)) + 1
   ncol <- trunc(sqrt(ngenes)) + 1
   if (nrow * (ncol - 1) >= ngenes) {
     ncol <- ncol - 1
   }
   par(mfrow = c(nrow, ncol))
   for (i in 1:ngenes) {
     plot(1:ncol(xx), xx[i, ], type = "n", xlab = xlabel,
          ylab = ylabel, axes = FALSE)
     box()
     axis(2)
     for (j in 1:nc) {
       j1 <- nn[j] + 1
       j2 <- nn[j] + table(y)[j]
       points(j1:j2, xx[i, j1:j2], col = j + 1)
     }
     title(main = as.character(geneid[i]))
     for (j in 1:(nc - 1)) {
       abline(v = cumsum(table(y))[j] + 0.5, lty = 2)
     }
     if (i == 1) {
       h <- c(0, table(y))
       for (j in 2:(nc + 1)) {
         text(sum(h[1:(j - 1)]) + 0.5 * h[j], max(xx[i,
                                                     ]), label = 
levels(y)[j - 1], col = j)
       }
     }
   }
   par(mfrow = c(1, 1))
}

library(pamr)
set.seed(120)
x <- matrix(rnorm(1000*20),ncol=20)
y <- sample(c(1:4),size=20,replace=TRUE)
mydata <- list(x=x,y=y)
mytrain <-   pamr.train(mydata)
pamr.geneplot.modif(mytrain, mydata, threshold=1.6, xlabel="Hello", 
ylabel="World")


From maechler at stat.math.ethz.ch  Fri Dec  7 18:38:36 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 7 Dec 2007 18:38:36 +0100
Subject: [R] R help mailing system configuration change?
In-Reply-To: <XFMail.071207104102.Ted.Harding@manchester.ac.uk>
References: <loom.20071207T073152-670@post.gmane.org>
	<XFMail.071207104102.Ted.Harding@manchester.ac.uk>
Message-ID: <18265.34076.514117.444077@stat.math.ethz.ch>

>>>>> "TH" == Ted Harding <Ted.Harding at manchester.ac.uk>
>>>>>     on Fri, 07 Dec 2007 10:41:02 -0000 (GMT) writes:

    TH> I have to express sympathy (with comments, below) with both
    TH> these posters!

    TH> On 07-Dec-07 07:40:30, Dieter Menne wrote:
    >> ?_? ?????? <hoontaechung <at> gmail.com> writes:
    >>> I got a reply for my previous several postings saying that
    >>> I was spamming the r-help mailing list.
    >>> I am very sorry to all subscribers if I did that.
    >>> But I've been reposting my message to the mailing list several
    >>> times because I didn't know whether my help post was actually
    >>> posted or not.
    >>> I remember from my previous experiences that, when I post a message,
    >>> I can see my own posting myself.
    >>> But this time, I didn't see my own message so I thought my message
    >>> got dropped for some reasons.
    >>> Was there any change in r-help mailing system configuration?
    >>> 
    >>> Thanks in advance,
    >>> 
    >>> Tae-Hoon Chung

    TH> It can sometimes be the case that a message which you have
    TH> posted takes a long time to be sent to you by the R list,
    TH> while it may have been distributed to many other list
    TH> readers quite quickly. At times I have experienced delays
    TH> of up to 3 hours (though normally it is within say 15 minutes).

I think it only takes a longer time, when it ends up
"spam-tagged" and has to wait approval ..

    TH> One way to check whether your posting has reached the list
    TH> is to check in the R-help archives at:

    TH> https://stat.ethz.ch/pipermail/r-help

Yes, indeed; and that's the *only* correct way.

{{And BTW: I do not at all like the many e-mails that go through
  Nabble .. which add their onw little spam lines to each.
  Gmane is fine though ... but let's not discuss this in the
  current thread, please!	   
}}

    TH> Select the most recent month and "View by Date". Your message
    TH> should appear near the end of this archive within a few minutes
    TH> of being accepted by the R-help list server.

    TH> But there is always also the possibility eperienced by Dieter:

     >> I have to second that, the same for me. The only way to access
     >> this list is via gmane, otherwise orbitl.com (a Ceylon-based
     >> anti-spamming list) will jump in. We have a dynamic address,
     >> so this list seems ban whole ranges.


     >> I have tried to remove me from orbitl; it may have worked or not,
     >> because of the dynamic address I do not always track. I tried to
     >> contact the webmaster at ETH twice, but got no response. 

Your problems do not at all concern the webmaster of ETH,
nor the webmaster of the math department of ETH (which is
physically hosting the mailman interface and archives to the R lists).

It does concern the "E-mail masters" of ETH (to a very small extent,
maybe more in the future), and  STAT.MATH.ETHZ.CH specifically,
and I am one of them {and am BCC'ing this message to another one}.

     >> This is an annoyance; I know that I should contact orbitl every time,
     >> but since that organization is a mess, it better would be removed
     >> from the ETH anti-spamming list.
     >> 
     >> Dieter

    TH> This sort of thing can paralyse innocent users.

Yes.  But it happens more and more, not only at ETH.

The "war on spam" has not been won, and it is 
`` costing lives every day'' (lost e-mails).

You may not be aware how much of all e-mail traffic is
spam/viruses nowadays ("90%" is one imprecise citation).
And that's not counting the hacker's attempt to break into mail
servers, notably if they are at "prestigious" places, DNS
attacks, etc.

Currently, we *must* use blacklist services, or our servers
would crumble.

The exact set of blacklist services of course is something that
has to be carefully selected by ``the e-mail masters'' 
and if you can show us that one of the blacklist servers has
become of quite reputable quality, we have to stop using that
specific one.

Best regards,

Martin Maechler, ETH Zurich


    TH> A couple of years ago, when I was using dial-up from home on
    TH> the BT Openworld ISP service (which allocated a dynamic IP
    TH> address on connection), I found at one point that I was
    TH> unable to send email to any UK academic institution whatever!

    TH> The error message was:
    TH> 550 host is listed in rbl-plus.mail-abuse.ja.net
    TH> The reason was that JA.NET (the UK Joint Academic Network)
    TH> had subscribed to a blacklisting service which included
    TH> every known dynamic IP address and of course the IP addresses
    TH> which BT gave me (the "host" in the above message) were
    TH> included.

    TH> For a while I worked round this because I had a log-in
    TH> account on a machine at Manchester University, so I could
    TH> dial-up, log in, and mail from there. That host was
    TH> acceptable ...

    TH> But then I dumped BT (and not just for that reason) and
    TH> switched ISP to Zen.co.uk, who gave me a permanent fixed
    TH> IP address. This evaded that particular problem.

    TH> But then I was hit by SORBS (e.g. https://www.us.sorbs.net )
    TH> which had blacklisted *every* IP address owned by Zen,
    TH> dynamic or static, apparently on the grounds that some of
    TH> these had (allegedly) been used to send spam and SORBS had
    TH> received complaints. Many institutions at the time used SORBS.

    TH> SORBS had (and still has) the interesting rule:

    TH> "Third and finally, if you are really not a spammer,
    TH> or you are truly reformed, de-listing is relatively
    TH> easy, and you can choose one of two options:

    TH> * Donate US$50 to a charity or trust approved by,
    TH> and not connected with, SORBS for each spam
    TH> received related to the listing. This is referred
    TH> to as the SORBS 'fine'.
 
    TH> * Wait for a period of 1 year for each spam received
    TH> related to the listing (e.g. if 3 spams were received,
    TH> wait 3 years)."

    TH> Clearly this made it out of the question for an ISP to get
    TH> themselves removed from the SORBS list, since it could amount
    TH> to many 1000s of $$! (And then there's the next time ... ).

    TH> The situation, as I understand it, was resolved when the
    TH> institutions stopped using SORBS, and I have had no such
    TH> trouble since.

    TH> I conclude from experiences like this that institutions
    TH> have a responsibility to treat bona-fide users fairly.
    TH> This means in particular avoiding "automated" blacklisting
    TH> of totally innocent people who have had the misfortune
    TH> to get on a blacklist through no fault of their own.

    TH> And this can, in turn, mean taking a close look at the
    TH> blacklisting services they consult, in order to ensure
    TH> that making use of them will not penalise people unfairly.

    TH> It's all very well for people working within institutions
    TH> (whose IP addresses will generally be "clean") to be unaware
    TH> of this kind of issue. But people connecting from outside
    TH> will be penalised unless care is taken to be fair to all.

    TH> Best wishes to all,
    TH> Ted.

    TH> --------------------------------------------------------------------
    TH> E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
    TH> Fax-to-email: +44 (0)870 094 0861
    TH> Date: 07-Dec-07                                       Time: 10:40:59
    TH> ------------------------------ XFMail ------------------------------

    TH> ______________________________________________
    TH> R-help at r-project.org mailing list
    TH> https://stat.ethz.ch/mailman/listinfo/r-help
    TH> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    TH> and provide commented, minimal, self-contained, reproducible code.


From mtmorgan at fhcrc.org  Fri Dec  7 18:51:53 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Fri, 07 Dec 2007 09:51:53 -0800
Subject: [R] finding most highly transcribed genes - ranking,
 sorting and subsets?
In-Reply-To: <003c01c838ec$9d797840$7400a8c0@AWALL> (alison waller's message
	of "Fri, 7 Dec 2007 11:17:08 -0500")
References: <003c01c838ec$9d797840$7400a8c0@AWALL>
Message-ID: <6phve7a75qe.fsf@gopher4.fhcrc.org>

Hi Alison --

It's a funny twist of terminology, isn't it? high rank (we're #1!)
corresponds to low value. Maybe a wimpy stats joke? Anyway, (a) if m
is assigned rownames (e.g., from the appropriate column of the 'genes'
data frame in the limma object, rownames(m) <- maList$genes$GeneName)
they'll be caried through the analysis and (b) if you've extracted m
from a limma MAList, then subsetting the MAList with hrow
(maList[hrow,]) will give you a new MAList with all the info carrying
through. This would be the better way to go.

Martin

"alison waller" <alison.waller at utoronto.ca> writes:

> Thanks so much Martin,
>
> This method is definitely more straightforward.  And you are right I don't
> think I was doing anything wrong before. However, I thought that rank, would
> rank the highest 1st, however after looking at the results using your
> methods, I realized it ranks the lowest number 1.  So I modified it for
> rank>18500.  And now I'm getting 300 rows for which the intensity is
> consistenly high.
>
> However, I am still laking some information.  For the results I can get a
> matrix of 300 rows and the corresponding intensities (from m) or rank (from
> h), but what I really want is the name of the original row, which
> corresponds to a specific spot on the array).
>
> I did msubset<-m[hrows,] and as mentioned I just get the rows numbered
> 1-300, while I want to essentially pickout the 300 rows from the original
> 19,000 rows maintaing the original row designation as it corresponds to a
> specific gene.
>
> Thanks again for any suggestions,
>
> Alison
>
> -----Original Message-----
> From: Martin Morgan [mailto:mtmorgan at fhcrc.org] 
> Sent: Thursday, December 06, 2007 4:06 PM
> To: alison waller
> Subject: Re: [R] finding most highly transcribed genes - ranking, sorting
> and subsets?
>
> Hi Alison --
>
> I'm not sure where your problem is coming from, but R can help you to
> more efficiently do your task. Skipping the bioc terminology and data
> structures, you have a matrix
>
>> m <- matrix(runif(100000), ncol=10)
>
> you'd like to determine the rank of values in each column
>
>> r <- apply(m, 2, rank)
>
> identfiy those with high rank
>
>> h <- r < 500
>
> and find the rows for which the rank is always high
>
>> hrows <- apply(h, 1, all)
>
> you can then use hrows to subset your original matrix (m[hrows,]) or
> otherwise, e.g., how many rows with high rank
>
>> sum(hrows)
> [1] 0
>
> or perhaps the distribution of the number of columns in which high
> ranking genes occur.
>
>> table(apply(h, 1, sum))
>
>    0    1    2    3    4 
> 5996 3132  765  100    7 
>
> Martin
>
> "alison waller" <alison.waller at utoronto.ca> writes:
>
>> Hello,
>>
>>  
>>
>> I am not only interested in finding out which genes are the most highly
> up-
>> or down-regulated (which I have done using the linear models and Bayesian
>> statistics in Limma), but I also want to know which genes are consistently
>> highly transcribed (ie. they have a high intensity in the channel of
>> interest eg. Cy5 or Cy3 across the set of experiments).  I might have
> missed
>> a straight forward way to do this, or a valuable function, but I've been
>> using my own methods and going around in circles.
>>
>>  
>>
>> So far I've normalized within and between arrays, then returned the RG
>> values using RG<-RG.MA, then I ranked each R and G values for each array
> as
>> below.
>>
>> rankRG<-RG
>>
>> rankRG$R[,1]<-rank(rankRG$R[,1])
>>
>> rankRG$R[,2]<-rank(rankRG$R[,2]) .. and so on for 6 columns(ie. arrays, as
>> well as the G's)
>>
>>  
>>
>> then I thought I could pull out a subset of rankRG using something like;
>>
>> topRG<-rankRG
>>
>> topRG$R<-subset(topRG$R,topRG$R[,1]<500&topRG$R[,2]<500&topRG$R[,5]<500)
>>
>>  
>>
>> However, this just returned me a matrix with one row of $R (the ranks were
>> <500 for columns 1,2, and 5 and greater than 500 for 3,4,and 6).  However,
> I
>> can't believe that there is only one gene that is in the top 500 for R
>> intensitiy among those three arrays.
>>
>>  
>>
>> Am I doing something wrong?  Can someone think of a better way of doing
>> this?
>>
>>  
>>
>> Thanks
>>
>>  
>>
>> Alison
>>
>>  
>>
>>  
>>
>> ******************************************
>> Alison S. Waller  M.A.Sc.
>> Doctoral Candidate
>> awaller at chem-eng.utoronto.ca
>> 416-978-4222 (lab)
>> Department of Chemical Engineering
>> Wallberg Building
>> 200 College st.
>> Toronto, ON
>> M5S 3E5
>>
>>   
>>
>>  
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> -- 
> Dr. Martin Morgan, PhD
> Computational Biology Shared Resource Director
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N.
> PO Box 19024 Seattle, WA 98109
>
> Location: Arnold Building M2 B169
> Phone: (206) 667-2793
>

-- 
Dr. Martin Morgan, PhD
Computational Biology Shared Resource Director
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M2 B169
Phone: (206) 667-2793


From jluo.rhelp at gmail.com  Fri Dec  7 19:27:11 2007
From: jluo.rhelp at gmail.com (Jack Luo)
Date: Fri, 7 Dec 2007 13:27:11 -0500
Subject: [R] low level plotting question on R
In-Reply-To: <475983F7.9080108@gmail.com>
References: <124ea520712070836td8189f1vd47a733dae2daa24@mail.gmail.com>
	<475983F7.9080108@gmail.com>
Message-ID: <124ea520712071027w54d79c1doc8885062c2c48a5d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/0ae970b6/attachment.pl 

From h.wickham at gmail.com  Fri Dec  7 19:36:20 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 7 Dec 2007 12:36:20 -0600
Subject: [R] R help mailing system configuration change?
In-Reply-To: <CAEBF111-283C-46A0-983D-76DD5D1E0F07@gmail.com>
References: <CAEBF111-283C-46A0-983D-76DD5D1E0F07@gmail.com>
Message-ID: <f8e6ff050712071036i42d24b36v49106de7a2e91c5a@mail.gmail.com>

On 12/6/07, ? ?? <hoontaechung at gmail.com> wrote:
> Hi, all;
>
> I got a reply for my previous several postings saying that I was
> spamming the r-help mailing list.
> I am very sorry to all subscribers if I did that.
> But I've been reposting my message to the mailing list several times
> because I didn't know whether my help post was actually posted or not.
> I remember from my previous experiences that, when I post a message, I
> can see my own posting myself.
> But this time, I didn't see my own message so I thought my message got
> dropped for some reasons.

Perhaps you have recently switched to gmail?  It's a "feature" of
gmail that you won't see your mails to a mailing list until someone
replies to them.  (I think this is because gmail only stores one copy
of the message, and that message appears only in your sent mail until
it gets a reply).

Hadley

-- 
http://had.co.nz/

From ggrothendieck at gmail.com  Fri Dec  7 19:41:48 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Dec 2007 13:41:48 -0500
Subject: [R] using "eval(parse(text)) " , gsub(pattern, replacement, x) ,
	to process "code" within a loop/custom function
In-Reply-To: <346709.39944.qm@web59302.mail.re1.yahoo.com>
References: <346709.39944.qm@web59302.mail.re1.yahoo.com>
Message-ID: <971536df0712071041i18a820dfu254163b68d79c35e@mail.gmail.com>

Use the same names (births, temp, ...) in each Rdata file and then load
each file into its own environment or proto object:

	library(proto); x1951 <- proto() # or x1951 <- new.env()
	load("1951.rda", envir = x1951)

Then pass the environment or proto object to each of your functions:

	f <- function(x) x$difference <- x$births - x$temp
	f(x1951)

The above completely avoids renaming variables and instead treats each
year as an object. If you use proto objects the home page
is: http://r-proto.googlecode.com

On Dec 6, 2007 12:10 PM, Thomas Pujol <thomas.pujol at yahoo.com> wrote:
> R-help users,
>  Thanks in advance for any assistance ... I truly appreciate your expertise.  I searched help and could not figure this out, and think you can probably offer some helpful tips. I apologize if I missed something, which I'm sure I probably did.
>
>  I have data for many "samples". (e.g. 1950, 1951, 1952, etc.)
>
>  For each "sample", I have many data-frames. (e.g. temp.1952, births.1952, gdp.1952, etc.)
>
>  (Because the data is rather "large" (and for other reasons), I have chosen to store the data as individual files, as opposed to a list of data frames.)
>
>  I wish to write a function that enables me to "run" any of many custom "functions/processes" on each sample of data.
>
>  I currently accomplish this by using a custom function that uses:
> "eval(parse(t=text.i2)) ", and "gsub(pat, rep, x)" (this changes the "sample number" for each line of text I submit to "eval(parse(t=text.i2))" ).
>
>  Is there a better/preferred/more flexible way to do this?
>
>  One issue/obstacle that I have encountered: Some of the custom functions I use need to take as input the value of "d" in the loop below.
> (Please see the sample function "fn.mn.d" below.)
>
> #creates sample data
> temp.1951 <- c(11,13,15)
> births.1951 <- c(123, 156, 178)
> temp.1952 <- c(21,23,25)
> births.1952 <- c(223, 256, 278)
> #######################
> #function that looks for a a pattern "pat.i" within "x", and replaces it with "rep"
> recurse <- function(x, pat.i,rep.i) {
> f <- function(x,pat,rep) if (mode(x) == "character") gsub(pat, rep, x)  else x
>   if (length(x) == 0) return(x)
>   if (is.list(x)) for(i in seq_along(x)) x[[i]] <- recurse(x[[i]], pat.i,rep.i)
>   else x <- f(x,pat.i,rep.i)
>   x
> #f <- function(x) if (mode(x) == "character") gsub("a", "green", x)  else x
> }# end recurse end
> #######################
>  #######################
> #function that processes code submitted as "text.i" for each date in "dates.i"
> fn.dateloop <- function(text.i, dates.i ) {
> for(d in 1: length(dates.i) ) {
> tempdate <- dates.i[d]
> text.i2 <- recurse(text.i, pat.i='#', rep.i=tempdate)
> temp0=eval(parse(t=text.i2))
> tempname <- paste(names(temp0)[1], tempdate, sep='.')
> save(list='temp0', file = tempname)
> } # next d
> } # end fn.dateloop
> #######################
>  #####################
> #a sample custom function that I want to run on each sample of data
> fn.mn <- function(x, y) {
> res = x - y
> names(res) = 'mn'
> res
> }
> #####################
> #####################
> #example of function that takes d as input...
> #I have not been able to get this to work with the custom function "fn.dateloop" above
> #I request assistance in learning how to accomplish this
> fn.mn.d <- function(x, y, d) {x[d] - y[d]}
> #####################
>  #####################
> setwd('c:/') #specifies location where sample data will be saved
> getwd() #checks location
> fn.mn(x=temp.1951, y=births.1951)
> fn.mn(x=temp.1952, y=births.1952)
> #
> fn.dateloop(text.i = "fn.mn(x=get('temp.#'), y=get('births.#') )" , dates.i=c('1951','1952') )
> get(load('mn.1951'))
> get(load('mn.1952'))
>
>
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jmburgos at u.washington.edu  Fri Dec  7 19:45:58 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Fri, 07 Dec 2007 10:45:58 -0800
Subject: [R] if/else for plot/lines?
In-Reply-To: <4759205C.4060905@ucsd.edu>
References: <4759205C.4060905@ucsd.edu>
Message-ID: <475994E6.6050408@u.washington.edu>

The simplest way would be to have a flag, an indicator variable that 
stores a value that indicates if a plot has been done before.  Something 
like this

plot (do my first plot here...)
is.plot=T

.... later in the code...

if (is.plot) {plot (do new plot here)} else {lines(add lines to the 
previous plot)}


Julian


Roger Levy wrote:
> I'm interested in writing a function that constructs a new plot on the
> current graphics device if no plot exists there yet, but adds lines to
> the existing plot if a plot is already there.  How can I do this?  It
> seems to me that the exists() function might be co-opted to do this, but
> it's not obvious how.
>
> Many thanks,
>
> Roger
>
>   

-- 
Julian M. Burgos

Fisheries Acoustics Research Lab
School of Aquatic and Fishery Science
University of Washington

1122 NE Boat Street
Seattle, WA  98105 

Phone: 206-221-6864


From ripley at stats.ox.ac.uk  Fri Dec  7 19:53:40 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 7 Dec 2007 18:53:40 +0000 (GMT)
Subject: [R] correlated data
In-Reply-To: <000901c838f2$ecb33df0$9d3ff880@demirtasxp157>
References: <000c01c83845$1bf5eda0$9d3ff880@demirtasxp157>
	<47592ABC.5080206@statistik.uni-dortmund.de>
	<000901c838f2$ecb33df0$9d3ff880@demirtasxp157>
Message-ID: <Pine.LNX.4.64.0712071850030.13069@gannet.stats.ox.ac.uk>

Look at package lme4.

If you have code set up for glme(), adapting it to glmmPQL in package MASS 
will be rather easier, but if you are starting afresh I would suggest you 
learn the lme4 syntax straight away.

On Fri, 7 Dec 2007, HAKAN DEMIRTAS wrote:

> Thanks for your reply. I am looking for an R package that has the same
> functionality of the glme() function within the correlatedData library in
> SPlus 6.2 or above. This generic function fits a generalized linear
> mixed-effects model in the formulation described in Breslow and Clayton
> (1993) but allowing for nested random effects. The within-group errors are
> allowed to be correlated and/or have additional heteroscedastic patterns.
> Regards, Hakan Demirtas

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From h.wickham at gmail.com  Fri Dec  7 20:02:05 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 7 Dec 2007 13:02:05 -0600
Subject: [R] [R-pkgs] fda, version 1.2.3
Message-ID: <f8e6ff050712071102o58d7de95iaf491b11e016652e@mail.gmail.com>

fda 1.2.3
========================

Version 1.2.3 of the fda package has just been released. This version adds to
previous versions a script to create most of the figures of chapter 6 of
"Applied Functional Data Analysis" by Ramsay and Silverman. Other changes
offer simpler calls to time warping / registration and functional principal
component functions.

The fda package supports the books "Functional Data Analysis" and "Applied
Functional Data Analysis" by Bernard Silverman and James Ramsay. Functional
data analysis, which lots of us like to call "FDA", is about the analysis of
information on curves or functions. FDA is a collection statistical techniques
for answering questions like, "What are the main ways in which the curves vary
from one to another?" In fact, most of the questions and problems associated
with multivariate data (PCA, LDA, clustering, ...) have functional
counterparts. More information about FDA can be found at
http://www.psych.mcgill.ca/misc/fda/.

Regards

Hadley Wickham
James Ramsey
Spencer Graves

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From aiminy at iastate.edu  Fri Dec  7 20:08:40 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Fri, 07 Dec 2007 13:08:40 -0600
Subject: [R] question in xyplot of lattice
Message-ID: <6.1.2.0.2.20071207130504.02d317b8@aiminy.mail.iastate.edu>

I try to make a xyplot like the following:

xyplot(y1+y2~id|groups, ...)

I also want to calculate cor(y1,y2) in each group, print it on each panel.

Does anyone know how to write panel function for this?

Thanks,

Aimin


From revelator13 at wp.pl  Fri Dec  7 20:08:54 2007
From: revelator13 at wp.pl (=?ISO-8859-2?Q?piotr_iksi=F1ski?=)
Date: Fri, 07 Dec 2007 20:08:54 +0100
Subject: [R] Internal functions
Message-ID: <47599a46a05cc@wp.pl>

I've complex problem and this's idealization of it:

v=1:10

fx<-function(x){
v[x]=v[x]*2}

fy<-function(y){
fx(y)}

for(i in 1:10){
fy(i)}

How to modife expresion (and only that):
{v[x]=v[x]^2}

to achive the redefinetion of the values of vector v, the result should 
be: v=(1:10)*2. I think, that could be done with using of internal 
functions, but I'm not the programmer.
Thanks for help
pg

----------------------------------------------------
P?yta CD i kartka pocztowa "Poczt?wka do ?w.Miko?aja 2007" ju? 
w sprzeda?y. Kupuj?c t? p?yt? pomagasz dzieciom 
z dom?w dziecka. wwW.pocztowkadoswietegomikolaja.pl
http://klik.wp.pl/?adr=http%3A%2F%2Fcorto.www.wp.pl%2Fas%2Fpocztowka.html&sid=122


From cberry at tajo.ucsd.edu  Fri Dec  7 20:29:04 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 7 Dec 2007 11:29:04 -0800
Subject: [R] Internal functions
In-Reply-To: <47599a46a05cc@wp.pl>
References: <47599a46a05cc@wp.pl>
Message-ID: <Pine.LNX.4.64.0712071128300.20596@tajo.ucsd.edu>



See

 	?body

HTH,

Chuck


On Fri, 7 Dec 2007, piotr iksi?ski wrote:

> I've complex problem and this's idealization of it:
>
> v=1:10
>
> fx<-function(x){
> v[x]=v[x]*2}
>
> fy<-function(y){
> fx(y)}
>
> for(i in 1:10){
> fy(i)}
>
> How to modife expresion (and only that):
> {v[x]=v[x]^2}
>
> to achive the redefinetion of the values of vector v, the result should
> be: v=(1:10)*2. I think, that could be done with using of internal
> functions, but I'm not the programmer.
> Thanks for help
> pg
>
> ----------------------------------------------------
> P?yta CD i kartka pocztowa "Poczt?wka do ?w.Miko?aja 2007" ju?
> w sprzeda?y. Kupuj?c t? p?yt? pomagasz dzieciom
> z dom?w dziecka. wwW.pocztowkadoswietegomikolaja.pl
> http://klik.wp.pl/?adr=http%3A%2F%2Fcorto.www.wp.pl%2Fas%2Fpocztowka.html&sid=122
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From deepayan.sarkar at gmail.com  Fri Dec  7 21:37:14 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 7 Dec 2007 12:37:14 -0800
Subject: [R] question in xyplot of lattice
In-Reply-To: <6.1.2.0.2.20071207130504.02d317b8@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20071207130504.02d317b8@aiminy.mail.iastate.edu>
Message-ID: <eb555e660712071237l54454ffehc5c871d111360fc7@mail.gmail.com>

On 12/7/07, Aimin Yan <aiminy at iastate.edu> wrote:
> I try to make a xyplot like the following:
>
> xyplot(y1+y2~id|groups, ...)
>
> I also want to calculate cor(y1,y2) in each group, print it on each panel.
>
> Does anyone know how to write panel function for this?

This should work:

panel.cor <- function(x, y, groups, subscripts, ...)
{
    require(grid)
    g <- groups[subscripts]
    ug <- unique(g)
    stopifnot(length(ug) == 2)
    y1 <- y[g == ug[1]]
    y2 <- y[g == ug[2]]
    grid.text(label = round(cor(y1, y2), 3),
              x = unit(0.5, "npc"),
              y = unit(0.5, "npc"))
}

xyplot(y1 + y2 ~ id | groups,
       panel = function(...) {
           panel.xyplot(...)
           panel.cor(...)
       })

See ?grid.text for finer control over placement.

-Deepayan


From alison.waller at utoronto.ca  Fri Dec  7 21:57:39 2007
From: alison.waller at utoronto.ca (alison waller)
Date: Fri, 7 Dec 2007 15:57:39 -0500
Subject: [R] finding most highly transcribed genes - ranking,
	sorting and subsets?
In-Reply-To: <6phve7a75qe.fsf@gopher4.fhcrc.org>
Message-ID: <007901c83913$cd7ad440$7400a8c0@AWALL>

Thanks - great, should have thought of option b)


-----Original Message-----
From: Martin Morgan [mailto:mtmorgan at fhcrc.org] 
Sent: Friday, December 07, 2007 12:52 PM
To: alison waller
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] finding most highly transcribed genes - ranking, sorting
and subsets?

Hi Alison --

It's a funny twist of terminology, isn't it? high rank (we're #1!)
corresponds to low value. Maybe a wimpy stats joke? Anyway, (a) if m
is assigned rownames (e.g., from the appropriate column of the 'genes'
data frame in the limma object, rownames(m) <- maList$genes$GeneName)
they'll be caried through the analysis and (b) if you've extracted m
from a limma MAList, then subsetting the MAList with hrow
(maList[hrow,]) will give you a new MAList with all the info carrying
through. This would be the better way to go.

Martin

"alison waller" <alison.waller at utoronto.ca> writes:

> Thanks so much Martin,
>
> This method is definitely more straightforward.  And you are right I don't
> think I was doing anything wrong before. However, I thought that rank,
would
> rank the highest 1st, however after looking at the results using your
> methods, I realized it ranks the lowest number 1.  So I modified it for
> rank>18500.  And now I'm getting 300 rows for which the intensity is
> consistenly high.
>
> However, I am still laking some information.  For the results I can get a
> matrix of 300 rows and the corresponding intensities (from m) or rank
(from
> h), but what I really want is the name of the original row, which
> corresponds to a specific spot on the array).
>
> I did msubset<-m[hrows,] and as mentioned I just get the rows numbered
> 1-300, while I want to essentially pickout the 300 rows from the original
> 19,000 rows maintaing the original row designation as it corresponds to a
> specific gene.
>
> Thanks again for any suggestions,
>
> Alison
>
> -----Original Message-----
> From: Martin Morgan [mailto:mtmorgan at fhcrc.org] 
> Sent: Thursday, December 06, 2007 4:06 PM
> To: alison waller
> Subject: Re: [R] finding most highly transcribed genes - ranking, sorting
> and subsets?
>
> Hi Alison --
>
> I'm not sure where your problem is coming from, but R can help you to
> more efficiently do your task. Skipping the bioc terminology and data
> structures, you have a matrix
>
>> m <- matrix(runif(100000), ncol=10)
>
> you'd like to determine the rank of values in each column
>
>> r <- apply(m, 2, rank)
>
> identfiy those with high rank
>
>> h <- r < 500
>
> and find the rows for which the rank is always high
>
>> hrows <- apply(h, 1, all)
>
> you can then use hrows to subset your original matrix (m[hrows,]) or
> otherwise, e.g., how many rows with high rank
>
>> sum(hrows)
> [1] 0
>
> or perhaps the distribution of the number of columns in which high
> ranking genes occur.
>
>> table(apply(h, 1, sum))
>
>    0    1    2    3    4 
> 5996 3132  765  100    7 
>
> Martin
>
> "alison waller" <alison.waller at utoronto.ca> writes:
>
>> Hello,
>>
>>  
>>
>> I am not only interested in finding out which genes are the most highly
> up-
>> or down-regulated (which I have done using the linear models and Bayesian
>> statistics in Limma), but I also want to know which genes are
consistently
>> highly transcribed (ie. they have a high intensity in the channel of
>> interest eg. Cy5 or Cy3 across the set of experiments).  I might have
> missed
>> a straight forward way to do this, or a valuable function, but I've been
>> using my own methods and going around in circles.
>>
>>  
>>
>> So far I've normalized within and between arrays, then returned the RG
>> values using RG<-RG.MA, then I ranked each R and G values for each array
> as
>> below.
>>
>> rankRG<-RG
>>
>> rankRG$R[,1]<-rank(rankRG$R[,1])
>>
>> rankRG$R[,2]<-rank(rankRG$R[,2]) .. and so on for 6 columns(ie. arrays,
as
>> well as the G's)
>>
>>  
>>
>> then I thought I could pull out a subset of rankRG using something like;
>>
>> topRG<-rankRG
>>
>> topRG$R<-subset(topRG$R,topRG$R[,1]<500&topRG$R[,2]<500&topRG$R[,5]<500)
>>
>>  
>>
>> However, this just returned me a matrix with one row of $R (the ranks
were
>> <500 for columns 1,2, and 5 and greater than 500 for 3,4,and 6).
However,
> I
>> can't believe that there is only one gene that is in the top 500 for R
>> intensitiy among those three arrays.
>>
>>  
>>
>> Am I doing something wrong?  Can someone think of a better way of doing
>> this?
>>
>>  
>>
>> Thanks
>>
>>  
>>
>> Alison
>>
>>  
>>
>>  
>>
>> ******************************************
>> Alison S. Waller  M.A.Sc.
>> Doctoral Candidate
>> awaller at chem-eng.utoronto.ca
>> 416-978-4222 (lab)
>> Department of Chemical Engineering
>> Wallberg Building
>> 200 College st.
>> Toronto, ON
>> M5S 3E5
>>
>>   
>>
>>  
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> -- 
> Dr. Martin Morgan, PhD
> Computational Biology Shared Resource Director
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N.
> PO Box 19024 Seattle, WA 98109
>
> Location: Arnold Building M2 B169
> Phone: (206) 667-2793
>

-- 
Dr. Martin Morgan, PhD
Computational Biology Shared Resource Director
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M2 B169
Phone: (206) 667-2793


From jsorkin at grecc.umaryland.edu  Fri Dec  7 22:00:03 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Fri, 07 Dec 2007 16:00:03 -0500
Subject: [R] Using R function in Excel
Message-ID: <47596E03.91DF.00CB.0@grecc.umaryland.edu>

Does anyone know a way that an function written in R can be called within a cell of an Excel spreadsheet. I would like to use the R function much as I use the native Excel functions, e.g. instead of using the excel function sum, =sum(A2,A6), I would like to use the function mysum written in R, e.g. =mysum(A2,A6).

Thanks,
John 

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)

Confidentiality Statement:
This email message, including any attachments, is for th...{{dropped:6}}


From nmprista at fc.ul.pt  Fri Dec  7 22:10:01 2007
From: nmprista at fc.ul.pt (Nuno Prista)
Date: Fri, 7 Dec 2007 16:10:01 -0500
Subject: [R] Cannot insert label on top axis
Message-ID: <009701c83915$9066c5d0$c0685280@ts.odu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071207/c79eaffd/attachment.pl 

From markleeds at verizon.net  Fri Dec  7 22:13:29 2007
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 07 Dec 2007 15:13:29 -0600 (CST)
Subject: [R] Cannot insert label on top axis
Message-ID: <25481282.1401561197062009694.JavaMail.root@vms069.mailsrvcs.net>

>From: Nuno Prista <nmprista at fc.ul.pt>
>Date: 2007/12/07 Fri PM 03:10:01 CST
>To: r-help at stat.math.ethz.ch
>Subject: [R] Cannot insert label on top axis

see mtext by doing ?mtext.


>Hi,
>
> 
>
>I am not being able to find code to insert a label on the top graphic, can
>anyone help?
>
> 
>
>Thanks in advance,
>
> 
>
>Nuno Prista
>
> 
>
> 
>
>X<-1:10
>
>plot(x, xlab="I can insert this label here but not on top axis")
>
>axis(3)
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Fri Dec  7 22:31:41 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 07 Dec 2007 22:31:41 +0100
Subject: [R] Using R function in Excel
In-Reply-To: <47596E03.91DF.00CB.0@grecc.umaryland.edu>
References: <47596E03.91DF.00CB.0@grecc.umaryland.edu>
Message-ID: <4759BBBD.5040306@statistik.uni-dortmund.de>

See the R(D)COM Server (package "rcom") and RExcel from
http://sunsite.univie.ac.at/rcom/

Uwe Ligges



John Sorkin wrote:
> Does anyone know a way that an function written in R can be called within a cell of an Excel spreadsheet. I would like to use the R function much as I use the native Excel functions, e.g. instead of using the excel function sum, =sum(A2,A6), I would like to use the function mysum written in R, e.g. =mysum(A2,A6).
> 
> Thanks,
> John 
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> 
> Confidentiality Statement:
> This email message, including any attachments, is for th...{{dropped:6}}
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rmh at temple.edu  Fri Dec  7 22:36:16 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 7 Dec 2007 16:36:16 -0500
Subject: [R] Using R function in Excel
In-Reply-To: <47596E03.91DF.00CB.0@grecc.umaryland.edu>
References: <47596E03.91DF.00CB.0@grecc.umaryland.edu>
Message-ID: <000001c83919$3811dec0$091df79b@sbm.fox.temple.edu>

You are looking for RExcel, which does exactly that.  Full documentation
and examples are included in the package.

http://sunsite.univie.ac.at/rcom/download/devel/

If you don't have R-2.6.1 yet, get the whole R system with
the RExcel installed

RAndFriendsSetup2061V1.82.exe


Otherwise, just get the RExcel installer, either

RExcel.installer_1.80-18.zip

or

RExcel.installer_1.80-18.tar.gz


Also look at the main RExcel/rcom site
http://sunsite.univie.ac.at/rcom/
and consider joining the mailing list there.


From christian.gold at grieg.uib.no  Fri Dec  7 23:53:24 2007
From: christian.gold at grieg.uib.no (Christian Gold)
Date: Fri, 07 Dec 2007 23:53:24 +0100
Subject: [R] How to prevent fix() from converting Dates into numeric
Message-ID: <4759CEE4.30809@grieg.uib.no>

Dear list members

Here is a strange problem that I have had for a long time, without 
finding out how to solve it. Whenever I use fix() on a data.frame that 
contains Dates, these are converted to numerics. As shown by the very 
simple example:

a <- data.frame(var1 = 1, today = Sys.Date() )
a
fix(a)
a

Why is that? And can anything be done against it?

Many thanks for your help!

Best,

Christian Gold
www.uib.no/people/cgo022


From c.gold at magnet.at  Fri Dec  7 23:58:26 2007
From: c.gold at magnet.at (Christian Gold)
Date: Fri, 07 Dec 2007 23:58:26 +0100
Subject: [R] How to prevent fix() from converting Dates into numeric
Message-ID: <4759D012.2060902@magnet.at>

Dear list members

Here is a strange problem that I have had for a long time, without
finding out how to solve it. Whenever I use fix() on a data.frame that
contains Dates, these are converted to numerics. As shown by the very
simple example:

a <- data.frame(var1 = 1, today = Sys.Date() )
a
fix(a)
a

Why is that? And can anything be done against it?

Many thanks for your help!

Best,

Christian Gold
www.uib.no/people/cgo022


From ecjbosu at aol.com  Sat Dec  8 07:02:16 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sat, 08 Dec 2007 00:02:16 -0600
Subject: [R] seq_len
Message-ID: <fjdc1i$9me$1@ger.gmane.org>

In a post on R-devel, Prof Ripley add the following comment
| > BTW, 1:dim(names)[1] is dangerous: it could be 1:0.  That was the
| > motivation for seq_len.

I use the dim(names)[1] and dim(x)[2] along with length(x) with varying 
levels of frustration depending on the object which I am trying to get 
the dimensions.  I found the reference to seq_len interesting since it 
is a function that I have never seen (probably just missed it reading 
the docs).

I was hoping someone could expand on the benefits of seq_len.

Happy Holidays
Joe


From ecjbosu at aol.com  Sat Dec  8 07:49:21 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sat, 08 Dec 2007 00:49:21 -0600
Subject: [R] Using R function in Excel
In-Reply-To: <47596E03.91DF.00CB.0@grecc.umaryland.edu>
References: <47596E03.91DF.00CB.0@grecc.umaryland.edu>
Message-ID: <475A3E71.6010108@aol.com>

John Sorkin wrote:
> Does anyone know a way that an function written in R can be called within a cell of an Excel spreadsheet. I would like to use the R function much as I use the native Excel functions, e.g. instead of using the excel function sum, =sum(A2,A6), I would like to use the function mysum written in R, e.g. =mysum(A2,A6).
> 
> Thanks,
> John 
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> 
> Confidentiality Statement:
> This email message, including any attachments, is for th...{{dropped:6}}
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
Take a look at
R/Scilab (D)COM Server V2.50 and RExcel V1.75 found at
http://cran.r-project.org/other-software.html

I have experimented with it for non R users to run basic R functions or 
custom models.  I like theI did find a problem with the VBA math 
function calls in the xla file on the computer that I was using.  I just 
added VBA. to all the left, mid, etc functions.


I can provide my modified .xla file if you wanted it.

Happy Holidays
Joe


From ecjbosu at aol.com  Sat Dec  8 07:49:21 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sat, 08 Dec 2007 00:49:21 -0600
Subject: [R] Using R function in Excel
In-Reply-To: <47596E03.91DF.00CB.0@grecc.umaryland.edu>
References: <47596E03.91DF.00CB.0@grecc.umaryland.edu>
Message-ID: <475A3E71.6010108@aol.com>

John Sorkin wrote:
> Does anyone know a way that an function written in R can be called within a cell of an Excel spreadsheet. I would like to use the R function much as I use the native Excel functions, e.g. instead of using the excel function sum, =sum(A2,A6), I would like to use the function mysum written in R, e.g. =mysum(A2,A6).
> 
> Thanks,
> John 
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> 
> Confidentiality Statement:
> This email message, including any attachments, is for th...{{dropped:6}}
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
Take a look at
R/Scilab (D)COM Server V2.50 and RExcel V1.75 found at
http://cran.r-project.org/other-software.html

I have experimented with it for non R users to run basic R functions or 
custom models.  I like theI did find a problem with the VBA math 
function calls in the xla file on the computer that I was using.  I just 
added VBA. to all the left, mid, etc functions.


I can provide my modified .xla file if you wanted it.

Happy Holidays
Joe


From ripley at stats.ox.ac.uk  Sat Dec  8 08:37:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 8 Dec 2007 07:37:47 +0000 (GMT)
Subject: [R] How to prevent fix() from converting Dates into numeric
In-Reply-To: <4759CEE4.30809@grieg.uib.no>
References: <4759CEE4.30809@grieg.uib.no>
Message-ID: <Pine.LNX.4.64.0712080734120.22482@gannet.stats.ox.ac.uk>

fix on a data frame calls edit: see ?edit.data.frame.  The help for 
fix does say

   Editing an \R object may change it in ways other than are obvious: see
   the comment under \code{\link{edit}}.

The simple answer is not to use fix() or edit() on other than the data 
frames they are documented to work on.

On Fri, 7 Dec 2007, Christian Gold wrote:

> Dear list members
>
> Here is a strange problem that I have had for a long time, without
> finding out how to solve it. Whenever I use fix() on a data.frame that
> contains Dates, these are converted to numerics. As shown by the very
> simple example:
>
> a <- data.frame(var1 = 1, today = Sys.Date() )
> a
> fix(a)
> a
>
> Why is that? And can anything be done against it?
>
> Many thanks for your help!
>
> Best,
>
> Christian Gold
> www.uib.no/people/cgo022
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From p_connolly at slingshot.co.nz  Sat Dec  8 10:07:47 2007
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Sat, 8 Dec 2007 22:07:47 +1300
Subject: [R] Junk or not Junk ???
In-Reply-To: <C37DD404.1578E%engrav@u.washington.edu>
References: <14193897.post@talk.nabble.com>
	<C37DD404.1578E%engrav@u.washington.edu>
Message-ID: <20071208090747.GN6584@slingshot.co.nz>

On Thu, 06-Dec-2007 at 04:29PM -0800, Loren Engrav wrote:

|> As for news readers
|> I found R and R.mac and R.Bio on the sites you recommend, thank you very
|> much, they would avoid the individual emails, but then I would have to go
|> look at them, which might be Ok
|> 
|> Deluge? Well, there are from R and Bio and R-Mac every morning 30 or 35, and
|> 10-15 more during the daytime, and ~50 deletes is painful
|> 
|> But then every morning one or two are useful so...

I find it absolutely essential to have a client that can display mail
in threads.  Deleting mail a thread at a time is an order of magnitude
more efficient.  Nabble does a fairly good job of showing threads, but
I would prefer to download every message and delete the threads I'm
ignoring.  Even on a good connexion, the delays downloading individual
messages add up.  And gmail is slower still.  Another advantage of
your own client is that you can use a monospaced font which is far
easier for reading code which is bound to happen on a list like this.

Most people I know have the misfortune of not having access to a mail
client that displays threads[1], but for anyone who has control over
such things, in the Windows world, I know Thunderbird is fairly good,
but if you're fortunate enough to be allowed to use Linux, there is
Mutt or you might like Emacs as a mail client which both do threads
very well without the need to use a mouse -- which I consider a huge
bonus.

[....]


|> 
|> Still would be fun to understand why some R are junk and some are not

As several have said, it's to do with your mail client and/or how mail
and spam filters are set up on your domain.  Nothing to do with this
list.

1. Read "misfortune of having to use Outlook or even Outlook Express"


best

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From lars.brinkmann at uleth.ca  Sat Dec  8 10:09:06 2007
From: lars.brinkmann at uleth.ca (Brinkmann, Lars)
Date: Sat, 8 Dec 2007 02:09:06 -0700
Subject: [R] How to extract numbers from ANOVA tables?
Message-ID: <9B0B21A0F89B6140AD09698F46BB766B010ACC82@EXCHCL2.uleth.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071208/57caf33c/attachment.pl 

From ripley at stats.ox.ac.uk  Sat Dec  8 11:10:33 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 8 Dec 2007 10:10:33 +0000 (GMT)
Subject: [R] How to extract numbers from ANOVA tables?
In-Reply-To: <9B0B21A0F89B6140AD09698F46BB766B010ACC82@EXCHCL2.uleth.ca>
References: <9B0B21A0F89B6140AD09698F46BB766B010ACC82@EXCHCL2.uleth.ca>
Message-ID: <Pine.LNX.4.64.0712081005070.11875@gannet.stats.ox.ac.uk>

On Sat, 8 Dec 2007, Brinkmann, Lars wrote:

> I need to extract numbers (eg. estimates, standard errors etc.) 
> summarized in an ANOVA table or summary() table for further 
> calculations. Can somebody help me or refer me to the needed resource on 
> the web?

The Value section of the relevant help page tells you how the values are 
stored.  If you need more details, reading the appropriate print() method 
will show you where the printed numbers come from (and occasionally they 
are computed in the print method).

For e.g. lm or glm fits coeF(summary(fit)) will extract the 
coefficients matrix.

Had you given an example of what you wanted to extract we could have given 
you an example of how to do it.

>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do!


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From christian.gold at grieg.uib.no  Sat Dec  8 11:33:46 2007
From: christian.gold at grieg.uib.no (Christian Gold)
Date: Sat, 08 Dec 2007 11:33:46 +0100
Subject: [R] How to prevent fix() from converting Dates into numeric
In-Reply-To: <Pine.LNX.4.64.0712080734120.22482@gannet.stats.ox.ac.uk>
References: <4759CEE4.30809@grieg.uib.no>
	<Pine.LNX.4.64.0712080734120.22482@gannet.stats.ox.ac.uk>
Message-ID: <475A730A.4070906@grieg.uib.no>

Thanks. So there is no solution, other than avoiding fix() and edit()?
What would then be the recommended way to make visible and inspect large 
data.frames (i.e. that are to big for sensibly displaying on the 
console)? Would I need to write the data to a file and open in a 
spreadsheet programme?

Christian



Prof Brian Ripley wrote:
> fix on a data frame calls edit: see ?edit.data.frame.  The help for fix 
> does say
> 
>   Editing an \R object may change it in ways other than are obvious: see
>   the comment under \code{\link{edit}}.
> 
> The simple answer is not to use fix() or edit() on other than the data 
> frames they are documented to work on.
> 
> On Fri, 7 Dec 2007, Christian Gold wrote:
> 
>> Dear list members
>>
>> Here is a strange problem that I have had for a long time, without
>> finding out how to solve it. Whenever I use fix() on a data.frame that
>> contains Dates, these are converted to numerics. As shown by the very
>> simple example:
>>
>> a <- data.frame(var1 = 1, today = Sys.Date() )
>> a
>> fix(a)
>> a
>>
>> Why is that? And can anything be done against it?
>>
>> Many thanks for your help!
>>
>> Best,
>>
>> Christian Gold
>> www.uib.no/people/cgo022
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From volchik2000 at list.ru  Sat Dec  8 11:51:50 2007
From: volchik2000 at list.ru (Yuri Volchik)
Date: Sat, 8 Dec 2007 10:51:50 -0000
Subject: [R] FW: R memory management
Message-ID: <000b01c83988$56b7ca00$04275e00$@ru>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071208/9c9df86f/attachment.pl 

From lbraglia at gmail.com  Sat Dec  8 12:24:46 2007
From: lbraglia at gmail.com (Luca Braglia)
Date: Sat, 8 Dec 2007 12:24:46 +0100
Subject: [R] [OT] A free (as in freedom) replacement for StatTransfer
Message-ID: <20071208112446.GA2894@debian>

Hello *

does anybody know a free (as in freedom) equivalent for Stat
Transfer

http://stattransfer.com/

Thank you
	Bye


From kchine at ebi.ac.uk  Sat Dec  8 12:45:47 2007
From: kchine at ebi.ac.uk (kchine at ebi.ac.uk)
Date: Sat, 8 Dec 2007 11:45:47 -0000 (GMT)
Subject: [R] Using R function in Excel
In-Reply-To: <47596E03.91DF.00CB.0@grecc.umaryland.edu>
References: <47596E03.91DF.00CB.0@grecc.umaryland.edu>
Message-ID: <38893.80.189.98.44.1197114347.squirrel@webmail.ebi.ac.uk>

Dear John,

The R Workbench (biocep project) comes with a powerful Spreadsheet view
fully connected to an R session. you can use it to do exactly what you
want and it has much more features :

 from within the spreadsheet view you can :
        *import any R Data (numeric, integer, character, logical, complex,
factor, matrix, data frame ) via the toolbar button import from R
(R+arrow towards the spreadsheet)
        *export selected cells to R and assign the content of the cells to
an R variable via the toolbar button export to R (R+arrow towards
R)
         you specify the type of export (numeric, integer, character,
logical, complex, factor, data frame )
         for data frame, you should append to the column name the type of
the column between parathesis ("weight(integer)",
"mesure1(numeric)", "state(factor)", ..)
        * Evaluate an R expression and use the current selection as
argument (the toolbar button R evaluate (R+ruuning man))
         for example you can type "t(%%)" in the R Expression field. This
transposes the selected cells matrix. The result is sent to the
clipboard and you can paste on will
        * type in a cell an expression to evaluate and use any R fucntion,
example in A4 type "=mean(A1:A3)" the content of A4 will be
computed using R fucntion mean and the
         cell value will be the mean of the vector A1:A3. all the R
functions dealing with numeric vectors or matrixes can be used
        * Copy/Paste to and from Excel
        * create as many SpreadSheet Views as needed and specify the
suitable dimensions

on windows and Mac OS, you can use this link to install the R Workbench
http://www.ebi.ac.uk/microarray-srv/frontendapp/rworkbench.jnlp

for more information :
http://www.ebi.ac.uk/microarray-srv/frontendapp/BIOCEP_README.txt

Karim

> Does anyone know a way that an function written in R can be called within
> a cell of an Excel spreadsheet. I would like to use the R function much as
> I use the native Excel functions, e.g. instead of using the excel function
> sum, =sum(A2,A6), I would like to use the function mysum written in R,
> e.g. =mysum(A2,A6).
>
> Thanks,
> John
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
> Confidentiality Statement:
> This email message, including any attachments, is for th...{{dropped:6}}
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Sat Dec  8 15:03:26 2007
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 8 Dec 2007 09:03:26 -0500
Subject: [R] How to extract numbers from ANOVA tables?
In-Reply-To: <9B0B21A0F89B6140AD09698F46BB766B010ACC82@EXCHCL2.uleth.ca>
Message-ID: <6bpls7$51m2um@toip6.srvr.bell.ca>

Dear Lars,

In addition to looking at the relevant help pages, a nice thing about R is
that the objects are there for you to examine. For example:

> mod <- lm(mpg ~ ., data=mtcars)
> av <- anova(mod)

> names(av)
[1] "Df"      "Sum Sq"  "Mean Sq" "F value" "Pr(>F)" 

> str(av)
Classes 'anova' and 'data.frame':       11 obs. of  5 variables:
 $ Df     : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Sum Sq : num  817.71  37.59   9.37  16.47  77.48 ...
 $ Mean Sq: num  817.71  37.59   9.37  16.47  77.48 ...
 $ F value: num  116.42   5.35   1.33   2.34  11.03 ...
 $ Pr(>F) : num  5.03e-10 3.09e-02 2.61e-01 1.41e-01 3.24e-03 ...
 - attr(*, "heading")= chr  "Analysis of Variance Table\n" "Response: mpg"

Thus, e.g., av$"Sum Sq" returns the sums of squares:

> av$"Sum Sq"
 [1] 817.7129524  37.5939529   9.3709293  16.4674349  77.4757948   3.9493082
 [7]   0.1297687  14.4742372   0.9717105   0.4066688 147.4944300

You could do the same thing with the object returned by summary().

I hope that this helps,
 John

--------------------------------
John Fox, Professor
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Brinkmann, Lars
> Sent: Saturday, December 08, 2007 4:09 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to extract numbers from ANOVA tables?
> 
> I need to extract numbers (eg. estimates, standard errors 
> etc.) summarized in an ANOVA table or summary() table for 
> further calculations. Can somebody help me or refer me to the 
> needed resource on the web?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cskiadas at gmail.com  Sat Dec  8 15:52:34 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Sat, 8 Dec 2007 09:52:34 -0500
Subject: [R] seq_len
In-Reply-To: <fjdc1i$9me$1@ger.gmane.org>
References: <fjdc1i$9me$1@ger.gmane.org>
Message-ID: <95537B63-9627-4DC0-80FB-0B91F04253DE@gmail.com>


On Dec 8, 2007, at 1:02 AM, Joe W. Byers wrote:

> In a post on R-devel, Prof Ripley add the following comment
> | > BTW, 1:dim(names)[1] is dangerous: it could be 1:0.  That was the
> | > motivation for seq_len.
>
> I use the dim(names)[1] and dim(x)[2] along with length(x) with  
> varying
> levels of frustration depending on the object which I am trying to get
> the dimensions.  I found the reference to seq_len interesting since it
> is a function that I have never seen (probably just missed it reading
> the docs).
>
> I was hoping someone could expand on the benefits of seq_len.

I think that example says it all. But in simpler form, suppose x is a  
vector, and you want to produce a regular sequence of integers of the  
same length. What should happen i the vector x has length 0? Here's  
the output of the two commands.

x<-numeric(0)
 > y<-length(x)
 > y
[1] 0
 > 1:y
[1] 1 0
 > seq_len(y)
integer(0)

Other than treating the edge case correctly, the only other advantage  
of seq_len, that I am aware of, is that it is faster. Not sure how  
often that ends up mattering though.

> Happy Holidays
> Joe
>

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From engrav at u.washington.edu  Sat Dec  8 16:04:11 2007
From: engrav at u.washington.edu (Loren Engrav)
Date: Sat, 08 Dec 2007 07:04:11 -0800
Subject: [R] Junk or not Junk ???
In-Reply-To: <20071208090747.GN6584@slingshot.co.nz>
Message-ID: <C37FF26B.158E3%engrav@u.washington.edu>

Well thank you all, this is solved

Safe domains do not work for lists
News reader solves the problem but at the cost of needing to "go and look"
Univ Wash Computing and Communications was no help

And all you all said "make a filter"
But I could not find custom filters in Mac Entourage

But it was staring me in the face, called Mailing List Manager

Give this Mailing List Manager an list address and voila, no junk from that
mailing list, all goes in the in box

Thank you so much

Loren Engrav
Univ Wash
Seattle


> From: Patrick Connolly <p_connolly at slingshot.co.nz>
> Date: Sat, 8 Dec 2007 22:07:47 +1300
> To: Loren Engrav <engrav at u.washington.edu>
> Cc: "r-help at r-project.org" <r-help at r-project.org>
> Subject: Re: [R] Junk or not Junk ???
> 
> On Thu, 06-Dec-2007 at 04:29PM -0800, Loren Engrav wrote:
> 
> |> As for news readers
> |> I found R and R.mac and R.Bio on the sites you recommend, thank you very
> |> much, they would avoid the individual emails, but then I would have to go
> |> look at them, which might be Ok
> |> 
> |> Deluge? Well, there are from R and Bio and R-Mac every morning 30 or 35,
> and
> |> 10-15 more during the daytime, and ~50 deletes is painful
> |> 
> |> But then every morning one or two are useful so...
> 
> I find it absolutely essential to have a client that can display mail
> in threads.  Deleting mail a thread at a time is an order of magnitude
> more efficient.  Nabble does a fairly good job of showing threads, but
> I would prefer to download every message and delete the threads I'm
> ignoring.  Even on a good connexion, the delays downloading individual
> messages add up.  And gmail is slower still.  Another advantage of
> your own client is that you can use a monospaced font which is far
> easier for reading code which is bound to happen on a list like this.
> 
> Most people I know have the misfortune of not having access to a mail
> client that displays threads[1], but for anyone who has control over
> such things, in the Windows world, I know Thunderbird is fairly good,
> but if you're fortunate enough to be allowed to use Linux, there is
> Mutt or you might like Emacs as a mail client which both do threads
> very well without the need to use a mouse -- which I consider a huge
> bonus.
> 
> [....]
> 
> 
> |> 
> |> Still would be fun to understand why some R are junk and some are not
> 
> As several have said, it's to do with your mail client and/or how mail
> and spam filters are set up on your domain.  Nothing to do with this
> list.
> 
> 1. Read "misfortune of having to use Outlook or even Outlook Express"
> 
> 
> best
> 
> -- 
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>    ___    Patrick Connolly
>  {~._.~}             Great minds discuss ideas
>  _( Y )_              Middle minds discuss events
> (:_~*~_:)            Small minds discuss people
>  (_)-(_)                              ..... Anon
>  
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From ripley at stats.ox.ac.uk  Sat Dec  8 16:12:50 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 8 Dec 2007 15:12:50 +0000 (GMT)
Subject: [R] Viewing a large data frame (was How to prevent fix() from
 converting Dates into numeric)
In-Reply-To: <475A730A.4070906@grieg.uib.no>
References: <4759CEE4.30809@grieg.uib.no>
	<Pine.LNX.4.64.0712080734120.22482@gannet.stats.ox.ac.uk>
	<475A730A.4070906@grieg.uib.no>
Message-ID: <Pine.LNX.4.64.0712081228150.13547@gannet.stats.ox.ac.uk>

See ?View for your new question (and please change the subject line when 
you change the subject: see the R posting guide)

You could also have used e.g. format() on the data frame before calling 
fix() if all you want to do was to view it.

On Sat, 8 Dec 2007, Christian Gold wrote:

> Thanks. So there is no solution, other than avoiding fix() and edit()?
> What would then be the recommended way to make visible and inspect large 
> data.frames (i.e. that are to big for sensibly displaying on the console)? 
> Would I need to write the data to a file and open in a spreadsheet programme?
>
> Christian
>
>
>
> Prof Brian Ripley wrote:
>> fix on a data frame calls edit: see ?edit.data.frame.  The help for fix 
>> does say
>>
>>   Editing an \R object may change it in ways other than are obvious: see
>>   the comment under \code{\link{edit}}.
>> 
>> The simple answer is not to use fix() or edit() on other than the data 
>> frames they are documented to work on.
>> 
>> On Fri, 7 Dec 2007, Christian Gold wrote:
>> 
>>> Dear list members
>>> 
>>> Here is a strange problem that I have had for a long time, without
>>> finding out how to solve it. Whenever I use fix() on a data.frame that
>>> contains Dates, these are converted to numerics. As shown by the very
>>> simple example:
>>> 
>>> a <- data.frame(var1 = 1, today = Sys.Date() )
>>> a
>>> fix(a)
>>> a
>>> 
>>> Why is that? And can anything be done against it?
>>> 
>>> Many thanks for your help!
>>> 
>>> Best,
>>> 
>>> Christian Gold
>>> www.uib.no/people/cgo022
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at comcast.net  Sat Dec  8 17:29:15 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 08 Dec 2007 10:29:15 -0600
Subject: [R] [OT] A free (as in freedom) replacement for StatTransfer
In-Reply-To: <20071208112446.GA2894@debian>
References: <20071208112446.GA2894@debian>
Message-ID: <1197131355.2955.37.camel@Bellerophon.localdomain>


On Sat, 2007-12-08 at 12:24 +0100, Luca Braglia wrote:
> Hello *
> 
> does anybody know a free (as in freedom) equivalent for Stat
> Transfer
> 
> http://stattransfer.com/
> 
> Thank you
> 	Bye

The answer is: it depends.

If you are looking for a single free open source application that covers
all of the formats that StatTransfer supports, then no.

However, many of the formats (at least in terms of being able to read
them) are supported by one or more components within R, either via the
base and recommended packages or via CRAN add-ons. Much of this is
covered in the R Data Import/Export manual.

Perhaps the most notable format that is lacking is the SAS proprietary
format (not the Transport format), which is not openly published and to
my knowledge, has not been independently reverse engineered.

Any of the commercial products that support that format, with the
possible exception of the SAS System Viewer (which is not open source,
but is free from SAS), will be closed source and will have to be
purchased. The non-SAS product vendors will have likely paid a licensing
fee to SAS for the technical details of the format, which they would
need to recover via customers paying for their product. They will also
have likely signed NDA's to prevent them from disclosing the relevant
source code and related materials that describe the format.

If there is specific functionality that you are looking for, let us know
and we can provide more targeted guidance.

HTH,

Marc Schwartz


From gufrgs at gmail.com  Sat Dec  8 18:04:13 2007
From: gufrgs at gmail.com (Giovane)
Date: Sat, 8 Dec 2007 15:04:13 -0200
Subject: [R] lm: how to calculate rsquared of the predicted values?
Message-ID: <bc1b93170712080904k73f6b94ew696a105303bc8a56@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071208/7fe9fe16/attachment.pl 

From aminzoll at ece.tamu.edu  Sat Dec  8 02:04:18 2007
From: aminzoll at ece.tamu.edu (aminzoll at ece.tamu.edu)
Date: Fri, 07 Dec 2007 19:04:18 -0600
Subject: [R] Constraint Partitioning
In-Reply-To: <20071119035734.2nsljura4kkcgwwk@webapps.ee.tamu.edu>
References: <20071119035734.2nsljura4kkcgwwk@webapps.ee.tamu.edu>
Message-ID: <20071207190418.zlmtm6qb0gwgwkgo@webapps.ee.tamu.edu>

Dear all,
  I appreciate to let me know if there is any method in R to find all  
the possible solutions (non-negative integers) to the following  
constraint partitioning problem:

  X_1+X_2+...+X_(2k)=N
   X_1+X_2<=r_1
   X_3+X_4<=r_2
   X_5+X_6<=r_3
   .....
   X(2k-1)+X(2k)=r_k

  Thank you,
  Amin Zollanvari


From snowch at coralms.com  Sat Dec  8 07:17:52 2007
From: snowch at coralms.com (christopher snow)
Date: Sat, 08 Dec 2007 06:17:52 +0000
Subject: [R] relationship between two factors
In-Reply-To: <4757F03B.60701@coralms.com>
References: <4757F03B.60701@coralms.com>
Message-ID: <475A3710.2080801@coralms.com>

Thanks to everyone for their answers. What a helpful community!

christopher snow wrote:
> I have a dataset with two variables that are factors:
>
> 1) Decision Making Satisfaction (DMS), values = A - Completely, B - 
> Mostly, C - Partly, D - Not at all
> 2) IT Satisfaction values (ITS), values = A - Completely, B - Mostly, C 
> - Partly, D - Not at all
>
> I would like to produce a table (matrix) and a chart of the factors, 
> with counts at the cross sections:
>
>          A   B   C   D
> A   
> B       counts
> C
> D
>
> How can I do this in R?
>
> Many thanks,
>
> Chris
>
>   


-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.


From pw2233 at columbia.edu  Sat Dec  8 11:18:59 2007
From: pw2233 at columbia.edu (pw2233 at columbia.edu)
Date: Sat, 08 Dec 2007 05:18:59 -0500
Subject: [R] help for segmented package
Message-ID: <20071208051859.xo11tq9mxlc88gss@cubmail.cc.columbia.edu>

Hi,

   I am trying to find m breakpoints of a linear regression model. I  
used the segmented package. It works fine for small number of  
predicators and breakpoints.(3 r.v. 3 points).  However, my model has  
14 variables it even would not work even for just one breakpoints!.  
The error message is always estimated breakpoints are out of range.
    Since my problem is time related problem. So I hink the  
breakpoints should be all at the same time for all variables. Can I  
achieve this requiement with this package? How?
Thanks
Best Regards
Peter

Here is the code I used and the data is also attached.

reduceddata <- read.table("reduceddata.txt")
  detach()
  attach(reduceddata)
library(segmented)
dat=data.frame(y=V2,x1=V3,x2=V4,x3=V5,
     x4=V6,x5=V7,x6=V8,x7=V9,x8=V10,x9=V11,
     x10=V12,x11=V13,x12=V14,x13=V15,x14=V16,x15=V16)
m=2
psi <- array(0, c(15,m))
for (i in 1:m)
{
psi[1,i]=range(V3)[1]+(i)/(m+1)*(range(V3)[2]-range(V3)[1])
psi[2,i]=range(V4)[1]+(i)/(m+1)*(range(V4)[2]-range(V4)[1])
psi[3,i]=range(V5)[1]+(i)/(m+1)*(range(V5)[2]-range(V5)[1])
psi[4,i]=range(V6)[1]+(i)/(m+1)*(range(V6)[2]-range(V6)[1])
psi[5,i]=range(V7)[1]+(i)/(m+1)*(range(V7)[2]-range(V7)[1])
psi[6,i]=range(V8)[1]+(i)/(m+1)*(range(V8)[2]-range(V8)[1])
psi[7,i]=range(V9)[1]+(i)/(m+1)*(range(V9)[2]-range(V9)[1])
psi[8,i]=range(V10)[1]+(i)/(m+1)*(range(V10)[2]-range(V10)[1])
psi[9,i]=range(V11)[1]+(i)/(m+1)*(range(V11)[2]-range(V11)[1])
psi[10,i]=range(V12)[1]+(i)/(m+1)*(range(V12)[2]-range(V12)[1])
psi[11,i]=range(V13)[1]+(i)/(m+1)*(range(V13)[2]-range(V13)[1])
psi[12,i]=range(V14)[1]+(i)/(m+1)*(range(V14)[2]-range(V14)[1])
psi[13,i]=range(V15)[1]+(i)/(m+1)*(range(V15)[2]-range(V15)[1])
psi[14,i]=range(V16)[1]+(i)/(m+1)*(range(V16)[2]-range(V16)[1])
psi[15,i]=range(V17)[1]+(i)/(m+1)*(range(V17)[2]-range(V17)[1])
}
out.lm=glm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12+x13+x14
	,data=dat)
o=segmented(out.lm,seg.Z=~x1+x2+x3+x4+x5+x6+ x7+x9+x8+x9+
	x10+x11+x12+x13+x14,
	psi=list(x1=0,x2=psi[2,],x3=psi[3,],
	x4=psi[4],x7=psi[7],x9=psi[9],
	x5=psi[5], x6=psi[6],x8=psi[8],
	x10=psi[10],x11=psi[11],x12=psi[12,],
	x13=psi[13],x14=psi[14,]),control=seg.control(display=TRUE))



-------------- next part --------------
Jan-91	7.858	6.64	7.38	7.7	6.3	6.34	7.7758	7.48	6.969	7.125	7.313	8.065	7.76	7.17	9.52	9.54
Feb-91	7.848	6.27	7.08	7.47	5.95	5.93	7.6225	7.33	7.063	6.891	7.063	7.9233	7.68	6.51	9.05	9.49
Mar-91	7.654	6.4	7.35	7.77	5.91	5.91	7.46	7.3	6.328	6.531	7	7.765	7.58	6.5	9	9.26
Apr-91	7.501	6.24	7.23	7.7	5.67	5.73	7.28	7.16	6.078	6.313	6.75	7.5683	7.47	6.16	9	9.24
May-91	7.329	6.13	7.12	7.7	5.51	5.65	7.0975	7.07	5.953	6.188	6.625	7.365	7.31	6.03	8.5	9.23
Jun-91	7.155	6.36	7.39	7.94	5.6	5.76	6.9525	6.94	6.078	6.563	7	7.185	7.11	6.26	8.5	9.12
Jul-91	6.998	6.31	7.38	7.91	5.58	5.71	6.8167	6.85	5.953	6.313	6.625	7.0083	6.94	6.25	8.5	9.12
Aug-91	6.845	5.78	6.8	7.43	5.39	5.47	6.65	6.76	5.75	5.875	6	6.815	6.76	5.79	8.5	9.1
Sep-91	6.714	5.57	6.5	7.14	5.25	5.29	6.4675	6.61	5.5	5.688	5.813	6.5992	6.62	5.6	8.2	8.93
Oct-91	6.566	5.33	6.23	6.87	5.03	5.08	6.2825	6.53	5.25	5.359	5.5	6.3717	6.54	5.32	8	8.78
Nov-91	6.414	4.89	5.9	6.62	4.6	4.66	6.0808	6.4	4.813	4.938	5.063	6.1142	6.42	4.92	7.58	8.43
Dec-91	6.245	4.38	5.39	6.19	4.12	4.16	5.8583	6.25	4.328	4.25	4.375	5.835	6.29	4.41	7.21	8.25
Jan-92	6.002	4.15	5.4	6.24	3.84	3.88	5.6508	6.01	4.141	4.25	4.625	5.575	6.09	4.07	6.5	8.02
Feb-92	5.8	4.29	5.72	6.58	3.84	3.94	5.4858	5.78	4.25	4.375	4.75	5.3708	5.85	4.13	6.5	8.15
Mar-92	5.611	4.63	6.18	6.95	4.05	4.19	5.3383	5.63	4.25	4.547	5.063	5.1875	5.66	4.42	6.5	8.14
Apr-92	5.427	4.3	5.93	6.78	3.81	3.93	5.1767	5.48	3.969	4.266	4.703	5.0158	5.5	4.13	6.5	8.26
May-92	5.29	4.19	5.81	6.69	3.66	3.78	5.015	5.38	4.016	4.25	4.75	4.8417	5.35	3.96	6.5	8.2
Jun-92	5.258	4.17	5.6	6.48	3.7	3.81	4.8325	5.25	3.922	4.125	4.375	4.6575	5.17	3.97	6.5	8.04
Jul-92	5.069	3.6	4.91	5.84	3.28	3.36	4.6067	5.13	3.391	3.625	3.75	4.44	5.07	3.5	6.02	7.78
Aug-92	4.874	3.47	4.72	5.6	3.14	3.23	4.4142	4.98	3.5	3.625	3.75	4.245	4.98	3.4	6	7.58
Sep-92	4.805	3.18	4.42	5.38	2.97	3.01	4.215	4.84	3.188	3.313	3.375	4.05	4.87	3.17	6	7.44
Oct-92	4.597	3.3	4.64	5.6	2.84	2.98	4.0458	4.74	3.281	3.641	3.938	3.8775	4.76	3.27	6	7.4
Nov-92	4.508	3.68	5.14	6.04	3.14	3.35	3.945	4.59	3.172	3.891	4.141	3.7642	4.6	3.6	6	7.49
Dec-92	4.432	3.71	5.21	6.08	3.25	3.39	3.8892	4.51	3.344	3.641	4.078	3.6817	4.5	3.55	6	7.53
Jan-93	4.36	3.5	4.93	5.83	3.06	3.17	3.835	4.44	3.203	3.438	3.75	3.61	4.4	3.33	6	7.49
Feb-93	4.333	3.39	4.58	5.43	2.95	3.08	3.76	4.34	3.203	3.328	3.578	3.5308	4.34	3.22	6	7.28
Mar-93	4.245	3.33	4.4	5.19	2.97	3.08	3.6517	4.31	3.203	3.375	3.625	3.4358	4.29	3.2	6	7.17
Apr-93	4.171	3.24	4.3	5.13	2.89	3	3.5633	4.24	3.141	3.313	3.563	3.36	4.25	3.16	6	7.06
May-93	4.103	3.36	4.4	5.2	2.96	3.07	3.4942	4.18	3.203	3.438	3.75	3.3	4.24	3.2	6	7.08
Jun-93	4.05	3.54	4.53	5.22	3.1	3.23	3.4417	4.1	3.203	3.563	3.781	3.2458	4.2	3.36	6	7.02
Jul-93	3.998	3.47	4.43	5.09	3.05	3.15	3.4308	4.09	3.188	3.563	3.813	3.2283	4.16	3.34	6	6.95
Aug-93	3.958	3.44	4.36	5.03	3.05	3.17	3.4283	4.04	3.203	3.438	3.563	3.2142	4.1	3.32	6	6.87
Sep-93	3.881	3.36	4.17	4.73	2.96	3.06	3.4433	3.96	3.188	3.375	3.531	3.2133	4.07	3.24	6	6.75
Oct-93	3.823	3.39	4.18	4.71	3.04	3.13	3.4508	3.95	3.188	3.5	3.688	3.2117	4.02	3.25	6	6.59
Nov-93	3.822	3.58	4.5	5.06	3.12	3.27	3.4425	3.9	3.578	3.516	3.781	3.1925	3.98	3.39	6	6.6
Dec-93	3.879	3.61	4.54	5.15	3.08	3.25	3.4342	3.9	3.297	3.5	3.813	3.1742	3.96	3.35	6	6.65
Jan-94	3.71	3.54	4.48	5.09	3.02	3.19	3.4375	3.87	3.141	3.391	3.703	3.1708	3.92	3.29	6	6.73
Feb-94	3.687	3.87	4.83	5.4	3.21	3.38	3.4775	3.76	3.594	4	4.344	3.1967	3.89	3.62	6	6.68
Mar-94	3.629	4.32	5.4	5.94	3.52	3.79	3.56	3.81	3.703	4.25	4.75	3.2517	3.84	4.03	6.06	6.84
Apr-94	3.672	4.82	5.99	6.52	3.74	4.13	3.6917	3.81	3.938	4.625	5.25	3.3283	3.79	4.38	6.45	7.04
May-94	3.726	5.31	6.34	6.78	4.19	4.64	3.8542	3.85	4.375	5	5.516	3.4458	3.76	4.9	6.99	7.33
Jun-94	3.804	5.27	6.27	6.7	4.18	4.58	3.9983	3.86	4.563	5.25	5.828	3.555	3.77	4.85	7.25	7.36
Jul-94	3.86	5.48	6.48	6.91	4.39	4.81	4.1658	3.91	4.516	5.328	5.828	3.6858	3.82	5.15	7.25	7.49
Aug-94	3.945	5.56	6.5	6.88	4.5	4.91	4.3425	3.97	4.875	5.328	5.813	3.825	3.88	5.17	7.51	7.59
Sep-94	4.039	5.76	6.69	7.08	4.64	5.02	4.5425	4.01	5	5.688	6.188	3.9842	3.94	5.4	7.75	7.57
Oct-94	4.187	6.11	7.04	7.4	4.96	5.39	4.7692	4.11	5.078	6	6.563	4.1733	4.05	5.79	7.75	7.6
Nov-94	4.367	6.54	7.44	7.72	5.25	5.69	5.0158	4.17	6	6.438	7.078	4.3767	4.17	6.11	8.15	7.56
Dec-94	4.589	7.14	7.71	7.78	5.64	6.21	5.31	4.32	5.984	7	7.75	4.6292	4.34	6.78	8.5	7.75
Jan-95	4.747	7.05	7.66	7.76	5.81	6.31	5.6025	4.44	6.078	6.688	7.25	4.8867	4.57	6.71	8.5	7.77
Feb-95	4.925	6.7	7.25	7.37	5.8	6.1	5.8383	4.49	6.125	6.438	6.75	5.1142	4.82	6.44	9	8
Mar-95	5.007	6.43	6.89	7.05	5.73	5.91	6.0142	4.69	6.141	6.438	6.75	5.3125	4.99	6.34	9	8.09
Apr-95	5.064	6.27	6.68	6.86	5.67	5.8	6.135	4.79	6.079	6.313	6.563	5.4875	5.14	6.27	9	7.99
May-95	5.141	6	6.27	6.41	5.7	5.73	6.1925	4.91	6.063	6.063	6.055	5.6133	5.27	6.07	9	7.87
Jun-95	5.179	5.64	5.8	5.93	5.5	5.46	6.2233	4.93	6.079	5.875	5.766	5.7283	5.33	5.8	9	7.62
Jul-95	5.144	5.59	5.89	6.01	5.47	5.41	6.2325	4.98	5.875	5.875	5.875	5.815	5.32	5.73	8.8	7.56
Aug-95	5.133	5.75	6.1	6.24	5.41	5.4	6.2483	5.01	5.907	5.938	5.954	5.895	5.3	5.79	8.75	7.6
Sep-95	5.111	5.62	5.89	6	5.26	5.28	6.2367	4.98	5.922	5.985	5.969	5.9533	5.25	5.73	8.75	7.61
Oct-95	5.116	5.59	5.77	5.86	5.3	5.34	6.1933	5.03	5.844	5.954	5.883	5.9767	5.22	5.76	8.75	7.53
Nov-95	5.119	5.43	5.57	5.69	5.35	5.29	6.1008	4.92	6.032	5.735	5.672	5.9725	5.18	5.64	8.75	7.48
Dec-95	5.059	5.31	5.39	5.51	5.16	5.15	5.9483	5.01	5.735	5.563	5.454	5.9167	5.16	5.49	8.65	7.22
Jan-96	5.033	5.09	5.2	5.36	5.02	4.97	5.785	5.01	5.469	5.336	5.196	5.8458	5.15	5.28	8.5	7.18
Feb-96	4.975	4.94	5.14	5.38	4.87	4.79	5.6383	4.9	5.352	5.289	5.274	5.7617	5.15	5.03	8.25	7.13
Mar-96	4.874	5.34	5.79	5.97	4.96	4.96	5.5475	4.92	5.454	5.516	5.704	5.69	5.08	5.3	8.25	7.29
Apr-96	4.841	5.54	6.11	6.3	4.99	5.08	5.4867	4.87	5.454	5.422	5.829	5.6275	5.01	5.42	8.25	7.55
May-96	4.823	5.64	6.27	6.48	5.02	5.12	5.4567	4.87	5.446	5.641	5.977	5.5725	4.97	5.47	8.25	7.72
Jun-96	4.809	5.81	6.49	6.69	5.11	5.26	5.4708	4.82	5.516	5.844	6.172	5.5358	4.93	5.64	8.25	7.77
Jul-96	4.819	5.85	6.45	6.64	5.17	5.32	5.4925	4.87	5.493	5.922	6.243	5.5158	4.92	5.75	8.25	7.87
Aug-96	4.839	5.67	6.21	6.39	5.09	5.17	5.4858	4.87	5.43	5.743	6.055	5.485	4.93	5.57	8.25	7.76
Sep-96	4.834	5.83	6.41	6.6	5.15	5.29	5.5033	4.84	5.438	5.75	5.985	5.4667	4.92	5.71	8.25	7.78
Oct-96	4.839	5.55	6.08	6.27	5.01	5.12	5.5	4.89	5.375	5.579	5.719	5.435	4.95	5.51	8.25	7.68
Nov-96	4.835	5.42	5.82	5.97	5.03	5.07	5.4992	4.76	5.391	5.547	5.696	5.405	4.96	5.43	8.25	7.55
Dec-96	4.842	5.47	5.91	6.07	4.87	5.02	5.5125	4.87	5.547	5.618	5.789	5.39	4.96	5.47	8.25	7.45
Jan-97	4.821	5.61	6.16	6.33	5.05	5.11	5.5558	4.92	5.461	5.711	5.954	5.3933	4.98	5.54	8.25	7.55
Feb-97	4.759	5.53	6.03	6.2	5	5.05	5.605	4.79	5.469	5.68	5.954	5.4117	5.01	5.47	8.25	7.53
Mar-97	4.78	5.8	6.38	6.54	5.14	5.24	5.6433	4.9	5.719	5.961	6.282	5.4317	4.99	5.69	8.3	7.61
Apr-97	4.822	5.99	6.61	6.76	5.17	5.35	5.6808	4.89	5.704	6.079	6.454	5.4608	4.98	5.9	8.5	7.73
May-97	4.864	5.87	6.42	6.57	5.13	5.35	5.7	4.94	5.711	6.008	6.289	5.4892	5	5.87	8.5	7.75
Jun-97	4.853	5.69	6.24	6.38	4.92	5.14	5.69	4.9	5.719	5.938	6.141	5.5058	5.07	5.78	8.5	7.67
Jul-97	4.887	5.54	6	6.12	5.07	5.12	5.6642	4.96	5.641	5.829	5.977	5.5117	5.1	5.7	8.5	7.52
Aug-97	4.904	5.56	6.06	6.16	5.13	5.17	5.655	4.97	5.68	5.86	6.079	5.5283	5.1	5.71	8.5	7.47
Sep-97	4.941	5.52	5.98	6.11	4.97	5.11	5.6292	4.92	5.672	5.852	6.008	5.5358	5.09	5.71	8.5	7.46
Oct-97	4.957	5.46	5.84	5.93	4.95	5.09	5.6217	4.98	5.633	5.805	5.922	5.5558	5.08	5.72	8.5	7.37
Nov-97	4.949	5.46	5.76	5.8	5.15	5.17	5.625	4.93	5.766	6.039	6.11	5.5858	5.07	5.78	8.5	7.34
Dec-97	4.963	5.53	5.74	5.77	5.16	5.24	5.63	4.97	5.852	6.008	6.079	5.6158	5.04	5.82	8.5	7.26
Jan-98	4.987	5.24	5.38	5.42	5.09	5.07	5.5992	4.96	5.704	5.75	5.774	5.625	5.04	5.56	8.5	7.12
Feb-98	4.968	5.31	5.43	5.49	5.11	5.07	5.5808	4.85	5.789	5.782	5.836	5.6392	5.04	5.55	8.5	7.06
Mar-98	4.917	5.39	5.57	5.61	5.03	5.04	5.5467	4.92	5.719	5.797	5.914	5.6433	5.02	5.61	8.5	7.07
Apr-98	4.903	5.38	5.58	5.61	5	5.08	5.4958	4.91	5.688	5.868	6.024	5.6325	5.02	5.63	8.5	7.09
May-98	4.881	5.44	5.61	5.63	5.03	5.15	5.46	4.91	5.696	5.805	5.93	5.6233	5.03	5.67	8.5	7.08
Jun-98	4.881	5.41	5.52	5.52	4.99	5.12	5.4367	4.87	5.748	5.871	5.94	5.6183	5.01	5.65	8.5	7.06
Jul-98	4.911	5.36	5.47	5.46	4.96	5.03	5.4217	4.9	5.696	5.822	5.897	5.6175	4.99	5.65	8.5	7.01
Aug-98	4.899	5.21	5.24	5.27	4.94	4.97	5.3925	4.9	5.727	5.692	5.648	5.6158	4.95	5.61	8.5	6.98
Sep-98	4.882	4.71	4.62	4.62	4.74	4.75	5.325	4.85	5.4	5.359	5.186	5.6	4.92	5.33	8.49	6.88
Oct-98	4.762	4.12	4.18	4.18	4.08	4.15	5.2133	4.84	5.277	5.131	4.865	5.5633	4.91	4.99	8.12	6.74
Nov-98	4.691	4.53	4.57	4.54	4.44	4.43	5.1358	4.77	5.048	5.28	5.244	5.5217	4.85	5.07	7.89	6.77
Dec-98	4.655	4.52	4.48	4.45	4.42	4.43	5.0517	4.74	5.118	5.172	5.213	5.4667	4.79	5.01	7.75	6.76
Jan-99	4.608	4.51	4.61	4.6	4.34	4.36	4.9908	4.63	4.946	5.036	5.108	5.4125	4.67	4.9	7.75	6.77
Feb-99	4.562	4.7	4.9	4.91	4.45	4.43	4.94	4.57	5.003	5.168	5.405	5.3592	4.63	4.95	7.75	6.82
Mar-99	4.519	4.78	5.11	5.14	4.48	4.52	4.8892	4.54	4.94	5.083	5.307	5.3033	4.6	4.98	7.75	6.91
Apr-99	4.49	4.69	5.03	5.08	4.28	4.36	4.8317	4.53	4.902	5.075	5.303	5.245	4.58	4.94	7.75	6.94
May-99	4.48	4.85	5.33	5.44	4.51	4.55	4.7825	4.48	4.93	5.193	5.503	5.1892	4.55	5.03	7.75	6.96
Jun-99	4.504	5.1	5.7	5.81	4.59	4.81	4.7567	4.48	5.223	5.633	5.803	5.15	4.52	5.31	7.75	7.14
Jul-99	4.5	5.03	5.62	5.68	4.6	4.62	4.7292	4.47	5.178	5.68	5.836	5.1208	4.5	5.58	8	7.35
Aug-99	4.562	5.2	5.77	5.84	4.76	4.88	4.7283	4.46	5.37	5.913	6.023	5.1067	4.47	5.83	8.06	7.48
Sep-99	4.608	5.25	5.75	5.8	4.73	4.91	4.7733	4.49	5.396	5.974	6.053	5.1142	4.48	5.89	8.25	7.57
Oct-99	4.666	5.43	5.94	6.03	4.88	4.98	4.8825	4.52	5.41	6.144	6.313	5.1908	4.5	6.04	8.25	7.56
Nov-99	4.773	5.55	5.92	5.97	5.07	5.17	4.9675	4.57	6.503	6.063	6.261	5.2542	4.56	5.97	8.37	7.56
Dec-99	4.852	5.84	6.14	6.19	5.23	5.43	5.0775	4.6	5.832	6.136	6.508	5.33	4.64	6.07	8.5	7.55
Jan-00	4.901	6.12	6.49	6.58	5.34	5.52	5.2117	4.63	5.856	6.238	6.659	5.4183	4.69	6.15	8.5	7.79
Feb-00	4.967	6.22	6.65	6.68	5.57	5.75	5.3383	4.67	5.907	6.328	6.76	5.5108	4.75	6.26	8.73	7.95
Mar-00	5.002	6.22	6.53	6.5	5.72	5.85	5.4583	4.68	6.133	6.53	6.97	5.6133	4.78	6.36	8.83	8.01
Apr-00	5.078	6.15	6.36	6.26	5.67	5.82	5.58	4.73	6.197	6.614	6.964	5.73	4.84	6.5	9	8
May-00	5.196	6.33	6.77	6.69	5.92	6.12	5.7033	4.83	6.641	7.064	7.453	5.8792	4.96	6.94	9.24	8.08
Jun-00	5.357	6.17	6.43	6.3	5.74	6.02	5.7925	4.88	6.649	7.014	7.214	6.0125	5.03	6.91	9.5	8.17
Jul-00	5.456	6.08	6.28	6.18	5.93	5.99	5.88	4.96	6.625	6.887	7.047	6.1317	5.11	6.86	9.5	8.09
Aug-00	5.509	6.18	6.17	6.06	6.1	6.09	5.9617	5.05	6.628	6.831	6.978	6.2317	5.25	6.76	9.5	8
Sep-00	5.548	6.13	6.02	5.93	6.03	6	6.035	5.09	6.62	6.761	6.811	6.3233	5.28	6.68	9.5	7.89
Oct-00	5.589	6.01	5.85	5.78	6.1	6.04	6.0833	5.13	6.621	6.721	6.725	6.3683	5.37	6.65	9.5	7.81
Nov-00	5.607	6.09	5.79	5.7	6.19	6.08	6.1283	5.2	6.827	6.678	6.618	6.4225	5.46	6.61	9.5	7.73
Dec-00	5.617	5.6	5.26	5.17	5.9	5.77	6.1083	5.21	6.565	6.208	5.997	6.4558	5.52	6.3	9.5	7.59
Jan-01	5.514	4.81	4.77	4.86	5.21	5	5.9992	5.22	5.622	5.361	5.284	6.4283	5.54	5.45	9.05	7.25
Feb-01	5.426	4.68	4.71	4.89	4.93	4.78	5.8708	5.17	5.278	4.955	4.925	6.3658	5.47	5.12	8.5	7.1
Mar-01	5.198	4.3	4.43	4.64	4.5	4.36	5.7108	5.09	5.078	4.711	4.67	6.2617	5.35	4.74	8.32	7.02
Apr-01	4.946	3.98	4.42	4.76	3.92	3.89	5.53	5.01	4.435	4.231	4.33	6.1158	5.21	4.41	7.8	7.01
May-01	4.745	3.78	4.51	4.93	3.68	3.69	5.3175	4.9	4.059	3.99	4.259	5.8917	5.47	4.01	7.24	7.08
Jun-01	4.498	3.58	4.35	4.81	3.51	3.46	5.1017	4.78	3.835	3.827	4.055	5.6425	5.03	3.74	6.98	7.1
Jul-01	4.274	3.62	4.31	4.76	3.54	3.48	4.8967	4.68	3.76	3.694	3.835	5.3917	4.6	3.7	6.75	7.1
Aug-01	4.106	3.47	4.04	4.57	3.39	3.31	4.6708	4.59	3.584	3.479	3.6	5.1308	4.39	3.49	6.67	6.99
Sep-01	3.974	2.82	3.45	4.12	2.87	2.84	4.395	4.45	2.637	2.532	2.65	4.82	4.19	2.84	6.28	6.86
Oct-01	3.628	2.33	3.14	3.91	2.22	2.19	4.0883	4.27	2.321	2.173	2.311	4.4567	3.89	2.26	5.53	6.66
Nov-01	3.368	2.18	3.22	3.97	1.93	1.94	3.7625	4.09	2.145	2.101	2.492	4.0717	3.59	2.03	5.1	6.56
Dec-01	3.074	2.22	3.62	4.39	1.72	1.81	3.4808	3.9	1.876	1.983	2.445	3.6867	3.39	1.9	4.84	6.69
Jan-02	2.823	2.16	3.56	4.34	1.64	1.72	3.26	3.79	1.829	1.989	2.42	3.3633	3.27	1.85	4.75	6.81
Feb-02	2.744	2.23	3.55	4.3	1.73	1.83	3.0558	3.65	1.883	2.068	2.496	3.0767	3.15	1.95	4.75	6.77
Mar-02	2.653	2.57	4.14	4.74	1.8	1.99	2.9117	3.55	1.88	2.332	3.006	2.8283	3.04	2.16	4.75	6.77
Apr-02	2.723	2.48	4.01	4.65	1.72	1.97	2.7867	3.48	1.842	2.1	2.613	2.6067	2.98	2.11	4.75	6.88
May-02	2.772	2.35	3.8	4.49	1.74	1.88	2.6675	3.38	1.844	2.09	2.634	2.4233	2.94	1.93	4.75	6.74
Jun-02	2.847	2.2	3.49	4.19	1.71	1.83	2.5525	3.32	1.836	1.948	2.251	2.2625	2.92	1.92	4.75	6.62
Jul-02	2.821	1.96	3.01	3.81	1.68	1.71	2.4142	3.28	1.818	1.863	2.07	2.1067	2.89	1.84	4.75	6.48
Aug-02	2.763	1.76	2.52	3.29	1.63	1.62	2.2717	3.21	1.82	1.815	1.943	1.9608	2.85	1.72	4.75	6.32
Sep-02	2.759	1.72	2.32	2.94	1.63	1.61	2.18	3.13	1.819	1.751	1.813	1.8683	2.81	1.74	4.75	6.23
Oct-02	2.708	1.65	2.25	2.95	1.61	1.58	2.1233	3.1	1.741	1.618	1.664	1.82	2.74	1.69	4.75	6.09
Nov-02	2.537	1.49	2.32	3.05	1.31	1.33	2.0658	3	1.38	1.471	1.705	1.7667	2.63	1.4	4.35	6.03
Dec-02	2.375	1.45	2.23	3.03	1.2	1.26	2.0017	2.9	1.382	1.383	1.447	1.7258	2.56	1.36	4.25	6.04
Jan-03	2.308	1.36	2.18	3.05	1.2	1.26	1.935	2.83	1.339	1.353	1.477	1.6883	2.47	1.3	4.25	5.91
Feb-03	2.257	1.3	2.05	2.9	1.16	1.18	1.8575	2.73	1.334	1.336	1.368	1.6425	2.3	1.27	4.25	5.88
Mar-03	2.21	1.24	1.98	2.78	1.13	1.12	1.7467	2.7	1.306	1.262	1.34	1.5858	2.25	1.2	4.25	5.76
Apr-03	2.208	1.27	2.06	2.93	1.14	1.15	1.6458	2.62	1.318	1.29	1.362	1.5333	2.17	1.23	4.25	5.68
May-03	2.13	1.18	1.75	2.52	1.08	1.09	1.5483	2.57	1.3189	1.2232	1.2214	1.4833	2.14	1.19	4.25	5.58
Jun-03	2.113	1.01	1.51	2.27	0.95	0.94	1.4492	2.5	1.1232	1.1239	1.2014	1.4192	2.12	1.02	4.22	5.36
Jul-03	2.018	1.12	1.93	2.87	0.89	0.94	1.3792	2.41	1.1036	1.1507	1.2789	1.3575	2.06	1.06	4	5.34
Aug-03	1.946	1.31	2.44	3.37	0.95	1.02	1.3417	2.36	1.117	1.2098	1.4714	1.3033	2.03	1.13	4	5.61
Sep-03	1.923	1.24	2.23	3.18	0.95	1.02	1.3017	2.32	1.1214	1.1795	1.2857	1.2467	1.95	1.13	4	5.89
Oct-03	1.909	1.25	2.26	3.19	0.93	1.01	1.2683	2.28	1.1201	1.2207	1.4551	1.1942	1.87	1.14	4	5.78
Nov-03	1.821	1.34	2.45	3.29	0.94	1.02	1.2558	2.25	1.1157	1.2295	1.4867	1.1708	1.86	1.17	4	5.79
Dec-03	1.902	1.31	2.44	3.27	0.9	1	1.2442	2.22	1.1195	1.2192	1.4582	1.1508	1.85	1.17	4	5.74
Jan-04	1.811	1.24	2.27	3.12	0.89	0.98	1.2342	2.22	1.0982	1.2107	1.4607	1.1317	1.85	1.12	4	5.63
Feb-04	1.841	1.24	2.25	3.07	0.92	0.99	1.2292	2.15	1.0973	1.1695	1.3645	1.1133	1.85	1.11	4	5.69
Mar-04	1.815	1.19	2	2.79	0.94	0.99	1.225	2.14	1.0914	1.1595	1.3401	1.0983	1.85	1.09	4	5.43
Apr-04	1.802	1.43	2.57	3.39	0.94	1.06	1.2383	2.12	1.1007	1.3682	1.8082	1.085	1.85	1.21	4	5.37
May-04	1.708	1.78	3.1	3.85	1.04	1.31	1.2883	2.08	1.1089	1.5789	2.0764	1.0833	1.85	1.46	4	5.73
Jun-04	1.758	2.12	3.26	3.93	1.27	1.58	1.3808	2.09	1.3582	1.942	2.4682	1.1183	1.88	1.76	4	5.96
Jul-04	1.816	2.1	3.05	3.69	1.36	1.69	1.4625	2.09	1.4929	1.9857	2.4632	1.1617	1.91	1.85	4.25	5.88
Aug-04	1.875	2.02	2.88	3.47	1.48	1.72	1.5217	2.11	1.6482	1.9907	2.3001	1.2117	1.94	1.89	4.42	5.77
Sep-04	1.931	2.12	2.83	3.36	1.64	1.84	1.595	2.15	1.8401	2.1695	2.4445	1.2767	1.97	2.04	4.58	5.63
Oct-04	1.96	2.23	2.85	3.35	1.74	1.99	1.6767	2.16	1.987	2.3007	2.5289	1.355	2	2.18	4.75	5.64
Nov-04	2.025	2.5	3.09	3.53	2.08	2.28	1.7733	2.18	2.2826	2.6239	2.9607	1.4508	2.02	2.46	4.93	5.65
Dec-04	2.118	2.67	3.21	3.6	2.2	2.45	1.8867	2.22	2.4178	2.7751	3.1004	1.5633	2.08	2.66	5.14	5.71
Jan-05	2.183	2.86	3.39	3.71	2.32	2.6	2.0217	2.27	2.5892	2.9582	3.271	1.6925	2.19	2.85	5.25	5.72
Feb-05	2.317	3.03	3.54	3.77	2.53	2.76	2.1708	2.3	2.6895	3.1495	3.5114	1.8358	2.28	3	5.49	5.68
Mar-05	2.4	3.3	3.91	4.17	2.74	2.98	2.3467	2.34	2.8582	3.3876	3.842	1.9958	2.39	3.23	5.58	5.76
Apr-05	2.515	3.32	3.79	4	2.78	3.07	2.5042	2.39	3.0826	3.4151	3.7101	2.1633	2.52	3.34	5.75	5.86
May-05	2.622	3.33	3.72	3.85	2.86	3.1	2.6333	2.43	3.1126	3.5314	3.7789	2.3317	2.61	3.44	5.98	5.8
Jun-05	2.676	3.36	3.69	3.77	2.97	3.11	2.7367	2.48	3.3401	3.6914	3.8632	2.4917	2.7	3.56	6.01	5.65
Jul-05	2.757	3.64	3.91	3.98	3.19	3.37	2.865	2.54	3.5107	3.9235	4.1745	2.6583	2.78	3.8	6.25	5.68
Aug-05	2.87	3.87	4.08	4.12	3.45	3.67	3.0192	2.61	3.6942	4.0817	4.3123	2.8325	2.89	3.99	6.44	5.83
Sep-05	2.972	3.85	3.96	4.01	3.47	3.68	3.1633	2.67	3.8584	4.2154	4.4067	3	2.97	4.01	6.59	5.85
Oct-05	3.074	4.18	4.29	4.33	3.7	3.98	3.3258	2.74	4.0882	4.4467	4.6765	3.1742	3.06	4.32	6.75	5.98
Nov-05	3.19	4.33	4.43	4.45	3.9	4.16	3.4783	2.82	4.2954	4.5795	4.7379	3.345	3.14	4.52	7	6.22
Dec-05	3.296	4.35	4.39	4.39	3.89	4.19	3.6183	2.86	4.3857	4.6901	4.8226	3.5117	3.24	4.62	7.15	6.29
Jan-06	3.347	4.45	4.35	4.35	4.2	4.3	3.7508	2.94	4.572	4.8126	4.9412	3.6742	3.36	4.69	7.26	6.3
Feb-06	3.604	4.68	4.64	4.57	4.41	4.51	3.8883	3	4.631	4.9907	5.1526	3.8367	3.46	4.88	7.5	6.31
Mar-06	3.624	4.77	4.74	4.72	4.51	4.61	4.0108	3.06	4.826	5.1196	5.2476	3.9958	3.56	5.01	7.53	6.43
Apr-06	3.759	4.9	4.89	4.9	4.59	4.72	4.1425	3.11	5.0245	5.2879	5.4217	4.1575	3.66	5.17	7.75	6.5
May-06	3.884	5	4.97	5	4.72	4.81	4.2817	3.21	5.1071	5.3215	5.4139	4.3183	3.79	5.25	7.93	6.61
Jun-06	4.09	5.16	5.09	5.07	4.79	4.95	4.4317	3.28	5.3451	5.6382	5.766	4.4825	3.94	5.46	8.02	6.65
Jul-06	4.177	5.22	5.07	5.04	4.96	5.09	4.5633	3.35	5.4045	5.5473	5.591	4.64	4.11	5.54	8.25	6.77
Aug-06	4.277	5.08	4.85	4.82	4.98	5	4.6642	3.45	5.3314	5.4501	5.4501	4.7742	4.34	5.44	8.25	6.75
Sep-06	4.382	4.97	4.69	4.67	4.85	4.91	4.7575	3.52	5.3229	5.3704	5.2985	4.8967	4.49	5.37	8.25	6.59
Oct-06	4.346	5.01	4.72	4.69	4.89	4.91	4.8267	3.57	5.3198	5.3898	5.3348	4.9967	4.6	5.35	8.25	6.54
Nov-06	4.358	5.01	4.64	4.58	4.96	4.96	4.8833	3.63	5.3479	5.3495	5.2439	5.0808	4.65	5.33	8.25	6.45
Dec-06	4.396	4.94	4.58	4.53	4.86	4.89	4.9325	3.67	5.3279	5.3651	5.3139	5.1533	4.69	5.31	8.25	6.4
Jan-07	4.392	5.06	4.79	4.75	4.96	4.94	4.9833	3.71	5.3201	5.4014	5.4414	5.2167	4.73	5.34	8.25	6.37
Feb-07	4.376	5.05	4.75	4.71	5.02	4.97	5.0142	3.71	5.3214	5.3723	5.3328	5.2658	4.75	5.34	8.25	6.4
Mar-07	4.299	4.92	4.51	4.48	4.97	4.9	5.0267	3.74	5.3195	5.3212	5.2009	5.3008	4.77	5.28	8.25	6.33
Apr-07	4.224	4.93	4.6	4.59	4.88	4.87	5.0292	3.76	5.3201	5.3581	5.2967	5.3242	4.79	5.31	8.25	6.28
May-07	4.293	4.91	4.69	4.67	4.77	4.8	5.0217	3.79	5.321	5.3844	5.3885	5.3375	4.76	5.32	8.25	6.37
Jun-07	4.283	4.96	5	5.03	4.66	4.78	5.005	3.79	5.3195	5.3817	5.4048	5.3358	4.76	5.36	8.25	6.58
Jul-07	4.277	4.96	4.82	4.88	4.84	4.86	4.9833	3.81	5.32	5.3768	5.3914	5.3242	4.78	5.34	8.25	6.74
Aug-07	4.359	4.47	4.34	4.43	4.34	4.56	4.9325	3.84	5.4801	5.3683	5.1847	5.3333	4.79	5.4	8.25	6.73
Sep-07	4.383	4.14	4.06	4.2	4.01	4.13	4.8633	3.85	5.5121	5.3655	5.0703	5.3433	4.79	5.33	8.03	6.59


From lbraglia at gmail.com  Sat Dec  8 18:37:40 2007
From: lbraglia at gmail.com (Luca Braglia)
Date: Sat, 8 Dec 2007 18:37:40 +0100
Subject: [R] [OT] A free (as in freedom) replacement for StatTransfer
In-Reply-To: <1197131355.2955.37.camel@Bellerophon.localdomain>
References: <20071208112446.GA2894@debian>
	<1197131355.2955.37.camel@Bellerophon.localdomain>
Message-ID: <20071208173739.GA2982@debian>

On 08/12/07 -  10:29, Marc Schwartz wrote:
> 
> On Sat, 2007-12-08 at 12:24 +0100, Luca Braglia wrote:
> > Hello *
> > 
> > does anybody know a free (as in freedom) equivalent for Stat
> > Transfer
> > 
> > http://stattransfer.com/
> > 
> > Thank you
> > 	Bye
> 
> The answer is: it depends.
> 
> If you are looking for a single free open source application that covers
> all of the formats that StatTransfer supports, then no.

It was this ... I was asking myself if the Foss movement had
already produced a similar, statistical-software-system
independent , tool.

Thanks

	Bye


From mxkuhn at gmail.com  Sat Dec  8 18:38:01 2007
From: mxkuhn at gmail.com (Max Kuhn)
Date: Sat, 8 Dec 2007 12:38:01 -0500
Subject: [R] lm: how to calculate rsquared of the predicted values?
In-Reply-To: <bc1b93170712080904k73f6b94ew696a105303bc8a56@mail.gmail.com>
References: <bc1b93170712080904k73f6b94ew696a105303bc8a56@mail.gmail.com>
Message-ID: <6731304c0712080938k33e0c3f1oabea0cb60b50fda3@mail.gmail.com>

>
> 1.  can I measure de R-squared value between the predicted(by the model) and
> real (observed) values.?
> 2. Measure the RMSE error .
>

There is a function in the caret package called postResample that will
do that (even if you aren't using resampling).

As has been previously noted on this list, there are a number of
formulas for R-squared. This function uses the square of the
correlation between the observed and predicted. The next version of
caret will offer a choice of formulas.

-- 

Max


From cberry at tajo.ucsd.edu  Sat Dec  8 18:38:44 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Sat, 8 Dec 2007 17:38:44 +0000 (UTC)
Subject: [R] Make natural splines constant outside boundary
References: <4758ABA1.3040708@ms.unimelb.edu.au>
Message-ID: <loom.20071208T173617-187@post.gmane.org>

Gad Abraham <g.abraham <at> ms.unimelb.edu.au> writes:

> 
> Hi,
> 
> I'm using natural cubic splines from splines::ns() in survival 
> regression (regressing inter-arrival times of patients to a queue on 
> queue size). The queue size fluctuates between 3600 and 3900.
> 
> I would like to be able to run predict.survreg() for sizes <3600 and 
>  >3900 by assuming that the rate for <3600 is the same as for 3600 and 
> that for >4000 it's the same as for 4000 (i.e., keep the splines cubic 
> within the boundaries but make them constant outside the boundaries).
> 

[snip]

> Any suggestions?

Here is one.

> range(ovarian$age)
[1] 38.8932 74.5041
> trim <- function(x) pmin(74.5041 ,pmax(38.8932 , x))
> s <- survreg(Surv(futime, fustat) ~ ns(age, knots=c(50,
60),Boundary.knots=c(38.8932, 74.5041)),data=ovarian)
> s2 <- survreg(Surv(futime, fustat) ~ ns(trim(age), knots=c(50,
60),Boundary.knots=c(38.8932, 74.5041)),data=ovarian)
> matplot(newage, cbind(predict(s,newdata=newage),predict(s2,newdata=newage)))
> 

HTH,

Chuck

> 
> Thanks,
> Gad
>


From pburns at pburns.seanet.com  Sat Dec  8 18:47:58 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 08 Dec 2007 17:47:58 +0000
Subject: [R] FW: R memory management
In-Reply-To: <000b01c83988$56b7ca00$04275e00$@ru>
References: <000b01c83988$56b7ca00$04275e00$@ru>
Message-ID: <475AD8CE.9090804@pburns.seanet.com>

The line:

  data. <- c(data., new.data)

will eat both memory and time voraciously.

You should change it by creating 'data.' to
be the final size it will be and then subscript
into it.  If you don't know the final size, then
you can grow it a lot a few times instead of
growing it a little lots of times.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Yuri Volchik wrote:

>Hi,
>
> 
>
>I'm using R to collect data for a number of exchanges through a socket
>connection and constantly running into memory problems even though task I
>believe is not that memory consuming. I guess there is a miscommunication
>between R and WinXP about freeing up memory.
>
>So this is the code:
>
> 
>
>for (x in 1:length(exchanges.to.get)) {
>
>   tickers<-sqlQuery(channel,paste("SELECT Symbol FROM symbols_list WHERE
>Exchange='",exchanges.to.get[x],"';",sep=''))[,1]
>
>   dir.create(paste(Working.dir,exchanges.to.get[x],'/',sep=''))
>
>   for (y in 1:length(tickers)) {
>
>     con2 <- socketConnection(Sys.info()["nodename"], port = ****)  #open
>socket connection to get data
>
>     writeLines(paste(command,',',tickers[y],',',interval,';',sep=''), con2)
>
>     data.<-readLines(con2)
>
>     end.of.data<-sum(c(data.=="!ENDMSG!",data.=="!SYNTAX_ERROR!"))
>
>     while(end.of.data!=1)
>{new.data<-readLines(con2);end.of.data<-sum(new.data=="!ENDMSG!");
>data.<-c(data.,new.data)}
>
>     if (length(data.)>3)
>write.table(data.[1:(length(data.)-2)],paste(Working.dir,exchanges.to.get[x]
>,'/',sub('\\*','\+',tickers[y]),'_.csv',sep=''),quote=F,col.names =
>F,row.names=F)
>
>     close(con2)
>
>   }
>
>  rm(tickers)
>
>  gc()
>
> 
>
> 
>
>With command  gcinfo(TRUE) I got the following info (some examples) :
>
> 
>
>Garbage collection 16362 = 15411+754+197 (level 0) ... 
>
>6.3 Mbytes of cons cells used (22%)
>
>2.2 Mbytes of vectors used (8%)
>
> 
>
>Garbage collection 16407 = 15454+756+197 (level 0) ... 
>
>13.1 Mbytes of cons cells used (46%)
>
>10.4 Mbytes of vectors used (39%)
>
> 
>
>Garbage collection 16410 = 15456+756+198 (level 2) ... 
>
>4.9 Mbytes of cons cells used (21%)
>
>0.9 Mbytes of vectors used (4%)
>
> 
>
>Garbage collection 16679 = 15634+796+249 (level 0) ... 
>
>150.7 Mbytes of cons cells used (95%)
>
>203.9 Mbytes of vectors used (75%)
>
> 
>
>Garbage collection 16680 = 15634+796+250 (level 2) ... 
>
>4.9 Mbytes of cons cells used (4%)
>
>0.9 Mbytes of vectors used (0%)
>
> 
>
>Garbage collection 16808 = 15754+802+252 (level 0) ... 
>
>6.1 Mbytes of cons cells used (7%)
>
>1.8 Mbytes of vectors used (1%)
>
> 
>
>But the end result is in Task Manager:
>
>RGui.exe  Mem Usage 470,472K  VM Size 541,988K
>
> 
>
>Even though R reports 
>
>Garbage collection 16808 = 15754+802+252 (level 0) ... 
>
>6.1 Mbytes of cons cells used (7%)
>
>1.8 Mbytes of vectors used (1%)
>
> 
>
>Has anybody encountered this problem and how you guys deal with it?  It
>seems like a memory leak to me, as tasks are not memory demandind, the
>biggest amount of data in a single file is about 40MB.
>
> 
>
>Thanks
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From diegol81 at gmail.com  Sat Dec  8 19:15:14 2007
From: diegol81 at gmail.com (diegol)
Date: Sat, 8 Dec 2007 10:15:14 -0800 (PST)
Subject: [R] MS Excel Data
In-Reply-To: <Pine.LNX.4.64.0711291401220.32238@gannet.stats.ox.ac.uk>
References: <597449.99873.qm@web53307.mail.re2.yahoo.com>
	<Pine.LNX.4.64.0711291401220.32238@gannet.stats.ox.ac.uk>
Message-ID: <14231028.post@talk.nabble.com>



Prof Brian Ripley wrote:
> 
> Why not read the 'R Data Import/Export' manual?  It ships with R, or can 
> be accessed from http://cran.r-project.org/manuals.html .
> 

Dear Professor,

I'd like to make a suggestion. 

When I first read some articles ands posts about R's wonderful capabilities,
the first thing I tried to do is import some data to which I had fitted
distributions in Excel, so I could compare the results with R. However,
after reading "R Data Import/Export" and some other references (including
the relevant parts of "Introduction to R"), I still found importing
excruciatingly difficult and frustrating, and I believe that to be true for
many people first approaching R. After a long while I did find the way
around it; however, some time later I came across the R Commander package
and GUI, which makes these simple tasks really simple.

My point is that such unexpected difficulties might deter beginners from
ever considering R again. I believe that including a reference to the R
Commander package in the "R Data Import/Export" document could be a step
forward in this respect. Once the user becomes more familiar with R, they
can spend some more time to learn programmed solutions to data importing.

Best regards.

-----
~~~~~~~~~~~~~~~~~~~~~~~~~~
Diego Mazzeo
Actuarial Science Student
Facultad de Ciencias Econ?micas
Universidad de Buenos Aires
Buenos Aires, Argentina
-- 
View this message in context: http://www.nabble.com/MS-Excel-Data-tf4897353.html#a14231028
Sent from the R help mailing list archive at Nabble.com.


From murdoch at stats.uwo.ca  Sat Dec  8 19:32:12 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 08 Dec 2007 13:32:12 -0500
Subject: [R] MS Excel Data
In-Reply-To: <14231028.post@talk.nabble.com>
References: <597449.99873.qm@web53307.mail.re2.yahoo.com>	<Pine.LNX.4.64.0711291401220.32238@gannet.stats.ox.ac.uk>
	<14231028.post@talk.nabble.com>
Message-ID: <475AE32C.4050904@stats.uwo.ca>

On 08/12/2007 1:15 PM, diegol wrote:
> 
> Prof Brian Ripley wrote:
>> Why not read the 'R Data Import/Export' manual?  It ships with R, or can 
>> be accessed from http://cran.r-project.org/manuals.html .
>>
> 
> Dear Professor,
> 
> I'd like to make a suggestion. 
> 
> When I first read some articles ands posts about R's wonderful capabilities,
> the first thing I tried to do is import some data to which I had fitted
> distributions in Excel, so I could compare the results with R. However,
> after reading "R Data Import/Export" and some other references (including
> the relevant parts of "Introduction to R"), I still found importing
> excruciatingly difficult and frustrating, and I believe that to be true for
> many people first approaching R. After a long while I did find the way
> around it; however, some time later I came across the R Commander package
> and GUI, which makes these simple tasks really simple.
> 
> My point is that such unexpected difficulties might deter beginners from
> ever considering R again. I believe that including a reference to the R
> Commander package in the "R Data Import/Export" document could be a step
> forward in this respect. Once the user becomes more familiar with R, they
> can spend some more time to learn programmed solutions to data importing.

Why not write one?  The source to that manual is available at 
https://svn.r-project.org/R/trunk/doc/manual/R-data.texi (in texinfo 
format).  If  you don't know how to handle the format, don't worry:  for 
a small change, you can just treat it like plain text.

If you email me a suggested patch I'll see about updating the manual.

Duncan Murdoch


From gufrgs at gmail.com  Sat Dec  8 20:07:27 2007
From: gufrgs at gmail.com (Giovane)
Date: Sat, 8 Dec 2007 17:07:27 -0200
Subject: [R] lm: how to calculate rsquared of the predicted values?
In-Reply-To: <6731304c0712080938k33e0c3f1oabea0cb60b50fda3@mail.gmail.com>
References: <bc1b93170712080904k73f6b94ew696a105303bc8a56@mail.gmail.com>
	<6731304c0712080938k33e0c3f1oabea0cb60b50fda3@mail.gmail.com>
Message-ID: <bc1b93170712081107h1c68e5a7ja825d6a482b4e903@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071208/c786686a/attachment.pl 

From cberry at tajo.ucsd.edu  Sat Dec  8 20:13:13 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Sat, 8 Dec 2007 11:13:13 -0800
Subject: [R] Make natural splines constant outside boundary
In-Reply-To: <loom.20071208T173617-187@post.gmane.org>
References: <4758ABA1.3040708@ms.unimelb.edu.au>
	<loom.20071208T173617-187@post.gmane.org>
Message-ID: <Pine.LNX.4.64.0712081110400.23748@tajo.ucsd.edu>

On Sat, 8 Dec 2007, Charles C. Berry wrote:

> Gad Abraham <g.abraham <at> ms.unimelb.edu.au> writes:
>
>>
>> Hi,
>>
>> I'm using natural cubic splines from splines::ns() in survival
>> regression (regressing inter-arrival times of patients to a queue on
>> queue size). The queue size fluctuates between 3600 and 3900.
>>
>> I would like to be able to run predict.survreg() for sizes <3600 and
>> >3900 by assuming that the rate for <3600 is the same as for 3600 and
>> that for >4000 it's the same as for 4000 (i.e., keep the splines cubic
>> within the boundaries but make them constant outside the boundaries).
>>
>
> [snip]
>
>> Any suggestions?
>
> Here is one.
>
>> range(ovarian$age)
> [1] 38.8932 74.5041
>> trim <- function(x) pmin(74.5041 ,pmax(38.8932 , x))
>> s <- survreg(Surv(futime, fustat) ~ ns(age, knots=c(50,
> 60),Boundary.knots=c(38.8932, 74.5041)),data=ovarian)
>> s2 <- survreg(Surv(futime, fustat) ~ ns(trim(age), knots=c(50,
> 60),Boundary.knots=c(38.8932, 74.5041)),data=ovarian)

Should have copy-and-pasted this here:

newage <- data.frame( age=seq(10,200,10 ) )

>> matplot(newage, cbind(predict(s,newdata=newage),predict(s2,newdata=newage)))

Sorry 'bout the line wraps and bad formatting.

Chuck

--
Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From bolker at ufl.edu  Sat Dec  8 22:25:18 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Sat, 08 Dec 2007 16:25:18 -0500
Subject: [R] OT: 3d surfaces with transparency
Message-ID: <475B0BBE.2020009@ufl.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


  I would be grateful if anyone had suggestions
about software that could (1) create 3D surface
plots, (2) handle transparency/alpha blending,
(3) generate output in some vector graphics format
that preserved the transparency.  I could also
live with a combination of two programs, one
to generate the basic figure and another to
modify the output surface to a transparent
color (but preserving vector-ness).

  I've been working with the rgl package, but
can't get the transparency to work for vector
output (rgl.postscript).  The rgl package uses
the gl2ps library -- by default transparency
is disabled in the output (GL2PS_NO_BLENDING
is set), giving reasonable PDF output but
without transparency.  Enabling it in the
source code gives ugly results.

  I ported my graphics to Mathematica, but its
PDF output (and SVG output) are both wonky.

  Any ideas???

  Ben Bolker

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFHWwu+c5UpGjwzenMRAktnAJwN5pYUIL7xAzwONg/lnS7YyoN1PgCgg+Sm
p3Hc69pxR2ZhT5BsBO794ZA=
=HJIS
-----END PGP SIGNATURE-----


From pedrosmarques at portugalmail.pt  Sat Dec  8 22:51:05 2007
From: pedrosmarques at portugalmail.pt (pedrosmarques at portugalmail.pt)
Date: Sat,  8 Dec 2007 21:51:05 +0000
Subject: [R] time series tests
Message-ID: <1197150665.475b11c97fe1c@gold4.portugalmail.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071208/61e7da38/attachment.pl 

From ima at difres.dk  Sat Dec  8 22:51:03 2007
From: ima at difres.dk (Irene Mantzouni)
Date: Sat, 8 Dec 2007 22:51:03 +0100
Subject: [R] level significance
Message-ID: <8A96F09B52875349A6B34475EDCCC277044AD3@lu-mail-san.dfu.local>

Hi all!
 
I am fitting a (mixed) model with a factor (F) and continuous response and predictor:
y~F+F:x
 
(How) can I check the significance of the model at each factor level (i.e. the model could be significant only at one of the levels)?
 
Thank you!


From pedrosmarques at portugalmail.pt  Sat Dec  8 23:00:15 2007
From: pedrosmarques at portugalmail.pt (pedrosmarques at portugalmail.pt)
Date: Sat,  8 Dec 2007 22:00:15 +0000
Subject: [R]  time series tests
In-Reply-To: <1197150665.475b11c97fe1c@gold4.portugalmail.pt>
References: <1197150665.475b11c97fe1c@gold4.portugalmail.pt>
Message-ID: <1197151215.475b13efaf561@gold4.portugalmail.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071208/d1b5058f/attachment.pl 

From deepayan.sarkar at gmail.com  Sat Dec  8 23:03:38 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 8 Dec 2007 14:03:38 -0800
Subject: [R] OT: 3d surfaces with transparency
In-Reply-To: <475B0BBE.2020009@ufl.edu>
References: <475B0BBE.2020009@ufl.edu>
Message-ID: <eb555e660712081403h219b53f0vb6911bbea1f80caf@mail.gmail.com>

On 12/8/07, Ben Bolker <bolker at ufl.edu> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
>   I would be grateful if anyone had suggestions
> about software that could (1) create 3D surface
> plots, (2) handle transparency/alpha blending,
> (3) generate output in some vector graphics format
> that preserved the transparency.  I could also
> live with a combination of two programs, one
> to generate the basic figure and another to
> modify the output surface to a transparent
> color (but preserving vector-ness).
>
>   I've been working with the rgl package, but
> can't get the transparency to work for vector
> output (rgl.postscript).  The rgl package uses
> the gl2ps library -- by default transparency
> is disabled in the output (GL2PS_NO_BLENDING
> is set), giving reasonable PDF output but
> without transparency.  Enabling it in the
> source code gives ugly results.
>
>   I ported my graphics to Mathematica, but its
> PDF output (and SVG output) are both wonky.
>
>   Any ideas???

You could consider wireframe from lattice, but this has many caveats.
For an example, see

http://dsarkar.fhcrc.org/lattice/book/figures.html?chapter=06;figure=06_18;theme=stdColor;code=right

For that matter, whats wrong with persp?

-Deepayan


From bolker at ufl.edu  Sat Dec  8 23:55:19 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Sat, 08 Dec 2007 17:55:19 -0500
Subject: [R] OT: 3d surfaces with transparency
In-Reply-To: <eb555e660712081403h219b53f0vb6911bbea1f80caf@mail.gmail.com>
References: <475B0BBE.2020009@ufl.edu>
	<eb555e660712081403h219b53f0vb6911bbea1f80caf@mail.gmail.com>
Message-ID: <475B20D7.5090504@ufl.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Deepayan Sarkar wrote:
> On 12/8/07, Ben Bolker <bolker at ufl.edu> wrote:
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>>
>>   I would be grateful if anyone had suggestions
>> about software that could (1) create 3D surface
>> plots, (2) handle transparency/alpha blending,
>> (3) generate output in some vector graphics format
>> that preserved the transparency.  I could also
>> live with a combination of two programs, one
>> to generate the basic figure and another to
>> modify the output surface to a transparent
>> color (but preserving vector-ness).
>>
 [snip]
>>   Any ideas???
> 
> You could consider wireframe from lattice, but this has many caveats.
> For an example, see
> 
> http://dsarkar.fhcrc.org/lattice/book/figures.html?chapter=06;figure=06_18;theme=stdColor;code=right
> 
> For that matter, whats wrong with persp?
> 
> -Deepayan


  I hadn't thought about the fact that transparency is easier
than it used to be (esp. with cairo device/PDF).
  OK, next question:  is there an easier way than the following
to create a 3D perspective plot with reference grids on some faces?
I  can pull the 3D grid code out of the rgl or scatterplot3d
packages and reimplement it here, I guess ...  another way to
hack this might (?) be to play with tick lengths?

library(cairoDevice)
xgrid = seq(0,1,by=0.1)
ygrid = seq(0,1,by=0.1)
zgrid = seq(0,200,by=20)
zmin = min(zgrid)
zmax = max(zgrid)
ymin = min(ygrid)
ymax = max(ygrid)
xmin = min(xgrid)
xmax = max(xgrid)

p1 = persp(0:1,0:1,matrix(0,ncol=2,nrow=2),axes=FALSE,
     xlab="",ylab="",zlab="",zlim=c(0,200))
invisible(lapply(xgrid,
       function(x) {
         t1 = trans3d(x,ymin,zmin,p1)
         t2 = trans3d(x,ymax,zmin,p1)
         segments(t1$x,t1$y,t2$x,t2$y,col="gray")
       }))
invisible(lapply(ygrid,
       function(y) {
         t1 = trans3d(xmin,y,zmin,p1)
         t2 = trans3d(xmax,y,zmin,p1)
         segments(t1$x,t1$y,t2$x,t2$y,col="gray")
       }))
invisible(lapply(xgrid,
       function(x) {
         t1 = trans3d(x,ymax,zmin,p1)
         t2 = trans3d(x,ymax,zmax,p1)
         segments(t1$x,t1$y,t2$x,t2$y,col="gray")
       }))
invisible(lapply(zgrid,
       function(z) {
         t1 = trans3d(xmin,ymax,z,p1)
         t2 = trans3d(xmax,ymax,z,p1)
         segments(t1$x,t1$y,t2$x,t2$y,col="gray")
       }))
par(new=TRUE)
p2 = persp(volcano,
  col=rgb(1,0,0,0.5),border=NA)

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFHWyDXc5UpGjwzenMRAvYVAJ9NTAjJ6wiTJlaFl2ewj74KXtruBwCgldTe
FufIEDizL9FA7Uk8LG8e/tY=
=HiWO
-----END PGP SIGNATURE-----


From milton_ruser at yahoo.com.br  Sun Dec  9 00:38:30 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sat, 8 Dec 2007 15:38:30 -0800 (PST)
Subject: [R] spliting windows 5 frames.
Message-ID: <423827.76595.qm@web56004.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071208/e7624e70/attachment.pl 

From jholtman at gmail.com  Sun Dec  9 00:53:43 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 8 Dec 2007 15:53:43 -0800
Subject: [R] spliting windows 5 frames.
In-Reply-To: <423827.76595.qm@web56004.mail.re3.yahoo.com>
References: <423827.76595.qm@web56004.mail.re3.yahoo.com>
Message-ID: <644e1f320712081553rd94277aofde9f4483615dd8@mail.gmail.com>

?layout

try:

layout(rbind(c(1, 2, 3),
             c(1, 4, 5)), widths=c(2, 1, 1))
layout.show(5)


and see if this is what you want.

On Dec 8, 2007 3:38 PM, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
> Dear all,
>
> I need split the output windows into two "frames" of equal size.
> On first frame I will print only a image, and on second frame I will print out for graphs (like those automaticaly generated by plot of a glm model.
> Below follow a visual which I looking for.
>
>
> x11(width=10,height=8)
> oldpar<-par()
> par(xaxt="n",yaxt="n")
> plot(c(0,10),c(0,10),type="n",xlab="",ylab="",col=0)
> polygon(c(0,4.8,4.8,0,0),c(1,1,9,9,1))
> polygon(c(10,5.2,5.2,10,10),c(1,1,9,9,1))
> polygon(c(5.5,7.4,7.4,5.5,5.5),c(5.5,5.5,8.3,8.3,5.5))
> polygon(c(5.5,7.4,7.4,5.5,5.5),c(2,2,5,5,2))
> polygon(c(7.7,9.7,9.7,7.7,7.7),c(5.5,5.5,8.3,8.3,5.5))
> polygon(c(7.7,9.7,9.7,7.7,7.7),c(2,2,5,5,2))
> par<-(oldpar)
>
>
> Thanks
>
> miltinho
> Brazil
>
>
>
>  para armazenamento!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From juryef at yahoo.com  Sun Dec  9 01:23:09 2007
From: juryef at yahoo.com (Judith Flores)
Date: Sat, 8 Dec 2007 16:23:09 -0800 (PST)
Subject: [R] Adding text outside lattice plot
Message-ID: <653609.13045.qm@web34709.mail.mud.yahoo.com>

Hello,

   I need to add some text in the upper left position,
outside a lattice plot. 

xyplot(x~y)
ltext(locator(1), label='My text')

doesn't work. 


I would appreciate any help. Tahnk you,

Judith


      ____________________________________________________________________________________
Be a better friend, newshound, and


From maura.monville at gmail.com  Sun Dec  9 01:42:54 2007
From: maura.monville at gmail.com (Maura E Monville)
Date: Sat, 8 Dec 2007 18:42:54 -0600
Subject: [R] package "growth" ... where is it ?
In-Reply-To: <36d691950712081521v4525b741pbb88ef561a8bed90@mail.gmail.com>
References: <36d691950712081521v4525b741pbb88ef561a8bed90@mail.gmail.com>
Message-ID: <36d691950712081642k738bd184o172a39d7ef2b179a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071208/49f868af/attachment.pl 

From deepayan.sarkar at gmail.com  Sun Dec  9 01:54:31 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 8 Dec 2007 16:54:31 -0800
Subject: [R] Adding text outside lattice plot
In-Reply-To: <653609.13045.qm@web34709.mail.mud.yahoo.com>
References: <653609.13045.qm@web34709.mail.mud.yahoo.com>
Message-ID: <eb555e660712081654u1164fee5w1b8be8d92f21635a@mail.gmail.com>

On 12/8/07, Judith Flores <juryef at yahoo.com> wrote:
> Hello,
>
>    I need to add some text in the upper left position,
> outside a lattice plot.
>
> xyplot(x~y)
> ltext(locator(1), label='My text')
>
> doesn't work.

library(grid)
ltext(grid.locator(), label='My text')

should.

-Deepayan


From news at jonasstein.de  Sun Dec  9 02:04:14 2007
From: news at jonasstein.de (Jonas Stein)
Date: Sun, 9 Dec 2007 01:04:14 +0000 (UTC)
Subject: [R] R + LaTeX formula
Message-ID: <fjfeue$e75$1@ger.gmane.org>

Hi,

what is actually the best method to include R-plots into LaTeX documents?
At the moment i use 

postscript("myplot.eps", width = 12.0, height = 9.0, horizontal = FALSE, 
onefile = TRUE, paper = "special",encoding = "TeXtext.enc")
plot(foo,bar)
dev.off()  

But it is a bit unhandy to scale later and its difficult to get nice 
formula in the plots.

And how should i write formulas on the axis or at specific points? 
Has someone had some effort in exporting plots to pstricks or pictex?

kind regards and thank you for reading so far,


-- 
Jonas Stein <news at jonasstein.de>


From marc_schwartz at comcast.net  Sun Dec  9 02:12:45 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 08 Dec 2007 19:12:45 -0600
Subject: [R] package "growth" ... where is it ?
In-Reply-To: <36d691950712081642k738bd184o172a39d7ef2b179a@mail.gmail.com>
References: <36d691950712081521v4525b741pbb88ef561a8bed90@mail.gmail.com>
	<36d691950712081642k738bd184o172a39d7ef2b179a@mail.gmail.com>
Message-ID: <1197162765.2955.68.camel@Bellerophon.localdomain>

On Sat, 2007-12-08 at 18:42 -0600, Maura E Monville wrote:
> I would like to install the package "growth" as it contains the function
> "corgram" and some other presumably useful stuff for time series analysis.
> I can see it is in R standard library list:
> http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/doc/html/packages.html<http://hosho.ees.hokudai.ac.jp/%7Ekubo/Rdoc/doc/html/packages.html>
> http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/growth/html/00Index.html
> <http://hosho.ees.hokudai.ac.jp/%7Ekubo/Rdoc/library/growth/html/00Index.html>
> 
> But the command install.packages("growth") as well as help.serach("growth")
> cannot find it,.
> Presumably it is not found in the official R repository.
> Does anyone know where to download it from ?
> 
> Thank you very much.

Maura,

It looks like growth is one of Jim Lindsey's packages here:

  http://popgen.unimaas.nl/~jlindsey/rcode.html

HTH,

Marc Schwartz


From marc_schwartz at comcast.net  Sun Dec  9 02:24:49 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 08 Dec 2007 19:24:49 -0600
Subject: [R] R + LaTeX formula
In-Reply-To: <fjfeue$e75$1@ger.gmane.org>
References: <fjfeue$e75$1@ger.gmane.org>
Message-ID: <1197163489.2955.80.camel@Bellerophon.localdomain>


On Sun, 2007-12-09 at 01:04 +0000, Jonas Stein wrote:
> Hi,
> 
> what is actually the best method to include R-plots into LaTeX
> documents?
> At the moment i use 
> 
> postscript("myplot.eps", width = 12.0, height = 9.0, horizontal =
> FALSE, 
> onefile = TRUE, paper = "special",encoding = "TeXtext.enc")
> plot(foo,bar)
> dev.off()  
> 
> But it is a bit unhandy to scale later and its difficult to get nice 
> formula in the plots.
> 
> And how should i write formulas on the axis or at specific points? 
> Has someone had some effort in exporting plots to pstricks or pictex?
> 
> kind regards and thank you for reading so far,

As per the Details section of ?postscript:

The postscript produced for a single R plot is EPS (Encapsulated
PostScript) compatible, and can be included into other documents, e.g.,
into LaTeX, using \includegraphics{<filename>}. For use in this way you
will probably want to set horizontal = FALSE, onefile = FALSE, paper =
"special". Note that the bounding box is for the device region: if you
find the white space around the plot region excessive, reduce the
margins of the figure region via par(mar=).


In your code above, change 'onefile = TRUE' to 'onefile = FALSE'.

For scaling you can use the LaTeX \includegraphics directive along with
several height and width arguments, such as:

  \includegraphics[width=4in]{myplot.eps}
  \includegraphics[height=4in]{myplot.eps}
  \includegraphics[scale=0.75]{myplot.eps}
  \includegraphics[width=0.4\textwidth]{myplot.eps}

You might want to review the following document:

  ftp://ftp.tex.ac.uk/tex-archive/info/epslatex.pdf 

For including formulae in R plots, see ?plotmath. You can run
example(plotmath) and there are many posts in the r-help archives on
this.

Beyond the above, you may want to look into using SWeave, whereby you
can create entire documents, with nicely formatted tables and plots
directly from R code. More information here:

  http://www.ci.tuwien.ac.at/~leisch/Sweave/

There are also a couple of articles on R News:

Friedrich Leisch. Sweave, part I: Mixing R and LATEX. R News,
2(3):28-31, December 2002.

Friedrich Leisch. Sweave, part II: Package vignettes. R News,
3(2):21-24, October 2003.

HTH,

Marc Schwartz


From tchur at optushome.com.au  Sun Dec  9 02:26:31 2007
From: tchur at optushome.com.au (Tim Churches)
Date: Sun, 09 Dec 2007 12:26:31 +1100
Subject: [R] [OT] A free (as in freedom) replacement for StatTransfer
In-Reply-To: <1197131355.2955.37.camel@Bellerophon.localdomain>
References: <20071208112446.GA2894@debian>
	<1197131355.2955.37.camel@Bellerophon.localdomain>
Message-ID: <475B4447.6060008@optushome.com.au>

Marc Schwartz wrote:
> Perhaps the most notable format that is lacking is the SAS proprietary
> format (not the Transport format), which is not openly published and to
> my knowledge, has not been independently reverse engineered.

The SAS proprietary dataset and format catalogue structures were
successfully reverse engineered by a small software firm called
Conceptual and were made available in a product called DBMS/Copy.
DBMS/Copy is/was similar to StatTransfer, but by 2002 was going a lot
further by adding support for much of the SAS data step syntax and some
core SAS procedures as well - in other words, it was rapidly becoming a
very viable and quite good pop person's SAS (with a modest one-off
license fee). However, the SAS Institute bought out the privately-held
Conceptual company, and now sells DBMS/Copy thhrough its wholly-owned
daat integration offshoot company, DataFlux, but without the SAS
datastep support features (to avoid competition with the mainstream SAS
cash cows) - see http://www.conceptual.com/

> Any of the commercial products that support that format, with the
> possible exception of the SAS System Viewer (which is not open source,
> but is free from SAS), will be closed source and will have to be
> purchased.

DBMS/Copy is definitely closed-source and is probably not nearly as
cheapl as it once was when sold by Conceptual. But it is a great product
for convert to or from SAS proprietary data sets and format catalogues,
and works well and quickly with even huge datasets.

Tim C


From marc_schwartz at comcast.net  Sun Dec  9 03:25:09 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 08 Dec 2007 20:25:09 -0600
Subject: [R] [OT] A free (as in freedom) replacement for StatTransfer
In-Reply-To: <475B4447.6060008@optushome.com.au>
References: <20071208112446.GA2894@debian>
	<1197131355.2955.37.camel@Bellerophon.localdomain>
	<475B4447.6060008@optushome.com.au>
Message-ID: <1197167109.2955.114.camel@Bellerophon.localdomain>


On Sun, 2007-12-09 at 12:26 +1100, Tim Churches wrote:
> Marc Schwartz wrote:
> > Perhaps the most notable format that is lacking is the SAS proprietary
> > format (not the Transport format), which is not openly published and to
> > my knowledge, has not been independently reverse engineered.
> 
> The SAS proprietary dataset and format catalogue structures were
> successfully reverse engineered by a small software firm called
> Conceptual and were made available in a product called DBMS/Copy.
> DBMS/Copy is/was similar to StatTransfer, but by 2002 was going a lot
> further by adding support for much of the SAS data step syntax and some
> core SAS procedures as well - in other words, it was rapidly becoming a
> very viable and quite good pop person's SAS (with a modest one-off
> license fee). However, the SAS Institute bought out the privately-held
> Conceptual company, and now sells DBMS/Copy thhrough its wholly-owned
> daat integration offshoot company, DataFlux, but without the SAS
> datastep support features (to avoid competition with the mainstream SAS
> cash cows) - see http://www.conceptual.com/
> 
> > Any of the commercial products that support that format, with the
> > possible exception of the SAS System Viewer (which is not open source,
> > but is free from SAS), will be closed source and will have to be
> > purchased.
> 
> DBMS/Copy is definitely closed-source and is probably not nearly as
> cheapl as it once was when sold by Conceptual. But it is a great product
> for convert to or from SAS proprietary data sets and format catalogues,
> and works well and quickly with even huge datasets.

I am familiar with DBMS/Copy as my company uses it. Single user Windows
licenses are not too bad (~$500 U.S.), but going beyond that gets
expensive quickly.

I was aware of the past corporate history of Conceptual and it's
transition to DataFlux/SAS, but not that the original developer (who's
name escapes me at the moment) had reverse engineered the SAS format. He
is a SAS employee now, working on other projects and the rumors
percolate out of Tech Support in Cary every now and then that DBMS/Copy
will be EOL'd. There have not been any updates/patches for version 8
since January of this year.

The product is problematic however, in that it will both open and write
SAS datasets that in actuality may not be readable by SAS itself. A
significant problem that caused us to actually have to purchase SAS
earlier this year to be able to validate aspects of a client data
transfer process and we don't use SAS for anything else.

Indeed, there is an inconsistency in the behavior of SAS, DBMS/Copy and
the SAS System Viewer when reading SAS datasets. Given that all three
are now under the purview of SAS, it is in some respects surprising and
in others, not so much.

Unfortunately, SAS is now bundling BASE, STAT and GRAPH as the entry
level offering, so one cannot just get BASE as a stand-alone product any
longer, which is all we needed. This makes the investment even more
expensive for those of us who have to purchase at full commercial
pricing.

Regards,

Marc


From hb at stat.berkeley.edu  Sun Dec  9 04:05:32 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Sat, 8 Dec 2007 19:05:32 -0800
Subject: [R] lm: how to calculate rsquared of the predicted values?
In-Reply-To: <bc1b93170712081107h1c68e5a7ja825d6a482b4e903@mail.gmail.com>
References: <bc1b93170712080904k73f6b94ew696a105303bc8a56@mail.gmail.com>
	<6731304c0712080938k33e0c3f1oabea0cb60b50fda3@mail.gmail.com>
	<bc1b93170712081107h1c68e5a7ja825d6a482b4e903@mail.gmail.com>
Message-ID: <59d7961d0712081905p199b1333jb02bb34bccb643cf@mail.gmail.com>

Did you try to install using install.packages()?

> install.packages("caret")
Warning in install.packages("caret") :
  argument 'lib' is missing: using '/scratch5/hb/R/R_LIBS/linux/library/'

trying URL 'http://cran.cnr.Berkeley.edu/src/contrib/caret_3.08.tar.gz'
Content type 'application/x-gzip' length 1428629 bytes (1.4 Mb)
opened URL
==================================================
downloaded 1.4 Mb

/server/scratch5/hb/R/R_LIBS/linux/library
* Installing *source* package 'caret' ...
** libs
gcc -std=gnu99 -I/usr/local/linux/R-2.6.1/include
-I/usr/local/linux/R-2.6.1/include  -I/usr/local/include    -fpic  -g
-O2 -c caret.c -o caret.o
gcc -std=gnu99 -shared -L/usr/local/lib64 -o caret.so caret.o
-L/usr/local/linux/R-2.6.1/lib -lR
** R
** data
** inst
** preparing package for lazy loading
Loading required package: lattice
** help
 >>> Building/Updating help pages for package 'caret'
     Formats: text html latex example

  BloodBrain                        text    html    latex
  applyProcessing                   text    html    latex   example
[snip]

The downloaded packages are in
        /tmp/Rtmpwxs3CH/downloaded_packages
> library(caret)
Loading required package: lattice
>

/Henrik


On 08/12/2007, Giovane <gufrgs at gmail.com> wrote:
> Thank you Max Kuhn,
>
> So I decided to install caret  (R version 2.6.0, gcc 4.1.2, caret 3.08,
> slackware 12 and kernel 2.6.17.11), and I've got a problem:
> R  CMD INSTALL caret
> * Installing to library '/usr/local/lib/R/library'
> * Installing *source* package 'caret' ...
> ** libs
> //(it goes with no problem..)
>
> //here comes the bug
>
> ** building package indices ...
> Error in load(zfile, envir = envir) : error reading from connection
> Calls: <Anonymous> ... .build_data_index -> list_data_in_pkg -> <Anonymous>
> -> switch -> load
> Execution halted
> ERROR: installing package indices failed
> ** Removing '/usr/local/lib/R/library/caret'
>
> Any ideas?
> On Dec 8, 2007 3:38 PM, Max Kuhn <mxkuhn at gmail.com> wrote:
>
> > >
> > > 1.  can I measure de R-squared value between the predicted(by the model)
> > and
> > > real (observed) values.?
> > > 2. Measure the RMSE error .
> > >
> >
> > There is a function in the caret package called postResample that will
> > do that (even if you aren't using resampling).
> >
> > As has been previously noted on this list, there are a number of
> > formulas for R-squared. This function uses the square of the
> > correlation between the observed and predicted. The next version of
> > caret will offer a choice of formulas.
> >
> > --
> >
> > Max
> >
>
>
>
> --
> Best regards,
>
> Giovane
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From milton_ruser at yahoo.com.br  Sun Dec  9 04:26:09 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sat, 8 Dec 2007 19:26:09 -0800 (PST)
Subject: [R] adjusting "levels" after subset a table
Message-ID: <732855.47176.qm@web56004.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071208/5409f1a7/attachment.pl 

From marc_schwartz at comcast.net  Sun Dec  9 04:48:49 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 08 Dec 2007 21:48:49 -0600
Subject: [R] adjusting "levels" after subset a table
In-Reply-To: <732855.47176.qm@web56004.mail.re3.yahoo.com>
References: <732855.47176.qm@web56004.mail.re3.yahoo.com>
Message-ID: <1197172129.20795.7.camel@Bellerophon.localdomain>


On Sat, 2007-12-08 at 19:26 -0800, Milton Cezar Ribeiro wrote:
> Dear all,
> 
> I have a data.frame with a factor collumn with about 10 levels.
> After extract a subset of this data.frame, by selecting 2 of my 10
> levels, the new data.frame continue with original number of levels.
> How can I adjust it in a manner that when I try levels(my.df) I
> receive the actualyzed number of levels?
> 
> By the way, I read my file using reab.table.
> 
> I tryed solve it with :  levels(my.df$my.var)<-unique(my.df$my.var)
> but the problem remain.
> 
> Many thanks,
> 
> miltinho
> Brazil

The default when subsetting factors (which happens when you subset a
data frame) is to retain the original set of levels, even if they don't
occur in the resultant subset. This is described in ?"[.factor" where
the 'drop' argument is FALSE by default.

To subset the factor and only retain levels for those values that are
still present, you can use:

  MyFactor <- factor(MyFactor)

or

  MyFactor <- MyFactor[, drop = TRUE]

after subsetting the data frame.

There is also a page in the R Wiki that describes some additional
approaches:

http://wiki.r-project.org/rwiki/doku.php?id=tips:data-manip:drop_unused_levels

HTH,

Marc Schwartz


From paulojus at c3sl.ufpr.br  Sun Dec  9 04:52:18 2007
From: paulojus at c3sl.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Sun, 9 Dec 2007 01:52:18 -0200 (BRST)
Subject: [R] adjusting "levels" after subset a table
In-Reply-To: <732855.47176.qm@web56004.mail.re3.yahoo.com>
References: <732855.47176.qm@web56004.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.58.0712090151020.7890@bowmore.c3sl.ufpr.br>

running
my.df$my.var <- factor(my.df$my.var)

seems to do the job

P.J.

On Sat, 8 Dec 2007, Milton Cezar Ribeiro wrote:

> Dear all,
>
> I have a data.frame with a factor collumn with about 10 levels.
> After extract a subset of this data.frame, by selecting 2 of my 10 levels, the new data.frame continue with original number of levels. How can I adjust it in a manner that when I try levels(my.df) I receive the actualyzed number of levels?
>
> By the way, I read my file using reab.table.
>
> I tryed solve it with :  levels(my.df$my.var)<-unique(my.df$my.var) but the problem remain.
>
> Many thanks,
>
> miltinho
> Brazil
>
>
>
>  para armazenamento!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
Paulo Justiniano Ribeiro Jr
LEG (Laboratorio de Estatistica e Geoinformacao)
Universidade Federal do Parana
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus AT  ufpr  br
http://www.leg.ufpr.br/~paulojus

-------------------------------------------------------------------------
53a Reuniao Anual da Regiao Brasileira da Soc. Internacional de Biometria
14 a 16/05/2008, UFLA, Lavras,MG
http://www.centenario.ufla.br/


From maj at stats.waikato.ac.nz  Sun Dec  9 06:28:53 2007
From: maj at stats.waikato.ac.nz (maj at stats.waikato.ac.nz)
Date: Sun, 9 Dec 2007 18:28:53 +1300 (NZDT)
Subject: [R] Large determinant problem
Message-ID: <49552.203.109.170.45.1197178133.squirrel@webmail.scms.waikato.ac.nz>

I thought I would have another try at explaining my problem. I think that
last time I may have buried it in irrelevant detail.

This output should explain my dilemma:

> dim(S)
[1] 1455  269
> summary(as.vector(S))
      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
-1.160e+04  0.000e+00  0.000e+00 -4.132e-08  0.000e+00  8.636e+03
> sum(as.vector(S)==0)/(1455*269)
[1] 0.8451794
# S is a large moderately sparse matrix with some large elements
> SS <- crossprod(S,S)
> (eigen(SS,only.values = TRUE)$values)[250:269]
 [1]  9.264883e+04  5.819672e+04  5.695073e+04  1.948626e+04  1.500891e+04
 [6]  1.177034e+04  9.696327e+03  8.037049e+03  7.134058e+03  1.316449e-07
[11]  9.077244e-08  6.417276e-08  5.046411e-08  1.998775e-08 -1.268081e-09
[16] -3.140881e-08 -4.478184e-08 -5.370730e-08 -8.507492e-08 -9.496699e-08
# S'S fails to be non-negative definite.

I can't show you how to produce S easily but below I attempt at a
reproducible version of the problem:

> set.seed(091207)
> X <- runif(1455*269,-1e4,1e4)
> p <- rbinom(1455*269,1,0.845)
> Y <- p*X
> dim(Y) <- c(1455,269)
> YY <- crossprod(Y,Y)
> (eigen(YY,only.values = TRUE)$values)[250:269]
 [1] 17951634238 17928076223 17725528630 17647734206 17218470634 16947982383
 [7] 16728385887 16569501198 16498812174 16211312750 16127786747 16006841514
[13] 15641955527 15472400630 15433931889 15083894866 14794357643 14586969350
[19] 14297854542 13986819627
# No sign of negative eigenvalues; phenomenon must be due
# to special structure of S.
# S is a matrix of empirical parameter scores at an approximate
# mle for a model with 269 paramters fitted to 1455 observations.
# Thus, for example, its column sums are approximately zero:
> summary(apply(S,2,sum))
      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
-1.148e-03 -2.227e-04 -7.496e-06 -6.011e-05  7.967e-05  8.254e-04

I'm starting to think that it may not be a good idea to attempt to compute
large information matrices and their determinants!

Murray Jorgensen


From ripley at stats.ox.ac.uk  Sun Dec  9 07:43:43 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 9 Dec 2007 06:43:43 +0000 (GMT)
Subject: [R] Large determinant problem
In-Reply-To: <49552.203.109.170.45.1197178133.squirrel@webmail.scms.waikato.ac.nz>
References: <49552.203.109.170.45.1197178133.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <Pine.LNX.4.64.0712090628420.31005@gannet.stats.ox.ac.uk>

Hmm, S'S is numerically singular.  crossprod(S) would be a better way to 
compute it than crossprod(S,S) (it does use a different algorithm), but 
look at the singular values of S, which I suspect will show that S is 
numerically singular.

Looks like the answer is 0.


On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:

> I thought I would have another try at explaining my problem. I think that
> last time I may have buried it in irrelevant detail.
>
> This output should explain my dilemma:
>
>> dim(S)
> [1] 1455  269
>> summary(as.vector(S))
>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
> -1.160e+04  0.000e+00  0.000e+00 -4.132e-08  0.000e+00  8.636e+03
>> sum(as.vector(S)==0)/(1455*269)
> [1] 0.8451794
> # S is a large moderately sparse matrix with some large elements
>> SS <- crossprod(S,S)
>> (eigen(SS,only.values = TRUE)$values)[250:269]
> [1]  9.264883e+04  5.819672e+04  5.695073e+04  1.948626e+04  1.500891e+04
> [6]  1.177034e+04  9.696327e+03  8.037049e+03  7.134058e+03  1.316449e-07
> [11]  9.077244e-08  6.417276e-08  5.046411e-08  1.998775e-08 -1.268081e-09
> [16] -3.140881e-08 -4.478184e-08 -5.370730e-08 -8.507492e-08 -9.496699e-08
> # S'S fails to be non-negative definite.
>
> I can't show you how to produce S easily but below I attempt at a
> reproducible version of the problem:
>
>> set.seed(091207)
>> X <- runif(1455*269,-1e4,1e4)
>> p <- rbinom(1455*269,1,0.845)
>> Y <- p*X
>> dim(Y) <- c(1455,269)
>> YY <- crossprod(Y,Y)
>> (eigen(YY,only.values = TRUE)$values)[250:269]
> [1] 17951634238 17928076223 17725528630 17647734206 17218470634 16947982383
> [7] 16728385887 16569501198 16498812174 16211312750 16127786747 16006841514
> [13] 15641955527 15472400630 15433931889 15083894866 14794357643 14586969350
> [19] 14297854542 13986819627
> # No sign of negative eigenvalues; phenomenon must be due
> # to special structure of S.
> # S is a matrix of empirical parameter scores at an approximate
> # mle for a model with 269 paramters fitted to 1455 observations.
> # Thus, for example, its column sums are approximately zero:
>> summary(apply(S,2,sum))
>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
> -1.148e-03 -2.227e-04 -7.496e-06 -6.011e-05  7.967e-05  8.254e-04
>
> I'm starting to think that it may not be a good idea to attempt to compute
> large information matrices and their determinants!
>
> Murray Jorgensen
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maj at stats.waikato.ac.nz  Sun Dec  9 08:44:36 2007
From: maj at stats.waikato.ac.nz (maj at stats.waikato.ac.nz)
Date: Sun, 9 Dec 2007 20:44:36 +1300 (NZDT)
Subject: [R] Large determinant problem
In-Reply-To: <Pine.LNX.4.64.0712090628420.31005@gannet.stats.ox.ac.uk>
References: <49552.203.109.170.45.1197178133.squirrel@webmail.scms.waikato.ac.nz>
	<Pine.LNX.4.64.0712090628420.31005@gannet.stats.ox.ac.uk>
Message-ID: <49632.203.109.170.45.1197186276.squirrel@webmail.scms.waikato.ac.nz>

I tried crossprod(S) but the results were identical. The term
-0.5*log(det(S)) is  a complexity penalty meant to make it unattractive to
include too many components in a finite mixture model. This case was for a
9-component mixture. At least up to 6 components the determinant behaved
as expected and increased with the number of components.

Thanks for your comments.

> Hmm, S'S is numerically singular.  crossprod(S) would be a better way to
> compute it than crossprod(S,S) (it does use a different algorithm), but
> look at the singular values of S, which I suspect will show that S is
> numerically singular.
>
> Looks like the answer is 0.
>
>
> On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:
>
>> I thought I would have another try at explaining my problem. I think
>> that
>> last time I may have buried it in irrelevant detail.
>>
>> This output should explain my dilemma:
>>
>>> dim(S)
>> [1] 1455  269
>>> summary(as.vector(S))
>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>> -1.160e+04  0.000e+00  0.000e+00 -4.132e-08  0.000e+00  8.636e+03
>>> sum(as.vector(S)==0)/(1455*269)
>> [1] 0.8451794
>> # S is a large moderately sparse matrix with some large elements
>>> SS <- crossprod(S,S)
>>> (eigen(SS,only.values = TRUE)$values)[250:269]
>> [1]  9.264883e+04  5.819672e+04  5.695073e+04  1.948626e+04
>> 1.500891e+04
>> [6]  1.177034e+04  9.696327e+03  8.037049e+03  7.134058e+03
>> 1.316449e-07
>> [11]  9.077244e-08  6.417276e-08  5.046411e-08  1.998775e-08
>> -1.268081e-09
>> [16] -3.140881e-08 -4.478184e-08 -5.370730e-08 -8.507492e-08
>> -9.496699e-08
>> # S'S fails to be non-negative definite.
>>
>> I can't show you how to produce S easily but below I attempt at a
>> reproducible version of the problem:
>>
>>> set.seed(091207)
>>> X <- runif(1455*269,-1e4,1e4)
>>> p <- rbinom(1455*269,1,0.845)
>>> Y <- p*X
>>> dim(Y) <- c(1455,269)
>>> YY <- crossprod(Y,Y)
>>> (eigen(YY,only.values = TRUE)$values)[250:269]
>> [1] 17951634238 17928076223 17725528630 17647734206 17218470634
>> 16947982383
>> [7] 16728385887 16569501198 16498812174 16211312750 16127786747
>> 16006841514
>> [13] 15641955527 15472400630 15433931889 15083894866 14794357643
>> 14586969350
>> [19] 14297854542 13986819627
>> # No sign of negative eigenvalues; phenomenon must be due
>> # to special structure of S.
>> # S is a matrix of empirical parameter scores at an approximate
>> # mle for a model with 269 paramters fitted to 1455 observations.
>> # Thus, for example, its column sums are approximately zero:
>>> summary(apply(S,2,sum))
>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>> -1.148e-03 -2.227e-04 -7.496e-06 -6.011e-05  7.967e-05  8.254e-04
>>
>> I'm starting to think that it may not be a good idea to attempt to
>> compute
>> large information matrices and their determinants!
>>
>> Murray Jorgensen
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>


From ripley at stats.ox.ac.uk  Sun Dec  9 08:56:32 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 9 Dec 2007 07:56:32 +0000 (GMT)
Subject: [R] Large determinant problem
In-Reply-To: <49632.203.109.170.45.1197186276.squirrel@webmail.scms.waikato.ac.nz>
References: <49552.203.109.170.45.1197178133.squirrel@webmail.scms.waikato.ac.nz>
	<Pine.LNX.4.64.0712090628420.31005@gannet.stats.ox.ac.uk>
	<49632.203.109.170.45.1197186276.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <Pine.LNX.4.64.0712090747510.32268@gannet.stats.ox.ac.uk>

On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:

> I tried crossprod(S) but the results were identical. The term
> -0.5*log(det(S)) is  a complexity penalty meant to make it unattractive to
> include too many components in a finite mixture model. This case was for a
> 9-component mixture. At least up to 6 components the determinant behaved
> as expected and increased with the number of components.

And the singular values were?

I am not surprised at this: if you have too many components some of them 
may not be contributing to the fit or duplicating others: both lead to 
numerically singular information matrices.  In many mixture-fitting 
problems the log-likelihood is unbounded but with many local maxima: it is 
very easy to find a poor one.

>
> Thanks for your comments.
>
>> Hmm, S'S is numerically singular.  crossprod(S) would be a better way to
>> compute it than crossprod(S,S) (it does use a different algorithm), but
>> look at the singular values of S, which I suspect will show that S is
>> numerically singular.
>>
>> Looks like the answer is 0.
>>
>>
>> On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:
>>
>>> I thought I would have another try at explaining my problem. I think
>>> that
>>> last time I may have buried it in irrelevant detail.
>>>
>>> This output should explain my dilemma:
>>>
>>>> dim(S)
>>> [1] 1455  269
>>>> summary(as.vector(S))
>>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>>> -1.160e+04  0.000e+00  0.000e+00 -4.132e-08  0.000e+00  8.636e+03
>>>> sum(as.vector(S)==0)/(1455*269)
>>> [1] 0.8451794
>>> # S is a large moderately sparse matrix with some large elements
>>>> SS <- crossprod(S,S)
>>>> (eigen(SS,only.values = TRUE)$values)[250:269]
>>> [1]  9.264883e+04  5.819672e+04  5.695073e+04  1.948626e+04
>>> 1.500891e+04
>>> [6]  1.177034e+04  9.696327e+03  8.037049e+03  7.134058e+03
>>> 1.316449e-07
>>> [11]  9.077244e-08  6.417276e-08  5.046411e-08  1.998775e-08
>>> -1.268081e-09
>>> [16] -3.140881e-08 -4.478184e-08 -5.370730e-08 -8.507492e-08
>>> -9.496699e-08
>>> # S'S fails to be non-negative definite.
>>>
>>> I can't show you how to produce S easily but below I attempt at a
>>> reproducible version of the problem:
>>>
>>>> set.seed(091207)
>>>> X <- runif(1455*269,-1e4,1e4)
>>>> p <- rbinom(1455*269,1,0.845)
>>>> Y <- p*X
>>>> dim(Y) <- c(1455,269)
>>>> YY <- crossprod(Y,Y)
>>>> (eigen(YY,only.values = TRUE)$values)[250:269]
>>> [1] 17951634238 17928076223 17725528630 17647734206 17218470634
>>> 16947982383
>>> [7] 16728385887 16569501198 16498812174 16211312750 16127786747
>>> 16006841514
>>> [13] 15641955527 15472400630 15433931889 15083894866 14794357643
>>> 14586969350
>>> [19] 14297854542 13986819627
>>> # No sign of negative eigenvalues; phenomenon must be due
>>> # to special structure of S.
>>> # S is a matrix of empirical parameter scores at an approximate
>>> # mle for a model with 269 paramters fitted to 1455 observations.
>>> # Thus, for example, its column sums are approximately zero:
>>>> summary(apply(S,2,sum))
>>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>>> -1.148e-03 -2.227e-04 -7.496e-06 -6.011e-05  7.967e-05  8.254e-04
>>>
>>> I'm starting to think that it may not be a good idea to attempt to
>>> compute
>>> large information matrices and their determinants!
>>>
>>> Murray Jorgensen
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From k at chicagogsb.edu  Sun Dec  9 09:23:17 2007
From: k at chicagogsb.edu (Kapoor, Bharat )
Date: Sun, 09 Dec 2007 02:23:17 -0600
Subject: [R] Barchart, Pareto
Message-ID: <49379889C4CA4940A9E1469DCFB9AD8504ADD7@GSBHEX1V.gsb.uchicago.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071209/14fc4588/attachment.pl 

From tura at centroin.com.br  Sun Dec  9 11:04:17 2007
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Sun, 09 Dec 2007 08:04:17 -0200
Subject: [R] level significance
In-Reply-To: <8A96F09B52875349A6B34475EDCCC277044AD3@lu-mail-san.dfu.local>
References: <8A96F09B52875349A6B34475EDCCC277044AD3@lu-mail-san.dfu.local>
Message-ID: <1197194657.5971.10.camel@R3-Thux>


On Sat, 2007-12-08 at 22:51 +0100, Irene Mantzouni wrote:
> Hi all!
>  
> I am fitting a (mixed) model with a factor (F) and continuous response and predictor:
> y~F+F:x
>  
> (How) can I check the significance of the model at each factor level (i.e. the model could be significant only at one of the levels)?
>  
> Thank you!

Irene

If I understand your doubt is necessary only use a summary command.

Example
set.seed(123)
x<-rnorm(300,sd=2)
F<-sample(rep(letters[1:3],100))
y<-rnorm(300,mean=2,sd=1.5)
model<-lm(y~F+F:x)
summary(model)

(..)
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.118128   0.151543  13.977   <2e-16 ***
Fb          -0.052866   0.213729  -0.247    0.805    
Fc          -0.229110   0.213726  -1.072    0.285    
Fa:x         0.036987   0.074549   0.496    0.620    
Fb:x         0.059439   0.083823   0.709    0.479    
Fc:x         0.003769   0.082373   0.046    0.964    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
(...)

In this case only the Intercept is significant

-- 
Bernardo Rangel Tura, M.D,MPH,Ph.D
National Institute of Cardiology
Brazil


From p.dalgaard at biostat.ku.dk  Sun Dec  9 11:15:03 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sun, 09 Dec 2007 11:15:03 +0100
Subject: [R] Large determinant problem
In-Reply-To: <Pine.LNX.4.64.0712090628420.31005@gannet.stats.ox.ac.uk>
References: <49552.203.109.170.45.1197178133.squirrel@webmail.scms.waikato.ac.nz>
	<Pine.LNX.4.64.0712090628420.31005@gannet.stats.ox.ac.uk>
Message-ID: <475BC027.8080000@biostat.ku.dk>

Prof Brian Ripley wrote:
> Hmm, S'S is numerically singular.  crossprod(S) would be a better way to 
> compute it than crossprod(S,S) (it does use a different algorithm), but 
> look at the singular values of S, which I suspect will show that S is 
> numerically singular.
>
> Looks like the answer is 0.
>
>   
Another possibility is that there is a scaling issue. Linear algebra 
routines are unhappy about having regressors on widely different scales, 
and the squaring of eigenvalues impied by taking crossproducts is no 
help. So what is the diagonal of S'S like? How about svd(S)? (Maybe 
after column normalization)
> On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:
>
>   
>> I thought I would have another try at explaining my problem. I think that
>> last time I may have buried it in irrelevant detail.
>>
>> This output should explain my dilemma:
>>
>>     
>>> dim(S)
>>>       
>> [1] 1455  269
>>     
>>> summary(as.vector(S))
>>>       
>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>> -1.160e+04  0.000e+00  0.000e+00 -4.132e-08  0.000e+00  8.636e+03
>>     
>>> sum(as.vector(S)==0)/(1455*269)
>>>       
>> [1] 0.8451794
>> # S is a large moderately sparse matrix with some large elements
>>     
>>> SS <- crossprod(S,S)
>>> (eigen(SS,only.values = TRUE)$values)[250:269]
>>>       
>> [1]  9.264883e+04  5.819672e+04  5.695073e+04  1.948626e+04  1.500891e+04
>> [6]  1.177034e+04  9.696327e+03  8.037049e+03  7.134058e+03  1.316449e-07
>> [11]  9.077244e-08  6.417276e-08  5.046411e-08  1.998775e-08 -1.268081e-09
>> [16] -3.140881e-08 -4.478184e-08 -5.370730e-08 -8.507492e-08 -9.496699e-08
>> # S'S fails to be non-negative definite.
>>
>> I can't show you how to produce S easily but below I attempt at a
>> reproducible version of the problem:
>>
>>     
>>> set.seed(091207)
>>> X <- runif(1455*269,-1e4,1e4)
>>> p <- rbinom(1455*269,1,0.845)
>>> Y <- p*X
>>> dim(Y) <- c(1455,269)
>>> YY <- crossprod(Y,Y)
>>> (eigen(YY,only.values = TRUE)$values)[250:269]
>>>       
>> [1] 17951634238 17928076223 17725528630 17647734206 17218470634 16947982383
>> [7] 16728385887 16569501198 16498812174 16211312750 16127786747 16006841514
>> [13] 15641955527 15472400630 15433931889 15083894866 14794357643 14586969350
>> [19] 14297854542 13986819627
>> # No sign of negative eigenvalues; phenomenon must be due
>> # to special structure of S.
>> # S is a matrix of empirical parameter scores at an approximate
>> # mle for a model with 269 paramters fitted to 1455 observations.
>> # Thus, for example, its column sums are approximately zero:
>>     
>>> summary(apply(S,2,sum))
>>>       
>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>> -1.148e-03 -2.227e-04 -7.496e-06 -6.011e-05  7.967e-05  8.254e-04
>>
>> I'm starting to think that it may not be a good idea to attempt to compute
>> large information matrices and their determinants!
>>
>> Murray Jorgensen
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From pedrosmarques at portugalmail.pt  Sun Dec  9 11:21:16 2007
From: pedrosmarques at portugalmail.pt (pedrosmarques at portugalmail.pt)
Date: Sun, 09 Dec 2007 10:21:16 +0000
Subject: [R] Problems working with large matrix (using package R.huge)
Message-ID: <1197195676.475bc19cb9cf2@gold5.portugalmail.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071209/10f1d4df/attachment.pl 

From maj at stats.waikato.ac.nz  Sun Dec  9 11:34:27 2007
From: maj at stats.waikato.ac.nz (maj at stats.waikato.ac.nz)
Date: Sun, 9 Dec 2007 23:34:27 +1300 (NZDT)
Subject: [R] Large determinant problem
In-Reply-To: <Pine.LNX.4.64.0712090747510.32268@gannet.stats.ox.ac.uk>
References: <49552.203.109.170.45.1197178133.squirrel@webmail.scms.waikato.ac.nz> 
	<Pine.LNX.4.64.0712090628420.31005@gannet.stats.ox.ac.uk>
	<49632.203.109.170.45.1197186276.squirrel@webmail.scms.waikato.ac.nz>
	<Pine.LNX.4.64.0712090747510.32268@gannet.stats.ox.ac.uk>
Message-ID: <49719.203.109.170.45.1197196467.squirrel@webmail.scms.waikato.ac.nz>

What you say about mixture models is true in general, however this fit was
the best of 100 random EM starts. Unbounded likelihoods I believe are only
a problem for continuous data mixture models and mine was discrete. Anyway
it's nearly midnight now here so I'd better sleep. Before I go, here are
the singular values:


> svd(S)$d
  [1] 1.207593e+05 1.049068e+05 9.308082e+04 8.332758e+04 6.929102e+04
  [6] 6.323142e+04 5.977638e+04 5.723191e+04 4.375631e+04 2.723792e+04
 [11] 2.592586e+04 2.411705e+04 2.392963e+04 2.196578e+04 2.169200e+04
 [16] 2.123290e+04 2.054479e+04 1.948157e+04 1.927687e+04 1.777423e+04
 [21] 1.768510e+04 1.754492e+04 1.735954e+04 1.643881e+04 1.600038e+04
 [26] 1.588009e+04 1.584179e+04 1.419902e+04 1.401829e+04 1.332706e+04
 [31] 1.310741e+04 1.282854e+04 1.240196e+04 1.229453e+04 1.198187e+04
 [36] 1.168831e+04 1.069801e+04 1.063407e+04 1.060623e+04 1.056741e+04
 [41] 1.037193e+04 1.018307e+04 9.954778e+03 9.691297e+03 9.544900e+03
 [46] 9.353932e+03 9.084223e+03 9.023719e+03 8.538460e+03 8.260557e+03
 [51] 7.789166e+03 7.624562e+03 7.552246e+03 7.371003e+03 7.249892e+03
 [56] 7.170754e+03 7.143712e+03 7.041465e+03 7.019497e+03 6.918243e+03
 [61] 6.725985e+03 6.635220e+03 6.610919e+03 6.600485e+03 6.378983e+03
 [66] 6.255341e+03 6.252620e+03 5.944109e+03 5.890990e+03 5.875790e+03
 [71] 5.812950e+03 5.786653e+03 5.754739e+03 5.743921e+03 5.729494e+03
 [76] 5.588519e+03 5.558093e+03 5.511866e+03 5.447340e+03 5.436718e+03
 [81] 5.390440e+03 5.389862e+03 5.351446e+03 5.323460e+03 5.231327e+03
 [86] 5.154886e+03 5.146495e+03 5.103094e+03 5.062339e+03 5.016310e+03
 [91] 5.007371e+03 5.003195e+03 4.987950e+03 4.984937e+03 4.971855e+03
 [96] 4.963557e+03 4.913927e+03 4.891866e+03 4.845879e+03 4.841233e+03
[101] 4.807681e+03 4.789150e+03 4.768244e+03 4.752387e+03 4.685244e+03
[106] 4.667949e+03 4.662146e+03 4.655817e+03 4.615451e+03 4.542832e+03
[111] 4.463354e+03 4.448647e+03 4.420757e+03 4.393323e+03 4.368262e+03
[116] 4.330368e+03 4.322231e+03 4.280486e+03 4.269604e+03 4.266072e+03
[121] 4.227934e+03 4.210416e+03 4.197196e+03 4.169111e+03 4.168029e+03
[126] 4.145750e+03 4.137148e+03 4.117092e+03 4.102093e+03 4.031528e+03
[131] 3.997150e+03 3.989493e+03 3.960800e+03 3.954143e+03 3.921214e+03
[136] 3.892764e+03 3.861505e+03 3.831798e+03 3.821399e+03 3.816648e+03
[141] 3.813275e+03 3.797050e+03 3.788435e+03 3.765362e+03 3.753526e+03
[146] 3.750739e+03 3.717638e+03 3.704314e+03 3.700483e+03 3.683338e+03
[151] 3.669548e+03 3.651310e+03 3.645356e+03 3.636891e+03 3.634490e+03
[156] 3.631998e+03 3.598744e+03 3.578298e+03 3.577353e+03 3.492344e+03
[161] 3.457991e+03 3.438116e+03 3.401560e+03 3.398088e+03 3.390086e+03
[166] 3.362965e+03 3.328079e+03 3.306448e+03 3.289258e+03 3.283123e+03
[171] 3.268046e+03 3.254232e+03 3.238759e+03 3.176306e+03 3.173192e+03
[176] 3.145273e+03 3.132647e+03 3.124703e+03 3.116454e+03 3.028187e+03
[181] 3.026404e+03 3.003130e+03 2.985991e+03 2.952215e+03 2.946402e+03
[186] 2.937366e+03 2.902973e+03 2.867319e+03 2.855981e+03 2.843939e+03
[191] 2.830485e+03 2.788518e+03 2.761445e+03 2.753757e+03 2.752846e+03
[196] 2.725580e+03 2.723263e+03 2.669216e+03 2.640574e+03 2.545404e+03
[201] 2.543216e+03 2.508090e+03 2.486351e+03 2.465191e+03 2.447437e+03
[206] 2.431466e+03 2.424620e+03 2.423907e+03 2.399220e+03 2.369538e+03
[211] 2.305238e+03 2.261185e+03 2.252992e+03 2.171784e+03 2.169940e+03
[216] 2.127546e+03 2.094436e+03 2.074605e+03 2.056932e+03 2.053942e+03
[221] 2.011659e+03 1.993672e+03 1.934327e+03 1.893751e+03 1.848455e+03
[226] 1.838315e+03 1.763492e+03 1.728018e+03 1.726965e+03 1.623798e+03
[231] 1.617925e+03 1.554590e+03 1.498835e+03 1.421876e+03 1.256465e+03
[236] 1.200904e+03 1.118300e+03 1.101870e+03 1.055408e+03 9.238208e+02
[241] 8.125509e+02 7.031272e+02 6.943645e+02 6.338677e+02 5.772709e+02
[246] 5.077392e+02 4.566595e+02 4.025622e+02 3.118065e+02 3.043827e+02
[251] 2.412400e+02 2.386435e+02 1.395932e+02 1.225108e+02 1.084912e+02
[256] 9.846993e+01 8.964959e+01 8.446336e+01 2.486490e-05 5.362792e-11
[261] 9.161356e-12 9.161356e-12 9.161356e-12 9.161356e-12 9.161356e-12
[266] 9.161356e-12 9.161356e-12 9.161356e-12 9.161356e-12

Murray

> On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:
>
>> I tried crossprod(S) but the results were identical. The term
>> -0.5*log(det(S)) is  a complexity penalty meant to make it unattractive
>> to
>> include too many components in a finite mixture model. This case was for
>> a
>> 9-component mixture. At least up to 6 components the determinant behaved
>> as expected and increased with the number of components.
>
> And the singular values were?
>
> I am not surprised at this: if you have too many components some of them
> may not be contributing to the fit or duplicating others: both lead to
> numerically singular information matrices.  In many mixture-fitting
> problems the log-likelihood is unbounded but with many local maxima: it is
> very easy to find a poor one.
>
>>
>> Thanks for your comments.
>>
>>> Hmm, S'S is numerically singular.  crossprod(S) would be a better way
>>> to
>>> compute it than crossprod(S,S) (it does use a different algorithm), but
>>> look at the singular values of S, which I suspect will show that S is
>>> numerically singular.
>>>
>>> Looks like the answer is 0.
>>>
>>>
>>> On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:
>>>
>>>> I thought I would have another try at explaining my problem. I think
>>>> that
>>>> last time I may have buried it in irrelevant detail.
>>>>
>>>> This output should explain my dilemma:
>>>>
>>>>> dim(S)
>>>> [1] 1455  269
>>>>> summary(as.vector(S))
>>>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>>>> -1.160e+04  0.000e+00  0.000e+00 -4.132e-08  0.000e+00  8.636e+03
>>>>> sum(as.vector(S)==0)/(1455*269)
>>>> [1] 0.8451794
>>>> # S is a large moderately sparse matrix with some large elements
>>>>> SS <- crossprod(S,S)
>>>>> (eigen(SS,only.values = TRUE)$values)[250:269]
>>>> [1]  9.264883e+04  5.819672e+04  5.695073e+04  1.948626e+04
>>>> 1.500891e+04
>>>> [6]  1.177034e+04  9.696327e+03  8.037049e+03  7.134058e+03
>>>> 1.316449e-07
>>>> [11]  9.077244e-08  6.417276e-08  5.046411e-08  1.998775e-08
>>>> -1.268081e-09
>>>> [16] -3.140881e-08 -4.478184e-08 -5.370730e-08 -8.507492e-08
>>>> -9.496699e-08
>>>> # S'S fails to be non-negative definite.
>>>>
>>>> I can't show you how to produce S easily but below I attempt at a
>>>> reproducible version of the problem:
>>>>
>>>>> set.seed(091207)
>>>>> X <- runif(1455*269,-1e4,1e4)
>>>>> p <- rbinom(1455*269,1,0.845)
>>>>> Y <- p*X
>>>>> dim(Y) <- c(1455,269)
>>>>> YY <- crossprod(Y,Y)
>>>>> (eigen(YY,only.values = TRUE)$values)[250:269]
>>>> [1] 17951634238 17928076223 17725528630 17647734206 17218470634
>>>> 16947982383
>>>> [7] 16728385887 16569501198 16498812174 16211312750 16127786747
>>>> 16006841514
>>>> [13] 15641955527 15472400630 15433931889 15083894866 14794357643
>>>> 14586969350
>>>> [19] 14297854542 13986819627
>>>> # No sign of negative eigenvalues; phenomenon must be due
>>>> # to special structure of S.
>>>> # S is a matrix of empirical parameter scores at an approximate
>>>> # mle for a model with 269 paramters fitted to 1455 observations.
>>>> # Thus, for example, its column sums are approximately zero:
>>>>> summary(apply(S,2,sum))
>>>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>>>> -1.148e-03 -2.227e-04 -7.496e-06 -6.011e-05  7.967e-05  8.254e-04
>>>>
>>>> I'm starting to think that it may not be a good idea to attempt to
>>>> compute
>>>> large information matrices and their determinants!
>>>>
>>>> Murray Jorgensen
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> --
>>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>> University of Oxford,             Tel:  +44 1865 272861 (self)
>>> 1 South Parks Road,                     +44 1865 272866 (PA)
>>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>>
>>>
>>
>>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>


From hb at stat.berkeley.edu  Sun Dec  9 11:53:29 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Sun, 9 Dec 2007 02:53:29 -0800
Subject: [R] Problems working with large matrix (using package R.huge)
In-Reply-To: <1197195676.475bc19cb9cf2@gold5.portugalmail.pt>
References: <1197195676.475bc19cb9cf2@gold5.portugalmail.pt>
Message-ID: <59d7961d0712090253o49848cb4g46f1afc3fc39f0ff@mail.gmail.com>

Hi,

I'm sorry, but I will most likely withdraw R.huge from CRAN anytime
soon.  The File***Matrix classes had some problems, which when solved
made it too slow.  See BufferedMatrix package in Bioconductor instead.

/Henrik

On 09/12/2007, pedrosmarques at portugalmail.pt
<pedrosmarques at portugalmail.pt> wrote:
>
>
> Hi all,
>
> Since I was having several problems trying to work with a large matrix I started to use the package R.huge but I'm having the following problem
>
> > x<-FileByteMatrix("covtype.data",nrow=581012,ncol=55)
> Error: cannot allocate vector of size 770.8 Mb
> In addition: Warning messages:
> 1: Reached total allocation of 447Mb: see help(memory.size) in: readChar(con = con, nchar = len)
> 2: Reached total allocation of 447Mb: see help(memory.size) in: readChar(con = con, nchar = len)
> 3: Reached total allocation of 447Mb: see help(memory.size) in: readChar(con = con, nchar = len)
> 4: Reached total allocation of 447Mb: see help(memory.size) in: readChar(con = con, nchar = len)
>
>
> How can I solve this??
>
> Best regards
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mark at wardle.org  Sun Dec  9 12:11:37 2007
From: mark at wardle.org (Mark Wardle)
Date: Sun, 9 Dec 2007 11:11:37 +0000
Subject: [R] Getting estimates from survfit.coxph
Message-ID: <b59a37130712090311r67e8ea3bi234396359ff31160@mail.gmail.com>

Dear all,

I'm having difficulty getting access to data generated by survfit and
print.survfit when they are using with a Cox model (survfit.coxph).

I would like to programmatically access the median survival time for
each strata together with the 95% confidence interval. I can get it on
screen, but can't get to it algorithmically. I found myself examining
the source of print.survfit to try to work out how it is done
internally, but is there a better way?

An example (and I realise that estimating survival curses from an
average status and time is incorrect in this instance, but it keeps
this example simple):

test1 <- list(time=  c(4, 3,1,1,2,2,3),
                status=c(1,NA,1,0,1,1,0),
                x=     c(0, 2,1,1,1,0,0),
                sex=   c(0, 0,0,0,1,1,1))
c1 <- coxph( Surv(time, status) ~ x + strata(sex), test1)  #stratified model

f1 <- survfit(c1)
sf1 <- summary(f1)
str(f1)
print(f1)
print(sf1)
str(sf1)

I'm sure I am missing something obvious. Apologies - but any help
greatfully received!

Best wishes,

Mark

P.S. I can get to diferrent estimates for median survival for
different groups using simpler mechanisms, but they yield different
estimates: From my data, so no reproducible (and ataxSurv() is a
wrapper function that calls plain Surv() after manipulating the data
simply):


# For an "average" patient: (doesn't make any sense biologically)
> survfit(surv.results$cox)
Call: survfit.coxph(object = surv.results$cox)

      n  events  median 0.95LCL 0.95UCL
    136      96       6       6       8

#
# predict a curve for a patient: (these are the answers I really want
to extract)
#
> survfit(surv.results$cox, newdata=data.frame(onset=50, ic.duration=10, simple.msa=c('MSA','Not MSA'), autoimmune=F, carcinoma=F))
Call: survfit.coxph(object = surv.results$cox, newdata = data.frame(onset = 50,
    ic.duration = 10, simple.msa = c("MSA", "Not MSA"), autoimmune = F,
    carcinoma = F))

       n events median 0.95LCL 0.95UCL
[1,] 136     96      8       7      11
[2,] 136     96      3       2       6

#
# without using Cox regression:
#
> survfit(ataxSurv(surv.support, surv.follow.up, surv.results$data) ~ simple.msa, data=surv.results$data)
Call: survfit(formula = ataxSurv(surv.support, surv.follow.up,
surv.results$data) ~
    simple.msa, data = surv.results$data)

   1 observation deleted due to missingness
                     n events median 0.95LCL 0.95UCL
simple.msa=Not MSA 120     80      8       6      11
simple.msa=MSA      19     17      2       1       4


-- 
Dr. Mark Wardle
Specialist registrar, Neurology
Cardiff, UK


From dieter.menne at menne-biomed.de  Sun Dec  9 12:37:42 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 9 Dec 2007 11:37:42 +0000 (UTC)
Subject: [R] Getting estimates from survfit.coxph
References: <b59a37130712090311r67e8ea3bi234396359ff31160@mail.gmail.com>
Message-ID: <loom.20071209T113343-436@post.gmane.org>

Mark Wardle <mark <at> wardle.org> writes:

> I'm having difficulty getting access to data generated by survfit and
> print.survfit when they are using with a Cox model (survfit.coxph).
> 
> I would like to programmatically access the median survival time for
> each strata together with the 95% confidence interval. I can get it on
> screen, but can't get to it algorithmically. I found myself examining
> the source of print.survfit to try to work out how it is done
> internally, but is there a better way?
> 
> An example (and I realise that estimating survival curses from an
> average status and time is incorrect in this instance, but it keeps
> this example simple):
> 
> test1 <- list(time=  c(4, 3,1,1,2,2,3),
>                 status=c(1,NA,1,0,1,1,0),
>                 x=     c(0, 2,1,1,1,0,0),
>                 sex=   c(0, 0,0,0,1,1,1))
> c1 <- coxph( Surv(time, status) ~ x + strata(sex), test1)  #stratified model
> 
> f1 <- survfit(c1)
> sf1 <- summary(f1)
> str(f1)
> print(f1)
> print(sf1)
> str(sf1)

(Disclaimer: there may be a better way got get it with library Design by 
Frank Harrell, but let's assume we have to do it the hard way)

Looks like it is a bit hidden. f1 is of class(print.survfit), as str(f1) 
tells us. So let's try getAnyhwere(print.survfit). In the lower part you 
find line like the following:  

     x1 <- pfun(nsubjects, stime, surv, x$n.risk, x$n.event, 
            x$lower, x$upper)
        if (is.matrix(x1)) {
            if (is.null(x$lower)) 
                dimnames(x1) <- list(NULL, plab)
            else dimnames(x1) <- list(NULL, c(plab, plab2))
        }
        else {
            if (is.null(x$lower)) 
                names(x1) <- plab
            else names(x1) <- c(plab, plab2)
        }
        if (show.rmean) 
            print(x1)
 
Make a copy of that function under a new name, and return x1. 

Dieter


From ripley at stats.ox.ac.uk  Sun Dec  9 13:25:28 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 9 Dec 2007 12:25:28 +0000 (GMT)
Subject: [R] Large determinant problem
In-Reply-To: <49719.203.109.170.45.1197196467.squirrel@webmail.scms.waikato.ac.nz>
References: <49552.203.109.170.45.1197178133.squirrel@webmail.scms.waikato.ac.nz>
	<Pine.LNX.4.64.0712090628420.31005@gannet.stats.ox.ac.uk>   
	<49632.203.109.170.45.1197186276.squirrel@webmail.scms.waikato.ac.nz> 
	<Pine.LNX.4.64.0712090747510.32268@gannet.stats.ox.ac.uk>
	<49719.203.109.170.45.1197196467.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <Pine.LNX.4.64.0712091223160.8586@gannet.stats.ox.ac.uk>

So S is numerically singular: the last 10 or 11 values are effectively 0.
As Peter Dalgaard said, scaling may help but only if the parameters are on 
drastically different scales.

On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:

> What you say about mixture models is true in general, however this fit was
> the best of 100 random EM starts. Unbounded likelihoods I believe are only
> a problem for continuous data mixture models and mine was discrete. Anyway
> it's nearly midnight now here so I'd better sleep. Before I go, here are
> the singular values:
>
>
>> svd(S)$d
>  [1] 1.207593e+05 1.049068e+05 9.308082e+04 8.332758e+04 6.929102e+04
>  [6] 6.323142e+04 5.977638e+04 5.723191e+04 4.375631e+04 2.723792e+04
> [11] 2.592586e+04 2.411705e+04 2.392963e+04 2.196578e+04 2.169200e+04
> [16] 2.123290e+04 2.054479e+04 1.948157e+04 1.927687e+04 1.777423e+04
> [21] 1.768510e+04 1.754492e+04 1.735954e+04 1.643881e+04 1.600038e+04
> [26] 1.588009e+04 1.584179e+04 1.419902e+04 1.401829e+04 1.332706e+04
> [31] 1.310741e+04 1.282854e+04 1.240196e+04 1.229453e+04 1.198187e+04
> [36] 1.168831e+04 1.069801e+04 1.063407e+04 1.060623e+04 1.056741e+04
> [41] 1.037193e+04 1.018307e+04 9.954778e+03 9.691297e+03 9.544900e+03
> [46] 9.353932e+03 9.084223e+03 9.023719e+03 8.538460e+03 8.260557e+03
> [51] 7.789166e+03 7.624562e+03 7.552246e+03 7.371003e+03 7.249892e+03
> [56] 7.170754e+03 7.143712e+03 7.041465e+03 7.019497e+03 6.918243e+03
> [61] 6.725985e+03 6.635220e+03 6.610919e+03 6.600485e+03 6.378983e+03
> [66] 6.255341e+03 6.252620e+03 5.944109e+03 5.890990e+03 5.875790e+03
> [71] 5.812950e+03 5.786653e+03 5.754739e+03 5.743921e+03 5.729494e+03
> [76] 5.588519e+03 5.558093e+03 5.511866e+03 5.447340e+03 5.436718e+03
> [81] 5.390440e+03 5.389862e+03 5.351446e+03 5.323460e+03 5.231327e+03
> [86] 5.154886e+03 5.146495e+03 5.103094e+03 5.062339e+03 5.016310e+03
> [91] 5.007371e+03 5.003195e+03 4.987950e+03 4.984937e+03 4.971855e+03
> [96] 4.963557e+03 4.913927e+03 4.891866e+03 4.845879e+03 4.841233e+03
> [101] 4.807681e+03 4.789150e+03 4.768244e+03 4.752387e+03 4.685244e+03
> [106] 4.667949e+03 4.662146e+03 4.655817e+03 4.615451e+03 4.542832e+03
> [111] 4.463354e+03 4.448647e+03 4.420757e+03 4.393323e+03 4.368262e+03
> [116] 4.330368e+03 4.322231e+03 4.280486e+03 4.269604e+03 4.266072e+03
> [121] 4.227934e+03 4.210416e+03 4.197196e+03 4.169111e+03 4.168029e+03
> [126] 4.145750e+03 4.137148e+03 4.117092e+03 4.102093e+03 4.031528e+03
> [131] 3.997150e+03 3.989493e+03 3.960800e+03 3.954143e+03 3.921214e+03
> [136] 3.892764e+03 3.861505e+03 3.831798e+03 3.821399e+03 3.816648e+03
> [141] 3.813275e+03 3.797050e+03 3.788435e+03 3.765362e+03 3.753526e+03
> [146] 3.750739e+03 3.717638e+03 3.704314e+03 3.700483e+03 3.683338e+03
> [151] 3.669548e+03 3.651310e+03 3.645356e+03 3.636891e+03 3.634490e+03
> [156] 3.631998e+03 3.598744e+03 3.578298e+03 3.577353e+03 3.492344e+03
> [161] 3.457991e+03 3.438116e+03 3.401560e+03 3.398088e+03 3.390086e+03
> [166] 3.362965e+03 3.328079e+03 3.306448e+03 3.289258e+03 3.283123e+03
> [171] 3.268046e+03 3.254232e+03 3.238759e+03 3.176306e+03 3.173192e+03
> [176] 3.145273e+03 3.132647e+03 3.124703e+03 3.116454e+03 3.028187e+03
> [181] 3.026404e+03 3.003130e+03 2.985991e+03 2.952215e+03 2.946402e+03
> [186] 2.937366e+03 2.902973e+03 2.867319e+03 2.855981e+03 2.843939e+03
> [191] 2.830485e+03 2.788518e+03 2.761445e+03 2.753757e+03 2.752846e+03
> [196] 2.725580e+03 2.723263e+03 2.669216e+03 2.640574e+03 2.545404e+03
> [201] 2.543216e+03 2.508090e+03 2.486351e+03 2.465191e+03 2.447437e+03
> [206] 2.431466e+03 2.424620e+03 2.423907e+03 2.399220e+03 2.369538e+03
> [211] 2.305238e+03 2.261185e+03 2.252992e+03 2.171784e+03 2.169940e+03
> [216] 2.127546e+03 2.094436e+03 2.074605e+03 2.056932e+03 2.053942e+03
> [221] 2.011659e+03 1.993672e+03 1.934327e+03 1.893751e+03 1.848455e+03
> [226] 1.838315e+03 1.763492e+03 1.728018e+03 1.726965e+03 1.623798e+03
> [231] 1.617925e+03 1.554590e+03 1.498835e+03 1.421876e+03 1.256465e+03
> [236] 1.200904e+03 1.118300e+03 1.101870e+03 1.055408e+03 9.238208e+02
> [241] 8.125509e+02 7.031272e+02 6.943645e+02 6.338677e+02 5.772709e+02
> [246] 5.077392e+02 4.566595e+02 4.025622e+02 3.118065e+02 3.043827e+02
> [251] 2.412400e+02 2.386435e+02 1.395932e+02 1.225108e+02 1.084912e+02
> [256] 9.846993e+01 8.964959e+01 8.446336e+01 2.486490e-05 5.362792e-11
> [261] 9.161356e-12 9.161356e-12 9.161356e-12 9.161356e-12 9.161356e-12
> [266] 9.161356e-12 9.161356e-12 9.161356e-12 9.161356e-12
>
> Murray
>
>> On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:
>>
>>> I tried crossprod(S) but the results were identical. The term
>>> -0.5*log(det(S)) is  a complexity penalty meant to make it unattractive
>>> to
>>> include too many components in a finite mixture model. This case was for
>>> a
>>> 9-component mixture. At least up to 6 components the determinant behaved
>>> as expected and increased with the number of components.
>>
>> And the singular values were?
>>
>> I am not surprised at this: if you have too many components some of them
>> may not be contributing to the fit or duplicating others: both lead to
>> numerically singular information matrices.  In many mixture-fitting
>> problems the log-likelihood is unbounded but with many local maxima: it is
>> very easy to find a poor one.
>>
>>>
>>> Thanks for your comments.
>>>
>>>> Hmm, S'S is numerically singular.  crossprod(S) would be a better way
>>>> to
>>>> compute it than crossprod(S,S) (it does use a different algorithm), but
>>>> look at the singular values of S, which I suspect will show that S is
>>>> numerically singular.
>>>>
>>>> Looks like the answer is 0.
>>>>
>>>>
>>>> On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:
>>>>
>>>>> I thought I would have another try at explaining my problem. I think
>>>>> that
>>>>> last time I may have buried it in irrelevant detail.
>>>>>
>>>>> This output should explain my dilemma:
>>>>>
>>>>>> dim(S)
>>>>> [1] 1455  269
>>>>>> summary(as.vector(S))
>>>>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>>>>> -1.160e+04  0.000e+00  0.000e+00 -4.132e-08  0.000e+00  8.636e+03
>>>>>> sum(as.vector(S)==0)/(1455*269)
>>>>> [1] 0.8451794
>>>>> # S is a large moderately sparse matrix with some large elements
>>>>>> SS <- crossprod(S,S)
>>>>>> (eigen(SS,only.values = TRUE)$values)[250:269]
>>>>> [1]  9.264883e+04  5.819672e+04  5.695073e+04  1.948626e+04
>>>>> 1.500891e+04
>>>>> [6]  1.177034e+04  9.696327e+03  8.037049e+03  7.134058e+03
>>>>> 1.316449e-07
>>>>> [11]  9.077244e-08  6.417276e-08  5.046411e-08  1.998775e-08
>>>>> -1.268081e-09
>>>>> [16] -3.140881e-08 -4.478184e-08 -5.370730e-08 -8.507492e-08
>>>>> -9.496699e-08
>>>>> # S'S fails to be non-negative definite.
>>>>>
>>>>> I can't show you how to produce S easily but below I attempt at a
>>>>> reproducible version of the problem:
>>>>>
>>>>>> set.seed(091207)
>>>>>> X <- runif(1455*269,-1e4,1e4)
>>>>>> p <- rbinom(1455*269,1,0.845)
>>>>>> Y <- p*X
>>>>>> dim(Y) <- c(1455,269)
>>>>>> YY <- crossprod(Y,Y)
>>>>>> (eigen(YY,only.values = TRUE)$values)[250:269]
>>>>> [1] 17951634238 17928076223 17725528630 17647734206 17218470634
>>>>> 16947982383
>>>>> [7] 16728385887 16569501198 16498812174 16211312750 16127786747
>>>>> 16006841514
>>>>> [13] 15641955527 15472400630 15433931889 15083894866 14794357643
>>>>> 14586969350
>>>>> [19] 14297854542 13986819627
>>>>> # No sign of negative eigenvalues; phenomenon must be due
>>>>> # to special structure of S.
>>>>> # S is a matrix of empirical parameter scores at an approximate
>>>>> # mle for a model with 269 paramters fitted to 1455 observations.
>>>>> # Thus, for example, its column sums are approximately zero:
>>>>>> summary(apply(S,2,sum))
>>>>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>>>>> -1.148e-03 -2.227e-04 -7.496e-06 -6.011e-05  7.967e-05  8.254e-04
>>>>>
>>>>> I'm starting to think that it may not be a good idea to attempt to
>>>>> compute
>>>>> large information matrices and their determinants!
>>>>>
>>>>> Murray Jorgensen
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>> --
>>>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>>> University of Oxford,             Tel:  +44 1865 272861 (self)
>>>> 1 South Parks Road,                     +44 1865 272866 (PA)
>>>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>>>
>>>>
>>>
>>>
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From f.harrell at vanderbilt.edu  Sun Dec  9 14:08:10 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 09 Dec 2007 07:08:10 -0600
Subject: [R] [OT] A free (as in freedom) replacement for StatTransfer
In-Reply-To: <1197167109.2955.114.camel@Bellerophon.localdomain>
References: <20071208112446.GA2894@debian>	<1197131355.2955.37.camel@Bellerophon.localdomain>	<475B4447.6060008@optushome.com.au>
	<1197167109.2955.114.camel@Bellerophon.localdomain>
Message-ID: <475BE8BA.402@vanderbilt.edu>

Marc Schwartz wrote:
> On Sun, 2007-12-09 at 12:26 +1100, Tim Churches wrote:
>> Marc Schwartz wrote:
>>> Perhaps the most notable format that is lacking is the SAS proprietary
>>> format (not the Transport format), which is not openly published and to
>>> my knowledge, has not been independently reverse engineered.
>> The SAS proprietary dataset and format catalogue structures were
>> successfully reverse engineered by a small software firm called
>> Conceptual and were made available in a product called DBMS/Copy.
>> DBMS/Copy is/was similar to StatTransfer, but by 2002 was going a lot
>> further by adding support for much of the SAS data step syntax and some
>> core SAS procedures as well - in other words, it was rapidly becoming a
>> very viable and quite good pop person's SAS (with a modest one-off
>> license fee). However, the SAS Institute bought out the privately-held
>> Conceptual company, and now sells DBMS/Copy thhrough its wholly-owned
>> daat integration offshoot company, DataFlux, but without the SAS
>> datastep support features (to avoid competition with the mainstream SAS
>> cash cows) - see http://www.conceptual.com/
>>
>>> Any of the commercial products that support that format, with the
>>> possible exception of the SAS System Viewer (which is not open source,
>>> but is free from SAS), will be closed source and will have to be
>>> purchased.
>> DBMS/Copy is definitely closed-source and is probably not nearly as
>> cheapl as it once was when sold by Conceptual. But it is a great product
>> for convert to or from SAS proprietary data sets and format catalogues,
>> and works well and quickly with even huge datasets.
> 
> I am familiar with DBMS/Copy as my company uses it. Single user Windows
> licenses are not too bad (~$500 U.S.), but going beyond that gets
> expensive quickly.
> 
> I was aware of the past corporate history of Conceptual and it's
> transition to DataFlux/SAS, but not that the original developer (who's
> name escapes me at the moment) had reverse engineered the SAS format. He
> is a SAS employee now, working on other projects and the rumors
> percolate out of Tech Support in Cary every now and then that DBMS/Copy
> will be EOL'd. There have not been any updates/patches for version 8
> since January of this year.
> 
> The product is problematic however, in that it will both open and write
> SAS datasets that in actuality may not be readable by SAS itself. A
> significant problem that caused us to actually have to purchase SAS
> earlier this year to be able to validate aspects of a client data
> transfer process and we don't use SAS for anything else.
> 
> Indeed, there is an inconsistency in the behavior of SAS, DBMS/Copy and
> the SAS System Viewer when reading SAS datasets. Given that all three
> are now under the purview of SAS, it is in some respects surprising and
> in others, not so much.
> 
> Unfortunately, SAS is now bundling BASE, STAT and GRAPH as the entry
> level offering, so one cannot just get BASE as a stand-alone product any
> longer, which is all we needed. This makes the investment even more
> expensive for those of us who have to purchase at full commercial
> pricing.
> 
> Regards,
> 
> Marc

What is very telling about the true nature of the proprietary world, and 
reinforces my belief in open source software, is that the SAS Viewer 
(which works well under wine in Linux) has critical bugs in its csv or 
tab delimited data export just as PROC EXPORT under SAS-proper does.  If 
you have commas or unmatched quotes in a character field (tab for the 
tab delimeter option), the resulting ascii csv file will be broken.  To 
pay that much for SAS and to have such glaring errors that are never 
fixed is amazing.  On a side note I see that OpenOffice can read Word 
2007 documents but Word cannot read any OpenOffice documents.

I have used Stat/Transfer to great success for reading SAS binary files, 
and it runs perfectly under wine on Linux (best luck by creating Stata 
files from it and using the stata.get function in Hmisc (which uses the 
foreign package) to read the result).  I don't have to use Stat/Transfer 
very often but prefer it to DBMS/Copy as it allows more independence 
from the questionable policies of SAS Institute.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From f.harrell at vanderbilt.edu  Sun Dec  9 14:12:32 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 09 Dec 2007 07:12:32 -0600
Subject: [R] R + LaTeX formula
In-Reply-To: <1197163489.2955.80.camel@Bellerophon.localdomain>
References: <fjfeue$e75$1@ger.gmane.org>
	<1197163489.2955.80.camel@Bellerophon.localdomain>
Message-ID: <475BE9C0.1090601@vanderbilt.edu>

Marc Schwartz wrote:
> On Sun, 2007-12-09 at 01:04 +0000, Jonas Stein wrote:
>> Hi,
>>
>> what is actually the best method to include R-plots into LaTeX
>> documents?
>> At the moment i use 
>>
>> postscript("myplot.eps", width = 12.0, height = 9.0, horizontal =
>> FALSE, 
>> onefile = TRUE, paper = "special",encoding = "TeXtext.enc")
>> plot(foo,bar)
>> dev.off()  
>>
>> But it is a bit unhandy to scale later and its difficult to get nice 
>> formula in the plots.
>>
>> And how should i write formulas on the axis or at specific points? 
>> Has someone had some effort in exporting plots to pstricks or pictex?
>>
>> kind regards and thank you for reading so far,
> 
> As per the Details section of ?postscript:
> 
> The postscript produced for a single R plot is EPS (Encapsulated
> PostScript) compatible, and can be included into other documents, e.g.,
> into LaTeX, using \includegraphics{<filename>}. For use in this way you
> will probably want to set horizontal = FALSE, onefile = FALSE, paper =
> "special". Note that the bounding box is for the device region: if you
> find the white space around the plot region excessive, reduce the
> margins of the figure region via par(mar=).
> 
> 
> In your code above, change 'onefile = TRUE' to 'onefile = FALSE'.
> 
> For scaling you can use the LaTeX \includegraphics directive along with
> several height and width arguments, such as:
> 
>   \includegraphics[width=4in]{myplot.eps}
>   \includegraphics[height=4in]{myplot.eps}
>   \includegraphics[scale=0.75]{myplot.eps}
>   \includegraphics[width=0.4\textwidth]{myplot.eps}
> 
> You might want to review the following document:
> 
>   ftp://ftp.tex.ac.uk/tex-archive/info/epslatex.pdf 
> 
> For including formulae in R plots, see ?plotmath. You can run
> example(plotmath) and there are many posts in the r-help archives on
> this.
> 
> Beyond the above, you may want to look into using SWeave, whereby you
> can create entire documents, with nicely formatted tables and plots
> directly from R code. More information here:
> 
>   http://www.ci.tuwien.ac.at/~leisch/Sweave/
> 
> There are also a couple of articles on R News:
> 
> Friedrich Leisch. Sweave, part I: Mixing R and LATEX. R News,
> 2(3):28-31, December 2002.
> 
> Friedrich Leisch. Sweave, part II: Package vignettes. R News,
> 3(2):21-24, October 2003.
> 
> HTH,
> 
> Marc Schwartz

In addition to Marc's excellent summary (as usual) please see 
http://biostat.mc.vanderbilt.edu/SgraphicsHints and especially the link 
about putting LaTeX typesetting inside graphics (which requires Perl).

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From f.harrell at vanderbilt.edu  Sun Dec  9 14:14:29 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 09 Dec 2007 07:14:29 -0600
Subject: [R] Getting estimates from survfit.coxph
In-Reply-To: <loom.20071209T113343-436@post.gmane.org>
References: <b59a37130712090311r67e8ea3bi234396359ff31160@mail.gmail.com>
	<loom.20071209T113343-436@post.gmane.org>
Message-ID: <475BEA35.7080405@vanderbilt.edu>

Dieter Menne wrote:
> Mark Wardle <mark <at> wardle.org> writes:
> 
>> I'm having difficulty getting access to data generated by survfit and
>> print.survfit when they are using with a Cox model (survfit.coxph).
>>
>> I would like to programmatically access the median survival time for
>> each strata together with the 95% confidence interval. I can get it on
>> screen, but can't get to it algorithmically. I found myself examining
>> the source of print.survfit to try to work out how it is done
>> internally, but is there a better way?
>>
>> An example (and I realise that estimating survival curses from an
>> average status and time is incorrect in this instance, but it keeps
>> this example simple):
>>
>> test1 <- list(time=  c(4, 3,1,1,2,2,3),
>>                 status=c(1,NA,1,0,1,1,0),
>>                 x=     c(0, 2,1,1,1,0,0),
>>                 sex=   c(0, 0,0,0,1,1,1))
>> c1 <- coxph( Surv(time, status) ~ x + strata(sex), test1)  #stratified model
>>
>> f1 <- survfit(c1)
>> sf1 <- summary(f1)
>> str(f1)
>> print(f1)
>> print(sf1)
>> str(sf1)
> 
> (Disclaimer: there may be a better way got get it with library Design by 
> Frank Harrell, but let's assume we have to do it the hard way)

Aside from getting confidence limits, the Quantile function in Design 
(Quantile.cph) will create an R function with the analytic 
representation of the quantiles of the fitted survival distribution.

Frank

> 
> Looks like it is a bit hidden. f1 is of class(print.survfit), as str(f1) 
> tells us. So let's try getAnyhwere(print.survfit). In the lower part you 
> find line like the following:  
> 
>      x1 <- pfun(nsubjects, stime, surv, x$n.risk, x$n.event, 
>             x$lower, x$upper)
>         if (is.matrix(x1)) {
>             if (is.null(x$lower)) 
>                 dimnames(x1) <- list(NULL, plab)
>             else dimnames(x1) <- list(NULL, c(plab, plab2))
>         }
>         else {
>             if (is.null(x$lower)) 
>                 names(x1) <- plab
>             else names(x1) <- c(plab, plab2)
>         }
>         if (show.rmean) 
>             print(x1)
>  
> Make a copy of that function under a new name, and return x1. 
> 
> Dieter
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ahenningsen at email.uni-kiel.de  Sun Dec  9 14:28:17 2007
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Sun, 9 Dec 2007 14:28:17 +0100
Subject: [R] Obtaining names of further arguments ('...')
Message-ID: <200712091428.18031.ahenningsen@email.uni-kiel.de>

Hi,

I would like to obtain the names of all objects that are provided as further 
arguments ("...") in a function call. I have created a minimal example that 
illustrates my wish (but isn't useful otherwise):

f1 <- function( ... ) return( deparse( substitute( ... ) ) )

x1 <- 1
x2 <- 2
x3 <- 3

f1( x1, x2, x3 )
[1] "x1"

However, I would like to obtain the names of *all* further arguments, 
i.e. "x1", "x2", and "x3".

Is this possible in R? Does anybody know how to do this?

Thanks,
Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445 or +49-4349-914871
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/


From murdoch at stats.uwo.ca  Sun Dec  9 15:09:57 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 09 Dec 2007 09:09:57 -0500
Subject: [R] Obtaining names of further arguments ('...')
In-Reply-To: <200712091428.18031.ahenningsen@email.uni-kiel.de>
References: <200712091428.18031.ahenningsen@email.uni-kiel.de>
Message-ID: <475BF735.9010400@stats.uwo.ca>

On 09/12/2007 8:28 AM, Arne Henningsen wrote:
> Hi,
> 
> I would like to obtain the names of all objects that are provided as further 
> arguments ("...") in a function call. I have created a minimal example that 
> illustrates my wish (but isn't useful otherwise):
> 
> f1 <- function( ... ) return( deparse( substitute( ... ) ) )
> 
> x1 <- 1
> x2 <- 2
> x3 <- 3
> 
> f1( x1, x2, x3 )
> [1] "x1"
> 
> However, I would like to obtain the names of *all* further arguments, 
> i.e. "x1", "x2", and "x3".

Those are not the names of the arguments, you passed them as unnamed 
arguments.  They are expressions, and could well be things like

f1( x1 + x3, mean(x2), 3)


> 
> Is this possible in R? Does anybody know how to do this?

Use names(list(...)) to get the names of the arguments.  To get what you 
want, use match.call() to get the call in unevaluated form, and then 
process it.  For example,

f2 <- function( ... ) {
   call <- match.call()
   args <- as.list(call)[-1]
   unlist(lapply(args, deparse))
}

 > f2(x1, x2, x3)
[1] "x1" "x2" "x3"
 > f2( x1 + x3, mean(x2), 3)
[1] "x1 + x3"  "mean(x2)" "3"

If your function has arguments other than ... you'll need a bit more 
work to remove them.  They'll end up as the names of the values in the 
final result.

Duncan Murdoch
Duncan Murdoch


From ferri.leberl at gmx.at  Sun Dec  9 15:14:41 2007
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Sun, 09 Dec 2007 15:14:41 +0100
Subject: [R] Arguments become nonnumeric
Message-ID: <1197209681.8063.6.camel@localhost>

Dear Everybody,
Enclosed are the tiny beginning of a program and its output.
As a pity, if I load the image <load("kanal.RData")> the elements of
<de.dd> are non numerical; <de.dd[2]+de.dd[3]> returns <Fehler in
de.dd[2] + de.dd[3] : nicht-numerisches Argument f??r bin??ren Operator>.
How can I keep the numbers numerical?
Thank you in advance.
hopefully,
Mag. Ferri Leberl
-------------- n?chster Teil --------------
#Programm zur Erstellung eines Wasserwegenetzplanes

#St??dte
#TLD.Stadtk??rzel<-c("Stadtname",??Breite,'Breite,??L??nge,'L??nge)
de.dd<-c("Dresden",51,2,13,44)
save.image("~/gmx/allalei/kanal/kanal.RData")

From ligges at statistik.uni-dortmund.de  Sun Dec  9 15:18:58 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 09 Dec 2007 15:18:58 +0100
Subject: [R] R + LaTeX formula
In-Reply-To: <475BE9C0.1090601@vanderbilt.edu>
References: <fjfeue$e75$1@ger.gmane.org>	<1197163489.2955.80.camel@Bellerophon.localdomain>
	<475BE9C0.1090601@vanderbilt.edu>
Message-ID: <475BF952.2000202@statistik.uni-dortmund.de>

I haven't read about R's math-notation facilities in this thread, hence 
my question for Jonas Stein: Have you already looked into ?plotmath ?


Uwe Ligges

Frank E Harrell Jr wrote:
> Marc Schwartz wrote:
>> On Sun, 2007-12-09 at 01:04 +0000, Jonas Stein wrote:
>>> Hi,
>>>
>>> what is actually the best method to include R-plots into LaTeX
>>> documents?
>>> At the moment i use 
>>>
>>> postscript("myplot.eps", width = 12.0, height = 9.0, horizontal =
>>> FALSE, 
>>> onefile = TRUE, paper = "special",encoding = "TeXtext.enc")
>>> plot(foo,bar)
>>> dev.off()  
>>>
>>> But it is a bit unhandy to scale later and its difficult to get nice 
>>> formula in the plots.
>>>
>>> And how should i write formulas on the axis or at specific points? 
>>> Has someone had some effort in exporting plots to pstricks or pictex?
>>>
>>> kind regards and thank you for reading so far,
>> As per the Details section of ?postscript:
>>
>> The postscript produced for a single R plot is EPS (Encapsulated
>> PostScript) compatible, and can be included into other documents, e.g.,
>> into LaTeX, using \includegraphics{<filename>}. For use in this way you
>> will probably want to set horizontal = FALSE, onefile = FALSE, paper =
>> "special". Note that the bounding box is for the device region: if you
>> find the white space around the plot region excessive, reduce the
>> margins of the figure region via par(mar=).
>>
>>
>> In your code above, change 'onefile = TRUE' to 'onefile = FALSE'.
>>
>> For scaling you can use the LaTeX \includegraphics directive along with
>> several height and width arguments, such as:
>>
>>   \includegraphics[width=4in]{myplot.eps}
>>   \includegraphics[height=4in]{myplot.eps}
>>   \includegraphics[scale=0.75]{myplot.eps}
>>   \includegraphics[width=0.4\textwidth]{myplot.eps}
>>
>> You might want to review the following document:
>>
>>   ftp://ftp.tex.ac.uk/tex-archive/info/epslatex.pdf 
>>
>> For including formulae in R plots, see ?plotmath. You can run
>> example(plotmath) and there are many posts in the r-help archives on
>> this.
>>
>> Beyond the above, you may want to look into using SWeave, whereby you
>> can create entire documents, with nicely formatted tables and plots
>> directly from R code. More information here:
>>
>>   http://www.ci.tuwien.ac.at/~leisch/Sweave/
>>
>> There are also a couple of articles on R News:
>>
>> Friedrich Leisch. Sweave, part I: Mixing R and LATEX. R News,
>> 2(3):28-31, December 2002.
>>
>> Friedrich Leisch. Sweave, part II: Package vignettes. R News,
>> 3(2):21-24, October 2003.
>>
>> HTH,
>>
>> Marc Schwartz
> 
> In addition to Marc's excellent summary (as usual) please see 
> http://biostat.mc.vanderbilt.edu/SgraphicsHints and especially the link 
> about putting LaTeX typesetting inside graphics (which requires Perl).
> 
> Frank
>


From jholtman at gmail.com  Sun Dec  9 16:11:39 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 9 Dec 2007 07:11:39 -0800
Subject: [R] Arguments become nonnumeric
In-Reply-To: <1197209681.8063.6.camel@localhost>
References: <1197209681.8063.6.camel@localhost>
Message-ID: <644e1f320712090711r7b3edcbod329c2946230147c@mail.gmail.com>

You program contains:

de.dd<-c("Dresden",51,2,13,44)

and since you are mixing characters with numerics and putting them
into a single vector, the data is coerced to a common data type, which
in this case is numeric.  What are you trying to do?  Do you want to
use a list?

> de.dd
[1] "Dresden" "51"      "2"       "13"      "44"
> de.dd<-list("Dresden",51,2,13,44)
> de.dd
[[1]]
[1] "Dresden"

[[2]]
[1] 51

[[3]]
[1] 2

[[4]]
[1] 13

[[5]]
[1] 44

A list will maintain the mode of each element.

On Dec 9, 2007 6:14 AM, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> Dear Everybody,
> Enclosed are the tiny beginning of a program and its output.
> As a pity, if I load the image <load("kanal.RData")> the elements of
> <de.dd> are non numerical; <de.dd[2]+de.dd[3]> returns <Fehler in
> de.dd[2] + de.dd[3] : nicht-numerisches Argument f?r bin?ren Operator>.
> How can I keep the numbers numerical?
> Thank you in advance.
> hopefully,
> Mag. Ferri Leberl
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Sun Dec  9 16:12:21 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 9 Dec 2007 07:12:21 -0800
Subject: [R] Arguments become nonnumeric
In-Reply-To: <1197209681.8063.6.camel@localhost>
References: <1197209681.8063.6.camel@localhost>
Message-ID: <644e1f320712090712h702de639w2ffe26eb0cbf33b4@mail.gmail.com>

Meant to say coerced to "character".

See "?c"

On Dec 9, 2007 6:14 AM, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> Dear Everybody,
> Enclosed are the tiny beginning of a program and its output.
> As a pity, if I load the image <load("kanal.RData")> the elements of
> <de.dd> are non numerical; <de.dd[2]+de.dd[3]> returns <Fehler in
> de.dd[2] + de.dd[3] : nicht-numerisches Argument f?r bin?ren Operator>.
> How can I keep the numbers numerical?
> Thank you in advance.
> hopefully,
> Mag. Ferri Leberl
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ligges at statistik.uni-dortmund.de  Sun Dec  9 16:34:51 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 09 Dec 2007 16:34:51 +0100
Subject: [R] Barchart, Pareto
In-Reply-To: <49379889C4CA4940A9E1469DCFB9AD8504ADD7@GSBHEX1V.gsb.uchicago.edu>
References: <49379889C4CA4940A9E1469DCFB9AD8504ADD7@GSBHEX1V.gsb.uchicago.edu>
Message-ID: <475C0B1B.5010701@statistik.uni-dortmund.de>



Kapoor, Bharat wrote:
> Hello
>  
> Well I am relatively new so some of these issues may not fall under the subject that I have used.
>  
> 1. How do I do a  Pareto. Following is the approach I took.
>  
> My data looks like this
> df2_9
>    Reaason.for.failure Frequency
> 
> 1             Phy Conn        1
> 
> 2        Power failure        3
> 
> 3      Server software        29
> 
> 4      Server hardware        2
> 
> 5    Server out of mem        32
> 
> 6 Inadequate bandwidth        1
> 
>  
> I modified the data using:
>> df <- df2_9[order(df2_9$Frequency,decreasing=TRUE),]
>> x$Percent <- 100*(x$Frequency)/(sum(x$Frequency))
> 
>   ReasonCode Frequency             ROF   Percent
> 
> 5        OOM        32  Out of Momeory 47.058824
> 
> 3         SS        29 Server Software 42.647059
> 
> 2         PF         3   Power failure  4.411765
> 
> 4         SH         2      Server h/w  2.941176
> 
> 1         PC         1   Physical Conn  1.470588
> 
> 6         IB         1      Insuff b/w  1.470588
> 
>  
> 
> barplot(x$Percent, space=0, names.arg=x$ReasonCode, main="Pareto", legend.text=x$ReasonCode, ylab="% error")
> 
>  
> 
> Here are my questions:
> 
> 1.    I could not print the data in ROF column below the bar charts,  How to get full labels for each bar insted I createda  new coumn ReasonCode just to fit below the bars.

Well, either reduce the size of that kind of text (see "cex.axis" in 
?par) or enlarge the corresponding margins (see "mar" in ?par)

> 2.     Can I color code each bar differently


See "col" in ?barplot


> 3.     How to set limit on the y-axis - by deafult it shows 40, I want it show Y-axis till 50?

See "ylim" in ?barplot


> 4.     How to display value of each bar inside or on top of it?

See the values barplot() returns and combine them with a call to text().


Can you please read the help pages (and the posting guide) before 
posting? At least two of your questions are directly answered in 
?barplot and the others by a help page ?barplot refers to: ?par.

Uwe Ligges



>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cgenolin at u-paris10.fr  Sun Dec  9 18:21:47 2007
From: cgenolin at u-paris10.fr (Christophe Genolini)
Date: Sun, 09 Dec 2007 18:21:47 +0100
Subject: [R] Oriented object programming
In-Reply-To: <mailman.15.1197198003.30997.r-help@r-project.org>
References: <mailman.15.1197198003.30997.r-help@r-project.org>
Message-ID: <475C242B.1060308@u-paris10.fr>

Hi all

Two questions:
 - I would like to learn more on oriented object programming with R. Is 
there any tutorial for that?
-  Without waking up a troll, I am not very familiar with diffusion 
list, I am more use to forum. On a diffusion list, how can I check if 
someone already asks a question? I mean, I went on CRAN-R website, I 
find the R mailing list archive. But is there a way to search some key 
word like "object programming"?

Thanks for your help


Christophe


From vdemart1 at tin.it  Sun Dec  9 18:33:26 2007
From: vdemart1 at tin.it (vittorio)
Date: Sun, 9 Dec 2007 18:33:26 +0100
Subject: [R] Setting the grid of a graph of  timeseries
Message-ID: <200712091833.27313.vdemart1@tin.it>

I have the following code

####################################################################

library(zoo)

miedate <- yearmon((2006)+seq(0,23)/12)
tab <- zoo(cbind(A = c(79.47, 89.13, 84.86, 75.68, 72.82, 78.87, 93.46,
78.18, 82.46, 77.25, 80.95, 84.39, 81.7, 74.76, 65.29, 60.3,
66.59, 73.19, 92.39, 65.76, 77.45, 74.22, 101.36, 100.01), B = c(77.95,
76.73, 51.2, 51.86, 51.29, 49.45, 53.88, 47.96, 55.07, 45.34,
37.07, 37.53, 47.79, 37.5, 30.35, 37.78, 34.13, 39.14, 39.89,
35.46, 36.54, 38.39, 47.33, 45.34)),miedate)

par(mai=c(0.75, 0.75, 0.1, 0.1))
my.panel <- function(...) {
   fmt <- "%b-%Y" # format for axis labels
   lines(...)
   panel.number <- parent.frame()$panel.number
   # if bottom panel
   if (!length(panel.number) || panel.number == NCOL(tab)) { 
      # next line only if non-labelled ticks wanted for each point
      axis(1, at = time(tab), lab = FALSE)
      ix <- seq(1, length(tab), 3)
      labs <- format(time(tab), fmt)
      axis(1, at = time(tab)[ix], lab = labs[ix], tcl = -0.7, cex.axis = 0.9)
   }
}

plot.zoo(tab, plot.type="single", panel=my.panel, lty=c(1,2,3,4,5,1), 
lwd=c(8,2,2,2,2,8),           
col=c("darkorange", "red2","darkblue","darkblue","green","midnightblue"), 
            xlab=" ",ylab = "euro/MWh",xaxt="n")

####################################################################

This code produces a graph with a non-decimal x-axis but by far more (human) 
readable because ticks corresponds to months exactly (and not to a weird 
decimal ticking!).
I would like to add a vertical grid with the same criterium.

How can I obtain this?

Ciao
Vittorio


From murdoch at stats.uwo.ca  Sun Dec  9 18:40:45 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 09 Dec 2007 12:40:45 -0500
Subject: [R] Oriented object programming
In-Reply-To: <475C242B.1060308@u-paris10.fr>
References: <mailman.15.1197198003.30997.r-help@r-project.org>
	<475C242B.1060308@u-paris10.fr>
Message-ID: <475C289D.3060009@stats.uwo.ca>

On 09/12/2007 12:21 PM, Christophe Genolini wrote:
> Hi all
> 
> Two questions:
>  - I would like to learn more on oriented object programming with R. Is 
> there any tutorial for that?

The usual reference is the book

  _Programming with Data_ (John M. Chambers, Springer, 1998)

I don't know of an online tutorial, but I wouldn't be surprised if one 
exists somewhere.

> -  Without waking up a troll, I am not very familiar with diffusion 
> list, I am more use to forum. On a diffusion list, how can I check if 
> someone already asks a question? I mean, I went on CRAN-R website, I 
> find the R mailing list archive. But is there a way to search some key 
> word like "object programming"?

Within R, RSiteSearch("object programming") will search a collection of 
documentation and mailing list archives.  (You can narrow the search on 
the results page, or by using some of the optional args to RSiteSearch.)

Duncan Murdoch


From milton_ruser at yahoo.com.br  Sun Dec  9 18:55:34 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sun, 9 Dec 2007 09:55:34 -0800 (PST)
Subject: [R] spatstat questions
Message-ID: <795360.6412.qm@web56006.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071209/130cf9d6/attachment.pl 

From markleeds at verizon.net  Sun Dec  9 18:56:41 2007
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sun, 09 Dec 2007 11:56:41 -0600 (CST)
Subject: [R] Oriented object programming
Message-ID: <3220056.16622821197223001894.JavaMail.root@vms124.mailsrvcs.net>

>From: Duncan Murdoch <murdoch at stats.uwo.ca>
>Date: 2007/12/09 Sun AM 11:40:45 CST
>To: Christophe Genolini <cgenolin at u-paris10.fr>
>Cc: r-help at r-project.org
>Subject: Re: [R] Oriented object programming

Roger Peng has some objected oriented documentation
at his site as part of a powerpoint presentation.
There is also some discussion at the end of John Fox's CAR book, I think in the appendix. MASS has some also
but I forget the chapter in MASS that focuses on it. 
I think that all three are useful but the level of detail varies in each.











>On 09/12/2007 12:21 PM, Christophe Genolini wrote:
>> Hi all
>> 
>> Two questions:
>>  - I would like to learn more on oriented object programming with R. Is 
>> there any tutorial for that?
>
>The usual reference is the book
>
>  _Programming with Data_ (John M. Chambers, Springer, 1998)
>
>I don't know of an online tutorial, but I wouldn't be surprised if one 
>exists somewhere.
>
>> -  Without waking up a troll, I am not very familiar with diffusion 
>> list, I am more use to forum. On a diffusion list, how can I check if 
>> someone already asks a question? I mean, I went on CRAN-R website, I 
>> find the R mailing list archive. But is there a way to search some key 
>> word like "object programming"?
>
>Within R, RSiteSearch("object programming") will search a collection of 
>documentation and mailing list archives.  (You can narrow the search on 
>the results page, or by using some of the optional args to RSiteSearch.)
>
>Duncan Murdoch
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Sun Dec  9 19:13:54 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 9 Dec 2007 13:13:54 -0500
Subject: [R] Oriented object programming
In-Reply-To: <475C242B.1060308@u-paris10.fr>
References: <mailman.15.1197198003.30997.r-help@r-project.org>
	<475C242B.1060308@u-paris10.fr>
Message-ID: <971536df0712091013x3877ccbdud127a80406c6de34@mail.gmail.com>

On Dec 9, 2007 12:21 PM, Christophe Genolini <cgenolin at u-paris10.fr> wrote:
> Hi all
>
> Two questions:
>  - I would like to learn more on oriented object programming with R. Is
> there any tutorial for that?
> -  Without waking up a troll, I am not very familiar with diffusion
> list, I am more use to forum. On a diffusion list, how can I check if
> someone already asks a question? I mean, I went on CRAN-R website, I
> find the R mailing list archive. But is there a way to search some key
> word like "object programming"?
>

There are two OO models in R called S3 and S4.  S3 is
inspired by the Dylan language and knowing it is essential to using R.
The manuals and possibly some of the contributed documentation (google
for CRAN contributed documentation) will have info on those although I am
not sure whether it will be sufficient.

The CRAN package zoo uses S3 and the CRAN package its uses S4 and they
have roughly
the same purpose so you can use them as examples by downloading their source.
http://cran.r-project.org/src/contrib/Descriptions/zoo.html
http://cran.r-project.org/src/contrib/Descriptions/its.html

There are also two addon packages that support different OO models.  The proto
package supports the prototype model (also known as the object-based model)
and is inspired by the Self programming language.  The R.oo package supports
a more conventional model similar to that in most other mainstream languages.
The home pages of these two packages are given in their CRAN descriptions:
http://cran.r-project.org/src/contrib/Descriptions/R.oo.html
http://cran.r-project.org/src/contrib/Descriptions/proto.html

You can use the RSiteSearch function from within R to search the
r-help archives and documentation.
or you can browse r-help and r-devel archives here or via gmane or nabble
https://stat.ethz.ch/pipermail/r-help
https://stat.ethz.ch/pipermail/r-devel
Googling with r-help, r-devel or r-project.org or even just R as one
of the words will often
locate material.


From Roger.Bivand at nhh.no  Sun Dec  9 19:17:04 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 9 Dec 2007 19:17:04 +0100 (CET)
Subject: [R] [R-sig-Geo] spatstat questions
In-Reply-To: <795360.6412.qm@web56006.mail.re3.yahoo.com>
References: <795360.6412.qm@web56006.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.64.0712091907370.19084@reclus.nhh.no>

On Sun, 9 Dec 2007, Milton Cezar Ribeiro wrote:

> Hi all,
>
> Is there a mailing list for spatstat R package?

The website is http://www.spatstat.org (seems to be down for me, but 
that's probably me); the maintainers also read this list.

> Another question: Can I find some ebooks on Spatial Pattern Analysis to 
> be downloaded?
>
> Finally, how can I use a shapefile as "mask" on spatstat? I read the 
> polygon using maptools::read.shape function and now I would like to use 
> this shape as mask to define the boundary of my point pattern data.

Don't use read.shape(), it returns a raw object that is difficult to 
handle. An example with readShapePoly:

library(maptools)
library(spatstat)
xx <- readShapePoly(system.file("shapes/sids.shp", package="maptools")[1])
.spatstat_check <- FALSE
NC <- as(as(xx, "SpatialPolygons"), "owin")
plot(NC)
class(NC)
pts <- runifpoint(1000, NC)
plot(pts, pch=16, cex=0.5, add=TRUE)

The .spatstat_check is to turn off topology checking - please use only 
when necessary. In general the internal structures expect polygons not to 
touch or intersect, but the North Carolina counties do touch, so I have 
turned off the check.

There are more notes on Dylan Beaudette's site at:

http://casoilresource.lawr.ucdavis.edu/drupal/node/319

Hope this helps,

Roger

>
> Kind regards
>
> Miltinho
> Brazil.
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ggrothendieck at gmail.com  Sun Dec  9 19:50:11 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 9 Dec 2007 13:50:11 -0500
Subject: [R] Setting the grid of a graph of timeseries
In-Reply-To: <200712091833.27313.vdemart1@tin.it>
References: <200712091833.27313.vdemart1@tin.it>
Message-ID: <971536df0712091050p682cc92bi679bbfad3869b404@mail.gmail.com>

- You don't need a panel function since there is only one.
- There should not be more than 2 components on lty and lwd
since tab has two columns and that is the cause of the
warning message you are seeing.
- with the recent version of zoo plot.type uses screen so
you can write the shorter screen = 1 instead of the lengthier
plot.type = "single"

Redone with an abline at the end to give the grid lines gives:

plot(tab, screen = 1, lty = c(1,2), lwd = c(8,2),
	col = c("darkorange", "red2"),
	xlab = " ", ylab = "euro/MWh", xaxt = "n")

fmt <- "%b-%Y" # format for axis labels
axis(1, at = time(tab), lab = FALSE)
ix <- seq(1, length(tab), 3)
labs <- format(time(tab), fmt)
axis(1, at = time(tab)[ix], lab = labs[ix], tcl = -0.7, cex.axis = 0.9)
abline(v = time(tab)[ix], lty = 2, col = "grey")

On Dec 9, 2007 12:33 PM, vittorio <vdemart1 at tin.it> wrote:
> I have the following code
>
> ####################################################################
>
> library(zoo)
>
> miedate <- yearmon((2006)+seq(0,23)/12)
> tab <- zoo(cbind(A = c(79.47, 89.13, 84.86, 75.68, 72.82, 78.87, 93.46,
> 78.18, 82.46, 77.25, 80.95, 84.39, 81.7, 74.76, 65.29, 60.3,
> 66.59, 73.19, 92.39, 65.76, 77.45, 74.22, 101.36, 100.01), B = c(77.95,
> 76.73, 51.2, 51.86, 51.29, 49.45, 53.88, 47.96, 55.07, 45.34,
> 37.07, 37.53, 47.79, 37.5, 30.35, 37.78, 34.13, 39.14, 39.89,
> 35.46, 36.54, 38.39, 47.33, 45.34)),miedate)
>
> par(mai=c(0.75, 0.75, 0.1, 0.1))
> my.panel <- function(...) {
>   fmt <- "%b-%Y" # format for axis labels
>   lines(...)
>   panel.number <- parent.frame()$panel.number
>   # if bottom panel
>   if (!length(panel.number) || panel.number == NCOL(tab)) {
>      # next line only if non-labelled ticks wanted for each point
>      axis(1, at = time(tab), lab = FALSE)
>      ix <- seq(1, length(tab), 3)
>      labs <- format(time(tab), fmt)
>      axis(1, at = time(tab)[ix], lab = labs[ix], tcl = -0.7, cex.axis = 0.9)
>   }
> }
>
> plot.zoo(tab, plot.type="single", panel=my.panel, lty=c(1,2,3,4,5,1),
> lwd=c(8,2,2,2,2,8),
> col=c("darkorange", "red2","darkblue","darkblue","green","midnightblue"),
>            xlab=" ",ylab = "euro/MWh",xaxt="n")
>
> ####################################################################
>
> This code produces a graph with a non-decimal x-axis but by far more (human)
> readable because ticks corresponds to months exactly (and not to a weird
> decimal ticking!).
> I would like to add a vertical grid with the same criterium.
>
> How can I obtain this?
>
> Ciao
> Vittorio
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ecjbosu at aol.com  Sun Dec  9 19:51:45 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sun, 09 Dec 2007 12:51:45 -0600
Subject: [R] seq_len
In-Reply-To: <95537B63-9627-4DC0-80FB-0B91F04253DE@gmail.com>
References: <fjdc1i$9me$1@ger.gmane.org>
	<95537B63-9627-4DC0-80FB-0B91F04253DE@gmail.com>
Message-ID: <475C3941.9070005@aol.com>

Charilaos Skiadas wrote:
>
> On Dec 8, 2007, at 1:02 AM, Joe W. Byers wrote:
>
>> In a post on R-devel, Prof Ripley add the following comment
>> | > BTW, 1:dim(names)[1] is dangerous: it could be 1:0.  That was the
>> | > motivation for seq_len.
>>
>> I use the dim(names)[1] and dim(x)[2] along with length(x) with varying
>> levels of frustration depending on the object which I am trying to get
>> the dimensions.  I found the reference to seq_len interesting since it
>> is a function that I have never seen (probably just missed it reading
>> the docs).
>>
>> I was hoping someone could expand on the benefits of seq_len.
>
> I think that example says it all. But in simpler form, suppose x is a 
> vector, and you want to produce a regular sequence of integers of the 
> same length. What should happen i the vector x has length 0? Here's 
> the output of the two commands.
>
> x<-numeric(0)
> > y<-length(x)
> > y
> [1] 0
> > 1:y
> [1] 1 0
> > seq_len(y)
> integer(0)
>
> Other than treating the edge case correctly, the only other advantage 
> of seq_len, that I am aware of, is that it is faster. Not sure how 
> often that ends up mattering though.
>
>> Happy Holidays
>> Joe
>>
>
> Haris Skiadas
> Department of Mathematics and Computer Science
> Hanover College
>
>
>
>

The essence of this feature is to get the correct index sequences when 
performing matrix lookups and loops without causing errors in the 
matrices or subscripts. If I build the correct x=seq_len or 
x=seq(along=) and use the 'in x' not 'in 1:dim(x)[1]' or 'in 
1:length(x)', my code will execute correctly without the NA/NAN error or 
subscript out of bounds, etc.  Is this correct?

Examples
 > x=numeric()
 > x
numeric(0)
 > for ( i in 1:x) print(i)
Error in 1:x : NA/NaN argument
 > for ( i in x) print(i)
 > x=2
 > for ( i in x) print(i)
[1] 2
 > seq_len(x)
[1] 1 2
 > x=seq_len(x)
 > for ( i in x) print(i)
[1] 1
[1] 2
 > for ( i in 1:x) print(i)
[1] 1
Warning message:
In 1:x : numerical expression has 2 elements: only the first used
 > for ( i in x) print(i)
[1] 1
[1] 2
 > x=2
 > for ( i in 1:x) print(i)
[1] 1
[1] 2
 > for ( i in x) print(i)
[1] 2
 > for (i in 1:length(x)) print(i)
[1] 1
 >


Thank you
Joe


From maura.monville at gmail.com  Sun Dec  9 20:08:40 2007
From: maura.monville at gmail.com (Maura E Monville)
Date: Sun, 9 Dec 2007 13:08:40 -0600
Subject: [R] package "growth" ... where is it ?
In-Reply-To: <1197162765.2955.68.camel@Bellerophon.localdomain>
References: <36d691950712081521v4525b741pbb88ef561a8bed90@mail.gmail.com>
	<36d691950712081642k738bd184o172a39d7ef2b179a@mail.gmail.com>
	<1197162765.2955.68.camel@Bellerophon.localdomain>
Message-ID: <36d691950712091108w65289b67i5edb860909cad809@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071209/0e200f7c/attachment.pl 

From Ted.Harding at manchester.ac.uk  Sun Dec  9 20:15:12 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Sun, 09 Dec 2007 19:15:12 -0000 (GMT)
Subject: [R] Adding info from summary(lm(...)) to plot
Message-ID: <XFMail.071209191512.Ted.Harding@manchester.ac.uk>

Hi Folks,

Say I have 2 continuous variables X,Y.

I can of course plot (X,Y) with

  plot(X,Y,pch="+",col="blue")

say, and add the regression line from lm(Y~X)
by extracting the coefficients 'a' of Intercept
and 'b' of X from Y.lm <- lm(Y~X).

Now, however, I want to have not only a general
explanatory title such as

  main="Plot of Y against X"

but also (and ideally just under the main title
and above the figure region of the plot) the
result of

  round(summary(Y.lm)[[4]],digits=3)

exactly as shown below:

            Estimate Std. Error t value Pr(>|t|)
(Intercept)   78.636      0.273 287.902    0.000
X             -0.074      0.029  -2.527    0.012

It would also be OK to have it as a "subtitle"
below the xlabel (which by default is X" in this case).

I've been trying to work out how to so this simply,
but without much success. I can see a complicated
way, which involves binding the row and column names,
and the values, of the above output, using paste()
along with "\n" at suitable places, but I'd like to
have something much less intricate than that!

I'd also like to be able to integrate the solution
nicely with the main title (or the X label) so that
there's no overlap.

Any suggestions?

with thanks,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 09-Dec-07                                       Time: 19:15:08
------------------------------ XFMail ------------------------------


From marc_schwartz at comcast.net  Sun Dec  9 20:21:12 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sun, 09 Dec 2007 13:21:12 -0600
Subject: [R] package "growth" ... where is it ?
In-Reply-To: <36d691950712091108w65289b67i5edb860909cad809@mail.gmail.com>
References: <36d691950712081521v4525b741pbb88ef561a8bed90@mail.gmail.com>
	<36d691950712081642k738bd184o172a39d7ef2b179a@mail.gmail.com>
	<1197162765.2955.68.camel@Bellerophon.localdomain>
	<36d691950712091108w65289b67i5edb860909cad809@mail.gmail.com>
Message-ID: <1197228072.20795.21.camel@Bellerophon.localdomain>

On Sun, 2007-12-09 at 13:08 -0600, Maura E Monville wrote:
> Thank you. I have downloaded the compressed package "growth-1.tgz". I
> have extracted the archive. But all my attempts to install such a
> package so that I can call its functions from R command line ... have
> failed. 
> It is not clear to me how to operate the "install.packages" command
> where the package has been already downloaded.
> I  tried different parameters, and repos=NULL, but I could not
> succeed.
> What is the pitfall here ? 
> 
> Thank you very much,
> Maura

Maura,

What operating system are you on?  If you are on Windows, you will want
the "compiled" version of the package, which is the .zip file.  The .tgz
archive is a "source" version of the package, which can be used for
example, under Linux.

In either case, you DO NOT want to extract the archive to it's
constituent components. You want to directly install the archive file.

If on Windows, you should use something like:

  install.packages("PathTo/growth.zip")

or install directly from Jim's web site:

 install.packages("http://popgen.unimaas.nl/~jlindsey/rcode/growth.zip")

Similary, if you are on Linux, you can use the same commands, just
replace the .zip with .tgz.  However, you will need 'root' access to do
this, unless you are installing to a local library tree.

This is covered in the R manuals and in the R FAQs.

HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Sun Dec  9 20:32:54 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 9 Dec 2007 14:32:54 -0500
Subject: [R] Adding info from summary(lm(...)) to plot
In-Reply-To: <XFMail.071209191512.Ted.Harding@manchester.ac.uk>
References: <XFMail.071209191512.Ted.Harding@manchester.ac.uk>
Message-ID: <971536df0712091132o6fb85dfeo21d27b051cc4b936@mail.gmail.com>

If x <- round(...whatever...) try:

title(capture.output(print(x)))


On Dec 9, 2007 2:15 PM, Ted Harding <Ted.Harding at manchester.ac.uk> wrote:
> Hi Folks,
>
> Say I have 2 continuous variables X,Y.
>
> I can of course plot (X,Y) with
>
>  plot(X,Y,pch="+",col="blue")
>
> say, and add the regression line from lm(Y~X)
> by extracting the coefficients 'a' of Intercept
> and 'b' of X from Y.lm <- lm(Y~X).
>
> Now, however, I want to have not only a general
> explanatory title such as
>
>  main="Plot of Y against X"
>
> but also (and ideally just under the main title
> and above the figure region of the plot) the
> result of
>
>  round(summary(Y.lm)[[4]],digits=3)
>
> exactly as shown below:
>
>            Estimate Std. Error t value Pr(>|t|)
> (Intercept)   78.636      0.273 287.902    0.000
> X             -0.074      0.029  -2.527    0.012
>
> It would also be OK to have it as a "subtitle"
> below the xlabel (which by default is X" in this case).
>
> I've been trying to work out how to so this simply,
> but without much success. I can see a complicated
> way, which involves binding the row and column names,
> and the values, of the above output, using paste()
> along with "\n" at suitable places, but I'd like to
> have something much less intricate than that!
>
> I'd also like to be able to integrate the solution
> nicely with the main title (or the X label) so that
> there's no overlap.
>
> Any suggestions?
>
> with thanks,
> Ted.
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 09-Dec-07                                       Time: 19:15:08
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Sun Dec  9 20:33:13 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sun, 09 Dec 2007 13:33:13 -0600
Subject: [R] Adding info from summary(lm(...)) to plot
In-Reply-To: <XFMail.071209191512.Ted.Harding@manchester.ac.uk>
References: <XFMail.071209191512.Ted.Harding@manchester.ac.uk>
Message-ID: <1197228793.20795.30.camel@Bellerophon.localdomain>


On Sun, 2007-12-09 at 19:15 +0000, Ted.Harding at manchester.ac.uk wrote:
> Hi Folks,
> 
> Say I have 2 continuous variables X,Y.
> 
> I can of course plot (X,Y) with
> 
>   plot(X,Y,pch="+",col="blue")
> 
> say, and add the regression line from lm(Y~X)
> by extracting the coefficients 'a' of Intercept
> and 'b' of X from Y.lm <- lm(Y~X).
> 
> Now, however, I want to have not only a general
> explanatory title such as
> 
>   main="Plot of Y against X"
> 
> but also (and ideally just under the main title
> and above the figure region of the plot) the
> result of
> 
>   round(summary(Y.lm)[[4]],digits=3)
> 
> exactly as shown below:
> 
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)   78.636      0.273 287.902    0.000
> X             -0.074      0.029  -2.527    0.012
> 
> It would also be OK to have it as a "subtitle"
> below the xlabel (which by default is X" in this case).
> 
> I've been trying to work out how to so this simply,
> but without much success. I can see a complicated
> way, which involves binding the row and column names,
> and the values, of the above output, using paste()
> along with "\n" at suitable places, but I'd like to
> have something much less intricate than that!
> 
> I'd also like to be able to integrate the solution
> nicely with the main title (or the X label) so that
> there's no overlap.
> 
> Any suggestions?
> 
> with thanks,
> Ted.

Ted, 

For plotting the fitted regression line, you can use:

  abline(ModelObject)

rather than extracting the coefs, etc.

See ?abline for more information.  There are also examples
in ?predict.lm using matplot()/matlines() for adding the
confidence/prediction intervals.

In terms of adding the model summary table, you might want to consider
doing this within a legend box using a monospace font. There was a
thread on this a couple of years back on r-help:

  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/58410.html

You may wish to review the full thread to get a sense for the evolution
of the solutions posed.

HTH,

Marc Schwartz


From rinamiehs at gmail.com  Sun Dec  9 21:04:50 2007
From: rinamiehs at gmail.com (Rina Oldager Miehs)
Date: Sun, 9 Dec 2007 21:04:50 +0100
Subject: [R] for (i in...)
Message-ID: <132b669b0712091204o30e2d307j36369fbbfb1d3ce@mail.gmail.com>

En indlejret tekst med ukendt tegns?t er blevet fjernet...
Navn: ikke tilg?ngelig
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071209/877bf190/attachment.pl 

From anna.paganoni at polimi.it  Sun Dec  9 21:24:56 2007
From: anna.paganoni at polimi.it (anna.paganoni at polimi.it)
Date: Sun, 09 Dec 2007 21:24:56 +0100
Subject: [R] CART analysis
Message-ID: <20071209212456.zi59tboc8wg4occ4@webmail.polimi.it>

I would like to know if is it possible implemet a partitioning tree  
using a function like rpart, or mvpart, and with formula a glm-object  
(as a logistic models) or a robust linear regression (as least sum of  
absolute errors).
In this case, the appropriate "method" to use is "mrt"? Or another one?
Thanks,
Anna Maria Paganoni


From murdoch at stats.uwo.ca  Sun Dec  9 21:27:59 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 09 Dec 2007 15:27:59 -0500
Subject: [R] for (i in...)
In-Reply-To: <132b669b0712091204o30e2d307j36369fbbfb1d3ce@mail.gmail.com>
References: <132b669b0712091204o30e2d307j36369fbbfb1d3ce@mail.gmail.com>
Message-ID: <475C4FCF.9050703@stats.uwo.ca>

On 09/12/2007 3:04 PM, Rina Oldager Miehs wrote:
> Hey
> 
> do anyone know why this error occurs??
> 
>> for(i in 1:n_trait){
> +    for( j in 1:n_trait){
> +    rG[i,j] <- (G_o[i,j]/(sqrt(G_o[i,i]%*%G_o[j,j]))
> +    rP[i,j] <- (P_o[i,j]/(sqrt(P_o[i,i]%*%P_o[j,j]))
> Error: unexpected symbol in:
> "   rG[i,j] <- (G_o[i,j]/(sqrt(G_o[i,i]%*%G_o[j,j]))
>    rP"

Your parentheses don't match on the rG line.  You have 3 lefts and 2 rights.

Duncan Murdoch

>>    rE[i,j] <- (R_o[i,j]/(sqrt(R_o[i,i]%*%R_o[j,j]))
> +    }
> Error: unexpected '}' in:
> "   rE[i,j] <- (R_o[i,j]/(sqrt(R_o[i,i]%*%R_o[j,j]))
>    }"
>> h2[i] <- (G_o[i,i]/P_o[i,i])
>> }
> Error: unexpected '}' in "}"
> 
> If i make parentes around each of the commands this comes instead:
> 
>> for(i in 1:n_trait){
> +    for( j in 1:n_trait){
> +    (rG[i,j] <- (G_o[i,j]/(sqrt(G_o[i,i]%*%G_o[j,j])))
> +    (rP[i,j] <- (P_o[i,j]/(sqrt(P_o[i,i]%*%P_o[j,j])))
> +    (rE[i,j] <- (R_o[i,j]/(sqrt(R_o[i,i]%*%R_o[j,j])))
> +    }
> Error: unexpected '}' in:
> "   (rE[i,j] <- (R_o[i,j]/(sqrt(R_o[i,i]%*%R_o[j,j])))
>    }"
>> h2[i] <- (G_o[i,i]/P_o[i,i])
>> }
> Error: unexpected '}' in "}"
> 
> I dont understand what i am wrigthing wrong because i have a for command
> rigth before this one and that works...
>> for (i in 1:n_trait){
> +    for ( j in 1:n_trait){
> +    R_o[i,j] = P_o[i,j]-G_o[i,j]
> +    }
> + }
> 
> Is it because it is matrices??
> 
> Cincerely Rina
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From francoisetsandrine.mercier at wanadoo.fr  Sun Dec  9 22:22:15 2007
From: francoisetsandrine.mercier at wanadoo.fr (Sandrine-et-Francois)
Date: Sun, 9 Dec 2007 22:22:15 +0100
Subject: [R] Lmer output for negative binomial data
Message-ID: <066701c83aa9$953df9d0$0d01a8c0@Amazone>

Dear R-list,
May I ask for help in interpretating the output of 'lmer' (from the lme4 
package) when dealing with negative binomial data ?

I'm using the functions glm.nb (from the MASS package) and lmer (from the 
lme4) to fit respectively fixed-effects and mixed-effects generalized linear 
models to data, generated from a negative binomial distribution : count ~ 
Neg.Bin (mu, theta). Here is the code:
==============================================================================
#Generate the data frame
set.seed(2153)
mydf<-data.frame(subjs=seq(1:nsubjids),
counts=rnbinom(nsubjids*ntimes, size=0.5, mu=1.8))

#Model
require(MASS); require(lme4)
summary(glm.nb(counts~1, data=mydf))
summary(lmer(counts~1+(1|subjs), 
family=negative.binomial(theta=fixed.nb0$theta), data=mydf))
==============================================================================
The glm.nb output gives : mu=exp(0.5306) and theta=0.513.
I use the theta estimate from glm.nb as input into lmer, and I obtain, 
mu=exp(0.5306).

The output from lmer gives the following for the Random effects:
Random effects:
 Groups   Name        Variance   Std.Dev.
 subjs    (Intercept) 3.5577e-10 1.8862e-05
 Residual             7.1155e-01 8.4353e-01
number of obs: 30, groups: subjs, 10

I interprete the "subjs" component as an individual error term "e" (so, that 
mu=exp(0.5306)*exp(e)) with e~N(0, 3.5577e-10) ? Is this correct ?
What about the 'Residual' term ?

Thanks for your help,
Best regards,
Fran?ois


From ahenningsen at email.uni-kiel.de  Sun Dec  9 22:30:31 2007
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Sun, 9 Dec 2007 22:30:31 +0100
Subject: [R] Obtaining names of further arguments ('...')
In-Reply-To: <475BF735.9010400@stats.uwo.ca>
References: <200712091428.18031.ahenningsen@email.uni-kiel.de>
	<475BF735.9010400@stats.uwo.ca>
Message-ID: <200712092230.39590.ahenningsen@email.uni-kiel.de>

On Sunday 09 December 2007 15:09, Duncan Murdoch wrote
> [...] To get what you
> want, use match.call() to get the call in unevaluated form, and then
> process it.  For example,
>
> f2 <- function( ... ) {
>    call <- match.call()
>    args <- as.list(call)[-1]
>    unlist(lapply(args, deparse))
> }
>
>  > f2(x1, x2, x3)
>
> [1] "x1" "x2" "x3"
>
>  > f2( x1 + x3, mean(x2), 3)
>
> [1] "x1 + x3"  "mean(x2)" "3"
>
> If your function has arguments other than ... you'll need a bit more
> work to remove them.  They'll end up as the names of the values in the
> final result.

This does exactly what I wanted!
@Duncan: Thank you very much for your quick and helpful answer!

Best wishes,
Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445 or +49-4349-914871
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20071209/9b82d8ba/attachment.bin 

From ferri.leberl at gmx.at  Sun Dec  9 22:36:24 2007
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Sun, 09 Dec 2007 22:36:24 +0100
Subject: [R] Arguments become nonnumeric
In-Reply-To: <644e1f320712090711r7b3edcbod329c2946230147c@mail.gmail.com>
References: <1197209681.8063.6.camel@localhost>
	<644e1f320712090711r7b3edcbod329c2946230147c@mail.gmail.com>
Message-ID: <1197236184.5585.18.camel@localhost>

Thank you, this seems promissing. How can I address then a certain
element e.g. to add the second and the third component?
Faithfully,
Mag. Ferri Leberl



Am Sonntag, den 09.12.2007, 07:11 -0800 schrieb jim holtman:
> You program contains:
> 
> de.dd<-c("Dresden",51,2,13,44)
> 
> and since you are mixing characters with numerics and putting them
> into a single vector, the data is coerced to a common data type, which
> in this case is numeric.  What are you trying to do?  Do you want to
> use a list?
> 
> > de.dd
> [1] "Dresden" "51"      "2"       "13"      "44"
> > de.dd<-list("Dresden",51,2,13,44)
> > de.dd
> [[1]]
> [1] "Dresden"
> 
> [[2]]
> [1] 51
> 
> [[3]]
> [1] 2
> 
> [[4]]
> [1] 13
> 
> [[5]]
> [1] 44
> 
> A list will maintain the mode of each element.
> 
> On Dec 9, 2007 6:14 AM, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> > Dear Everybody,
> > Enclosed are the tiny beginning of a program and its output.
> > As a pity, if I load the image <load("kanal.RData")> the elements of
> > <de.dd> are non numerical; <de.dd[2]+de.dd[3]> returns <Fehler in
> > de.dd[2] + de.dd[3] : nicht-numerisches Argument f?r bin?ren Operator>.
> > How can I keep the numbers numerical?
> > Thank you in advance.
> > hopefully,
> > Mag. Ferri Leberl
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> 
> 
>


From juryef at yahoo.com  Sun Dec  9 22:59:50 2007
From: juryef at yahoo.com (Judith Flores)
Date: Sun, 9 Dec 2007 13:59:50 -0800 (PST)
Subject: [R] Saving lattice plot as a PDF
Message-ID: <335995.83858.qm@web34707.mail.mud.yahoo.com>

Hi there,

    I need to save a series of lattice plots as a PDF,
this is my code so far:

windows(height=8,width=6)
plot.new()
library('grid')
lattice.options(layout.heights=list(top.padding=list(x=0.15,
units="inches")))
print(plot1, split=c(1,1,2,3), more=TRUE)
print(plot2, split=c(1,2,2,3), more=TRUE)
print(plot3, split=c(1,3,2,3), more=TRUE)
print(plot4, split=c(2,1,2,3), more=TRUE)
print(plot5, split=c(2,2,2,3), more=TRUE)
print(plot6, split=c(2,3,2,3), more=FALSE)

ltext(grid.locator(), label='text', cex=1.3)
ltext(grid.locator(), label='text', cex=1.3)

And when I open the PDF created I see "text" at the
bottom of may layout, even though I placed it at the
top of the sheet.. If I save it as a metafile of PNG
the layoout is correct.

I have tried trellis.devices, what do I need to do to
retain the laout properties in a PDF format?

Thank you,

Judith






      ____________________________________________________________________________________
Looking for last minute shopping deals?


From alecwang80 at gmail.com  Sun Dec  9 23:45:36 2007
From: alecwang80 at gmail.com (Alex Wang)
Date: Sun, 9 Dec 2007 16:45:36 -0600
Subject: [R] How to read in expressions as function parameters?
Message-ID: <855c8c750712091445r4b5653c1g2bd1ccab61c724b9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071209/d53a4c13/attachment.pl 

From gregdingle at yahoo.com  Sun Dec  9 23:46:58 2007
From: gregdingle at yahoo.com (Greg Dingle)
Date: Sun, 9 Dec 2007 14:46:58 -0800
Subject: [R] cairo and rapache
Message-ID: <D84B878C-CBD3-4473-BB32-E55AF3598FBC@yahoo.com>

I'm trying to get Cairo and RApache to work together on OS X leopard.

Initializing Cairo stops the whole HTTP response. Apache logs show  
this message:

Break on  
__THE_PROCESS_HAS_FORKED_AND_YOU_CANNOT_USE_THIS_COREFOUNDATION_FUNCTIONALITY___YOU_MUST_EXEC__ 
() to debug.
The process has forked and you cannot use this CoreFoundation  
functionality safely. You MUST exec().

This reference seems to explain the problem but it is beyond me how to  
fix it.

http://developer.apple.com/releasenotes/CoreFoundation/CoreFoundation.html

Greg


From jholtman at gmail.com  Mon Dec 10 00:12:26 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 9 Dec 2007 15:12:26 -0800
Subject: [R] Arguments become nonnumeric
In-Reply-To: <1197236184.5585.18.camel@localhost>
References: <1197209681.8063.6.camel@localhost>
	<644e1f320712090711r7b3edcbod329c2946230147c@mail.gmail.com>
	<1197236184.5585.18.camel@localhost>
Message-ID: <644e1f320712091512m182855ban9a57ce7e4a352f28@mail.gmail.com>

Several different ways.  You can look in the Intro to R to learn more:

> my.list <- list(a=matrix(1:9,3), b=1:4, c="this is  a string", d=list(1,2,3))
> my.list
$a
     [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6    9

$b
[1] 1 2 3 4

$c
[1] "this is  a string"

$d
$d[[1]]
[1] 1

$d[[2]]
[1] 2

$d[[3]]
[1] 3


> my.list$a
     [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6    9
> my.list$d
[[1]]
[1] 1

[[2]]
[1] 2

[[3]]
[1] 3

> my.list$d[[2]]
[1] 2
> my.list[[4]][[2]] # same as above
[1] 2
> my.list[[1]] # same as my.list$a
     [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6    9
>


On Dec 9, 2007 1:36 PM, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> Thank you, this seems promissing. How can I address then a certain
> element e.g. to add the second and the third component?
> Faithfully,
> Mag. Ferri Leberl
>
>
>
> Am Sonntag, den 09.12.2007, 07:11 -0800 schrieb jim holtman:
>
> > You program contains:
> >
> > de.dd<-c("Dresden",51,2,13,44)
> >
> > and since you are mixing characters with numerics and putting them
> > into a single vector, the data is coerced to a common data type, which
> > in this case is numeric.  What are you trying to do?  Do you want to
> > use a list?
> >
> > > de.dd
> > [1] "Dresden" "51"      "2"       "13"      "44"
> > > de.dd<-list("Dresden",51,2,13,44)
> > > de.dd
> > [[1]]
> > [1] "Dresden"
> >
> > [[2]]
> > [1] 51
> >
> > [[3]]
> > [1] 2
> >
> > [[4]]
> > [1] 13
> >
> > [[5]]
> > [1] 44
> >
> > A list will maintain the mode of each element.
> >
> > On Dec 9, 2007 6:14 AM, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> > > Dear Everybody,
> > > Enclosed are the tiny beginning of a program and its output.
> > > As a pity, if I load the image <load("kanal.RData")> the elements of
> > > <de.dd> are non numerical; <de.dd[2]+de.dd[3]> returns <Fehler in
> > > de.dd[2] + de.dd[3] : nicht-numerisches Argument f?r bin?ren Operator>.
> > > How can I keep the numbers numerical?
> > > Thank you in advance.
> > > hopefully,
> > > Mag. Ferri Leberl
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> >
> >
> >
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From p_connolly at slingshot.co.nz  Mon Dec 10 00:17:53 2007
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Mon, 10 Dec 2007 12:17:53 +1300
Subject: [R] Fitting large titles in a plot
In-Reply-To: <14193900.post@talk.nabble.com>
References: <14193900.post@talk.nabble.com>
Message-ID: <20071209231753.GO6584@slingshot.co.nz>

On Thu, 06-Dec-2007 at 07:16AM -0800, Svempa wrote:

|> 
|> I want to fit a fairly long main title for a plot, supposedly by changing row
|> after a while. As for now it starts way outside the picture margin at the
|> left and continues way out right passed the right margins.
|> 
|> >plot(A,main="This is my really long title and it's so long that I can see
|> just about half of it.")
|> 
|> Any suggestions? Shouldn't be that hard.

Something like this will probably be close enough:

main="This is my really long title and it's so long that I can see\njust about half of it."

HTH

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From deepayan.sarkar at gmail.com  Mon Dec 10 00:46:15 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sun, 9 Dec 2007 15:46:15 -0800
Subject: [R] Saving lattice plot as a PDF
In-Reply-To: <335995.83858.qm@web34707.mail.mud.yahoo.com>
References: <335995.83858.qm@web34707.mail.mud.yahoo.com>
Message-ID: <eb555e660712091546g7e39eb80j230195295e3ec659@mail.gmail.com>

On 12/9/07, Judith Flores <juryef at yahoo.com> wrote:
> Hi there,
>
>     I need to save a series of lattice plots as a PDF,
> this is my code so far:
>
> windows(height=8,width=6)
> plot.new()
> library('grid')
> lattice.options(layout.heights=list(top.padding=list(x=0.15,
> units="inches")))
> print(plot1, split=c(1,1,2,3), more=TRUE)
> print(plot2, split=c(1,2,2,3), more=TRUE)
> print(plot3, split=c(1,3,2,3), more=TRUE)
> print(plot4, split=c(2,1,2,3), more=TRUE)
> print(plot5, split=c(2,2,2,3), more=TRUE)
> print(plot6, split=c(2,3,2,3), more=FALSE)
>
> ltext(grid.locator(), label='text', cex=1.3)
> ltext(grid.locator(), label='text', cex=1.3)
>
> And when I open the PDF created I see "text" at the
> bottom of may layout, even though I placed it at the
> top of the sheet.. If I save it as a metafile of PNG
> the layoout is correct.

I suspect the ``native'' coordinate system (which is all ltext() will
know about) is different for PDF. The following might work as
alternative for the ltext calls.

grid.text(label = "text",
          cex = 1.3,
          vp = do.call(viewport, grid.locator(unit="npc")))

-Deepayan


From lists at revelle.net  Mon Dec 10 01:21:51 2007
From: lists at revelle.net (William Revelle)
Date: Sun, 9 Dec 2007 18:21:51 -0600
Subject: [R] for (i in...)
In-Reply-To: <475C4FCF.9050703@stats.uwo.ca>
References: <132b669b0712091204o30e2d307j36369fbbfb1d3ce@mail.gmail.com>
	<475C4FCF.9050703@stats.uwo.ca>
Message-ID: <p06240803c38236772524@[165.124.160.253]>

At 3:27 PM -0500 12/9/07, Duncan Murdoch wrote:
>On 09/12/2007 3:04 PM, Rina Oldager Miehs wrote:
>>  Hey
>>
>>  do anyone know why this error occurs??
>>
>  >> for(i in 1:n_trait){
>>  +    for( j in 1:n_trait){
>>  +    rG[i,j] <- (G_o[i,j]/(sqrt(G_o[i,i]%*%G_o[j,j]))
>>  +    rP[i,j] <- (P_o[i,j]/(sqrt(P_o[i,i]%*%P_o[j,j]))
>>  Error: unexpected symbol in:
>  > "   rG[i,j] <- (G_o[i,j]/(sqrt(G_o[i,i]%*%G_o[j,j]))
>>     rP"
>
>Your parentheses don't match on the rG line.  You have 3 lefts and 2 rights.
>
>Duncan Murdoch


Rina,
   Alternatively, you could avoid the entire double loop by using 
matrix operations.  I show this and then reproduce it using the first 
section of your code:

#how to use matrices to avoid looping
n_trait <- 5
X <- matrix( rnorm(100*n_trait),ncol= n_trait) #create a data matrix
G_o <- t(X) %*% X   #create the G_o matrix as a  matrix of cross products
G_o          #show the cross products

dG <- diag(1/sqrt(diag(G_o)))  #divide by the sqrt of the diagonal
rG <- dG %*% G_o %*% dG
rG                              #show the result
#######################
  #abbreviated Rina code using loops
  for(i in 1:n_trait){
     for( j in 1:n_trait){
  rG[i,j] <- G_o[i,j]/(sqrt(G_o[i,i]%*%G_o[j,j])) }}
rG         #compare to above



Bill

-- 
William Revelle		http://personality-project.org/revelle.html
Professor			http://personality-project.org/personality.html
Department of Psychology             http://www.wcas.northwestern.edu/psych/
Northwestern University	http://www.northwestern.edu/
Use R for statistics:                          http://personality-project.org/r


From john.bullock at aya.yale.edu  Mon Dec 10 01:30:34 2007
From: john.bullock at aya.yale.edu (John G. Bullock)
Date: Sun, 9 Dec 2007 16:30:34 -0800
Subject: [R] lattice: placing y-axis labels on right-hand side of panel when
	relation="sliced"
Message-ID: <fji1d6$igp$1@ger.gmane.org>


Hello,

        I'm using lattice to create a multi-panel figure.  I would like
to draw each panel's y-axis ticks and labels on the right-hand
side of the panel.  Ordinarily, I would do this by specifying
scales=list(y=list(draw=T, alternating=2)).  But in this case, I am
using relation="sliced" to determine the y-axis limits.  So
"alternating" is ignored.  Is there any way -- short of using grid
functions -- to place the y-axis ticks and labels on the right-hand
sides of these panels?

        I checked the R Graph Gallery and the figures to Deepayan's
new book but saw nothing that would help.  I'm using R 2.5.0 and
lattice 0.16-1 on Windows XP.

Thank you,
--John


From juryef at yahoo.com  Mon Dec 10 03:58:37 2007
From: juryef at yahoo.com (Judith Flores)
Date: Sun, 9 Dec 2007 18:58:37 -0800 (PST)
Subject: [R] Viewport and grid.draw
Message-ID: <813919.55955.qm@web34703.mail.mud.yahoo.com>

Hi Deepayan and everyone,

   I need to add a common legend to a group of latice
graphs, I have tried different ways using viewport and
grid.draw without success.

Here is what I have:

plot.new()

library(grid)
library('IDPmisc')

print(plot1, split=c(1,1,2,4), more=TRUE)
print(plot4, split=c(2,1,2,4), more=TRUE)
print(plot2, split=c(1,2,2,4), more=TRUE)
print(plot5, split=c(2,2,2,4), more=TRUE)
print(plot3, split=c(1,3,2,4), more=TRUE)
print(plot6, split=c(1,4,1,4), more=FALSE)
grid.text(vp=do.call(viewport,grid.locator(unit="npc")),
label='A', gp=gpar(fontsize=20))
grid.text(vp=do.call(viewport,grid.locator(unit="npc")),
label='B', gp=gpar(fontsize=20))





key1<-draw.leg(key=list(text=list(c('some text',
'text2')),points=list(pch=c(17,15))))
vp.key<-viewport(grid.locator(unit="npc"))
pushViewport(vp.key)
grid.draw(key1)

grid.locator cannot be one of the arguments of
viewport, but waht would be the analog of this
argument in viewport then?

Thank you,

Judith




      ____________________________________________________________________________________
Be a better friend, newshound, and


From corr at fas.harvard.edu  Mon Dec 10 04:43:13 2007
From: corr at fas.harvard.edu (Anders Schwartz Corr)
Date: Sun, 9 Dec 2007 22:43:13 -0500 (EST)
Subject: [R] Error in read.spss() for .por files
Message-ID: <Pine.LNX.4.64.0712092225210.349@ls02.fas.harvard.edu>

Hi, There are two unanswered reports of this error (below) in read.spss() 
when used with a .por file. I had the same problem and in order to 
successfully read the file into R I downloaded spss, saved the data as 
.dat tab-delimited, and then used read.table(). The point is that spss 
successfully read the same .por data file (the Polity II data freely 
available from ICPSR) that R couldn't read. So, I assume this is a bug in 
R.

> politydata<-read.spss("polityii.por")
Error in read.spss("polityii.por") : error reading portable-file 
dictionary
In addition: Warning message:
Expected variable count record in: read.spss("polityii.por")
>


From deepayan.sarkar at gmail.com  Mon Dec 10 04:51:12 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sun, 9 Dec 2007 19:51:12 -0800
Subject: [R] Viewport and grid.draw
In-Reply-To: <813919.55955.qm@web34703.mail.mud.yahoo.com>
References: <813919.55955.qm@web34703.mail.mud.yahoo.com>
Message-ID: <eb555e660712091951g3b1800a2gf511b0d2fd008c8f@mail.gmail.com>

On Dec 9, 2007 6:58 PM, Judith Flores <juryef at yahoo.com> wrote:
> Hi Deepayan and everyone,
>
>    I need to add a common legend to a group of latice
> graphs, I have tried different ways using viewport and
> grid.draw without success.

Try looking at the first example in

http://dsarkar.fhcrc.org/lattice/book/Chapter12-Interaction/edited.R

for inspiration.

-Deepayan

>
> Here is what I have:
>
> plot.new()
>
> library(grid)
> library('IDPmisc')
>
> print(plot1, split=c(1,1,2,4), more=TRUE)
> print(plot4, split=c(2,1,2,4), more=TRUE)
> print(plot2, split=c(1,2,2,4), more=TRUE)
> print(plot5, split=c(2,2,2,4), more=TRUE)
> print(plot3, split=c(1,3,2,4), more=TRUE)
> print(plot6, split=c(1,4,1,4), more=FALSE)
> grid.text(vp=do.call(viewport,grid.locator(unit="npc")),
> label='A', gp=gpar(fontsize=20))
> grid.text(vp=do.call(viewport,grid.locator(unit="npc")),
> label='B', gp=gpar(fontsize=20))
>
>
>
>
>
> key1<-draw.leg(key=list(text=list(c('some text',
> 'text2')),points=list(pch=c(17,15))))
> vp.key<-viewport(grid.locator(unit="npc"))
> pushViewport(vp.key)
> grid.draw(key1)
>
> grid.locator cannot be one of the arguments of
> viewport, but waht would be the analog of this
> argument in viewport then?
>
> Thank you,
>
> Judith
>
>
>
>
>       ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From k at chicagogsb.edu  Mon Dec 10 05:43:17 2007
From: k at chicagogsb.edu (Kapoor, Bharat )
Date: Sun, 09 Dec 2007 22:43:17 -0600
Subject: [R] Tutorial for Basic Stats
Message-ID: <49379889C4CA4940A9E1469DCFB9AD8504ADDC@GSBHEX1V.gsb.uchicago.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071209/c5a2c637/attachment.ksh 

From p at dirac.org  Mon Dec 10 06:25:32 2007
From: p at dirac.org (caffeine)
Date: Sun, 9 Dec 2007 21:25:32 -0800 (PST)
Subject: [R]  Having trouble getting GARCH parameters (basic/newbie)
Message-ID: <14247536.post@talk.nabble.com>


I'm having no luck getting GARCH parameter estimations.  It seems simple
enough, but I don't know what I'm doing.  I'm a newbie both at R and GARCH
models, so whatever is going wrong, it's probably very basic.  Here's what I
do:

1. I first load the tseries package with:

   library("tseries")

2. I then load the data with:

   g <- read.table("test.csv", header=T)
  
3. It appears to work because I can print the data (a "hello world" type
dataset with only 5 datapoints):

   > g
         Data
   1 0.002337
   2 0.010037
   3 0.007608
   4 0.005620
   5 0.006050

4. Looks good.  So now I try to get the parameters of a fitted GARCH(1,1)
model:

   > gFit <- garch( g, order=c(1,1) )

Here's where the problem is.  R returns:

   ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 

and that's it.  No further output is generated; R just sits there.   My
dataset had 5000 elements, but R just hung there after printing the above
message.   So I cut the dataset down to 100 elements, and then down to just
5 elements.

I'm pretty sure 5 elements should be a snap for R; I think the estimation
would be done in seconds, so something is definitely wrong.

Help?

-- 
View this message in context: http://www.nabble.com/Having-trouble-getting-GARCH-parameters-%28basic-newbie%29-tp14247536p14247536.html
Sent from the R help mailing list archive at Nabble.com.


From guy.brock at louisville.edu  Mon Dec 10 06:25:39 2007
From: guy.brock at louisville.edu (Guy Brock)
Date: Mon, 10 Dec 2007 00:25:39 -0500
Subject: [R] cyclic dependency error
References: <4756DA1B020000310001117A@gwise.louisville.edu>
	<475C878302000031000113BA@gwise.louisville.edu>
Message-ID: <475C8785.26F4.0031.0@gwise.louisville.edu>


Dear all, 
I am encountering a cyclic dependency error when running R CMD check on an R package I wrote (R version 2.6.1, Mac OS X 10.4), see the error message below.    

Creating a new generic function for "print" in "clValid"
Creating a new generic function for "summary" in "clValid"
Creating a new generic function for "plot" in "clValid"
Error in loadNamespace(package, c(which.lib.loc, lib.loc), keep.source = keep.source) : 
        cyclic name space dependencies are not supported
Error : package/namespace load failed for 'clValid'
Error: unable to load R code in package 'clValid'
Execution halted
ERROR: lazy loading failed for package 'clValid'
** Removing '/Users/guybrock/Documents/projects/DattaPackages/Cluster/clValid_0.5-4.Rcheck/clValid'


The NAMESPACE file is below,

.onLoad <- function(lib, pkg) require(methods)
importFrom(graphics, plot)
exportClasses(clValid)
exportMethods(clusters, clusterMethods, nClusters, measNames, measures, optimalScores, plot, print, show, summary)
S3method(print,sota)
S3method(plot,sota)
export(clValid, sota, dunn, connectivity, BHI, BSI, stability, matchGO)

My question is - how can I locate the source of the cyclic dependency error?  This is an updated version of a package which did not previously have this error, so somehow I introduced this in the modifications I made.  Note that R CMD check on Windows (R version 2.6.0) doesn't catch this, although R CMD build --binary fails on Windows with the same error.   

Thanks,
Guy


From g.abraham at ms.unimelb.edu.au  Mon Dec 10 06:45:55 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Mon, 10 Dec 2007 16:45:55 +1100
Subject: [R] Make natural splines constant outside boundary
In-Reply-To: <Pine.LNX.4.64.0712081110400.23748@tajo.ucsd.edu>
References: <4758ABA1.3040708@ms.unimelb.edu.au>	<loom.20071208T173617-187@post.gmane.org>
	<Pine.LNX.4.64.0712081110400.23748@tajo.ucsd.edu>
Message-ID: <475CD293.1000402@ms.unimelb.edu.au>

Charles C. Berry wrote:
> On Sat, 8 Dec 2007, Charles C. Berry wrote:
> 
>> Gad Abraham <g.abraham <at> ms.unimelb.edu.au> writes:
>>
>>> Hi,
>>>
>>> I'm using natural cubic splines from splines::ns() in survival
>>> regression (regressing inter-arrival times of patients to a queue on
>>> queue size). The queue size fluctuates between 3600 and 3900.
>>>
>>> I would like to be able to run predict.survreg() for sizes <3600 and
>>>> 3900 by assuming that the rate for <3600 is the same as for 3600 and
>>> that for >4000 it's the same as for 4000 (i.e., keep the splines cubic
>>> within the boundaries but make them constant outside the boundaries).
>>>
>> [snip]
>>
>>> Any suggestions?
>> Here is one.
>>
>>> range(ovarian$age)
>> [1] 38.8932 74.5041
>>> trim <- function(x) pmin(74.5041 ,pmax(38.8932 , x))
>>> s <- survreg(Surv(futime, fustat) ~ ns(age, knots=c(50,
>> 60),Boundary.knots=c(38.8932, 74.5041)),data=ovarian)
>>> s2 <- survreg(Surv(futime, fustat) ~ ns(trim(age), knots=c(50,
>> 60),Boundary.knots=c(38.8932, 74.5041)),data=ovarian)
> 
> Should have copy-and-pasted this here:
> 
> newage <- data.frame( age=seq(10,200,10 ) )
> 
>>> matplot(newage, cbind(predict(s,newdata=newage),predict(s2,newdata=newage)))

Thanks Chuck, that works nicely.

Cheers,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham

-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.


From ripley at stats.ox.ac.uk  Mon Dec 10 07:20:04 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Dec 2007 06:20:04 +0000 (GMT)
Subject: [R] Having trouble getting GARCH parameters (basic/newbie)
In-Reply-To: <14247536.post@talk.nabble.com>
References: <14247536.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0712100616550.7947@gannet.stats.ox.ac.uk>

You are probably using Windows: you didn't say.  If so, this is a known 
bug in the tseries package: it uses Fortran I/O and that fails in Rgui. 
The fix (until the package is updated) is to use Rterm instead.

On Sun, 9 Dec 2007, caffeine wrote:

>
> I'm having no luck getting GARCH parameter estimations.  It seems simple
> enough, but I don't know what I'm doing.  I'm a newbie both at R and GARCH
> models, so whatever is going wrong, it's probably very basic.  Here's what I
> do:
>
> 1. I first load the tseries package with:
>
>   library("tseries")
>
> 2. I then load the data with:
>
>   g <- read.table("test.csv", header=T)
>
> 3. It appears to work because I can print the data (a "hello world" type
> dataset with only 5 datapoints):
>
>   > g
>         Data
>   1 0.002337
>   2 0.010037
>   3 0.007608
>   4 0.005620
>   5 0.006050
>
> 4. Looks good.  So now I try to get the parameters of a fitted GARCH(1,1)
> model:
>
>   > gFit <- garch( g, order=c(1,1) )
>
> Here's where the problem is.  R returns:
>
>   ***** ESTIMATION WITH ANALYTICAL GRADIENT *****
>
> and that's it.  No further output is generated; R just sits there.   My
> dataset had 5000 elements, but R just hung there after printing the above
> message.   So I cut the dataset down to 100 elements, and then down to just
> 5 elements.
>
> I'm pretty sure 5 elements should be a snap for R; I think the estimation
> would be done in seconds, so something is definitely wrong.
>
> Help?
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gilhamto at gmail.com  Mon Dec 10 07:32:15 2007
From: gilhamto at gmail.com (G Ilhamto)
Date: Mon, 10 Dec 2007 01:32:15 -0500
Subject: [R] help with fatal error
Message-ID: <bebd16360712092232n54ff6749j8369da5d73f7b54@mail.gmail.com>

Dear helper,

After some cleaning, I found out that I cannot open my R2.6.0 console
with message: "Fatal error: uanble to retore saved data in .Rdata",
and every time I retry it just kick me off. I am wondering if I still
be able to retrieve the data that I've been working for a month.

I have installed the new R2.6.1 and it has no problem to open, only
there is nothing in it I also wonder if I will be able to transfer all
the files in 2.6.0 to 2.6.1. I appreciate anyone who can help.

Thank you,
Ilham


From mark at wardle.org  Mon Dec 10 08:28:06 2007
From: mark at wardle.org (Mark Wardle)
Date: Mon, 10 Dec 2007 07:28:06 +0000
Subject: [R] Getting estimates from survfit.coxph
In-Reply-To: <475BEA35.7080405@vanderbilt.edu>
References: <b59a37130712090311r67e8ea3bi234396359ff31160@mail.gmail.com>
	<loom.20071209T113343-436@post.gmane.org>
	<475BEA35.7080405@vanderbilt.edu>
Message-ID: <b59a37130712092328o4d85e64dpa293939e3fa47259@mail.gmail.com>

On 09/12/2007, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> Dieter Menne wrote:
> ....

Thank you both!

Best wishes,

Mark

-- 
Dr. Mark Wardle
Specialist registrar, Neurology
Cardiff, UK


From xh.along at gmail.com  Mon Dec 10 04:02:47 2007
From: xh.along at gmail.com (Tony zeng)
Date: Mon, 10 Dec 2007 11:02:47 +0800
Subject: [R] Estimating the variability of a population by one sample?
Message-ID: <21add9100712091902r3777c5eehdf34063ac2eabcdb@mail.gmail.com>

Hi all,
       I am meetting one problem.I am estimating  the variability of a
population using one sample.I must do two-side and one-side hypothesis
test,and estimate the confidence interval,But I don't know which
function I can use !
  Thank you !


From dieter.menne at menne-biomed.de  Mon Dec 10 09:36:28 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 10 Dec 2007 08:36:28 +0000 (UTC)
Subject: [R] Viewport and grid.draw
References: <813919.55955.qm@web34703.mail.mud.yahoo.com>
	<eb555e660712091951g3b1800a2gf511b0d2fd008c8f@mail.gmail.com>
Message-ID: <loom.20071210T083529-286@post.gmane.org>

Deepayan Sarkar <deepayan.sarkar <at> gmail.com> writes:

> Try looking at the first example in
> http://dsarkar.fhcrc.org/lattice/book/Chapter12-Interaction/edited.R 
> for inspiration.

May I suggest to use

<http://dsarkar.fhcrc.org/lattice/book/figures.html>

instead. Nice.

Dieter


From jim at bitwrit.com.au  Mon Dec 10 10:12:12 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Mon, 10 Dec 2007 20:12:12 +1100
Subject: [R] package "growth" ... where is it ?
In-Reply-To: <36d691950712091108w65289b67i5edb860909cad809@mail.gmail.com>
References: <36d691950712081521v4525b741pbb88ef561a8bed90@mail.gmail.com>	<36d691950712081642k738bd184o172a39d7ef2b179a@mail.gmail.com>	<1197162765.2955.68.camel@Bellerophon.localdomain>
	<36d691950712091108w65289b67i5edb860909cad809@mail.gmail.com>
Message-ID: <475D02EC.3070207@bitwrit.com.au>

Maura E Monville wrote:
> Thank you. I have downloaded the compressed package "growth-1.tgz". I have
> extracted the archive. But all my attempts to install such a package so that
> I can call its functions from R command line ... have failed.
> It is not clear to me how to operate the "install.packages" command where
> the package has been already downloaded.
> I  tried different parameters, and repos=NULL, but I could not succeed.
> What is the pitfall here ?
> 
Hi Monica,

I downloaded growth.tgz and rmutil.tgz (which growth requires) and 
installed them like this:

R CMD INSTALL rmutil.tgz --no-latex
R CMD INSTALL growth.tgz --no-latex

The examples seem to work, so maybe this would work for you.

Jim


From jones3745 at verizon.net  Sat Dec  8 21:07:22 2007
From: jones3745 at verizon.net (Thomas L Jones, PhD)
Date: Sat, 8 Dec 2007 15:07:22 -0500
Subject: [R] Function to tell you how an object is put together
Message-ID: <000301c839d5$f27b8e20$2f01a8c0@dell2400>

Question: Suppose I have an arbitrary object. Is there a function which will 
accept the object as an argument and sort of give the format of  the object, 
how it is put together, etc.? The analysis would include the attributes and 
the names of the attributes. Also, things like whether or not the object is 
an array; whether or not it is a matrix, etc.

Tom Jones


From snowch at coralms.com  Sun Dec  9 10:35:21 2007
From: snowch at coralms.com (christopher snow)
Date: Sun, 09 Dec 2007 09:35:21 +0000
Subject: [R] adding group totals to a table
Message-ID: <475BB6D9.8050503@coralms.com>

I have a table with two columns:

A   1
A   1
A   2
B   2
C   0

I would like to produce a third column that contains the counts of each 
unique combination of col1 and col2:

A   1  2
A   1  2
A   2  1
B   2  1
C   0  1

How can I do this in R?

Thanks in advance ...


-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.


From zunqiu at hotmail.com  Sun Dec  9 22:55:15 2007
From: zunqiu at hotmail.com (czqiu)
Date: Sun, 9 Dec 2007 13:55:15 -0800 (PST)
Subject: [R] how to let R read different equations I write and execute it?
Message-ID: <14243817.post@talk.nabble.com>


Hello All,
I am a newcomer to R. Currently, I am trying to write an interactive code in
R to let it read the equation which I write and execute the equation in a
curve function.

For example: I want to plot curve by using: curve (equation, -2,2, n=1000)
what I want is that the R code ask me what equation I want to use (which I
know how to write this code):
then, I type for example 3*x.

then 3*x will be assigned to the 'equation' (I don't know how to do this
step)
then curve function will work to plot.

but I dont' know how I can let the equation to be read in and assigned to
equation.
I know how to do this kind of thing for numerical values but not for logical
equations.

Anyone can help out?

thanks a lot!
czqiu
-- 
View this message in context: http://www.nabble.com/how-to-let-R-read-different-equations-I-write-and-execute-it--tp14243817p14243817.html
Sent from the R help mailing list archive at Nabble.com.


From yihsuc at gmail.com  Sun Dec  9 21:10:09 2007
From: yihsuc at gmail.com (YIHSU CHEN)
Date: Sun, 9 Dec 2007 12:10:09 -0800
Subject: [R] editor under MAC system
Message-ID: <31beaa9a0712091210n31b6f292q582c7610234ad1e3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071209/bcb7dc96/attachment.ksh 

From i.visser at uva.nl  Mon Dec 10 10:23:43 2007
From: i.visser at uva.nl (Ingmar Visser)
Date: Mon, 10 Dec 2007 10:23:43 +0100
Subject: [R] Function to tell you how an object is put together
In-Reply-To: <000301c839d5$f27b8e20$2f01a8c0@dell2400>
References: <000301c839d5$f27b8e20$2f01a8c0@dell2400>
Message-ID: <473860BF-DE9B-4901-A791-F9A9EC59BEC7@uva.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/00ca906d/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Mon Dec 10 10:40:34 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 10 Dec 2007 10:40:34 +0100
Subject: [R] Function to tell you how an object is put together
References: <000301c839d5$f27b8e20$2f01a8c0@dell2400>
Message-ID: <00ad01c83b10$b69458b0$0540210a@www.domain>

probably you're looking for str(), e.g.,

lis <- list(x = 1:10, y = letters[1:3])
str(lis)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Thomas L Jones, PhD" <jones3745 at verizon.net>
To: "R-project help" <r-help at stat.math.ethz.ch>
Sent: Saturday, December 08, 2007 9:07 PM
Subject: [R] Function to tell you how an object is put together


> Question: Suppose I have an arbitrary object. Is there a function 
> which will
> accept the object as an argument and sort of give the format of  the 
> object,
> how it is put together, etc.? The analysis would include the 
> attributes and
> the names of the attributes. Also, things like whether or not the 
> object is
> an array; whether or not it is a matrix, etc.
>
> Tom Jones
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From helprhelp at gmail.com  Mon Dec 10 10:58:17 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 10 Dec 2007 04:58:17 -0500
Subject: [R] editor under MAC system
In-Reply-To: <31beaa9a0712091210n31b6f292q582c7610234ad1e3@mail.gmail.com>
References: <31beaa9a0712091210n31b6f292q582c7610234ad1e3@mail.gmail.com>
Message-ID: <cdf817830712100158o5bf20f9kef834c3852d4d0ee@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/03dadd34/attachment.pl 

From corr at fas.harvard.edu  Mon Dec 10 11:02:38 2007
From: corr at fas.harvard.edu (Anders Schwartz Corr)
Date: Mon, 10 Dec 2007 05:02:38 -0500 (EST)
Subject: [R] adding group totals to a table
In-Reply-To: <475BB6D9.8050503@coralms.com>
References: <475BB6D9.8050503@coralms.com>
Message-ID: <Pine.LNX.4.64.0712100442160.349@ls02.fas.harvard.edu>


Hi Chris, This is a very rough first conceptual program you could use -- 
double check all the syntax as I know it won't work at first go. It's 
untested but can be jiggered to work! Good luck! Anders

Use unique() to get your unique combinations, then loop or vectorize 
through each unique combination to count the combos.

unique(data)->x
thirdcol<-NULL
for(i in 1:dim(data)[1]){
 	for(k in 1:dim(x)[1]{
 	thirdcol<-cbind(thirdcol,dim(data[data[i,] %in% x[k,]])[1])
}}

newdata<-cbind(data,thirdcol)

On Sun, 9 Dec 2007, christopher snow wrote:

> I have a table with two columns:
>
> A   1
> A   1
> A   2
> B   2
> C   0
>
> I would like to produce a third column that contains the counts of each
> unique combination of col1 and col2:
>
> A   1  2
> A   1  2
> A   2  1
> B   2  1
> C   0  1
>
> How can I do this in R?
>
> Thanks in advance ...
>
>
> -- 
> This message has been scanned for viruses and
> dangerous content by MailScanner, and is
> believed to be clean.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From taekyunk at gmail.com  Mon Dec 10 11:22:40 2007
From: taekyunk at gmail.com (T.K.)
Date: Mon, 10 Dec 2007 02:22:40 -0800
Subject: [R] adding group totals to a table
In-Reply-To: <475BB6D9.8050503@coralms.com>
References: <475BB6D9.8050503@coralms.com>
Message-ID: <36923f1d0712100222n137424d2pcbac655c39daa676@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/d97b6dc8/attachment.pl 

From taekyunk at gmail.com  Mon Dec 10 11:35:25 2007
From: taekyunk at gmail.com (T.K.)
Date: Mon, 10 Dec 2007 02:35:25 -0800
Subject: [R] adding group totals to a table
In-Reply-To: <36923f1d0712100222n137424d2pcbac655c39daa676@mail.gmail.com>
References: <475BB6D9.8050503@coralms.com>
	<36923f1d0712100222n137424d2pcbac655c39daa676@mail.gmail.com>
Message-ID: <36923f1d0712100235q220a281ci48f14c114d3a0490@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/689cfafd/attachment.pl 

From villegas.ro at gmail.com  Mon Dec 10 12:17:35 2007
From: villegas.ro at gmail.com (Rod)
Date: Mon, 10 Dec 2007 12:17:35 +0100
Subject: [R] editor under MAC system
In-Reply-To: <cdf817830712100158o5bf20f9kef834c3852d4d0ee@mail.gmail.com>
References: <31beaa9a0712091210n31b6f292q582c7610234ad1e3@mail.gmail.com>
	<cdf817830712100158o5bf20f9kef834c3852d4d0ee@mail.gmail.com>
Message-ID: <29cf68350712100317l7aaf828ah532c3649c41ba380@mail.gmail.com>

I recommend Smultron a nice editor for R and other statistical apps.
(http://dataninja.wordpress.com/2006/06/14/r-syntax-highlighting-for-smultron/)

Rod.

PD. Please post your Mac question in R-SIG-MAC list.


On Dec 10, 2007 10:58 AM, Weiwei Shi <helprhelp at gmail.com> wrote:
> editor which goes with R for Mac is pretty good, IMO.
>
>
> On Dec 9, 2007 3:10 PM, YIHSU CHEN <yihsuc at gmail.com> wrote:
>
> > Dear R-user;
> > I recently switched from PC to MAC.  Is there a compatible editor as
> > Win-editor with package RWinEdit for MAC?
> >
> > Thanks
> >
> > Yihsu Chen
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Weiwei Shi, Ph.D
> Research Scientist
> GeneGO, Inc.
>
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cwdegier at gmail.com  Mon Dec 10 12:45:35 2007
From: cwdegier at gmail.com (Cornelis de Gier)
Date: Mon, 10 Dec 2007 12:45:35 +0100
Subject: [R] moving average and NA values
Message-ID: <d80a8b750712100345v3a5e7b02x5801dd1e4fe1c683@mail.gmail.com>

The S-plus function moving.ave(data, span = 2) calculates the moving
average, but it does not have an argument to tell it how to deal with
NA values, so it will return NA for all averages as shown below.

Is there an R or S moving average function which is able to omit some
NA values in the dataset?

In the simple sample shown below it would be possible to just remove
the rows with NA values. The dataset on which I want to use the moving
average function with a span of 270 is a time series dataset, just
removing rows would corrupt this dataset and make it unfit to plot.

Cornelis

t <- (1:10)
moving.ave(t,2)
$aves:
 [1] 1.0 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5

$sizes:
 [1] 1 2 2 2 2 2 2 2 2 2

t[5] <- NA
moving.ave(t,2)
$aves:
 [1] NA NA NA NA NA NA NA NA NA NA

$sizes:
 [1] 1 2 2 2 2 2 2 2 2 2


From dieter.menne at menne-biomed.de  Mon Dec 10 12:19:18 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 10 Dec 2007 11:19:18 +0000 (UTC)
Subject: [R] Estimating the variability of a population by one sample?
References: <21add9100712091902r3777c5eehdf34063ac2eabcdb@mail.gmail.com>
Message-ID: <loom.20071210T111634-215@post.gmane.org>

Tony zeng <xh.along <at> gmail.com> writes:


>        I am meetting one problem.I am estimating  the variability of a
> population using one sample.I must do two-side and one-side hypothesis
> test,and estimate the confidence interval,But I don't know which
> function I can use !

You should describe your problem a bit more clearly, best by giving a sample
set, and how the result should look like. With a bit of guesswork, the 
following might come close to what you want :

t.test(rnorm(100,0.21))

rnorm() generates the test vector, put your data there instead.

Dieter


From gustaf.rydevik at gmail.com  Mon Dec 10 13:35:20 2007
From: gustaf.rydevik at gmail.com (Gustaf Rydevik)
Date: Mon, 10 Dec 2007 13:35:20 +0100
Subject: [R] Tutorial for Basic Stats
In-Reply-To: <49379889C4CA4940A9E1469DCFB9AD8504ADDC@GSBHEX1V.gsb.uchicago.edu>
References: <49379889C4CA4940A9E1469DCFB9AD8504ADDC@GSBHEX1V.gsb.uchicago.edu>
Message-ID: <45f568c70712100435l7f0dfdafr6713d8f40c432b94@mail.gmail.com>

On Dec 10, 2007 5:43 AM, Kapoor, Bharat <k at chicagogsb.edu> wrote:
> Thanks in advance - am looking for  a Tutorial for doing basic stats. I have already looked/looking at the R-intro.pdf at the R site.
>
> Regards
> BK
>
>         [[alternative HTML version deleted]]
>

google "introductory statistics R", and you'll find a nice pdf by J. Verzani.

/Gustaf

-- 
Gustaf Rydevik, M.Sci.
tel: +46(0)703 051 451
address:Essingetorget 40,112 66 Stockholm, SE
skype:gustaf_rydevik


From scionforbai at gmail.com  Mon Dec 10 13:39:24 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Mon, 10 Dec 2007 13:39:24 +0100
Subject: [R] how to let R read different equations I write and execute
	it?
In-Reply-To: <14243817.post@talk.nabble.com>
References: <14243817.post@talk.nabble.com>
Message-ID: <e9ee1f0a0712100439n3929b8feyec892f9b69c4fafc@mail.gmail.com>

Hi Czqiu,

short answer:
?expression
?eval
?quote
?parse
?deparse
?substitute

Bye,

Scionforbai


From cgenolin at u-paris10.fr  Mon Dec 10 13:42:58 2007
From: cgenolin at u-paris10.fr (Christophe Genolini)
Date: Mon, 10 Dec 2007 13:42:58 +0100
Subject: [R] Rerolling k-means
In-Reply-To: <mailman.21.1197284404.31132.r-help@r-project.org>
References: <mailman.21.1197284404.31132.r-help@r-project.org>
Message-ID: <475D3452.6070400@u-paris10.fr>

Hi all

I am working on k-means algorithm (in R: kmeans( ) ). The R-help advice 
us to try several random start in order to avoid local minimum. Does one 
know if there is a procedure that automaticly run this rerolling and 
select the best partition ? Or any studies that gives clues on the 
number of rerolling ?

Thanks for helping.

Christophe


From roger.bos at us.rothschild.com  Mon Dec 10 14:04:10 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Mon, 10 Dec 2007 08:04:10 -0500
Subject: [R] Junk or not Junk ???
In-Reply-To: <20071208090747.GN6584@slingshot.co.nz>
References: <14193897.post@talk.nabble.com><C37DD404.1578E%engrav@u.washington.edu>
	<20071208090747.GN6584@slingshot.co.nz>
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD34FC52F5@RINNYCSE000.rth.ad.rothschild.com>

Ok, outlook may not be that great, but even in outlook you can create a
separate folder called "R" and create a rule so all email with "[R]" or
"[Rdevel]" etc. going into the R folder.  That way you can easily scan
your R mail separate from your regular mail and then delete everything
you don't need.  That's what I do on my work PC.


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Patrick Connolly
Sent: Saturday, December 08, 2007 4:08 AM
To: Loren Engrav
Cc: r-help at r-project.org
Subject: Re: [R] Junk or not Junk ???

On Thu, 06-Dec-2007 at 04:29PM -0800, Loren Engrav wrote:

|> As for news readers
|> I found R and R.mac and R.Bio on the sites you recommend, thank you 
|> very much, they would avoid the individual emails, but then I would 
|> have to go look at them, which might be Ok
|> 
|> Deluge? Well, there are from R and Bio and R-Mac every morning 30 or 
|> 35, and
|> 10-15 more during the daytime, and ~50 deletes is painful
|> 
|> But then every morning one or two are useful so...

I find it absolutely essential to have a client that can display mail in
threads.  Deleting mail a thread at a time is an order of magnitude more
efficient.  Nabble does a fairly good job of showing threads, but I
would prefer to download every message and delete the threads I'm
ignoring.  Even on a good connexion, the delays downloading individual
messages add up.  And gmail is slower still.  Another advantage of your
own client is that you can use a monospaced font which is far easier for
reading code which is bound to happen on a list like this.

Most people I know have the misfortune of not having access to a mail
client that displays threads[1], but for anyone who has control over
such things, in the Windows world, I know Thunderbird is fairly good,
but if you're fortunate enough to be allowed to use Linux, there is Mutt
or you might like Emacs as a mail client which both do threads very well
without the need to use a mouse -- which I consider a huge bonus.

[....]


|> 
|> Still would be fun to understand why some R are junk and some are not

As several have said, it's to do with your mail client and/or how mail
and spam filters are set up on your domain.  Nothing to do with this
list.

1. Read "misfortune of having to use Outlook or even Outlook Express"


best

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.

   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

********************************************************************** *
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient. 


From johannes_graumann at web.de  Mon Dec 10 14:28:19 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 10 Dec 2007 14:28:19 +0100
Subject: [R] Extracting clusters from Data Frame
Message-ID: <fjjet2$6k3$1@ger.gmane.org>

Hello,

I have a large data frame (1006222 rows), which I subject to a crude
clustering attempt that results in a vector stating whether the datapoint
represented by a row belongs to a cluster or not. Conceptually this looks
something like this:
Value   Cluster?
0.01    FALSE
0.03    TRUE
0.04    TRUE
0.05    TRUE
0.07    FALSE
...
What I'm looking for is an efficient strategy to extract all consecutive
rows associated with "TRUE" as a single cluster (data.frame
representation?) without cluttering memory with thousends of data.frames.
I was thinking of an independent data.frame that would contain a column of
lists that reference all indexes from the big one which are contained in
one cluster ...
Can anyone kindly nudge me and let me know how to deal with this
efficiently?

Joh


From P.Dalgaard at biostat.ku.dk  Mon Dec 10 14:35:15 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 10 Dec 2007 14:35:15 +0100
Subject: [R] Tutorial for Basic Stats
In-Reply-To: <45f568c70712100435l7f0dfdafr6713d8f40c432b94@mail.gmail.com>
References: <49379889C4CA4940A9E1469DCFB9AD8504ADDC@GSBHEX1V.gsb.uchicago.edu>
	<45f568c70712100435l7f0dfdafr6713d8f40c432b94@mail.gmail.com>
Message-ID: <475D4093.7030101@biostat.ku.dk>

Gustaf Rydevik wrote:
> On Dec 10, 2007 5:43 AM, Kapoor, Bharat <k at chicagogsb.edu> wrote:
>   
>> Thanks in advance - am looking for  a Tutorial for doing basic stats. I have already looked/looking at the R-intro.pdf at the R site.
>>
>> Regards
>> BK
>>
>>         [[alternative HTML version deleted]]
>>
>>     
>
> google "introductory statistics R", and you'll find a nice pdf by J. Verzani.
>
>   
...as well as other references ;-)

Seriously, it all depends on what you consider "basic stats".
(Especially the level of math prerequisites varies widely among new R
users.)

You might also want to check out the lists at

http://cran.r-project.org/other-docs.html
http://www.r-project.org/other-docs.html

(I kind of like the short document by Owen, but it  does assume that 
you are not scared by a few sums and integrals.)
> /Gustaf
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From gustaf.rydevik at gmail.com  Mon Dec 10 15:06:03 2007
From: gustaf.rydevik at gmail.com (Gustaf Rydevik)
Date: Mon, 10 Dec 2007 15:06:03 +0100
Subject: [R] Extracting clusters from Data Frame
In-Reply-To: <fjjet2$6k3$1@ger.gmane.org>
References: <fjjet2$6k3$1@ger.gmane.org>
Message-ID: <45f568c70712100606o26bd0008t2b708a0142bdf640@mail.gmail.com>

On Dec 10, 2007 2:28 PM, Johannes Graumann <johannes_graumann at web.de> wrote:
> Hello,
>
> I have a large data frame (1006222 rows), which I subject to a crude
> clustering attempt that results in a vector stating whether the datapoint
> represented by a row belongs to a cluster or not. Conceptually this looks
> something like this:
> Value   Cluster?
> 0.01    FALSE
> 0.03    TRUE
> 0.04    TRUE
> 0.05    TRUE
> 0.07    FALSE
> ...
> What I'm looking for is an efficient strategy to extract all consecutive
> rows associated with "TRUE" as a single cluster (data.frame
> representation?) without cluttering memory with thousends of data.frames.
> I was thinking of an independent data.frame that would contain a column of
> lists that reference all indexes from the big one which are contained in
> one cluster ...
> Can anyone kindly nudge me and let me know how to deal with this
> efficiently?
>
> Joh
>

How about :
orig.data<-sample(c(TRUE,FALSE),100,replace=T)
Cluster<-data.frame(c.ndx=cumsum(rle(orig.data)$lengths),c.size=rle(orig.data)$lengths,c.type=rle(orig.data)$values)
Cluster<-Cluster[Cluster$c.type==TRUE,]

##Then, to get all original data belonging to cluster three:
orig.data[rev(Cluster[3,"c.ndx"]-seq(length.out=Cluster[3,"c.size"])+1)]


Not the neatest solution, but I'm sure someone here can improve on it.
/Gustaf

-- 
Gustaf Rydevik, M.Sci.
tel: +46(0)703 051 451
address:Essingetorget 40,112 66 Stockholm, SE
skype:gustaf_rydevik


From therneau at mayo.edu  Mon Dec 10 15:21:31 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Mon, 10 Dec 2007 08:21:31 -0600 (CST)
Subject: [R] Getting estimates from survfit.coxph
Message-ID: <200712101421.lBAELVF14219@hsrnfs-101.mayo.edu>

  The problem will be fixed in the next resease of the survival code.  (That is, 
it is fixed on our local version of R).  The summary.survfit result now includes 
an element 'table' containing the matrix that is shown by print.survfit.   
  
  	Terry


From mxkuhn at gmail.com  Mon Dec 10 15:22:48 2007
From: mxkuhn at gmail.com (Max Kuhn)
Date: Mon, 10 Dec 2007 09:22:48 -0500
Subject: [R] editor under MAC system
In-Reply-To: <29cf68350712100317l7aaf828ah532c3649c41ba380@mail.gmail.com>
References: <31beaa9a0712091210n31b6f292q582c7610234ad1e3@mail.gmail.com>
	<cdf817830712100158o5bf20f9kef834c3852d4d0ee@mail.gmail.com>
	<29cf68350712100317l7aaf828ah532c3649c41ba380@mail.gmail.com>
Message-ID: <6731304c0712100622we3f4303n84e9b10e7d661b0c@mail.gmail.com>

I've been using ForgEdit. It is still in beta, but it works well. I
have a syntax highlighting file htat I can send you (and I need to
post on the gui/ide website).

-- 

Max


From jrkrideau at yahoo.ca  Mon Dec 10 15:40:50 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 10 Dec 2007 09:40:50 -0500 (EST)
Subject: [R] Tutorial for Basic Stats
In-Reply-To: <49379889C4CA4940A9E1469DCFB9AD8504ADDC@GSBHEX1V.gsb.uchicago.edu>
Message-ID: <268022.75606.qm@web32814.mail.mud.yahoo.com>


--- "Kapoor, Bharat " <k at chicagogsb.edu> wrote:

> Thanks in advance - am looking for  a Tutorial for
> doing basic stats. I have already looked/looking at
> the R-intro.pdf at the R site.

Have a look at the Books and Others categories (left
side of main R page.

For some basic totorials perhaps 

http://www.math.ilstu.edu/dhkim/Rstuff/Rtutor.html

http://zoonek2.free.fr/UNIX/48_R/all.html


From rvaradhan at jhmi.edu  Mon Dec 10 16:08:32 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Mon, 10 Dec 2007 10:08:32 -0500
Subject: [R] Large determinant problem
In-Reply-To: <49632.203.109.170.45.1197186276.squirrel@webmail.scms.waikato.ac.nz>
References: <49552.203.109.170.45.1197178133.squirrel@webmail.scms.waikato.ac.nz>
	<Pine.LNX.4.64.0712090628420.31005@gannet.stats.ox.ac.uk>
	<49632.203.109.170.45.1197186276.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <000f01c83b3e$879c7cd0$7c94100a@win.ad.jhu.edu>

It is evident that you do not have enough information in the data to
estimate 9 mixture components.  This is clearly indicated by a positive
semi-definite information matrix, S, that is less than full rank.  You can
monitor the rank of the information matrix, as you increase the number of
components, and stop when you suspect rank-deficiency.

Ravi.


----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of maj at stats.waikato.ac.nz
Sent: Sunday, December 09, 2007 2:45 AM
To: Prof Brian Ripley
Cc: r-help at r-project.org
Subject: Re: [R] Large determinant problem

I tried crossprod(S) but the results were identical. The term
-0.5*log(det(S)) is  a complexity penalty meant to make it unattractive to
include too many components in a finite mixture model. This case was for a
9-component mixture. At least up to 6 components the determinant behaved
as expected and increased with the number of components.

Thanks for your comments.

> Hmm, S'S is numerically singular.  crossprod(S) would be a better way to
> compute it than crossprod(S,S) (it does use a different algorithm), but
> look at the singular values of S, which I suspect will show that S is
> numerically singular.
>
> Looks like the answer is 0.
>
>
> On Sun, 9 Dec 2007, maj at stats.waikato.ac.nz wrote:
>
>> I thought I would have another try at explaining my problem. I think
>> that
>> last time I may have buried it in irrelevant detail.
>>
>> This output should explain my dilemma:
>>
>>> dim(S)
>> [1] 1455  269
>>> summary(as.vector(S))
>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>> -1.160e+04  0.000e+00  0.000e+00 -4.132e-08  0.000e+00  8.636e+03
>>> sum(as.vector(S)==0)/(1455*269)
>> [1] 0.8451794
>> # S is a large moderately sparse matrix with some large elements
>>> SS <- crossprod(S,S)
>>> (eigen(SS,only.values = TRUE)$values)[250:269]
>> [1]  9.264883e+04  5.819672e+04  5.695073e+04  1.948626e+04
>> 1.500891e+04
>> [6]  1.177034e+04  9.696327e+03  8.037049e+03  7.134058e+03
>> 1.316449e-07
>> [11]  9.077244e-08  6.417276e-08  5.046411e-08  1.998775e-08
>> -1.268081e-09
>> [16] -3.140881e-08 -4.478184e-08 -5.370730e-08 -8.507492e-08
>> -9.496699e-08
>> # S'S fails to be non-negative definite.
>>
>> I can't show you how to produce S easily but below I attempt at a
>> reproducible version of the problem:
>>
>>> set.seed(091207)
>>> X <- runif(1455*269,-1e4,1e4)
>>> p <- rbinom(1455*269,1,0.845)
>>> Y <- p*X
>>> dim(Y) <- c(1455,269)
>>> YY <- crossprod(Y,Y)
>>> (eigen(YY,only.values = TRUE)$values)[250:269]
>> [1] 17951634238 17928076223 17725528630 17647734206 17218470634
>> 16947982383
>> [7] 16728385887 16569501198 16498812174 16211312750 16127786747
>> 16006841514
>> [13] 15641955527 15472400630 15433931889 15083894866 14794357643
>> 14586969350
>> [19] 14297854542 13986819627
>> # No sign of negative eigenvalues; phenomenon must be due
>> # to special structure of S.
>> # S is a matrix of empirical parameter scores at an approximate
>> # mle for a model with 269 paramters fitted to 1455 observations.
>> # Thus, for example, its column sums are approximately zero:
>>> summary(apply(S,2,sum))
>>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
>> -1.148e-03 -2.227e-04 -7.496e-06 -6.011e-05  7.967e-05  8.254e-04
>>
>> I'm starting to think that it may not be a good idea to attempt to
>> compute
>> large information matrices and their determinants!
>>
>> Murray Jorgensen
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From wwwhsd at gmail.com  Mon Dec 10 16:34:29 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 10 Dec 2007 13:34:29 -0200
Subject: [R] How to read in expressions as function parameters?
In-Reply-To: <855c8c750712091445r4b5653c1g2bd1ccab61c724b9@mail.gmail.com>
References: <855c8c750712091445r4b5653c1g2bd1ccab61c724b9@mail.gmail.com>
Message-ID: <da79af330712100734x679e1a47ic95aa9b8103ee3c7@mail.gmail.com>

Try this:

form<- scan(file = "", what = character(0), n=1,strip.white =
 TRUE,quiet=TRUE )
form2<-parse(text=form)
foo <- function(x){}
body(foo) <- form2
curve(foo,10,100)

On 09/12/2007, Alex Wang <alecwang80 at gmail.com> wrote:
> Hi:
>
>    There, I've got a question about how to read in expressions as function
> parameters and it really bothered me.
>
>  I'm going to use curve() function to plot curves, and I'd like to write a
> menu function to let use input math expressions.
>
>   say, if I'd like curve(3*x*x-4/x, 10, 100), I use scan() to  read in the
> math expression
>
> form<- scan(file = "", what = character(0), n=1,strip.white =
> TRUE,quiet=TRUE )
> then get form as a string. "3*x*x-4/x"
> Use parse() to convert it into expression
> form2<-parse(text=form)
> then form2=expression(3*x*x-4/x).
>
> However, curve(form2,10,100) doesn't work at all.
>
> Is there anyway to get this done?
>
> Thanks a lot!
>
> Alex
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From pedrosmarques at portugalmail.pt  Mon Dec 10 16:53:31 2007
From: pedrosmarques at portugalmail.pt (pedrosmarques at portugalmail.pt)
Date: Mon, 10 Dec 2007 15:53:31 +0000
Subject: [R] time series tests
Message-ID: <1197302011.475d60fb21357@gold3.portugalmail.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/54de171c/attachment.pl 

From ggrothendieck at gmail.com  Mon Dec 10 16:57:42 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Dec 2007 10:57:42 -0500
Subject: [R] moving average and NA values
In-Reply-To: <d80a8b750712100345v3a5e7b02x5801dd1e4fe1c683@mail.gmail.com>
References: <d80a8b750712100345v3a5e7b02x5801dd1e4fe1c683@mail.gmail.com>
Message-ID: <971536df0712100757r2e2d6800r6b477a3386ec4164@mail.gmail.com>

rollapply in the zoo package can do that:

> library(zoo)
> x <- zoo(1:10)
> x[5] <- NA
> rollapply(x, 3, mean, na.rm = TRUE)
  2   3   4   5   6   7   8   9
2.0 3.0 3.5 5.0 6.5 7.0 8.0 9.0
> xm <- rollapply(x, 3, mean, na.rm = TRUE)
> xm
  2   3   4   5   6   7   8   9
2.0 3.0 3.5 5.0 6.5 7.0 8.0 9.0
> coredata(xm) # unzoo it
[1] 2.0 3.0 3.5 5.0 6.5 7.0 8.0 9.0

See the two zoo vignettes for more info.

On Dec 10, 2007 6:45 AM, Cornelis de Gier <cwdegier at gmail.com> wrote:
> The S-plus function moving.ave(data, span = 2) calculates the moving
> average, but it does not have an argument to tell it how to deal with
> NA values, so it will return NA for all averages as shown below.
>
> Is there an R or S moving average function which is able to omit some
> NA values in the dataset?
>
> In the simple sample shown below it would be possible to just remove
> the rows with NA values. The dataset on which I want to use the moving
> average function with a span of 270 is a time series dataset, just
> removing rows would corrupt this dataset and make it unfit to plot.
>
> Cornelis
>
> t <- (1:10)
> moving.ave(t,2)
> $aves:
>  [1] 1.0 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5
>
> $sizes:
>  [1] 1 2 2 2 2 2 2 2 2 2
>
> t[5] <- NA
> moving.ave(t,2)
> $aves:
>  [1] NA NA NA NA NA NA NA NA NA NA
>
> $sizes:
>  [1] 1 2 2 2 2 2 2 2 2 2
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mihainica at yahoo.com  Mon Dec 10 17:04:33 2007
From: mihainica at yahoo.com (Mihai Nica)
Date: Mon, 10 Dec 2007 08:04:33 -0800 (PST)
Subject: [R] Savannah R Presentation
Message-ID: <986066.3160.qm@web50805.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/d671265d/attachment.pl 

From ggrothendieck at gmail.com  Mon Dec 10 17:08:03 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Dec 2007 11:08:03 -0500
Subject: [R] adding group totals to a table
In-Reply-To: <475BB6D9.8050503@coralms.com>
References: <475BB6D9.8050503@coralms.com>
Message-ID: <971536df0712100808x6004741ep849fc0a0e8ac34b5@mail.gmail.com>

split the data frame, cbind the count and unsplit back:

 DF <- data.frame(A = c("A", "A", "A", "B", "C"), B = c(1, 1, 2, 2, 0))

> g <- paste(DF$A, DF$B)
> s <- split(DF, g)
> u <- lapply(s, function(x) cbind(x, Occurs = nrow(x)))
> unsplit(u, g)
  A B Occurs
1 A 1      2
2 A 1      2
3 A 2      1
4 B 2      1
5 C 0      1

See ?split where there is a very similar example.

On Dec 9, 2007 4:35 AM, christopher snow <snowch at coralms.com> wrote:
> I have a table with two columns:
>
> A   1
> A   1
> A   2
> B   2
> C   0
>
> I would like to produce a third column that contains the counts of each
> unique combination of col1 and col2:
>
> A   1  2
> A   1  2
> A   2  1
> B   2  1
> C   0  1
>
> How can I do this in R?
>
> Thanks in advance ...
>
>
> --
> This message has been scanned for viruses and
> dangerous content by MailScanner, and is
> believed to be clean.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brownja at msu.edu  Mon Dec 10 17:10:06 2007
From: brownja at msu.edu (James T Brown)
Date: Mon, 10 Dec 2007 11:10:06 -0500
Subject: [R] Building R on Sun Solaris 10 (SPARC) using Sun Studio 12
Message-ID: <475D64DE.90302@msu.edu>

R Help List:

Just curious if anyone has successfully built R on a SPARC
platform running Sun Solaris 10 using the latest Sun Studio
12 set of compilers.   If so, I would be interested in the
compile flags that you used.

I have tried several different builds of version 2.5.1, 2.6.0,
and 2.6.1 using various different compile flags and I am able
to compile and check, but for whatever reason, the "foreign"
package crashes whenever it is loaded.   I need the "foreign"
package in order to install the "maptools" package.   This is
the error that I am getting when attempting to load "foreign":

> > library(foreign)
>
>  *** caught segfault ***
> address fbb1dc40, cause 'invalid permissions'
>
> Traceback:
>  1: .C("spss_init", PACKAGE = "foreign")
>  2: fun(...)
>  3: doTryCatch(return(expr), name, parentenv, handler)
>  4: tryCatchOne(expr, names, parentenv, handlers[[1]])
>  5: tryCatchList(expr, classes, parentenv, handlers)
>  6: tryCatch(expr, error = function(e) {    call <- 
> conditionCall(e)    if (!is.null(call)) {        if 
> (identical(call[[1]], quote(doTryCatch)))             call <- 
> sys.call(-4)        dcall <- deparse(call)[1]        prefix <- 
> paste("Error in", dcall, ": ")        LONGCALL <- 30        if 
> (nchar(dcall) > LONGCALL)             prefix <- paste(prefix, "\n\t", 
> sep = "")    }    else prefix <- "Error : "    msg <- paste(prefix, 
> conditionMessage(e), "\n", sep = "")    
> .Internal(seterrmessage(msg[1]))    if (!silent && 
> identical(getOption("show.error.messages"),         TRUE)) {        
> cat(msg, file = stderr())        .Internal(printDeferredWarnings())    
> }    invisible(structure(msg, class = "try-error"))})
>  7: try({    fun(...)    NULL})
>  8: runHook(".onLoad", package, env, package.lib, package)
>  9: loadNamespace(package, c(which.lib.loc, lib.loc), keep.source = 
> keep.source)
> 10: doTryCatch(return(expr), name, parentenv, handler)
> 11: tryCatchOne(expr, names, parentenv, handlers[[1]])
> 12: tryCatchList(expr, classes, parentenv, handlers)
> 13: tryCatch(expr, error = function(e) {    call <- 
> conditionCall(e)    if (!is.null(call)) {        if 
> (identical(call[[1]], quote(doTryCatch)))             call <- 
> sys.call(-4)        dcall <- deparse(call)[1]        prefix <- 
> paste("Error in", dcall, ": ")        LONGCALL <- 30        if 
> (nchar(dcall) > LONGCALL)             prefix <- paste(prefix, "\n\t", 
> sep = "")    }    else prefix <- "Error : "    msg <- paste(prefix, 
> conditionMessage(e), "\n", sep = "")    
> .Internal(seterrmessage(msg[1]))    if (!silent && 
> identical(getOption("show.error.messages"),         TRUE)) {        
> cat(msg, file = stderr())        .Internal(printDeferredWarnings())    
> }    invisible(structure(msg, class = "try-error"))})
> 14: try({    ns <- loadNamespace(package, c(which.lib.loc, lib.loc), 
> keep.source = keep.source)    dataPath <- file.path(which.lib.loc, 
> package, "data")    env <- attachNamespace(ns, pos = pos, dataPath = 
> dataPath)})
> 15: library(foreign)
>
> Possible actions:
> 1: abort (with core dump, if enabled)
> 2: normal R exit
> 3: exit R without saving workspace
> 4: exit R saving workspace



The latest build was compiled with the following flags:

> ./configure --prefix=/usr/local/R-2.5.1
>             --with-blas
>             --with-lapack
>             --with-tcl-config=/usr/local/lib/tclConfig.sh
>             --with-tk-config=/usr/local/lib/tkConfig.sh
>             --without-iconv
>             R_PAPERSIZE=letter
>             SHLIB_CXXLDFLAGS="-G /opt/SUNWspro/lib/libCrun.so"
>             CC=/opt/SUNWspro/bin/cc CXX=/opt/SUNWspro/bin/CC
>             F77=/opt/SUNWspro/bin/f77 F90=/opt/SUNWspro/bin/f95
>             FC=/opt/SUNWspro/bin/f95 CFLAGS="-mt -ftrap=%none 
> -xarch=sparcvis -fPIC -xmemalign=4s"
>             CXXFLAGS="-mt -ftrap=%none -xarch=sparcvis -xmemalign=4s"
>             FFLAGS="-mt -ftrap=%none -shared -xarch=sparcvis"
>             FCFLAGS="-mt -ftrap=%none -shared -xarch=sparcvis"
>             LDFLAGS="-V -fPIC -L/usr/local/lib -L/opt/SUNWspro/lib 
> -L/usr/sfw/lib -L/usr/lib
>                      
> -R/usr/local/lib:/opt/SUNWspro/lib:/usr/sfw/lib:/usr/lib"


I have been messing with the "xmemalign" flag, but doesn't seem to have much
of an impact.   I am curious if there may be a simple compile flag in Sun
Studio 12 that can be set to fix this problem.


At any rate, if anyone has been able to successfully build on Sun Solaris 10
(SPARC) using Sun Studio 12 and the "foreign" package loads without 
crashing,
I would be most appreciative if you could let me take a look at your 
".configure"
options.  


NOTE: So far, the only package that I am having trouble with is 
"foreign".   Everything
else seems to build and check ok.  In fact, when "foreign" is built, 
there are no errors
reported during the compile.   Also, I have tried 
"install.packages("foreign") from
within R to upgrade to the latest version of "foreign".  It compiles and 
installs, but
once again, it crashes when R attempts to use it producing the 
"segfault" error.


Any help would be most welcome.


Thanks.



Jim


-- 
=========================================
James T Brown
Depts. of Geography/Fisheries & Wildlife
Michigan State University


From jholtman at gmail.com  Mon Dec 10 17:16:00 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 10 Dec 2007 08:16:00 -0800
Subject: [R] adding group totals to a table
In-Reply-To: <475BB6D9.8050503@coralms.com>
References: <475BB6D9.8050503@coralms.com>
Message-ID: <644e1f320712100816p693f0547qc3bb137d2490585d@mail.gmail.com>

?ave

> x
  V1 V2
1  A  1
2  A  1
3  A  2
4  B  2
5  C  0
> x$len <- ave(seq_along(x$V1), x$V1, x$V2, FUN=length)
> x
  V1 V2 len
1  A  1   2
2  A  1   2
3  A  2   1
4  B  2   1
5  C  0   1
>


On Dec 9, 2007 1:35 AM, christopher snow <snowch at coralms.com> wrote:
> I have a table with two columns:
>
> A   1
> A   1
> A   2
> B   2
> C   0
>
> I would like to produce a third column that contains the counts of each
> unique combination of col1 and col2:
>
> A   1  2
> A   1  2
> A   2  1
> B   2  1
> C   0  1
>
> How can I do this in R?
>
> Thanks in advance ...
>
>
> --
> This message has been scanned for viruses and
> dangerous content by MailScanner, and is
> believed to be clean.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jrkrideau at yahoo.ca  Mon Dec 10 17:17:11 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 10 Dec 2007 11:17:11 -0500 (EST)
Subject: [R] Barchart, Pareto
In-Reply-To: <49379889C4CA4940A9E1469DCFB9AD8504ADD7@GSBHEX1V.gsb.uchicago.edu>
Message-ID: <703989.92616.qm@web32811.mail.mud.yahoo.com>


--- "Kapoor, Bharat " <k at chicagogsb.edu> wrote:

> Hello
>  
> Well I am relatively new so some of these issues may
> not fall under the subject that I have used.
>  
> 1. How do I do a  Pareto.

You might want to have a look at the qcc package. See 
http://www.stat.unipg.it/~luca/Rnews_2004-1-pag11-17.pdf
for some information
 Following is the approach
> I took.
>  
> My data looks like this
> df2_9
>    Reaason.for.failure Frequency
> 
> 1             Phy Conn        1
> 
> 2        Power failure        3
> 
> 3      Server software        29
> 
> 4      Server hardware        2
> 
> 5    Server out of mem        32
> 
> 6 Inadequate bandwidth        1
> 
>  
> I modified the data using:
> > df <-
> df2_9[order(df2_9$Frequency,decreasing=TRUE),]
> > x$Percent <- 100*(x$Frequency)/(sum(x$Frequency))
> 
>   ReasonCode Frequency             ROF   Percent
> 
> 5        OOM        32  Out of Momeory 47.058824
> 
> 3         SS        29 Server Software 42.647059
> 
> 2         PF         3   Power failure  4.411765
> 
> 4         SH         2      Server h/w  2.941176
> 
> 1         PC         1   Physical Conn  1.470588
> 
> 6         IB         1      Insuff b/w  1.470588
> 
>  
> 
> barplot(x$Percent, space=0, names.arg=x$ReasonCode,
> main="Pareto", legend.text=x$ReasonCode, ylab="%
> error")
> 
>  
> 
> Here are my questions:
> 
> 1.    I could not print the data in ROF column below
> the bar charts,  How to get full labels for each bar
> insted I createda  new coumn ReasonCode just to fit
> below the bars.



Given the lenght of the names you may have a problem. 
?mtext is the obvious way but the names are too long. 
Have a look at
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/80007.html
for rotating the labels or use the staxlab function in
plotrix (see below)


> 2.     Can I color code each bar differently
> 
> 3.     How to set limit on the y-axis - by deafult
> it shows 40, I want it show Y-axis till 50?

library(plotrix)
bp <- barplot(x$Percent, space=0,
names.arg=x$ReasonCode, main="Pareto",
 legend.text=x$ReasonCode, ylab="% error", col=1:5,
ylim=c(0,50), xaxt="n")
 staxlab(side=1, 1:6, x[,3])
 
> 4.     How to display value of each bar inside or on
> top of it?

?text


From ptit_bleu at yahoo.fr  Mon Dec 10 17:25:54 2007
From: ptit_bleu at yahoo.fr (Ptit_Bleu)
Date: Mon, 10 Dec 2007 08:25:54 -0800 (PST)
Subject: [R] Sweave : change value in rnw file to generate multiple "single"
 reports ?
Message-ID: <14256204.post@talk.nabble.com>


Hello,

I'm still trying to make the life of my colleagues easier. Nice, isn't it ?
At the moment, I'm looking for a way to generate multiple "single report".
In fact I have a .rnw file which send a query to a MySQL database
(rs<-dbSendQuery(con, statement="select * from treatdata where
name='Device1'")

But of course my colleagues have many devices and don't want to enter the
rnw file to change the name of the device.

Is there a way to pass arguments to Sweave like Sweave("myfile.rnw",
namevar="Device2") and it will change namevar by Device2 into the file
myfile.rnw before creating the .tex file ?
Or is it possible to do it via another language, that is: loading the rnw
file, modifying it, saving it and finally calling Sweave ?

Whatever the solution, I need you help. In advance thank you.
Ptit Bleu.   
-- 
View this message in context: http://www.nabble.com/Sweave-%3A-change-value-in-rnw-file-to-generate-multiple-%22single%22-reports---tp14256204p14256204.html
Sent from the R help mailing list archive at Nabble.com.


From dbickel at uottawa.ca  Mon Dec 10 17:26:34 2007
From: dbickel at uottawa.ca (David Bickel)
Date: Mon, 10 Dec 2007 11:26:34 -0500
Subject: [R] 3-D plot of likelihood
Message-ID: <D33A5B5753579B44AD89F58D40147582043298BB@MSMAIL2.uottawa.o.univ>

Could anyone recommend a package for visualizing a likelihood function
of two scalar parameters? I would prefer a three-dimensional plot
similar to the kind Mathematica is known for, perhaps generated by a
package not specific to likelihood.

David

______________________________
David R. Bickel
Ottawa Institute of Systems Biology
BMI Dept., University of Ottawa

http://www.oisb.ca/members.htm


From deepayan.sarkar at gmail.com  Mon Dec 10 17:33:59 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 10 Dec 2007 08:33:59 -0800
Subject: [R] Viewport and grid.draw
In-Reply-To: <loom.20071210T083529-286@post.gmane.org>
References: <813919.55955.qm@web34703.mail.mud.yahoo.com>
	<eb555e660712091951g3b1800a2gf511b0d2fd008c8f@mail.gmail.com>
	<loom.20071210T083529-286@post.gmane.org>
Message-ID: <eb555e660712100833m7dee02d6u3b56ce37672db5b7@mail.gmail.com>

On 12/10/07, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Deepayan Sarkar <deepayan.sarkar <at> gmail.com> writes:
>
> > Try looking at the first example in
> > http://dsarkar.fhcrc.org/lattice/book/Chapter12-Interaction/edited.R
> > for inspiration.
>
> May I suggest to use
>
> <http://dsarkar.fhcrc.org/lattice/book/figures.html>
>
> instead. Nice.

I don't think you can access Ch 12 code from that (those plots are by
nature not scriptable, and I haven't done the extra work to make the
code available anyway).

-Deepayan


From ggrothendieck at gmail.com  Mon Dec 10 17:44:02 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Dec 2007 11:44:02 -0500
Subject: [R] Sweave : change value in rnw file to generate multiple
	"single" reports ?
In-Reply-To: <14256204.post@talk.nabble.com>
References: <14256204.post@talk.nabble.com>
Message-ID: <971536df0712100844x82d594au30abb1652092035@mail.gmail.com>

The ability to pass arguments on the

R CMD Sweave

line is something I would very much like to have as well.

Currently you need to create a shell or batch file that accepts
the arguments, place those into environment variables using shell
or batch code and then run sweave.  R code in your swevae file
would then read the environment variables using Sys.getenv.

On Dec 10, 2007 11:25 AM, Ptit_Bleu <ptit_bleu at yahoo.fr> wrote:
>
> Hello,
>
> I'm still trying to make the life of my colleagues easier. Nice, isn't it ?
> At the moment, I'm looking for a way to generate multiple "single report".
> In fact I have a .rnw file which send a query to a MySQL database
> (rs<-dbSendQuery(con, statement="select * from treatdata where
> name='Device1'")
>
> But of course my colleagues have many devices and don't want to enter the
> rnw file to change the name of the device.
>
> Is there a way to pass arguments to Sweave like Sweave("myfile.rnw",
> namevar="Device2") and it will change namevar by Device2 into the file
> myfile.rnw before creating the .tex file ?
> Or is it possible to do it via another language, that is: loading the rnw
> file, modifying it, saving it and finally calling Sweave ?
>
> Whatever the solution, I need you help. In advance thank you.
> Ptit Bleu.
> --
> View this message in context: http://www.nabble.com/Sweave-%3A-change-value-in-rnw-file-to-generate-multiple-%22single%22-reports---tp14256204p14256204.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Mon Dec 10 17:49:00 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 10 Dec 2007 17:49:00 +0100
Subject: [R] 3-D plot of likelihood
In-Reply-To: <D33A5B5753579B44AD89F58D40147582043298BB@MSMAIL2.uottawa.o.univ>
References: <D33A5B5753579B44AD89F58D40147582043298BB@MSMAIL2.uottawa.o.univ>
Message-ID: <475D6DFC.6080902@statistik.uni-dortmund.de>

See ?persp

Uwe Ligges

David Bickel wrote:
> Could anyone recommend a package for visualizing a likelihood function
> of two scalar parameters? I would prefer a three-dimensional plot
> similar to the kind Mathematica is known for, perhaps generated by a
> package not specific to likelihood.
> 
> David
> 
> ______________________________
> David R. Bickel
> Ottawa Institute of Systems Biology
> BMI Dept., University of Ottawa
> 
> http://www.oisb.ca/members.htm
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Mon Dec 10 17:56:05 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 10 Dec 2007 11:56:05 -0500
Subject: [R] Sweave : change value in rnw file to generate multiple
 "single" reports ?
In-Reply-To: <14256204.post@talk.nabble.com>
References: <14256204.post@talk.nabble.com>
Message-ID: <475D6FA5.4090106@stats.uwo.ca>

On 12/10/2007 11:25 AM, Ptit_Bleu wrote:
> Hello,
> 
> I'm still trying to make the life of my colleagues easier. Nice, isn't it ?
> At the moment, I'm looking for a way to generate multiple "single report".
> In fact I have a .rnw file which send a query to a MySQL database
> (rs<-dbSendQuery(con, statement="select * from treatdata where
> name='Device1'")
> 
> But of course my colleagues have many devices and don't want to enter the
> rnw file to change the name of the device.
> 
> Is there a way to pass arguments to Sweave like Sweave("myfile.rnw",
> namevar="Device2") and it will change namevar by Device2 into the file
> myfile.rnw before creating the .tex file ?

Not as far as I know, but you could wrap Sweave() in another function 
that uses save() to write a file of variables, and start your .Rnw file 
with load("file.Rdata").

> Or is it possible to do it via another language, that is: loading the rnw
> file, modifying it, saving it and finally calling Sweave ?

Surely yes, there are lots of ways to automatically edit a text file.

Duncan Murdoch

> 
> Whatever the solution, I need you help. In advance thank you.
> Ptit Bleu.


From ligges at statistik.uni-dortmund.de  Mon Dec 10 17:52:22 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 10 Dec 2007 17:52:22 +0100
Subject: [R] Sweave : change value in rnw file to generate multiple
 "single" reports ?
In-Reply-To: <14256204.post@talk.nabble.com>
References: <14256204.post@talk.nabble.com>
Message-ID: <475D6EC6.6080405@statistik.uni-dortmund.de>

Within or before SWeave, you can use R.
Hence you can write some R function that first changes the rnw and runs 
SWeave thereafter, or even better, write something in your SWeave code 
that reads a user customized variables, e.g. from an environment 
variable or from some text file.

Uwe Ligges



Ptit_Bleu wrote:
> Hello,
> 
> I'm still trying to make the life of my colleagues easier. Nice, isn't it ?
> At the moment, I'm looking for a way to generate multiple "single report".
> In fact I have a .rnw file which send a query to a MySQL database
> (rs<-dbSendQuery(con, statement="select * from treatdata where
> name='Device1'")
> 
> But of course my colleagues have many devices and don't want to enter the
> rnw file to change the name of the device.
> 
> Is there a way to pass arguments to Sweave like Sweave("myfile.rnw",
> namevar="Device2") and it will change namevar by Device2 into the file
> myfile.rnw before creating the .tex file ?
> Or is it possible to do it via another language, that is: loading the rnw
> file, modifying it, saving it and finally calling Sweave ?
> 
> Whatever the solution, I need you help. In advance thank you.
> Ptit Bleu.


From dieter.menne at menne-biomed.de  Mon Dec 10 17:53:32 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 10 Dec 2007 16:53:32 +0000 (UTC)
Subject: [R] Sweave : change value in rnw file to generate multiple
	"single" reports ?
References: <14256204.post@talk.nabble.com>
Message-ID: <loom.20071210T165129-614@post.gmane.org>

Ptit_Bleu <ptit_bleu <at> yahoo.fr> writes:

... 
> Is there a way to pass arguments to Sweave like Sweave("myfile.rnw",
> namevar="Device2") and it will change namevar by Device2 into the file
> myfile.rnw before creating the .tex file ?
> Or is it possible to do it via another language, that is: loading the rnw
> file, modifying it, saving it and finally calling Sweave ?


You can set an environment variable in the system (SET DELTA=...), and read it
in from R:

Sys.getenv("DELTA") 

Works for Windows with a rather inconsistent implementation of environment
strings, so it should work the better with Linux.

Dieter


From murdoch at stats.uwo.ca  Mon Dec 10 17:58:32 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 10 Dec 2007 11:58:32 -0500
Subject: [R] 3-D plot of likelihood
In-Reply-To: <D33A5B5753579B44AD89F58D40147582043298BB@MSMAIL2.uottawa.o.univ>
References: <D33A5B5753579B44AD89F58D40147582043298BB@MSMAIL2.uottawa.o.univ>
Message-ID: <475D7038.70303@stats.uwo.ca>

On 12/10/2007 11:26 AM, David Bickel wrote:
> Could anyone recommend a package for visualizing a likelihood function
> of two scalar parameters? I would prefer a three-dimensional plot
> similar to the kind Mathematica is known for, perhaps generated by a
> package not specific to likelihood.

The basic strategy is to calculate a matrix of values of the likelihood, 
corresponding to all combinations of values of two vectors.  Then 
persp()  (in graphics) or persp3d() (in rgl) can display the surface.

I don't know of anything that corresponds to curve() to do the matrix 
calculations automatically, but it may well exist.

Duncan Murdoch

> 
> David
> 
> ______________________________
> David R. Bickel
> Ottawa Institute of Systems Biology
> BMI Dept., University of Ottawa
> 
> http://www.oisb.ca/members.htm
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Thierry.ONKELINX at inbo.be  Mon Dec 10 17:49:52 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 10 Dec 2007 17:49:52 +0100
Subject: [R] Sweave : change value in rnw file to generate multiple
	"single"reports ?
In-Reply-To: <14256204.post@talk.nabble.com>
References: <14256204.post@talk.nabble.com>
Message-ID: <2E9C414912813E4EB981326983E0A10403FBF76D@inboexch.inbo.be>

I think something like this would work.

Change your query to 

rs <- dbSendQuery(con, statement = paste("select * from treatdata where
name='", whichDevice,"'", sep = ""))

And then create a script like.

whichDevice <- "Device1"
Sweave("myfile.rnw")
whichDevice <- "Device2"
Sweave("myfile.rnw")

HTH,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be 
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: r-help-bounces op r-project.org [mailto:r-help-bounces op r-project.org]
Namens Ptit_Bleu
Verzonden: maandag 10 december 2007 17:26
Aan: r-help op r-project.org
Onderwerp: [R] Sweave : change value in rnw file to generate multiple
"single"reports ?


Hello,

I'm still trying to make the life of my colleagues easier. Nice, isn't
it ?
At the moment, I'm looking for a way to generate multiple "single
report".
In fact I have a .rnw file which send a query to a MySQL database
(rs<-dbSendQuery(con, statement="select * from treatdata where
name='Device1'")

But of course my colleagues have many devices and don't want to enter
the
rnw file to change the name of the device.

Is there a way to pass arguments to Sweave like Sweave("myfile.rnw",
namevar="Device2") and it will change namevar by Device2 into the file
myfile.rnw before creating the .tex file ?
Or is it possible to do it via another language, that is: loading the
rnw
file, modifying it, saving it and finally calling Sweave ?

Whatever the solution, I need you help. In advance thank you.
Ptit Bleu.   
-- 
View this message in context:
http://www.nabble.com/Sweave-%3A-change-value-in-rnw-file-to-generate-mu
ltiple-%22single%22-reports---tp14256204p14256204.html
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help op r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Mon Dec 10 18:20:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Dec 2007 17:20:44 +0000 (GMT)
Subject: [R] 3-D plot of likelihood
In-Reply-To: <475D6DFC.6080902@statistik.uni-dortmund.de>
References: <D33A5B5753579B44AD89F58D40147582043298BB@MSMAIL2.uottawa.o.univ>
	<475D6DFC.6080902@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.64.0712101717460.24878@gannet.stats.ox.ac.uk>

On Mon, 10 Dec 2007, Uwe Ligges wrote:

> See ?persp

If you have a suitable OS, I would recommend using package rgl and 
persp3d: the ability to change viewpoint interactively is very useful.

>
> Uwe Ligges
>
> David Bickel wrote:
>> Could anyone recommend a package for visualizing a likelihood function
>> of two scalar parameters? I would prefer a three-dimensional plot
>> similar to the kind Mathematica is known for, perhaps generated by a
>> package not specific to likelihood.
>>
>> David
>>
>> ______________________________
>> David R. Bickel
>> Ottawa Institute of Systems Biology
>> BMI Dept., University of Ottawa
>>
>> http://www.oisb.ca/members.htm
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tura at centroin.com.br  Mon Dec 10 18:48:14 2007
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Mon, 10 Dec 2007 15:48:14 -0200
Subject: [R] CART analysis
In-Reply-To: <20071209212456.zi59tboc8wg4occ4@webmail.polimi.it>
References: <20071209212456.zi59tboc8wg4occ4@webmail.polimi.it>
Message-ID: <1197308894.28860.3.camel@R3-Thux>


On Sun, 2007-12-09 at 21:24 +0100, anna.paganoni at polimi.it wrote:
> I would like to know if is it possible implemet a partitioning tree  
> using a function like rpart, or mvpart, and with formula a glm-object  
> (as a logistic models) or a robust linear regression (as least sum of  
> absolute errors).
> In this case, the appropriate "method" to use is "mrt"? Or another one?
> Thanks,
> Anna Maria Paganoni

Well, 


I think de package tree solve your problem. If your response variable
don't use logistic approach if your variable is numeric use standard
formula

Any doubt read :
http://cran.r-project.org/doc/packages/tree.pdf


and mail me 

-- 
Bernardo Rangel Tura, M.D,MPH,Ph.D
National Institute of Cardiology
Brazil


From ashoka.polpitiya at gmail.com  Mon Dec 10 18:55:36 2007
From: ashoka.polpitiya at gmail.com (Ashoka Polpitiya)
Date: Mon, 10 Dec 2007 09:55:36 -0800
Subject: [R] Rerolling k-means
In-Reply-To: <475D3452.6070400@u-paris10.fr>
References: <mailman.21.1197284404.31132.r-help@r-project.org>
	<475D3452.6070400@u-paris10.fr>
Message-ID: <fb74d7d50712100955u173ce10aiab099163cc6803c7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/4fab522d/attachment.pl 

From jmburgos at u.washington.edu  Mon Dec 10 19:00:15 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Mon, 10 Dec 2007 10:00:15 -0800
Subject: [R] Help rewriting looping structure?
In-Reply-To: <14196412.post@talk.nabble.com>
References: <14196412.post@talk.nabble.com>
Message-ID: <475D7EAF.4040602@u.washington.edu>

Hi TLowe,

I'm not quite sure if I understand what you are trying to do.  If you 
are trying to get the cumulative sum of your data frame along each 
column you can simply do

rcumsum=function(x){cumsum(x)/sum(x)}
apply(tdat,2,rcumsum)

Yet that is not what your code is doing.  With a bit of clarification I 
may help you some more.

Julian


TLowe wrote:
> Hey Folks,
> 
> Could somebody help me rewrite the following code?
> 
> I am looping through all records across 5 fields to calculate the cumulative
> percentage of each record (relative to each individual field).
> 
> Is there a way to rewrite it so I don't have to loop through each individual
> record?
> 
> ##### tdat is my data frame
> ##### j is my field index
> ##### k is my record index
> ##### tsum is the sum of all values in field j
> ##### tmp is a vector containing the values in field j
> ##### tdat[k,paste("cpct,j,sep="")] creates new fields "cpct1",...,"cpct5" 
> 
> 
for(j in 1:5) {
  tsum<- sum(tdat[,j]);
    for(k in 1:nrow(tdat)) {
      td<- tdat[k,j];
      tmp<-tdat[,j];
  ##### sum values <= to current value and divide by the total sum
    tdat[k,paste("cpct",j,sep="")]<- sum(tmp[tmp <= td]) / tsum;
    }
}
> 
> 
> Thanks,
> TLowe


From murdoch at stats.uwo.ca  Mon Dec 10 19:32:43 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 10 Dec 2007 13:32:43 -0500
Subject: [R] Sweave : change value in rnw file to generate
 multiple	"single"reports ?
In-Reply-To: <2E9C414912813E4EB981326983E0A10403FBF76D@inboexch.inbo.be>
References: <14256204.post@talk.nabble.com>
	<2E9C414912813E4EB981326983E0A10403FBF76D@inboexch.inbo.be>
Message-ID: <475D864B.9080601@stats.uwo.ca>

On 12/10/2007 11:49 AM, ONKELINX, Thierry wrote:
> I think something like this would work.
> 
> Change your query to 
> 
> rs <- dbSendQuery(con, statement = paste("select * from treatdata where
> name='", whichDevice,"'", sep = ""))
> 
> And then create a script like.
> 
> whichDevice <- "Device1"
> Sweave("myfile.rnw")
> whichDevice <- "Device2"
> Sweave("myfile.rnw")

Yes, I had forgotten that works.  I usually use R CMD Sweave, where (as 
Gabor mentioned) it's harder to set variables.

Duncan Murdoch


From ggrothendieck at gmail.com  Mon Dec 10 19:46:44 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Dec 2007 13:46:44 -0500
Subject: [R] Sweave : change value in rnw file to generate multiple
	"single"reports ?
In-Reply-To: <475D864B.9080601@stats.uwo.ca>
References: <14256204.post@talk.nabble.com>
	<2E9C414912813E4EB981326983E0A10403FBF76D@inboexch.inbo.be>
	<475D864B.9080601@stats.uwo.ca>
Message-ID: <971536df0712101046l593e1e47vfba877db96510717@mail.gmail.com>

On Dec 10, 2007 1:32 PM, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 12/10/2007 11:49 AM, ONKELINX, Thierry wrote:
> > I think something like this would work.
> >
> > Change your query to
> >
> > rs <- dbSendQuery(con, statement = paste("select * from treatdata where
> > name='", whichDevice,"'", sep = ""))
> >
> > And then create a script like.
> >
> > whichDevice <- "Device1"
> > Sweave("myfile.rnw")
> > whichDevice <- "Device2"
> > Sweave("myfile.rnw")
>
> Yes, I had forgotten that works.  I usually use R CMD Sweave, where (as
> Gabor mentioned) it's harder to set variables.
>

Note that while the Thierry's idea is quite good it since it avoids the use
of shell or batch files it still does have the disadvantage of requiring an
extra file in the case where the report is being spawned from some other
system such as a web application written in php as it requires both a
.R file and a .Rnw file.

On the other hand if R CMD Sweave accepted arguments one could do it all
in the Sweave file and avoid the extra complexity of an intermediate .R or
shell/batch file in the first place.


From epurdom at stat.Berkeley.EDU  Mon Dec 10 20:11:30 2007
From: epurdom at stat.Berkeley.EDU (Elizabeth Purdom)
Date: Mon, 10 Dec 2007 11:11:30 -0800
Subject: [R] Question re: RWinEdt conflicting with my WinEdt
Message-ID: <475D8F62.1090008@stat.berkeley.edu>

Hi,

This is a question regarding the RWinEdt package for R. I have WinEdt 
5.5, RWinEdt 1.7-9, and R 2.6.0 on WindowsXP.

Somehow my configuration or start up files between RWinEdt and WinEdt 
are getting confused. Usually when I open either one, the last files I 
was working on *with that program* are opened automatically. So if I 
last used mydoc.tex in WinEdt and myprogram.R in RWinEdt those would 
open in the appropriate programs and there was no mixing up of the file 
lists between the two. But in the last month or so (and I don't think 
I've updated anything) whatever files I was last working with on either 
one show up. So if close WinEdt and then open RWinEdt, mydoc.tex shows 
up instead of myprogram.R, and vice versa. I assume some kind of 
initialization files have been moved or deleted, so they are borrowing 
from each other, but I don't know how to fix this and it's quite 
annoying. This is the case whether I start RWinEdt within R, or from a 
shortcut on my desktop with no interface with R.

Thanks for any help,
Elizabeth Purdom


From topkatz at msn.com  Mon Dec 10 20:16:52 2007
From: topkatz at msn.com (Talbot Katz)
Date: Mon, 10 Dec 2007 14:16:52 -0500
Subject: [R] Reading through a group of .RData files
Message-ID: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/b7655aef/attachment.pl 

From ligges at statistik.uni-dortmund.de  Mon Dec 10 20:26:45 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 10 Dec 2007 20:26:45 +0100
Subject: [R] Question re: RWinEdt conflicting with my WinEdt
In-Reply-To: <475D8F62.1090008@stat.berkeley.edu>
References: <475D8F62.1090008@stat.berkeley.edu>
Message-ID: <475D92F5.1000600@statistik.uni-dortmund.de>

WinEdt's default is:
Start with WinEdt.ini initialization file and that again uses WinEdt.prj 
as the default for loading your .tex files, for example.

Using RWinEdt, R.ini is used as the initialization file and that one 
uses R.prj as the default for loading your .R files.

If that does not work as before, you have changed something. Just take a 
look whether the settings are correct in both instances of WinEdt.

Uwe Ligges



Elizabeth Purdom wrote:
> Hi,
> 
> This is a question regarding the RWinEdt package for R. I have WinEdt 
> 5.5, RWinEdt 1.7-9, and R 2.6.0 on WindowsXP.
> 
> Somehow my configuration or start up files between RWinEdt and WinEdt 
> are getting confused. Usually when I open either one, the last files I 
> was working on *with that program* are opened automatically. So if I 
> last used mydoc.tex in WinEdt and myprogram.R in RWinEdt those would 
> open in the appropriate programs and there was no mixing up of the file 
> lists between the two. But in the last month or so (and I don't think 
> I've updated anything) whatever files I was last working with on either 
> one show up. So if close WinEdt and then open RWinEdt, mydoc.tex shows 
> up instead of myprogram.R, and vice versa. I assume some kind of 
> initialization files have been moved or deleted, so they are borrowing 
> from each other, but I don't know how to fix this and it's quite 
> annoying. This is the case whether I start RWinEdt within R, or from a 
> shortcut on my desktop with no interface with R.
> 
> Thanks for any help,
> Elizabeth Purdom
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bcarvalh at jhsph.edu  Mon Dec 10 20:31:19 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 10 Dec 2007 14:31:19 -0500 (EST)
Subject: [R] Reading through a group of .RData files
In-Reply-To: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>
References: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0712101427520.31359@enigma.local>

note that load() returns, invisibly, a string with the names of the 
objects that were loaded. something in the lines of:

myObj <- load(file.path(fnDir, cvListFiles[i]))
myFunction(get(myObj))
rm(list=myObj)

might be closer to what you want.

moreover, if length(myObj) > 1, you might want sth like:

lapply(myObj, function(x) myFunction(get(x)))

instead...

best
b

On Mon, 10 Dec 2007, Talbot Katz wrote:

>
> Hi.
>
> I have a procedure that reads a directory, loops through a set of particular .RData files, loading each one, and feeding its object(s) into a function, as follows:
>
> cvListFiles<-list.files(fnDir);
> for(i in grep(paste("^",pfnStub,".*\\.RData$",sep=""),cvListFiles)){
> load(paste(fnDir,cvListFiles[i],sep="/"));
> myFunction(rliObject);
> rm(rliObject);
> };
>
> where fnDir is the directory I'm reading, and pfnStub is a string that begins the name of each of the files I want to load.  As you can see, I'm assuming that each of the selected .RData files contains an object named "rliObject" and I'm hoping that nothing in any of the files I'm loading overwrites an object in my environment.  I'd like to clean this up so that I can extract the object(s) from each data file, and feed them to my function, whatever their names are, without corrupting my environment.  I'd appreciate any assistance.  Thanks!
>
> --  TMK  --212-460-5430 home917-656-5351 cell
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From emmanuel.brasil at gmail.com  Mon Dec 10 20:41:11 2007
From: emmanuel.brasil at gmail.com (Pedro Emmanuel Alvarenga Americano do Brasil)
Date: Mon, 10 Dec 2007 16:41:11 -0300
Subject: [R] SAS PROC NLMIXED into R
Message-ID: <a37cfb5a0712101141w560e2282qe1593c9d219f8d4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/a76b2925/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Dec 10 20:47:31 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Dec 2007 19:47:31 +0000 (GMT)
Subject: [R] Reading through a group of .RData files
In-Reply-To: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>
References: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0712101946440.26410@gannet.stats.ox.ac.uk>

On Mon, 10 Dec 2007, Talbot Katz wrote:

>
> Hi.
>
> I have a procedure that reads a directory, loops through a set of 
> particular .RData files, loading each one, and feeding its object(s) 
> into a function, as follows:
>
> cvListFiles<-list.files(fnDir);
> for(i in grep(paste("^",pfnStub,".*\\.RData$",sep=""),cvListFiles)){
> load(paste(fnDir,cvListFiles[i],sep="/"));
> myFunction(rliObject);
> rm(rliObject);
> };
>
> where fnDir is the directory I'm reading, and pfnStub is a string that 
> begins the name of each of the files I want to load.  As you can see, 
> I'm assuming that each of the selected .RData files contains an object 
> named "rliObject" and I'm hoping that nothing in any of the files I'm 
> loading overwrites an object in my environment.  I'd like to clean this 
> up so that I can extract the object(s) from each data file, and feed 
> them to my function, whatever their names are, without corrupting my 
> environment.  I'd appreciate any assistance.  Thanks!

You can load() each to a new enviroment: see the arguments of load().

>
> --  TMK  --212-460-5430 home917-656-5351 cell
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Peter.Ruckdeschel at uni-bayreuth.de  Mon Dec 10 21:04:46 2007
From: Peter.Ruckdeschel at uni-bayreuth.de (Peter Ruckdeschel)
Date: Mon, 10 Dec 2007 12:04:46 -0800 (PST)
Subject: [R] Integral implicit function
In-Reply-To: <20071206224507660.00000003820@ae-030530>
References: <20071206224507660.00000003820@ae-030530>
Message-ID: <14260535.post@talk.nabble.com>


Have you tried vectorizing the inner function? 
Hint: integrate() calls the integrand vectorwise... 
in your example,
      integrate(Vectorize(f),lower=1,upper=2)
should do what you expected.

Best,
Peter


Eddy H. G. Bekkers wrote:
> 
> Hi,
> 
> Could somebody help me with the following. I want to calculate the
> integral over an implicit function. I thought to integrate over a function
> depending on uniroot. In previous topics I found a thread about finding
> the root of an integral. And that works. But the other way around, does
> not work. Does R support this?
> 
> I included the following example. The function in the example is very easy
> and can be solved explicitly, but when it does not work for such an easy
> function it will certainly not work for a more difficult function. First
> the root of an integral (which works) and then the integral of a function
> dependent on uniroot:
> 
> # Calculating the root of an integral
> 
> a<- function(x,y)
>     {x-y}
> 
> b<- function(y)
>     {integrate(a,lower=1,upper=2,y=y)$value}
> 
> d<- uniroot(b,c(0,10))$root
> 
> print(d)
> 
> # Calculating the integral of a function dependent on uniroot
> 
> e<- function(u,v)
>     {u-v}
> 
> f<- function(v)
>     {uniroot(e,c(0,10),v=v)$root}
> 
> g<- integrate(f,lower=1,upper=2)$value
> 
> print(g)
> 
> Does anyone have suggestions how to proceed? By the way, the implicit
> function I am targeting does have a unique solution, it is only not
> explicitly solvable, i.e. in the example above, you cannot solve u as a
> function of v explicitly, so as to substitute it in the integrand. 
> 
> 
> Thanks a lot in advance for your help,
> 
> Best regards,
> 
> Eddy Bekkers
> Department of Economics
> Erasmus University Rotterdam
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Integral-implicit-function-tp14201932p14260535.html
Sent from the R help mailing list archive at Nabble.com.


From epurdom at stat.Berkeley.EDU  Mon Dec 10 21:04:39 2007
From: epurdom at stat.Berkeley.EDU (Elizabeth Purdom)
Date: Mon, 10 Dec 2007 12:04:39 -0800
Subject: [R] Question re: RWinEdt conflicting with my WinEdt
In-Reply-To: <475D92F5.1000600@statistik.uni-dortmund.de>
References: <475D8F62.1090008@stat.berkeley.edu>
	<475D92F5.1000600@statistik.uni-dortmund.de>
Message-ID: <475D9BD7.4050600@stat.berkeley.edu>

Hello Uwe, Thanks for your reply. I realized in RWinEdt that it was 
linking to another project that I had made in WinEdt -- I didn't realize 
I could create or change projects within RWinEdt, so I don't know how I 
managed that! But now that I do know, that is also very convenient. So I 
think it's solved, or at least if it reoccurs I can work around it.
Thanks,
Elizabeth

Uwe Ligges wrote:
> WinEdt's default is:
> Start with WinEdt.ini initialization file and that again uses WinEdt.prj 
> as the default for loading your .tex files, for example.
> 
> Using RWinEdt, R.ini is used as the initialization file and that one 
> uses R.prj as the default for loading your .R files.
> 
> If that does not work as before, you have changed something. Just take a 
> look whether the settings are correct in both instances of WinEdt.
> 
> Uwe Ligges
> 
> 
> 
> Elizabeth Purdom wrote:
>> Hi,
>>
>> This is a question regarding the RWinEdt package for R. I have WinEdt 
>> 5.5, RWinEdt 1.7-9, and R 2.6.0 on WindowsXP.
>>
>> Somehow my configuration or start up files between RWinEdt and WinEdt 
>> are getting confused. Usually when I open either one, the last files I 
>> was working on *with that program* are opened automatically. So if I 
>> last used mydoc.tex in WinEdt and myprogram.R in RWinEdt those would 
>> open in the appropriate programs and there was no mixing up of the 
>> file lists between the two. But in the last month or so (and I don't 
>> think I've updated anything) whatever files I was last working with on 
>> either one show up. So if close WinEdt and then open RWinEdt, 
>> mydoc.tex shows up instead of myprogram.R, and vice versa. I assume 
>> some kind of initialization files have been moved or deleted, so they 
>> are borrowing from each other, but I don't know how to fix this and 
>> it's quite annoying. This is the case whether I start RWinEdt within 
>> R, or from a shortcut on my desktop with no interface with R.
>>
>> Thanks for any help,
>> Elizabeth Purdom
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From maj at stats.waikato.ac.nz  Mon Dec 10 21:16:26 2007
From: maj at stats.waikato.ac.nz (maj at stats.waikato.ac.nz)
Date: Tue, 11 Dec 2007 09:16:26 +1300 (NZDT)
Subject: [R] Large determinant problem
Message-ID: <1172.203.109.180.75.1197317786.squirrel@webmail.scms.waikato.ac.nz>



Ravi Varadhan wrote:
> It is evident that you do not have enough information in the data to
> estimate 9 mixture components.  This is clearly indicated by a positive
> semi-definite information matrix, S, that is less than full rank.  You can
> monitor the rank of the information matrix, as you increase the number of
> components, and stop when you suspect rank-deficiency.
>
> Ravi.
>

What you say is likely to be true, but I was interested to see if this was
reflected in some of the traditional model selection criteria (AIC, BIC,
...). In this case numerical problems caused by overfitting prevent the
calculation of a diagnostic measure for overfitting. Incidentally here
other measures of overfitting that I was able to calculate continue to
indicate underfitting. Of course in the mixture model case these measures
are heuristic only as the assumptions behind their asymptotic
justification are not valid.

Murray
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From Jeff_Bardwell at baylor.edu  Mon Dec 10 21:17:38 2007
From: Jeff_Bardwell at baylor.edu (Bardwell, Jeff H)
Date: Mon, 10 Dec 2007 14:17:38 -0600
Subject: [R] Multiple Reponse CART Analysis
Message-ID: <D3EBC71C44A1AA4BBA17B0B5AC68A0C501574380@MAIL-I-K.baylor.edu>

Dear R friends-
 
I'm attempting to generate a regression tree with one gradient predictor and multiple responses, trying to test if change in size (turtle.data$Clength) acts as a single predictor of ten multiple diet taxa abundances (prey.data)  Neither rpart or mvpart seem to allow me to do multiple responses.  (Or if they can, I'm not using the functions properly.)

> library(rpart)
> turtle.rtree<-rpart(prey.data~., data=turtle.data$Clength, method="anova", maxsurrogate=0, minsplit=8, minbucket=4, xval=10); plot(turtle.rtree); text(turtle.rtree)
Error in terms.formula(formula, data = data) : 
        '.' in formula and no 'data' argument

When I switch response for predictor, it works.  But this is the opposite of what I wanted to test and gives me splits at abundance values, not carapace length values.
> turtle.rtree<-rpart(turtle.data$Clength~., data=prey.data, method="anova", maxsurrogate=0, minsplit=8, minbucket=4, xval=10); plot(turtle.rtree); text(turtle.rtree)
> 
 
I've heard polymars recommended for this sort of situation.  I've downloaded the polyspline library, but get bogged down in the equation.  Also, it doesn't seem like polymars will generate a tree even if I do get it working.  Can rpart be modified in some way to accomodate multiple response parameters?  If anyone's ever come across this situation before, pointers would be much appreciated.  Thanks.
 
Sincerely,
Jeff Bardwell
  
 
Jeff H Bardwell, M.S.
Biology Department
ENV 1101 Lab Coordinator
Goebel 115, OH: Thu 1pm-4pm
710-6596 (e-mail preferred)


From p.murrell at auckland.ac.nz  Mon Dec 10 21:27:13 2007
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 11 Dec 2007 09:27:13 +1300
Subject: [R] Set the grid.layout programmatically?
In-Reply-To: <858353.71152.qm@web56606.mail.re3.yahoo.com>
References: <858353.71152.qm@web56606.mail.re3.yahoo.com>
Message-ID: <475DA121.7070108@stat.auckland.ac.nz>

Hi


Felipe Carrillo wrote:
> Hello all:
> Does anyone know if the grid layout function can be
> set dynamically? For example if I have a column that
> will be changing as new data is added monthly I would
> like grid to detect the new data and create another
> row or column for the new plot on the same window
> instead of adding the new layout manually. The code
> below shows a grid.layout of hardcoded two rows and
> two columns.Thanks
> 
> pushViewport(viewport(layout=grid.layout(2,2))) 


Take a look at grid.frame()

Paul


> Felipe D. Carrillo
>   Fishery Biologist
>   US Fish & Wildlife Service
>   California, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From king.812 at osu.edu  Mon Dec 10 21:37:14 2007
From: king.812 at osu.edu (WAYNE KING)
Date: Mon, 10 Dec 2007 15:37:14 -0500
Subject: [R] Problem with graphics device in Mac OS X
Message-ID: <a1174a3d23.a3d23a1174@osu.edu>

Hello List,
   I am teaching a basic course where students are encouraged to use R. There are a few students using Mac OS X. As a test we downloaded and installed the latest .dmg file (R-2.6.1.dmg) onto a intel Mac running 10.5.1. A device query yields

> getOption("device")
"quartz"

But any plot command does not bring up a plot (e.g. plot(), boxplot(), hist()).

I found a thread concerning X11 windows under Mac OS X but I feel these users will most likely be just using the native quartz device.

Invoking a call to quartz() first does not seem to help, e.g.

>quartz()
>plot(rnorm(100,0,1))

produces no output and no error message (Nothing happens). A call to dev.cur() seems to indicate a device is active.
>quartz()
>dev.cur()
quartz
2

but again a plot command produces no figure. Sorry am I not a Mac OS user and I did check the archives but found mostly discussions on X11() under Mac OS X. 

Wayne


From gilhamto at gmail.com  Mon Dec 10 21:42:37 2007
From: gilhamto at gmail.com (G Ilhamto)
Date: Mon, 10 Dec 2007 15:42:37 -0500
Subject: [R] help with fatal error message
Message-ID: <bebd16360712101242g276a7c9dy2e2c57a224ad972@mail.gmail.com>

Dear helper,

After some cleaning, I found out that I cannot open my R2.6.0 console
with message: "Fatal error: unable to restore saved data in .Rdata",
and every time I retry it just kick me off. I am wondering if I still
be able to retrieve the data that I've been working for a month.

I have installed the new R2.6.1 and it has no problem to open, only
there is nothing in it I also wonder if I will be able to transfer all
the files in 2.6.0 to 2.6.1. I appreciate anyone who can help.

Thank you,
Ilham


From hanson at depauw.edu  Mon Dec 10 21:47:06 2007
From: hanson at depauw.edu (Bryan Hanson)
Date: Mon, 10 Dec 2007 15:47:06 -0500
Subject: [R] Problem with graphics device in Mac OS X
In-Reply-To: <a1174a3d23.a3d23a1174@osu.edu>
Message-ID: <C3830FFA.9BE6%hanson@depauw.edu>

For whatever reason, on the Mac, you have to open a new Quartz device window
before making the graphics call.  So, from the menu, pull down under Window
to New Quartz Device Window.  Then all graphics calls go to that (initially
empty) window, and any further calls replace the previous contents of the
window.  This window doesn't print so well, but your students can divert
their output to a pdf easily for really nice plots.

BTW, people were reporting problems with OS 10.5 and R.  These may have been
fixed, but if you have trouble, it's discussed in the archives.

Bryan


On 12/10/07 3:37 PM, "WAYNE KING" <king.812 at osu.edu> wrote:

> Hello List,
>    I am teaching a basic course where students are encouraged to use R. There
> are a few students using Mac OS X. As a test we downloaded and installed the
> latest .dmg file (R-2.6.1.dmg) onto a intel Mac running 10.5.1. A device query
> yields
> 
>> getOption("device")
> "quartz"
> 
> But any plot command does not bring up a plot (e.g. plot(), boxplot(),
> hist()).
> 
> I found a thread concerning X11 windows under Mac OS X but I feel these users
> will most likely be just using the native quartz device.
> 
> Invoking a call to quartz() first does not seem to help, e.g.
> 
>> quartz()
>> plot(rnorm(100,0,1))
> 
> produces no output and no error message (Nothing happens). A call to dev.cur()
> seems to indicate a device is active.
>> quartz()
>> dev.cur()
> quartz
> 2
> 
> but again a plot command produces no figure. Sorry am I not a Mac OS user and
> I did check the archives but found mostly discussions on X11() under Mac OS X.
> 
> Wayne
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jo.irisson at gmail.com  Mon Dec 10 21:50:25 2007
From: jo.irisson at gmail.com (jiho)
Date: Mon, 10 Dec 2007 21:50:25 +0100
Subject: [R] Problem with graphics device in Mac OS X
In-Reply-To: <a1174a3d23.a3d23a1174@osu.edu>
References: <a1174a3d23.a3d23a1174@osu.edu>
Message-ID: <12510AA8-4239-4F0B-ABD0-1449C41FE7B7@gmail.com>

I think you should post this to R-Sig-Mac.
I don't notice any problem at all on my system with the same  
configuration.

On 2007-December-10  , at 21:37 , WAYNE KING wrote:

> Hello List,
>   I am teaching a basic course where students are encouraged to use  
> R. There are a few students using Mac OS X. As a test we downloaded  
> and installed the latest .dmg file (R-2.6.1.dmg) onto a intel Mac  
> running 10.5.1. A device query yields
>
>> getOption("device")
> "quartz"
>
> But any plot command does not bring up a plot (e.g. plot(),  
> boxplot(), hist()).
>
> I found a thread concerning X11 windows under Mac OS X but I feel  
> these users will most likely be just using the native quartz device.
>
> Invoking a call to quartz() first does not seem to help, e.g.
>
>> quartz()
>> plot(rnorm(100,0,1))
>
> produces no output and no error message (Nothing happens). A call to  
> dev.cur() seems to indicate a device is active.
>> quartz()
>> dev.cur()
> quartz
> 2
>
> but again a plot command produces no figure. Sorry am I not a Mac OS  
> user and I did check the archives but found mostly discussions on  
> X11() under Mac OS X.
>
> Wayne
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

JiHO
---
http://jo.irisson.free.fr/


From Greg.Snow at imail.org  Mon Dec 10 22:04:10 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Mon, 10 Dec 2007 14:04:10 -0700
Subject: [R] Sweave : change value in rnw file to generate multiple
 "single" reports ?
In-Reply-To: <14256204.post@talk.nabble.com>
References: <14256204.post@talk.nabble.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBD627E0@LP-EXCHVS07.CO.IHC.COM>

Others have given some good advice, and what works best for you will
depend on exactly what you want to do and how you think about the
problem.  Here are just a couple of other ideas to throw into the mix.

If you want to edit the .rnw file, then the tools sed, awk, or perl (or
probably several others) can all do this quite easily.  For example if
you have a file (base.rnw) that has MYDEVICE everywhere that you want to
replace with Device2 or whatever then the following command at a command
prompt will create a new file (dev2.rnw) that is a copy of base.rnw with
all occurences of MYDEVICE replaced with Device2:

 perl -pe "s/MYDEVICE/Device2/g" base.rnw > dev2.rnw

Another tool that you may want to learn is "make".  You can set up a
makefile that shows how each file relates to the next in a chain of
steps, then just type something like:

 make device2.pdf

At the command line and it will automatically make the changes to your
.rnw file and possibly to data or other files involved, then run all the
steps needed to create device2.pdf, then deletes all the intermediate
files that you don't need (device2.dvi, ...).  It also has the benefit
of being able to update all the appropriate files when you update a data
file, script file, or .rnw file.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Ptit_Bleu
> Sent: Monday, December 10, 2007 9:26 AM
> To: r-help at r-project.org
> Subject: [R] Sweave : change value in rnw file to generate 
> multiple "single" reports ?
> 
> 
> Hello,
> 
> I'm still trying to make the life of my colleagues easier. 
> Nice, isn't it ?
> At the moment, I'm looking for a way to generate multiple 
> "single report".
> In fact I have a .rnw file which send a query to a MySQL 
> database (rs<-dbSendQuery(con, statement="select * from 
> treatdata where
> name='Device1'")
> 
> But of course my colleagues have many devices and don't want 
> to enter the rnw file to change the name of the device.
> 
> Is there a way to pass arguments to Sweave like Sweave("myfile.rnw",
> namevar="Device2") and it will change namevar by Device2 into 
> the file myfile.rnw before creating the .tex file ?
> Or is it possible to do it via another language, that is: 
> loading the rnw file, modifying it, saving it and finally 
> calling Sweave ?
> 
> Whatever the solution, I need you help. In advance thank you.
> Ptit Bleu.   
> --
> View this message in context: 
> http://www.nabble.com/Sweave-%3A-change-value-in-rnw-file-to-g
> enerate-multiple-%22single%22-reports---tp14256204p14256204.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From keithlj at suddenlink.net  Mon Dec 10 22:36:16 2007
From: keithlj at suddenlink.net (Keith Jones)
Date: Mon, 10 Dec 2007 15:36:16 -0600
Subject: [R] CRAN Index Problems
Message-ID: <p06240800c383613c0cb7@[192.168.0.2]>

Hi,

I am getting the following error: Warning: unable to access index for 
repository 
http://cran.hostingzero.com/bin/macosx/universal/contrib/2.6 with I 
run "Get List" in the Package Install window.  I am running Leopard 
and 2.6.1.  What can I do?

Thanks,

Keith Jones


From nmprista at fc.ul.pt  Mon Dec 10 22:41:03 2007
From: nmprista at fc.ul.pt (Nuno Prista)
Date: Mon, 10 Dec 2007 16:41:03 -0500
Subject: [R] function centralm - does it exist
Message-ID: <001f01c83b75$65b78ec0$c0685280@ts.odu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/09749c5f/attachment.pl 

From mwkimpel at gmail.com  Mon Dec 10 22:43:20 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Mon, 10 Dec 2007 16:43:20 -0500
Subject: [R] 00LOCK error with site-library
Message-ID: <475DB2F8.1000108@gmail.com>

I have identical R.profiles and R_HOME directories set up on both my 
local machine and a remote linux cluster. To keep my libraries and R 
install separate, I use a site-library on both machines.

The first line of my .Rprofile is:
'.libPaths(new= "~/R_HOME/site-library") #tell R where site-library is'

Until R-2.6.0 this was working fine on both machines, but since I have 
been unable to upgrade packages on the remote machine. Every attempt at 
upgrade results in a directory '00LOCK' being place in the 
site-directory and then the upgrade fails.

This has happened with R-2.6.0, 2.6.1, and now with R-devel (2.7.0).

If I have this setup incorrectly I am puzzled as to why it works with my 
local machine and why it worked with versions of R prior to 2.6.0.

Ideas?
Thanks,
Mark

-- 

Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com


From murdoch at stats.uwo.ca  Mon Dec 10 22:49:11 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 10 Dec 2007 16:49:11 -0500
Subject: [R] CRAN Index Problems
In-Reply-To: <p06240800c383613c0cb7@[192.168.0.2]>
References: <p06240800c383613c0cb7@[192.168.0.2]>
Message-ID: <475DB457.30109@stats.uwo.ca>

On 10/12/2007 4:36 PM, Keith Jones wrote:
> Hi,
> 
> I am getting the following error: Warning: unable to access index for 
> repository 
> http://cran.hostingzero.com/bin/macosx/universal/contrib/2.6 with I 
> run "Get List" in the Package Install window.  I am running Leopard 
> and 2.6.1.  What can I do?

You need to change your CRAN mirror.  Apparently hostingzero.com is not 
responding.  Running chooseCRANmirror() from the console will let you do 
that.  (There may be a way from the menu too.)

Duncan Murdoch


From john.bullock at aya.yale.edu  Mon Dec 10 17:56:01 2007
From: john.bullock at aya.yale.edu (John G. Bullock)
Date: Mon, 10 Dec 2007 08:56:01 -0800
Subject: [R] lattice: placing y-axis labels on right-hand side of panel
	when relation="sliced"
References: <fji1d6$igp$1@ger.gmane.org>
Message-ID: <fjjr4b$k0i$1@ger.gmane.org>


>        I'm using lattice to create a multi-panel figure.  I would like
> to draw each panel's y-axis ticks and labels on the right-hand
> side of the panel.  Ordinarily, I would do this by specifying
> scales=list(y=list(draw=T, alternating=2)).  But in this case, I am
> using relation="sliced" to determine the y-axis limits.  So
> "alternating" is ignored.  Is there any way -- short of using grid
> functions -- to place the y-axis ticks and labels on the right-hand
> sides of these panels?

panel.axis() is what I had in mind.


From keithlj at suddenlink.net  Mon Dec 10 18:03:18 2007
From: keithlj at suddenlink.net (Keith Jones)
Date: Mon, 10 Dec 2007 11:03:18 -0600
Subject: [R] R crashes in Leopard
Message-ID: <p06240800c38320a205a8@[192.168.0.2]>

Hi,

I have upgraded to Leopard and I just installed 2.6.1.  I opened R 
and then I opened package installer and clicked "get List".  After 
about five seconds R stopped responding. and Leopard reported that R 
had stopped responding and wanted to sent a report to Apple.  What do 
I need to do get a list of packages so I can download and install 
them?

Thanks,

Keith Jones


From gavin.simpson at ucl.ac.uk  Mon Dec 10 23:01:15 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 10 Dec 2007 22:01:15 +0000
Subject: [R] Multiple Reponse CART Analysis
In-Reply-To: <D3EBC71C44A1AA4BBA17B0B5AC68A0C501574380@MAIL-I-K.baylor.edu>
References: <D3EBC71C44A1AA4BBA17B0B5AC68A0C501574380@MAIL-I-K.baylor.edu>
Message-ID: <1197324075.7454.25.camel@graptoleberis.geog.ucl.ac.uk>

On Mon, 2007-12-10 at 14:17 -0600, Bardwell, Jeff H wrote:
> Dear R friends-
>  
> I'm attempting to generate a regression tree with one gradient
> predictor and multiple responses, trying to test if change in size
> (turtle.data$Clength) acts as a single predictor of ten multiple diet
> taxa abundances (prey.data)  Neither rpart or mvpart seem to allow me
> to do multiple responses.  (Or if they can, I'm not using the
> functions properly.)
> 
> > library(rpart)
> > turtle.rtree<-rpart(prey.data~., data=turtle.data$Clength,
> method="anova", maxsurrogate=0, minsplit=8, minbucket=4, xval=10);
> plot(turtle.rtree); text(turtle.rtree)
> Error in terms.formula(formula, data = data) : 
>         '.' in formula and no 'data' argument

rpart doesn't do multiple responses - try package mvpart for a drop-in
replacement. Alternatively look at package party.

Also, you are not using formula correctly. What you should have written
is:

prey.data ~ Clength, data = turtle.data

What R does is look for variables in the formula from within the data
argument, and IIRC data is supposed to be a data frame. If it doesn't
find what it needs in data, it looks in the workspace. This probably
isn't correct - the real answer having to do with environments and
parents etc., but effectively in this case this is what happens.

> 
> When I switch response for predictor, it works.  But this is the
> opposite of what I wanted to test and gives me splits at abundance
> values, not carapace length values.
> > turtle.rtree<-rpart(turtle.data$Clength~., data=prey.data,
> method="anova", maxsurrogate=0, minsplit=8, minbucket=4, xval=10);
> plot(turtle.rtree); text(turtle.rtree)
> > 

Of course, it has to expand . from data and prey.data is a data frame. R
picks up turtle.data$Clength from the workspace. But this isn't a
multivariate tree. You are confusing the problem above with not being
able to deal with multiple responses.

If mvpart is not working then you need to show why and how it fails.
>From your description, the response is abundances of prey species, this
should be fine in mvpart.

Note though, that mvpart seems to need a data.matrix as the response so
something like this should work:

data.matrix(prey.data) ~ Clength, data = turtle.data

HTH

G

>  
> I've heard polymars recommended for this sort of situation.  I've
> downloaded the polyspline library, but get bogged down in the
> equation.  Also, it doesn't seem like polymars will generate a tree
> even if I do get it working.  Can rpart be modified in some way to
> accomodate multiple response parameters?  If anyone's ever come across
> this situation before, pointers would be much appreciated.  Thanks.
>  
> Sincerely,
> Jeff Bardwell

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From mark at wardle.org  Mon Dec 10 23:35:37 2007
From: mark at wardle.org (Mark Wardle)
Date: Mon, 10 Dec 2007 22:35:37 +0000
Subject: [R] Getting estimates from survfit.coxph
In-Reply-To: <200712101421.lBAELVF14219@hsrnfs-101.mayo.edu>
References: <200712101421.lBAELVF14219@hsrnfs-101.mayo.edu>
Message-ID: <b59a37130712101435w7b63c62ev5aae7b91876e7687@mail.gmail.com>

Can't ask for more than that!

Many many thanks for making all these tools available. I wince when I
see my colleagues struggling with SPSS or SAS! R and the bundled and
third-party libraries have saved me an inordinate amount of time and
effort!

Best wishes,

Mark

On 10/12/2007, Terry Therneau <therneau at mayo.edu> wrote:
>   The problem will be fixed in the next resease of the survival code.  (That is,
> it is fixed on our local version of R).  The summary.survfit result now includes
> an element 'table' containing the matrix that is shown by print.survfit.
>
>         Terry
>
>
>
> ______________________________________________________________________
> This email has been scanned by the MessageLabs Email Security System.
> For more information please visit http://www.messagelabs.com/email
> ______________________________________________________________________
>


-- 
Dr. Mark Wardle
Specialist registrar, Neurology
Cardiff, UK


From pgilbert at bank-banque-canada.ca  Mon Dec 10 23:50:33 2007
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 10 Dec 2007 17:50:33 -0500
Subject: [R] 00LOCK error with site-library
In-Reply-To: <475DB2F8.1000108@gmail.com>
References: <475DB2F8.1000108@gmail.com>
Message-ID: <475DC2B9.1060609@bank-banque-canada.ca>



Mark W Kimpel wrote:
> I have identical R.profiles and R_HOME directories set up on both my 
> local machine and a remote linux cluster. To keep my libraries and R 
> install separate, I use a site-library on both machines.
> 
> The first line of my .Rprofile is:
> '.libPaths(new= "~/R_HOME/site-library") #tell R where site-library is'
> 
> Until R-2.6.0 this was working fine on both machines, but since I have 
> been unable to upgrade packages on the remote machine. Every attempt at 
> upgrade results in a directory '00LOCK' being place in the 
> site-directory and then the upgrade fails.

This directory gets placed there and then removed when the upgrade is 
finished, to prevent two processes from trying to simultaneously update 
the directory. If you terminated an upgrade in an unexpected way, say by 
dropping your connection, then the file will be left and you need to 
manually remove it before you try to upgrade again.

If you have removed it, and it keeps re-appearing, it is a sign of 
something else going wrong (usually outside of R).

Paul
> 
> This has happened with R-2.6.0, 2.6.1, and now with R-devel (2.7.0).
> 
> If I have this setup incorrectly I am puzzled as to why it works with my 
> local machine and why it worked with versions of R prior to 2.6.0.
> 
> Ideas?
> Thanks,
> Mark
> 
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential in...{{dropped:26}}


From p.dalgaard at biostat.ku.dk  Mon Dec 10 23:51:37 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 10 Dec 2007 23:51:37 +0100
Subject: [R] Getting estimates from survfit.coxph
In-Reply-To: <200712101421.lBAELVF14219@hsrnfs-101.mayo.edu>
References: <200712101421.lBAELVF14219@hsrnfs-101.mayo.edu>
Message-ID: <475DC2F9.4060003@biostat.ku.dk>

Terry Therneau wrote:
>   The problem will be fixed in the next resease of the survival code.  (That is, 
> it is fixed on our local version of R).  The summary.survfit result now includes 
> an element 'table' containing the matrix that is shown by print.survfit.   
>   
>   	Terry
>   
Hi Terry,

Speaking of coxph...

I noticed a silly inconsistency between summary.coxph and other modeling 
functions:

 > m <- summary( coxph( Surv(start, stop, event) ~ x, test2))
 > coef(m)
NULL
 > m$coef
coef exp(coef) se(coef) z p
x -0.02110521 0.979116 0.7951769 -0.02654153 0.98

 > coef(summary(lm.D9))
Estimate Std. Error t value Pr(>|t|)
(Intercept) 5.032 0.2202177 22.850117 9.547128e-15
groupTrt -0.371 0.3114349 -1.191260 2.490232e-01

 > summary(lm.D9)$coef
Estimate Std. Error t value Pr(>|t|)
(Intercept) 5.032 0.2202177 22.850117 9.547128e-15
groupTrt -0.371 0.3114349 -1.191260 2.490232e-01


the problem being that stats:::coef.default is looking for 
object$coefficients but summary.coxph uses rval$coef.

And while we're at it, if your "p" column was renamed "Pr(>|z|)" or so, 
then printCoefmat could have been used. Apart from significance stars 
(which you might well dislike), this also provides a nicer display of 
the p values themselves:

 > printCoefmat(summary(lm.D9)$coef)
Estimate Std. Error t value Pr(>|t|)
(Intercept) 5.03200 0.22022 22.8501 9.547e-15 ***
groupTrt -0.37100 0.31143 -1.1913 0.249
---
Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From deepayan.sarkar at gmail.com  Mon Dec 10 23:51:58 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 10 Dec 2007 14:51:58 -0800
Subject: [R] lattice: placing y-axis labels on right-hand side of panel
	when relation="sliced"
In-Reply-To: <fjjr4b$k0i$1@ger.gmane.org>
References: <fji1d6$igp$1@ger.gmane.org> <fjjr4b$k0i$1@ger.gmane.org>
Message-ID: <eb555e660712101451i66150c80t342c1303d26217fb@mail.gmail.com>

On 12/10/07, John G. Bullock <john.bullock at aya.yale.edu> wrote:
>
> >        I'm using lattice to create a multi-panel figure.  I would like
> > to draw each panel's y-axis ticks and labels on the right-hand
> > side of the panel.  Ordinarily, I would do this by specifying
> > scales=list(y=list(draw=T, alternating=2)).  But in this case, I am
> > using relation="sliced" to determine the y-axis limits.  So
> > "alternating" is ignored.  Is there any way -- short of using grid
> > functions -- to place the y-axis ticks and labels on the right-hand
> > sides of these panels?
>
> panel.axis() is what I had in mind.

You might consider the 'axis' argument (see ?axis.default). The main
catch is that you will need to allocate the space explicitly.

-Deepayan


From neil.stewart at warwick.ac.uk  Tue Dec 11 00:41:34 2007
From: neil.stewart at warwick.ac.uk (Neil Stewart)
Date: Mon, 10 Dec 2007 23:41:34 +0000 (GMT)
Subject: [R] Neat conditional assignment
Message-ID: <Pine.SOL.4.44.0712102329450.8786-100000@mimosa.csv.warwick.ac.uk>

I would like to make a new vector with values taken from two existing
vectors conditional upon a third. At present I am doing this with a for loop
but wonder if there is not a neater way, given some of the very neat things
R can do.

a<-rep(c("A","B"),50)
b<-rep(1,100)
c<-rep(2,100)

a is thus "A" "B" "A" "B" "A" "B"...
b is thus 1 1 1 1 1 1 ...
c is thus 2 2 2 2 2 2 ...

I want d[i] to be b[i] if a[i]=="A" and c[i] if a[i]=="B". I'm current using
a for loop:

d<-rep(0,100)     # initialise d
for(i in 1:length(a)) {if(a[i]=="A") d[i]<-b[i] else d[i]<-c[i]}

d is thus 1 2 1 2 1 2 1 2 1 ...

Is it possible to do something simpler, say along the lines of the c-style
?: conditional statement, or at least avoiding the for loop.

d <- a=="A"?b:c   # doesn't work, but you get the idea

Thanks in advance,
Neil


From jholtman at gmail.com  Tue Dec 11 00:46:06 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 10 Dec 2007 15:46:06 -0800
Subject: [R] Neat conditional assignment
In-Reply-To: <Pine.SOL.4.44.0712102329450.8786-100000@mimosa.csv.warwick.ac.uk>
References: <Pine.SOL.4.44.0712102329450.8786-100000@mimosa.csv.warwick.ac.uk>
Message-ID: <644e1f320712101546i40d3cc7fredd3390b92644306@mail.gmail.com>

?ifelse

> a<-rep(c("A","B"),50)
> b<-rep(1,100)
> c<-rep(2,100)
> d <- ifelse(a == "A", b, c)
> head(d)
[1] 1 2 1 2 1 2



On Dec 10, 2007 3:41 PM, Neil Stewart <neil.stewart at warwick.ac.uk> wrote:
> I would like to make a new vector with values taken from two existing
> vectors conditional upon a third. At present I am doing this with a for loop
> but wonder if there is not a neater way, given some of the very neat things
> R can do.
>
> a<-rep(c("A","B"),50)
> b<-rep(1,100)
> c<-rep(2,100)
>
> a is thus "A" "B" "A" "B" "A" "B"...
> b is thus 1 1 1 1 1 1 ...
> c is thus 2 2 2 2 2 2 ...
>
> I want d[i] to be b[i] if a[i]=="A" and c[i] if a[i]=="B". I'm current using
> a for loop:
>
> d<-rep(0,100)     # initialise d
> for(i in 1:length(a)) {if(a[i]=="A") d[i]<-b[i] else d[i]<-c[i]}
>
> d is thus 1 2 1 2 1 2 1 2 1 ...
>
> Is it possible to do something simpler, say along the lines of the c-style
> ?: conditional statement, or at least avoiding the for loop.
>
> d <- a=="A"?b:c   # doesn't work, but you get the idea
>
> Thanks in advance,
> Neil
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From Achim.Zeileis at wu-wien.ac.at  Tue Dec 11 00:54:03 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 11 Dec 2007 00:54:03 +0100 (CET)
Subject: [R] Neat conditional assignment
In-Reply-To: <Pine.SOL.4.44.0712102329450.8786-100000@mimosa.csv.warwick.ac.uk>
Message-ID: <Pine.LNX.4.44.0712110053320.24326-100000@disco.wu-wien.ac.at>

On Mon, 10 Dec 2007, Neil Stewart wrote:

> I would like to make a new vector with values taken from two existing
> vectors conditional upon a third. At present I am doing this with a for loop
> but wonder if there is not a neater way, given some of the very neat things
> R can do.
>
> a<-rep(c("A","B"),50)
> b<-rep(1,100)
> c<-rep(2,100)
>
> a is thus "A" "B" "A" "B" "A" "B"...
> b is thus 1 1 1 1 1 1 ...
> c is thus 2 2 2 2 2 2 ...
>
> I want d[i] to be b[i] if a[i]=="A" and c[i] if a[i]=="B". I'm current using
> a for loop:
>
> d<-rep(0,100)     # initialise d
> for(i in 1:length(a)) {if(a[i]=="A") d[i]<-b[i] else d[i]<-c[i]}
>
> d is thus 1 2 1 2 1 2 1 2 1 ...
>
> Is it possible to do something simpler, say along the lines of the c-style
> ?: conditional statement, or at least avoiding the for loop.
>
> d <- a=="A"?b:c   # doesn't work, but you get the idea

d <- ifelse(a == "A", b, c)

is what you want!
Z

> Thanks in advance,
> Neil
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From markleeds at verizon.net  Tue Dec 11 00:51:31 2007
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 10 Dec 2007 17:51:31 -0600 (CST)
Subject: [R] Neat conditional assignment
Message-ID: <19939976.17505671197330691759.JavaMail.root@vms227.mailsrvcs.net>

>From: Neil Stewart <neil.stewart at warwick.ac.uk>
>Date: 2007/12/10 Mon PM 05:41:34 CST
>To: r-help at stat.math.ethz.ch
>Subject: [R] Neat conditional assignment

d <- ifelse(a == "A",b,c)

>I would like to make a new vector with values taken from two existing
>vectors conditional upon a third. At present I am doing this with a for loop
>but wonder if there is not a neater way, given some of the very neat things
>R can do.
>
>a<-rep(c("A","B"),50)
>b<-rep(1,100)
>c<-rep(2,100)
>
>a is thus "A" "B" "A" "B" "A" "B"...
>b is thus 1 1 1 1 1 1 ...
>c is thus 2 2 2 2 2 2 ...
>
>I want d[i] to be b[i] if a[i]=="A" and c[i] if a[i]=="B". I'm current using
>a for loop:
>
>d<-rep(0,100)     # initialise d
>for(i in 1:length(a)) {if(a[i]=="A") d[i]<-b[i] else d[i]<-c[i]}
>
>d is thus 1 2 1 2 1 2 1 2 1 ...
>
>Is it possible to do something simpler, say along the lines of the c-style
>?: conditional statement, or at least avoiding the for loop.
>
>d <- a=="A"?b:c   # doesn't work, but you get the idea
>
>Thanks in advance,
>Neil
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Tue Dec 11 00:52:58 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Dec 2007 18:52:58 -0500
Subject: [R] Neat conditional assignment
In-Reply-To: <Pine.SOL.4.44.0712102329450.8786-100000@mimosa.csv.warwick.ac.uk>
References: <Pine.SOL.4.44.0712102329450.8786-100000@mimosa.csv.warwick.ac.uk>
Message-ID: <971536df0712101552w58236f61r40627b4106fa878d@mail.gmail.com>

With this specific example this will work:

   as.numeric(factor(a))

If that is not the real case but its something else you could try any
of these which probably generalize better:

   (a == "A") + 2 * (a == "B")
   1 + (a == "B")
   ifelse(a == "A", 1, 2)

On Dec 10, 2007 6:41 PM, Neil Stewart <neil.stewart at warwick.ac.uk> wrote:
> I would like to make a new vector with values taken from two existing
> vectors conditional upon a third. At present I am doing this with a for loop
> but wonder if there is not a neater way, given some of the very neat things
> R can do.
>
> a<-rep(c("A","B"),50)
> b<-rep(1,100)
> c<-rep(2,100)
>
> a is thus "A" "B" "A" "B" "A" "B"...
> b is thus 1 1 1 1 1 1 ...
> c is thus 2 2 2 2 2 2 ...
>
> I want d[i] to be b[i] if a[i]=="A" and c[i] if a[i]=="B". I'm current using
> a for loop:
>
> d<-rep(0,100)     # initialise d
> for(i in 1:length(a)) {if(a[i]=="A") d[i]<-b[i] else d[i]<-c[i]}
>
> d is thus 1 2 1 2 1 2 1 2 1 ...
>
> Is it possible to do something simpler, say along the lines of the c-style
> ?: conditional statement, or at least avoiding the for loop.
>
> d <- a=="A"?b:c   # doesn't work, but you get the idea
>
> Thanks in advance,
> Neil
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mwkimpel at gmail.com  Tue Dec 11 00:53:57 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Mon, 10 Dec 2007 18:53:57 -0500
Subject: [R] 00LOCK error with site-library
In-Reply-To: <475DC2B9.1060609@bank-banque-canada.ca>
References: <475DB2F8.1000108@gmail.com>
	<475DC2B9.1060609@bank-banque-canada.ca>
Message-ID: <475DD195.9030200@gmail.com>

Paul,

Yes, I have manually removed it multiple times and it continues to 
reappear. I'll try to just upgrade a few packages at a time and see if 
there is one in particular that is causing the problem. Failing that, 
I'll run this past my system admin.

Thanks for your help,
Mark

Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com

******************************************************************


Paul Gilbert wrote:
> 
> 
> Mark W Kimpel wrote:
>> I have identical R.profiles and R_HOME directories set up on both my 
>> local machine and a remote linux cluster. To keep my libraries and R 
>> install separate, I use a site-library on both machines.
>>
>> The first line of my .Rprofile is:
>> '.libPaths(new= "~/R_HOME/site-library") #tell R where site-library is'
>>
>> Until R-2.6.0 this was working fine on both machines, but since I have 
>> been unable to upgrade packages on the remote machine. Every attempt 
>> at upgrade results in a directory '00LOCK' being place in the 
>> site-directory and then the upgrade fails.
> 
> This directory gets placed there and then removed when the upgrade is 
> finished, to prevent two processes from trying to simultaneously update 
> the directory. If you terminated an upgrade in an unexpected way, say by 
> dropping your connection, then the file will be left and you need to 
> manually remove it before you try to upgrade again.
> 
> If you have removed it, and it keeps re-appearing, it is a sign of 
> something else going wrong (usually outside of R).
> 
> Paul
>>
>> This has happened with R-2.6.0, 2.6.1, and now with R-devel (2.7.0).
>>
>> If I have this setup incorrectly I am puzzled as to why it works with 
>> my local machine and why it worked with versions of R prior to 2.6.0.
>>
>> Ideas?
>> Thanks,
>> Mark
>>
> ==================================================================================== 
> 
> 
> La version fran?aise suit le texte anglais.
> 
> ------------------------------------------------------------------------------------ 
> 
> 
> This email may contain privileged and/or confidential information, and 
> the Bank of
> Canada does not waive any related rights. Any distribution, use, or 
> copying of this
> email or the information it contains by other than the intended 
> recipient is
> unauthorized. If you received this email in error please delete it 
> immediately from
> your system and notify the sender promptly by email that you have done so.
> ------------------------------------------------------------------------------------ 
> 
> 
> Le pr?sent courriel peut contenir de l'information privil?gi?e ou 
> confidentielle.
> La Banque du Canada ne renonce pas aux droits qui s'y rapportent. Toute 
> diffusion,
> utilisation ou copie de ce courriel ou des renseignements qu'il contient 
> par une
> personne autre que le ou les destinataires d?sign?s est interdite. Si 
> vous recevez
> ce courriel par erreur, veuillez le supprimer imm?diatement et envoyer 
> sans d?lai ?
> l'exp?diteur un message ?lectronique pour l'aviser que vous avez ?limin? 
> de votre
> ordinateur toute copie du courriel re?u.
>


From Ted.Harding at manchester.ac.uk  Tue Dec 11 00:56:01 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Mon, 10 Dec 2007 23:56:01 -0000 (GMT)
Subject: [R] Neat conditional assignment
In-Reply-To: <Pine.SOL.4.44.0712102329450.8786-100000@mimosa.csv.warwick.ac.uk>
Message-ID: <XFMail.071210235601.Ted.Harding@manchester.ac.uk>

On 10-Dec-07 23:41:34, Neil Stewart wrote:
> I would like to make a new vector with values taken from two
> existing vectors conditional upon a third. At present I am
> doing this with a for loop but wonder if there is not a neater
> way, given some of the very neat things R can do.
> 
> a<-rep(c("A","B"),50)
> b<-rep(1,100)
> c<-rep(2,100)
> 
> a is thus "A" "B" "A" "B" "A" "B"...
> b is thus 1 1 1 1 1 1 ...
> c is thus 2 2 2 2 2 2 ...
> 
> I want d[i] to be b[i] if a[i]=="A" and c[i] if a[i]=="B".
> I'm current using a for loop:
> 
> d<-rep(0,100)     # initialise d
> for(i in 1:length(a)) {if(a[i]=="A") d[i]<-b[i] else d[i]<-c[i]}
> 
> d is thus 1 2 1 2 1 2 1 2 1 ...
> 
> Is it possible to do something simpler, say along the lines
> of the c-style ?: conditional statement, or at least avoiding
> the for loop.
> 
> d <- a=="A"?b:c   # doesn't work, but you get the idea
> 
> Thanks in advance,
> Neil

You could use one of at least two simple approaches.

When b and c are numeric, then you could use

 d <- b*(a=="A") + c*(a=="B")

since the numeric operation coerces the logical 'a=="A"'
into 1 or 0 according as it is TRUE or FALSE, and vice versa.

If b and c are not numeric (and indeed generally, but the
above is neater for numeric variables), then:

 d <- b  ;  d[a=="B"] <- c[a=="B"]

which simply over-writes the b-values in d where a=="B".

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-Dec-07                                       Time: 23:55:45
------------------------------ XFMail ------------------------------


From ecjbosu at aol.com  Tue Dec 11 01:45:34 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Mon, 10 Dec 2007 18:45:34 -0600
Subject: [R] holidayNYSE missing some
In-Reply-To: <638044.86846.qm@web50706.mail.re2.yahoo.com>
References: <638044.86846.qm@web50706.mail.re2.yahoo.com>
Message-ID: <475DDDAE.10706@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071210/a01f1f83/attachment.pl 

From Brian.O'Gorman at noaa.gov  Tue Dec 11 01:56:24 2007
From: Brian.O'Gorman at noaa.gov (Brian O'Gorman)
Date: Mon, 10 Dec 2007 15:56:24 -0900
Subject: [R] error trying to load biOps under 2.6.1 running XP
Message-ID: <475DE038.9040301@noaa.gov>

I'm running R 2.6.1 on a WXP machine. When I do the following I get an 
error message.

 > library(biOps)
Error in dyn.load(file, ...) :
  unable to load shared library 
'C:/PROGRA~1/R/R-26~1.1/library/biOps/libs/biOps.dll':
  LoadLibrary failure:  The specified module could not be found.
Error: package/namespace load failed for 'biOps'

Please help, or comments about what I'm doing wrong? (It seems to me the 
correct directory or folder isn't being located.)
Thanks in advance.

-- 
"The struggle for today, is not altogether for today - it is for a vast future also."
	-- Abraham Linclon, December 3, 1861


From ggrothendieck at gmail.com  Tue Dec 11 02:30:03 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Dec 2007 20:30:03 -0500
Subject: [R] Neat conditional assignment
In-Reply-To: <971536df0712101552w58236f61r40627b4106fa878d@mail.gmail.com>
References: <Pine.SOL.4.44.0712102329450.8786-100000@mimosa.csv.warwick.ac.uk>
	<971536df0712101552w58236f61r40627b4106fa878d@mail.gmail.com>
Message-ID: <971536df0712101730t44a0957fw2bcb44697d53694c@mail.gmail.com>

On Dec 10, 2007 6:52 PM, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> With this specific example this will work:
>
>   as.numeric(factor(a))
>
> If that is not the real case but its something else you could try any
> of these which probably generalize better:
>
>   (a == "A") + 2 * (a == "B")
>   1 + (a == "B")
>   ifelse(a == "A", 1, 2)


Just in case this wasn't clear:

> a<-rep(c("A","B"),50)
> b<-rep(1,100)
> c<-rep(2,100)
>
> b * (a == "A") + c * (a == "B")
  [1] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1
 [38] 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2
 [75] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2
> b + (c-b) * (a == "B")
  [1] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1
 [38] 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2
 [75] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2
> ifelse(a == "A", b, c)
  [1] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1
 [38] 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2
 [75] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2
>
>

>
>
> On Dec 10, 2007 6:41 PM, Neil Stewart <neil.stewart at warwick.ac.uk> wrote:
> > I would like to make a new vector with values taken from two existing
> > vectors conditional upon a third. At present I am doing this with a for loop
> > but wonder if there is not a neater way, given some of the very neat things
> > R can do.
> >
> > a<-rep(c("A","B"),50)
> > b<-rep(1,100)
> > c<-rep(2,100)
> >
> > a is thus "A" "B" "A" "B" "A" "B"...
> > b is thus 1 1 1 1 1 1 ...
> > c is thus 2 2 2 2 2 2 ...
> >
> > I want d[i] to be b[i] if a[i]=="A" and c[i] if a[i]=="B". I'm current using
> > a for loop:
> >
> > d<-rep(0,100)     # initialise d
> > for(i in 1:length(a)) {if(a[i]=="A") d[i]<-b[i] else d[i]<-c[i]}
> >
> > d is thus 1 2 1 2 1 2 1 2 1 ...
> >
> > Is it possible to do something simpler, say along the lines of the c-style
> > ?: conditional statement, or at least avoiding the for loop.
> >
> > d <- a=="A"?b:c   # doesn't work, but you get the idea
> >
> > Thanks in advance,
> > Neil
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From bolker at ufl.edu  Tue Dec 11 03:57:05 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 10 Dec 2007 18:57:05 -0800 (PST)
Subject: [R] 3-D plot of likelihood
In-Reply-To: <475D7038.70303@stats.uwo.ca>
References: <D33A5B5753579B44AD89F58D40147582043298BB@MSMAIL2.uottawa.o.univ>
	<475D7038.70303@stats.uwo.ca>
Message-ID: <14266744.post@talk.nabble.com>




Duncan Murdoch-2 wrote:
> 
> On 12/10/2007 11:26 AM, David Bickel wrote:
>> Could anyone recommend a package for visualizing a likelihood function
>> of two scalar parameters? I would prefer a three-dimensional plot
>> similar to the kind Mathematica is known for, perhaps generated by a
>> package not specific to likelihood.
> 
> The basic strategy is to calculate a matrix of values of the likelihood, 
> corresponding to all combinations of values of two vectors.  Then 
> persp()  (in graphics) or persp3d() (in rgl) can display the surface.
> 
> I don't know of anything that corresponds to curve() to do the matrix 
> calculations automatically, but it may well exist.
> 
> Duncan Murdoch
> 
> 

See ?curve3d in the emdbook package ...

  Ben Bolker

-- 
View this message in context: http://www.nabble.com/3-D-plot-of-likelihood-tp14256532p14266744.html
Sent from the R help mailing list archive at Nabble.com.


From leffgh at 163.com  Tue Dec 11 04:42:36 2007
From: leffgh at 163.com (Bin Yue)
Date: Mon, 10 Dec 2007 19:42:36 -0800 (PST)
Subject: [R]  the observed "log odds" in logistic regression
Message-ID: <14267125.post@talk.nabble.com>


 
Dear list:
     After reading the following two links:
http://luna.cas.usf.edu/~mbrannic/files/regression/Logistic.html
http://www.tufts.edu/~gdallal/logistic.htm
     I've known the mathematical basis for logistic regression.However I am
still not so sure about the "logit "
     For a categorical independent variable, It is  easy to understand the
procedures  how "log odds" are calculated. As I know, First the observations
are grouped according to the IV and DV, generating a contingency table.The
columns are the levels of IV, and the rows are the levels of DV(0, or 1).For
each column,we get the proprotions  for DV=0 and DV=1 at given IV. Using the
proportions  the log odds can be computed.Is that right?
   My problem  is this : in my data set , the IVs are continuous variables,
do I still have to generate such a table and compute the log odds for each
level of IV according to which the log odds are calculated?  
   In R , fitted(fit) gives the fitted probability for DV to be 1.  Dose the
observed probability exist ? If it does exist , how can I extract it ? If
the IV is cartegorical , the DV can readily changed to be a tow-culumned
matrix, thus log(the observed probabily/(1-the observed probability) might
be the "log odds". I wonder what if the IV is continuous ?
     And about the residuals. It seems that  the residual is not the actual
DV minus the fitted probability. For in my model extreme residuals lie well
beyond (0,1).  I wonder how   the residual is computed.
      Would you please help me ?  Thank all very much again.
    Regards,
    Bin Yue


-----
Best regards,
Bin Yue

*************
student for a Master program in South Botanical Garden , CAS

-- 
View this message in context: http://www.nabble.com/the-observed-%22log-odds%22-in-logistic-regression-tp14267125p14267125.html
Sent from the R help mailing list archive at Nabble.com.


From alecwang80 at gmail.com  Tue Dec 11 06:47:42 2007
From: alecwang80 at gmail.com (Alexwang)
Date: Mon, 10 Dec 2007 21:47:42 -0800 (PST)
Subject: [R] How to read in expressions as function parameters?
In-Reply-To: <da79af330712100734x679e1a47ic95aa9b8103ee3c7@mail.gmail.com>
References: <855c8c750712091445r4b5653c1g2bd1ccab61c724b9@mail.gmail.com>
	<da79af330712100734x679e1a47ic95aa9b8103ee3c7@mail.gmail.com>
Message-ID: <14268331.post@talk.nabble.com>



Thanks a lot!




Henrique Dallazuanna wrote:
> 
> Try this:
> 
> form<- scan(file = "", what = character(0), n=1,strip.white =
>  TRUE,quiet=TRUE )
> form2<-parse(text=form)
> foo <- function(x){}
> body(foo) <- form2
> curve(foo,10,100)
> 
> On 09/12/2007, Alex Wang <alecwang80 at gmail.com> wrote:
>> Hi:
>>
>>    There, I've got a question about how to read in expressions as
>> function
>> parameters and it really bothered me.
>>
>>  I'm going to use curve() function to plot curves, and I'd like to write
>> a
>> menu function to let use input math expressions.
>>
>>   say, if I'd like curve(3*x*x-4/x, 10, 100), I use scan() to  read in
>> the
>> math expression
>>
>> form<- scan(file = "", what = character(0), n=1,strip.white =
>> TRUE,quiet=TRUE )
>> then get form as a string. "3*x*x-4/x"
>> Use parse() to convert it into expression
>> form2<-parse(text=form)
>> then form2=expression(3*x*x-4/x).
>>
>> However, curve(form2,10,100) doesn't work at all.
>>
>> Is there anyway to get this done?
>>
>> Thanks a lot!
>>
>> Alex
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> -- 
> Henrique Dallazuanna
> Curitiba-Paran?-Brasil
> 25? 25' 40" S 49? 16' 22" O
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/How-to-read-in-expressions-as-function-parameters--tp14244375p14268331.html
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Tue Dec 11 08:43:36 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Dec 2007 07:43:36 +0000 (GMT)
Subject: [R] Building R on Sun Solaris 10 (SPARC) using Sun Studio 12
In-Reply-To: <475D64DE.90302@msu.edu>
References: <475D64DE.90302@msu.edu>
Message-ID: <Pine.LNX.4.64.0712110738050.9400@gannet.stats.ox.ac.uk>

On Mon, 10 Dec 2007, James T Brown wrote:

> R Help List:
>
> Just curious if anyone has successfully built R on a SPARC
> platform running Sun Solaris 10 using the latest Sun Studio
> 12 set of compilers.   If so, I would be interested in the
> compile flags that you used.

Yes, the flags given in the R-admin manual.  (We don't currently have a 
Solaris 10 Sparc box running, but this was with the current Sun Studio 12 
at the time, just after it came out.)


> I have tried several different builds of version 2.5.1, 2.6.0,
> and 2.6.1 using various different compile flags and I am able
> to compile and check, but for whatever reason, the "foreign"
> package crashes whenever it is loaded.   I need the "foreign"
> package in order to install the "maptools" package.   This is
> the error that I am getting when attempting to load "foreign":
>
>>> library(foreign)
>>
>>  *** caught segfault ***
>> address fbb1dc40, cause 'invalid permissions'
>>
>> Traceback:
>>  1: .C("spss_init", PACKAGE = "foreign")
>>  2: fun(...)
>>  3: doTryCatch(return(expr), name, parentenv, handler)
>>  4: tryCatchOne(expr, names, parentenv, handlers[[1]])
>>  5: tryCatchList(expr, classes, parentenv, handlers)
>>  6: tryCatch(expr, error = function(e) {    call <-
>> conditionCall(e)    if (!is.null(call)) {        if
>> (identical(call[[1]], quote(doTryCatch)))             call <-
>> sys.call(-4)        dcall <- deparse(call)[1]        prefix <-
>> paste("Error in", dcall, ": ")        LONGCALL <- 30        if
>> (nchar(dcall) > LONGCALL)             prefix <- paste(prefix, "\n\t",
>> sep = "")    }    else prefix <- "Error : "    msg <- paste(prefix,
>> conditionMessage(e), "\n", sep = "")
>> .Internal(seterrmessage(msg[1]))    if (!silent &&
>> identical(getOption("show.error.messages"),         TRUE)) {
>> cat(msg, file = stderr())        .Internal(printDeferredWarnings())
>> }    invisible(structure(msg, class = "try-error"))})
>>  7: try({    fun(...)    NULL})
>>  8: runHook(".onLoad", package, env, package.lib, package)
>>  9: loadNamespace(package, c(which.lib.loc, lib.loc), keep.source =
>> keep.source)
>> 10: doTryCatch(return(expr), name, parentenv, handler)
>> 11: tryCatchOne(expr, names, parentenv, handlers[[1]])
>> 12: tryCatchList(expr, classes, parentenv, handlers)
>> 13: tryCatch(expr, error = function(e) {    call <-
>> conditionCall(e)    if (!is.null(call)) {        if
>> (identical(call[[1]], quote(doTryCatch)))             call <-
>> sys.call(-4)        dcall <- deparse(call)[1]        prefix <-
>> paste("Error in", dcall, ": ")        LONGCALL <- 30        if
>> (nchar(dcall) > LONGCALL)             prefix <- paste(prefix, "\n\t",
>> sep = "")    }    else prefix <- "Error : "    msg <- paste(prefix,
>> conditionMessage(e), "\n", sep = "")
>> .Internal(seterrmessage(msg[1]))    if (!silent &&
>> identical(getOption("show.error.messages"),         TRUE)) {
>> cat(msg, file = stderr())        .Internal(printDeferredWarnings())
>> }    invisible(structure(msg, class = "try-error"))})
>> 14: try({    ns <- loadNamespace(package, c(which.lib.loc, lib.loc),
>> keep.source = keep.source)    dataPath <- file.path(which.lib.loc,
>> package, "data")    env <- attachNamespace(ns, pos = pos, dataPath =
>> dataPath)})
>> 15: library(foreign)
>>
>> Possible actions:
>> 1: abort (with core dump, if enabled)
>> 2: normal R exit
>> 3: exit R without saving workspace
>> 4: exit R saving workspace
>
>
>
> The latest build was compiled with the following flags:
>
>> ./configure --prefix=/usr/local/R-2.5.1
>>             --with-blas
>>             --with-lapack
>>             --with-tcl-config=/usr/local/lib/tclConfig.sh
>>             --with-tk-config=/usr/local/lib/tkConfig.sh
>>             --without-iconv
>>             R_PAPERSIZE=letter
>>             SHLIB_CXXLDFLAGS="-G /opt/SUNWspro/lib/libCrun.so"
>>             CC=/opt/SUNWspro/bin/cc CXX=/opt/SUNWspro/bin/CC
>>             F77=/opt/SUNWspro/bin/f77 F90=/opt/SUNWspro/bin/f95
>>             FC=/opt/SUNWspro/bin/f95 CFLAGS="-mt -ftrap=%none
>> -xarch=sparcvis -fPIC -xmemalign=4s"
>>             CXXFLAGS="-mt -ftrap=%none -xarch=sparcvis -xmemalign=4s"
>>             FFLAGS="-mt -ftrap=%none -shared -xarch=sparcvis"
>>             FCFLAGS="-mt -ftrap=%none -shared -xarch=sparcvis"
>>             LDFLAGS="-V -fPIC -L/usr/local/lib -L/opt/SUNWspro/lib
>> -L/usr/sfw/lib -L/usr/lib
>>
>> -R/usr/local/lib:/opt/SUNWspro/lib:/usr/sfw/lib:/usr/lib"
>
>
> I have been messing with the "xmemalign" flag, but doesn't seem to have much
> of an impact.   I am curious if there may be a simple compile flag in Sun
> Studio 12 that can be set to fix this problem.
>
>
> At any rate, if anyone has been able to successfully build on Sun Solaris 10
> (SPARC) using Sun Studio 12 and the "foreign" package loads without
> crashing,
> I would be most appreciative if you could let me take a look at your
> ".configure"
> options.
>
>
> NOTE: So far, the only package that I am having trouble with is
> "foreign".   Everything
> else seems to build and check ok.  In fact, when "foreign" is built,
> there are no errors
> reported during the compile.   Also, I have tried
> "install.packages("foreign") from
> within R to upgrade to the latest version of "foreign".  It compiles and
> installs, but
> once again, it crashes when R attempts to use it producing the
> "segfault" error.
>
>
> Any help would be most welcome.
>
>
> Thanks.
>
>
>
> Jim
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dieter.menne at menne-biomed.de  Tue Dec 11 08:43:47 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 11 Dec 2007 07:43:47 +0000 (UTC)
Subject: [R] the observed "log odds" in logistic regression
References: <14267125.post@talk.nabble.com>
Message-ID: <loom.20071211T073203-589@post.gmane.org>

Bin Yue <leffgh <at> 163.com> writes:

>      After reading the following two links:
> http://luna.cas.usf.edu/~mbrannic/files/regression/Logistic.html
> http://www.tufts.edu/~gdallal/logistic.htm
>      I've known the mathematical basis for logistic regression.However I am
> still not so sure about the "logit "
>      For a categorical independent variable, It is  easy to understand the
> procedures  how "log odds" are calculated. As I know, First the observations
> are grouped according to the IV and DV, generating a contingency table.
..
>    My problem  is this : in my data set , the IVs are continuous variables,
> do I still have to generate such a table and compute the log odds for each
> level of IV according to which the log odds are calculated?  

Let's assume you are going to use glm in package stats. glm can be fed with 
data in three ways; in your case, you should use the "one-row/one 0-1 event"
format, that is the "long" style. You do not have to compute any logit, 
glm will do that for your.

The example coming closest to your's is the birthwt example in 
MASS/scripts/ch07.R  and chapter 7 in Venables/Ripley MASS. Try to generate 
a small, self-running example with a data set similar to your's, and you have 
a good chance to get a more detailed answer.

Dieter


From tura at centroin.com.br  Tue Dec 11 08:49:46 2007
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Tue, 11 Dec 2007 05:49:46 -0200
Subject: [R] the observed "log odds" in logistic regression
In-Reply-To: <14267125.post@talk.nabble.com>
References: <14267125.post@talk.nabble.com>
Message-ID: <1197359386.28860.23.camel@R3-Thux>


On Mon, 2007-12-10 at 19:42 -0800, Bin Yue wrote:
(...)
>    My problem  is this : in my data set , the IVs are continuous variables,
> do I still have to generate such a table and compute the log odds for each
> level of IV according to which the log odds are calculated?  

If IV is a continuous variable isn't possible you create a contingency
table because don't exist levels.

Similar is not possible calculate de log odds of P(IV=x) but is possible
calculate log odds of P(IV<x) or log odds of P(IV=x+delta) with delta
tend to zero. 

In this case is common create a cut-off for IV and fit log odds of
P(IV>x)

>    In R , fitted(fit) gives the fitted probability for DV to be 1.  Dose the
> observed probability exist ? If it does exist , how can I extract it ? If
> the IV is cartegorical , the DV can readily changed to be a tow-culumned
> matrix, thus log(the observed probabily/(1-the observed probability) might
> be the "log odds". I wonder what if the IV is continuous ?
>      And about the residuals. It seems that  the residual is not the actual
> DV minus the fitted probability. For in my model extreme residuals lie well
> beyond (0,1).  I wonder how   the residual is computed.
>       Would you please help me ?  Thank all very much again.

So to help you send a small part of your data and a reproductive example
to us because is more easy understand your question this way
-- 
Bernardo Rangel Tura, M.D,MPH,Ph.D
National Institute of Cardiology
Brazil


From leffgh at 163.com  Tue Dec 11 09:05:33 2007
From: leffgh at 163.com (Bin Yue)
Date: Tue, 11 Dec 2007 00:05:33 -0800 (PST)
Subject: [R] the observed "log odds" in logistic regression
In-Reply-To: <loom.20071211T073203-589@post.gmane.org>
References: <14267125.post@talk.nabble.com>
	<loom.20071211T073203-589@post.gmane.org>
Message-ID: <14269459.post@talk.nabble.com>


Dieter Menne:
Thank you for your reply!

I know that I don't have to do any logit , but I want to understand how R
fit the glm models.
I will read the examples your suggested .
Best regards,
Bin Yue


Dieter Menne wrote:
> 
> Bin Yue <leffgh <at> 163.com> writes:
> 
>>      After reading the following two links:
>> http://luna.cas.usf.edu/~mbrannic/files/regression/Logistic.html
>> http://www.tufts.edu/~gdallal/logistic.htm
>>      I've known the mathematical basis for logistic regression.However I
>> am
>> still not so sure about the "logit "
>>      For a categorical independent variable, It is  easy to understand
>> the
>> procedures  how "log odds" are calculated. As I know, First the
>> observations
>> are grouped according to the IV and DV, generating a contingency table.
> ..
>>    My problem  is this : in my data set , the IVs are continuous
>> variables,
>> do I still have to generate such a table and compute the log odds for
>> each
>> level of IV according to which the log odds are calculated?  
> 
> Let's assume you are going to use glm in package stats. glm can be fed
> with 
> data in three ways; in your case, you should use the "one-row/one 0-1
> event"
> format, that is the "long" style. You do not have to compute any logit, 
> glm will do that for your.
> 
> The example coming closest to your's is the birthwt example in 
> MASS/scripts/ch07.R  and chapter 7 in Venables/Ripley MASS. Try to
> generate 
> a small, self-running example with a data set similar to your's, and you
> have 
> a good chance to get a more detailed answer.
> 
> Dieter
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 


-----
Best regards,
Bin Yue

*************
student for a Master program in South Botanical Garden , CAS

-- 
View this message in context: http://www.nabble.com/the-observed-%22log-odds%22-in-logistic-regression-tp14267125p14269459.html
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Tue Dec 11 09:11:45 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Dec 2007 08:11:45 +0000 (GMT)
Subject: [R] error trying to load biOps under 2.6.1 running XP
In-Reply-To: <475DE038.9040301@noaa.gov>
References: <475DE038.9040301@noaa.gov>
Message-ID: <Pine.LNX.4.64.0712110752020.9400@gannet.stats.ox.ac.uk>

On Mon, 10 Dec 2007, Brian O'Gorman wrote:

> I'm running R 2.6.1 on a WXP machine. When I do the following I get an error 
> message.
>
>> library(biOps)
> Error in dyn.load(file, ...) :
> unable to load shared library 
> 'C:/PROGRA~1/R/R-26~1.1/library/biOps/libs/biOps.dll':
> LoadLibrary failure:  The specified module could not be found.
> Error: package/namespace load failed for 'biOps'
>
> Please help, or comments about what I'm doing wrong? (It seems to me the 
> correct directory or folder isn't being located.)

That message should also have given a message box with more information, 
saying what the problem was: it does not usually mean the 'shared library' 
named.  It does for me:

'The application failed to start because libfftw3-3.dll was not found.'

Other DLLs are also missing: jpeg62.dll and libtiff3.dll (use pedump to 
find this out).  So you need to use Google to find those DLLs and put them 
on your PATH.

You also need to read 
http://cran.r-project.org/bin/windows/contrib/2.6/ReadMe
and suggest to Uwe Ligges what he adds a note about this (it applies to 
SoPhy for libtiff3.dll as well).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maechler at stat.math.ethz.ch  Tue Dec 11 09:15:17 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 11 Dec 2007 09:15:17 +0100
Subject: [R] Decent R code does NOT end lines in ';' !
In-Reply-To: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>
References: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>
Message-ID: <18270.18197.90476.422873@stat.math.ethz.ch>

>>>>> "TK" == Talbot Katz <topkatz at msn.com>
>>>>>     on Mon, 10 Dec 2007 14:16:52 -0500 writes:

    TK> Hi.
 
    TK> I have a procedure that reads a directory, loops through
    TK> a set of particular .RData files, loading each one, and
    TK> feeding its object(s) into a function, as follows:

    TK> cvListFiles<-list.files(fnDir);
    TK> for(i in grep(paste("^",pfnStub,".*\\.RData$",sep=""),cvListFiles)){
    TK> load(paste(fnDir,cvListFiles[i],sep="/"));
    TK> myFunction(rliObject);
    TK> rm(rliObject);
    TK> };
 
	[.............]

I don't know where you got the idea to end R statements with ";".
I find it atrocious. It just does not belong to the S language
and its R implementation.

Please don't do it
 -- it hurts the eyes of most experienced R users
 -- it's extra clutter
 -- internally each extra ";" is an extra empty statement

OTOH, *please* do
 -- indent R code and 
 -- use spaces to increase readability 

This would make your above code into something like 

 cvListFiles <- list.files(fnDir)
 for(i in grep(paste("^", pfnStub,".*\\.RData$", sep = ""), cvListFiles)) {
     load(paste(fnDir, cvListFiles[i], sep = "/"))
     myFunction(rliObject)
     rm(rliObject)
 }

Something many readers on this list will much prefer to your
original, and hence start considering to look at ..

Martin Maechler,
(S user since ca. 1987; member of R-core)


From f.harrell at vanderbilt.edu  Tue Dec 11 09:28:40 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 11 Dec 2007 02:28:40 -0600
Subject: [R] the observed "log odds" in logistic regression
In-Reply-To: <1197359386.28860.23.camel@R3-Thux>
References: <14267125.post@talk.nabble.com> <1197359386.28860.23.camel@R3-Thux>
Message-ID: <475E4A38.9010504@vanderbilt.edu>

Bernardo Rangel Tura wrote:
> On Mon, 2007-12-10 at 19:42 -0800, Bin Yue wrote:
> (...)
>>    My problem  is this : in my data set , the IVs are continuous variables,
>> do I still have to generate such a table and compute the log odds for each
>> level of IV according to which the log odds are calculated?  
> 
> If IV is a continuous variable isn't possible you create a contingency
> table because don't exist levels.
> 
> Similar is not possible calculate de log odds of P(IV=x) but is possible
> calculate log odds of P(IV<x) or log odds of P(IV=x+delta) with delta
> tend to zero. 

Incorrect.  You can easily create the log odds for IV=x1 vs. IV=x2

> 
> In this case is common create a cut-off for IV and fit log odds of
> P(IV>x)

Not needed, and if you do, the resulting odds ratios are actually no 
longer scientific quantities of interest, i.e., they have no exact 
interpretation outside your sample of x's.  They are averaged over an 
unspecified distribution of x's.

Frank

> 
>>    In R , fitted(fit) gives the fitted probability for DV to be 1.  Dose the
>> observed probability exist ? If it does exist , how can I extract it ? If
>> the IV is cartegorical , the DV can readily changed to be a tow-culumned
>> matrix, thus log(the observed probabily/(1-the observed probability) might
>> be the "log odds". I wonder what if the IV is continuous ?
>>      And about the residuals. It seems that  the residual is not the actual
>> DV minus the fitted probability. For in my model extreme residuals lie well
>> beyond (0,1).  I wonder how   the residual is computed.
>>       Would you please help me ?  Thank all very much again.
> 
> So to help you send a small part of your data and a reproductive example
> to us because is more easy understand your question this way


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From code.monkey91 at yahoo.com  Tue Dec 11 00:07:13 2007
From: code.monkey91 at yahoo.com (R. W.)
Date: Mon, 10 Dec 2007 15:07:13 -0800 (PST)
Subject: [R] Simulating Case Control Data
Message-ID: <784724.18916.qm@web45807.mail.sp1.yahoo.com>

Dear R-Help-List,

I was wondering if anyone had experience simulating
case-control data in R?  I've been looking through
literature, and found that the main examples make
heavy parametric assumptions on the distributions of
the exposure (E), covariates (Z), and disease status
(D).  I would appreciate any guidance toward
resources/examples/literature that simulate
case-control data with fewer assumptions about the
underlying distributions of E, Z and D.

Thank you,
-R


      ____________________________________________________________________________________
Never miss a thing.  Make Yahoo your home page.


From u.amato at iac.cnr.it  Tue Dec 11 10:39:47 2007
From: u.amato at iac.cnr.it (Umberto Amato)
Date: Tue, 11 Dec 2007 10:39:47 +0100
Subject: [R] Error with rgl loading in BATCH mode
Message-ID: <1636121D4BAD492897522639D61B7C75@na.iac.cnr.it>

Dear all,
I'm trying to run a script that requires KernSmooth in BATCH mode but I get 
an error while loading rgl library that is needed by KernSmooth. Actually I 
have to run several batch files through a queue of a cluster, so I wouldn't 
need graphics at all.
I installed the latest releases of R (2.6.1) and rgl on my CentOS 4.2 Linux 
(clone of Red Hat EL) endowed with Rocks 4.2.1 suite for clusters.
If I run the R script from a ssh session on my PC equipped with X Server (I 
use Xming), everithing goes smoothly, rgl package is loaded correctly and 
the script runs smoothly. However when run in batch mode (that is through a 
submission to the cluster batch queue, where X Server evidently is not 
available) I have the following error when loading rgl:

Loading required package: KernSmooth
KernSmooth 2.22 installed
Copyright M. P. Wand 1997
Loading required package: mvtnorm
Loading required package: rgl
Error in dyn.load(file, ...) :
  unable to load shared library '/opt/R-2.6.1/library/rgl/libs/rgl.so':
  libGLU.so.1: cannot open shared object file: No such file or directory
Error : .onLoad failed in 'loadNamespace' for 'rgl'
Error: package 'rgl' could not be loaded
Execution halted

I am issuing the follwing R bach command:
R CMD BATCH --vanilla --slave KS_S1.R

This kind of error appeared 3 years ago on this same list in a different 
contest: no final solution was given, but apparently newer versions of rgl 
fixed the problem. Does anyone have a clue about how to manage this problem 
for BATCH mode? Alternatively, since rgl shouldn't be needed at all becasue 
running in BATCH mode, does someone know how to prevent KernSmooth loading 
rgl?

Thanks so much

Umberto Amato
Istituto per le Applicazioni del Calcolo 'Mauro Picone' CNR
Sede di Napoli
Via Pietro Castellino 111
I-80131 Napoli, Italy

Tel. +39 0816132377
Fax +39 0816132597
E-mail: u.amato at iac.cnr.it


From petr.pikal at precheza.cz  Tue Dec 11 10:58:34 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Tue, 11 Dec 2007 10:58:34 +0100
Subject: [R] Odp:  help with fatal error message
In-Reply-To: <bebd16360712101242g276a7c9dy2e2c57a224ad972@mail.gmail.com>
Message-ID: <OFF0F96D66.4B444C0C-ONC12573AE.0036171C-C12573AE.0036B9D9@precheza.cz>

Hi

AFAIK sometimes this can happen if your .Rdata was saved when some 
packages had been in action. But in that case you should have your message 
extended with someting like:

"cannot load package ...." or similar. 

It helped me when I started plain R session invoked necessary packages and 
opened a particular workspace. However if your .Rdata is corrupted I do 
not know about any procedure how to restore it. Therefore it is 
recommended to save copies of your data in other formats (txt, csv, ...) 
and I usually save history periodically to separate files so I can go back 
to what I have done before.
Regards

Petr
petr.pikal at precheza.cz

r-help-bounces at r-project.org napsal dne 10.12.2007 21:42:37:

> Dear helper,
> 
> After some cleaning, I found out that I cannot open my R2.6.0 console
> with message: "Fatal error: unable to restore saved data in .Rdata",
> and every time I retry it just kick me off. I am wondering if I still
> be able to retrieve the data that I've been working for a month.
> 
> I have installed the new R2.6.1 and it has no problem to open, only
> there is nothing in it I also wonder if I will be able to transfer all
> the files in 2.6.0 to 2.6.1. I appreciate anyone who can help.
> 
> Thank you,
> Ilham
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Tue Dec 11 11:24:25 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Dec 2007 10:24:25 +0000 (GMT)
Subject: [R] Error with rgl loading in BATCH mode
In-Reply-To: <1636121D4BAD492897522639D61B7C75@na.iac.cnr.it>
References: <1636121D4BAD492897522639D61B7C75@na.iac.cnr.it>
Message-ID: <Pine.LNX.4.64.0712110959410.11162@gannet.stats.ox.ac.uk>

I don't think this is anything to do with BATCH mode: the machine you are 
running R BATCH on does not have GLU installed.  rgl *will* load on a 
system without an X server running.

KernSmooth does *not* require the rgl *package*, so you are (twice) 
blaming an innocent party.  You have doctored the output to give a false 
impression, and should be blaming some other package that loads all of 
KernSmooth, mvtnorm and rgl (perhaps package ks).

On Tue, 11 Dec 2007, Umberto Amato wrote:

> Dear all,
> I'm trying to run a script that requires KernSmooth in BATCH mode but I get
> an error while loading rgl library that is needed by KernSmooth. Actually I
> have to run several batch files through a queue of a cluster, so I wouldn't
> need graphics at all.
> I installed the latest releases of R (2.6.1) and rgl on my CentOS 4.2 Linux
> (clone of Red Hat EL) endowed with Rocks 4.2.1 suite for clusters.
> If I run the R script from a ssh session on my PC equipped with X Server (I
> use Xming), everithing goes smoothly, rgl package is loaded correctly and
> the script runs smoothly. However when run in batch mode (that is through a
> submission to the cluster batch queue, where X Server evidently is not
> available) I have the following error when loading rgl:

You truncated the output here without indication to give a misleading 
context.

> Loading required package: KernSmooth
> KernSmooth 2.22 installed
> Copyright M. P. Wand 1997
> Loading required package: mvtnorm
> Loading required package: rgl
> Error in dyn.load(file, ...) :
>  unable to load shared library '/opt/R-2.6.1/library/rgl/libs/rgl.so':
>  libGLU.so.1: cannot open shared object file: No such file or directory
> Error : .onLoad failed in 'loadNamespace' for 'rgl'
> Error: package 'rgl' could not be loaded
> Execution halted
>
> I am issuing the follwing R bach command:
> R CMD BATCH --vanilla --slave KS_S1.R
>
> This kind of error appeared 3 years ago on this same list in a different
> contest: no final solution was given, but apparently newer versions of rgl
> fixed the problem. Does anyone have a clue about how to manage this problem
> for BATCH mode? Alternatively, since rgl shouldn't be needed at all becasue
> running in BATCH mode, does someone know how to prevent KernSmooth loading
> rgl?
>
> Thanks so much
>
> Umberto Amato
> Istituto per le Applicazioni del Calcolo 'Mauro Picone' CNR
> Sede di Napoli
> Via Pietro Castellino 111
> I-80131 Napoli, Italy
>
> Tel. +39 0816132377
> Fax +39 0816132597
> E-mail: u.amato at iac.cnr.it
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tom.soyer at gmail.com  Tue Dec 11 12:17:35 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Tue, 11 Dec 2007 05:17:35 -0600
Subject: [R] Alternative to For Loop?
Message-ID: <65cc7bdf0712110317p437653e0g68bb6d8dd060e8f6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071211/75d6420f/attachment.pl 

From murdoch at stats.uwo.ca  Tue Dec 11 12:36:25 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 11 Dec 2007 06:36:25 -0500
Subject: [R] Alternative to For Loop?
In-Reply-To: <65cc7bdf0712110317p437653e0g68bb6d8dd060e8f6@mail.gmail.com>
References: <65cc7bdf0712110317p437653e0g68bb6d8dd060e8f6@mail.gmail.com>
Message-ID: <475E7639.5020704@stats.uwo.ca>

tom soyer wrote:
> Hi,
>
> I am doing a calculation on a long series using a For Loop. Here is an
> example of the calculation:
>
> accumulate=function(x){
>  y=0
>  z=0
>  for(i in 1:length(x)){
>   y=y+x[i]
>   z=c(z,y)
>
>  }
>  return(z[2:length(z)])
> }
>
>   
>> x=c(1:10)
>> x
>>     
>  [1]  1  2  3  4  5  6  7  8  9 10
>   
>> accumulate(x)
>>     
>  [1]  1  3  6 10 15 21 28 36 45 55
>   
>
> Although the For Loop works, looping through an array is time consuming and
> inefficient when the series is large. Does anyone know a faster way of doing
> the same calculation? Thanks!

cumsum(x) is what you want.

Duncan Murdoch


From rfrancois at mango-solutions.com  Tue Dec 11 12:36:54 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Tue, 11 Dec 2007 11:36:54 -0000
Subject: [R] Alternative to For Loop?
References: <65cc7bdf0712110317p437653e0g68bb6d8dd060e8f6@mail.gmail.com>
Message-ID: <3CBFCFB1FEFFA841BA83ADF2F2A9C6FA048B16@mango-data1.Mango.local>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20071211/efaefe2d/attachment.pl 

From ptit_bleu at yahoo.fr  Tue Dec 11 12:49:49 2007
From: ptit_bleu at yahoo.fr (Ptit_Bleu)
Date: Tue, 11 Dec 2007 03:49:49 -0800 (PST)
Subject: [R] Sweave : change value in rnw file to generate multiple
 "single" reports ?
In-Reply-To: <971536df0712100844x82d594au30abb1652092035@mail.gmail.com>
References: <14256204.post@talk.nabble.com>
	<971536df0712100844x82d594au30abb1652092035@mail.gmail.com>
Message-ID: <14272449.post@talk.nabble.com>


Thanks everybody for your help (and sorry to be silent : no network today
morning),
I think I will first try the solution of Greg Snow-2 or the one of Thierry
(because they gave examples of script - as a newbye it is very important to
me).

I'm also interested in the solutions proposed by Uwe and Gabor, but I'm not
familiar with environment variables (I will try ?Sys.getenv) and I have no
idea how to edit .rnw file with R (but as it seems that everything is
possible with R, so why not).

Again thank you,
And if you have further advices to help me writing scripts (in R for
example), do not hesitate.
Have a nice day,
Ptit Bleu.

PS : I'm using R version 2.5.1 (2007-06-27) with XP
-- 
View this message in context: http://www.nabble.com/Sweave-%3A-change-value-in-rnw-file-to-generate-multiple-%22single%22-reports---tp14256204p14272449.html
Sent from the R help mailing list archive at Nabble.com.


From tom.soyer at gmail.com  Tue Dec 11 13:01:29 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Tue, 11 Dec 2007 06:01:29 -0600
Subject: [R] Alternative to For Loop?
In-Reply-To: <3CBFCFB1FEFFA841BA83ADF2F2A9C6FA048B16@mango-data1.Mango.local>
References: <65cc7bdf0712110317p437653e0g68bb6d8dd060e8f6@mail.gmail.com>
	<3CBFCFB1FEFFA841BA83ADF2F2A9C6FA048B16@mango-data1.Mango.local>
Message-ID: <65cc7bdf0712110401q6fb3c255jb829e725ab1353b0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071211/0bfd4285/attachment.pl 

From nshephard at gmail.com  Tue Dec 11 13:03:10 2007
From: nshephard at gmail.com (Neil Shephard)
Date: Tue, 11 Dec 2007 04:03:10 -0800 (PST)
Subject: [R] editor under MAC system
In-Reply-To: <31beaa9a0712091210n31b6f292q582c7610234ad1e3@mail.gmail.com>
References: <31beaa9a0712091210n31b6f292q582c7610234ad1e3@mail.gmail.com>
Message-ID: <14272456.post@talk.nabble.com>




YIHSU CHEN-3 wrote:
> 
> Dear R-user;
> I recently switched from PC to MAC.  Is there a compatible editor as
> Win-editor with package RWinEdit for MAC?
> 
> 

I'd recommend using Emacs with ESS (see http://ess.r-project.org/).  The
advantage of this (beyond the seamless integration) is that its pretty
platform neutral, and what you learn on your new Mac system will be portable
(i.e. the same method of writing/interacting with your R script/session
whether your on Mac/M$-windows/*NIX variant).

Neil
-- 
View this message in context: http://www.nabble.com/editor-under-MAC-system-tp14249571p14272456.html
Sent from the R help mailing list archive at Nabble.com.


From yn19832 at msn.com  Tue Dec 11 13:06:14 2007
From: yn19832 at msn.com (livia)
Date: Tue, 11 Dec 2007 04:06:14 -0800 (PST)
Subject: [R]  If Else Function
Message-ID: <14272459.post@talk.nabble.com>


Hello everyone, 

I would like to use the "if" statements and I was thinking sth like

If () {} else{ if() {} else{} } 

Is this a possible solution and is there any syntax error?

Could anyone give me some advice? Many thanks.
-- 
View this message in context: http://www.nabble.com/If-Else-Function-tp14272459p14272459.html
Sent from the R help mailing list archive at Nabble.com.


From S.Ellison at lgc.co.uk  Tue Dec 11 13:23:29 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Tue, 11 Dec 2007 12:23:29 +0000
Subject: [R] Barchart, Pareto
Message-ID: <s75e814c.019@tedmail.lgc.co.uk>



--- "Kapoor, Bharat " <k at chicagogsb.edu> wrote:
 
> 1.    I could not print the data in ROF column below
> the bar charts,  How to get full labels for each bar
> insted I createda  new coumn ReasonCode just to fit
> below the bars.

You could plot the barchart horizontally, and increase the margins to
allow the full labels:

windows(6,4) #or x11(6,4)
par(omd=c(0.2,1,0,1))
oo<-order(Frequency, decreasing=F)
barplot(Frequency[oo],names.arg=Reason.for.failure[oo], horiz=T,
las=1)


From petr.pikal at precheza.cz  Tue Dec 11 13:47:02 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Tue, 11 Dec 2007 13:47:02 +0100
Subject: [R] Odp:   If Else Function
In-Reply-To: <14272459.post@talk.nabble.com>
Message-ID: <OF1A2291AB.811A2383-ONC12573AE.0045B421-C12573AE.00462690@precheza.cz>

Hi

r-help-bounces at r-project.org napsal dne 11.12.2007 13:06:14:

> 
> Hello everyone, 
> 
> I would like to use the "if" statements and I was thinking sth like
> 
> If () {} else{ if() {} else{} } 
> 
> Is this a possible solution and is there any syntax error?

Except the first letter, nothing sooms to be wrong.

> if (1==1) {print("a")} else { if(1==1) {print("b")} else{print("c")} } 
[1] "a"
> if (1==2) {print("a")} else { if(1==1) {print("b")} else{print("c")} } 
[1] "b"
> if (1==2) {print("a")} else { if(1==2) {print("b")} else{print("c")} } 
[1] "c"
>

I beleive you noticed a difference between if() and ifelse() functions. 
And of course if you spread your command in several rows be carefull not 
to have valid syntax before you ended a whole command.
e.g.
if (1==2) {print("a")} 
else { if(1==2) {print("b")} else{print("c")} }

Error: unexpected 'else' in "else"


Regards
Petr


> 
> Could anyone give me some advice? Many thanks.
> -- 
> View this message in context: http://www.nabble.com/If-Else-Function-
> tp14272459p14272459.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mbordese at gmail.com  Tue Dec 11 13:53:50 2007
From: mbordese at gmail.com (Matias Bordese)
Date: Tue, 11 Dec 2007 09:53:50 -0300
Subject: [R] error trying to load biOps under 2.6.1 running XP
In-Reply-To: <Pine.LNX.4.64.0712110752020.9400@gannet.stats.ox.ac.uk>
References: <475DE038.9040301@noaa.gov>
	<Pine.LNX.4.64.0712110752020.9400@gannet.stats.ox.ac.uk>
Message-ID: <a5ab98fc0712110453y38d7063tab6cbcd071daca4d@mail.gmail.com>

On Dec 11, 2007 5:11 AM, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Mon, 10 Dec 2007, Brian O'Gorman wrote:
>
> > I'm running R 2.6.1 on a WXP machine. When I do the following I get an error
> > message.
> >
> >> library(biOps)
> > Error in dyn.load(file, ...) :
> > unable to load shared library
> > 'C:/PROGRA~1/R/R-26~1.1/library/biOps/libs/biOps.dll':
> > LoadLibrary failure:  The specified module could not be found.
> > Error: package/namespace load failed for 'biOps'
> >
> > Please help, or comments about what I'm doing wrong? (It seems to me the
> > correct directory or folder isn't being located.)
>
> That message should also have given a message box with more information,
> saying what the problem was: it does not usually mean the 'shared library'
> named.  It does for me:
>
> 'The application failed to start because libfftw3-3.dll was not found.'
>
> Other DLLs are also missing: jpeg62.dll and libtiff3.dll (use pedump to
> find this out).  So you need to use Google to find those DLLs and put them
> on your PATH.

As described, the problem seems to be you don't have all the necessary
libraries installed.

You should install the libtiff and libjpeg library binaries. You can
get them from http://gnuwin32.sourceforge.net/packages/tiff.htm and
http://gnuwin32.sourceforge.net/packages/jpeg.htm. You also must check
that the binaries of the libraries are installed in a directory in
your PATH environment variable.

You also need to have libfftw3 installed on your system. You can get
it from ftp://ftp.fftw.org/pub/fftw/fftw-3.1.2-dll.zip, and as with
the other libraries, you should add it to your PATH variable too.

> You also need to read
> http://cran.r-project.org/bin/windows/contrib/2.6/ReadMe
> and suggest to Uwe Ligges what he adds a note about this (it applies to
> SoPhy for libtiff3.dll as well).

Yes, I guess we should add a note.


I hope you get it to work, and no doubt in asking what you need.
Thanks,
Best,
Mat?as Bordese.


From vincent.goulet at act.ulaval.ca  Tue Dec 11 15:03:04 2007
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Tue, 11 Dec 2007 09:03:04 -0500
Subject: [R] editor under MAC system
In-Reply-To: <14272456.post@talk.nabble.com>
References: <31beaa9a0712091210n31b6f292q582c7610234ad1e3@mail.gmail.com>
	<14272456.post@talk.nabble.com>
Message-ID: <1F1C0D82-C41B-418E-94F1-796A643C71C6@act.ulaval.ca>

Le mar. 11 d?c. ? 07:03, Neil Shephard a ?crit :

> YIHSU CHEN-3 wrote:
>>
>> Dear R-user;
>> I recently switched from PC to MAC.  Is there a compatible editor as
>> Win-editor with package RWinEdit for MAC?
>>
>>
>
> I'd recommend using Emacs with ESS (see http://ess.r-project.org/).   
> The
> advantage of this (beyond the seamless integration) is that its pretty
> platform neutral, and what you learn on your new Mac system will be  
> portable
> (i.e. the same method of writing/interacting with your R script/ 
> session
> whether your on Mac/M$-windows/*NIX variant).

I concur. Emacs is one of very few cross-platform editors with a  
special mode for R/S-Plus. If you want to give Emacs a try, I  
recommend Aquamacs (http://aquamacs.org) on OS X. It ships with the  
latest ESS and is better integrated to the OS than a regular Emacs.

In the future, post Mac related question to r-sig-mac, though.

HTH     Vincent

>
>
> Neil
> -- 
> View this message in context: http://www.nabble.com/editor-under-MAC-system-tp14249571p14272456.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

---
   Vincent Goulet, Associate Professor
   ?cole d'actuariat
   Universit? Laval, Qu?bec
   Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From tom.olsson at dnbnor.com  Tue Dec 11 15:29:37 2007
From: tom.olsson at dnbnor.com (Tom.O)
Date: Tue, 11 Dec 2007 06:29:37 -0800 (PST)
Subject: [R]  creating images from console commands
Message-ID: <14275003.post@talk.nabble.com>


Hi

I have forgotten the name of the function, but I know it exists. I want to
be able to export commands enterd / or output in the console to an image.
Does anyone recognize the function I am looking for.

Thanks Tom  

-- 
View this message in context: http://www.nabble.com/creating-images-from-console-commands-tp14275003p14275003.html
Sent from the R help mailing list archive at Nabble.com.


From frankcsliu at gmail.com  Tue Dec 11 15:37:42 2007
From: frankcsliu at gmail.com (Frank Liu)
Date: Tue, 11 Dec 2007 22:37:42 +0800
Subject: [R] Hmisc compilation problem
Message-ID: <475EA0B6.2010107@gmail.com>

I upgraded my system to Fedora Core 7 and got a compilation problem when
installing Hmisc package.
According to the error messages (shown below), I have tried to updated
all of my glibc, gcc, automake rpm packages,
but that did not help. Could you help me pointed out what package I have
missed? Thank you.


> version
_
platform i386-redhat-linux-gnu
arch i386
os linux-gnu
system i386, linux-gnu
status
major 2
minor 6.1
year 2007
month 11
day 26
svn rev 43537
language R
version.string R version 2.6.1 (2007-11-26)

> install.packages("Hmisc")
Warning in install.packages("Hmisc") :
argument 'lib' is missing: using '/usr/lib/R/library'
--- Please select a CRAN mirror for use in this session ---
Loading Tcl/Tk interface ... done
trying URL 'http://cran.csie.ntu.edu.tw/src/contrib/Hmisc_3.4-3.tar.gz'
Content type 'application/x-gzip' length 519009 bytes (506 Kb)
opened URL
==================================================
downloaded 506 Kb

/usr/lib/R/library
* Installing *source* package 'Hmisc' ...
** libs
gfortran -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m32 -march=i386
-mtune=generic -fasynchronous-unwind-tables -c cidxcn.f -o cidxcn.o
gfortran -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m32 -march=i386
-mtune=generic -fasynchronous-unwind-tables -c cidxcp.f -o cidxcp.o
gfortran -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m32 -march=i386
-mtune=generic -fasynchronous-unwind-tables -c hoeffd.f -o hoeffd.o
gfortran -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m32 -march=i386
-mtune=generic -fasynchronous-unwind-tables -c jacklins.f -o jacklins.o
gfortran -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m32 -march=i386
-mtune=generic -fasynchronous-unwind-tables -c largrec.f -o largrec.o
gcc -std=gnu99 -I/usr/lib/R/include -I/usr/lib/R/include
-I/usr/local/include -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2
-fexceptions -fstack-protector --param=ssp-buffer-size=4 -m32
-march=i386 -mtune=generic -fasynchronous-unwind-tables -c ranksort.c -o
ranksort.o
In file included from ranksort.c:1:
/usr/lib/R/include/R.h:28:20: error: stdlib.h: No such file or directory
/usr/lib/R/include/R.h:29:19: error: stdio.h: No such file or directory
In file included from
/usr/lib/gcc/i386-redhat-linux/4.1.2/include/syslimits.h:7,
from /usr/lib/gcc/i386-redhat-linux/4.1.2/include/limits.h:11,
from /usr/lib/R/include/R.h:30,
from ranksort.c:1:
/usr/lib/gcc/i386-redhat-linux/4.1.2/include/limits.h:122:61: error:
limits.h: No such file or directory
In file included from ranksort.c:1:
/usr/lib/R/include/R.h:32:18: error: math.h: No such file or directory
/usr/lib/R/include/R.h:33:19: error: errno.h: No such file or directory
In file included from /usr/lib/R/include/R.h:50,
from ranksort.c:1:
/usr/lib/R/include/R_ext/RS.h:24:39: error: string.h: No such file or
directory
make: *** [ranksort.o] Error 1
ERROR: compilation failed for package 'Hmisc'
** Removing '/usr/lib/R/library/Hmisc'

The downloaded packages are in
/tmp/Rtmpn7J8Y0/downloaded_packages
Updating HTML index of packages in '.Library'
Warning message:
In install.packages("Hmisc") :
installation of package 'Hmisc' had non-zero exit status


-- 
Frank C.S. Liu
Assistant Professor,                      
Graduate Institute of Political Science   E-mail: frankcsliu at gmail.com
National Sun Yat-sen University (NYSYU)   Office:+886.7.525.2000 #5555
Kaohsiung, Taiwan 804, R.O.C.             FAX:+886.7.525.5540


From C.fezzi at uea.ac.uk  Tue Dec 11 15:55:51 2007
From: C.fezzi at uea.ac.uk (Carlo Fezzi)
Date: Tue, 11 Dec 2007 14:55:51 -0000
Subject: [R] R computing speed
Message-ID: <001001c83c05$ec8aedd0$ee64de8b@envcserge19>

Dear helpers,

I am using R version 2.5.1 to estimate a multinomial logit model using my
own maximum likelihood function (I work with share data and the default
function of R cannot deal with that).

However, the computer (I have an Athlon XP 3200+ with 512 GB ram) takes
quite a while to estimate the model.

With 3 categories, 5 explanatory variables and roughly 5000 observations it
takes 2-3 min. For 10 categories and 10 explanatory variables (still 5000
obs) more than 1 hour.

Is there any way I can speed up this process? (Modifying the code or
modifying some R options maybe?)

I would be really grateful if anybody could help me with this issue, I
attach my code below.

Many thanks,

Carlo

***************************************
Carlo Fezzi

Centre for Social and Economic Research 
on the Global Environment (CSERGE),
School of Environmental Sciences,
University of East Anglia,
Norwich, NR4 7TJ
United Kingdom.

***************************************



# MULTILOGIT

# This function computes the estimates of a multinomial logit model

# inputs: 	a matrix vector of 1 and 0 (y) or of shares
# 		a matrix of regressors (x) - MUST HAVE COLUMN NAMES! -
# 		names of the variables, default = colnames(x)
# 		optimization methods, default = 'BFGS'
#		base category, default = 1
#		restrictions, default = NULL
# 		weights, default all equal to 1


# outputs: 	an object of class "multilogit.c"

# McFadden D. (1974) "Conditional logit analysis of qualitative choice
behavior", in Zarembka P. (ed.), Frontiers in Econometrics, Academic Press.


multilogit.c <- function(y, xi, xi.names = colnames(xi), c.base=1,
rest=NULL, w = rep(1,nrow(y)), method='BFGS')
{
	
	n.obs <- sum(w)
	xi<-cbind(1,xi)
	colnames(xi)[1]<-"Intercept"

	nx<-ncol(xi)
	ny<-ncol(y)
	
	beta<-numeric(nx*ny)
	
	negll<- function(beta,y,xi)
	{
		beta[rest]<-0
		beta[(((c.base-1)*nx)+1):(c.base*nx)]<-0
		lli <- y  * (xi%*%matrix(beta,nx,ny) - log ( apply(exp(
xi%*%matrix(beta,nx,ny)) ,1,sum ) ) 	)
		lli<-lli*w
		-sum(lli)
	}

	pi<- apply((y*w),2,mean)/mean(w)
	
	ll0 <- (t(pi)%*%log(pi))*sum(w)
	
	result<-c(	optim(par = rep(0,nx*(ny)), fn = negll, y=y, xi=xi,
hessian=T, method=method),
			list(varnames=xi.names, rest=rest, nx=nx, ny=ny,
npar=nx*(ny-1)-length(rest), ll0=ll0, 	pi=pi, xi=xi,
n.obs=n.obs,c.base=c.base,w=w))
	
	result$par <- result$par[-(((c.base-1)*nx)+1):-(c.base*nx)]
	result$hessian <-
result$hessian[-(((c.base-1)*nx)+1):-(c.base*nx),-(((c.base-1)*nx)+1):-(c.ba
se*nx)]

	class(result)<-"multilogit.c"
	return(result)
}


From ptit_bleu at yahoo.fr  Tue Dec 11 16:04:49 2007
From: ptit_bleu at yahoo.fr (Ptit_Bleu)
Date: Tue, 11 Dec 2007 07:04:49 -0800 (PST)
Subject: [R] Sweave : change value in rnw file to generate multiple
 "single" reports ?
In-Reply-To: <475D6EC6.6080405@statistik.uni-dortmund.de>
References: <14256204.post@talk.nabble.com>
	<475D6EC6.6080405@statistik.uni-dortmund.de>
Message-ID: <14275809.post@talk.nabble.com>


Dear Uwe,

Could you please give me the name of the function to open, to modify and to
save a r script ?
Just the name and I will look for the way to use them.

And if you have a link explaining how to use user customized variables from
an environment variable, it will be very helpful (as it is completely new to
me).

Thanks in advance,
Ptit Bleu.

------------------------------------------------


Within or before SWeave, you can use R.
Hence you can write some R function that first changes the rnw and runs 
SWeave thereafter, or even better, write something in your SWeave code 
that reads a user customized variables, e.g. from an environment 
variable or from some text file.

Uwe Ligges
-- 
View this message in context: http://www.nabble.com/Sweave-%3A-change-value-in-rnw-file-to-generate-multiple-%22single%22-reports---tp14256204p14275809.html
Sent from the R help mailing list archive at Nabble.com.


From P.Dalgaard at biostat.ku.dk  Tue Dec 11 16:08:30 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 11 Dec 2007 16:08:30 +0100
Subject: [R] Hmisc compilation problem
In-Reply-To: <475EA0B6.2010107@gmail.com>
References: <475EA0B6.2010107@gmail.com>
Message-ID: <475EA7EE.6030304@biostat.ku.dk>

Frank Liu wrote:
> I upgraded my system to Fedora Core 7 and got a compilation problem when
> installing Hmisc package.
> According to the error messages (shown below), I have tried to updated
> all of my glibc, gcc, automake rpm packages,
> but that did not help. Could you help me pointed out what package I have
> missed? Thank you.
>
>
>   

[pd at janus ~]$ rpm -qf /usr/include/stdio.h
glibc-headers-2.6-4


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jholtman at gmail.com  Tue Dec 11 16:08:45 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 11 Dec 2007 07:08:45 -0800
Subject: [R] R computing speed
In-Reply-To: <001001c83c05$ec8aedd0$ee64de8b@envcserge19>
References: <001001c83c05$ec8aedd0$ee64de8b@envcserge19>
Message-ID: <644e1f320712110708t7dfbf043v45b8ad926f7148d2@mail.gmail.com>

I would suggest that you use Rprof to get a profile of the code to see
where time is being spent.  You did not provide commented, minimal,
self-contained, reproducible code, so it is hard to tell from just
looking at the code to determine what is happening.  Rprof should
provide an idea of where to look in your code for optimization.  You
might consider colMeans instead of the "apply", but I am not sure if
this will make a significant change in the execution time.

On Dec 11, 2007 6:55 AM, Carlo Fezzi <C.fezzi at uea.ac.uk> wrote:
> Dear helpers,
>
> I am using R version 2.5.1 to estimate a multinomial logit model using my
> own maximum likelihood function (I work with share data and the default
> function of R cannot deal with that).
>
> However, the computer (I have an Athlon XP 3200+ with 512 GB ram) takes
> quite a while to estimate the model.
>
> With 3 categories, 5 explanatory variables and roughly 5000 observations it
> takes 2-3 min. For 10 categories and 10 explanatory variables (still 5000
> obs) more than 1 hour.
>
> Is there any way I can speed up this process? (Modifying the code or
> modifying some R options maybe?)
>
> I would be really grateful if anybody could help me with this issue, I
> attach my code below.
>
> Many thanks,
>
> Carlo
>
> ***************************************
> Carlo Fezzi
>
> Centre for Social and Economic Research
> on the Global Environment (CSERGE),
> School of Environmental Sciences,
> University of East Anglia,
> Norwich, NR4 7TJ
> United Kingdom.
>
> ***************************************
>
>
>
> # MULTILOGIT
>
> # This function computes the estimates of a multinomial logit model
>
> # inputs:       a matrix vector of 1 and 0 (y) or of shares
> #               a matrix of regressors (x) - MUST HAVE COLUMN NAMES! -
> #               names of the variables, default = colnames(x)
> #               optimization methods, default = 'BFGS'
> #               base category, default = 1
> #               restrictions, default = NULL
> #               weights, default all equal to 1
>
>
> # outputs:      an object of class "multilogit.c"
>
> # McFadden D. (1974) "Conditional logit analysis of qualitative choice
> behavior", in Zarembka P. (ed.), Frontiers in Econometrics, Academic Press.
>
>
> multilogit.c <- function(y, xi, xi.names = colnames(xi), c.base=1,
> rest=NULL, w = rep(1,nrow(y)), method='BFGS')
> {
>
>        n.obs <- sum(w)
>        xi<-cbind(1,xi)
>        colnames(xi)[1]<-"Intercept"
>
>        nx<-ncol(xi)
>        ny<-ncol(y)
>
>        beta<-numeric(nx*ny)
>
>        negll<- function(beta,y,xi)
>        {
>                beta[rest]<-0
>                beta[(((c.base-1)*nx)+1):(c.base*nx)]<-0
>                lli <- y  * (xi%*%matrix(beta,nx,ny) - log ( apply(exp(
> xi%*%matrix(beta,nx,ny)) ,1,sum ) )     )
>                lli<-lli*w
>                -sum(lli)
>        }
>
>        pi<- apply((y*w),2,mean)/mean(w)
>
>        ll0 <- (t(pi)%*%log(pi))*sum(w)
>
>        result<-c(      optim(par = rep(0,nx*(ny)), fn = negll, y=y, xi=xi,
> hessian=T, method=method),
>                        list(varnames=xi.names, rest=rest, nx=nx, ny=ny,
> npar=nx*(ny-1)-length(rest), ll0=ll0,   pi=pi, xi=xi,
> n.obs=n.obs,c.base=c.base,w=w))
>
>        result$par <- result$par[-(((c.base-1)*nx)+1):-(c.base*nx)]
>        result$hessian <-
> result$hessian[-(((c.base-1)*nx)+1):-(c.base*nx),-(((c.base-1)*nx)+1):-(c.ba
> se*nx)]
>
>        class(result)<-"multilogit.c"
>        return(result)
> }
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ripley at stats.ox.ac.uk  Tue Dec 11 16:19:19 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Dec 2007 15:19:19 +0000 (GMT)
Subject: [R] Hmisc compilation problem
In-Reply-To: <475EA0B6.2010107@gmail.com>
References: <475EA0B6.2010107@gmail.com>
Message-ID: <Pine.LNX.4.64.0712111509590.8045@auk.stats>

I think you have glibc-headers missing.  On an F8 system I get

eclectus% rpm -qf /usr/include/limits.h
glibc-headers-2.7-2
eclectus% rpm -qf /usr/include/stdio.h
glibc-headers-2.7-2
eclectus% rpm -qf /usr/include/stdlib.h
glibc-headers-2.7-2

You may find that glibc-devel is also missing and needed.

Did you install the R-devel RPM (you cannot have built R on that system)?
That's the way to pull in the dependencies you need to make packages.


On Tue, 11 Dec 2007, Frank Liu wrote:

> I upgraded my system to Fedora Core 7 and got a compilation problem when
> installing Hmisc package.
> According to the error messages (shown below), I have tried to updated
> all of my glibc, gcc, automake rpm packages,
> but that did not help. Could you help me pointed out what package I have
> missed? Thank you.
>
>
>> version
> _
> platform i386-redhat-linux-gnu
> arch i386
> os linux-gnu
> system i386, linux-gnu
> status
> major 2
> minor 6.1
> year 2007
> month 11
> day 26
> svn rev 43537
> language R
> version.string R version 2.6.1 (2007-11-26)
>
>> install.packages("Hmisc")
> Warning in install.packages("Hmisc") :
> argument 'lib' is missing: using '/usr/lib/R/library'
> --- Please select a CRAN mirror for use in this session ---
> Loading Tcl/Tk interface ... done
> trying URL 'http://cran.csie.ntu.edu.tw/src/contrib/Hmisc_3.4-3.tar.gz'
> Content type 'application/x-gzip' length 519009 bytes (506 Kb)
> opened URL
> ==================================================
> downloaded 506 Kb
>
> /usr/lib/R/library
> * Installing *source* package 'Hmisc' ...
> ** libs
> gfortran -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector --param=ssp-buffer-size=4 -m32 -march=i386
> -mtune=generic -fasynchronous-unwind-tables -c cidxcn.f -o cidxcn.o
> gfortran -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector --param=ssp-buffer-size=4 -m32 -march=i386
> -mtune=generic -fasynchronous-unwind-tables -c cidxcp.f -o cidxcp.o
> gfortran -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector --param=ssp-buffer-size=4 -m32 -march=i386
> -mtune=generic -fasynchronous-unwind-tables -c hoeffd.f -o hoeffd.o
> gfortran -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector --param=ssp-buffer-size=4 -m32 -march=i386
> -mtune=generic -fasynchronous-unwind-tables -c jacklins.f -o jacklins.o
> gfortran -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector --param=ssp-buffer-size=4 -m32 -march=i386
> -mtune=generic -fasynchronous-unwind-tables -c largrec.f -o largrec.o
> gcc -std=gnu99 -I/usr/lib/R/include -I/usr/lib/R/include
> -I/usr/local/include -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2
> -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m32
> -march=i386 -mtune=generic -fasynchronous-unwind-tables -c ranksort.c -o
> ranksort.o
> In file included from ranksort.c:1:
> /usr/lib/R/include/R.h:28:20: error: stdlib.h: No such file or directory
> /usr/lib/R/include/R.h:29:19: error: stdio.h: No such file or directory
> In file included from
> /usr/lib/gcc/i386-redhat-linux/4.1.2/include/syslimits.h:7,
> from /usr/lib/gcc/i386-redhat-linux/4.1.2/include/limits.h:11,
> from /usr/lib/R/include/R.h:30,
> from ranksort.c:1:
> /usr/lib/gcc/i386-redhat-linux/4.1.2/include/limits.h:122:61: error:
> limits.h: No such file or directory
> In file included from ranksort.c:1:
> /usr/lib/R/include/R.h:32:18: error: math.h: No such file or directory
> /usr/lib/R/include/R.h:33:19: error: errno.h: No such file or directory
> In file included from /usr/lib/R/include/R.h:50,
> from ranksort.c:1:
> /usr/lib/R/include/R_ext/RS.h:24:39: error: string.h: No such file or
> directory
> make: *** [ranksort.o] Error 1
> ERROR: compilation failed for package 'Hmisc'
> ** Removing '/usr/lib/R/library/Hmisc'
>
> The downloaded packages are in
> /tmp/Rtmpn7J8Y0/downloaded_packages
> Updating HTML index of packages in '.Library'
> Warning message:
> In install.packages("Hmisc") :
> installation of package 'Hmisc' had non-zero exit status
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From christos at nuverabio.com  Tue Dec 11 16:46:32 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 11 Dec 2007 10:46:32 -0500
Subject: [R] book on regular expressions
Message-ID: <000f01c83c0d$047f1d60$0e010a0a@headquarters.silicoinsights>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071211/d9591441/attachment.pl 

From tlumley at u.washington.edu  Tue Dec 11 16:57:10 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 11 Dec 2007 07:57:10 -0800 (PST)
Subject: [R] Simulating Case Control Data
In-Reply-To: <784724.18916.qm@web45807.mail.sp1.yahoo.com>
References: <784724.18916.qm@web45807.mail.sp1.yahoo.com>
Message-ID: <Pine.LNX.4.64.0712110742080.6584@homer24.u.washington.edu>

On Mon, 10 Dec 2007, R. W. wrote:

> Dear R-Help-List,
>
> I was wondering if anyone had experience simulating
> case-control data in R?

I think the only simple method that allows you to specify any arbitrary 
population distribution of predictors and does not rely on the logistic 
regression model being true is to simulate cohorts and then take a 
case-control sample from each one

Eg for a case-control sample of 500 cases and 1000 controls where there is 
about a 1% cumulative incidence
1. Generate all your predictor variables for a cohort of 50,000 people, 
from any distributions you want
2. Specify the disease model. This could be logistic
     logit(p(Y=1))=eta = b0+b1x1+b2x2+...
     p = exp(eta)/(1+exp(eta))
   or it could be anything else.
3. Now sum(p) gives the expected number of cases. Adjust b0 so that this 
is a bit bigger than your desired number, eg 550.
4. Generate Y for the population by rbinom(50000,1,p)
5. Choose 500 cases and 1000 controls using sample().

 	-thomas


From summer.nitely at gmail.com  Tue Dec 11 16:57:42 2007
From: summer.nitely at gmail.com (Summer Nitely)
Date: Tue, 11 Dec 2007 08:57:42 -0700
Subject: [R] Estimated survival function in Epi package
Message-ID: <8a41df090712110757j16afc4a7tee3f2351a9e1932f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071211/b79137c3/attachment.pl 

From C.fezzi at uea.ac.uk  Tue Dec 11 17:01:17 2007
From: C.fezzi at uea.ac.uk (Carlo Fezzi)
Date: Tue, 11 Dec 2007 16:01:17 -0000
Subject: [R] R computing speed
In-Reply-To: <644e1f320712110708t7dfbf043v45b8ad926f7148d2@mail.gmail.com>
Message-ID: <001401c83c0f$1027d100$ee64de8b@envcserge19>

Dear Jim,

Thanks a lot for your suggestion. Indeed substituting the function apply
with rowSums made a great difference. Now the code is much faster!

Cheers,

Carlo

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of jim holtman
Sent: 11 December 2007 15:09
To: Carlo Fezzi
Cc: r-help at r-project.org
Subject: Re: [R] R computing speed

I would suggest that you use Rprof to get a profile of the code to see
where time is being spent.  You did not provide commented, minimal,
self-contained, reproducible code, so it is hard to tell from just
looking at the code to determine what is happening.  Rprof should
provide an idea of where to look in your code for optimization.  You
might consider colMeans instead of the "apply", but I am not sure if
this will make a significant change in the execution time.

On Dec 11, 2007 6:55 AM, Carlo Fezzi <C.fezzi at uea.ac.uk> wrote:
> Dear helpers,
>
> I am using R version 2.5.1 to estimate a multinomial logit model using my
> own maximum likelihood function (I work with share data and the default
> function of R cannot deal with that).
>
> However, the computer (I have an Athlon XP 3200+ with 512 GB ram) takes
> quite a while to estimate the model.
>
> With 3 categories, 5 explanatory variables and roughly 5000 observations
it
> takes 2-3 min. For 10 categories and 10 explanatory variables (still 5000
> obs) more than 1 hour.
>
> Is there any way I can speed up this process? (Modifying the code or
> modifying some R options maybe?)
>
> I would be really grateful if anybody could help me with this issue, I
> attach my code below.
>
> Many thanks,
>
> Carlo
>
> ***************************************
> Carlo Fezzi
>
> Centre for Social and Economic Research
> on the Global Environment (CSERGE),
> School of Environmental Sciences,
> University of East Anglia,
> Norwich, NR4 7TJ
> United Kingdom.
>
> ***************************************
>
>
>
> # MULTILOGIT
>
> # This function computes the estimates of a multinomial logit model
>
> # inputs:       a matrix vector of 1 and 0 (y) or of shares
> #               a matrix of regressors (x) - MUST HAVE COLUMN NAMES! -
> #               names of the variables, default = colnames(x)
> #               optimization methods, default = 'BFGS'
> #               base category, default = 1
> #               restrictions, default = NULL
> #               weights, default all equal to 1
>
>
> # outputs:      an object of class "multilogit.c"
>
> # McFadden D. (1974) "Conditional logit analysis of qualitative choice
> behavior", in Zarembka P. (ed.), Frontiers in Econometrics, Academic
Press.
>
>
> multilogit.c <- function(y, xi, xi.names = colnames(xi), c.base=1,
> rest=NULL, w = rep(1,nrow(y)), method='BFGS')
> {
>
>        n.obs <- sum(w)
>        xi<-cbind(1,xi)
>        colnames(xi)[1]<-"Intercept"
>
>        nx<-ncol(xi)
>        ny<-ncol(y)
>
>        beta<-numeric(nx*ny)
>
>        negll<- function(beta,y,xi)
>        {
>                beta[rest]<-0
>                beta[(((c.base-1)*nx)+1):(c.base*nx)]<-0
>                lli <- y  * (xi%*%matrix(beta,nx,ny) - log ( apply(exp(
> xi%*%matrix(beta,nx,ny)) ,1,sum ) )     )
>                lli<-lli*w
>                -sum(lli)
>        }
>
>        pi<- apply((y*w),2,mean)/mean(w)
>
>        ll0 <- (t(pi)%*%log(pi))*sum(w)
>
>        result<-c(      optim(par = rep(0,nx*(ny)), fn = negll, y=y, xi=xi,
> hessian=T, method=method),
>                        list(varnames=xi.names, rest=rest, nx=nx, ny=ny,
> npar=nx*(ny-1)-length(rest), ll0=ll0,   pi=pi, xi=xi,
> n.obs=n.obs,c.base=c.base,w=w))
>
>        result$par <- result$par[-(((c.base-1)*nx)+1):-(c.base*nx)]
>        result$hessian <-
>
result$hessian[-(((c.base-1)*nx)+1):-(c.base*nx),-(((c.base-1)*nx)+1):-(c.ba
> se*nx)]
>
>        class(result)<-"multilogit.c"
>        return(result)
> }
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gavin.simpson at ucl.ac.uk  Tue Dec 11 17:03:40 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 11 Dec 2007 16:03:40 +0000
Subject: [R] function centralm - does it exist
In-Reply-To: <001f01c83b75$65b78ec0$c0685280@ts.odu.edu>
References: <001f01c83b75$65b78ec0$c0685280@ts.odu.edu>
Message-ID: <1197389020.3235.19.camel@prometheus.geog.ucl.ac.uk>

On Mon, 2007-12-10 at 16:41 -0500, Nuno Prista wrote:
> Hi,
> 
>  
> 
> James S. Clark - Statistical computation for environmental sciences in R -
> mentions a function centralm (pg25) that I believe should be present in the
> base package but I can't find it. It is supposed to calculate means,
> variances, skewness, kurtosis of arrays. Does it exist in some other
> package?

You clearly haven't read and followed the preceding pages of the
chapter ;-)

See page 12, about 1/3 the way down the page.

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From yn19832 at msn.com  Tue Dec 11 17:22:30 2007
From: yn19832 at msn.com (livia)
Date: Tue, 11 Dec 2007 08:22:30 -0800 (PST)
Subject: [R] Odp:   If Else Function
In-Reply-To: <OF1A2291AB.811A2383-ONC12573AE.0045B421-C12573AE.00462690@precheza.cz>
References: <14272459.post@talk.nabble.com>
	<OF1A2291AB.811A2383-ONC12573AE.0045B421-C12573AE.00462690@precheza.cz>
Message-ID: <14277285.post@talk.nabble.com>


Hi, I guess it is because I spread the command in different rows. Thank you
very much for your help.

Petr Pikal wrote:
> 
> Hi
> 
> r-help-bounces at r-project.org napsal dne 11.12.2007 13:06:14:
> 
>> 
>> Hello everyone, 
>> 
>> I would like to use the "if" statements and I was thinking sth like
>> 
>> If () {} else{ if() {} else{} } 
>> 
>> Is this a possible solution and is there any syntax error?
> 
> Except the first letter, nothing sooms to be wrong.
> 
>> if (1==1) {print("a")} else { if(1==1) {print("b")} else{print("c")} } 
> [1] "a"
>> if (1==2) {print("a")} else { if(1==1) {print("b")} else{print("c")} } 
> [1] "b"
>> if (1==2) {print("a")} else { if(1==2) {print("b")} else{print("c")} } 
> [1] "c"
>>
> 
> I beleive you noticed a difference between if() and ifelse() functions. 
> And of course if you spread your command in several rows be carefull not 
> to have valid syntax before you ended a whole command.
> e.g.
> if (1==2) {print("a")} 
> else { if(1==2) {print("b")} else{print("c")} }
> 
> Error: unexpected 'else' in "else"
> 
> 
> Regards
> Petr
> 
> 
>> 
>> Could anyone give me some advice? Many thanks.
>> -- 
>> View this message in context: http://www.nabble.com/If-Else-Function-
>> tp14272459p14272459.html
>> Sent from the R help mailing list archive at Nabble.com.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/If-Else-Function-tp14272459p14277285.html
Sent from the R help mailing list archive at Nabble.com.


From b.otto at uke.uni-hamburg.de  Tue Dec 11 17:27:10 2007
From: b.otto at uke.uni-hamburg.de (Benjamin Otto)
Date: Tue, 11 Dec 2007 17:27:10 +0100
Subject: [R] Mono in postscript device
Message-ID: <000601c83c12$b19d19c0$9f05a20a@matrix.com>

Hi,

Plotting a graphic into a postscript device using family="mono" returns the
following error message:

>>	family 'mono' not included in PostScript device

Looking at postscriptFonts() however lists the "Courier" font as availeable
mono font. So where is the problem?


Thanks guys for your help.
Best regards

Benjamin



sessionInfo()

R version 2.5.0 (2007-04-23) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "tools"     "stats"     "graphics"  "grDevices" "utils"    
[6] "datasets"  "methods"   "base"     

other attached packages:
      affy     affyio    Biobase hgu133acdf    hgu133a 
  "1.14.0"    "1.4.0"   "1.14.0"   "1.16.0"   "1.16.0" 
 
======================================
Benjamin Otto
University Hospital Hamburg-Eppendorf
Institute For Clinical Chemistry
Martinistr. 52
D-20246 Hamburg

Tel.: +49 40 42803 1908
Fax.: +49 40 42803 4971
======================================



-- 
Pflichtangaben gem?? Gesetz ?ber elektronische Handelsregister und Genossenschaftsregister sowie das Unternehmensregister (EHUG):

Universit?tsklinikum Hamburg-Eppendorf
K?rperschaft des ?ffentlichen Rechts
Gerichtsstand: Hamburg

Vorstandsmitglieder:
Prof. Dr. J?rg F. Debatin (Vorsitzender)
Dr. Alexander Kirstein
Ricarda Klein
Prof. Dr. Dr. Uwe Koch-Gromus


From ripley at stats.ox.ac.uk  Tue Dec 11 17:47:37 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Dec 2007 16:47:37 +0000 (GMT)
Subject: [R] Mono in postscript device
In-Reply-To: <000601c83c12$b19d19c0$9f05a20a@matrix.com>
References: <000601c83c12$b19d19c0$9f05a20a@matrix.com>
Message-ID: <Pine.LNX.4.64.0712111642260.24871@gannet.stats.ox.ac.uk>

On Tue, 11 Dec 2007, Benjamin Otto wrote:

> Hi,
>
> Plotting a graphic into a postscript device using family="mono" returns the
> following error message:
>
>>> 	family 'mono' not included in PostScript device
>
> Looking at postscriptFonts() however lists the "Courier" font as availeable
> mono font. So where is the problem?

The 'problem' is that you have not provided us with reproducible code, and 
used an obsolete version of R.  At least some of the ways you might have 
done this you needed to specify the 'fonts' argument to postscript().

As a minimal example,

> postscript(family="mono")
> plot(1:10)
> dev.off()

works in R 2.6.1.  If that does not work for you, you need to update.
If it does, you need to consider what you did differently.

> Thanks guys for your help.
> Best regards
>
> Benjamin
>
> sessionInfo()
>
> R version 2.5.0 (2007-04-23)
> i386-pc-mingw32

[...]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From h.wickham at gmail.com  Tue Dec 11 18:02:27 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 11 Dec 2007 11:02:27 -0600
Subject: [R] book on regular expressions
In-Reply-To: <000f01c83c0d$047f1d60$0e010a0a@headquarters.silicoinsights>
References: <000f01c83c0d$047f1d60$0e010a0a@headquarters.silicoinsights>
Message-ID: <f8e6ff050712110902h3a213df8x2220e58c1d747735@mail.gmail.com>

> Could someone recommend a good book on regular expressions with focus on
> applications/use as it might relate to R.  I remember there was a mention of
> such a reference book recently, but I could not locate that message on the
> archive.

Mastering regular expressions by Jeffrey Friedl
(http://books.google.com/books?q=editions%3AISBN0596528124) is the
classic reference.  Although it doesn't cover R explicitly, R does
support perl compatible regular expressions which are discussed in the
book.  Regardless, most of the topics are general enough not to depend
tightly on the specific regexp engine.

Hadley

-- 
http://had.co.nz/


From york at zipcon.net  Tue Dec 11 18:02:18 2007
From: york at zipcon.net (Anne York)
Date: Tue, 11 Dec 2007 09:02:18 -0800 (PST)
Subject: [R] book on regular expressions
In-Reply-To: <000f01c83c0d$047f1d60$0e010a0a@headquarters.silicoinsights>
References: <000f01c83c0d$047f1d60$0e010a0a@headquarters.silicoinsights>
Message-ID: <Pine.LNX.4.64.0712110901040.17019@localhost>



On Tue, 11 Dec 2007, Christos Hatzis wrote:

CH > Hello,
CH >  
CH > Could someone recommend a good book on regular expressions with focus on
CH > applications/use as it might relate to R.  I remember there was a mention of
CH > such a reference book recently, but I could not locate that message on the
CH > archive.
CH >  
CH > Thanks.
CH > -Christos  



There are MANY FREE tutorials on regular expressions. Just 
google regular expressions tutorial.

Anne


From ggrothendieck at gmail.com  Tue Dec 11 18:08:15 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 11 Dec 2007 12:08:15 -0500
Subject: [R] book on regular expressions
In-Reply-To: <000f01c83c0d$047f1d60$0e010a0a@headquarters.silicoinsights>
References: <000f01c83c0d$047f1d60$0e010a0a@headquarters.silicoinsights>
Message-ID: <971536df0712110908m4cabfac9gd1f42abccdf44b3b@mail.gmail.com>

Its not a book but there are some references to sites that explain
regular expressions in the Links box of the gsubfn home page:

http://gsubfn.googlecode.com

On Dec 11, 2007 10:46 AM, Christos Hatzis <christos at nuverabio.com> wrote:
> Hello,
>
> Could someone recommend a good book on regular expressions with focus on
> applications/use as it might relate to R.  I remember there was a mention of
> such a reference book recently, but I could not locate that message on the
> archive.
>
> Thanks.
> -Christos
>
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com <http://www.nuverabio.com/>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at imail.org  Tue Dec 11 18:32:48 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Tue, 11 Dec 2007 10:32:48 -0700
Subject: [R] Sweave : change value in rnw file to generate multiple
 "single" reports ?
In-Reply-To: <14275809.post@talk.nabble.com>
References: <14256204.post@talk.nabble.com><475D6EC6.6080405@statistik.uni-dortmund.de>
	<14275809.post@talk.nabble.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBD628D2@LP-EXCHVS07.CO.IHC.COM>

I'm not Uwe, but here are some of the things to try if you want to do
this fully in R.

Look at the help for scan,  one approach to reading in a .rnw file is to
use scan, but set it to read each row of the file as a character matrix.
The 'see also' section of the scan help page has some other functions
that could be used to read in the file as well.

Modify the read in character strings with the sub or gsub functions.

Use cat (with a file argument) to write the character strings back to a
file.

Also look at the commandArgs function for one way to pass options (the
device name(s)) to your R script.

You may also want to read (or reread) the "Invoking R" section of "An
Introduction to R".

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Ptit_Bleu
> Sent: Tuesday, December 11, 2007 8:05 AM
> To: r-help at r-project.org
> Subject: Re: [R] Sweave : change value in rnw file to 
> generate multiple "single" reports ?
> 
> 
> Dear Uwe,
> 
> Could you please give me the name of the function to open, to 
> modify and to save a r script ?
> Just the name and I will look for the way to use them.
> 
> And if you have a link explaining how to use user customized 
> variables from an environment variable, it will be very 
> helpful (as it is completely new to me).
> 
> Thanks in advance,
> Ptit Bleu.
> 
> ------------------------------------------------
> 
> 
> Within or before SWeave, you can use R.
> Hence you can write some R function that first changes the 
> rnw and runs SWeave thereafter, or even better, write 
> something in your SWeave code that reads a user customized 
> variables, e.g. from an environment variable or from some text file.
> 
> Uwe Ligges
> --
> View this message in context: 
> http://www.nabble.com/Sweave-%3A-change-value-in-rnw-file-to-g
> enerate-multiple-%22single%22-reports---tp14256204p14275809.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From pbarros at ualg.pt  Tue Dec 11 19:20:16 2007
From: pbarros at ualg.pt (Pedro de Barros)
Date: Tue, 11 Dec 2007 18:20:16 +0000
Subject: [R] ggplot - Setting the y-scale in a bar plot
Message-ID: <20071211182017.9DEECF8001@smtp3.ualg.pt>

Dear All (probably Hadley),

I am now trying to customise some plots using a bar geom.

I do not want to use the default binning statistic, but rather 
calculate the bar heigths separately. I do manage this, but for 
comparison purposes I would like to have a set of plots all with the 
same y-axis height. But I do not seem to find out how to fix the 
scale of the y-axis in this case.
Any tips?
Using R 2.6.1 on Windows.

Thanks for any help,
Pedro

I attach below the code I am using:
plotdata<-data.frame(x=factor(2:8), y=0.1*(2:8))
plot1<-ggplot()
plot1<-plot1+layer(data=plotdata, 
mapping=aes_string(x='x',y='y'),geom='bar', stat='identity')

RangeY <-c(0,1)
YBreaks <- (0:10)*diff(RangeY)/10
YTickLabels<- as.character(YBreaks)

plot2 <- plot1 + scale_y_continuous(limits=RangeY, breaks=YBreaks, 
labels=YTickLabels, expand=c(0,0))
print(plot2)


From kzembowe at jhuccp.org  Tue Dec 11 19:23:36 2007
From: kzembowe at jhuccp.org (Zembower, Kevin)
Date: Tue, 11 Dec 2007 13:23:36 -0500
Subject: [R] Using predict()?
Message-ID: <2E8AE992B157C0409B18D0225D0B476306CD9237@XCH-VN01.sph.ad.jhsph.edu>

I'm trying to solve a homework problem using R. The problem gives a list
of cricket chirps per second and corresponding temperature, and asks to
give the equation for the linear model and then predict the temperature
to produce 18 chirps per second. So far, I have:

> # Homework 11.2.1 and 11.3.3
> chirps <- scan()
1: 20
2: 16
3: 19.8
4: 18.4
5: 17.1
6: 15.5
7: 14.7
8: 17.1
9: 15.4
10: 16.2
11: 15
12: 17.2
13: 16
14: 17
15: 14.4
16: 
Read 15 items
> temp <- scan()
1: 88.6
2: 71.6
3: 93.3
4: 84.3
5: 80.6
6: 75.2
7: 69.7
8: 82
9: 69.4
10: 83.3
11: 79.6
12: 82.5
13: 80.6
14: 83.5
15: 76.3
16: 
Read 15 items
> chirps
 [1] 20.0 16.0 19.8 18.4 17.1 15.5 14.7 17.1 15.4 16.2 15.0 17.2 16.0
17.0 14.4
> temp
 [1] 88.6 71.6 93.3 84.3 80.6 75.2 69.7 82.0 69.4 83.3 79.6 82.5 80.6
83.5 76.3
> chirps.res <- lm(chirps ~ temp)
> summary(chirps.res)

Call:
lm(formula = chirps ~ temp)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.56146 -0.58088  0.02972  0.58807  1.53047 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.31433    3.10963  -0.101 0.921028    
temp         0.21201    0.03873   5.474 0.000107 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.9715 on 13 degrees of freedom
Multiple R-Squared: 0.6975,     Adjusted R-squared: 0.6742 
F-statistic: 29.97 on 1 and 13 DF,  p-value: 0.0001067
> # From the linear model summary output above, the equation for the
least squares line is:
> #    y = -0.3143 + 0.2120*x or chirps = -0.3143 + 0.2120*temp
> 

I can then determine the answer to the prediction, using algebra and R:
> pred_temp <- (18+0.3143)/0.2120
> pred_temp
[1] 86.3882

However, I'd like to try to use the predict() function. Since 'chirps'
and 'temp' are just vectors of numbers, and not dataframes, these
failed:
predict(chirps.res, newdata=data.frame(chirp=18))
predict(chirps.res, newdata="chirp=18")
predict(chirps.res, newdata=18)

I then tried to turn my two vectors into a dataframe. I would have bet
money that this would have worked, but it didn't:
> df <- data.frame(chirps, temp)
>  chirps.res <- lm(chirps ~ temp, data=df)
> predict(chirps.res, newdata=data.frame(chirps=18))

Can anyone tell me how to use predict() in this circumstance?

Thanks for your help and advice.

-Kevin

Kevin Zembower
Internet Services Group manager
Center for Communication Programs
Bloomberg School of Public Health
Johns Hopkins University
111 Market Place, Suite 310
Baltimore, Maryland  21202
410-659-6139 


From topkatz at msn.com  Tue Dec 11 19:42:17 2007
From: topkatz at msn.com (Talbot Katz)
Date: Tue, 11 Dec 2007 13:42:17 -0500
Subject: [R] Reading through a group of .RData files
In-Reply-To: <Pine.LNX.4.64.0712101427520.31359@enigma.local>
References: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>
	<Pine.LNX.4.64.0712101427520.31359@enigma.local>
Message-ID: <BAY108-W113C574592F6D1DC524702AA640@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071211/dd6897df/attachment.pl 

From bcarvalh at jhsph.edu  Tue Dec 11 19:43:42 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 11 Dec 2007 13:43:42 -0500
Subject: [R] Reading through a group of .RData files
In-Reply-To: <BAY108-W113C574592F6D1DC524702AA640@phx.gbl>
References: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>
	<Pine.LNX.4.64.0712101427520.31359@enigma.local>
	<BAY108-W113C574592F6D1DC524702AA640@phx.gbl>
Message-ID: <EF7C65FF-57C6-4A5B-91E1-A174EBAF166B@jhsph.edu>

actually, the lapply() was supposed to be used with your particular  
function...

to remove, all you need is:

rm(list=myObj)

b

On Dec 11, 2007, at 1:42 PM, Talbot Katz wrote:

> Thanks again, Benilton, I found your suggestions very helpful.  But  
> I couldn't seem to use the lapply / get combination to remove the  
> objects that were loaded.  The best I could come up with was:
>
> eval( parse( text = paste( "rm(", paste( myObj, collapse = "," ),  
> " )", sep = "" ) ) )
>
> I'm wondering whether there's a cleaner / easier way to remove a set  
> of objects whose names are listed in a character array (as returned  
> by load) ?
>
> Thank you!
>
> --  TMK  --
> 212-460-5430 home
> 917-656-5351 cell
>
>
>
> > Date: Mon, 10 Dec 2007 14:31:19 -0500
> > From: bcarvalh at jhsph.edu
> > To: topkatz at msn.com
> > CC: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Reading through a group of .RData files
> >
> > note that load() returns, invisibly, a string with the names of the
> > objects that were loaded. something in the lines of:
> >
> > myObj <- load(file.path(fnDir, cvListFiles[i]))
> > myFunction(get(myObj))
> > rm(list=myObj)
> >
> > might be closer to what you want.
> >
> > moreover, if length(myObj) > 1, you might want sth like:
> >
> > lapply(myObj, function(x) myFunction(get(x)))
> >
> > instead...
> >
> > best
> > b
> >
> > On Mon, 10 Dec 2007, Talbot Katz wrote:
> >
> > >
> > > Hi.
> > >
> > > I have a procedure that reads a directory, loops through a set  
> of particular .RData files, loading each one, and feeding its  
> object(s) into a function, as follows:
> > >
> > > cvListFiles<-list.files(fnDir);
> > > for(i in grep(paste("^",pfnStub,".*\\.RData 
> $",sep=""),cvListFiles)){
> > > load(paste(fnDir,cvListFiles[i],sep="/"));
> > > myFunction(rliObject);
> > > rm(rliObject);
> > > };
> > >
> > > where fnDir is the directory I'm reading, and pfnStub is a  
> string that begins the name of each of the files I want to load. As  
> you can see, I'm assuming that each of the selected .RData files  
> contains an object named "rliObject" and I'm hoping that nothing in  
> any of the files I'm loading overwrites an object in my environment.  
> I'd like to clean this up so that I can extract the object(s) from  
> each data file, and feed them to my function, whatever their names  
> are, without corrupting my environment. I'd appreciate any  
> assistance. Thanks!
> > >
> > > -- TMK --212-460-5430 home917-656-5351 cell
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
>


From davidr at rhotrading.com  Tue Dec 11 19:50:48 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 11 Dec 2007 12:50:48 -0600
Subject: [R] book on regular expressions
In-Reply-To: <000f01c83c0d$047f1d60$0e010a0a@headquarters.silicoinsights>
References: <000f01c83c0d$047f1d60$0e010a0a@headquarters.silicoinsights>
Message-ID: <F9F2A641C593D7408925574C05A7BE7763731C@rhopost.rhotrading.com>

I've found the O'Reilly pocket reference "Regular Expression" (about USD
10.00) quite handy.
It covers Perl, C, PHP, Python, Java, and .Net.

David L. Reiner
Rho Trading Securities, LLC

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Christos Hatzis
Sent: Tuesday, December 11, 2007 9:47 AM
To: r-help at r-project.org
Subject: [R] book on regular expressions

Hello,
 
Could someone recommend a good book on regular expressions with focus on
applications/use as it might relate to R.  I remember there was a
mention of
such a reference book recently, but I could not locate that message on
the
archive.
 
Thanks.
-Christos  
 
Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com <http://www.nuverabio.com/> 
 
 

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From christos at nuverabio.com  Tue Dec 11 19:56:37 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 11 Dec 2007 13:56:37 -0500
Subject: [R] book on regular expressions
In-Reply-To: <F9F2A641C593D7408925574C05A7BE7763731C@rhopost.rhotrading.com>
References: <000f01c83c0d$047f1d60$0e010a0a@headquarters.silicoinsights>
	<F9F2A641C593D7408925574C05A7BE7763731C@rhopost.rhotrading.com>
Message-ID: <004301c83c27$8fa908f0$0e010a0a@headquarters.silicoinsights>

Great.
Two books were consistently recommended: the pocket guide "Regular
Expressions" and the more extensive "Mastering Regular Expressions", both by
O'Reilly.

Thanks to all those who responded.

-Christos 

> -----Original Message-----
> From: davidr at rhotrading.com [mailto:davidr at rhotrading.com] 
> Sent: Tuesday, December 11, 2007 1:51 PM
> To: christos at nuverabio.com; r-help at r-project.org
> Subject: RE: [R] book on regular expressions
> 
> I've found the O'Reilly pocket reference "Regular Expression" 
> (about USD
> 10.00) quite handy.
> It covers Perl, C, PHP, Python, Java, and .Net.
> 
> David L. Reiner
> Rho Trading Securities, LLC
> 
> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org]
> On Behalf Of Christos Hatzis
> Sent: Tuesday, December 11, 2007 9:47 AM
> To: r-help at r-project.org
> Subject: [R] book on regular expressions
> 
> Hello,
>  
> Could someone recommend a good book on regular expressions 
> with focus on applications/use as it might relate to R.  I 
> remember there was a mention of such a reference book 
> recently, but I could not locate that message on the archive.
>  
> Thanks.
> -Christos  
>  
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com <http://www.nuverabio.com/> 
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From johannes_graumann at web.de  Tue Dec 11 20:36:39 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Tue, 11 Dec 2007 20:36:39 +0100
Subject: [R] Extracting clusters from Data Frame
References: <fjjet2$6k3$1@ger.gmane.org>
	<45f568c70712100606o26bd0008t2b708a0142bdf640@mail.gmail.com>
Message-ID: <fjmos4$tgs$1@ger.gmane.org>

Gustaf Rydevik wrote:

> On Dec 10, 2007 2:28 PM, Johannes Graumann <johannes_graumann at web.de>
> wrote:
>> Hello,
>>
>> I have a large data frame (1006222 rows), which I subject to a crude
>> clustering attempt that results in a vector stating whether the datapoint
>> represented by a row belongs to a cluster or not. Conceptually this looks
>> something like this:
>> Value   Cluster?
>> 0.01    FALSE
>> 0.03    TRUE
>> 0.04    TRUE
>> 0.05    TRUE
>> 0.07    FALSE
>> ...
>> What I'm looking for is an efficient strategy to extract all consecutive
>> rows associated with "TRUE" as a single cluster (data.frame
>> representation?) without cluttering memory with thousends of data.frames.
>> I was thinking of an independent data.frame that would contain a column
>> of lists that reference all indexes from the big one which are contained
>> in one cluster ...
>> Can anyone kindly nudge me and let me know how to deal with this
>> efficiently?
>>
>> Joh
>>
> 
> How about :
> orig.data<-sample(c(TRUE,FALSE),100,replace=T)
>
Cluster<-data.frame(c.ndx=cumsum(rle(orig.data)$lengths),c.size=rle(orig.data)$lengths,c.type=rle(orig.data)$values)
> Cluster<-Cluster[Cluster$c.type==TRUE,]
> 
> ##Then, to get all original data belonging to cluster three:
> orig.data[rev(Cluster[3,"c.ndx"]-seq(length.out=Cluster[3,"c.size"])+1)]
> 
> 
> Not the neatest solution, but I'm sure someone here can improve on it.
> /Gustaf

Thank you for this example! "rle" was indeed what safed my day!

Joh


From eugen_pircalabelu at yahoo.com  Tue Dec 11 20:40:31 2007
From: eugen_pircalabelu at yahoo.com (eugen pircalabelu)
Date: Tue, 11 Dec 2007 11:40:31 -0800 (PST)
Subject: [R] question regarding arima function and predicted values
Message-ID: <68174.42487.qm@web38612.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071211/2b8913d4/attachment.pl 

From brownja at msu.edu  Tue Dec 11 21:07:14 2007
From: brownja at msu.edu (James T Brown)
Date: Tue, 11 Dec 2007 15:07:14 -0500
Subject: [R] Building R on Sun Solaris 10 (SPARC) using Sun Studio 12
In-Reply-To: <Pine.LNX.4.64.0712110738050.9400@gannet.stats.ox.ac.uk>
References: <475D64DE.90302@msu.edu>
	<Pine.LNX.4.64.0712110738050.9400@gannet.stats.ox.ac.uk>
Message-ID: <475EEDF2.8050605@msu.edu>

Prof Ripley:

>>
>> Just curious if anyone has successfully built R on a SPARC
>> platform running Sun Solaris 10 using the latest Sun Studio
>> 12 set of compilers.   If so, I would be interested in the
>> compile flags that you used.
>
> Yes, the flags given in the R-admin manual.  (We don't currently have 
> a Solaris 10 Sparc box running, but this was with the current Sun 
> Studio 12 at the time, just after it came out.)
>

Thanks for reply.

Yes, I have tried this, although I had to remove the "-xc99" flag from
the CC environment variable.  When used as shown in the installation
manual, the compile fails:

In file included from regex.c:107:
/usr/include/stdbool.h:42:2: #error "Use of <stdbool.h> is valid only in 
a c99 compilation environment."
*** Error code 1
The following command caused the error:
cpp -M -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H regex.c > regex.d
make: Fatal error: Command failed for target `regex.d'
Current working directory /apps/local/src/R-2.6.1/src/main
*** Error code 1



By removing the "-xc99" flag and setting the others options as specified
in the installation manual, the R package builds completely, but the
"foreign" package continues to crash when I attempt to load it - all
of the other packages that I have tried seem to work just fine.   I
suspect that since those that have built with GCC are not reporting
any trouble, it would be difficult to point a finger at the "foreign"
code and claim that this is a bug, but yet, I am not having any trouble
with any of the other packages that have been built using Sun Studio 12.

Here are the latest "configure" options that I have tried (as
specified in the manual, minus the "-xc99" flag):

  % ./configure --prefix=/usr/local/R-2.6.1
               CC=/opt/SUNWspro/bin/cc
               CFLAGS="-O -xlibmieee"
               LDFLAGS="-L/usr/local/lib -R/usr/local/lib"
               CXX=/opt/SUNWspro/bin/CC
               CXXFLAGS=-O
               F77=/opt/SUNWspro/bin/f95
               FFLAGS=-O
               FC=/opt/SUNWspro/bin/f95
               FCFLAGS=-O
               CPPFLAGS="-I/usr/local/include"
               SHLIB_CXXLDFLAGS="-G -lCstd"

  % make

  % /usr/local/R-2.6.1/bin/R

    > library(foreign)

 *** caught segfault ***
address fdde9918, cause 'invalid permissions'

Traceback:
 1: .C("spss_init", PACKAGE = "foreign")
 2: fun(...)
 3: doTryCatch(return(expr), name, parentenv, handler)
 4: tryCatchOne(expr, names, parentenv, handlers[[1]])
 5: tryCatchList(expr, classes, parentenv, handlers)
 6: tryCatch(expr, error = function(e) {    call <- conditionCall(e)    
if (!is.null(call)) {        if (identical(call[[1]], 
quote(doTryCatch)))             call <- sys.call(-4)        dcall <- 
deparse(call)[1]        prefix <- paste("Error in", dcall, ": ")        
LONG <- 75        msg <- conditionMessage(e)        sm <- strsplit(msg, 
"\n")[[1]]        if (14 + nchar(dcall, type = "w") + nchar(sm[1], type 
= "w") >             LONG)             prefix <- paste(prefix, "\n  ", 
sep = "")    }    else prefix <- "Error : "    msg <- paste(prefix, 
conditionMessage(e), "\n", sep = "")    
.Internal(seterrmessage(msg[1]))    if (!silent && 
identical(getOption("show.error.messages"),         TRUE)) {        
cat(msg, file = stderr())        .Internal(printDeferredWarnings())    
}    invisible(structure(msg, class = "try-error"))})
 7: try({    fun(...)    NULL})
 8: runHook(".onLoad", package, env, package.lib, package)
 9: loadNamespace(package, c(which.lib.loc, lib.loc), keep.source = 
keep.source)
10: doTryCatch(return(expr), name, parentenv, handler)
11: tryCatchOne(expr, names, parentenv, handlers[[1]])
12: tryCatchList(expr, classes, parentenv, handlers)
13: tryCatch(expr, error = function(e) {    call <- conditionCall(e)    
if (!is.null(call)) {        if (identical(call[[1]], 
quote(doTryCatch)))             call <- sys.call(-4)        dcall <- 
deparse(call)[1]        prefix <- paste("Error in", dcall, ": ")        
LONG <- 75        msg <- conditionMessage(e)        sm <- strsplit(msg, 
"\n")[[1]]        if (14 + nchar(dcall, type = "w") + nchar(sm[1], type 
= "w") >             LONG)             prefix <- paste(prefix, "\n  ", 
sep = "")    }    else prefix <- "Error : "    msg <- paste(prefix, 
conditionMessage(e), "\n", sep = "")    
.Internal(seterrmessage(msg[1]))    if (!silent && 
identical(getOption("show.error.messages"),         TRUE)) {        
cat(msg, file = stderr())        .Internal(printDeferredWarnings())    
}    invisible(structure(msg, class = "try-error"))})
14: try({    ns <- loadNamespace(package, c(which.lib.loc, lib.loc), 
keep.source = keep.source)    dataPath <- file.path(which.lib.loc, 
package, "data")    env <- attachNamespace(ns, pos = pos, dataPath = 
dataPath)})
15: library(foreign)

Possible actions:
1: abort (with core dump, if enabled)
2: normal R exit
3: exit R without saving workspace
4: exit R saving workspace







I have recently applied several patches for Sun Studio 12.   I am
tempted to try Sun Studio 11 to see if there is any difference.

I guess the other alternative would be to grab the R package from
"www.sunfreeware.com".   However, most of the 3rd-party packages
on my system have been built with Sun Studio 12 - was just hoping
to be able to do the same with R.   I was also curious if anyone
else is using the latest (patched) Sun Studio 12 set of compilers
on Sun Solaris 10 (sparc) and had any luck.  I am running out of
different complier flags to try.   I have tried several different
flags and most times, the R packages will build, but everytime
that I attempt to load "foreign", it crashes.  Again, all of the
other R packages that I have tried seem to be fine - I am simply
running into trouble with "foreign" which I believe is needed for
me to install the "maptools" package.


All the best.


Jim










>
>> I have tried several different builds of version 2.5.1, 2.6.0,
>> and 2.6.1 using various different compile flags and I am able
>> to compile and check, but for whatever reason, the "foreign"
>> package crashes whenever it is loaded.   I need the "foreign"
>> package in order to install the "maptools" package.   This is
>> the error that I am getting when attempting to load "foreign":
>>
>>>> library(foreign)
>>>
>>>  *** caught segfault ***
>>> address fbb1dc40, cause 'invalid permissions'
>>>
>>> Traceback:
>>>  1: .C("spss_init", PACKAGE = "foreign")
>>>  2: fun(...)
>>>  3: doTryCatch(return(expr), name, parentenv, handler)
>>>  4: tryCatchOne(expr, names, parentenv, handlers[[1]])
>>>  5: tryCatchList(expr, classes, parentenv, handlers)
>>>  6: tryCatch(expr, error = function(e) {    call <-
>>> conditionCall(e)    if (!is.null(call)) {        if
>>> (identical(call[[1]], quote(doTryCatch)))             call <-
>>> sys.call(-4)        dcall <- deparse(call)[1]        prefix <-
>>> paste("Error in", dcall, ": ")        LONGCALL <- 30        if
>>> (nchar(dcall) > LONGCALL)             prefix <- paste(prefix, "\n\t",
>>> sep = "")    }    else prefix <- "Error : "    msg <- paste(prefix,
>>> conditionMessage(e), "\n", sep = "")
>>> .Internal(seterrmessage(msg[1]))    if (!silent &&
>>> identical(getOption("show.error.messages"),         TRUE)) {
>>> cat(msg, file = stderr())        .Internal(printDeferredWarnings())
>>> }    invisible(structure(msg, class = "try-error"))})
>>>  7: try({    fun(...)    NULL})
>>>  8: runHook(".onLoad", package, env, package.lib, package)
>>>  9: loadNamespace(package, c(which.lib.loc, lib.loc), keep.source =
>>> keep.source)
>>> 10: doTryCatch(return(expr), name, parentenv, handler)
>>> 11: tryCatchOne(expr, names, parentenv, handlers[[1]])
>>> 12: tryCatchList(expr, classes, parentenv, handlers)
>>> 13: tryCatch(expr, error = function(e) {    call <-
>>> conditionCall(e)    if (!is.null(call)) {        if
>>> (identical(call[[1]], quote(doTryCatch)))             call <-
>>> sys.call(-4)        dcall <- deparse(call)[1]        prefix <-
>>> paste("Error in", dcall, ": ")        LONGCALL <- 30        if
>>> (nchar(dcall) > LONGCALL)             prefix <- paste(prefix, "\n\t",
>>> sep = "")    }    else prefix <- "Error : "    msg <- paste(prefix,
>>> conditionMessage(e), "\n", sep = "")
>>> .Internal(seterrmessage(msg[1]))    if (!silent &&
>>> identical(getOption("show.error.messages"),         TRUE)) {
>>> cat(msg, file = stderr())        .Internal(printDeferredWarnings())
>>> }    invisible(structure(msg, class = "try-error"))})
>>> 14: try({    ns <- loadNamespace(package, c(which.lib.loc, lib.loc),
>>> keep.source = keep.source)    dataPath <- file.path(which.lib.loc,
>>> package, "data")    env <- attachNamespace(ns, pos = pos, dataPath =
>>> dataPath)})
>>> 15: library(foreign)
>>>
>>> Possible actions:
>>> 1: abort (with core dump, if enabled)
>>> 2: normal R exit
>>> 3: exit R without saving workspace
>>> 4: exit R saving workspace
>>
>>
>>
>> The latest build was compiled with the following flags:
>>
>>> ./configure --prefix=/usr/local/R-2.5.1
>>>             --with-blas
>>>             --with-lapack
>>>             --with-tcl-config=/usr/local/lib/tclConfig.sh
>>>             --with-tk-config=/usr/local/lib/tkConfig.sh
>>>             --without-iconv
>>>             R_PAPERSIZE=letter
>>>             SHLIB_CXXLDFLAGS="-G /opt/SUNWspro/lib/libCrun.so"
>>>             CC=/opt/SUNWspro/bin/cc CXX=/opt/SUNWspro/bin/CC
>>>             F77=/opt/SUNWspro/bin/f77 F90=/opt/SUNWspro/bin/f95
>>>             FC=/opt/SUNWspro/bin/f95 CFLAGS="-mt -ftrap=%none
>>> -xarch=sparcvis -fPIC -xmemalign=4s"
>>>             CXXFLAGS="-mt -ftrap=%none -xarch=sparcvis -xmemalign=4s"
>>>             FFLAGS="-mt -ftrap=%none -shared -xarch=sparcvis"
>>>             FCFLAGS="-mt -ftrap=%none -shared -xarch=sparcvis"
>>>             LDFLAGS="-V -fPIC -L/usr/local/lib -L/opt/SUNWspro/lib
>>> -L/usr/sfw/lib -L/usr/lib
>>>
>>> -R/usr/local/lib:/opt/SUNWspro/lib:/usr/sfw/lib:/usr/lib"
>>
>>
>> I have been messing with the "xmemalign" flag, but doesn't seem to 
>> have much
>> of an impact.   I am curious if there may be a simple compile flag in 
>> Sun
>> Studio 12 that can be set to fix this problem.
>>
>>
>> At any rate, if anyone has been able to successfully build on Sun 
>> Solaris 10
>> (SPARC) using Sun Studio 12 and the "foreign" package loads without
>> crashing,
>> I would be most appreciative if you could let me take a look at your
>> ".configure"
>> options.
>>
>>
>> NOTE: So far, the only package that I am having trouble with is
>> "foreign".   Everything
>> else seems to build and check ok.  In fact, when "foreign" is built,
>> there are no errors
>> reported during the compile.   Also, I have tried
>> "install.packages("foreign") from
>> within R to upgrade to the latest version of "foreign".  It compiles and
>> installs, but
>> once again, it crashes when R attempts to use it producing the
>> "segfault" error.
>>
>>
>> Any help would be most welcome.
>>
>>
>> Thanks.
>>
>>
>>
>> Jim
>>
>>
>>
>


-- 
=========================================
James T Brown
UNIX System Administrator
Depts. of Geography/Fisheries & Wildlife
107 Geography Bldg.
Michigan State University
East Lansing, MI   48824

email:  brownja at msu.edu


From rkozar at interia.pl  Tue Dec 11 21:32:33 2007
From: rkozar at interia.pl (threshold)
Date: Tue, 11 Dec 2007 12:32:33 -0800 (PST)
Subject: [R]  matrix graph
Message-ID: <14282791.post@talk.nabble.com>


Hi All, simple question: 
do you know how to graph the following object/matrix in a 'surface manner':

          [,1]     [,2]     [,3]    [,4]   [,5]    [,6]
[1,] -0.154 -0.065 0.129 0.637 0.780 0.221
[2,]  0.236  0.580 0.448 0.729 0.859 0.475
[3,]  0.401  0.506 0.310 0.650 0.822 0.448
[4,]  0.548  0.625 0.883 0.825 0.945 0.637
[5,]  0.544  0.746 0.823 0.877 0.861 0.642
[6,]  0.262  0.399 0.432 0.620 0.711 0.404

will be very grateful for hints.

rob
-- 
View this message in context: http://www.nabble.com/matrix-graph-tp14282791p14282791.html
Sent from the R help mailing list archive at Nabble.com.


From gufrgs at gmail.com  Tue Dec 11 21:35:24 2007
From: gufrgs at gmail.com (Giovane)
Date: Tue, 11 Dec 2007 18:35:24 -0200
Subject: [R] =?iso-8859-1?q?postResample_R=B2_and_lm=28=29_R=B2?=
Message-ID: <bc1b93170712111235s6292de4bva3121ff82916f3d3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071211/2dd53816/attachment.pl 

From hb at stat.berkeley.edu  Tue Dec 11 21:51:09 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 11 Dec 2007 12:51:09 -0800
Subject: [R] Reading through a group of .RData files
In-Reply-To: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>
References: <BAY108-W33CDE36BD24B94C37C0BDFAA6B0@phx.gbl>
Message-ID: <59d7961d0712111251tab9b610y4a5ab27eeb72f7e4@mail.gmail.com>

Hi,

depending on what you do and how (and why) you save objects in RData
files in the first place, you might be interested in knowing of the
loadObject()/saveObject() methods of R.utils, as well as
loadCache()/saveCache() in R.cache.

The R.utils methods are basically "clever" wrappers around
load()/save() in the 'base' package that does not rely on saving and
loading the variable name but rather the object.  To save multiple
objects you have wrap them up in a list structure or in an
environment.  Example:

x <- 1:100
saveObject(x, file="foo.RData")
y <- loadObject("foo.RData")
stopifnot(identical(x,y))

u <- list(x=x, y=y)
saveObject(u, file="bar.RData")
v <- loadObject("bar.RData")
stopifnot(identical(u,v))

The R.cache methods let you store objects/results to a file cache
without having to worry about filenames.  Instead the objects are
identified by lookup keys generated from other R objects.  This is
useful for temporary/semi-temporary storing of results, especially
computationally expensive results.  The file cache is persistent
between sessions.  Example:

x <- 1:100
key <- list("x")
saveCache(x, key=key)
y <- loadCache(key)
stopifnot(identical(x,y))

u <- list(x=x, y=y)
key <- list("u")
saveCache(u, key=key)
v <- loadCache(key)
stopifnot(identical(u,v))

Although not of immediate interest, the pathname of the above cache
files can be found by
findCache(key), e.g.
"~/.Rcache/78488a47006df5d333db9e200fc539c5.Rcache".  There are
methods for specifying the root of the file cache, and having
different subdirectories for different projects.

The above example is not showing the full power of using R.cache.
Instead consider this example:

slowFcn <- function(x, y, force=FALSE) {
  # Cached results?
  key <- list(x=x, y=y)
  if (!force) {
    res <- loadCache(key=key)
    if (!is.null(res))
      return(res);
  }

  # Emulate a computational expensive calculation
  Sys.sleep(10)

  res <- list(x=x, y=y, xy=x*y)

  # Save to cache
  saveCache(res, key=key)

  res
}

# First call takes time
> system.time(res1 <- slowFcn(x=1, y=2))
   user  system elapsed
      0       0      10

# All successive calls with the same arguments are instant
> system.time(res2 <- slowFcn(x=1, y=2))
   user  system elapsed
   0.02    0.00    0.01

> stopifnot(identical(res1, res2))

Cheers

Henrik

On 10/12/2007, Talbot Katz <topkatz at msn.com> wrote:
>
> Hi.
>
> I have a procedure that reads a directory, loops through a set of particular .RData files, loading each one, and feeding its object(s) into a function, as follows:
>
> cvListFiles<-list.files(fnDir);
> for(i in grep(paste("^",pfnStub,".*\\.RData$",sep=""),cvListFiles)){
> load(paste(fnDir,cvListFiles[i],sep="/"));
> myFunction(rliObject);
> rm(rliObject);
> };
>
> where fnDir is the directory I'm reading, and pfnStub is a string that begins the name of each of the files I want to load.  As you can see, I'm assuming that each of the selected .RData files contains an object named "rliObject" and I'm hoping that nothing in any of the files I'm loading overwrites an object in my environment.  I'd like to clean this up so that I can extract the object(s) from each data file, and feed them to my function, whatever their names are, without corrupting my environment.  I'd appreciate any assistance.  Thanks!
>
> --  TMK  --212-460-5430 home917-656-5351 cell
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dylan.beaudette at gmail.com  Tue Dec 11 22:12:45 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Tue, 11 Dec 2007 13:12:45 -0800
Subject: [R] matrix graph
In-Reply-To: <14282791.post@talk.nabble.com>
References: <14282791.post@talk.nabble.com>
Message-ID: <200712111312.45178.dylan.beaudette@gmail.com>

On Tuesday 11 December 2007, threshold wrote:
> Hi All, simple question:
> do you know how to graph the following object/matrix in a 'surface manner':
>
>           [,1]     [,2]     [,3]    [,4]   [,5]    [,6]
> [1,] -0.154 -0.065 0.129 0.637 0.780 0.221
> [2,]  0.236  0.580 0.448 0.729 0.859 0.475
> [3,]  0.401  0.506 0.310 0.650 0.822 0.448
> [4,]  0.548  0.625 0.883 0.825 0.945 0.637
> [5,]  0.544  0.746 0.823 0.877 0.861 0.642
> [6,]  0.262  0.399 0.432 0.620 0.711 0.404
>
> will be very grateful for hints.
>
> rob

?image

-- 
Dylan Beaudette
Soil Resource Laboratory
http://casoilresource.lawr.ucdavis.edu/
University of California at Davis
530.754.7341


From tplate at acm.org  Tue Dec 11 22:12:54 2007
From: tplate at acm.org (Tony Plate)
Date: Tue, 11 Dec 2007 14:12:54 -0700
Subject: [R] matrix graph
In-Reply-To: <14282791.post@talk.nabble.com>
References: <14282791.post@talk.nabble.com>
Message-ID: <475EFD56.8010900@acm.org>

Try these:

 > x <- matrix(rnorm(100), ncol=10)
 > persp(x)
 > contour(x)

Also, look at the R graph gallery: http://addictedtor.free.fr/graphiques/

-- Tony Plate

threshold wrote:
> Hi All, simple question: 
> do you know how to graph the following object/matrix in a 'surface manner':
> 
>           [,1]     [,2]     [,3]    [,4]   [,5]    [,6]
> [1,] -0.154 -0.065 0.129 0.637 0.780 0.221
> [2,]  0.236  0.580 0.448 0.729 0.859 0.475
> [3,]  0.401  0.506 0.310 0.650 0.822 0.448
> [4,]  0.548  0.625 0.883 0.825 0.945 0.637
> [5,]  0.544  0.746 0.823 0.877 0.861 0.642
> [6,]  0.262  0.399 0.432 0.620 0.711 0.404
> 
> will be very grateful for hints.
> 
> rob


From h.wickham at gmail.com  Tue Dec 11 22:15:07 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 11 Dec 2007 15:15:07 -0600
Subject: [R] ggplot - Setting the y-scale in a bar plot
In-Reply-To: <20071211182017.9DEECF8001@smtp3.ualg.pt>
References: <20071211182017.9DEECF8001@smtp3.ualg.pt>
Message-ID: <f8e6ff050712111315s623d86ma571c690895e25ec@mail.gmail.com>

Hi Pedro,

What's the problem exactly?  You'll need to compute the range
yourself, and then use scale_y_continuous as you have below.

(Also you can abbreviate the bar chart plotting command to:
qplot(x, y, data=plotdata, geom="bar", stat="identity"))

Hadley

On 12/11/07, Pedro de Barros <pbarros at ualg.pt> wrote:
> Dear All (probably Hadley),
>
> I am now trying to customise some plots using a bar geom.
>
> I do not want to use the default binning statistic, but rather
> calculate the bar heigths separately. I do manage this, but for
> comparison purposes I would like to have a set of plots all with the
> same y-axis height. But I do not seem to find out how to fix the
> scale of the y-axis in this case.
> Any tips?
> Using R 2.6.1 on Windows.
>
> Thanks for any help,
> Pedro
>
> I attach below the code I am using:
> plotdata<-data.frame(x=factor(2:8), y=0.1*(2:8))
> plot1<-ggplot()
> plot1<-plot1+layer(data=plotdata,
> mapping=aes_string(x='x',y='y'),geom='bar', stat='identity')
>
> RangeY <-c(0,1)
> YBreaks <- (0:10)*diff(RangeY)/10
> YTickLabels<- as.character(YBreaks)
>
> plot2 <- plot1 + scale_y_continuous(limits=RangeY, breaks=YBreaks,
> labels=YTickLabels, expand=c(0,0))
> print(plot2)
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://had.co.nz/


From pbarros at ualg.pt  Tue Dec 11 23:04:02 2007
From: pbarros at ualg.pt (Pedro de Barros)
Date: Tue, 11 Dec 2007 22:04:02 +0000
Subject: [R] ggplot - Setting the y-scale in a bar plot
In-Reply-To: <f8e6ff050712111315s623d86ma571c690895e25ec@mail.gmail.com>
References: <20071211182017.9DEECF8001@smtp3.ualg.pt>
	<f8e6ff050712111315s623d86ma571c690895e25ec@mail.gmail.com>
Message-ID: <20071211220403.3AA6BF8001@smtp3.ualg.pt>

Hi Hadley,

Well, the problem seems to be that ggplot is not recognizing the 
scale when set by scale_y_continuous, the maximum value stays exactly 
at the data range, irrespective of what I provide as range.
I am not familiar yet with proto, therefore I do have some difficulty 
delving into the code to find out exactly what is wrong, but I hope 
you can tell if I am doing something pretty stupid, or f it is a 
feature or a bug...

Cheers,
Pedro
At 21:15 2007/12/11, you wrote:
>Hi Pedro,
>
>What's the problem exactly?  You'll need to compute the range
>yourself, and then use scale_y_continuous as you have below.
>
>(Also you can abbreviate the bar chart plotting command to:
>qplot(x, y, data=plotdata, geom="bar", stat="identity"))
>
>Hadley
>
>On 12/11/07, Pedro de Barros <pbarros at ualg.pt> wrote:
> > Dear All (probably Hadley),
> >
> > I am now trying to customise some plots using a bar geom.
> >
> > I do not want to use the default binning statistic, but rather
> > calculate the bar heigths separately. I do manage this, but for
> > comparison purposes I would like to have a set of plots all with the
> > same y-axis height. But I do not seem to find out how to fix the
> > scale of the y-axis in this case.
> > Any tips?
> > Using R 2.6.1 on Windows.
> >
> > Thanks for any help,
> > Pedro
> >
> > I attach below the code I am using:
> > plotdata<-data.frame(x=factor(2:8), y=0.1*(2:8))
> > plot1<-ggplot()
> > plot1<-plot1+layer(data=plotdata,
> > mapping=aes_string(x='x',y='y'),geom='bar', stat='identity')
> >
> > RangeY <-c(0,1)
> > YBreaks <- (0:10)*diff(RangeY)/10
> > YTickLabels<- as.character(YBreaks)
> >
> > plot2 <- plot1 + scale_y_continuous(limits=RangeY, breaks=YBreaks,
> > labels=YTickLabels, expand=c(0,0))
> > print(plot2)
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>--
>http://had.co.nz/


From jones3745 at verizon.net  Tue Dec 11 23:13:10 2007
From: jones3745 at verizon.net (Thomas L Jones, PhD)
Date: Tue, 11 Dec 2007 17:13:10 -0500
Subject: [R] Writing a file to the disk
Message-ID: <000301c83c43$04859d80$2f01a8c0@dell2400>

I am having difficulty writing the code for the following operation:
I have a numeric vector pred_out of length 156. (N = 156). Under Windows XP, 
I need to write it to the disk in text format. Perhaps some kind soul would 
provide the code fragment. The file name is sheet_vec.txt. Please see [1] 
for the complete pathname.

Each number to be written out has the following format:

LL.RRRRRR where LL is the integral part and RRRRRR is the fractional part to 
six decimal places. Example: "10.226207" The integral part, LL, is a maximum 
of two digits long. All numbers are positive.

Another hypothetical example:

10.226207
 6.556988
 4.395678
etc. Each number is to be on a separate line. Please correct me if I am 
wrong: A \n (newline) character ends each line.

After writing the file, it can be closed but not destroyed. (What I plan to 
do next is copy and paste the pred_out data into a spreadsheet.) Compressed 
formats (zip, etc.) should be avoided if possible.

Your ideas?

Tom Jones

[1] The full Windows pathname is:

C:/Documents and Settings/Tom/My Documents/Election Day Study Nov 7, 06/
  > sheet_vec.txt

Sorry, but the pathname is too long to fit on one line.


From murdoch at stats.uwo.ca  Tue Dec 11 23:25:10 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 11 Dec 2007 17:25:10 -0500
Subject: [R] Writing a file to the disk
In-Reply-To: <000301c83c43$04859d80$2f01a8c0@dell2400>
References: <000301c83c43$04859d80$2f01a8c0@dell2400>
Message-ID: <475F0E46.1030607@stats.uwo.ca>

On 11/12/2007 5:13 PM, Thomas L Jones, PhD wrote:
> I am having difficulty writing the code for the following operation:
> I have a numeric vector pred_out of length 156. (N = 156). Under Windows XP, 
> I need to write it to the disk in text format. Perhaps some kind soul would 
> provide the code fragment. The file name is sheet_vec.txt. Please see [1] 
> for the complete pathname.
> 
> Each number to be written out has the following format:
> 
> LL.RRRRRR where LL is the integral part and RRRRRR is the fractional part to 
> six decimal places. Example: "10.226207" The integral part, LL, is a maximum 
> of two digits long. All numbers are positive.
> 
> Another hypothetical example:
> 
> 10.226207
>  6.556988
>  4.395678
> etc. Each number is to be on a separate line. Please correct me if I am 
> wrong: A \n (newline) character ends each line.
> 
> After writing the file, it can be closed but not destroyed. (What I plan to 
> do next is copy and paste the pred_out data into a spreadsheet.) Compressed 
> formats (zip, etc.) should be avoided if possible.
> 
> Your ideas?
> 
> Tom Jones
> 
> [1] The full Windows pathname is:
> 
> C:/Documents and Settings/Tom/My Documents/Election Day Study Nov 7, 06/
>   > sheet_vec.txt
> 
> Sorry, but the pathname is too long to fit on one line.

Get the formatted numbers you want in a character vector, then use 
writeLines to write it out.  For example:

x <- rnorm(20)
text <- sprintf("%9.6f", x)
writeLines(text, file.choose())

This will open a file selection directory where you can navigate to the 
correct folder and enter the filename.  Alternatively, you could put the 
whole path to the file in place of file.choose().  As a third choice, 
you could just use "clipboard"; on Windows, this will write the text 
directly into the clipboard, and you can paste from there to your 
spreadsheet.

Duncan Murdoch


From ugurozdemir at yahoo.com  Wed Dec 12 00:44:53 2007
From: ugurozdemir at yahoo.com (Ugur Ozdemir)
Date: Tue, 11 Dec 2007 15:44:53 -0800 (PST)
Subject: [R] Using the same y-axis range for each plot in "plot.ts"
Message-ID: <585429.40775.qm@web60711.mail.yahoo.com>

Hi,

I would like to plot multiple time-series plots
separately in the same window but I would like to have
the same y-axis range for all of the plots for the
sake of comparison. I am pretty sure there is a quick
way to do it in plot.ts which I could not find.

Any help is very much appreciated.

Ugur



Microsoft gives you windows, Linux gives you the whole house.


      ____________________________________________________________________________________
Be a better friend, newshound, and


From ggrothendieck at gmail.com  Wed Dec 12 00:57:52 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 11 Dec 2007 18:57:52 -0500
Subject: [R] Using the same y-axis range for each plot in "plot.ts"
In-Reply-To: <585429.40775.qm@web60711.mail.yahoo.com>
References: <585429.40775.qm@web60711.mail.yahoo.com>
Message-ID: <971536df0712111557n2902d0e6k47c1605a443ad3b6@mail.gmail.com>

Try using plot.zoo from the zoo package.  This plots them
all within the range 3000 to 5000:

library(zoo)
plot(as.zoo(EuStockMarkets), ylim = c(3000, 5000))


On Dec 11, 2007 6:44 PM, Ugur Ozdemir <ugurozdemir at yahoo.com> wrote:
> Hi,
>
> I would like to plot multiple time-series plots
> separately in the same window but I would like to have
> the same y-axis range for all of the plots for the
> sake of comparison. I am pretty sure there is a quick
> way to do it in plot.ts which I could not find.
>
> Any help is very much appreciated.
>
> Ugur
>
>
>
> Microsoft gives you windows, Linux gives you the whole house.
>
>
>      ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From milheiros at gmail.com  Wed Dec 12 01:43:32 2007
From: milheiros at gmail.com (Filipe Almeida)
Date: Wed, 12 Dec 2007 00:43:32 +0000
Subject: [R] tm package - how to transform a TermDocMatrix to a data.frame
Message-ID: <1c285b0b0712111643y1f88a8a3if331baf46cd385ca@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/2483b4bc/attachment.pl 

From jzhang1982 at gmail.com  Wed Dec 12 02:00:48 2007
From: jzhang1982 at gmail.com (Jian Zhang)
Date: Wed, 12 Dec 2007 09:00:48 +0800
Subject: [R] How to plot the grid figure using R?
Message-ID: <200712120900460319130@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/85edf552/attachment.pl 

From milton_ruser at yahoo.com.br  Wed Dec 12 02:33:39 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Tue, 11 Dec 2007 17:33:39 -0800 (PST)
Subject: [R] Markov Chain Monte Carlo (MCMR) in R for spatial simulation.
Message-ID: <633896.55815.qm@web56006.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071211/5ac8e2d0/attachment.pl 

From mxkuhn at gmail.com  Wed Dec 12 02:37:48 2007
From: mxkuhn at gmail.com (Max Kuhn)
Date: Tue, 11 Dec 2007 20:37:48 -0500
Subject: [R] =?iso-8859-1?q?postResample_R=B2_and_lm=28=29_R=B2?=
In-Reply-To: <bc1b93170712111235s6292de4bva3121ff82916f3d3@mail.gmail.com>
References: <bc1b93170712111235s6292de4bva3121ff82916f3d3@mail.gmail.com>
Message-ID: <6731304c0712111737i482c9ed6w2bb5af7023b18116@mail.gmail.com>

On Dec 11, 2007 3:35 PM, Giovane <gufrgs at gmail.com> wrote:

>
> So here comes my doubt: why do I have an value of 67.52% for R? when
> creating the model(that is , the model explains 67.52% of the data) and
> when I use this same model on the same input data, why does postResample
> return a very different value associated to R??
>

Let's get in the WayBack machine and return to 4 days ago when I said:

> As has been previously noted on this list, there are a number of
> formulas for R-squared. This function uses the square of the
> correlation between the observed and predicted. The next version of
> caret will offer a choice of formulas.

For your data:

> cor(prediction, input$TOTAL)^2
[1] 0.3300378

For R-squared, summary.lm uses

   ans$r.squared <- mss/(mss + rss)
   ans$adj.r.squared <- 1 - (1 - ans$r.squared) * ((n - df.int)/rdf)

and for your data rdf = 31, df.int = 0 and n = 35.

In other words, the Rsquared estimate form summary.lm adjusts for the
degrees of freedom and postResample does not.

Why doesn't it use the df? In ?postResample you would see

"Note that many models have more predictors (or parameters) than data
points, so the typical mean squared error denominator (n - p) does not
apply. Root mean squared error is calculated using sqrt(mean((pred -
obs)^2)). Also, R-squared is calculated as the square of the
correlation between the observed and predicted outcomes."

Since caret is useful for comparing different types of models, we use
biased estimate of the root MSE since we would like to directly
compare the RMSE from different models (say a linear regression and a
support vector machine). Many of these models do not have an explicit
number of parameters, so we use

   mse <- mean((pred - obs)^2)


Max


From vera333uk at yahoo.co.uk  Wed Dec 12 03:29:55 2007
From: vera333uk at yahoo.co.uk (NikTuz)
Date: Tue, 11 Dec 2007 18:29:55 -0800 (PST)
Subject: [R]  Efron's locfdr package - a component missing
Message-ID: <14287868.post@talk.nabble.com>


Hello:

Could you possibly help me.
In Efron's 2004 paper "Selection and Estimation ..."
it was mentioned that so-called effect density estimate, denoted by g1(mu)
was included in the locfdr package. However, I can't find it in the
description
of the package. Any suggestions?

Sincerely,
Nik
-- 
View this message in context: http://www.nabble.com/Efron%27s-locfdr-package---a-component-missing-tp14287868p14287868.html
Sent from the R help mailing list archive at Nabble.com.


From milton_ruser at yahoo.com.br  Wed Dec 12 03:32:11 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Tue, 11 Dec 2007 18:32:11 -0800 (PST)
Subject: [R] Enc: Res:  How to plot the grid figure using R?
Message-ID: <722508.89740.qm@web56009.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071211/b7cba539/attachment.pl 

From edgar.merkle at wichita.edu  Wed Dec 12 04:09:13 2007
From: edgar.merkle at wichita.edu (Ed Merkle)
Date: Tue, 11 Dec 2007 21:09:13 -0600
Subject: [R] Sys.setlocale() and text()
Message-ID: <475F50D9.20003@wichita.edu>

Dear HelpeRs,

I have a question about the Sys.setlocale() command and plotting.  I am 
running Windows XP, with R 2.6.1.  My default locale is English_United 
States.1252.

I am trying to add a lowercase sigma to a plot using the following code:

Sys.setlocale("LC_CTYPE","greek")
plot(1:10,1:10)
text(4,3,"\xF3")


For R 2.6.1, this code gives me the glyph from my default (1252) instead 
of from the 1253 codes.  For an older version of R (2.3.0) on the same 
computer, this code gives me the lowercase sigma that I wanted.  I have 
been unable to pinpoint what has changed.  Thanks for the help, and I 
apologize if I am missing something obvious.


-- 
Ed Merkle, PhD
Assistant Professor
Dept. of Psychology
Wichita State University
Wichita, KS 67260


From wgavioli at fas.harvard.edu  Wed Dec 12 04:15:29 2007
From: wgavioli at fas.harvard.edu (Wayne Aldo Gavioli)
Date: Tue, 11 Dec 2007 22:15:29 -0500
Subject: [R] Importing Large Dataset into Excel
Message-ID: <1197429329.475f525155179@webmail.fas.harvard.edu>


Hello all,


I seem to be having a problem importing a data set from Excel into R.  I'm using
the "read.table" command to import the data with the following line of code:

> newborn<-read.table("newborn edit.csv", header=T, sep=",")


where "newborn edit.csv" is the name of the file.  Unfortunately, I'm getting
back the following error message:


"Error in scan(file,, what, nmax, sep, dc, quote, skip, nlines, na.string, :
line 528 did not have 44 elements"


As far as I can tell, line 528 of the spreadsheet table does have the same
number of elements as the other rows - by chance can this error message mean
anything else?  Also, is there an easier way to import data from R into Excel
using a single line of R code?


Thanks,


Wayne


From vera333uk at yahoo.co.uk  Wed Dec 12 04:17:57 2007
From: vera333uk at yahoo.co.uk (NikTuz)
Date: Tue, 11 Dec 2007 19:17:57 -0800 (PST)
Subject: [R]  locfdr VS fdrtool
Message-ID: <14288241.post@talk.nabble.com>



Hello:

Could someone share the info about how these two packages compare
to each other. I take it fdrtool is supposed to be "better", but I'm not
sure
if it is so.

Nik
-- 
View this message in context: http://www.nabble.com/locfdr-VS-fdrtool-tp14288241p14288241.html
Sent from the R help mailing list archive at Nabble.com.


From wgavioli at fas.harvard.edu  Wed Dec 12 04:18:10 2007
From: wgavioli at fas.harvard.edu (Wayne Aldo Gavioli)
Date: Tue, 11 Dec 2007 22:18:10 -0500
Subject: [R] Importing Large Dataset into Excel
In-Reply-To: <1197429329.475f525155179@webmail.fas.harvard.edu>
References: <1197429329.475f525155179@webmail.fas.harvard.edu>
Message-ID: <1197429490.475f52f2c929f@webmail.fas.harvard.edu>

Sorry, the title of this should read "From Excel into R".


Quoting Wayne Aldo Gavioli <wgavioli at fas.harvard.edu>:

>
> Hello all,
>
>
> I seem to be having a problem importing a data set from Excel into R.  I'm
> using
> the "read.table" command to import the data with the following line of code:
>
> > newborn<-read.table("newborn edit.csv", header=T, sep=",")
>
>
> where "newborn edit.csv" is the name of the file.  Unfortunately, I'm getting
> back the following error message:
>
>
> "Error in scan(file,, what, nmax, sep, dc, quote, skip, nlines, na.string, :
> line 528 did not have 44 elements"
>
>
> As far as I can tell, line 528 of the spreadsheet table does have the same
> number of elements as the other rows - by chance can this error message mean
> anything else?  Also, is there an easier way to import data from R into Excel
> using a single line of R code?
>
>
> Thanks,
>
>
> Wayne
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Wed Dec 12 04:24:35 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 11 Dec 2007 19:24:35 -0800
Subject: [R] Importing Large Dataset into Excel
In-Reply-To: <1197429329.475f525155179@webmail.fas.harvard.edu>
References: <1197429329.475f525155179@webmail.fas.harvard.edu>
Message-ID: <644e1f320712111924y17a0d367t6876a49c53010c7e@mail.gmail.com>

?count.fields

count.fields will tell you how many items are in each line.  As you
said, they should all be the same, but this will confirm it.

field.count <- count.fields("newborn edit.csv", sep=",")
table(field.count)  # determine count of the fields on a line

On Dec 11, 2007 7:15 PM, Wayne Aldo Gavioli <wgavioli at fas.harvard.edu> wrote:
>
> Hello all,
>
>
> I seem to be having a problem importing a data set from Excel into R.  I'm using
> the "read.table" command to import the data with the following line of code:
>
> > newborn<-read.table("newborn edit.csv", header=T, sep=",")
>
>
> where "newborn edit.csv" is the name of the file.  Unfortunately, I'm getting
> back the following error message:
>
>
> "Error in scan(file,, what, nmax, sep, dc, quote, skip, nlines, na.string, :
> line 528 did not have 44 elements"
>
>
> As far as I can tell, line 528 of the spreadsheet table does have the same
> number of elements as the other rows - by chance can this error message mean
> anything else?  Also, is there an easier way to import data from R into Excel
> using a single line of R code?
>
>
> Thanks,
>
>
> Wayne
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From morphwj at comcast.net  Wed Dec 12 04:59:06 2007
From: morphwj at comcast.net (morphwj at comcast.net)
Date: Wed, 12 Dec 2007 03:59:06 +0000
Subject: [R] Install from Local Zip
Message-ID: <121220070359.6698.475F5C8A00077B3600001A2A22007374780699089F9D0103@comcast.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/7d3d894d/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Dec 12 05:20:11 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Dec 2007 04:20:11 +0000 (GMT)
Subject: [R] Install from Local Zip
In-Reply-To: <121220070359.6698.475F5C8A00077B3600001A2A22007374780699089F9D0103@comcast.net>
References: <121220070359.6698.475F5C8A00077B3600001A2A22007374780699089F9D0103@comcast.net>
Message-ID: <Pine.LNX.4.64.0712120418380.1098@gannet.stats.ox.ac.uk>

On Wed, 12 Dec 2007, morphwj at comcast.net wrote:

> To install from a local zip file, install.packages option repos=NULL. 
> Then, how can dependencies of the local zip file, which are located in 
> the repository of contributor packages, be automatically installed?

They cannot.  You need a repository to know about dependencies, which are 
in the PACKAGES file.

> Or, is there a better way than to tell the person installing from local 
> zip what the dependent files are, and to install them from the 
> repository first?

Yes, create a local repository and use a file:// URL to access it. 
Details are in the R-admin manual (and in the archives of this list or 
R-devel).


> Bill Morphet
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mwkimpel at gmail.com  Wed Dec 12 05:32:47 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Tue, 11 Dec 2007 23:32:47 -0500
Subject: [R] 00LOCK error with site-library
In-Reply-To: <6phbq8yt6sa.fsf@gopher4.fhcrc.org>
References: <475DB2F8.1000108@gmail.com>	<475DC2B9.1060609@bank-banque-canada.ca>
	<475DD195.9030200@gmail.com> <6phbq8yt6sa.fsf@gopher4.fhcrc.org>
Message-ID: <475F646F.3060005@gmail.com>

I'm still having problems with this 00LOCK error. Tonight I recompiled 
R-devel from scratch, created a new site-library directory after moving 
the old one to scratch storage, and tried reinstalling my packages. 13 
out of 290 packages successfully reinstalled and, once again, 00LOCK has 
appeared.

I tried to hack around this by installing packages one at a time, each 
time looking for 00LOCK and, if present, unlinking it recursively. This 
fails, although I can unlink other packages with identical permissions 
from within R. Outside of R, in the bash shell, I can rm -r 00LOCK 
without any problems. What follows is just some output demonstrating this.

Why does unlink not work for this one particular directory?
Mark

  x <- unlink("00LOCK", recursive = TRUE)
 > x
[1] 1 # unlink fails
 > dir()
  [1] "00LOCK"                  "acepack"
  [3] "affxparser"              "affyio"
  [5] "AffymetrixDataTestFiles" "akima"
  [7] "amap"                    "aws"
  [9] "Biobase"                 "hapmap100kxba"
[11] "kernlab"                 "plasmodiumanophelescdf"
[13] "R.css"                   "som"
[15] "stjudem"                 "xlahomology"
 > ?unlink
 > y <- unlink(x = "00LOCK", recursive = TRUE)
 > y
[1] 1
 > y <- unlink(x = "acepack", recursive = TRUE)
 > y
[1] 0 #u unlink succeeds

 From the command line:

ls -al

drwxr-xr-x   3 mkimpel psych 4096 2007-12-11 23:13 00LOCK
drwxr-xr-x  13 mkimpel psych 4096 2007-12-11 23:12 affxparser
drwxr-xr-x  10 mkimpel psych 4096 2007-12-11 23:12 affyio



Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com


From ggrothendieck at gmail.com  Wed Dec 12 06:46:08 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Dec 2007 00:46:08 -0500
Subject: [R] Sys.setlocale() and text()
In-Reply-To: <475F50D9.20003@wichita.edu>
References: <475F50D9.20003@wichita.edu>
Message-ID: <971536df0712112146m1905d070x23b34975ed8d944c@mail.gmail.com>

Try this:

plot(1:10, main = quote(sigma ^ 2))


On Dec 11, 2007 10:09 PM, Ed Merkle <edgar.merkle at wichita.edu> wrote:
> Dear HelpeRs,
>
> I have a question about the Sys.setlocale() command and plotting.  I am
> running Windows XP, with R 2.6.1.  My default locale is English_United
> States.1252.
>
> I am trying to add a lowercase sigma to a plot using the following code:
>
> Sys.setlocale("LC_CTYPE","greek")
> plot(1:10,1:10)
> text(4,3,"\xF3")
>
>
> For R 2.6.1, this code gives me the glyph from my default (1252) instead
> of from the 1253 codes.  For an older version of R (2.3.0) on the same
> computer, this code gives me the lowercase sigma that I wanted.  I have
> been unable to pinpoint what has changed.  Thanks for the help, and I
> apologize if I am missing something obvious.
>
>
> --
> Ed Merkle, PhD
> Assistant Professor
> Dept. of Psychology
> Wichita State University
> Wichita, KS 67260
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From vera333uk at yahoo.co.uk  Wed Dec 12 06:46:24 2007
From: vera333uk at yahoo.co.uk (NikTuz)
Date: Tue, 11 Dec 2007 21:46:24 -0800 (PST)
Subject: [R] Efron's locfdr package - a component missing
In-Reply-To: <14287868.post@talk.nabble.com>
References: <14287868.post@talk.nabble.com>
Message-ID: <14289411.post@talk.nabble.com>



 I got in touch with Prof. Efron himself and he said that that part indeed
was not
included in the package, so there's nothing to be done.
-- 
View this message in context: http://www.nabble.com/Efron%27s-locfdr-package---a-component-missing-tp14287868p14289411.html
Sent from the R help mailing list archive at Nabble.com.


From landronimirc at gmail.com  Wed Dec 12 06:59:29 2007
From: landronimirc at gmail.com (Liviu Andronic)
Date: Wed, 12 Dec 2007 06:59:29 +0100
Subject: [R] Markov Chain Monte Carlo (MCMR) in R for spatial simulation.
In-Reply-To: <633896.55815.qm@web56006.mail.re3.yahoo.com>
References: <633896.55815.qm@web56006.mail.re3.yahoo.com>
Message-ID: <68b1e2610712112159x124b964fp221f9a9c628c09c7@mail.gmail.com>

On 12/12/07, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
> Dear all,
>
> I would like to know if is there some MCMR capability (functions) develped to improve spatial explicit landscape simulations in R.

Check the CRAN "Spatial" View [1]. Also search on this page [2] for "markov".

Liviu

[1] http://cran.at.r-project.org/src/contrib/Views/
[2] http://cran.at.r-project.org/src/contrib/PACKAGES.html


From chs23 at student.open.ac.uk  Wed Dec 12 07:45:54 2007
From: chs23 at student.open.ac.uk (Chris.H. Snow)
Date: Wed, 12 Dec 2007 06:45:54 GMT
Subject: [R] Book: The New S Language
Message-ID: <fc.004c5401060233fd004c5401060233fd.6023447@oufcnt2.open.ac.uk>

I'm struggling a bit with R with understanding functions and what's going
on under the hood.

For example, I was given this snippet of code via this mailing list:

DF <- data.frame(A =c("A", "A", "A", "B", "C"), B=c(1,1,2,2,0))
g <- paste(DF$A, DF$B)
s <- split(DF, g)

split appears to be taken the string provided by paste, e.g. "A 1", and
then deciding that the first character "A" in this string relates to DF$A,
and the second character "1" to DF$B.

The split help pages don't really tell me why split is working like this -
maybe I'm just looking in the wrong place in the help page.

I've got plenty of books on R, but only one of them gives a bit of info on
the use of split - not in enough detail though.  Does the book referenced
in the help pages ("The New S Language") give more insight into
this and other functions?

Will I just get an Aha! moment with R after using it for a while?

Thanks in advance...

Chris


From petr.pikal at precheza.cz  Wed Dec 12 08:17:50 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Wed, 12 Dec 2007 08:17:50 +0100
Subject: [R] Odp:  Book: The New S Language
In-Reply-To: <fc.004c5401060233fd004c5401060233fd.6023447@oufcnt2.open.ac.uk>
Message-ID: <OF67D54944.FCB09551-ONC12573AF.0026B0E1-C12573AF.00280691@precheza.cz>

Hi

r-help-bounces at r-project.org napsal dne 12.12.2007 07:45:54:

> I'm struggling a bit with R with understanding functions and what's 
going
> on under the hood.
> 
> For example, I was given this snippet of code via this mailing list:
> 
> DF <- data.frame(A =c("A", "A", "A", "B", "C"), B=c(1,1,2,2,0))
> g <- paste(DF$A, DF$B)
> s <- split(DF, g)
> 
> split appears to be taken the string provided by paste, e.g. "A 1", and
> then deciding that the first character "A" in this string relates to 
DF$A,
> and the second character "1" to DF$B.
> 
> The split help pages don't really tell me why split is working like this 
-
> maybe I'm just looking in the wrong place in the help page.

Maybe you misunderstand what split does:

> DF <- data.frame(A =c("A", "A", "A", "B", "C"), B=c(1,1,2,2,0))
a data frame with 2 columns

> g <- paste(DF$A, DF$B)

a character vector (which is internally turned into factor). And split 
splits your data frame DF into groups according to levels of "g". It does 
not at all lookup into columns of your data frame (and the help page does 
not even give you a suspicion that it shall behave like that).

first sentence in help page tells you
split divides the data in the vector x into the groups defined by f.

compare your result with
> g.f<-factor(g)
> str(g.f)
 Factor w/ 4 levels "A 1","A 2","B 2",..: 1 1 2 3 4
> g.n<-as.numeric(g.f)
> g.n
[1] 1 1 2 3 4

split(DF, g.f)
split(DF, g.n)

And try to explain with your idea how split works this
DF$A<-rnorm(5)
s <- split(DF, g)

Regards
Petr

> 
> I've got plenty of books on R, but only one of them gives a bit of info 
on
> the use of split - not in enough detail though.  Does the book 
referenced
> in the help pages ("The New S Language") give more insight into
> this and other functions?
> 
> Will I just get an Aha! moment with R after using it for a while?
> 
> Thanks in advance...
> 
> Chris
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ptit_bleu at yahoo.fr  Wed Dec 12 08:36:00 2007
From: ptit_bleu at yahoo.fr (Ptit_Bleu)
Date: Tue, 11 Dec 2007 23:36:00 -0800 (PST)
Subject: [R] Sweave : change value in rnw file to generate multiple
 "single" reports ?
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBBD628D2@LP-EXCHVS07.CO.IHC.COM>
References: <14256204.post@talk.nabble.com>
	<475D6EC6.6080405@statistik.uni-dortmund.de>
	<14275809.post@talk.nabble.com>
	<07E228A5BE53C24CAD490193A7381BBBD628D2@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <14290306.post@talk.nabble.com>




Greg wrote :
Hope this helps,

Sure it will !
Thanks again,
Have a nice day,
Ptit Bleu.
-- 
View this message in context: http://www.nabble.com/Sweave-%3A-change-value-in-rnw-file-to-generate-multiple-%22single%22-reports---tp14256204p14290306.html
Sent from the R help mailing list archive at Nabble.com.


From phgrosjean at sciviews.org  Wed Dec 12 09:29:20 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 12 Dec 2007 09:29:20 +0100
Subject: [R] Importing Large Dataset into Excel
In-Reply-To: <644e1f320712111924y17a0d367t6876a49c53010c7e@mail.gmail.com>
References: <1197429329.475f525155179@webmail.fas.harvard.edu>
	<644e1f320712111924y17a0d367t6876a49c53010c7e@mail.gmail.com>
Message-ID: <475F9BE0.6030503@sciviews.org>

The problem is often a misspecification of the comment.char argument. 
For read.table(), it defaults to '#'. This means that everywhere you 
have a '#' char in your Excel sheet, the rest of the line is ignored. 
This results in a different number of items per line.

You should better use read.csv() which provides better default arguments 
for your particular problem.
Best,

Philippe Grosjean


jim holtman wrote:
> ?count.fields
> 
> count.fields will tell you how many items are in each line.  As you
> said, they should all be the same, but this will confirm it.
> 
> field.count <- count.fields("newborn edit.csv", sep=",")
> table(field.count)  # determine count of the fields on a line
> 
> On Dec 11, 2007 7:15 PM, Wayne Aldo Gavioli <wgavioli at fas.harvard.edu> wrote:
>> Hello all,
>>
>>
>> I seem to be having a problem importing a data set from Excel into R.  I'm using
>> the "read.table" command to import the data with the following line of code:
>>
>>> newborn<-read.table("newborn edit.csv", header=T, sep=",")
>>
>> where "newborn edit.csv" is the name of the file.  Unfortunately, I'm getting
>> back the following error message:
>>
>>
>> "Error in scan(file,, what, nmax, sep, dc, quote, skip, nlines, na.string, :
>> line 528 did not have 44 elements"
>>
>>
>> As far as I can tell, line 528 of the spreadsheet table does have the same
>> number of elements as the other rows - by chance can this error message mean
>> anything else?  Also, is there an easier way to import data from R into Excel
>> using a single line of R code?
>>
>>
>> Thanks,
>>
>>
>> Wayne
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
>


From ripley at stats.ox.ac.uk  Wed Dec 12 09:47:46 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Dec 2007 08:47:46 +0000 (GMT)
Subject: [R] Importing Large Dataset into Excel
In-Reply-To: <475F9BE0.6030503@sciviews.org>
References: <1197429329.475f525155179@webmail.fas.harvard.edu>
	<644e1f320712111924y17a0d367t6876a49c53010c7e@mail.gmail.com>
	<475F9BE0.6030503@sciviews.org>
Message-ID: <Pine.LNX.4.64.0712120841180.11612@gannet.stats.ox.ac.uk>

I would say that the issue is more often the character ', which is allowed 
as a quote in read.table and not in read.csv.

As for

>>> Also, is there an easier way to import data from R into Excel
>>> using a single line of R code?

I think it means import from Excel into R, and there are several simpler 
ways described in the 'R Data Import/Export manual'.


On Wed, 12 Dec 2007, Philippe Grosjean wrote:

> The problem is often a misspecification of the comment.char argument.
> For read.table(), it defaults to '#'. This means that everywhere you
> have a '#' char in your Excel sheet, the rest of the line is ignored.
> This results in a different number of items per line.
>
> You should better use read.csv() which provides better default arguments
> for your particular problem.
> Best,
>
> Philippe Grosjean
>
>
> jim holtman wrote:
>> ?count.fields
>>
>> count.fields will tell you how many items are in each line.  As you
>> said, they should all be the same, but this will confirm it.
>>
>> field.count <- count.fields("newborn edit.csv", sep=",")
>> table(field.count)  # determine count of the fields on a line
>>
>> On Dec 11, 2007 7:15 PM, Wayne Aldo Gavioli <wgavioli at fas.harvard.edu> wrote:
>>> Hello all,
>>>
>>>
>>> I seem to be having a problem importing a data set from Excel into R.  I'm using
>>> the "read.table" command to import the data with the following line of code:
>>>
>>>> newborn<-read.table("newborn edit.csv", header=T, sep=",")
>>>
>>> where "newborn edit.csv" is the name of the file.  Unfortunately, I'm getting
>>> back the following error message:
>>>
>>>
>>> "Error in scan(file,, what, nmax, sep, dc, quote, skip, nlines, na.string, :
>>> line 528 did not have 44 elements"
>>>
>>>
>>> As far as I can tell, line 528 of the spreadsheet table does have the same
>>> number of elements as the other rows - by chance can this error message mean
>>> anything else?  Also, is there an easier way to import data from R into Excel
>>> using a single line of R code?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Roger.Bivand at nhh.no  Wed Dec 12 10:04:46 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 12 Dec 2007 10:04:46 +0100 (CET)
Subject: [R] [R-sig-Geo] Markov Chain Monte Carlo (MCMR) in R for
 spatial simulation.
In-Reply-To: <68b1e2610712112159x124b964fp221f9a9c628c09c7@mail.gmail.com>
References: <633896.55815.qm@web56006.mail.re3.yahoo.com>
	<68b1e2610712112159x124b964fp221f9a9c628c09c7@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0712121002200.29680@reclus.nhh.no>

On Wed, 12 Dec 2007, Liviu Andronic wrote:

> On 12/12/07, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
>> Dear all,
>>
>> I would like to know if is there some MCMR capability (functions) 
>> develped to improve spatial explicit landscape simulations in R.
>
> Check the CRAN "Spatial" View [1]. Also search on this page [2] for 
> "markov".
>

Checking the task views is always good advice. A bit more detail on what 
"spatial explicit landscape simulations" are would help - can you use 
RandomFields, perhaps not? Can you use mechansisms in MCMCpack - see the 
Bayesian task view as well as the spatial one?

Roger

> Liviu
>
> [1] http://cran.at.r-project.org/src/contrib/Views/
> [2] http://cran.at.r-project.org/src/contrib/PACKAGES.html
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ethanhaaswaswrong at gmail.com  Wed Dec 12 10:26:39 2007
From: ethanhaaswaswrong at gmail.com (ethanhaas)
Date: Wed, 12 Dec 2007 01:26:39 -0800 (PST)
Subject: [R] Getting error message using R: Please help (it's coursework due
 in on Friday)
Message-ID: <14291568.post@talk.nabble.com>


I've been trying for the past 3 weeks to use R (much better than Matlab but I
am very bad with computers so I am very new to all of this) and know how to
input the data (hey, it's a start!) but every time I type in the following:

dcm <- decompose(information)

I get the following error message:

Error in decompose(information) : time series has no or less than 3 periods

Could you please aid me so that I can progress to the next step. What I am
in fact trying to do at the moment is plot a graph of the information so I
had assumed it would be:

dcm <- decompose(information)
Plot(dcm$trend)

but obviously it does not appear to be the case.

By the way the code I have right now in case it helps is:

information <- scan("everything.txt")
timeseries <- ts(information, frequency = 11, start = c(1970, 1994))
using <- window(timeseries, end = c(1990, 11))
dcm <- decompose(information)
Plot(dcm$trend)


Thank you very much in advance, I really appreciate the help.
-- 
View this message in context: http://www.nabble.com/Getting-error-message-using-R%3A-Please-help-%28it%27s-coursework-due-in-on-Friday%29-tp14291568p14291568.html
Sent from the R help mailing list archive at Nabble.com.


From Roger.Bivand at nhh.no  Wed Dec 12 10:30:15 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 12 Dec 2007 10:30:15 +0100 (CET)
Subject: [R] [R-sig-Geo] How to plot the grid figure using R?
In-Reply-To: <200712120900460319130@gmail.com>
References: <200712120900460319130@gmail.com>
Message-ID: <Pine.LNX.4.64.0712121007330.29680@reclus.nhh.no>

On Wed, 12 Dec 2007, Jian Zhang wrote:

> Now I have the forest plot data with x, y locations, and I measured the DBH for every indivicuals.
>
> The data looks like that:
> x=runif(100,0,100)
> y=runif(100,0,100)
> dbh=runif(100,1,100)
> rdata=data.frame(x,y,dbh)
>> rdata[1:5,]
>           x         y      dbh
> 1 99.5354145  1.412844 34.10112
> 2  0.8259361 87.737036 39.12914
> 3  6.5678613 65.699032 22.55990
> 4 67.2987881 72.053877 45.83978
> 5  2.2491372 23.622898 68.77065
>
> My question is: How can I plot the grid (25??25,5??5,...) figure by DBH? 
> Can you introduce a simply method to do it by R language? I know that it 
> can be done very easy by the software ArcGis.

Yes, but these are vector data, a DBH value at each point. To plot a grid 
(25 by 25, ...), you must assign the values to a grid, and there are lots 
of ways of doing that. Continuing the answer by Thierry Onkelinx:

set.seed(1)
x=runif(100,0,100)
y=runif(100,0,100)
dbh=runif(100,1,100)
rdata=data.frame(x,y,dbh)

library(sp)
coordinates(rdata) <- ~ x + y

grd <- GridTopology(c(2,2), c(4,4), c(25,25))
SP <- as(SpatialGrid(grd), "SpatialPixels")
whre <- overlay(SP, rdata)
SP_rdata <- SpatialPixelsDataFrame(SP, data=data.frame(dbh=rep(NA,
   25*25)))
SP_rdata$dbh[whre] <- rdata$dbh
# this takes the last dbh value is more than one falls in each grid cell
# so a bit more work is needed here
pts <- list("sp.points", rdata)
spplot(SP_rdata, "dbh", sp.layout=list(pts))

Or interpolate, and mask out empty cells:

library(gstat)
SP_rdata2 <- idw(dbh ~ 1, rdata, newdata=SP, idp=2.0, nmax=4)
is.na(SP_rdata2$var1.pred) <- !(1:625 %in% whre)
spplot(SP_rdata2, "var1.pred", sp.layout=list(pts))

for example.

Roger

>
> ????????
> ????????
> ????????
> ????????
>
>
> Thanks very much.
>
 	[[alternative HTML version deleted]]

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

From Bernhard_Pfaff at fra.invesco.com  Wed Dec 12 10:53:02 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Wed, 12 Dec 2007 09:53:02 -0000
Subject: [R] question regarding arima function and predicted values
In-Reply-To: <68174.42487.qm@web38612.mail.mud.yahoo.com>
References: <68174.42487.qm@web38612.mail.mud.yahoo.com>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C30CF5511D@GBHENXMB02.corp.amvescap.net>


>Good evening!
>
>I have a question regarding  forecast package and time series analysis.
>My syntax:
>
>x<-c(253, 252, 275, 275, 272, 254, 272, 252, 249, 300, 244, 
>258, 255, 285, 301, 278, 279, 304, 275, 276, 313, 292, 302, 
>322, 281, 298, 305, 295, 286, 327, 286, 270, 289, 293, 287, 
>267, 267, 288, 304, 273, 264, 254, 263, 265, 278)
>library(forecast)
>arima(x, order=c(1,1,2), seasonal=list(order=c(0,1,0), period=12))->l
>auto.arima(x)->k
>sd(l$resid)
>sd(k$resid)
>predict(l,n.ahead=1)
>predict(k,n.ahead=1)
>
>1. I understand that auto.arima will find the best time series 
>model choosing the smaller AIC, BIC and AICc from competing 
>models, but my model finds a smaller AIC than that of the 
>auto.arima. but the sd of the residuals for my model is 
>somehow bigger. 
>Why? Am I missing something? 
>Now the sd of the residuals for my model is somehow bigger, as 
>well as the se for the predicted value.  What model would you 
>choose between this two and why?   
>

Hello Eugen,

in a nutshell, I would not use neither of these models, but an ARMA(1,
0, 1) fitted to the log(x). Now, to your questions. If you use the
"trace = TRUE" argument in auto.arima(), you will see that your model
specification (l) is not tested. Why is this? Because, you supply a
vector and the frequency is 1 (i.e. frequency(x). If you now spot at the
code in auto.arima() it is clear that seasonal differences are not
tested for. 

Try this instead:

x <- ts(x, frequency = 12)
k <- auto.arima(x, D = 1, trace = TRUE)
logLik(k)
k$aic

Hence, this yields an ARIMA(1, 0, 1)(2, 1, 0)[12] as an "optimal" model
specification, which yields an even "better" result than your l model.
However, the results you report for l and k can be attributed to
over-fitting / over-differencing. If you examine your series more
closely:

plot(x)
acf(x)
pacf(x)
library(urca)
ur.kpss(x)
plot(ur.za(x))

i.e. the traditional approach for the identification stage in the
Box-Jenkins approach, you will detect, that
1) The series seems not to be stationary with respect to its variance,
but is not "trending".
2) ACF and PACF tapers off slowly and neither has a single spike nor
gives the PACF hindsight of seasonality.
3) Your series is stationary with a structural break.


Therefore, one can use the log-transform of x for variance stabilisation
and specify an ARMA(1, 0, 1)-model:

xl <- log(x)
m <- arima(xl, order=c(1, 0, 1))
m


Best,
Bernhard


>2. This question is more theoretical 
>
> m<-sample(c(10:20),10,replace=T)
> f<-sample(c(10:20),10,replace=T)
> t<-m+f
> s<-rbind(m,f,t)
> s
>
>Let's say I have a panel sample at disposal and consider m to 
>be the monthly average quantity of juice consumption for the  
>male part of the sample and f to be the monthly average 
>quantity of juice consumption for the  female part of the 
>sample, and t the average quantity of juice consumption for 
>the whole sample. For the mean of the whole sample i have a 
>confidence interval of say +/-2 each month (say I have a 
>sample of 2000 individuals). If I try to come up with a 
>confidence interval only for the male population (which in my 
>sample is  say 1000) it would certainly by bigger, because i 
>now have a male sample of 1000 for determining the mean 
>consumption for the whole male population. So my confidence 
>interval is bigger for mean male consumption than for the 
>whole sample (because N declines from 2000 to 1000). Now if I 
>tried to predict the the next month's consumption for both my 
>time series (male and whole sample) the prediction would not 
>"care" that when establishing the
> mean consumption i used first 2000 people and then 1000. Am I right?
>Imagine that each month (from 10 that I sampled above) has 
>such a confidence interval of +/-3. Now how would a future 
>prediction would incorporate this fact: that my mean 
>consumption is not measured via a Census, but using a sample, 
>and that the number is an estimation of the real consumption, 
>within a confidence interval?
>Is there a good reference text for this incorporation of the 
>confidence interval  of past values in determining  the future 
>values ? 
>
>Thank you and have a great day!
>
>
>
>
>       
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
*****************************************************************
Confidentiality Note: The information contained in this ...{{dropped:10}}


From b.otto at uke.uni-hamburg.de  Wed Dec 12 10:54:17 2007
From: b.otto at uke.uni-hamburg.de (Benjamin Otto)
Date: Wed, 12 Dec 2007 10:54:17 +0100
Subject: [R] Mono in postscript device
In-Reply-To: <Pine.LNX.4.64.0712111642260.24871@gannet.stats.ox.ac.uk>
References: <000601c83c12$b19d19c0$9f05a20a@matrix.com>
	<Pine.LNX.4.64.0712111642260.24871@gannet.stats.ox.ac.uk>
Message-ID: <001001c83ca4$fb988900$9f05a20a@matrix.com>

Dear Prof. Ripley,

Your version does work indeed, I hadn't tested this one. I tried the
following two versions where neither of them did:

Version 1:
> postscript(filename="myfile.ps")
> par(mfrow=c(2,2),family="mono")
> plot(1:10)
> plot(1:10)
> plot(1:10)
> plot(1:10)
> dev.off()

Version 2:
> postscript(filename="myfile.ps")
> plot(1:10,family="mono")
> dev.off()

Best regards,

Benjamin Otto

-----Urspr?ngliche Nachricht-----
Von: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Gesendet: Tuesday, December 11, 2007 5:48 PM
An: Benjamin Otto
Cc: R-Help
Betreff: Re: [R] Mono in postscript device

On Tue, 11 Dec 2007, Benjamin Otto wrote:

> Hi,
>
> Plotting a graphic into a postscript device using family="mono" 
> returns the following error message:
>
>>> 	family 'mono' not included in PostScript device
>
> Looking at postscriptFonts() however lists the "Courier" font as 
> availeable mono font. So where is the problem?

The 'problem' is that you have not provided us with reproducible code, and
used an obsolete version of R.  At least some of the ways you might have
done this you needed to specify the 'fonts' argument to postscript().

As a minimal example,

> postscript(family="mono")
> plot(1:10)
> dev.off()

works in R 2.6.1.  If that does not work for you, you need to update.
If it does, you need to consider what you did differently.

> Thanks guys for your help.
> Best regards
>
> Benjamin
>
> sessionInfo()
>
> R version 2.5.0 (2007-04-23)
> i386-pc-mingw32

[...]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



-- 
Pflichtangaben gem?? Gesetz ?ber elektronische Handelsregister und Genossenschaftsregister sowie das Unternehmensregister (EHUG):

Universit?tsklinikum Hamburg-Eppendorf
K?rperschaft des ?ffentlichen Rechts
Gerichtsstand: Hamburg

Vorstandsmitglieder:
Prof. Dr. J?rg F. Debatin (Vorsitzender)
Dr. Alexander Kirstein
Ricarda Klein
Prof. Dr. Dr. Uwe Koch-Gromus

From ripley at stats.ox.ac.uk  Wed Dec 12 10:59:24 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Dec 2007 09:59:24 +0000 (GMT)
Subject: [R] Mono in postscript device
In-Reply-To: <001001c83ca4$fb988900$9f05a20a@matrix.com>
References: <000601c83c12$b19d19c0$9f05a20a@matrix.com>
	<Pine.LNX.4.64.0712111642260.24871@gannet.stats.ox.ac.uk>
	<001001c83ca4$fb988900$9f05a20a@matrix.com>
Message-ID: <Pine.LNX.4.64.0712120955260.19764@gannet.stats.ox.ac.uk>

So this was an RTFM issue: you needed to declare fonts="mono".

> postscript(file="myfile.ps", fonts="mono")
> par(family="mono")
> plot(1:10)
> dev.off()

works, as documented.


On Wed, 12 Dec 2007, Benjamin Otto wrote:

> Dear Prof. Ripley,
>
> Your version does work indeed, I hadn't tested this one. I tried the
> following two versions where neither of them did:
>
> Version 1:
>> postscript(filename="myfile.ps")

The argument is *file* ....

>> par(mfrow=c(2,2),family="mono")
>> plot(1:10)
>> plot(1:10)
>> plot(1:10)
>> plot(1:10)
>> dev.off()
>
> Version 2:
>> postscript(filename="myfile.ps")
>> plot(1:10,family="mono")
>> dev.off()
>
> Best regards,
>
> Benjamin Otto
>
> -----Urspr?ngliche Nachricht-----
> Von: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Gesendet: Tuesday, December 11, 2007 5:48 PM
> An: Benjamin Otto
> Cc: R-Help
> Betreff: Re: [R] Mono in postscript device
>
> On Tue, 11 Dec 2007, Benjamin Otto wrote:
>
>> Hi,
>>
>> Plotting a graphic into a postscript device using family="mono"
>> returns the following error message:
>>
>>>> 	family 'mono' not included in PostScript device
>>
>> Looking at postscriptFonts() however lists the "Courier" font as
>> availeable mono font. So where is the problem?
>
> The 'problem' is that you have not provided us with reproducible code, and
> used an obsolete version of R.  At least some of the ways you might have
> done this you needed to specify the 'fonts' argument to postscript().
>
> As a minimal example,
>
>> postscript(family="mono")
>> plot(1:10)
>> dev.off()
>
> works in R 2.6.1.  If that does not work for you, you need to update.
> If it does, you need to consider what you did differently.
>
>> Thanks guys for your help.
>> Best regards
>>
>> Benjamin
>>
>> sessionInfo()
>>
>> R version 2.5.0 (2007-04-23)
>> i386-pc-mingw32
>
> [...]
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From mark_difford at yahoo.co.uk  Wed Dec 12 11:05:54 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Wed, 12 Dec 2007 02:05:54 -0800 (PST)
Subject: [R]  lm/model.matrix confusion (? bug)
Message-ID: <14292188.post@talk.nabble.com>


Dear List-members,

Hopefully someone will help through my confusion:

In order to get the same coefficients as we get from the following

##
require (MASS)
summary ( lm(Gas ~ Insul/Temp - 1, data = whiteside) )

......................

we need to do the following (if we use model.matrix to specify the model)

##
summary ( lm(Gas ~ model.matrix(~ Insul/Temp - 1) - 1, data = whiteside) )

......................

That is, we need to take out "two intercepts."  Is this "correct"? 
Shouldn't lm check to see if an intercept has been requested as part of the
model formula?

If I do
##
summary(lm(as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside)),
data=whiteside))

.......................

we get a strange model.  But the formula part of this model qualifies as a
valid formula
##
as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside))

----------------

just as if I were to write: lm(Gas ~ Insul/Temp - 1, data=whiteside)

But we know that the _correct_ formula is the following

##
as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside) -1)

-----------------

(Sorry, this is getting really long) --- So, my question/confusion comes
down to wanting to know why lm() doesn't check to see if an intercept has
been specified when the model has been specified using model.matrix.

Regards,
Mark.

-- 
View this message in context: http://www.nabble.com/lm-model.matrix-confusion-%28--bug%29-tp14292188p14292188.html
Sent from the R help mailing list archive at Nabble.com.


From mark_difford at yahoo.co.uk  Wed Dec 12 11:10:31 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Wed, 12 Dec 2007 02:10:31 -0800 (PST)
Subject: [R] lm/model.matrix confusion (? bug)
In-Reply-To: <14292188.post@talk.nabble.com>
References: <14292188.post@talk.nabble.com>
Message-ID: <14292212.post@talk.nabble.com>


Whoops!  Sorry, forgot my session stuff, just in case ...

R version 2.6.1 RC (2007-11-22 r43520) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_South Africa.1252;LC_CTYPE=English_South
Africa.1252;LC_MONETARY=English_South
Africa.1252;LC_NUMERIC=C;LC_TIME=English_South Africa.1252

attached base packages:
[1] tcltk     splines   stats     graphics  grDevices utils     datasets 
methods   base     

other attached packages:
[1] debug_1.1.0    mvbutils_1.1.1 MASS_7.2-38    sfsmisc_1.0-0  ade4_1.4-5    
Design_2.1-1  
[7] survival_2.34  Hmisc_3.4-3   

loaded via a namespace (and not attached):
[1] cluster_1.11.9     gamlss_1.7-0       grid_2.6.1         lattice_0.17-2    
latticeExtra_0.3-1
[6] rcompgen_0.1-17    tools_2.6.1 



Mark Difford wrote:
> 
> Dear List-members,
> 
> Hopefully someone will help through my confusion:
> 
> In order to get the same coefficients as we get from the following
> 
> ##
> require (MASS)
> summary ( lm(Gas ~ Insul/Temp - 1, data = whiteside) )
> 
> ......................
> 
> we need to do the following (if we use model.matrix to specify the model)
> 
> ##
> summary ( lm(Gas ~ model.matrix(~ Insul/Temp - 1) - 1, data = whiteside) )
> 
> ......................
> 
> That is, we need to take out "two intercepts."  Is this "correct"? 
> Shouldn't lm check to see if an intercept has been requested as part of
> the model formula?
> 
> If I do
> ##
> summary(lm(as.formula(Gas ~ model.matrix (~ Insul/Temp-1,
> data=whiteside)), data=whiteside))
> 
> .......................
> 
> we get a strange model.  But the formula part of this model qualifies as a
> valid formula
> ##
> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside))
> 
> ----------------
> 
> just as if I were to write: lm(Gas ~ Insul/Temp - 1, data=whiteside)
> 
> But we know that the _correct_ formula is the following
> 
> ##
> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside) -1)
> 
> -----------------
> 
> (Sorry, this is getting really long) --- So, my question/confusion comes
> down to wanting to know why lm() doesn't check to see if an intercept has
> been specified when the model has been specified using model.matrix.
> 
> Regards,
> Mark.
> 
> 

-- 
View this message in context: http://www.nabble.com/lm-model.matrix-confusion-%28--bug%29-tp14292188p14292212.html
Sent from the R help mailing list archive at Nabble.com.


From P.Dalgaard at biostat.ku.dk  Wed Dec 12 11:14:33 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 12 Dec 2007 11:14:33 +0100
Subject: [R] Getting error message using R: Please help (it's coursework
 due in on Friday)
In-Reply-To: <14291568.post@talk.nabble.com>
References: <14291568.post@talk.nabble.com>
Message-ID: <475FB489.1020009@biostat.ku.dk>

ethanhaas wrote:
> I've been trying for the past 3 weeks to use R (much better than Matlab but I
> am very bad with computers so I am very new to all of this) and know how to
> input the data (hey, it's a start!) but every time I type in the following:
>
> dcm <- decompose(information)
>
> I get the following error message:
>
> Error in decompose(information) : time series has no or less than 3 periods
>
> Could you please aid me so that I can progress to the next step. What I am
> in fact trying to do at the moment is plot a graph of the information so I
> had assumed it would be:
>
> dcm <- decompose(information)
> Plot(dcm$trend)
>
> but obviously it does not appear to be the case.
>
> By the way the code I have right now in case it helps is:
>
> information <- scan("everything.txt")
> timeseries <- ts(information, frequency = 11, start = c(1970, 1994))
> using <- window(timeseries, end = c(1990, 11))
> dcm <- decompose(information)
> Plot(dcm$trend)
>
>   
Er, are you sure it is "information" that you want to decompose()? (If
so, what are the computations of "timeseries" and "using" supposed to be
good for?)

> Thank you very much in advance, I really appreciate the help.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From P.Dalgaard at biostat.ku.dk  Wed Dec 12 11:35:20 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 12 Dec 2007 11:35:20 +0100
Subject: [R] Importing Large Dataset into Excel
In-Reply-To: <475F9BE0.6030503@sciviews.org>
References: <1197429329.475f525155179@webmail.fas.harvard.edu>	<644e1f320712111924y17a0d367t6876a49c53010c7e@mail.gmail.com>
	<475F9BE0.6030503@sciviews.org>
Message-ID: <475FB968.4020907@biostat.ku.dk>

Philippe Grosjean wrote:
> The problem is often a misspecification of the comment.char argument. 
> For read.table(), it defaults to '#'. This means that everywhere you 
> have a '#' char in your Excel sheet, the rest of the line is ignored. 
> This results in a different number of items per line.
>
> You should better use read.csv() which provides better default arguments 
> for your particular problem.
> Best,
>
>   
Or read.delim/read.delim2, which should be even better at TAB-separated
files.

In general, be very suspicious of read.table() with such files, not only
because of the '#' but also because it expects columns separated by
_arbitrary_ amounts of whitespace. I.e., n TABs  counts as one, so empty
fields are skipped over.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From berwin at maths.uwa.edu.au  Wed Dec 12 11:36:32 2007
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Wed, 12 Dec 2007 18:36:32 +0800
Subject: [R] lm/model.matrix confusion (? bug)
In-Reply-To: <14292188.post@talk.nabble.com>
References: <14292188.post@talk.nabble.com>
Message-ID: <20071212183632.22155700@berwin-nus1>

G'day Mark,

On Wed, 12 Dec 2007 02:05:54 -0800 (PST)
Mark Difford <mark_difford at yahoo.co.uk> wrote:

> In order to get the same coefficients as we get from the following
[...] 
> we need to do the following (if we use model.matrix to specify the
> model)

By why would you want to do this?

> ##
> summary ( lm(Gas ~ model.matrix(~ Insul/Temp - 1) - 1, data =
> whiteside) )
> 
> That is, we need to take out "two intercepts."  Is this "correct"?

Yes.
 
> Shouldn't lm check to see if an intercept has been requested as part
> of the model formula?

No, it does not.  In the Details section of lm's help page you will
find the following:

     A formula has an implied intercept term.  To remove this use
     either 'y ~ x - 1' or 'y ~ 0 + x'.  See 'formula' for more details
     of allowed formulae.

Thus, except if you explicitly ask for a constant term not be included,
lm will add a constant term (a column of ones) additionally to what
ever you have specified on the right hand side of the formula.

> If I do
> ##
> summary(lm(as.formula(Gas ~ model.matrix (~ Insul/Temp-1,
> data=whiteside)), data=whiteside))
> 
> we get a strange model.  

Well, you get a model in which not all parameters are identifiable, and
a particular parameter that is not identifiable is estimated by NA.  I
am not sure what is strange about this.

> But the formula part of this model qualifies
> as a valid formula
> ##
> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside))

Debatable, the above command only shows that it can be coerced into a
valid formula. :)

> just as if I were to write: lm(Gas ~ Insul/Temp - 1, data=whiteside)
> 
> But we know that the _correct_ formula is the following
 
> ##
> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside) -1)

Why is this formula any more correct than the other one?  Both specify
exactly the same model.  It is just that one does it in an
overparameterised way.

> (Sorry, this is getting really long) --- So, my question/confusion
> comes down to wanting to know why lm() doesn't check to see if an
> intercept has been specified when the model has been specified using
> model.matrix.

Because lm() is documented not to check this.  If you do not want to
have an intercept in the model you have to specifically ask it for.

Also, comparing the output of 
	summary( lm(Gas ~ Insul/Temp - 1, data = whiteside) )
and
	summary( lm(Gas ~ Insul/Temp, data = whiteside ) )

you can see that lm() does not check whether there is an implicit
intercept in the model.  Compare the (Adjusted) R-squared values
returned; one case is using the formula for models with no intercept
the other one the formula for models with intercept.  Similar story
with the reported F-statistics.  

Cheers,

	Berwin

=========================== Full address =============================
Berwin A Turlach                            Tel.: +65 6515 4416 (secr)
Dept of Statistics and Applied Probability        +65 6515 6650 (self)
Faculty of Science                          FAX : +65 6872 3919       
National University of Singapore     
6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
Singapore 117546                    http://www.stat.nus.edu.sg/~statba


From gavin.simpson at ucl.ac.uk  Wed Dec 12 11:37:03 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 12 Dec 2007 10:37:03 +0000
Subject: [R] lm/model.matrix confusion (? bug)
In-Reply-To: <14292188.post@talk.nabble.com>
References: <14292188.post@talk.nabble.com>
Message-ID: <1197455823.9595.20.camel@prometheus.geog.ucl.ac.uk>


On Wed, 2007-12-12 at 02:05 -0800, Mark Difford wrote:
> Dear List-members,
> 
> Hopefully someone will help through my confusion:
> 
> In order to get the same coefficients as we get from the following
> 
> ##
> require (MASS)
> summary ( lm(Gas ~ Insul/Temp - 1, data = whiteside) )
> 
> ......................
> 
> we need to do the following (if we use model.matrix to specify the model)
> 
> ##
> summary ( lm(Gas ~ model.matrix(~ Insul/Temp - 1) - 1, data = whiteside) )
> 
> ......................
> 
> That is, we need to take out "two intercepts."  Is this "correct"?

Yes - if you insist on doing things that way

>  
> Shouldn't lm check to see if an intercept has been requested as part of the
> model formula?

It does, (i.e. whether the user specified -1 in the formula argument),
but you specified it inside model.matrix() so the formula parsing code
never sees it. One wouldn't want lm to need to know about all functions
that have/could ever be written in R, past or future, to know how to
divine that here you wanted "-1" to mean "remove intercept please Mr. R
Parser" and not "subtract 1 from this result please Mr. R Parser".

You are really abusing the reason for having the formula interface. The
whole point of it is to make it easy for user to specify a model, from
which R generates the model matrix for you. Why use the formula at all
if you have already produced your model matrix?

See ?lm.fit for a fast way of fitting linear models (used within lm() )
if you have all the bits in place (i.e. you already have a model matrix)
and are happy to take care of other details yourself.

<snip />
> (Sorry, this is getting really long) --- So, my question/confusion comes
> down to wanting to know why lm() doesn't check to see if an intercept has
> been specified when the model has been specified using model.matrix.

I guess because you can't programme around every possible usage that a
user might dream up to try. If it did, lm would be an absolute pig and
run like one. It is slow enough as it is with all the user-friendly
stuff in there to help. (By slow enough, I mean it is perfectly speedy
in routine use, but you wouldn't want to use it in a permutation
test-like environment if you were after speed - you'd be better off
working with lm.fit directly)

The model.matrix bit is returning a matrix (without an entry for an
intercept), which is fine on the rhs of a formula. As all you've done
here is (effectively) create the following model:

> form <- formula(paste("Gas ~", 
+                 paste(colnames(model.matrix (~ Insul/Temp-1,
+                                data=whiteside)), collapse = " + ")))
> form
Gas ~ InsulBefore + InsulAfter + InsulBefore:Temp + InsulAfter:Temp

I.e., that is what your convoluted formula is being interpreted as, you
then still need to remove the intercept that R will automagically add to
the model when it subsequently creates the model matrix.

HTH

G

> 
> Regards,
> Mark.
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From chengbinw at gmail.com  Wed Dec 12 11:59:05 2007
From: chengbinw at gmail.com (Wang Chengbin)
Date: Wed, 12 Dec 2007 18:59:05 +0800
Subject: [R] Matrix Inversion
Message-ID: <d3a104da0712120259r6a31e546t15aedb97f3bb0426@mail.gmail.com>

I got the following error:

a = read.csv("mat.csv")
b = as.matrix(a)
tb = t(b)
bb = tb %*% b
dim(bb)
ibb = solve(bb)
bb %*% ibb

> ibb = solve(bb)
Error in solve.default(bb) :
  system is computationally singular: reciprocal condition number =
1.77573e-19
>
Are there any ways to find more information about why it is singular?

Thanks.

From d.scott at auckland.ac.nz  Wed Dec 12 12:00:53 2007
From: d.scott at auckland.ac.nz (David Scott)
Date: Thu, 13 Dec 2007 00:00:53 +1300 (NZDT)
Subject: [R] Importing Large Dataset into Excel
In-Reply-To: <475FB968.4020907@biostat.ku.dk>
References: <1197429329.475f525155179@webmail.fas.harvard.edu>
	<644e1f320712111924y17a0d367t6876a49c53010c7e@mail.gmail.com>
	<475F9BE0.6030503@sciviews.org> <475FB968.4020907@biostat.ku.dk>
Message-ID: <Pine.LNX.4.64.0712122349130.23907@stat12.stat.auckland.ac.nz>

On Wed, 12 Dec 2007, Peter Dalgaard wrote:

> Philippe Grosjean wrote:
> The problem is often a misspecification of the comment.char argument. 
> For read.table(), it defaults to '#'. This means that everywhere you 
> have a '#' char in your Excel sheet, the rest of the line is ignored. 
> This results in a different number of items per line.
>
> You should better use read.csv() which provides better default arguments 
> for your particular problem.
> Best,
>
> 
Or read.delim/read.delim2, which should be even better at TAB-separated
files.

In general, be very suspicious of read.table() with such files, not only
because of the '#' but also because it expects columns separated by
_arbitrary_ amounts of whitespace. I.e., n TABs  counts as one, so empty
fields are skipped over.

******* End of other contributions (not sure why my mailer didn't mark 
them)

I would also say be very suspicious of Excel writing .csv files.
I found by looking at the .csv file in an editor that for some reason when 
there were empty fields in the original .xls file that for some records, 
Excel didn't add in enough commas to make up the correct number of fields.
It did for some records but not for others. Excel truly works in 
misterious ways.

read.csv has an argument fill which should fix this problem. In my case I 
was actually reading the .csv file into mySQL and the solution was to 
select the whole of the .xls file and format it as text before writing the 
.csv file.

David SCott







_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland 1142,    NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz

Graduate Officer, Department of Statistics
Director of Consulting, Department of Statistics


From r.hankin at noc.soton.ac.uk  Wed Dec 12 12:13:06 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 12 Dec 2007 11:13:06 +0000
Subject: [R] Matrix Inversion
In-Reply-To: <d3a104da0712120259r6a31e546t15aedb97f3bb0426@mail.gmail.com>
References: <d3a104da0712120259r6a31e546t15aedb97f3bb0426@mail.gmail.com>
Message-ID: <C72285BB-9A62-45EA-AF99-2161EB68545A@noc.soton.ac.uk>

Hello Wang

matrix bb is symmetric positive semidefinite, so
algebraically the eigenvalues are nonnegative.

I would use

bb <- crossprod(b)

to calculate bb (faster and possibly more accurate)

Look at eigen(bb,TRUE,TRUE)$values

(see ?eigen for the meaning of the arguments) to see how
many very small eigenvalues you have.  The number of zero
eigenvalues is equal to the number of linear relations
in the columns of b.


HTH


rksh



On 12 Dec 2007, at 10:59, Wang Chengbin wrote:

> I got the following error:
>
> a = read.csv("mat.csv")
> b = as.matrix(a)
> tb = t(b)
> bb = tb %*% b
> dim(bb)
> ibb = solve(bb)
> bb %*% ibb
>
>> ibb = solve(bb)
> Error in solve.default(bb) :
>   system is computationally singular: reciprocal condition number =
> 1.77573e-19
>>
> Are there any ways to find more information about why it is singular?
>
> Thanks.
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst and Neutral Theorist,
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From daniel.valverde at uab.cat  Wed Dec 12 12:13:59 2007
From: daniel.valverde at uab.cat (Dani Valverde)
Date: Wed, 12 Dec 2007 12:13:59 +0100
Subject: [R] Package compilation
Message-ID: <475FC277.7050602@uab.cat>

Hello,
Is there an easy way to compile a package so that it can be installed as 
a binary file under Windows? Now I have the source code. I am not used 
in compiling, and the documentation seems too hard for me, so an easy 
way would be great.
Best regards,

Dani

-- 
Daniel Valverde Saub?

Grup de Biologia Molecular de Llevats
Facultat de Veterin?ria de la Universitat Aut?noma de Barcelona
Edifici V, Campus UAB
08193 Cerdanyola del Vall?s- SPAIN

Centro de Investigaci?n Biom?dica en Red
en Bioingenier?a, Biomateriales y
Nanomedicina (CIBER-BBN)

Grup d'Aplicacions Biom?diques de la RMN
Facultat de Bioci?ncies
Universitat Aut?noma de Barcelona
Edifici Cs, Campus UAB
08193 Cerdanyola del Vall?s- SPAIN
+34 93 5814126


From ethanhaaswaswrong at gmail.com  Wed Dec 12 12:20:32 2007
From: ethanhaaswaswrong at gmail.com (ethanhaas)
Date: Wed, 12 Dec 2007 03:20:32 -0800 (PST)
Subject: [R] Getting error message using R: Please help (it's coursework
 due in on Friday)
In-Reply-To: <475FB489.1020009@biostat.ku.dk>
References: <14291568.post@talk.nabble.com> <475FB489.1020009@biostat.ku.dk>
Message-ID: <14292665.post@talk.nabble.com>


I simply called it information but I can also call it data if you wish, it
doesn't make a difference.
The timeseries is simply the time series of this data between the two dates
and using is the data up to 1990 as opposed to 1994.


P.Dalgaard wrote:
> 
> ethanhaas wrote:
>> I've been trying for the past 3 weeks to use R (much better than Matlab
>> but I
>> am very bad with computers so I am very new to all of this) and know how
>> to
>> input the data (hey, it's a start!) but every time I type in the
>> following:
>>
>> dcm <- decompose(information)
>>
>> I get the following error message:
>>
>> Error in decompose(information) : time series has no or less than 3
>> periods
>>
>> Could you please aid me so that I can progress to the next step. What I
>> am
>> in fact trying to do at the moment is plot a graph of the information so
>> I
>> had assumed it would be:
>>
>> dcm <- decompose(information)
>> Plot(dcm$trend)
>>
>> but obviously it does not appear to be the case.
>>
>> By the way the code I have right now in case it helps is:
>>
>> information <- scan("everything.txt")
>> timeseries <- ts(information, frequency = 11, start = c(1970, 1994))
>> using <- window(timeseries, end = c(1990, 11))
>> dcm <- decompose(information)
>> Plot(dcm$trend)
>>
>>   
> Er, are you sure it is "information" that you want to decompose()? (If
> so, what are the computations of "timeseries" and "using" supposed to be
> good for?)
> 
>> Thank you very much in advance, I really appreciate the help.
>>   
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> 35327907
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Getting-error-message-using-R%3A-Please-help-%28it%27s-coursework-due-in-on-Friday%29-tp14291568p14292665.html
Sent from the R help mailing list archive at Nabble.com.


From P.Dalgaard at biostat.ku.dk  Wed Dec 12 12:22:18 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 12 Dec 2007 12:22:18 +0100
Subject: [R] Matrix Inversion
In-Reply-To: <d3a104da0712120259r6a31e546t15aedb97f3bb0426@mail.gmail.com>
References: <d3a104da0712120259r6a31e546t15aedb97f3bb0426@mail.gmail.com>
Message-ID: <475FC46A.2000205@biostat.ku.dk>

Wang Chengbin wrote:
> I got the following error:
>
> a = read.csv("mat.csv")
> b = as.matrix(a)
> tb = t(b)
> bb = tb %*% b
> dim(bb)
> ibb = solve(bb)
> bb %*% ibb
>
>   
>> ibb = solve(bb)
>>     
> Error in solve.default(bb) :
>   system is computationally singular: reciprocal condition number =
> 1.77573e-19
>   
> Are there any ways to find more information about why it is singular?
>
> Thanks.
>   
Yes. Since the matrix is positive semidefinite by construction, I'd
probably go for chol(bb, pivot=TRUE), then the first "rank" elements of
"pivot" gives you a maximal subset of linearly independent columns, and
you can proceed by something like lm(b[,-subset]~b[,subset]) to see what
the linear dependencies are.

(Other approaches could be eigen() and svd().)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From P.Dalgaard at biostat.ku.dk  Wed Dec 12 12:25:46 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 12 Dec 2007 12:25:46 +0100
Subject: [R] Getting error message using R: Please help (it's coursework
 due in on Friday)
In-Reply-To: <14292665.post@talk.nabble.com>
References: <14291568.post@talk.nabble.com> <475FB489.1020009@biostat.ku.dk>
	<14292665.post@talk.nabble.com>
Message-ID: <475FC53A.9090903@biostat.ku.dk>

ethanhaas wrote:
> I simply called it information but I can also call it data if you wish, it
> doesn't make a difference.
> The timeseries is simply the time series of this data between the two dates
> and using is the data up to 1990 as opposed to 1994.
>
>   
And isn't it one of the latter two you want to decompose()?

> P.Dalgaard wrote:
>   
>> ethanhaas wrote:
>>     
>>> I've been trying for the past 3 weeks to use R (much better than Matlab
>>> but I
>>> am very bad with computers so I am very new to all of this) and know how
>>> to
>>> input the data (hey, it's a start!) but every time I type in the
>>> following:
>>>
>>> dcm <- decompose(information)
>>>
>>> I get the following error message:
>>>
>>> Error in decompose(information) : time series has no or less than 3
>>> periods
>>>
>>> Could you please aid me so that I can progress to the next step. What I
>>> am
>>> in fact trying to do at the moment is plot a graph of the information so
>>> I
>>> had assumed it would be:
>>>
>>> dcm <- decompose(information)
>>> Plot(dcm$trend)
>>>
>>> but obviously it does not appear to be the case.
>>>
>>> By the way the code I have right now in case it helps is:
>>>
>>> information <- scan("everything.txt")
>>> timeseries <- ts(information, frequency = 11, start = c(1970, 1994))
>>> using <- window(timeseries, end = c(1990, 11))
>>> dcm <- decompose(information)
>>> Plot(dcm$trend)
>>>
>>>   
>>>       
>> Er, are you sure it is "information" that you want to decompose()? (If
>> so, what are the computations of "timeseries" and "using" supposed to be
>> good for?)
>>
>>     
>>> Thank you very much in advance, I really appreciate the help.
>>>   
>>>       
>> -- 
>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>> 35327918
>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>> 35327907
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>     
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Christian.Kohler at klinik.uni-regensburg.de  Wed Dec 12 12:31:51 2007
From: Christian.Kohler at klinik.uni-regensburg.de (Christian Kohler)
Date: Wed, 12 Dec 2007 12:31:51 +0100
Subject: [R] how to obtain the CPU time of my program
In-Reply-To: <000b01c7f619$18ee8a90$2de42880@chemeng.ucl.ac.uk>
References: <000b01c7f619$18ee8a90$2de42880@chemeng.ucl.ac.uk>
Message-ID: <475FC6A7.70100@klinik.uni-regensburg.de>

Hi,

first of all 'sorry' for my rather late post on this topic...

Did you already came across the R command 'Rprof' ? This might help you
to find answers to your problem in terms of CPU time/command.
I use 'Rprof' like this:

>Rprof()
>source("yourCode.R")
>Rprof(NULL)

This produces a file called Rprof.out in your working directory.
Quit R and type 'R CMD Rprof Rprof.out'.

I am too new in using those profiling methods of R but I think they are
pretty handy.

Try out ?Rprof for detailed information.

Cheers,
Christian



gang xu wrote:
> Dear R users and experts,
>
> I am current running a program (a series of commands) in R. such as:
>
> A <- as.matrix(read.table("C:/LP.txt"));
>
> a=which(memb==q); b=a; B=as.matrix(A[a,b])
>
> LS=sum(B)/2;
>
> TL=sum(A)/2
>
> i<-c(1:NN);
>
> D=sum(A[a,i]);
>
> how can i obtain the CPU time used for these commands ?
>
> I have seen the system.time function but i am not sure how to use it. Could anyone help me ?
>
> ****** If we have a number of commands, how can we know the CPU for these commands *******
>
>
> Thanks a lot for your time and help
>
> Have a nice day!
>
> Warm Regards
>
> Marshall
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 

Christian Kohler
Institute of Functional Genomics
Computational Diagnostics
University of Regensburg (BioPark I)
D-93147 Regensburg (Germany)

Tel. +49 941 943 5055
Fax  +49 941 943 5020
christian.kohler at klinik.uni-regensburg.de


From Patrik.Ohagen at mpa.se  Wed Dec 12 13:19:55 2007
From: Patrik.Ohagen at mpa.se (=?iso-8859-1?Q?=D6hagen_Patrik?=)
Date: Wed, 12 Dec 2007 13:19:55 +0100
Subject: [R] Measure of agreement??
Message-ID: <2BAF2D3C41D1274E9228E63287F19B7E4FD4B8@mailsrv2.loginmpa.mpa.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/7f823c0c/attachment.pl 

From vistocco at unicas.it  Wed Dec 12 11:33:35 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Wed, 12 Dec 2007 11:33:35 +0100
Subject: [R] Importing Large Dataset into Excel
In-Reply-To: <644e1f320712111924y17a0d367t6876a49c53010c7e@mail.gmail.com>
References: <1197429329.475f525155179@webmail.fas.harvard.edu>
	<644e1f320712111924y17a0d367t6876a49c53010c7e@mail.gmail.com>
Message-ID: <475FB8FF.4000503@unicas.it>

If you are using a windows system you could take a look at the 
xlsReadWrite packages (there are functions for reading xls files).

domenico

jim holtman wrote:
> ?count.fields
>
> count.fields will tell you how many items are in each line.  As you
> said, they should all be the same, but this will confirm it.
>
> field.count <- count.fields("newborn edit.csv", sep=",")
> table(field.count)  # determine count of the fields on a line
>
> On Dec 11, 2007 7:15 PM, Wayne Aldo Gavioli <wgavioli at fas.harvard.edu> wrote:
>   
>> Hello all,
>>
>>
>> I seem to be having a problem importing a data set from Excel into R.  I'm using
>> the "read.table" command to import the data with the following line of code:
>>
>>     
>>> newborn<-read.table("newborn edit.csv", header=T, sep=",")
>>>       
>> where "newborn edit.csv" is the name of the file.  Unfortunately, I'm getting
>> back the following error message:
>>
>>
>> "Error in scan(file,, what, nmax, sep, dc, quote, skip, nlines, na.string, :
>> line 528 did not have 44 elements"
>>
>>
>> As far as I can tell, line 528 of the spreadsheet table does have the same
>> number of elements as the other rows - by chance can this error message mean
>> anything else?  Also, is there an easier way to import data from R into Excel
>> using a single line of R code?
>>     
>>
>> Thanks,
>>
>>
>> Wayne
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
>
>
>


From ripley at stats.ox.ac.uk  Wed Dec 12 13:36:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Dec 2007 12:36:07 +0000 (GMT)
Subject: [R] Package compilation
In-Reply-To: <475FC277.7050602@uab.cat>
References: <475FC277.7050602@uab.cat>
Message-ID: <Pine.LNX.4.64.0712121230470.2868@gannet.stats.ox.ac.uk>

The easiest way is to read (in the manual)

   If you have only a source package that is known to work with current R
   and just want a binary Windows build of it, you could make use of the
   building service offered at win-builder.r-project.org.

Is that what was too hard for you?  If so, you are out of luck.
(Kudos to Uwe Liggges for this service.)

On Wed, 12 Dec 2007, Dani Valverde wrote:

> Hello,
> Is there an easy way to compile a package so that it can be installed as
> a binary file under Windows? Now I have the source code. I am not used
> in compiling, and the documentation seems too hard for me, so an easy
> way would be great.
> Best regards,
>
> Dani
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From epistat at gmail.com  Wed Dec 12 13:38:18 2007
From: epistat at gmail.com (zhijie zhang)
Date: Wed, 12 Dec 2007 20:38:18 +0800
Subject: [R] Hep on using GAM() in R
Message-ID: <2fc17e30712120438g63130b41v897dc2c6d6d9792@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/f280430f/attachment.pl 

From S.Ellison at lgc.co.uk  Wed Dec 12 13:42:18 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Wed, 12 Dec 2007 12:42:18 +0000
Subject: [R] book on regular expressions
Message-ID: <s75fd735.098@tedmail.lgc.co.uk>



>>> "hadley wickham" <h.wickham at gmail.com> 11/12/2007 17:02 >>>
> Could someone recommend a good book on regular expressions with focus
on
> applications/use as it might relate to R.  I remember there was a
mention of
> such a reference book recently, but I could not locate that message
on the
> archive.


It's not specific to R, which wraps regular expressions up in quoted
strings so has a couple of wrinkles compared to Perl, but I found the
O'Reilly " Regular Expression Pocket Reference" a good quick reference,
and rather cheaper than the whole book. It's hard to see R users needing
chapter and verse on the complete gamut of regexps.

Steve E


From ethanhaaswaswrong at gmail.com  Wed Dec 12 13:52:43 2007
From: ethanhaaswaswrong at gmail.com (ethanhaas)
Date: Wed, 12 Dec 2007 04:52:43 -0800 (PST)
Subject: [R] Getting error message using R: Please help (it's coursework
 due in on Friday)
In-Reply-To: <475FC53A.9090903@biostat.ku.dk>
References: <14291568.post@talk.nabble.com> <475FB489.1020009@biostat.ku.dk>
	<14292665.post@talk.nabble.com> <475FC53A.9090903@biostat.ku.dk>
Message-ID: <14294474.post@talk.nabble.com>


What I'm trying to do is have plots and summary statistics of x
(autocorrelations for example) where x refers to the data in using, and I
thought decomposing would be the way to go about it.



P.Dalgaard wrote:
> 
> ethanhaas wrote:
>> I simply called it information but I can also call it data if you wish,
>> it
>> doesn't make a difference.
>> The timeseries is simply the time series of this data between the two
>> dates
>> and using is the data up to 1990 as opposed to 1994.
>>
>>   
> And isn't it one of the latter two you want to decompose()?
> 
>> P.Dalgaard wrote:
>>   
>>> ethanhaas wrote:
>>>     
>>>> I've been trying for the past 3 weeks to use R (much better than Matlab
>>>> but I
>>>> am very bad with computers so I am very new to all of this) and know
>>>> how
>>>> to
>>>> input the data (hey, it's a start!) but every time I type in the
>>>> following:
>>>>
>>>> dcm <- decompose(information)
>>>>
>>>> I get the following error message:
>>>>
>>>> Error in decompose(information) : time series has no or less than 3
>>>> periods
>>>>
>>>> Could you please aid me so that I can progress to the next step. What I
>>>> am
>>>> in fact trying to do at the moment is plot a graph of the information
>>>> so
>>>> I
>>>> had assumed it would be:
>>>>
>>>> dcm <- decompose(information)
>>>> Plot(dcm$trend)
>>>>
>>>> but obviously it does not appear to be the case.
>>>>
>>>> By the way the code I have right now in case it helps is:
>>>>
>>>> information <- scan("everything.txt")
>>>> timeseries <- ts(information, frequency = 11, start = c(1970, 1994))
>>>> using <- window(timeseries, end = c(1990, 11))
>>>> dcm <- decompose(information)
>>>> Plot(dcm$trend)
>>>>
>>>>   
>>>>       
>>> Er, are you sure it is "information" that you want to decompose()? (If
>>> so, what are the computations of "timeseries" and "using" supposed to be
>>> good for?)
>>>
>>>     
>>>> Thank you very much in advance, I really appreciate the help.
>>>>   
>>>>       
>>> -- 
>>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>>> 35327918
>>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>>> 35327907
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>     
>>
>>   
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> 35327907
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Getting-error-message-using-R%3A-Please-help-%28it%27s-coursework-due-in-on-Friday%29-tp14291568p14294474.html
Sent from the R help mailing list archive at Nabble.com.


From P.Dalgaard at biostat.ku.dk  Wed Dec 12 13:58:37 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 12 Dec 2007 13:58:37 +0100
Subject: [R] Getting error message using R: Please help (it's coursework
 due in on Friday)
In-Reply-To: <14294474.post@talk.nabble.com>
References: <14291568.post@talk.nabble.com>
	<475FB489.1020009@biostat.ku.dk>	<14292665.post@talk.nabble.com>
	<475FC53A.9090903@biostat.ku.dk> <14294474.post@talk.nabble.com>
Message-ID: <475FDAFD.7030208@biostat.ku.dk>

ethanhaas wrote:
> What I'm trying to do is have plots and summary statistics of x
> (autocorrelations for example) where x refers to the data in using, and I
> thought decomposing would be the way to go about it.
>
>
>   
You are not listening. I give up.

> P.Dalgaard wrote:
>   
>> ethanhaas wrote:
>>     
>>> I simply called it information but I can also call it data if you wish,
>>> it
>>> doesn't make a difference.
>>> The timeseries is simply the time series of this data between the two
>>> dates
>>> and using is the data up to 1990 as opposed to 1994.
>>>
>>>   
>>>       
>> And isn't it one of the latter two you want to decompose()?
>>
>>     
>>> P.Dalgaard wrote:
>>>   
>>>       
>>>> ethanhaas wrote:
>>>>     
>>>>         
>>>>> I've been trying for the past 3 weeks to use R (much better than Matlab
>>>>> but I
>>>>> am very bad with computers so I am very new to all of this) and know
>>>>> how
>>>>> to
>>>>> input the data (hey, it's a start!) but every time I type in the
>>>>> following:
>>>>>
>>>>> dcm <- decompose(information)
>>>>>
>>>>> I get the following error message:
>>>>>
>>>>> Error in decompose(information) : time series has no or less than 3
>>>>> periods
>>>>>
>>>>> Could you please aid me so that I can progress to the next step. What I
>>>>> am
>>>>> in fact trying to do at the moment is plot a graph of the information
>>>>> so
>>>>> I
>>>>> had assumed it would be:
>>>>>
>>>>> dcm <- decompose(information)
>>>>> Plot(dcm$trend)
>>>>>
>>>>> but obviously it does not appear to be the case.
>>>>>
>>>>> By the way the code I have right now in case it helps is:
>>>>>
>>>>> information <- scan("everything.txt")
>>>>> timeseries <- ts(information, frequency = 11, start = c(1970, 1994))
>>>>> using <- window(timeseries, end = c(1990, 11))
>>>>> dcm <- decompose(information)
>>>>> Plot(dcm$trend)
>>>>>
>>>>>   
>>>>>       
>>>>>           
>>>> Er, are you sure it is "information" that you want to decompose()? (If
>>>> so, what are the computations of "timeseries" and "using" supposed to be
>>>> good for?)
>>>>
>>>>     
>>>>         
>>>>> Thank you very much in advance, I really appreciate the help.
>>>>>   
>>>>>       
>>>>>           
>>>> -- 
>>>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>>>> 35327918
>>>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>>>> 35327907
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>>     
>>>>         
>>>   
>>>       
>> -- 
>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>> 35327918
>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>> 35327907
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>     
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ethanhaaswaswrong at gmail.com  Wed Dec 12 14:03:08 2007
From: ethanhaaswaswrong at gmail.com (ethanhaas)
Date: Wed, 12 Dec 2007 05:03:08 -0800 (PST)
Subject: [R] Getting error message using R: Please help (it's coursework
 due in on Friday)
In-Reply-To: <475FDAFD.7030208@biostat.ku.dk>
References: <14291568.post@talk.nabble.com> <475FB489.1020009@biostat.ku.dk>
	<14292665.post@talk.nabble.com> <475FC53A.9090903@biostat.ku.dk>
	<14294474.post@talk.nabble.com> <475FDAFD.7030208@biostat.ku.dk>
Message-ID: <14294733.post@talk.nabble.com>


I was, and the answer is yes, but I figured the easiest way for you to
understand is for me to say exactly what I am trying to do. Perhaps I don't
even need to decompose (although I think I do). If I can get past this
problem it would make everything else so much easier.



P.Dalgaard wrote:
> 
> ethanhaas wrote:
>> What I'm trying to do is have plots and summary statistics of x
>> (autocorrelations for example) where x refers to the data in using, and I
>> thought decomposing would be the way to go about it.
>>
>>
>>   
> You are not listening. I give up.
> 
>> P.Dalgaard wrote:
>>   
>>> ethanhaas wrote:
>>>     
>>>> I simply called it information but I can also call it data if you wish,
>>>> it
>>>> doesn't make a difference.
>>>> The timeseries is simply the time series of this data between the two
>>>> dates
>>>> and using is the data up to 1990 as opposed to 1994.
>>>>
>>>>   
>>>>       
>>> And isn't it one of the latter two you want to decompose()?
>>>
>>>     
>>>> P.Dalgaard wrote:
>>>>   
>>>>       
>>>>> ethanhaas wrote:
>>>>>     
>>>>>         
>>>>>> I've been trying for the past 3 weeks to use R (much better than
>>>>>> Matlab
>>>>>> but I
>>>>>> am very bad with computers so I am very new to all of this) and know
>>>>>> how
>>>>>> to
>>>>>> input the data (hey, it's a start!) but every time I type in the
>>>>>> following:
>>>>>>
>>>>>> dcm <- decompose(information)
>>>>>>
>>>>>> I get the following error message:
>>>>>>
>>>>>> Error in decompose(information) : time series has no or less than 3
>>>>>> periods
>>>>>>
>>>>>> Could you please aid me so that I can progress to the next step. What
>>>>>> I
>>>>>> am
>>>>>> in fact trying to do at the moment is plot a graph of the information
>>>>>> so
>>>>>> I
>>>>>> had assumed it would be:
>>>>>>
>>>>>> dcm <- decompose(information)
>>>>>> Plot(dcm$trend)
>>>>>>
>>>>>> but obviously it does not appear to be the case.
>>>>>>
>>>>>> By the way the code I have right now in case it helps is:
>>>>>>
>>>>>> information <- scan("everything.txt")
>>>>>> timeseries <- ts(information, frequency = 11, start = c(1970, 1994))
>>>>>> using <- window(timeseries, end = c(1990, 11))
>>>>>> dcm <- decompose(information)
>>>>>> Plot(dcm$trend)
>>>>>>
>>>>>>   
>>>>>>       
>>>>>>           
>>>>> Er, are you sure it is "information" that you want to decompose()? (If
>>>>> so, what are the computations of "timeseries" and "using" supposed to
>>>>> be
>>>>> good for?)
>>>>>
>>>>>     
>>>>>         
>>>>>> Thank you very much in advance, I really appreciate the help.
>>>>>>   
>>>>>>       
>>>>>>           
>>>>> -- 
>>>>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>>>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>>>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>>>>> 35327918
>>>>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>>>>> 35327907
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>>
>>>>>     
>>>>>         
>>>>   
>>>>       
>>> -- 
>>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>>> 35327918
>>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>>> 35327907
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>     
>>
>>   
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> 35327907
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Getting-error-message-using-R%3A-Please-help-%28it%27s-coursework-due-in-on-Friday%29-tp14291568p14294733.html
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Wed Dec 12 14:04:26 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Dec 2007 13:04:26 +0000 (GMT)
Subject: [R] book on regular expressions
In-Reply-To: <s75fd735.098@tedmail.lgc.co.uk>
References: <s75fd735.098@tedmail.lgc.co.uk>
Message-ID: <Pine.LNX.4.64.0712121253190.3901@gannet.stats.ox.ac.uk>

On Wed, 12 Dec 2007, S Ellison wrote:

>
>
>>>> "hadley wickham" <h.wickham at gmail.com> 11/12/2007 17:02 >>>
>> Could someone recommend a good book on regular expressions with focus
> on
>> applications/use as it might relate to R.  I remember there was a
> mention of
>> such a reference book recently, but I could not locate that message
> on the
>> archive.
>
>
> It's not specific to R, which wraps regular expressions up in quoted
> strings so has a couple of wrinkles compared to Perl, but I found the
> O'Reilly " Regular Expression Pocket Reference" a good quick reference,
> and rather cheaper than the whole book. It's hard to see R users needing
> chapter and verse on the complete gamut of regexps.

One has to remember that books go out of date: for example the advent of 
UTF-8 locales has led to a number of changes in the implementations. 
The O'Reilly books have third (2006) and second (2007) editions.  (BTW the 
pocket reference is not by the same author as 'Mastering Regular 
Expressions', so in no sense is part of 'the whole book'.)

I don't think anyone has mentioned the references given on the help page 
?regexp: they are a great deal more reliable than some of the third-party 
write-ups that have been mentioned.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From idimakos at upatras.gr  Wed Dec 12 14:06:09 2007
From: idimakos at upatras.gr (Ioannis Dimakos)
Date: Wed, 12 Dec 2007 15:06:09 +0200 (EET)
Subject: [R] Measure of agreement??
In-Reply-To: <2BAF2D3C41D1274E9228E63287F19B7E4FD4B8@mailsrv2.loginmpa.mpa.se>
References: <2BAF2D3C41D1274E9228E63287F19B7E4FD4B8@mailsrv2.loginmpa.mpa.se>
Message-ID: <44985.91.140.49.185.1197464769.squirrel@mail.upatras.gr>

Patrik,

> help.search("kappa")

is a good place to start.  Alternatively,

> RSiteSearch("measure of agreement")

provides helpful tips.

HTH,

I.

=================================
On ???, ?????????? 12, 2007 14:19, ?hagen Patrik wrote:
>
>
> Dear List,
>
> Please put me on the right track. I have been searching for the R
> functions for measures of agreement. Where should I look?
>
> Patrik ?hagen
> L?kemedelsverket
> Box 26
> 751 03 Uppsala
> Tel. +46(0)18-174924
> patrik.ohagen at mpa.se
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ioannis C. Dimakos, Ph.D.
University of Patras
Department of Elementary Education
Patras, GR-26500 GREECE


From agoralczyk at gmail.com  Wed Dec 12 14:14:28 2007
From: agoralczyk at gmail.com (Armin Goralczyk)
Date: Wed, 12 Dec 2007 14:14:28 +0100
Subject: [R] editor under MAC system
In-Reply-To: <1F1C0D82-C41B-418E-94F1-796A643C71C6@act.ulaval.ca>
References: <31beaa9a0712091210n31b6f292q582c7610234ad1e3@mail.gmail.com>
	<14272456.post@talk.nabble.com>
	<1F1C0D82-C41B-418E-94F1-796A643C71C6@act.ulaval.ca>
Message-ID: <a695fbee0712120514l72e32266p3edf592532f9605c@mail.gmail.com>

On Dec 11, 2007 3:03 PM, Vincent Goulet <vincent.goulet at act.ulaval.ca> wrote:
> Le mar. 11 d?c. ? 07:03, Neil Shephard a ?crit :
>
> > YIHSU CHEN-3 wrote:
> >>
> >> Dear R-user;
> >> I recently switched from PC to MAC.  Is there a compatible editor as
> >> Win-editor with package RWinEdit for MAC?
> >>
> >>
> >
> > I'd recommend using Emacs with ESS (see http://ess.r-project.org/).
> > The
> > advantage of this (beyond the seamless integration) is that its pretty
> > platform neutral, and what you learn on your new Mac system will be
> > portable
> > (i.e. the same method of writing/interacting with your R script/
> > session
> > whether your on Mac/M$-windows/*NIX variant).
>
> I concur. Emacs is one of very few cross-platform editors with a
> special mode for R/S-Plus. If you want to give Emacs a try, I
> recommend Aquamacs (http://aquamacs.org) on OS X. It ships with the
> latest ESS and is better integrated to the OS than a regular Emacs.
>
> In the future, post Mac related question to r-sig-mac, though.
>
> HTH     Vincent
>

I agree that emacs seems to the best editor (especially if one knows
it already).
But there is one _big_ drawback: you cannot plot (interactively) to a
quartz device which looks much nicer than the usual X11 (or any other)
device.

-- 
Armin Goralczyk, M.D.
--
Universit?tsmedizin G?ttingen
Abteilung Allgemein- und Viszeralchirurgie
Rudolf-Koch-Str. 40
39099 G?ttingen
--
Dept. of General Surgery
University of G?ttingen
G?ttingen, Germany
--
http://www.chirurgie-goettingen.de

From ligges at statistik.uni-dortmund.de  Wed Dec 12 14:14:25 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 12 Dec 2007 14:14:25 +0100
Subject: [R] Using predict()?
In-Reply-To: <2E8AE992B157C0409B18D0225D0B476306CD9237@XCH-VN01.sph.ad.jhsph.edu>
References: <2E8AE992B157C0409B18D0225D0B476306CD9237@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <475FDEB1.7020402@statistik.uni-dortmund.de>



Zembower, Kevin wrote:
> I'm trying to solve a homework problem using R. The problem gives a list
> of cricket chirps per second and corresponding temperature, and asks to
> give the equation for the linear model and then predict the temperature
> to produce 18 chirps per second. So far, I have:
> 
>> # Homework 11.2.1 and 11.3.3
>> chirps <- scan()
> 1: 20
> 2: 16
> 3: 19.8
> 4: 18.4
> 5: 17.1
> 6: 15.5
> 7: 14.7
> 8: 17.1
> 9: 15.4
> 10: 16.2
> 11: 15
> 12: 17.2
> 13: 16
> 14: 17
> 15: 14.4
> 16: 
> Read 15 items
>> temp <- scan()
> 1: 88.6
> 2: 71.6
> 3: 93.3
> 4: 84.3
> 5: 80.6
> 6: 75.2
> 7: 69.7
> 8: 82
> 9: 69.4
> 10: 83.3
> 11: 79.6
> 12: 82.5
> 13: 80.6
> 14: 83.5
> 15: 76.3
> 16: 
> Read 15 items
>> chirps
>  [1] 20.0 16.0 19.8 18.4 17.1 15.5 14.7 17.1 15.4 16.2 15.0 17.2 16.0
> 17.0 14.4
>> temp
>  [1] 88.6 71.6 93.3 84.3 80.6 75.2 69.7 82.0 69.4 83.3 79.6 82.5 80.6
> 83.5 76.3
>> chirps.res <- lm(chirps ~ temp)
>> summary(chirps.res)
> 
> Call:
> lm(formula = chirps ~ temp)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max 
> -1.56146 -0.58088  0.02972  0.58807  1.53047 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)    
> (Intercept) -0.31433    3.10963  -0.101 0.921028    
> temp         0.21201    0.03873   5.474 0.000107 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Residual standard error: 0.9715 on 13 degrees of freedom
> Multiple R-Squared: 0.6975,     Adjusted R-squared: 0.6742 
> F-statistic: 29.97 on 1 and 13 DF,  p-value: 0.0001067
>> # From the linear model summary output above, the equation for the
> least squares line is:
>> #    y = -0.3143 + 0.2120*x or chirps = -0.3143 + 0.2120*temp
>>
> 
> I can then determine the answer to the prediction, using algebra and R:
>> pred_temp <- (18+0.3143)/0.2120
>> pred_temp
> [1] 86.3882
> 
> However, I'd like to try to use the predict() function. Since 'chirps'
> and 'temp' are just vectors of numbers, and not dataframes, these
> failed:
> predict(chirps.res, newdata=data.frame(chirp=18))
> predict(chirps.res, newdata="chirp=18")
> predict(chirps.res, newdata=18)
> 


Well, "chirps" (not "chirp", BTW) was your *response* in the lm() call!
Your new data has to be called "temp", otherwise either the regression 
did not make sense or you are confusing different things.

Best,
Uwe Ligges


> I then tried to turn my two vectors into a dataframe. I would have bet
> money that this would have worked, but it didn't:
>> df <- data.frame(chirps, temp)
>>  chirps.res <- lm(chirps ~ temp, data=df)
>> predict(chirps.res, newdata=data.frame(chirps=18))
> 
> Can anyone tell me how to use predict() in this circumstance?
> 
> Thanks for your help and advice.
> 
> -Kevin
> 
> Kevin Zembower
> Internet Services Group manager
> Center for Communication Programs
> Bloomberg School of Public Health
> Johns Hopkins University
> 111 Market Place, Suite 310
> Baltimore, Maryland  21202
> 410-659-6139 
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gavin.simpson at ucl.ac.uk  Wed Dec 12 14:12:49 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 12 Dec 2007 13:12:49 +0000
Subject: [R] Getting error message using R: Please help (it's	coursework
	due in on Friday)
In-Reply-To: <14294733.post@talk.nabble.com>
References: <14291568.post@talk.nabble.com> <475FB489.1020009@biostat.ku.dk>
	<14292665.post@talk.nabble.com> <475FC53A.9090903@biostat.ku.dk>
	<14294474.post@talk.nabble.com> <475FDAFD.7030208@biostat.ku.dk>
	<14294733.post@talk.nabble.com>
Message-ID: <1197465169.9595.62.camel@prometheus.geog.ucl.ac.uk>


On Wed, 2007-12-12 at 05:03 -0800, ethanhaas wrote:
> I was, and the answer is yes, but I figured the easiest way for you to
> understand is for me to say exactly what I am trying to do. Perhaps I don't
> even need to decompose (although I think I do). If I can get past this
> problem it would make everything else so much easier.

Peter was trying to get you to see that you should be doing one of the
following:

dcm <- decompose(timeseries)

or

dcm <- decompose(using)

*Not* decompose(information), which will try to decompose your data
frame, which is not a timeseries object in the sense of ?ts and
therefore doesn't have periods and the like, hence the error.

Do either of the two suggestions that Peter clearly suggested you try
(and I repeat above) work for you and give you what you want?

HTH

G

> 
> 
> 
> P.Dalgaard wrote:
> > 
> > ethanhaas wrote:
> >> What I'm trying to do is have plots and summary statistics of x
> >> (autocorrelations for example) where x refers to the data in using, and I
> >> thought decomposing would be the way to go about it.
> >>
> >>
> >>   
> > You are not listening. I give up.
> > 
> >> P.Dalgaard wrote:
> >>   
> >>> ethanhaas wrote:
> >>>     
> >>>> I simply called it information but I can also call it data if you wish,
> >>>> it
> >>>> doesn't make a difference.
> >>>> The timeseries is simply the time series of this data between the two
> >>>> dates
> >>>> and using is the data up to 1990 as opposed to 1994.
> >>>>
> >>>>   
> >>>>       
> >>> And isn't it one of the latter two you want to decompose()?
> >>>
> >>>     
> >>>> P.Dalgaard wrote:
> >>>>   
> >>>>       
> >>>>> ethanhaas wrote:
> >>>>>     
> >>>>>         
> >>>>>> I've been trying for the past 3 weeks to use R (much better than
> >>>>>> Matlab
> >>>>>> but I
> >>>>>> am very bad with computers so I am very new to all of this) and know
> >>>>>> how
> >>>>>> to
> >>>>>> input the data (hey, it's a start!) but every time I type in the
> >>>>>> following:
> >>>>>>
> >>>>>> dcm <- decompose(information)
> >>>>>>
> >>>>>> I get the following error message:
> >>>>>>
> >>>>>> Error in decompose(information) : time series has no or less than 3
> >>>>>> periods
> >>>>>>
> >>>>>> Could you please aid me so that I can progress to the next step. What
> >>>>>> I
> >>>>>> am
> >>>>>> in fact trying to do at the moment is plot a graph of the information
> >>>>>> so
> >>>>>> I
> >>>>>> had assumed it would be:
> >>>>>>
> >>>>>> dcm <- decompose(information)
> >>>>>> Plot(dcm$trend)
> >>>>>>
> >>>>>> but obviously it does not appear to be the case.
> >>>>>>
> >>>>>> By the way the code I have right now in case it helps is:
> >>>>>>
> >>>>>> information <- scan("everything.txt")
> >>>>>> timeseries <- ts(information, frequency = 11, start = c(1970, 1994))
> >>>>>> using <- window(timeseries, end = c(1990, 11))
> >>>>>> dcm <- decompose(information)
> >>>>>> Plot(dcm$trend)
> >>>>>>
> >>>>>>   
> >>>>>>       
> >>>>>>           
> >>>>> Er, are you sure it is "information" that you want to decompose()? (If
> >>>>> so, what are the computations of "timeseries" and "using" supposed to
> >>>>> be
> >>>>> good for?)
> >>>>>
> >>>>>     
> >>>>>         
> >>>>>> Thank you very much in advance, I really appreciate the help.
> >>>>>>   
> >>>>>>       
> >>>>>>           
> >>>>> -- 
> >>>>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
> >>>>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >>>>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> >>>>> 35327918
> >>>>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> >>>>> 35327907
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>>
> >>>>>     
> >>>>>         
> >>>>   
> >>>>       
> >>> -- 
> >>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
> >>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> >>> 35327918
> >>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> >>> 35327907
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >>>     
> >>
> >>   
> > 
> > 
> > -- 
> >    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> > 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> > 35327907
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ligges at statistik.uni-dortmund.de  Wed Dec 12 14:16:46 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 12 Dec 2007 14:16:46 +0100
Subject: [R] tm package - how to transform a TermDocMatrix to a
	data.frame
In-Reply-To: <1c285b0b0712111643y1f88a8a3if331baf46cd385ca@mail.gmail.com>
References: <1c285b0b0712111643y1f88a8a3if331baf46cd385ca@mail.gmail.com>
Message-ID: <475FDF3E.90403@statistik.uni-dortmund.de>



Filipe Almeida wrote:
> Hi to all,
> 
> I'm using the tm package in a Windows machine.
> 
> This is my sample:
>> tts1 <- TermDocMatrix(tts, weighting="tf-idf")
>> typeof(tts1)
> [1] "S4"
> 
> How can i transform or put the tts1 TermDocMatrix in a simple data.frame or
> simple matrix. I need to compute some functions. For example, I need to
> calculate the euclidian distance:
>> dist(tts1dist(x, method = "euclidean")


x <- as.matrix(tts1 at Data)

Uwe Ligges


> Thanks in advance!
> 
> Filipe Almeida
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From HDoran at air.org  Wed Dec 12 14:27:27 2007
From: HDoran at air.org (Doran, Harold)
Date: Wed, 12 Dec 2007 08:27:27 -0500
Subject: [R] IRT Likelihood problem
Message-ID: <2323A6D37908A847A7C32F1E3662C80E012DEBD4@dc1ex01.air.org>

I have the following item response theory (IRT) likelihood that I want
to maximize w.r.t. to theta (student ability).

L(\theta) = \prod(p(x))

Where p(x) is the 3-parameter logistic model when items are scored
dichotomously (x_{ij} = 0 or 1) and p(x) is Muraki's generalized partial
credit model when items are scored polytomously (x_{ij} = 0 \ldots J).

Now, I wrote the following two functions to maximize the likelihood. The
first one is for the 3PL and works when all items are scored
dichotmously. The second is for the GPCM and gives the MLE when all
items are scored polytomously.

In the code below, this requires that I first have in hand estimates of
the item parameters so that the maximization is only w.r.t. theta. 

# x = response pattern
# b = vector of location parameters
# a = vector of discrimination parameters 
# c = vector of guessing paramete
theta.3pl <- function(x, b, a, c){
 opt <- function(theta) -sum(dbinom(x, 1, c + ((1-c)/(1 +
exp(-1.7*a*(theta - b)))),  log = TRUE))
 start_val <- log(sum(x)/(length(x)/sum(x)))
 out <- optim(start_val , opt, method = "BFGS", hessian = TRUE)
 out$par
 }

# score = the category the student scored in for item i
# d     = the item parameters for the ith item 
# a     = the discrimination p
# Muraki's GPCM
pcm.max <- function(score, d, a){
    pcm <- function(theta, d, score, a)
exp(sum(a*(theta-d[1:score])))/sum(exp(cumsum(a*(theta-d))))
    opt <- function(theta) -sum(log(mapply(pcm, d, theta = theta, score=
score )))
    start_val <- log(sum(score-1)/(length(score-1)/sum(score-1)))
    out <- optim(start_val, opt, method = "BFGS", hessian = TRUE)
    round(out$par, 2)
  }

However, I have data for which there is a mixture of item types. Some
are dichotomous and others are polytomous. Therefore, I need to somehow
modify these functions to work in a conditional statement that first
evaluates whether the item is dichotomous or not and then uses the right
function to write out and then maximize the likelihood. However, I'm a
bit stumped on how I might code this. Can anyone suggest how that might
work?

For example, assume I have a test consisting of 3 items. The first two
are dichotomous and the last is polytomous. Assume the students score on
these three items is:

x <- c(1,0,3) # that is, 'right', 'wrong', 'scored in category 3'

And further assume the item parameters for these items are

Item 1 c = .11, b = 1.2, a = .58
Item 2 c = .20, b = .65, a = 1.2
Item 3 d = (0, -1.4, -.28, .95)

Now, my function pcm.max also reduces to Master's partial credit model
when a = 1 for all items. And, because Master's partial credit reduces
to the Rasch model when items are scored dichotmously, the only function
I need is pcm.max when working within the Rasch family of models.
However, Muraki's model does not reduce to the 3PL because of the
guessing parameter when items are scored dichotomously, so I think the
problem is slightly more complex in this scenario and requires the use
of both functions.

Last, and a bit tangentially, it is possible when maximizing the 3PL
that I will converge upon a local and not global maximum, a situation
that does not occur with Rasch or 2PL. I only know of two methods for
knowing whether I converged upon the right max. First, plot the
likelihood and look at it or second, use different starting points and
see if I converge to the same max. However, I am maximizing this
likelihood over 80,000 students (or so) and I don't think either of
these two methods are viable. If anyone has a suggestion on how I could
proceed thoughtfully on that issue I welcome that as well.

Many thanks,
Harold


From h.wickham at gmail.com  Wed Dec 12 14:27:40 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 12 Dec 2007 07:27:40 -0600
Subject: [R] book on regular expressions
In-Reply-To: <Pine.LNX.4.64.0712121253190.3901@gannet.stats.ox.ac.uk>
References: <s75fd735.098@tedmail.lgc.co.uk>
	<Pine.LNX.4.64.0712121253190.3901@gannet.stats.ox.ac.uk>
Message-ID: <f8e6ff050712120527h7889e8c9r7039b232c282fb16@mail.gmail.com>

> I don't think anyone has mentioned the references given on the help page
> ?regexp: they are a great deal more reliable than some of the third-party
> write-ups that have been mentioned.

They are good technical references, but I don't think they're very
helpful for someone trying to learn regular expressions.  For example,
mastering regular expressions takes a very task based approach which
is helpful if you want to learn how to match an email address or url
correctly with a regular expression (harder than most people think),
or to match paired brackets.

Hadley

-- 
http://had.co.nz/


From ethanhaaswaswrong at gmail.com  Wed Dec 12 14:33:22 2007
From: ethanhaaswaswrong at gmail.com (ethanhaas)
Date: Wed, 12 Dec 2007 05:33:22 -0800 (PST)
Subject: [R] Getting error message using R: Please help (it's	coursework
 due in on Friday)
In-Reply-To: <1197465169.9595.62.camel@prometheus.geog.ucl.ac.uk>
References: <14291568.post@talk.nabble.com> <475FB489.1020009@biostat.ku.dk>
	<14292665.post@talk.nabble.com> <475FC53A.9090903@biostat.ku.dk>
	<14294474.post@talk.nabble.com> <475FDAFD.7030208@biostat.ku.dk>
	<14294733.post@talk.nabble.com>
	<1197465169.9595.62.camel@prometheus.geog.ucl.ac.uk>
Message-ID: <14295127.post@talk.nabble.com>


Although I was wrong on what I was trying to decompose that didn't work
either however after a conversation with the lecturer the problem has been
found and solved.


Gavin Simpson wrote:
> 
> 
> On Wed, 2007-12-12 at 05:03 -0800, ethanhaas wrote:
>> I was, and the answer is yes, but I figured the easiest way for you to
>> understand is for me to say exactly what I am trying to do. Perhaps I
>> don't
>> even need to decompose (although I think I do). If I can get past this
>> problem it would make everything else so much easier.
> 
> Peter was trying to get you to see that you should be doing one of the
> following:
> 
> dcm <- decompose(timeseries)
> 
> or
> 
> dcm <- decompose(using)
> 
> *Not* decompose(information), which will try to decompose your data
> frame, which is not a timeseries object in the sense of ?ts and
> therefore doesn't have periods and the like, hence the error.
> 
> Do either of the two suggestions that Peter clearly suggested you try
> (and I repeat above) work for you and give you what you want?
> 
> HTH
> 
> G
> 
>> 
>> 
>> 
>> P.Dalgaard wrote:
>> > 
>> > ethanhaas wrote:
>> >> What I'm trying to do is have plots and summary statistics of x
>> >> (autocorrelations for example) where x refers to the data in using,
>> and I
>> >> thought decomposing would be the way to go about it.
>> >>
>> >>
>> >>   
>> > You are not listening. I give up.
>> > 
>> >> P.Dalgaard wrote:
>> >>   
>> >>> ethanhaas wrote:
>> >>>     
>> >>>> I simply called it information but I can also call it data if you
>> wish,
>> >>>> it
>> >>>> doesn't make a difference.
>> >>>> The timeseries is simply the time series of this data between the
>> two
>> >>>> dates
>> >>>> and using is the data up to 1990 as opposed to 1994.
>> >>>>
>> >>>>   
>> >>>>       
>> >>> And isn't it one of the latter two you want to decompose()?
>> >>>
>> >>>     
>> >>>> P.Dalgaard wrote:
>> >>>>   
>> >>>>       
>> >>>>> ethanhaas wrote:
>> >>>>>     
>> >>>>>         
>> >>>>>> I've been trying for the past 3 weeks to use R (much better than
>> >>>>>> Matlab
>> >>>>>> but I
>> >>>>>> am very bad with computers so I am very new to all of this) and
>> know
>> >>>>>> how
>> >>>>>> to
>> >>>>>> input the data (hey, it's a start!) but every time I type in the
>> >>>>>> following:
>> >>>>>>
>> >>>>>> dcm <- decompose(information)
>> >>>>>>
>> >>>>>> I get the following error message:
>> >>>>>>
>> >>>>>> Error in decompose(information) : time series has no or less than
>> 3
>> >>>>>> periods
>> >>>>>>
>> >>>>>> Could you please aid me so that I can progress to the next step.
>> What
>> >>>>>> I
>> >>>>>> am
>> >>>>>> in fact trying to do at the moment is plot a graph of the
>> information
>> >>>>>> so
>> >>>>>> I
>> >>>>>> had assumed it would be:
>> >>>>>>
>> >>>>>> dcm <- decompose(information)
>> >>>>>> Plot(dcm$trend)
>> >>>>>>
>> >>>>>> but obviously it does not appear to be the case.
>> >>>>>>
>> >>>>>> By the way the code I have right now in case it helps is:
>> >>>>>>
>> >>>>>> information <- scan("everything.txt")
>> >>>>>> timeseries <- ts(information, frequency = 11, start = c(1970,
>> 1994))
>> >>>>>> using <- window(timeseries, end = c(1990, 11))
>> >>>>>> dcm <- decompose(information)
>> >>>>>> Plot(dcm$trend)
>> >>>>>>
>> >>>>>>   
>> >>>>>>       
>> >>>>>>           
>> >>>>> Er, are you sure it is "information" that you want to decompose()?
>> (If
>> >>>>> so, what are the computations of "timeseries" and "using" supposed
>> to
>> >>>>> be
>> >>>>> good for?)
>> >>>>>
>> >>>>>     
>> >>>>>         
>> >>>>>> Thank you very much in advance, I really appreciate the help.
>> >>>>>>   
>> >>>>>>       
>> >>>>>>           
>> >>>>> -- 
>> >>>>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5,
>> Entr.B
>> >>>>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>> >>>>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>> >>>>> 35327918
>> >>>>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>> >>>>> 35327907
>> >>>>>
>> >>>>> ______________________________________________
>> >>>>> R-help at r-project.org mailing list
>> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>> PLEASE do read the posting guide
>> >>>>> http://www.R-project.org/posting-guide.html
>> >>>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>>>
>> >>>>>
>> >>>>>     
>> >>>>>         
>> >>>>   
>> >>>>       
>> >>> -- 
>> >>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>> >>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>> >>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>> >>> 35327918
>> >>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>> >>> 35327907
>> >>>
>> >>> ______________________________________________
>> >>> R-help at r-project.org mailing list
>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> PLEASE do read the posting guide
>> >>> http://www.R-project.org/posting-guide.html
>> >>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >>>
>> >>>     
>> >>
>> >>   
>> > 
>> > 
>> > -- 
>> >    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>> >  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>> > 35327918
>> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>> > 35327907
>> > 
>> > ______________________________________________
>> > R-help at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> > 
>> > 
>> 
> -- 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
>  ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
>  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
>  UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Getting-error-message-using-R%3A-Please-help-%28it%27s-coursework-due-in-on-Friday%29-tp14291568p14295127.html
Sent from the R help mailing list archive at Nabble.com.


From vistocco at unicas.it  Wed Dec 12 14:55:29 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Wed, 12 Dec 2007 14:55:29 +0100
Subject: [R] Measure of agreement??
In-Reply-To: <44985.91.140.49.185.1197464769.squirrel@mail.upatras.gr>
References: <2BAF2D3C41D1274E9228E63287F19B7E4FD4B8@mailsrv2.loginmpa.mpa.se>
	<44985.91.140.49.185.1197464769.squirrel@mail.upatras.gr>
Message-ID: <475FE851.3080703@unicas.it>

There is an agreement plot (and a relative measure) in the vcd package.
Maybe it could be useful.

domenico

Ioannis Dimakos wrote:
> Patrik,
>
>   
>> help.search("kappa")
>>     
>
> is a good place to start.  Alternatively,
>
>   
>> RSiteSearch("measure of agreement")
>>     
>
> provides helpful tips.
>
> HTH,
>
> I.
>
> =================================
> On ???, ?????????? 12, 2007 14:19, ?hagen Patrik wrote:
>   
>> Dear List,
>>
>> Please put me on the right track. I have been searching for the R
>> functions for measures of agreement. Where should I look?
>>
>> Patrik ?hagen
>> L?kemedelsverket
>> Box 26
>> 751 03 Uppsala
>> Tel. +46(0)18-174924
>> patrik.ohagen at mpa.se
>>
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
>
>


From corey.sparks at utsa.edu  Wed Dec 12 14:59:05 2007
From: corey.sparks at utsa.edu (Corey Sparks)
Date: Wed, 12 Dec 2007 07:59:05 -0600
Subject: [R] Importing Large Dataset into Excel
Message-ID: <2768A5B569B1D54EA47861B9A05422E19B4329@jade1604.UTSARR.NET>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/e34164a1/attachment.pl 

From michael.lundholm at ne.su.se  Wed Dec 12 15:07:30 2007
From: michael.lundholm at ne.su.se (Michael Lundholm)
Date: Wed, 12 Dec 2007 15:07:30 +0100
Subject: [R] OS-dependent behaviour of strucchange?
Message-ID: <475FEB22.6010002@ne.su.se>

Using the following code:

library(strucchange)
load(file="y.rda")
ar1<-formula(y~lag(y,k=-1))
plot(Fstats(ar1))

(where the the data file can be downloaded from
www.ne.su.se/~mlu/downloads/y.rda) I have a problem replicating
identical plots on different implementations of R for different
operating systems; I get completely different results under Debian Linux
compared to Windows XP.

I have tested three different Windows XP desktops running R 2.6.1 (one)
or 2.6.0 (two) and two different Debian Linux desktops running R 2.6.1. 
In all cases strucchange was version 1.3-2. The Windows desktops all
produced the same result (see the file windows.ps available at the same
address as the data). The Linux desktops also all produced identical
results (the file linux.ps also on the same address) although this was
different from that produced by the Windows desktops. Testing the
different desktops we made sure that we used exactly the same code and
data files.

Completely lost about the possible cause and therefore grateful for any
advice,

/Michael Lundholm

-- 
Docent Michael Lundholm
http://www.ne.su.se/~mlu

Reclaim Your Inbox!
http://www.mozilla.org/products/thunderbird


From Krustev at hotmail.com  Mon Dec 10 18:36:25 2007
From: Krustev at hotmail.com (Teodor Krastev)
Date: Mon, 10 Dec 2007 12:36:25 -0500
Subject: [R] XML R function description
Message-ID: <BAY120-DAV9DDB82E8DAF6A3A520020B76B0@phx.gbl>

Hi,

I was wondering if there is a standard for R function description in XML (or 
any plain text) format.

Rd files are the closest thing I found, but they do not describe the 
argument (or return value) types.
The purpose would be to write a program to automate the creation of C# 
wrapper around any R function, but without the argument type Rd descriptions 
are useless.

Any clues...

thank you
Teodor Krastev


From lobry at biomserv.univ-lyon1.fr  Tue Dec 11 22:27:56 2007
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Tue, 11 Dec 2007 22:27:56 +0100
Subject: [R] [OT] vernacular names for circular diagrams
In-Reply-To: <mailman.19.1197370804.9403.r-help@r-project.org>
References: <mailman.19.1197370804.9403.r-help@r-project.org>
Message-ID: <p06002019c384ae89478c@[192.168.1.10]>

Dear useRs,

by a circular diagram representation I mean what you will get by entering
this at your R promt:

pie(1:5)

Nice to have R as a lingua franca :-)

The folowing quote is from page 360 in this very interesting paper:

@article{SpenceI2005,
     title = {No Humble Pie: The Origins and Usage of a Statistical Chart},
     author = {Spence, I.},
     journal = {Journal of Educational and Behavioral Statistics},
     volume = {30},
     pages = {353-368},
     year = {2005}
}

QUOTE
Like us, the French employ a gastronomical metaphor when
they refer to Playfair's pie chart, but they have preferred
instead to invoke the name of the wonderful round soft
cheese from Normandy - the camembert. When I spent 4 months
in Paris a few years ago, a friend invited my wife and me to
lunch with her elderly father who lives in Rouen, Normandy,
about an hour North of Paris. Her father inquired -
coincidentally during the cheese course - what work I was
doing in Paris; I replied that I was researching the
activities of a Scot, William Playfair, during the
revolutionary period. I told him that Playfair had invented
several statistical graphs, including the pie chart, which I
referred to, in French, as <<le camembert.>> After a stunned
silence of perhaps a couple of seconds, the distinguished
elderly gentleman looked me in the eye and exclaimed, <<Mon
Dieu ! Notre camembert?>>
UNQUOTE

So, I'm just curious: how do you refer in your own language to
this kind of graphic? How do you call it?

Best,

Jean

-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 27 56     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From shubhak at ambaresearch.com  Wed Dec 12 13:16:15 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Wed, 12 Dec 2007 17:46:15 +0530
Subject: [R] iid.test package/ncdf package
Message-ID: <A36876D3F8A5734FA84A4338135E7CC302DF3BF9@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/453741de/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Wed Dec 12 15:33:45 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 12 Dec 2007 15:33:45 +0100
Subject: [R] [OT] vernacular names for circular diagrams
In-Reply-To: <p06002019c384ae89478c@[192.168.1.10]>
References: <mailman.19.1197370804.9403.r-help@r-project.org>
	<p06002019c384ae89478c@[192.168.1.10]>
Message-ID: <475FF149.20103@biostat.ku.dk>

Jean lobry wrote:
> Dear useRs,
>
> by a circular diagram representation I mean what you will get by entering
> this at your R promt:
>
> pie(1:5)
>
> Nice to have R as a lingua franca :-)
>
> The folowing quote is from page 360 in this very interesting paper:
>
> @article{SpenceI2005,
>      title = {No Humble Pie: The Origins and Usage of a Statistical Chart},
>      author = {Spence, I.},
>      journal = {Journal of Educational and Behavioral Statistics},
>      volume = {30},
>      pages = {353-368},
>      year = {2005}
> }
>
> QUOTE
> Like us, the French employ a gastronomical metaphor when
> they refer to Playfair's pie chart, but they have preferred
> instead to invoke the name of the wonderful round soft
> cheese from Normandy - the camembert. When I spent 4 months
> in Paris a few years ago, a friend invited my wife and me to
> lunch with her elderly father who lives in Rouen, Normandy,
> about an hour North of Paris. Her father inquired -
> coincidentally during the cheese course - what work I was
> doing in Paris; I replied that I was researching the
> activities of a Scot, William Playfair, during the
> revolutionary period. I told him that Playfair had invented
> several statistical graphs, including the pie chart, which I
> referred to, in French, as <<le camembert.>> After a stunned
> silence of perhaps a couple of seconds, the distinguished
> elderly gentleman looked me in the eye and exclaimed, <<Mon
> Dieu ! Notre camembert?>>
> UNQUOTE
>
> So, I'm just curious: how do you refer in your own language to
> this kind of graphic? How do you call it?
>
> Best,
>
> Jean
>
>   
<Grin>

In Danish it is "Lagkagediagram" as in the layer cakes that are
traditional at birthday parties (and thrown at eachother's faces in
slapstick comedy).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Dan.Kelley at dal.ca  Wed Dec 12 15:41:31 2007
From: Dan.Kelley at dal.ca (dankelley)
Date: Wed, 12 Dec 2007 06:41:31 -0800 (PST)
Subject: [R]  how to shorten elements in a data frame?
Message-ID: <14296509.post@talk.nabble.com>


I have a data frame ("res$data" in the example below), and I'd like to walk
through it, doing an approx() to interpolate each element in turn based on
the value of the item named "pressure" that is in the data.frame.

I've put some code below.  I had hoped that the commented-out "length() <-"
line would let me adjust the lengths of the items, but that has no effect,
since I get an error that the replacement has 2 rows (correct, as I'm
testing this), although the data has 87 rows (again, that's the correct
value for the field upon which I'm doing the approx() interpolation.)

Q: is there a way I can change the length of an element in a data frame, so
that I can replace straight into it? 

PS. the code doesn't know the names of the items in the data frame in
advance, although it is certain that one of them is called "pressure". 
(This is an oceanographic application, in case anyone wonders why pressure
would be an independent variable for use in interpolation.)


<code>

#interpolate to target pressures in "pt".
npt <- length(pt)
for (datum.name in data.names) {
    if (datum.name != "pressure") {
        length(res$data[[datum.name]]) <- npt
        res$data[[datum.name]] <- approx(x$data[["pressure"]],
x$data[[datum.name]], pt)$y
    }
}

</code>

-- 
View this message in context: http://www.nabble.com/how-to-shorten-elements-in-a-data-frame--tp14296509p14296509.html
Sent from the R help mailing list archive at Nabble.com.


From mark_difford at yahoo.co.uk  Wed Dec 12 15:46:22 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Wed, 12 Dec 2007 06:46:22 -0800 (PST)
Subject: [R] lm/model.matrix confusion (? bug)
In-Reply-To: <20071212183632.22155700@berwin-nus1>
References: <14292188.post@talk.nabble.com>
	<20071212183632.22155700@berwin-nus1>
Message-ID: <14296513.post@talk.nabble.com>


Hi Gavin, Berwin,

Thanks for your detailed replies.  I'll make a general reply, if you don't
mind.

To reiterate, my main point is that if model.matrix() can be used in this
way, then lm() shouldn't add an intercept.

>> ... lm(Gas ~ model.matrix(~ Insul/Temp - 1), data = whiteside) ....

And the documentation for lm() indicates. albeit indirectly, that
model.matrix can be used in this way.  It calls for a formula, or something
that can be coerced to one.  And the following meets that criterion:
as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside)), and this
specifies no intercept.

On the question of why I want to mess about in such a labarynthine way. 
Well my email was largely expository.  With a straight call to lm(), I
wouldn't bother with model.matrix.

So, it really was about getting at lm coefficients inside a function (when
you have to get the terms &c. from somewhere else), and trying to understand
properly how things work, and why they work the way they do, and even if
they should work the way they do.

For instance:---

    if (ols) {
        obj <- x[[1]]
        mt <- terms(obj)
        mf <- model.frame(obj)
        y <- model.response(mf)
        X <- model.matrix(mt, mf, contrasts = obj$contrasts)
        if (attr(mt, "intercept") == 1)                    ## This is my
hack to overcome the double-intercept problem
            { olscf <- summary(lm(y ~ X))$coefficients }
        else {
            olscf <- summary(lm(y ~ X - 1))$coefficients
        }
        rownames(olscf) <- rownames(coef(obj))

Thanks again for your input.

Regards,
Mark.





Berwin A Turlach wrote:
> 
> G'day Mark,
> 
> On Wed, 12 Dec 2007 02:05:54 -0800 (PST)
> Mark Difford <mark_difford at yahoo.co.uk> wrote:
> 
>> In order to get the same coefficients as we get from the following
> [...] 
>> we need to do the following (if we use model.matrix to specify the
>> model)
> 
> By why would you want to do this?
> 
>> ##
>> summary ( lm(Gas ~ model.matrix(~ Insul/Temp - 1) - 1, data =
>> whiteside) )
>> 
>> That is, we need to take out "two intercepts."  Is this "correct"?
> 
> Yes.
>  
>> Shouldn't lm check to see if an intercept has been requested as part
>> of the model formula?
> 
> No, it does not.  In the Details section of lm's help page you will
> find the following:
> 
>      A formula has an implied intercept term.  To remove this use
>      either 'y ~ x - 1' or 'y ~ 0 + x'.  See 'formula' for more details
>      of allowed formulae.
> 
> Thus, except if you explicitly ask for a constant term not be included,
> lm will add a constant term (a column of ones) additionally to what
> ever you have specified on the right hand side of the formula.
> 
>> If I do
>> ##
>> summary(lm(as.formula(Gas ~ model.matrix (~ Insul/Temp-1,
>> data=whiteside)), data=whiteside))
>> 
>> we get a strange model.  
> 
> Well, you get a model in which not all parameters are identifiable, and
> a particular parameter that is not identifiable is estimated by NA.  I
> am not sure what is strange about this.
> 
>> But the formula part of this model qualifies
>> as a valid formula
>> ##
>> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside))
> 
> Debatable, the above command only shows that it can be coerced into a
> valid formula. :)
> 
>> just as if I were to write: lm(Gas ~ Insul/Temp - 1, data=whiteside)
>> 
>> But we know that the _correct_ formula is the following
>  
>> ##
>> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside) -1)
> 
> Why is this formula any more correct than the other one?  Both specify
> exactly the same model.  It is just that one does it in an
> overparameterised way.
> 
>> (Sorry, this is getting really long) --- So, my question/confusion
>> comes down to wanting to know why lm() doesn't check to see if an
>> intercept has been specified when the model has been specified using
>> model.matrix.
> 
> Because lm() is documented not to check this.  If you do not want to
> have an intercept in the model you have to specifically ask it for.
> 
> Also, comparing the output of 
> 	summary( lm(Gas ~ Insul/Temp - 1, data = whiteside) )
> and
> 	summary( lm(Gas ~ Insul/Temp, data = whiteside ) )
> 
> you can see that lm() does not check whether there is an implicit
> intercept in the model.  Compare the (Adjusted) R-squared values
> returned; one case is using the formula for models with no intercept
> the other one the formula for models with intercept.  Similar story
> with the reported F-statistics.  
> 
> Cheers,
> 
> 	Berwin
> 
> =========================== Full address =============================
> Berwin A Turlach                            Tel.: +65 6515 4416 (secr)
> Dept of Statistics and Applied Probability        +65 6515 6650 (self)
> Faculty of Science                          FAX : +65 6872 3919       
> National University of Singapore     
> 6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
> Singapore 117546                    http://www.stat.nus.edu.sg/~statba
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/lm-model.matrix-confusion-%28--bug%29-tp14292188p14296513.html
Sent from the R help mailing list archive at Nabble.com.


From ajung at gfz-potsdam.de  Wed Dec 12 16:00:37 2007
From: ajung at gfz-potsdam.de (Andre Jung)
Date: Wed, 12 Dec 2007 16:00:37 +0100
Subject: [R] combine variables to matrix
Message-ID: <475FF795.1000601@gfz-potsdam.de>

I just got stuck with a quite simple question. I've just read in an 
ASCII table from a plain text file with read.table(). It's a 1200x1200 
table. R has assigned variables for each column: V1,V2,V3,V4,...
For small data sets

data <- read.table("data.txt");
data.matrix <- cbind(V1,V2,V3);

works. But how could I put together 1200 columns?

I've searched the R mailing help and stumbled upon this entry:
https://stat.ethz.ch/pipermail/r-help/2007-July/137121.html
which doesn't help me.

thanks for your help.

andre


From lieven.desmet at wis.kuleuven.be  Wed Dec 12 16:04:03 2007
From: lieven.desmet at wis.kuleuven.be (Lieven Desmet)
Date: Wed, 12 Dec 2007 16:04:03 +0100
Subject: [R] discrepancy between periodogram implementations ? per and
	spec.pgram
Message-ID: <475FF863.6030403@wis.kuleuven.be>

hello,

I have been using the per function in package longmemo to obtain a 
simple raw periodogram.
I am considering to switch to the function spec.pgram since I want to be 
able to do tapering.
To compare both I used spec.pgram with the options as suggested in the 
documentation of per {longmemo} to make them correspond.

Now I have found on a variety of examples that there is a shift between 
the log of the periodogram with per and that with spec.pgram. This 
vertical shift amounts to  approx. 1.8  on the log scale  (the graph of 
spec.pgram being above the one from per).

Is there some explanation for this ? Is the one from spec.pgram the 
better one as suggested in the documentation of per {longmemo}? Finally 
how are these related to an estimate of the spectral density obtained 
from spec.arima ?

Many thanks for help and clarification.

Lieven Desmet


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ngottlieb at marinercapital.com  Wed Dec 12 16:18:20 2007
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Wed, 12 Dec 2007 10:18:20 -0500
Subject: [R] book on regular expressions
In-Reply-To: <f8e6ff050712120527h7889e8c9r7039b232c282fb16@mail.gmail.com>
References: <s75fd735.098@tedmail.lgc.co.uk><Pine.LNX.4.64.0712121253190.3901@gannet.stats.ox.ac.uk>
	<f8e6ff050712120527h7889e8c9r7039b232c282fb16@mail.gmail.com>
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D880151E474@500MAIL.goldbox.com>

I recommend the following Book on regular expression matching: 

"Mastering Regular Expressions" by Jeffrey Freidl
Publisher: O'Reilly
ISBN:0-596-52812-4

Based on the my "blood, sweat and tears" of using regex type patterns
which can be very obfuscated, in addition to reading
the syntax from the above book there is some very good software
tools to test the patterns:

See these link:
http://regexlib.com/default.aspx

http://www.regular-expressions.info/

Finally in the software spirit of R and Shareware an invaluable too to
Test expressions is the software:
1-Expresso  http://www.ultrapico.com/
2-Regexdesigner.Net http://www.sellsbrothers.com/

Hope this Helps...
Neil

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of hadley wickham
Sent: Wednesday, December 12, 2007 8:28 AM
To: Prof Brian Ripley
Cc: r-help at r-project.org; S Ellison
Subject: Re: [R] book on regular expressions

> I don't think anyone has mentioned the references given on the help 
> page
> ?regexp: they are a great deal more reliable than some of the 
> third-party write-ups that have been mentioned.

They are good technical references, but I don't think they're very
helpful for someone trying to learn regular expressions.  For example,
mastering regular expressions takes a very task based approach which is
helpful if you want to learn how to match an email address or url
correctly with a regular expression (harder than most people think), or
to match paired brackets.

Hadley

--
http://had.co.nz/

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--------------------------------------------------------



This information is being sent at the recipient's reques...{{dropped:16}}


From edgar.merkle at wichita.edu  Wed Dec 12 16:20:30 2007
From: edgar.merkle at wichita.edu (Ed Merkle)
Date: Wed, 12 Dec 2007 09:20:30 -0600
Subject: [R] Sys.setlocale() and text()
In-Reply-To: <971536df0712112146m1905d070x23b34975ed8d944c@mail.gmail.com>
References: <475F50D9.20003@wichita.edu>
	<971536df0712112146m1905d070x23b34975ed8d944c@mail.gmail.com>
Message-ID: <475FFC3E.2020902@wichita.edu>

Thanks very much for the response.  I think I left out an important 
detail, however.

I want my lowercase sigma to be displayed in a specific font from the 
Rdevga file (my project involves fonts).  So far as I know, quote() does 
not allow me to select a font.  Thus, I am specifically interested in 
the text() command and reasons why my example code performs differently 
in R 2.3.0 vs 2.6.1.

Thanks,
Ed


Gabor Grothendieck wrote:
> Try this:
> 
> plot(1:10, main = quote(sigma ^ 2))
> 
> 
> On Dec 11, 2007 10:09 PM, Ed Merkle <edgar.merkle at wichita.edu> wrote:
>> Dear HelpeRs,
>>
>> I have a question about the Sys.setlocale() command and plotting.  I am
>> running Windows XP, with R 2.6.1.  My default locale is English_United
>> States.1252.
>>
>> I am trying to add a lowercase sigma to a plot using the following code:
>>
>> Sys.setlocale("LC_CTYPE","greek")
>> plot(1:10,1:10)
>> text(4,3,"\xF3")
>>
>>
>> For R 2.6.1, this code gives me the glyph from my default (1252) instead
>> of from the 1253 codes.  For an older version of R (2.3.0) on the same
>> computer, this code gives me the lowercase sigma that I wanted.  I have
>> been unable to pinpoint what has changed.  Thanks for the help, and I
>> apologize if I am missing something obvious.
>>
>>
>> --
>> Ed Merkle, PhD
>> Assistant Professor
>> Dept. of Psychology
>> Wichita State University
>> Wichita, KS 67260
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From ngottlieb at marinercapital.com  Wed Dec 12 16:24:48 2007
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Wed, 12 Dec 2007 10:24:48 -0500
Subject: [R] Need good Reference Material and Reading about Gaussian Copulas
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D880151E475@500MAIL.goldbox.com>

Can anyone advise me on some pratical papers or books 
On Gaussian Copulas? Anything in the genre of Copulas Dummies
Would be a help.

As simpe, and approachable with minimal pedantic style.
Thanks,
Neil
--------------------------------------------------------



This information is being sent at the recipient's reques...{{dropped:16}}


From P.Dalgaard at biostat.ku.dk  Wed Dec 12 16:30:19 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 12 Dec 2007 16:30:19 +0100
Subject: [R] combine variables to matrix
In-Reply-To: <475FF795.1000601@gfz-potsdam.de>
References: <475FF795.1000601@gfz-potsdam.de>
Message-ID: <475FFE8B.4000909@biostat.ku.dk>

Andre Jung wrote:
> I just got stuck with a quite simple question. I've just read in an
> ASCII table from a plain text file with read.table(). It's a 1200x1200
> table. R has assigned variables for each column: V1,V2,V3,V4,...
> For small data sets
>
> data <- read.table("data.txt");
> data.matrix <- cbind(V1,V2,V3);
>
as.matrix(data) ?

(or, if you know the dimensions,

M <- matrix(scan("data.text"), 1200, 1200)

)

> works. But how could I put together 1200 columns?
>
> I've searched the R mailing help and stumbled upon this entry:
> https://stat.ethz.ch/pipermail/r-help/2007-July/137121.html
> which doesn't help me.
>
> thanks for your help.
>
> andre
>
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From gavin.simpson at ucl.ac.uk  Wed Dec 12 16:33:05 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 12 Dec 2007 15:33:05 +0000
Subject: [R] lm/model.matrix confusion (? bug)
In-Reply-To: <14296513.post@talk.nabble.com>
References: <14292188.post@talk.nabble.com>
	<20071212183632.22155700@berwin-nus1> <14296513.post@talk.nabble.com>
Message-ID: <1197473585.9595.106.camel@prometheus.geog.ucl.ac.uk>


On Wed, 2007-12-12 at 06:46 -0800, Mark Difford wrote:
> Hi Gavin, Berwin,
> 
> Thanks for your detailed replies.  I'll make a general reply, if you don't
> mind.
> 
> To reiterate, my main point is that if model.matrix() can be used in this
> way, then lm() shouldn't add an intercept.
> 
> >> ... lm(Gas ~ model.matrix(~ Insul/Temp - 1), data = whiteside) ....
> 
> And the documentation for lm() indicates. albeit indirectly, that
> model.matrix can be used in this way.  It calls for a formula, or something
> that can be coerced to one.  And the following meets that criterion:
> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside)), and this
> specifies no intercept.

This is the problem - it doesn't! 

What is says is model.matrix() don't add an intercept to the returned
model matrix. R then tries to work out what to do with the returned
matrix that is now on the rhs of the formula. But note that at no point
have you gotten round the fact that it will add an intercept *after* the
formula is parsed and a -1 is not found. This is because model.matrix is
called again, internally in lm but not with your formula as an argument
but with a terms object and nowhere in this object is it specified not
to include an intercept.

To do this, lm would have to parse the formula for model.matrix(....)
and if it finds something work accordingly.

> 
> On the question of why I want to mess about in such a labarynthine way. 
> Well my email was largely expository.  With a straight call to lm(), I
> wouldn't bother with model.matrix.
> 
> So, it really was about getting at lm coefficients inside a function (when
> you have to get the terms &c. from somewhere else), and trying to understand
> properly how things work, and why they work the way they do, and even if
> they should work the way they do.
> 
> For instance:---
> 
>     if (ols) {
>         obj <- x[[1]]
>         mt <- terms(obj)
>         mf <- model.frame(obj)
>         y <- model.response(mf)
>         X <- model.matrix(mt, mf, contrasts = obj$contrasts)
>         if (attr(mt, "intercept") == 1)                    ## This is my
> hack to overcome the double-intercept problem
>             { olscf <- summary(lm(y ~ X))$coefficients }
>         else {
>             olscf <- summary(lm(y ~ X - 1))$coefficients
>         }
>         rownames(olscf) <- rownames(coef(obj))

In this instance, you don't want to be working with lm. You can use
lm.fit which returns an object with $coefficients, so I would guess you
need something like this (not tested)

    if (ols) {
        obj <- x[[1]]
        mt <- terms(obj)
        mf <- model.frame(obj)
        y <- model.response(mf)
        X <- model.matrix(mt, mf, contrasts = obj$contrasts)
	fit <- lm.fit(X, y) ## store fit in case you need something else
	olscf <- coef(fit)  ## get coefficients using an extractor function
        ##if (attr(mt, "intercept") == 1)  ## This is my hack to overcome the double-intercept problem
        ##    { olscf <- summary(lm(y ~ X))$coefficients }
        ##else {
        ##    olscf <- summary(lm(y ~ X - 1))$coefficients
        ##}
        ##rownames(olscf) <- rownames(coef(obj))
}

Note also, that even if you are using your hack, you don't need to do

summary(lm(y ~ X))$coefficients

as

coef(lm(y ~ X))

will do.

lm is mainly there for top-level work. If you need to do things like you
have above, use lm.fit or do the qr decomposition yourself.

Or try a different way to build the formula you need and work with lm;
Bill Venables has a nice piece in R News Vol 2 Issue 2 in the
Programmer's Niche section on something that might be of use if you want
to build a correct formula for your needs from the colnames of X and y?

All the best,

G

> 
> Thanks again for your input.
> 
> Regards,
> Mark.
> 
> 
> 
> 
> 
> Berwin A Turlach wrote:
> > 
> > G'day Mark,
> > 
> > On Wed, 12 Dec 2007 02:05:54 -0800 (PST)
> > Mark Difford <mark_difford at yahoo.co.uk> wrote:
> > 
> >> In order to get the same coefficients as we get from the following
> > [...] 
> >> we need to do the following (if we use model.matrix to specify the
> >> model)
> > 
> > By why would you want to do this?
> > 
> >> ##
> >> summary ( lm(Gas ~ model.matrix(~ Insul/Temp - 1) - 1, data =
> >> whiteside) )
> >> 
> >> That is, we need to take out "two intercepts."  Is this "correct"?
> > 
> > Yes.
> >  
> >> Shouldn't lm check to see if an intercept has been requested as part
> >> of the model formula?
> > 
> > No, it does not.  In the Details section of lm's help page you will
> > find the following:
> > 
> >      A formula has an implied intercept term.  To remove this use
> >      either 'y ~ x - 1' or 'y ~ 0 + x'.  See 'formula' for more details
> >      of allowed formulae.
> > 
> > Thus, except if you explicitly ask for a constant term not be included,
> > lm will add a constant term (a column of ones) additionally to what
> > ever you have specified on the right hand side of the formula.
> > 
> >> If I do
> >> ##
> >> summary(lm(as.formula(Gas ~ model.matrix (~ Insul/Temp-1,
> >> data=whiteside)), data=whiteside))
> >> 
> >> we get a strange model.  
> > 
> > Well, you get a model in which not all parameters are identifiable, and
> > a particular parameter that is not identifiable is estimated by NA.  I
> > am not sure what is strange about this.
> > 
> >> But the formula part of this model qualifies
> >> as a valid formula
> >> ##
> >> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside))
> > 
> > Debatable, the above command only shows that it can be coerced into a
> > valid formula. :)
> > 
> >> just as if I were to write: lm(Gas ~ Insul/Temp - 1, data=whiteside)
> >> 
> >> But we know that the _correct_ formula is the following
> >  
> >> ##
> >> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside) -1)
> > 
> > Why is this formula any more correct than the other one?  Both specify
> > exactly the same model.  It is just that one does it in an
> > overparameterised way.
> > 
> >> (Sorry, this is getting really long) --- So, my question/confusion
> >> comes down to wanting to know why lm() doesn't check to see if an
> >> intercept has been specified when the model has been specified using
> >> model.matrix.
> > 
> > Because lm() is documented not to check this.  If you do not want to
> > have an intercept in the model you have to specifically ask it for.
> > 
> > Also, comparing the output of 
> > 	summary( lm(Gas ~ Insul/Temp - 1, data = whiteside) )
> > and
> > 	summary( lm(Gas ~ Insul/Temp, data = whiteside ) )
> > 
> > you can see that lm() does not check whether there is an implicit
> > intercept in the model.  Compare the (Adjusted) R-squared values
> > returned; one case is using the formula for models with no intercept
> > the other one the formula for models with intercept.  Similar story
> > with the reported F-statistics.  
> > 
> > Cheers,
> > 
> > 	Berwin
> > 
> > =========================== Full address =============================
> > Berwin A Turlach                            Tel.: +65 6515 4416 (secr)
> > Dept of Statistics and Applied Probability        +65 6515 6650 (self)
> > Faculty of Science                          FAX : +65 6872 3919       
> > National University of Singapore     
> > 6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
> > Singapore 117546                    http://www.stat.nus.edu.sg/~statba
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ripley at stats.ox.ac.uk  Wed Dec 12 16:44:33 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Dec 2007 15:44:33 +0000 (GMT)
Subject: [R] discrepancy between periodogram implementations ? per and
 spec.pgram
In-Reply-To: <475FF863.6030403@wis.kuleuven.be>
References: <475FF863.6030403@wis.kuleuven.be>
Message-ID: <Pine.LNX.4.64.0712121532050.9158@gannet.stats.ox.ac.uk>

There are several definitions of a periodgram.  Note that

> log(2*pi)
[1] 1.837877

See the comments in ?spectrum about scalings.

I think the comments in ?per incorrectly ignore the scaling issues: per() 
does not take the base frequency into account and has an extra divisor of 
2*pi.  E.g.

> x <- rnorm(64)
> spec.pgram(x, taper=0, detrend=F)$spec/per(x)[-1]
  [1] 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185
  [9] 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185
[17] 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185
[25] 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185


On Wed, 12 Dec 2007, Lieven Desmet wrote:

> hello,
>
> I have been using the per function in package longmemo to obtain a
> simple raw periodogram.
> I am considering to switch to the function spec.pgram since I want to be
> able to do tapering.
> To compare both I used spec.pgram with the options as suggested in the
> documentation of per {longmemo} to make them correspond.
>
> Now I have found on a variety of examples that there is a shift between
> the log of the periodogram with per and that with spec.pgram. This
> vertical shift amounts to  approx. 1.8  on the log scale  (the graph of
> spec.pgram being above the one from per).
>
> Is there some explanation for this ? Is the one from spec.pgram the
> better one as suggested in the documentation of per {longmemo}? Finally
> how are these related to an estimate of the spectral density obtained
> from spec.arima ?

What is spec.arima?  If you meant spec.ar, that is on the same scale as 
spec.pgram for series with base frequency 1 (and for all series for R >= 
2.7.0).


> Many thanks for help and clarification.
>
> Lieven Desmet

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From cganduri at gmail.com  Wed Dec 12 16:45:40 2007
From: cganduri at gmail.com (chandrasekhar ganduri)
Date: Wed, 12 Dec 2007 10:45:40 -0500
Subject: [R] Adding data labels to Lattice plots
Message-ID: <1f3dc4a0712120745j70f56daco3af02a552ef14bc9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/9faa3374/attachment.pl 

From Bernhard_Pfaff at fra.invesco.com  Wed Dec 12 16:48:34 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Wed, 12 Dec 2007 15:48:34 -0000
Subject: [R] Need good Reference Material and Reading about Gaussian
	Copulas
In-Reply-To: <0946E293C7C22A45A0E33BA14FAA8D880151E475@500MAIL.goldbox.com>
References: <0946E293C7C22A45A0E33BA14FAA8D880151E475@500MAIL.goldbox.com>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C30D0FD3AD@GBHENXMB02.corp.amvescap.net>

Hello Neil,

you will find decent and well-written papers on:

http://www.math.ethz.ch/~embrecht/

http://www.ma.hw.ac.uk/~mcneil/

http://www.math.uni-leipzig.de/~tschmidt/#publications


Best,
Bernhard

ps: Incidentally, the monograph http://press.princeton.edu/titles/8056.html contains nice illustrations too. See packages QRMlib, Copula (JSS: Enjoy the Joy of Copulas: With a Package copula, Vol. 21, Issue 4, Oct 2007), mlCopulaSelection, sbgcop and fCopulae on CRAN for implementations of copulae. 
  

>-----Urspr?ngliche Nachricht-----
>Von: r-help-bounces at r-project.org 
>[mailto:r-help-bounces at r-project.org] Im Auftrag von 
>ngottlieb at marinercapital.com
>Gesendet: Mittwoch, 12. Dezember 2007 16:25
>An: R-help at r-project.org
>Betreff: [R] Need good Reference Material and Reading about 
>Gaussian Copulas
>
>Can anyone advise me on some pratical papers or books 
>On Gaussian Copulas? Anything in the genre of Copulas Dummies
>Would be a help.
>
>As simpe, and approachable with minimal pedantic style.
>Thanks,
>Neil
>--------------------------------------------------------
>
>
>
>This information is being sent at the recipient's 
>reques...{{dropped:16}}
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
*****************************************************************
Confidentiality Note: The information contained in this ...{{dropped:10}}


From Dan.Kelley at dal.ca  Wed Dec 12 16:56:38 2007
From: Dan.Kelley at dal.ca (dankelley)
Date: Wed, 12 Dec 2007 07:56:38 -0800 (PST)
Subject: [R] how to shorten elements in a data frame?
In-Reply-To: <14296509.post@talk.nabble.com>
References: <14296509.post@talk.nabble.com>
Message-ID: <14297847.post@talk.nabble.com>


In case anyone with a similar need comes across this thread, I am posting
below a solution to my problem, in which I construct a new data frame that
has the desired dimensions, instead of trying to change the dimensions of
the existing data frame.

<code>
	npt <- length(pt)
	# Step through each variable.
	data.names <- names(x$data)
	data.new <- as.data.frame(array(NA, dim=c(npt, dim(x$data)[2])))
	names(data.new) <- data.names
	for (datum.name in data.names) {
		if (datum.name != "pressure") {
			data.new[[datum.name]] <- approx(x$data[["pressure"]],
x$data[[datum.name]], pt)$y
		}
	}
	# Now replace pressure
	data.new[["pressure"]] <- pt
	res$data <- data.new
</code>


-- 
View this message in context: http://www.nabble.com/how-to-shorten-elements-in-a-data-frame--tp14296509p14297847.html
Sent from the R help mailing list archive at Nabble.com.


From jctoll at gmail.com  Wed Dec 12 17:16:02 2007
From: jctoll at gmail.com (James)
Date: Wed, 12 Dec 2007 09:16:02 -0700
Subject: [R] Overlay PDF on histogram
Message-ID: <CDCD66E2-6E16-4F28-B198-92B36560B53D@gmail.com>

Hi,

I thought that I had read somewhere that there was a really simple  
way to overlay the probability density function of a normal  
distribution over a histogram, after the histogram has already been  
plotted.  Possibly a one word command.

I've found this email from the archives, but I don't think this is  
what I'm looking for.  I thought there was something more simple than  
the curve function.

http://tolstoy.newcastle.edu.au/R/help/05/04/3523.html

R> x <- rnorm(100)
R> hist(x, freq = FALSE)
R> curve(dnorm, col = 2, add = TRUE)

Is there a simple way to do this, or is my memory failing me?  Thank  
you.

James


From ggrothendieck at gmail.com  Wed Dec 12 17:21:30 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Dec 2007 11:21:30 -0500
Subject: [R] Sys.setlocale() and text()
In-Reply-To: <475FFC3E.2020902@wichita.edu>
References: <475F50D9.20003@wichita.edu>
	<971536df0712112146m1905d070x23b34975ed8d944c@mail.gmail.com>
	<475FFC3E.2020902@wichita.edu>
Message-ID: <971536df0712120821y1cbbc71fu863bd1c37bd0905b@mail.gmail.com>

Try this:

plot(1:10)
vf <- c("serif", "plain")
text(5.5, 5, "\\*s", vfont=vf)
?Hershey

On Dec 12, 2007 10:20 AM, Ed Merkle <edgar.merkle at wichita.edu> wrote:
> Thanks very much for the response.  I think I left out an important
> detail, however.
>
> I want my lowercase sigma to be displayed in a specific font from the
> Rdevga file (my project involves fonts).  So far as I know, quote() does
> not allow me to select a font.  Thus, I am specifically interested in
> the text() command and reasons why my example code performs differently
> in R 2.3.0 vs 2.6.1.
>
> Thanks,
> Ed
>
>
>
> Gabor Grothendieck wrote:
> > Try this:
> >
> > plot(1:10, main = quote(sigma ^ 2))
> >
> >
> > On Dec 11, 2007 10:09 PM, Ed Merkle <edgar.merkle at wichita.edu> wrote:
> >> Dear HelpeRs,
> >>
> >> I have a question about the Sys.setlocale() command and plotting.  I am
> >> running Windows XP, with R 2.6.1.  My default locale is English_United
> >> States.1252.
> >>
> >> I am trying to add a lowercase sigma to a plot using the following code:
> >>
> >> Sys.setlocale("LC_CTYPE","greek")
> >> plot(1:10,1:10)
> >> text(4,3,"\xF3")
> >>
> >>
> >> For R 2.6.1, this code gives me the glyph from my default (1252) instead
> >> of from the 1253 codes.  For an older version of R (2.3.0) on the same
> >> computer, this code gives me the lowercase sigma that I wanted.  I have
> >> been unable to pinpoint what has changed.  Thanks for the help, and I
> >> apologize if I am missing something obvious.
> >>
> >>
> >> --
> >> Ed Merkle, PhD
> >> Assistant Professor
> >> Dept. of Psychology
> >> Wichita State University
> >> Wichita, KS 67260
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
>


From darteta001 at ikasle.ehu.es  Wed Dec 12 17:23:55 2007
From: darteta001 at ikasle.ehu.es (darteta001 at ikasle.ehu.es)
Date: Wed, 12 Dec 2007 17:23:55 +0100 (CET)
Subject: [R] Reproducibility of experiment
Message-ID: <8641331554darteta001@ikasle.ehu.es>

Dear list,

I have an experiment that I have run 10 times in order to find out its 
reproducibility. I wonder if there is any function that I can use for 
obtaining a significance value of reproducibility or agreement of 
measurements. I thought of coefficient of variation but, as far as I 
know, I would have to set a threshold for saying the experiment is not 
reproducible. Any pointers to something more "objective" would be very 
helpful.

Thanks

David


From NAlbicelli at bear.com  Wed Dec 12 17:23:16 2007
From: NAlbicelli at bear.com (Albicelli, Nicholas (Exchange))
Date: Wed, 12 Dec 2007 11:23:16 -0500
Subject: [R] Need good Reference Material and Reading about Gaussian
	Copulas
Message-ID: <CFA6D077FE49FF4FB602A8F9747DE536026687EC@whexchmb13.bsna.bsroot.bear.com>

www.defaultrisk.com <http://www.defaultrisk.com/>  has good resources
(albeit as applied to structured credit) - do a search on "CDO copula"

 

HTH

 

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of ngottlieb at marinercapital.com
Sent: Wednesday, December 12, 2007 10:25 AM
To: R-help at r-project.org
Subject: [R] Need good Reference Material and Reading about Gaussian
Copulas

 

Can anyone advise me on some pratical papers or books 

On Gaussian Copulas? Anything in the genre of Copulas Dummies

Would be a help.

 

As simpe, and approachable with minimal pedantic style.

Thanks,

Neil

--------------------------------------------------------







This information is being sent at the recipient's
reques...{{dropped:16}}



______________________________________________

R-help at r-project.org mailing list

https://stat.ethz.ch/mailman/listinfo/r-help

PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html

and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------


***********************************************************************
Bear Stearns is not responsible for any recommendation, ...{{dropped:17}}


From gavin.simpson at ucl.ac.uk  Wed Dec 12 17:28:57 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 12 Dec 2007 16:28:57 +0000
Subject: [R] Overlay PDF on histogram
In-Reply-To: <CDCD66E2-6E16-4F28-B198-92B36560B53D@gmail.com>
References: <CDCD66E2-6E16-4F28-B198-92B36560B53D@gmail.com>
Message-ID: <1197476937.9595.123.camel@prometheus.geog.ucl.ac.uk>


On Wed, 2007-12-12 at 09:16 -0700, James wrote:
> Hi,
> 
> I thought that I had read somewhere that there was a really simple  
> way to overlay the probability density function of a normal  
> distribution over a histogram, after the histogram has already been  
> plotted.  Possibly a one word command.
> 
> I've found this email from the archives, but I don't think this is  
> what I'm looking for.  I thought there was something more simple than  
> the curve function.
> 
> http://tolstoy.newcastle.edu.au/R/help/05/04/3523.html
> 
> R> x <- rnorm(100)
> R> hist(x, freq = FALSE)
> R> curve(dnorm, col = 2, add = TRUE)
> 
> Is there a simple way to do this, or is my memory failing me?  Thank  
> you.
> 
> James

That seems simple enough to me. 1 line, the holy grail!

Is there a problem with using curve in your actual application?

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From michael.lundholm at ne.su.se  Wed Dec 12 17:35:31 2007
From: michael.lundholm at ne.su.se (Michael Lundholm)
Date: Wed, 12 Dec 2007 17:35:31 +0100
Subject: [R] OS-dependent behaviour of strucchange?
In-Reply-To: <476009C6.5020005@ne.su.se>
References: <475FEB22.6010002@ne.su.se> <476009C6.5020005@ne.su.se>
Message-ID: <47600DD3.1060208@ne.su.se>

Unfortunately I did did provide an accurate link to the files I referred
to in my question. The following link works

http://people.su.se/~lundh/downloads/y.rda

Excuse me
/Michael Lundholm

-- 
Docent Michael Lundholm
http://www.ne.su.se/~mlu

Reclaim Your Inbox!
http://www.mozilla.org/products/thunderbird


From spencer.graves at pdf.com  Wed Dec 12 17:32:35 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 12 Dec 2007 08:32:35 -0800
Subject: [R] S3 and S4 clash
In-Reply-To: <Pine.LNX.4.64.0712062112280.30805@gannet.stats.ox.ac.uk>
References: <47585913.5070206@pdf.com>
	<Pine.LNX.4.64.0712062112280.30805@gannet.stats.ox.ac.uk>
Message-ID: <47600D23.3090007@pdf.com>

Dear Prof. Ripley: 

      Thanks for your help.  I very much appreciate your great 
generosity in both your substantive contributions to the code and 
documentation for R and in responding to so many questions, helping to 
make R what it is today. 

      In the real application that led to the toy example below, I had 
defined

            AIC.factanal <- function(object, ..., k=c(2, "BIC")){ ... }

      1.  If I understand your suggestion, I'd be better NOT defining an 
"AIC.factanal" but instead define only "logLik.factanal" and let methods 
dispatch go through "AIC.default" to "logLik.factanal".  Is this correct? 

      2.  Is it inappropriate to supply other optional values for an 
argument in a generic function like this? 

      3.  If it is OK to specify options for "k" like this, how would 
you suggest I do it?  Use the S4 standard? 

      Thanks again. 
      Spencer Graves

Prof Brian Ripley wrote:
> I'd say that was pretty clearly a bug in stats4 (which as I recall was 
> needed to get around the scoping awkwardnesses of S4).
>
> But could you not write a logLik method for your class?  E.g.
>
>> logLik.bar <- function(object, ...) structure(pi, class="logLik", df=1)
>> AIC(bar.tmp)
> [1] -4.283185
>> library(stats4)
>> AIC(bar.tmp)
> [1] -4.283185
>
>
> On Thu, 6 Dec 2007, Spencer Graves wrote:
>
>> Hello:
>>
>>      How can I work around the conflict between the S3 and S4
>> illustrated in the example below?  I'm developing a package that
>> requires a function in 'stats4', but when 'stats4' is attached, it
>> breaks my AIC function.  I could give my AIC function another name so it
>> no longer uses the generic dispatch, but I wonder if there is another 
>> way.
>>
>>      Thanks,
>>      Spencer Graves
>> ################################
>> bar.tmp <- structure(1, class = "bar")
>> AIC.bar <- function(object, ..., k=2) {
>>  3
>> }
>>
>> > AIC(bar.tmp)
>> [1] 3
>> > library(stats4)
>> > AIC(bar.tmp)
>> Error in UseMethod("logLik") : no applicable method for "logLik"
>> > detach("package:stats4")
>> > AIC(bar.tmp)
>> [1] 3
>> > objects()
>> [1] "AIC.bar" "bar.tmp"
>> > sessionInfo()
>> R version 2.6.1 (2007-11-26)
>> i386-pc-mingw32
>>
>> locale:
>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
>> States.1252;LC_MONETARY=English_United
>> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> loaded via a namespace (and not attached):
>> [1] stats4_2.6.1
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From ripley at stats.ox.ac.uk  Wed Dec 12 18:19:41 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Dec 2007 17:19:41 +0000 (GMT)
Subject: [R] Sys.setlocale() and text()
In-Reply-To: <475FFC3E.2020902@wichita.edu>
References: <475F50D9.20003@wichita.edu>
	<971536df0712112146m1905d070x23b34975ed8d944c@mail.gmail.com>
	<475FFC3E.2020902@wichita.edu>
Message-ID: <Pine.LNX.4.64.0712121653530.9743@gannet.stats.ox.ac.uk>

On Wed, 12 Dec 2007, Ed Merkle wrote:

> Thanks very much for the response.  I think I left out an important
> detail, however.
>
> I want my lowercase sigma to be displayed in a specific font from the
> Rdevga file (my project involves fonts).  So far as I know, quote() does
> not allow me to select a font.  Thus, I am specifically interested in
> the text() command and reasons why my example code performs differently
> in R 2.3.0 vs 2.6.1.

I think the only thing which needs explaining is why it worked in 2.3.1: 
you can't usually switch 8-bit locales in an application on Windows and 
expect to get the right font.  Most likely it was fortuitous that it 
worked: it was only intended to work in a multibyte locale (see the entry 
in CHANGES for 2.3.0), and that principally to aid testing of non-Western 
locales by the developers.

But if you are really interested in differences then you will need to 
study the changes in the source code for yourself: the price you paid does 
not include that level of support.

One of the holdups has been that we tried still to support Windows 9X/ME. 
That will be dropped as from R 2.7.0 and so we can use Unicode to access 
the glyphs via text().  Your example works for me in R-devel, so please 
try that in a day or two (I think it needs a recent change).

Note though that switching locales within a session is fragile: it 
invalidates all existing 8-bit data, and so for example repainting the 
graphics window or the console can change the glyphs displayed.  (In 
R-devel, try changing back to US and resize the graphics window.)


>
> Thanks,
> Ed
>
>
> Gabor Grothendieck wrote:
>> Try this:
>>
>> plot(1:10, main = quote(sigma ^ 2))
>>
>>
>> On Dec 11, 2007 10:09 PM, Ed Merkle <edgar.merkle at wichita.edu> wrote:
>>> Dear HelpeRs,
>>>
>>> I have a question about the Sys.setlocale() command and plotting.  I am
>>> running Windows XP, with R 2.6.1.  My default locale is English_United
>>> States.1252.
>>>
>>> I am trying to add a lowercase sigma to a plot using the following code:
>>>
>>> Sys.setlocale("LC_CTYPE","greek")
>>> plot(1:10,1:10)
>>> text(4,3,"\xF3")
>>>
>>>
>>> For R 2.6.1, this code gives me the glyph from my default (1252) instead
>>> of from the 1253 codes.  For an older version of R (2.3.0) on the same
>>> computer, this code gives me the lowercase sigma that I wanted.  I have
>>> been unable to pinpoint what has changed.  Thanks for the help, and I
>>> apologize if I am missing something obvious.
>>>
>>>
>>> --
>>> Ed Merkle, PhD
>>> Assistant Professor
>>> Dept. of Psychology
>>> Wichita State University
>>> Wichita, KS 67260
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From MurdockEL at EPI.WUSTL.EDU  Wed Dec 12 18:26:27 2007
From: MurdockEL at EPI.WUSTL.EDU (Murdock, Erin L.)
Date: Wed, 12 Dec 2007 11:26:27 -0600
Subject: [R]  CTRL-C behaviour with RODBC on Solaris2.8
Message-ID: <08D45B148920D549953268BFFE0E0C99923EF7@eprgmail2>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/5614b72e/attachment.pl 

From mbstahl at west-inc.com  Wed Dec 12 18:26:30 2007
From: mbstahl at west-inc.com (Michelle Bourassa Stahl)
Date: Wed, 12 Dec 2007 10:26:30 -0700
Subject: [R] eliminating cancel button in winDialogString call
Message-ID: <000301c83ce4$23551e10$69ff5a30$@com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/dcbcd5a0/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Dec 12 18:45:43 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Dec 2007 17:45:43 +0000 (GMT)
Subject: [R] eliminating cancel button in winDialogString call
In-Reply-To: <000301c83ce4$23551e10$69ff5a30$@com>
References: <000301c83ce4$23551e10$69ff5a30$@com>
Message-ID: <Pine.LNX.4.64.0712121740570.10724@gannet.stats.ox.ac.uk>

On Wed, 12 Dec 2007, Michelle Bourassa Stahl wrote:

> I would like to use the function winDialogString to get input from a user to
> a program but would like to eliminate the cancel button option and just have
> the OK button appear. The problem is that if a user enters data then
> accidently hits Cancel instead of OK, the program may either run incorrectly
> or quit due to the error of having NULL for a variable. I can think of some
> unsophisticated ways around this like testing for a NULL value from a call
> to winDialogString and asking the user to reenter,  however the better
> solution would be to just eliminate the Cancel button option. If I could
> find the code for the function winDialogString I might be able to create my
> own winDialgStringNoCancel function, however when I enter winDialogString at
> the > in the R console, I just get the .Internal call which is of no help to
> me. Can anyone help?

But it does help: it tells you to look in the C code in the sources. 
I'll help you along by telling you it is in src/gnuwin32/extra.c, and 
refers to a graphapp function that draws the dialog box.

>
>
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
> Michelle Bourassa Stahl
>
> Biometrician
>
> Western EcoSystems Technology, Inc.
>
> 2003 Central Avenue, Cheyenne, WY 82001
>
> 307-634-1756
>
> mbstahl at west-inc.com
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jmburgos at u.washington.edu  Wed Dec 12 18:47:02 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Wed, 12 Dec 2007 09:47:02 -0800
Subject: [R] matrix graph
In-Reply-To: <14282791.post@talk.nabble.com>
References: <14282791.post@talk.nabble.com>
Message-ID: <47601E96.4070209@u.washington.edu>

The basic functions you need are
image()
contour()

although I like better the plot.surface() function in the 'fields' package.

Julian

threshold wrote:
> Hi All, simple question: 
> do you know how to graph the following object/matrix in a 'surface manner':
> 
>           [,1]     [,2]     [,3]    [,4]   [,5]    [,6]
> [1,] -0.154 -0.065 0.129 0.637 0.780 0.221
> [2,]  0.236  0.580 0.448 0.729 0.859 0.475
> [3,]  0.401  0.506 0.310 0.650 0.822 0.448
> [4,]  0.548  0.625 0.883 0.825 0.945 0.637
> [5,]  0.544  0.746 0.823 0.877 0.861 0.642
> [6,]  0.262  0.399 0.432 0.620 0.711 0.404
> 
> will be very grateful for hints.
> 
> rob


From marc_schwartz at comcast.net  Wed Dec 12 18:49:36 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 12 Dec 2007 11:49:36 -0600
Subject: [R] Reproducibility of experiment
In-Reply-To: <8641331554darteta001@ikasle.ehu.es>
References: <8641331554darteta001@ikasle.ehu.es>
Message-ID: <1197481776.3176.50.camel@Bellerophon.localdomain>


On Wed, 2007-12-12 at 17:23 +0100, darteta001 at ikasle.ehu.es wrote:
> Dear list,
> 
> I have an experiment that I have run 10 times in order to find out its 
> reproducibility. I wonder if there is any function that I can use for 
> obtaining a significance value of reproducibility or agreement of 
> measurements. I thought of coefficient of variation but, as far as I 
> know, I would have to set a threshold for saying the experiment is not 
> reproducible. Any pointers to something more "objective" would be very 
> helpful.
> 
> Thanks
> 
> David

I suspect that you are going to have to be more specific regarding the
subject matter and the experimental design so that those with the
requisite expertise could comment.

If this is looking at a continuous measure (ie. instrumentation
measurements), you could look at Bland-Altman methods. More information
here:

  http://www-users.york.ac.uk/~mb55/meas/meas.htm

Otherwise, given that Google returns almost a million hits with the
phrase "reproducibility of experiment"...

HTH,

Marc Schwartz


From mark_difford at yahoo.co.uk  Wed Dec 12 18:56:27 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Wed, 12 Dec 2007 09:56:27 -0800 (PST)
Subject: [R] lm/model.matrix confusion (? bug)
In-Reply-To: <1197473585.9595.106.camel@prometheus.geog.ucl.ac.uk>
References: <14292188.post@talk.nabble.com>
	<20071212183632.22155700@berwin-nus1>
	<14296513.post@talk.nabble.com>
	<1197473585.9595.106.camel@prometheus.geog.ucl.ac.uk>
Message-ID: <14300313.post@talk.nabble.com>


Hi Gavin,

> ... In this instance, you don't want to be working with lm.  You can use
> lm.fit which returns an object with $coefficients, so I would guess you
> need something like this (not tested) ... & more ...

That's a great help, many thank's for walking me through it; that's the
stuff I need to get me through to the next level.

Best Regards,
Mark.


Gavin Simpson wrote:
> 
> 
> On Wed, 2007-12-12 at 06:46 -0800, Mark Difford wrote:
>> Hi Gavin, Berwin,
>> 
>> Thanks for your detailed replies.  I'll make a general reply, if you
>> don't
>> mind.
>> 
>> To reiterate, my main point is that if model.matrix() can be used in this
>> way, then lm() shouldn't add an intercept.
>> 
>> >> ... lm(Gas ~ model.matrix(~ Insul/Temp - 1), data = whiteside) ....
>> 
>> And the documentation for lm() indicates. albeit indirectly, that
>> model.matrix can be used in this way.  It calls for a formula, or
>> something
>> that can be coerced to one.  And the following meets that criterion:
>> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside)), and this
>> specifies no intercept.
> 
> This is the problem - it doesn't! 
> 
> What is says is model.matrix() don't add an intercept to the returned
> model matrix. R then tries to work out what to do with the returned
> matrix that is now on the rhs of the formula. But note that at no point
> have you gotten round the fact that it will add an intercept *after* the
> formula is parsed and a -1 is not found. This is because model.matrix is
> called again, internally in lm but not with your formula as an argument
> but with a terms object and nowhere in this object is it specified not
> to include an intercept.
> 
> To do this, lm would have to parse the formula for model.matrix(....)
> and if it finds something work accordingly.
> 
>> 
>> On the question of why I want to mess about in such a labarynthine way. 
>> Well my email was largely expository.  With a straight call to lm(), I
>> wouldn't bother with model.matrix.
>> 
>> So, it really was about getting at lm coefficients inside a function
>> (when
>> you have to get the terms &c. from somewhere else), and trying to
>> understand
>> properly how things work, and why they work the way they do, and even if
>> they should work the way they do.
>> 
>> For instance:---
>> 
>>     if (ols) {
>>         obj <- x[[1]]
>>         mt <- terms(obj)
>>         mf <- model.frame(obj)
>>         y <- model.response(mf)
>>         X <- model.matrix(mt, mf, contrasts = obj$contrasts)
>>         if (attr(mt, "intercept") == 1)                    ## This is my
>> hack to overcome the double-intercept problem
>>             { olscf <- summary(lm(y ~ X))$coefficients }
>>         else {
>>             olscf <- summary(lm(y ~ X - 1))$coefficients
>>         }
>>         rownames(olscf) <- rownames(coef(obj))
> 
> In this instance, you don't want to be working with lm. You can use
> lm.fit which returns an object with $coefficients, so I would guess you
> need something like this (not tested)
> 
>     if (ols) {
>         obj <- x[[1]]
>         mt <- terms(obj)
>         mf <- model.frame(obj)
>         y <- model.response(mf)
>         X <- model.matrix(mt, mf, contrasts = obj$contrasts)
> 	fit <- lm.fit(X, y) ## store fit in case you need something else
> 	olscf <- coef(fit)  ## get coefficients using an extractor function
>         ##if (attr(mt, "intercept") == 1)  ## This is my hack to overcome
> the double-intercept problem
>         ##    { olscf <- summary(lm(y ~ X))$coefficients }
>         ##else {
>         ##    olscf <- summary(lm(y ~ X - 1))$coefficients
>         ##}
>         ##rownames(olscf) <- rownames(coef(obj))
> }
> 
> Note also, that even if you are using your hack, you don't need to do
> 
> summary(lm(y ~ X))$coefficients
> 
> as
> 
> coef(lm(y ~ X))
> 
> will do.
> 
> lm is mainly there for top-level work. If you need to do things like you
> have above, use lm.fit or do the qr decomposition yourself.
> 
> Or try a different way to build the formula you need and work with lm;
> Bill Venables has a nice piece in R News Vol 2 Issue 2 in the
> Programmer's Niche section on something that might be of use if you want
> to build a correct formula for your needs from the colnames of X and y?
> 
> All the best,
> 
> G
> 
>> 
>> Thanks again for your input.
>> 
>> Regards,
>> Mark.
>> 
>> 
>> 
>> 
>> 
>> Berwin A Turlach wrote:
>> > 
>> > G'day Mark,
>> > 
>> > On Wed, 12 Dec 2007 02:05:54 -0800 (PST)
>> > Mark Difford <mark_difford at yahoo.co.uk> wrote:
>> > 
>> >> In order to get the same coefficients as we get from the following
>> > [...] 
>> >> we need to do the following (if we use model.matrix to specify the
>> >> model)
>> > 
>> > By why would you want to do this?
>> > 
>> >> ##
>> >> summary ( lm(Gas ~ model.matrix(~ Insul/Temp - 1) - 1, data =
>> >> whiteside) )
>> >> 
>> >> That is, we need to take out "two intercepts."  Is this "correct"?
>> > 
>> > Yes.
>> >  
>> >> Shouldn't lm check to see if an intercept has been requested as part
>> >> of the model formula?
>> > 
>> > No, it does not.  In the Details section of lm's help page you will
>> > find the following:
>> > 
>> >      A formula has an implied intercept term.  To remove this use
>> >      either 'y ~ x - 1' or 'y ~ 0 + x'.  See 'formula' for more details
>> >      of allowed formulae.
>> > 
>> > Thus, except if you explicitly ask for a constant term not be included,
>> > lm will add a constant term (a column of ones) additionally to what
>> > ever you have specified on the right hand side of the formula.
>> > 
>> >> If I do
>> >> ##
>> >> summary(lm(as.formula(Gas ~ model.matrix (~ Insul/Temp-1,
>> >> data=whiteside)), data=whiteside))
>> >> 
>> >> we get a strange model.  
>> > 
>> > Well, you get a model in which not all parameters are identifiable, and
>> > a particular parameter that is not identifiable is estimated by NA.  I
>> > am not sure what is strange about this.
>> > 
>> >> But the formula part of this model qualifies
>> >> as a valid formula
>> >> ##
>> >> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside))
>> > 
>> > Debatable, the above command only shows that it can be coerced into a
>> > valid formula. :)
>> > 
>> >> just as if I were to write: lm(Gas ~ Insul/Temp - 1, data=whiteside)
>> >> 
>> >> But we know that the _correct_ formula is the following
>> >  
>> >> ##
>> >> as.formula(Gas ~ model.matrix (~ Insul/Temp-1, data=whiteside) -1)
>> > 
>> > Why is this formula any more correct than the other one?  Both specify
>> > exactly the same model.  It is just that one does it in an
>> > overparameterised way.
>> > 
>> >> (Sorry, this is getting really long) --- So, my question/confusion
>> >> comes down to wanting to know why lm() doesn't check to see if an
>> >> intercept has been specified when the model has been specified using
>> >> model.matrix.
>> > 
>> > Because lm() is documented not to check this.  If you do not want to
>> > have an intercept in the model you have to specifically ask it for.
>> > 
>> > Also, comparing the output of 
>> > 	summary( lm(Gas ~ Insul/Temp - 1, data = whiteside) )
>> > and
>> > 	summary( lm(Gas ~ Insul/Temp, data = whiteside ) )
>> > 
>> > you can see that lm() does not check whether there is an implicit
>> > intercept in the model.  Compare the (Adjusted) R-squared values
>> > returned; one case is using the formula for models with no intercept
>> > the other one the formula for models with intercept.  Similar story
>> > with the reported F-statistics.  
>> > 
>> > Cheers,
>> > 
>> > 	Berwin
>> > 
>> > =========================== Full address =============================
>> > Berwin A Turlach                            Tel.: +65 6515 4416 (secr)
>> > Dept of Statistics and Applied Probability        +65 6515 6650 (self)
>> > Faculty of Science                          FAX : +65 6872 3919       
>> > National University of Singapore     
>> > 6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
>> > Singapore 117546                    http://www.stat.nus.edu.sg/~statba
>> > 
>> > ______________________________________________
>> > R-help at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> > 
>> > 
>> 
> -- 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
>  ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
>  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
>  UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/lm-model.matrix-confusion-%28--bug%29-tp14292188p14300313.html
Sent from the R help mailing list archive at Nabble.com.


From jmburgos at u.washington.edu  Wed Dec 12 19:02:41 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Wed, 12 Dec 2007 10:02:41 -0800
Subject: [R] combine variables to matrix
In-Reply-To: <475FF795.1000601@gfz-potsdam.de>
References: <475FF795.1000601@gfz-potsdam.de>
Message-ID: <47602241.5010103@u.washington.edu>

Hi Andre,

I don't quite understand what you are trying to do.  Why you are using 
cbind to join columns of a dataset that it is already in table form?  It 
is true that read.table will give you a data.frame instead of a matrix, 
but if for some reason you need a matrix you can do simply

data.matrix=as.matrix(data)

Julian


Andre Jung wrote:
> I just got stuck with a quite simple question. I've just read in an 
> ASCII table from a plain text file with read.table(). It's a 1200x1200 
> table. R has assigned variables for each column: V1,V2,V3,V4,...
> For small data sets
> 
> data <- read.table("data.txt");
> data.matrix <- cbind(V1,V2,V3);
> 
> works. But how could I put together 1200 columns?
> 
> I've searched the R mailing help and stumbled upon this entry:
> https://stat.ethz.ch/pipermail/r-help/2007-July/137121.html
> which doesn't help me.
> 
> thanks for your help.
> 
> andre
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From heberto.ghezzo at mcgill.ca  Wed Dec 12 19:01:15 2007
From: heberto.ghezzo at mcgill.ca (R Heberto Ghezzo, Dr)
Date: Wed, 12 Dec 2007 13:01:15 -0500
Subject: [R] [OT] vernacular names for circular diagrams
References: <mailman.19.1197370804.9403.r-help@r-project.org><p06002019c384ae89478c@[192.168.1.10]>
	<475FF149.20103@biostat.ku.dk>
Message-ID: <05BE78B0CF1BBC4BBA4AA255568D8611029A9A24@EXCHANGE2VS1.campus.mcgill.ca>

>From Montreal,
Some people here call it the 'pizza diagram'
?some not eatable names?
salut



-----Original Message-----
From: r-help-bounces at r-project.org on behalf of Peter Dalgaard
Sent: Wed 12/12/2007 9:33 AM
To: Jean lobry
Cc: r-help at r-project.org
Subject: Re: [R] [OT] vernacular names for circular diagrams
 
Jean lobry wrote:
> Dear useRs,
>
> by a circular diagram representation I mean what you will get by entering
> this at your R promt:
>
> pie(1:5)
>
> Nice to have R as a lingua franca :-)
>
> The folowing quote is from page 360 in this very interesting paper:
>
> @article{SpenceI2005,
>      title = {No Humble Pie: The Origins and Usage of a Statistical Chart},
>      author = {Spence, I.},
>      journal = {Journal of Educational and Behavioral Statistics},
>      volume = {30},
>      pages = {353-368},
>      year = {2005}
> }
>
> QUOTE
> Like us, the French employ a gastronomical metaphor when
> they refer to Playfair's pie chart, but they have preferred
> instead to invoke the name of the wonderful round soft
> cheese from Normandy - the camembert. When I spent 4 months
> in Paris a few years ago, a friend invited my wife and me to
> lunch with her elderly father who lives in Rouen, Normandy,
> about an hour North of Paris. Her father inquired -
> coincidentally during the cheese course - what work I was
> doing in Paris; I replied that I was researching the
> activities of a Scot, William Playfair, during the
> revolutionary period. I told him that Playfair had invented
> several statistical graphs, including the pie chart, which I
> referred to, in French, as <<le camembert.>> After a stunned
> silence of perhaps a couple of seconds, the distinguished
> elderly gentleman looked me in the eye and exclaimed, <<Mon
> Dieu ! Notre camembert?>>
> UNQUOTE
>
> So, I'm just curious: how do you refer in your own language to
> this kind of graphic? How do you call it?
>
> Best,
>
> Jean
>
>   
<Grin>

In Danish it is "Lagkagediagram" as in the layer cakes that are
traditional at birthday parties (and thrown at eachother's faces in
slapstick comedy).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jmburgos at u.washington.edu  Wed Dec 12 19:09:35 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Wed, 12 Dec 2007 10:09:35 -0800
Subject: [R] Importing Large Dataset into Excel
In-Reply-To: <1197429329.475f525155179@webmail.fas.harvard.edu>
References: <1197429329.475f525155179@webmail.fas.harvard.edu>
Message-ID: <476023DF.1090202@u.washington.edu>

Hi Wayne,

I'm assuming that you file is really a comma-separated file (*.csv) and 
not an Excel workbook (*.xls) saved with a .csv extension, right?  That 
(in my experience) is a common mistake.
You should open your file with a simple text editor (notepad will do if 
the file is not too large) and review line 528, instead of reviewing the 
spreadsheet in Excel.  You should be able to spot the problem right away.

Julian


Wayne Aldo Gavioli wrote:
> Hello all,
> 
> 
> I seem to be having a problem importing a data set from Excel into R.  I'm using
> the "read.table" command to import the data with the following line of code:
> 
>> newborn<-read.table("newborn edit.csv", header=T, sep=",")
> 
> 
> where "newborn edit.csv" is the name of the file.  Unfortunately, I'm getting
> back the following error message:
> 
> 
> "Error in scan(file,, what, nmax, sep, dc, quote, skip, nlines, na.string, :
> line 528 did not have 44 elements"
> 
> 
> As far as I can tell, line 528 of the spreadsheet table does have the same
> number of elements as the other rows - by chance can this error message mean
> anything else?  Also, is there an easier way to import data from R into Excel
> using a single line of R code?
> 
> 
> Thanks,
> 
> 
> Wayne
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From erinberryman at vandals.uidaho.edu  Wed Dec 12 20:15:02 2007
From: erinberryman at vandals.uidaho.edu (Erin Berryman)
Date: Wed, 12 Dec 2007 11:15:02 -0800
Subject: [R] xYplot problem
Message-ID: <BAYC1-PASMTP02997BADEBDD97000E2FD6FF650@CEZ.ICE>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/caf4a35d/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Dec 12 20:29:42 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Dec 2007 19:29:42 +0000 (GMT)
Subject: [R] Rgui: workaround for hang from tseries and other packages using
 Fortran I/O
Message-ID: <Pine.LNX.4.64.0712121828470.27897@gannet.stats.ox.ac.uk>

Several people have reported hanging when using garch() from tseries under 
Rgui (Windows), as well as from other packages using Fortran I/O.

We will have a revised tseries that does not suffer from the problem 
shortly, but here is a generic workaround: set the environment variable
GFORTRAN_STDOUT_UNIT to -1 before running R (critically before using any 
package or module such as Lapack, BLAS ... that uses Fortran)
One simple way to do this is to have

if(.Platform$GUI == "Rgui") {
     Sys.setenv(GFORTRAN_STDOUT_UNIT = "-1")
     Sys.setenv(GFORTRAN_STDERR_UNIT = "-1")
}

in your ~/.Rprofile file.  The second line is not needed for tseries but 
could be needed for other programs.  This will be in R-devel and R-patched 
shortly.

Note that Fortran output is still lost: this is just a palliative measure 
and the real fix for tseries generates the desired output.

(I had to read through the libgfortran to find this: it is confusing that 
Windows I/O is done by a file named unix.c!)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From koen.hufkens at ua.ac.be  Wed Dec 12 20:44:47 2007
From: koen.hufkens at ua.ac.be (Hufkens Koen)
Date: Wed, 12 Dec 2007 20:44:47 +0100
Subject: [R] two-way categorical anova post-hoc data extraction
Message-ID: <832A948B92E5754D9062CCF62F9ED892045C2F@xmail01.ad.ua.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/e92f9d67/attachment.pl 

From francoisetsandrine.mercier at wanadoo.fr  Wed Dec 12 20:55:37 2007
From: francoisetsandrine.mercier at wanadoo.fr (Sandrine-et-Francois)
Date: Wed, 12 Dec 2007 20:55:37 +0100
Subject: [R] [OT] vernacular names for circular diagrams
References: <mailman.19.1197370804.9403.r-help@r-project.org><p06002019c384ae89478c@[192.168.1.10]><475FF149.20103@biostat.ku.dk>
	<05BE78B0CF1BBC4BBA4AA255568D8611029A9A24@EXCHANGE2VS1.campus.mcgill.ca>
Message-ID: <087d01c83cf8$fbacc920$0d01a8c0@Amazone>

Salut Jean,
I guess here in Alsace, in between France, Germany and Switzerland, we would 
call it a Flamenk?che diagram ;-)))
Best regards,
Fran?ois



----- Original Message ----- 
From: "R Heberto Ghezzo, Dr" <heberto.ghezzo at mcgill.ca>
To: "Peter Dalgaard" <P.Dalgaard at biostat.ku.dk>; "Jean lobry" 
<lobry at biomserv.univ-lyon1.fr>
Cc: <r-help at r-project.org>
Sent: Wednesday, December 12, 2007 7:01 PM
Subject: Re: [R] [OT] vernacular names for circular diagrams


>From Montreal,
Some people here call it the 'pizza diagram'
?some not eatable names?
salut



-----Original Message-----
From: r-help-bounces at r-project.org on behalf of Peter Dalgaard
Sent: Wed 12/12/2007 9:33 AM
To: Jean lobry
Cc: r-help at r-project.org
Subject: Re: [R] [OT] vernacular names for circular diagrams

Jean lobry wrote:
> Dear useRs,
>
> by a circular diagram representation I mean what you will get by entering
> this at your R promt:
>
> pie(1:5)
>
> Nice to have R as a lingua franca :-)
>
> The folowing quote is from page 360 in this very interesting paper:
>
> @article{SpenceI2005,
>      title = {No Humble Pie: The Origins and Usage of a Statistical 
> Chart},
>      author = {Spence, I.},
>      journal = {Journal of Educational and Behavioral Statistics},
>      volume = {30},
>      pages = {353-368},
>      year = {2005}
> }
>
> QUOTE
> Like us, the French employ a gastronomical metaphor when
> they refer to Playfair's pie chart, but they have preferred
> instead to invoke the name of the wonderful round soft
> cheese from Normandy - the camembert. When I spent 4 months
> in Paris a few years ago, a friend invited my wife and me to
> lunch with her elderly father who lives in Rouen, Normandy,
> about an hour North of Paris. Her father inquired -
> coincidentally during the cheese course - what work I was
> doing in Paris; I replied that I was researching the
> activities of a Scot, William Playfair, during the
> revolutionary period. I told him that Playfair had invented
> several statistical graphs, including the pie chart, which I
> referred to, in French, as <<le camembert.>> After a stunned
> silence of perhaps a couple of seconds, the distinguished
> elderly gentleman looked me in the eye and exclaimed, <<Mon
> Dieu ! Notre camembert?>>
> UNQUOTE
>
> So, I'm just curious: how do you refer in your own language to
> this kind of graphic? How do you call it?
>
> Best,
>
> Jean
>
>
<Grin>

In Danish it is "Lagkagediagram" as in the layer cakes that are
traditional at birthday parties (and thrown at eachother's faces in
slapstick comedy).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bates at stat.wisc.edu  Wed Dec 12 20:57:04 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 12 Dec 2007 13:57:04 -0600
Subject: [R] difficulties getting coef() to work in some lmer() calls
In-Reply-To: <f255bd8a0712031145v39ca0878tb45d0040cdf217be@mail.gmail.com>
References: <f255bd8a0712031145v39ca0878tb45d0040cdf217be@mail.gmail.com>
Message-ID: <40e66e0b0712121157q48de214cj48f1ade5c5d0140f@mail.gmail.com>

On Dec 3, 2007 1:45 PM, David Park <dkp at gwu.edu> wrote:
> I'm working with Andrew Gelman on a book project and we're having some
> difficulties getting coef() to work in some lmer() calls.
>
> Some versions of the model work and some do not.  For example, this works
> (in that we can run the model and do coef() from the output):
>
> R2 <- lmer(y2 ~ factor(z.inc) + z.st.inc.full + z.st.rel.full + (1 + factor(
> z.inc) | st.num), family=binomial(link="logit"))
>
> But this does not (the model runs but coef() doesn't work):
>
> R3 <- lmer(y2 ~ factor(z.inc) + z.st.inc.full + z.st.rel.full + (1 + z.inc |
> st.num ), family=binomial(link="logit"))
>
> We get the following error:
> Error in coef(R3) : unable to align random and fixed effects

Which indicates that the coefficients being estimated for each level
of st.num do not correspond to any of the fixed-effects coefficients,
hence the coef method, in the sense that it is used in nlme and lme4,
cannot be applied.

You will need to change the fixed-effects specification so that it
includes z.inc.  This may mean modifying the contrasts from
factor(z.inc).  I would suggest switching to polynomial contrasts then
replacing the linear term by z.inc

> The data are at
> http://www.stat.columbia.edu/~gelman/temp/lmerexample.txt<http://www.stat.columbia.edu/%7Egelman/temp/lmerexample.txt>
> The file to read the data are at:
> http://www.stat.columbia.edu/~gelman/temp/lmerexample.R<http://www.stat.columbia.edu/%7Egelman/temp/lmerexample.txt>
>
> Any help would be appreciated.
>
> David K. Park
>
> --
> David K. Park
> Visiting Researcher (Fall 2007 / Spring 2008)
> Applied Statistics Center
> Columbia University
> 1016 Social Work Bldg
> (Amsterdam Ave. at 122 St.)
> New York, NY 10027
>
> Assistant Professor
> Department of Political Science
> George Washington University
> Monroe Hall
> 2115 G Street, NW
> Washington, D.C. 20052
> http://home.gwu.edu/~dkp/ <http://home.gwu.edu/%7Edkp/>
>
> Direct: (202) 994-2331
> Dept: (202) 994-6290
> Fax: (202) 994-7743
>
> "The great tragedy of Science - the slaying of a beautiful hypothesis
> by an ugly fact."
> -- Thomas H. Huxley
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Wed Dec 12 20:59:47 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 12 Dec 2007 13:59:47 -0600
Subject: [R] two-way categorical anova post-hoc data extraction
In-Reply-To: <832A948B92E5754D9062CCF62F9ED892045C2F@xmail01.ad.ua.ac.be>
References: <832A948B92E5754D9062CCF62F9ED892045C2F@xmail01.ad.ua.ac.be>
Message-ID: <f8e6ff050712121159o616e30f6k72ef069a1ca11e98@mail.gmail.com>

> Hi list,
>
> I have a question regarding post-hoc extraction of data from a two-way categorical anova.
>
> I have a categorical anova of this form:
>
> width ~ steepness + patchiness (4 steepness levels, 4 patchiness levels)
>
> This simple setup answers if for the widths I collected across different levels of steepness and patchiness significant differences can be found. Is there a way to look at these differences in detail. Lets say that the steepness parameter is significant, then I would like to know between which levels they are significant or if there are levels where this isn't the case.
>
> It's a basic question but my R knowledge has faded somewhat...

I've never found this particularly easy to do.  For a recent client I wrote:

library(effects)
library(multcomp)
library(multcompView)


effectsum <- function(model, effect) {
  effects <- as.data.frame(all.effects(model)[[effect]])

  mcp <- list("Tukey")
  names(mcp) <- effect
  class(mcp) <- "mcp"

  glht_sum <- summary(glht(model, linfct = mcp))
  p <- as.vector(glht_sum$test$pvalues)
  names(p) <- gsub(" ", "", names(glht_sum$test$tstat))

  groups <- multcompLetters(p)
  effects[, 1] <- factor(effects[, 1])
  effects$group <- groups$Letters[as.character(effects[, 1])]

  effects
}


which allows you to do:

mtcars$cyl <- factor(mtcars$cyl)
simple <- lm(mpg ~ wt + cyl, data=mtcars)
effects <- effectsum(simple, "cyl")

library(ggplot2)
qplot(cyl, fit, data=effects, min = lower, max = upper,
geom="pointrange", ylab="Mean effect") + geom_text(aes(label = group,
y = min(lower) - diff(range(lower)) * 0.07))

ggplot(mtcars, aes(x = cyl, y = mpg)) + geom_crossbar(aes(min=lower,
max=upper, y=fit), data=effects, width=0.2) +
geom_point(aes(colour=wt))

This gives lsmeans (aka population marginal means) with their standard
errors, and groups generated from all pairwise comparisons adjusted
for multiple comparisons.

I would love to improve this code: to deal with all factors in a model
automatically.  Additionally, all.effects and multcompLetters are
rather fragile with respect to level names - if you get an error try
removing any non-alphanumeric characters,

Hadley

-- 
http://had.co.nz/


From Antonio_Paredes at aphis.usda.gov  Wed Dec 12 22:12:00 2007
From: Antonio_Paredes at aphis.usda.gov (Antonio_Paredes at aphis.usda.gov)
Date: Wed, 12 Dec 2007 15:12:00 -0600
Subject: [R] R and Excel Interface
Message-ID: <OF9247DAF5.7BFBA0BA-ON862573AF.00729E43-862573AF.00741C05@aphis.usda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/4440e64f/attachment.pl 

From cpaulsen at u.washington.edu  Wed Dec 12 22:25:08 2007
From: cpaulsen at u.washington.edu (Caroline Paulsen)
Date: Wed, 12 Dec 2007 13:25:08 -0800
Subject: [R] Defining the "random" term in function "negbin" of AOD package
Message-ID: <000b01c83d05$788593d0$6600a8c0@cpaulsen>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/07dc8ab9/attachment.pl 

From mwkimpel at gmail.com  Wed Dec 12 22:33:40 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Wed, 12 Dec 2007 16:33:40 -0500
Subject: [R] 64-bit package compilation problems on powerpc
Message-ID: <476053B4.4080702@gmail.com>

I have been working with my sysAdmin to get R-devel up and running with 
64-bit compilation on our PowerPC cluster. We've got the base install to 
compile with the following flags:

LDFLAGS=-m64 FFLAGS="-m64 -mpowerpc64" FCFLAGS="-m64 -mpowerpc64" 
CFLAGS="-m64 -mpowerpc64" ../configure 
--prefix='/N/hd03/mkimpel/BigRed/R_HOME/R-devel/R-build' 
--x-libraries=/usr/X11R6/lib64

About 50% of my package installs are failing with a message similar to 
that below:

SpikeofOneArray.o' is incompatible with powerpc:common64 output
make: *** [CALIB.so] Error 1

My sysAdmin suggested removing the -mpowerpc64 bit flag if I encountered 
problems, but I though I would run this past our wizards to see if 
anyone had experience with this problem before recompiling.

Mark
-- 

Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com


From dale.w.steele at gmail.com  Wed Dec 12 22:35:51 2007
From: dale.w.steele at gmail.com (Dale Steele)
Date: Wed, 12 Dec 2007 16:35:51 -0500
Subject: [R] problem applying a conditional formula to each element of a
	matrix
Message-ID: <72e8303a0712121335le972756oe911e5612755fa06@mail.gmail.com>

I'm applying a function (Cov.f) defined below to each element of a
distance matrix.  When I run the code below, I get a warning message
(below) and elements of returned matrix [2,3] and [3,2] are not zero
as I would expect. Clearly, there is an error... What am I doing
wrong? Thanks.  --Dale

Warning message:
In if (h <= phi) { :
  the condition has length > 1 and only the first element will be used

# function

Cov.f <- function(h, sigmasq, phi) {
  if (h <= phi) {Cij <- sigmasq * (1 - ( 1.5 * (h/phi)  - 0.5 *
(h/phi)^3))  } else
  if (h > phi)  {Cij <- 0}
  return(Cij)
      }

x.coord <- c(5.7, 6.7, 9.8)
y.coord <- c(42.7, 10.2, 77.4)
coords <- cbind(x.coord, y.coord)
distance.matrix <- as.matrix(dist(coords, method="euclidean"))
distance.matrix
Cov.f(distance.matrix, 3.9, 58.1)


From gunter.berton at gene.com  Wed Dec 12 23:17:04 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 12 Dec 2007 14:17:04 -0800
Subject: [R] R and Excel Interface
In-Reply-To: <OF9247DAF5.7BFBA0BA-ON862573AF.00729E43-862573AF.00741C05@aphis.usda.gov>
References: <OF9247DAF5.7BFBA0BA-ON862573AF.00729E43-862573AF.00741C05@aphis.usda.gov>
Message-ID: <00bd01c83d0c$b9f915b0$3a0b2c0a@gne.windows.gene.com>

RSiteSearch("RExcel")

Please do at least start by using already available R information resources.



Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Antonio_Paredes at aphis.usda.gov
Sent: Wednesday, December 12, 2007 1:12 PM
To: r-help at r-project.org
Subject: [R] R and Excel Interface

Hello everyone,

I'll to request some input on what is available for use as an R/Excel 
interface; any help will be appreciated.

Tony. 
	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From vistocco at unicas.it  Wed Dec 12 23:16:30 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Wed, 12 Dec 2007 23:16:30 +0100
Subject: [R] R and Excel Interface
In-Reply-To: <OF9247DAF5.7BFBA0BA-ON862573AF.00729E43-862573AF.00741C05@aphis.usda.gov>
References: <OF9247DAF5.7BFBA0BA-ON862573AF.00729E43-862573AF.00741C05@aphis.usda.gov>
Message-ID: <47605DBE.2010507@unicas.it>

Antonio_Paredes at aphis.usda.gov wrote:
> Hello everyone,
>
> I'll to request some input on what is available for use as an R/Excel 
> interface; any help will be appreciated.
>
> Tony. 
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   
See the rcom package (working from R) or the RExcel add in (working from 
Excel).

Anyway RSiteSearch("R Excel interface") give you a lot of information.

domenico


From erinberryman at vandals.uidaho.edu  Wed Dec 12 23:45:39 2007
From: erinberryman at vandals.uidaho.edu (Erin Berryman)
Date: Wed, 12 Dec 2007 14:45:39 -0800
Subject: [R] xYplot problem
In-Reply-To: <eb555e660712121337y7c783315ne05bad0b1593e36d@mail.gmail.com>
References: <BAYC1-PASMTP02997BADEBDD97000E2FD6FF650@CEZ.ICE>
	<eb555e660712121337y7c783315ne05bad0b1593e36d@mail.gmail.com>
Message-ID: <BAYC1-PASMTP031A460344D70D38A8CCF5FF650@CEZ.ICE>


On Dec 12, 2007, at 1:37 PM, Deepayan Sarkar wrote:

> On 12/12/07, Erin Berryman <erinberryman at vandals.uidaho.edu> wrote:
>> Dear R community,
>>
>> Since upgrading to R v.2.6.1 and re-installing package Hmisc (binary
>> for Mac OS X v.3.4-3), I have been getting a error when trying to
>> make xYplots:
>>
>>> plotcv<-c(34.88, 41.51, 45.81, 51.05, 51.66)
>>> plotcv.se<-c(2.406551, 3.071291, 4.331407, 3.213873, 4.838150)
>>> month<-c(6, 7, 8, 9, 10)
>>> library(Hmisc)
>>> xYplot(Cbind(plotcv, plotcv + plotcv.se, plotcv - plotcv.se)  ~
>> month)
>>
>> Error in xyplot.formula(formula = Cbind(plotcv, plotcv + plotcv.se,
>> plotcv -  :
>>    argument "x" is missing, with no default
>>
>> I've used xYplots extensively in the past with no problems.
>>
>> I am using Mac OS X.4.10 (Tiger).
>>
>>> version
>> platform       powerpc-apple-darwin8.10.1
>> arch           powerpc
>> os             darwin8.10.1
>> system         powerpc, darwin8.10.1
>> status
>> major          2
>> minor          6.1
>> year           2007
>> month          11
>> day            26
>> svn rev        43537
>> language       R
>> version.string R version 2.6.1 (2007-11-26)
>>
>> Any help would be greatly appreciated!
>
> Are you sure you are actually _using_ the updated version? That is,
> did you try this in a fresh R session (and not the one in which you
> updated)?
>

Yes:

R version 2.6.1 (2007-11-26)
Copyright (C) 2007 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

Perhaps some mistake occurred when updating?

> -Deepayan

Erin


From vistocco at unicas.it  Wed Dec 12 23:45:10 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Wed, 12 Dec 2007 23:45:10 +0100
Subject: [R] problem applying a conditional formula to each element of
 a	matrix
In-Reply-To: <72e8303a0712121335le972756oe911e5612755fa06@mail.gmail.com>
References: <72e8303a0712121335le972756oe911e5612755fa06@mail.gmail.com>
Message-ID: <47606476.5050508@unicas.it>

The conditional have to be a single element:
 > ?"if"

 cond: A length-one logical vector that is not 'NA'. Conditions of
          length greater than one are accepted with a warning, but only
          the first element is used.  Other types are coerced to
          logical if possible, ignoring any class.

In your case you have a matrix of logical values:
 > distance.matrix <= 58.1
     1     2     3
1 TRUE  TRUE  TRUE
2 TRUE  TRUE FALSE
3 TRUE FALSE  TRUE

A possible solution (maybe not the better):

Cov.f <- function(h, sigmasq, phi) {
    Cij <- h
    Cij[h<=phi] <- sigmasq * (1 - ( 1.5 * (Cij[h<=phi]/phi) - 0.5 
*(Cij[h<=phi]/phi)^3))
    Cij[h>phi] <- 0
    return(Cij)
}

domenico

Dale Steele wrote:
> I'm applying a function (Cov.f) defined below to each element of a
> distance matrix.  When I run the code below, I get a warning message
> (below) and elements of returned matrix [2,3] and [3,2] are not zero
> as I would expect. Clearly, there is an error... What am I doing
> wrong? Thanks.  --Dale
>
> Warning message:
> In if (h <= phi) { :
>   the condition has length > 1 and only the first element will be used
>
> # function
>
> Cov.f <- function(h, sigmasq, phi) {
>   if (h <= phi) {Cij <- sigmasq * (1 - ( 1.5 * (h/phi)  - 0.5 *
> (h/phi)^3))  } else
>   if (h > phi)  {Cij <- 0}
>   return(Cij)
>       }
>
> x.coord <- c(5.7, 6.7, 9.8)
> y.coord <- c(42.7, 10.2, 77.4)
> coords <- cbind(x.coord, y.coord)
> distance.matrix <- as.matrix(dist(coords, method="euclidean"))
> distance.matrix
> Cov.f(distance.matrix, 3.9, 58.1)
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From jholtman at gmail.com  Wed Dec 12 23:50:45 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 12 Dec 2007 14:50:45 -0800
Subject: [R] problem applying a conditional formula to each element of a
	matrix
In-Reply-To: <72e8303a0712121335le972756oe911e5612755fa06@mail.gmail.com>
References: <72e8303a0712121335le972756oe911e5612755fa06@mail.gmail.com>
Message-ID: <644e1f320712121450k46e73fe9vd7da959dcf78604b@mail.gmail.com>

I think you want to use 'ifelse':

> Cov.f <- function(h, sigmasq, phi) {
+     ifelse(h <= phi, sigmasq * (1 - ( 1.5 * (h/phi)  - 0.5 * (h/phi)^3)), 0)
+ }
>
> x.coord <- c(5.7, 6.7, 9.8)
> y.coord <- c(42.7, 10.2, 77.4)
> coords <- cbind(x.coord, y.coord)
> distance.matrix <- as.matrix(dist(coords, method="euclidean"))
> distance.matrix
         1        2        3
1  0.00000 32.51538 34.94138
2 32.51538  0.00000 67.27146
3 34.94138 67.27146  0.00000
> Cov.f(distance.matrix, 3.9, 58.1)
          1         2         3
1 3.9000000 0.9678766 0.8059627
2 0.9678766 3.9000000 0.0000000
3 0.8059627 0.0000000 3.9000000
>

On Dec 12, 2007 1:35 PM, Dale Steele <dale.w.steele at gmail.com> wrote:
> I'm applying a function (Cov.f) defined below to each element of a
> distance matrix.  When I run the code below, I get a warning message
> (below) and elements of returned matrix [2,3] and [3,2] are not zero
> as I would expect. Clearly, there is an error... What am I doing
> wrong? Thanks.  --Dale
>
> Warning message:
> In if (h <= phi) { :
>  the condition has length > 1 and only the first element will be used
>
> # function
>
> Cov.f <- function(h, sigmasq, phi) {
>  if (h <= phi) {Cij <- sigmasq * (1 - ( 1.5 * (h/phi)  - 0.5 *
> (h/phi)^3))  } else
>  if (h > phi)  {Cij <- 0}
>  return(Cij)
>      }
>
> x.coord <- c(5.7, 6.7, 9.8)
> y.coord <- c(42.7, 10.2, 77.4)
> coords <- cbind(x.coord, y.coord)
> distance.matrix <- as.matrix(dist(coords, method="euclidean"))
> distance.matrix
> Cov.f(distance.matrix, 3.9, 58.1)
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From m_olshansky at yahoo.com  Wed Dec 12 23:50:48 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 12 Dec 2007 14:50:48 -0800 (PST)
Subject: [R] combine variables to matrix
In-Reply-To: <475FF795.1000601@gfz-potsdam.de>
Message-ID: <224010.86289.qm@web32204.mail.mud.yahoo.com>

You can always make a loop (V1 corresponds to column
1, etc.) but as.matrix() is simpler, i.e. in your case

data.matrix <- as.matrix(data)

--- Andre Jung <ajung at gfz-potsdam.de> wrote:

> I just got stuck with a quite simple question. I've
> just read in an 
> ASCII table from a plain text file with
> read.table(). It's a 1200x1200 
> table. R has assigned variables for each column:
> V1,V2,V3,V4,...
> For small data sets
> 
> data <- read.table("data.txt");
> data.matrix <- cbind(V1,V2,V3);
> 
> works. But how could I put together 1200 columns?
> 
> I've searched the R mailing help and stumbled upon
> this entry:
>
https://stat.ethz.ch/pipermail/r-help/2007-July/137121.html
> which doesn't help me.
> 
> thanks for your help.
> 
> andre
> 
> > ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From deepayan.sarkar at gmail.com  Thu Dec 13 00:42:50 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 12 Dec 2007 15:42:50 -0800
Subject: [R] xYplot problem
In-Reply-To: <BAYC1-PASMTP031A460344D70D38A8CCF5FF650@CEZ.ICE>
References: <BAYC1-PASMTP02997BADEBDD97000E2FD6FF650@CEZ.ICE>
	<eb555e660712121337y7c783315ne05bad0b1593e36d@mail.gmail.com>
	<BAYC1-PASMTP031A460344D70D38A8CCF5FF650@CEZ.ICE>
Message-ID: <eb555e660712121542w4d40382dhf1d23cfeb4dc5132@mail.gmail.com>

On 12/12/07, Erin Berryman <erinberryman at vandals.uidaho.edu> wrote:
>
> On Dec 12, 2007, at 1:37 PM, Deepayan Sarkar wrote:
>
> > On 12/12/07, Erin Berryman <erinberryman at vandals.uidaho.edu> wrote:
> >> Dear R community,
> >>
> >> Since upgrading to R v.2.6.1 and re-installing package Hmisc (binary
> >> for Mac OS X v.3.4-3), I have been getting a error when trying to
> >> make xYplots:
> >>
> >>> plotcv<-c(34.88, 41.51, 45.81, 51.05, 51.66)
> >>> plotcv.se<-c(2.406551, 3.071291, 4.331407, 3.213873, 4.838150)
> >>> month<-c(6, 7, 8, 9, 10)
> >>> library(Hmisc)
> >>> xYplot(Cbind(plotcv, plotcv + plotcv.se, plotcv - plotcv.se)  ~
> >> month)
> >>
> >> Error in xyplot.formula(formula = Cbind(plotcv, plotcv + plotcv.se,
> >> plotcv -  :
> >>    argument "x" is missing, with no default
> >>
> >> I've used xYplots extensively in the past with no problems.
> >>
> >> I am using Mac OS X.4.10 (Tiger).
> >>
> >>> version
> >> platform       powerpc-apple-darwin8.10.1
> >> arch           powerpc
> >> os             darwin8.10.1
> >> system         powerpc, darwin8.10.1
> >> status
> >> major          2
> >> minor          6.1
> >> year           2007
> >> month          11
> >> day            26
> >> svn rev        43537
> >> language       R
> >> version.string R version 2.6.1 (2007-11-26)
> >>
> >> Any help would be greatly appreciated!
> >
> > Are you sure you are actually _using_ the updated version? That is,
> > did you try this in a fresh R session (and not the one in which you
> > updated)?
> >
>
> Yes:
>
> R version 2.6.1 (2007-11-26)
> Copyright (C) 2007 The R Foundation for Statistical Computing
> ISBN 3-900051-07-0
>
> Perhaps some mistake occurred when updating?

I meant the version of Hmisc, but that should be fine too. Anyway, on
my installation the definition of xYplot looks fine (and works for
your example):

> xYplot
function (formula, data = sys.frame(sys.parent()), groups, subset,
    xlab = NULL, ylab = NULL, ylim = NULL, panel = panel.xYplot,
    prepanel = prepanel.xYplot, scales = NULL, minor.ticks = NULL,
    ...)
{

[...]

    do.call("xyplot", c(list(x = formula, data = data, prepanel = prepanel,
        panel = panel), if (length(ylab)) list(ylab = ylab),
        if (length(ylim)) list(ylim = ylim), if (length(xlab))
list(xlab = xlab),
        if (length(scales)) list(scales = scales), if
(length(minor.ticks)) list(minor.ticks = minor.ticks),
        if (!missing(groups)) list(groups = groups), if
(!missing(subset)) list(subset = subset),
        list(...)))
}
<environment: namespace:Hmisc>


If you see the same thing, in particular for the

    do.call("xyplot", c(list(x = formula, data = data,

part, then I don't have a solution for your problem.

> sessionInfo()
R version 2.7.0 Under development (unstable) (2007-11-21 r43517)
x86_64-unknown-linux-gnu

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base

other attached packages:
[1] Hmisc_3.4-3    lattice_0.17-2

loaded via a namespace (and not attached):
[1] cluster_1.11.9 grid_2.7.0


From m_olshansky at yahoo.com  Thu Dec 13 01:26:21 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 12 Dec 2007 16:26:21 -0800 (PST)
Subject: [R] iid.test package/ncdf package
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC302DF3BF9@BAN-MAILSRV03.Amba.com>
Message-ID: <531266.38457.qm@web32203.mail.mud.yahoo.com>

What do you mean by IID tests?

You certainly can create sequences of iid
variables/vectors in R (see ?rnorn, ?runif, ?mvrnorm
from package MASS, etc.).

--- Shubha Vishwanath Karanth
<shubhak at ambaresearch.com> wrote:

> Hi R,
> 
>  
> 
> I want to conduct iid tests. So, I went to download
> 'iid.test' package.
> And the dependent package is 'ncdf', which is not
> available. So, how do
> I conduct the IID tests? Can I do these iid tests in
> Base R?
> 
>  
> 
> Thanks in advance,
> 
> Shubha
> 
> This e-mail may contain confidential and/or
> privileged i...{{dropped:12}}
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From dhewitt at vims.edu  Thu Dec 13 01:57:53 2007
From: dhewitt at vims.edu (David Hewitt)
Date: Wed, 12 Dec 2007 16:57:53 -0800 (PST)
Subject: [R] Adding data labels to Lattice plots
In-Reply-To: <1f3dc4a0712120745j70f56daco3af02a552ef14bc9@mail.gmail.com>
References: <1f3dc4a0712120745j70f56daco3af02a552ef14bc9@mail.gmail.com>
Message-ID: <14307638.post@talk.nabble.com>




> I am new to lattice graphics...
> 

Me too, for the most part, so this might not be great advice.



> print(xyplot(MeanScore ~ PointsInTime, d2, groups = cat, type = 'o',
>         xlab = "Points in Time", ylab = "Mean Score", aspect = 0.7,
>         auto.key = list(points = TRUE, lines = TRUE, space = "right")))
> 

This will get the labels on the points, but you need a way to avoid
overlapping labels. And the xyplot is not maintaining the lines and grouping
in this new arrangement.

panel1 = function(x, y) {
     panel.xyplot(x, y)
     panel.text(PointsInTime, MeanScore,
          labels=round(MeanScore, digits=2), pos=2, offset=0.8)
     }

xyplot(MeanScore ~ PointsInTime, data=stuff, groups=cat, type='o',
        xlab = "Points in Time", ylab = "Mean Score", aspect = 0.7,
        auto.key = list(points = TRUE, lines = TRUE, space = "right"),
        panel=panel1)

-----
David Hewitt
Virginia Institute of Marine Science
http://www.vims.edu/fish/students/dhewitt/
-- 
View this message in context: http://www.nabble.com/Adding-data-labels-to-Lattice-plots-tp14297921p14307638.html
Sent from the R help mailing list archive at Nabble.com.


From rmh at temple.edu  Thu Dec 13 01:06:25 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 12 Dec 2007 19:06:25 -0500
Subject: [R] Writing a file to the disk
In-Reply-To: <000301c83c43$04859d80$2f01a8c0@dell2400>
Message-ID: <000001c83d24$8a157c40$0c01a8c0@RMHHP>

Since your target is a spreadsheet on Windows, consider writing
directly to the spreadsheet.  Use either the package xlsReadWrite
or the RExcel interface which allows you to embed R functions inside
Excel cells.  Look at the main RExcel/rcom site
http://sunsite.univie.ac.at/rcom/
and consider joining the mailing list there.


From rmh at temple.edu  Thu Dec 13 01:06:25 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 12 Dec 2007 19:06:25 -0500
Subject: [R] Writing a file to the disk
In-Reply-To: <000301c83c43$04859d80$2f01a8c0@dell2400>
Message-ID: <000101c83d24$8a93d400$0c01a8c0@RMHHP>

Since your target is a spreadsheet on Windows, it would make sense to write
directly to the spreadsheet.  You can use either the xlsReadWrite package,
   Description: Read and write Excelfiles natively (v97-2003/BIFF8)
or RExcel, which allows you to imbed R functions inside Excel cells.
Look at the main RExcel/rcom site
http://sunsite.univie.ac.at/rcom/
and consider joining the mailing list there.


From deepayan.sarkar at gmail.com  Thu Dec 13 02:18:01 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 12 Dec 2007 17:18:01 -0800
Subject: [R] OT: 3d surfaces with transparency
In-Reply-To: <475B20D7.5090504@ufl.edu>
References: <475B0BBE.2020009@ufl.edu>
	<eb555e660712081403h219b53f0vb6911bbea1f80caf@mail.gmail.com>
	<475B20D7.5090504@ufl.edu>
Message-ID: <eb555e660712121718k31a9c1b3uc1c87ccce718e9a5@mail.gmail.com>

On 12/8/07, Ben Bolker <bolker at ufl.edu> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Deepayan Sarkar wrote:
> > On 12/8/07, Ben Bolker <bolker at ufl.edu> wrote:
> >> -----BEGIN PGP SIGNED MESSAGE-----
> >> Hash: SHA1
> >>
> >>
> >>   I would be grateful if anyone had suggestions
> >> about software that could (1) create 3D surface
> >> plots, (2) handle transparency/alpha blending,
> >> (3) generate output in some vector graphics format
> >> that preserved the transparency.  I could also
> >> live with a combination of two programs, one
> >> to generate the basic figure and another to
> >> modify the output surface to a transparent
> >> color (but preserving vector-ness).
> >>
>  [snip]
> >>   Any ideas???
> >
> > You could consider wireframe from lattice, but this has many caveats.
> > For an example, see
> >
> > http://dsarkar.fhcrc.org/lattice/book/figures.html?chapter=06;figure=06_18;theme=stdColor;code=right
> >
> > For that matter, whats wrong with persp?
> >
> > -Deepayan
>
>
>   I hadn't thought about the fact that transparency is easier
> than it used to be (esp. with cairo device/PDF).
>   OK, next question:  is there an easier way than the following
> to create a 3D perspective plot with reference grids on some faces?
> I  can pull the 3D grid code out of the rgl or scatterplot3d
> packages and reimplement it here, I guess ...  another way to
> hack this might (?) be to play with tick lengths?

(I had been meaning to reply, but it slipped my mind.)

I can't see an easy way to do this for persp, but it's technically
possible (though not a one-liner) for cloud and wireframe. I can look
into it if you think it will help.

-Deepayan


From mtmorgan at fhcrc.org  Thu Dec 13 02:21:10 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Wed, 12 Dec 2007 17:21:10 -0800
Subject: [R] XML R function description
In-Reply-To: <BAY120-DAV9DDB82E8DAF6A3A520020B76B0@phx.gbl> (Teodor
	Krastev's message of "Mon, 10 Dec 2007 12:36:25 -0500")
References: <BAY120-DAV9DDB82E8DAF6A3A520020B76B0@phx.gbl>
Message-ID: <6ph8x3zqtix.fsf@gopher4.fhcrc.org>

Teodor --

Nice question! I don't think there is anything exactly like you are
looking for.

One problem is that in general R function arguments (and return
values) are not typed (!) and of course saying in the docs that a
function takes a particular type of argument is no guarantee that is
the case. A solution is to provide typing (e.g., via the TypeInfo
package in Bioconductor) or to restrict yourself to S4 methods (for
which at least some arguments are typed, though not generally return
values [though I believe they could be specified]).

A very different approach would recognize that R function arguments
are S-expressions of different types (raw, logical, character, int,
double, etc). Marking up R functions in XML then becomes the task of
(a) representing S-expressions or at least their content appropriately
(e.g., an important feature is to allow for NA; see StatDataML), (b)
identifying, perhaps on the fly since new packages are made available
all the time, functions to be exposed (with, e.g., 'get' and the
appropriate 'mode' argument), (c) determining the untyped signature
via 'formals', (d) creating an appropriate map between the (untyped)
signature and the XML representation of (a). This would mark up R
functions in XML without strong typing (i.e., with the level of type
specification currently available to R users!), but still seems,
naively, to be do-able.

I think it's worth asking whether you really want to expose 'all of R'
to C#, because this has significant security consequences (in a server
kind of environment) in addition to implicitly requiring use of the R
language semantics and evaluation model, and without some careful
thought needless movement of data across the R/C# boundary.

Mostly speculative ideas here, so please edit accordingly. Followup
definitely belongs in R-devel, where you might get some much more
informed input.

Others might suggest less exciting solutions, like (D)COM-based
packages referenced on the 'Other', packages, and FAQs links of the R
home page.

Martin

"Teodor Krastev" <Krustev at hotmail.com> writes:

> Hi,
>
> I was wondering if there is a standard for R function description in
> XML (or any plain text) format.
>
> Rd files are the closest thing I found, but they do not describe the
> argument (or return value) types.  The purpose would be to write a
> program to automate the creation of C# wrapper around any R
> function, but without the argument type Rd descriptions are useless.
>
> Any clues...
>
> thank you
> Teodor Krastev
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Computational Biology / Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M2 B169
Phone: (206) 667-2793


From m_olshansky at yahoo.com  Thu Dec 13 03:22:11 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 12 Dec 2007 18:22:11 -0800 (PST)
Subject: [R] Importing Large Dataset into Excel
In-Reply-To: <476023DF.1090202@u.washington.edu>
Message-ID: <454026.44176.qm@web32213.mail.mud.yahoo.com>

One can also do:

x <- readLines("newborn edit.csv",n=529)

and examine x[528] (or x[529] - depending on whether
line 528 was line 528 of the file or line 528 after
reading the header line).

--- Julian Burgos <jmburgos at u.washington.edu> wrote:

> Hi Wayne,
> 
> I'm assuming that you file is really a
> comma-separated file (*.csv) and 
> not an Excel workbook (*.xls) saved with a .csv
> extension, right?  That 
> (in my experience) is a common mistake.
> You should open your file with a simple text editor
> (notepad will do if 
> the file is not too large) and review line 528,
> instead of reviewing the 
> spreadsheet in Excel.  You should be able to spot
> the problem right away.
> 
> Julian
> 
> 
> Wayne Aldo Gavioli wrote:
> > Hello all,
> > 
> > 
> > I seem to be having a problem importing a data set
> from Excel into R.  I'm using
> > the "read.table" command to import the data with
> the following line of code:
> > 
> >> newborn<-read.table("newborn edit.csv", header=T,
> sep=",")
> > 
> > 
> > where "newborn edit.csv" is the name of the file. 
> Unfortunately, I'm getting
> > back the following error message:
> > 
> > 
> > "Error in scan(file,, what, nmax, sep, dc, quote,
> skip, nlines, na.string, :
> > line 528 did not have 44 elements"
> > 
> > 
> > As far as I can tell, line 528 of the spreadsheet
> table does have the same
> > number of elements as the other rows - by chance
> can this error message mean
> > anything else?  Also, is there an easier way to
> import data from R into Excel
> > using a single line of R code?
> > 
> > 
> > Thanks,
> > 
> > 
> > Wayne
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From bolker at ufl.edu  Thu Dec 13 04:09:48 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 12 Dec 2007 22:09:48 -0500
Subject: [R] OT: 3d surfaces with transparency
In-Reply-To: <eb555e660712121718k31a9c1b3uc1c87ccce718e9a5@mail.gmail.com>
References: <475B0BBE.2020009@ufl.edu>	
	<eb555e660712081403h219b53f0vb6911bbea1f80caf@mail.gmail.com>	
	<475B20D7.5090504@ufl.edu>
	<eb555e660712121718k31a9c1b3uc1c87ccce718e9a5@mail.gmail.com>
Message-ID: <4760A27C.8070204@ufl.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Deepayan Sarkar wrote:
> On 12/8/07, Ben Bolker <bolker at ufl.edu> wrote:
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> Deepayan Sarkar wrote:
>>> On 12/8/07, Ben Bolker <bolker at ufl.edu> wrote:
>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>> Hash: SHA1
>>>>
>>>>
>>>>   I would be grateful if anyone had suggestions
>>>> about software that could (1) create 3D surface
>>>> plots, (2) handle transparency/alpha blending,
>>>> (3) generate output in some vector graphics format
>>>> that preserved the transparency.  I could also
>>>> live with a combination of two programs, one
>>>> to generate the basic figure and another to
>>>> modify the output surface to a transparent
>>>> color (but preserving vector-ness).
>>>>
>>  [snip]
>>>>   Any ideas???
>>> You could consider wireframe from lattice, but this has many caveats.
>>> For an example, see
>>>
>>> http://dsarkar.fhcrc.org/lattice/book/figures.html?chapter=06;figure=06_18;theme=stdColor;code=right
>>>
>>> For that matter, whats wrong with persp?
>>>
>>> -Deepayan
>>
>>   I hadn't thought about the fact that transparency is easier
>> than it used to be (esp. with cairo device/PDF).
>>   OK, next question:  is there an easier way than the following
>> to create a 3D perspective plot with reference grids on some faces?
>> I  can pull the 3D grid code out of the rgl or scatterplot3d
>> packages and reimplement it here, I guess ...  another way to
>> hack this might (?) be to play with tick lengths?
> 
> (I had been meaning to reply, but it slipped my mind.)
> 
> I can't see an easy way to do this for persp, but it's technically
> possible (though not a one-liner) for cloud and wireframe. I can look
> into it if you think it will help.
> 
> -Deepayan

  I managed to hack this up OK in persp, I can send it/post it
if anyone is interested.

  If I find time I might try to hack persp to do the grids
internally.

  thanks ...
    Ben

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFHYKJ8c5UpGjwzenMRAlU0AKCd5Egbdp5bOPE17qn+Ux3x+1FzBwCePu6i
VEkX+OlTjYsRcPrDuTzK2Fc=
=IjMF
-----END PGP SIGNATURE-----


From spencer.graves at pdf.com  Thu Dec 13 05:07:09 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 12 Dec 2007 20:07:09 -0800
Subject: [R] Rd files with unknown encoding?
Message-ID: <4760AFED.5050908@pdf.com>


      How can I identify the problem generating a warning in R CMD check 
for "Rd files with unknown encoding"? 

      Google identified an email from John Fox with a reply from Brian 
Ripley about this last 12 Jun 2007.  This suggests that I may have 
accidentally entered some possibly non-printing character into the 
offending Rd file.  The message tells me which file, but I don't know 
which lines in the file.  Is there some way of finding the offending 
character(s) without laboriously running R CMD check after deleting 
different portions of the file until I isolate the problem? 

      Thanks,
      Spencer Graves


From epistat at gmail.com  Thu Dec 13 05:36:08 2007
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 13 Dec 2007 12:36:08 +0800
Subject: [R] Probelms on using gam(mgcv)
Message-ID: <2fc17e30712122036w40d46d12hbe7f3ae4fc845612@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/be7fae51/attachment.pl 

From ahimsa at camposarceiz.com  Thu Dec 13 05:50:06 2007
From: ahimsa at camposarceiz.com (ahimsa campos-arceiz)
Date: Thu, 13 Dec 2007 13:50:06 +0900
Subject: [R] [OT] vernacular names for circular diagrams
In-Reply-To: <05BE78B0CF1BBC4BBA4AA255568D8611029A9A24@EXCHANGE2VS1.campus.mcgill.ca>
References: <mailman.19.1197370804.9403.r-help@r-project.org>
	<p06002019c384ae89478c@192.168.1.10> <475FF149.20103@biostat.ku.dk>
	<05BE78B0CF1BBC4BBA4AA255568D8611029A9A24@EXCHANGE2VS1.campus.mcgill.ca>
Message-ID: <45e920ef0712122050q63a7cc0w2ac17ea859a2f31a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/1dbc69f8/attachment.pl 

From res90sx5 at verizon.net  Thu Dec 13 05:49:42 2007
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Wed, 12 Dec 2007 20:49:42 -0800
Subject: [R] Using predict()?
In-Reply-To: <475FDEB1.7020402@statistik.uni-dortmund.de>
References: <2E8AE992B157C0409B18D0225D0B476306CD9237@XCH-VN01.sph.ad.jhsph.edu>
	<475FDEB1.7020402@statistik.uni-dortmund.de>
Message-ID: <010f01c83d43$940b4990$0201a8c0@Aragorn>

> -----Original Message-----
> From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On Behalf
> Of Uwe Ligges
> Sent: Wednesday, December 12, 2007 5:14 AM
> To: Zembower, Kevin
> Cc: r-help at r-project.org
> Subject: Re: [R] Using predict()?
> 
> 
> 
> Zembower, Kevin wrote:
> > I'm trying to solve a homework problem using R. The problem gives a list
> > of cricket chirps per second and corresponding temperature, and asks to
> > give the equation for the linear model and then predict the temperature
> > to produce 18 chirps per second. So far, I have:
> >
> >> # Homework 11.2.1 and 11.3.3
> >> chirps <- scan()
> > 1: 20
> > 2: 16
> > 3: 19.8
> > 4: 18.4
> > 5: 17.1
> > 6: 15.5
> > 7: 14.7
> > 8: 17.1
> > 9: 15.4
> > 10: 16.2
> > 11: 15
> > 12: 17.2
> > 13: 16
> > 14: 17
> > 15: 14.4
> > 16:
> > Read 15 items
> >> temp <- scan()
> > 1: 88.6
> > 2: 71.6
> > 3: 93.3
> > 4: 84.3
> > 5: 80.6
> > 6: 75.2
> > 7: 69.7
> > 8: 82
> > 9: 69.4
> > 10: 83.3
> > 11: 79.6
> > 12: 82.5
> > 13: 80.6
> > 14: 83.5
> > 15: 76.3
> > 16:
> > Read 15 items
> >> chirps
> >  [1] 20.0 16.0 19.8 18.4 17.1 15.5 14.7 17.1 15.4 16.2 15.0 17.2 16.0
> > 17.0 14.4
> >> temp
> >  [1] 88.6 71.6 93.3 84.3 80.6 75.2 69.7 82.0 69.4 83.3 79.6 82.5 80.6
> > 83.5 76.3
> >> chirps.res <- lm(chirps ~ temp)
> >> summary(chirps.res)
> >
> > Call:
> > lm(formula = chirps ~ temp)
> >
> > Residuals:
> >      Min       1Q   Median       3Q      Max
> > -1.56146 -0.58088  0.02972  0.58807  1.53047
> >
> > Coefficients:
> >             Estimate Std. Error t value Pr(>|t|)
> > (Intercept) -0.31433    3.10963  -0.101 0.921028
> > temp         0.21201    0.03873   5.474 0.000107 ***
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > Residual standard error: 0.9715 on 13 degrees of freedom
> > Multiple R-Squared: 0.6975,     Adjusted R-squared: 0.6742
> > F-statistic: 29.97 on 1 and 13 DF,  p-value: 0.0001067
> >> # From the linear model summary output above, the equation for the
> > least squares line is:
> >> #    y = -0.3143 + 0.2120*x or chirps = -0.3143 + 0.2120*temp
> >>
> >
> > I can then determine the answer to the prediction, using algebra and R:
> >> pred_temp <- (18+0.3143)/0.2120
> >> pred_temp
> > [1] 86.3882
> >
> > However, I'd like to try to use the predict() function. Since 'chirps'
> > and 'temp' are just vectors of numbers, and not dataframes, these
> > failed:
> > predict(chirps.res, newdata=data.frame(chirp=18))
> > predict(chirps.res, newdata="chirp=18")
> > predict(chirps.res, newdata=18)
> >
> 
> 
> Well, "chirps" (not "chirp", BTW) was your *response* in the lm() call!
> Your new data has to be called "temp", otherwise either the regression
> did not make sense or you are confusing different things.
> 
> Best,
> Uwe Ligges
> 
> > Baltimore, Maryland  21202
> > 410-659-6139
> >

Kevin,

To add to what Uwe Ligges wrote, your model is modeling chirps as a function of temp.  It looks like you are trying to turn that around and use chirps to predict a temp.  predict() won't do that.  Check any introductory regression text of why you probably don't want to do that anyway.  If you want to predict temp from chirps you should run

temp.lm <- lm(temp ~ chirps)
predict(temp.lm, newdata=data.frame(chirps=18))

The slope and intercept will be different from that found in chirps.res (in the absence of a perfect correlation between chirps and temp).

Hope this is helpful,

Dan

Daniel Nordlund
Bothell, WA


From sleepingcell at gmail.com  Thu Dec 13 06:48:11 2007
From: sleepingcell at gmail.com (xiechao)
Date: Wed, 12 Dec 2007 21:48:11 -0800 (PST)
Subject: [R]  ggplot2: can not find "current.grobTree()"
Message-ID: <14310411.post@talk.nabble.com>


Dear All (and Hadley),

Is the function "current.grobTree()" implemented in ggplot2? According to
the draft ggplot2 book, on page 43, we can get a list of all grobs with
current.grobTree(). But when I try that, I get 'Error: could not find
function "current.grobTree"'.

However, when I try help(current.grobTree), I do get a "fake" document.

Best Regards
Xie Chao 
-- 
View this message in context: http://www.nabble.com/ggplot2%3A-can-not-find-%22current.grobTree%28%29%22-tp14310411p14310411.html
Sent from the R help mailing list archive at Nabble.com.


From M.LESNOFF at CGIAR.ORG  Thu Dec 13 06:52:16 2007
From: M.LESNOFF at CGIAR.ORG (Lesnoff, Matthieu (ILRI))
Date: Thu, 13 Dec 2007 08:52:16 +0300
Subject: [R] Defining the "random" term in function "negbin" of AOD
	package
In-Reply-To: <000b01c83d05$788593d0$6600a8c0@cpaulsen>
References: <000b01c83d05$788593d0$6600a8c0@cpaulsen>
Message-ID: <6BEABCD5CA640A44A848448A42A03B730442EEA7@ilrikeadx1.ILRI.CGIARAD.ORG>

Dear Caroline

> How could I adjust this to function with the "negbin" function?
> Specifically, what would I use for the required "random" term?

The random argument is used to specify either a global parameter "phi" (random = ~ 1) or specific parameters "phi" for the levels of a given group factor (random = ~ group) (see ?negbin for what represents "phi" in negbin: Var[y] = ? + phi * ?^2)

In your example, it shoud be: negbin(formula = ..., random = ~ 1, data = ...)

Note that if your model has too many parameters, negbin may fail to reach the MLE.

You also can try the package 'gamlss' on CRAN. For your example, you can use the function gamlss as follows:

fm <- gamlss(formula = ..., family = NBI, sigma.formula = ~ 1, data = ...)
summary(fm)

Regards

ML


--------------------------------------------------
Matthieu Lesnoff
International Livestock Research Institute (ILRI)
PO BOX 30709, Nairobi, 00100 GPO, Kenya
Tel:   Off: (+254) 20 422 3000 (ext. 4801)
       Res: (+254) 20 422 3134
       Mob: (+254) 725 785 570
       Sec: (+254) 20 422 3013 
Fax: (+254) 20 422 3001
Email: m.lesnoff at cgiar.org 
--------------------------------------------------          

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Caroline Paulsen
> Sent: 13 December 2007 00:25
> To: r-help at r-project.org
> Subject: [R] Defining the "random" term in function "negbin" 
> of AOD package
> 
> I have tried glm.nb in the MASS package, but many models (I 
> have 250 models with different combinations of predictors for 
> fish counts data) either fail to converge or even diverge.
> 
>  
> 
> I'm attempting to use the negbin function in the AOD package, 
> but am unsure what to use for the "random" term, which is 
> supposed to provide a right hand formula for the 
> overdispersion parameter. I'm not even sure what this 
> statement means. Any advice you have would be greatly appreciated.
> 
>  
> 
> negbin(formula, random, data, phi.ini = NULL, warnings = FALSE, 
>          na.action = na.omit, fixpar = list(),
>          hessian = TRUE, control = list(maxit = 2000), ...)
> 
>  
> 
> My largest model using glm.nb looks like this:
> 
>  
> 
> negBin.glm1 <- glm.nb(Count ~ offset(log(Tow.Area)) + Basin + 
> Bathy + Hypoxia + Period + Depth + Basin*Depth + Bathy*Depth 
> + Hypoxia*Depth +
> 
>               Period*Depth + Basin*Period + Bathy*Period + 
> Hypoxia*Period + Hypoxia:Period:Depth + Bathy:Period:Depth +
> 
>               Basin:Period:Depth, 
> control=glm.control(maxit=1000), method="glm.fit",
> 
>                 data=Combined.Counts.df)
> 
>  
> 
> 
>  
> 
>  
> 
>  
> 
> Caroline E. Paulsen
> 
> Masters Candidate
> 
> School of Aquatic and Fishery Sciences
> 
> University of Washington
> 
> phone: 206.852.9539
> 
> email: cpaulsen at u.washington.edu
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From adschai at optonline.net  Thu Dec 13 07:08:35 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Thu, 13 Dec 2007 06:08:35 +0000 (GMT)
Subject: [R] [Not R question] using java COM bridge to pass array of double
 through R(D)COM SetSymbol method
Message-ID: <e1c1cf023ad79.4760cc63@optonline.net>

Hi 

My apology as I am really getting stuck. I know that this is not appropriate to post here but I do not know where to turn to. In any case, any thoughts or experience that you could share would be really appreciated.

I have a need to use java to access R on windows. And I need to spawn each R process for each of my subtask in java application. That's why Rserve would not be the right tool (as far as I read on their web site). So I am left with R(D)COM.

I use java COM bridge like j-integra and JACOB. My problem is that I cannot pass array of double through its proxy. Somehow, pass something like:

proxy.setSymbol("test", new double[] {2, 4});

this will work. but if I do

proxy.setSymbol("test", new double[] {2.0, 4.0});

this won't work.  Nor proxy.setSymbol("test", new Object[] {2.0,4.0}); 

So I'm quite desperate about the setSymbol. I'm sure that some of you might have write wrapper around R and might be able to provide me some insight. Thank you so much in advance.

- adschai


From melissa.s.cline at gmail.com  Thu Dec 13 02:04:43 2007
From: melissa.s.cline at gmail.com (melissa cline)
Date: Wed, 12 Dec 2007 17:04:43 -0800
Subject: [R] what does cut(data, breaks=n) actually do?
Message-ID: <8c9747eb0712121704i718a6204gee58968c31bbb404@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/2d726286/attachment.pl 

From fernandsanche at yahoo.es  Wed Dec 12 23:06:20 2007
From: fernandsanche at yahoo.es (Fernando Sanchez)
Date: Wed, 12 Dec 2007 23:06:20 +0100 (CET)
Subject: [R] MARS model implementation
Message-ID: <551287.30352.qm@web23406.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071212/343bc2d7/attachment.pl 

From michael.watson at bbsrc.ac.uk  Thu Dec 13 08:48:12 2007
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 13 Dec 2007 07:48:12 -0000
Subject: [R] Importing Large Dataset into Excel
References: <1197429329.475f525155179@webmail.fas.harvard.edu>
	<476023DF.1090202@u.washington.edu>
Message-ID: <8975119BCD0AC5419D61A9CF1A923E9504AA22AD@iahce2ksrv1.iah.bbsrc.ac.uk>

It will be either:

1) line 528 contains a quote character somewhere: ' or "
2) line 528 contains a comma in one of the fields, not as a field separator
3) line 528 contains a comment character #


-----Original Message-----
From: r-help-bounces at r-project.org on behalf of Julian Burgos
Sent: Wed 12/12/2007 6:09 PM
To: Wayne Aldo Gavioli
Cc: r-help at r-project.org
Subject: Re: [R] Importing Large Dataset into Excel
 
Hi Wayne,

I'm assuming that you file is really a comma-separated file (*.csv) and 
not an Excel workbook (*.xls) saved with a .csv extension, right?  That 
(in my experience) is a common mistake.
You should open your file with a simple text editor (notepad will do if 
the file is not too large) and review line 528, instead of reviewing the 
spreadsheet in Excel.  You should be able to spot the problem right away.

Julian


Wayne Aldo Gavioli wrote:
> Hello all,
> 
> 
> I seem to be having a problem importing a data set from Excel into R.  I'm using
> the "read.table" command to import the data with the following line of code:
> 
>> newborn<-read.table("newborn edit.csv", header=T, sep=",")
> 
> 
> where "newborn edit.csv" is the name of the file.  Unfortunately, I'm getting
> back the following error message:
> 
> 
> "Error in scan(file,, what, nmax, sep, dc, quote, skip, nlines, na.string, :
> line 528 did not have 44 elements"
> 
> 
> As far as I can tell, line 528 of the spreadsheet table does have the same
> number of elements as the other rows - by chance can this error message mean
> anything else?  Also, is there an easier way to import data from R into Excel
> using a single line of R code?
> 
> 
> Thanks,
> 
> 
> Wayne
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Thu Dec 13 09:32:37 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 13 Dec 2007 09:32:37 +0100
Subject: [R] what does cut(data, breaks=n) actually do?
In-Reply-To: <8c9747eb0712121704i718a6204gee58968c31bbb404@mail.gmail.com>
References: <8c9747eb0712121704i718a6204gee58968c31bbb404@mail.gmail.com>
Message-ID: <4760EE25.6020707@biostat.ku.dk>

melissa cline wrote:
> Hello,
>
> I'm trying to bin a quantity into 2-3 bins for calculating entropy and
> mutual information.  One of the approaches I'm exploring is the cut()
> function, which is what the mutualInfo function in binDist uses.  When it's
> called in the format cut(data, breaks=n), it somehow splits the data into n
> distinct bins.  Can anyone tell me how cut() decides where to cut?
>
>   
This is one case where reading the actual R code is easier that 
explaining what it does.  From cut.default

    if (length(breaks) == 1) {
        if (is.na(breaks) | breaks < 2)
            stop("invalid number of intervals")
        nb <- as.integer(breaks + 1)
        dx <- diff(rx <- range(x, na.rm = TRUE))
        if (dx == 0)
            dx <- rx[1]
        breaks <- seq.int(rx[1] - dx/1000, rx[2] + dx/1000, length.out = nb)
    }

so basically it takes the range, extends it a bit and splits in into 
<breaks> equally long segments.

(For the sometimes more attractive option of splitting into groups of 
roughly equal size, there is cut2 in the Hmisc package, or use quantile())

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From vistocco at unicas.it  Thu Dec 13 10:17:20 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Thu, 13 Dec 2007 10:17:20 +0100
Subject: [R] what does cut(data, breaks=n) actually do?
In-Reply-To: <8c9747eb0712121704i718a6204gee58968c31bbb404@mail.gmail.com>
References: <8c9747eb0712121704i718a6204gee58968c31bbb404@mail.gmail.com>
Message-ID: <4760F8A0.708@unicas.it>

cut(data, breaks=n)
splits the data in n bins of (approximately) the same size.

The used size is obtained by:
max(data) - min(data)
------------------------------------
                 n

 > x=rnorm(x)
 > cut(x,breaks=3)
 [1] (1.79,9.97]  (-6.39,1.79] (9.97,18.2]  (9.97,18.2]  (-6.39,1.79]
 [6] (1.79,9.97]  (-6.39,1.79] (1.79,9.97]  (-6.39,1.79] (-6.39,1.79]
Levels: (-6.39,1.79] (1.79,9.97] (9.97,18.2]

Then you have:
 > 18.2-9.97
[1] 8.23
 > 9.97-1.79
[1] 8.18
 > 1.79+6.39
[1] 8.18
 >

 > (max(x)-min(x))/3
[1] 8.164187

I don't know the reasons for the little differences (I am wondering about).
I hope it is useful.
domenico

melissa cline wrote:
> Hello,
>
> I'm trying to bin a quantity into 2-3 bins for calculating entropy and
> mutual information.  One of the approaches I'm exploring is the cut()
> function, which is what the mutualInfo function in binDist uses.  When it's
> called in the format cut(data, breaks=n), it somehow splits the data into n
> distinct bins.  Can anyone tell me how cut() decides where to cut?
>
> Thanks,
>
> Melissa
>
>
>
> ---------------------------------------------------------------
> Melissa Cline, Independent Investigator
> MCD Biology, UCSC
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From florencio.gonzalez at gi.ieo.es  Thu Dec 13 10:18:17 2007
From: florencio.gonzalez at gi.ieo.es (florencio.gonzalez at gi.ieo.es)
Date: Thu, 13 Dec 2007 10:18:17 +0100
Subject: [R] confidence intervals for y predicted in non linearregression
In-Reply-To: <20071213050913.GQ6584@slingshot.co.nz>
References: <9BD63C7EDBFC234DB982DF636397395E06C9C7@oceanoastur.gi.ieo.es> 
	<20071213050913.GQ6584@slingshot.co.nz>
Message-ID: <1197537497.4760f8d9467d4@webmail.ieo.es>


I?m sorry for no explain this before. It?s nls2 from INRA not from CRAN that i
was unsecesfull to install.

Mensaje citado por Patrick Connolly <p_connolly at slingshot.co.nz>:

> On Wed, 05-Dec-2007 at 01:40PM +0100, Florencio Gonz?lez wrote:
>
> |>
> |> Hi Thanks for your suggestion, I'm trying to install this package in
> Ubuntu
> |> (7.10) but unsuccessfully. Also tried in MacOSX, and no success too.
>
> You'll have to tell us what was 'unsuccessful' about it.  I see
> there's a newer version (7/12/07) on CRAN now.  Note that you can't
> use an R version older than 2.6.0.
>
> HTH
>
> --
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>    ___    Patrick Connolly
>  {~._.~}          		 Great minds discuss ideas
>  _( Y )_  	  	        Middle minds discuss events
> (:_~*~_:) 	       		 Small minds discuss people
>  (_)-(_)  	                           ..... Anon
>
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>




----------------------------------------------------------------
Este Mensaje fue enviado a traves de https://webmail.ieo.es

La informaci?n contenida en este e-mail y sus ficheros adjuntos es totalmente confidencial y no deber?a ser usado si no fuera usted alguno de los destinatarios. Si ha recibido este e-mail por error, por favor avise al remitente y b?rrelo de su buz?n o de cualquier otro medio de almacenamiento.   This email is confidential and should not be used by anyone who is not the original intended  recipient. If you have received this e-mail in  error please inform the sender and delete it from  your mailbox or any other storage mechanism.


From jo.irisson at gmail.com  Thu Dec 13 10:45:32 2007
From: jo.irisson at gmail.com (jiho)
Date: Thu, 13 Dec 2007 10:45:32 +0100
Subject: [R] use ggplot in a function to which a column name is given
Message-ID: <6C8569E6-BE75-44C6-862D-00F5082DADF9@gmail.com>

Hi everyone, Hi ggplot users in particular,

ggplot makes it very easy to plot things given their names when you  
use it interactively (and therefore can provide the names of the  
columns).
	qplot(x,foo,data=A) where A has columns (x,y,foo,bar) for example

but I would like to use this from inside a function to which the name  
of the column is given. I cannot find an elegant way to make this  
work. Here are my attempts:

#------------------------------------------------------------

library(ggplot2)

A = data.frame(x=rep(1:10,10), y=rep(1:10,each=10), u=runif(100),  
v=rnorm(100))

# goal: extract values for y<=5 and plot them, either for u or v

foo1 <- function(uv="u")
{
	# solution 1: do not use the data argument at all
	# 	(forces the use of qplot, could be more elegant)
	B = A[A$y<=5,]
	qplot(B$x, B$y, fill=B[[uv]], geom="tile")
}

foo2 <- function(uv="u")
{
	# solution 2: extract and rename the colums, then use the data argument
	# 	(enables ggplot but could be shorter)
	B = A[A$y<=5,c("x","y",uv)]
	names(B)[3] = "value"
	# rem: cannot use rename since rename(B,c(uv="value")) would not work
	qplot(x, y, fill=value, data=B, geom="tile")
	# or
	# ggplot(B,aes(x=x,y=y,fill=value)) + geom_tile()
}

foo3 <- function(uv="u")
{
	# solution 3: use the data argument and perform the extraction  
directly in it
	#	(elegant and powerful but can't make it work)
	ggplot(A[A$y<=5,c("x","y",uv)],aes(x=x,y=y,fill=???)) + geom_tile()
	# or
	ggplot(A[A$y<=5,],aes(x=x,y=y,fill=???)) + geom_tile()
	# or ...	
}

print(foo1("u"))
print(foo1("v"))
print(foo2("u"))
print(foo3("u"))

#------------------------------------------------------------

Any help in making foo3 work would be appreciated. Thanks in advance  
for your expertise.

JiHO
---
http://jo.irisson.free.fr/


From info at aghmed.fsnet.co.uk  Thu Dec 13 10:48:55 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Thu, 13 Dec 2007 09:48:55 +0000
Subject: [R] Summary, was Re:  Nine questions about methods and generics
In-Reply-To: <Zen-1J0Fg4-0005As-ON@heisenberg.zen.co.uk>
References: <Zen-1J0Fg4-0005As-ON@heisenberg.zen.co.uk>
Message-ID: <Zen-1J2kgf-0007m8-Oi@pythagoras.zen.co.uk>

At 12:17 06/12/2007, Michael Dewey wrote:
>I have a series of question about methods and generics.

To avoid extra clutter I have deleted the rather long question which 
is obtainable from
https://www.stat.math.ethz.ch/pipermail/r-help//2007-December/147754.html

I received a very helpful response off-list the gist of which I 
summarise below. Since it was off-list I have removed the attribution.

S Poetry might help you.  I believe that it does answer
why the answer to Q5 is yes.  It doesn't matter what
argument name you choose -- just be consistent.
And of course S Poetry is available from:
http://www.burns-stat.com




Michael Dewey
http://www.aghmed.fsnet.co.uk


From jo.irisson at gmail.com  Thu Dec 13 10:52:42 2007
From: jo.irisson at gmail.com (jiho)
Date: Thu, 13 Dec 2007 10:52:42 +0100
Subject: [R] use ggplot in a function to which a column name is given
In-Reply-To: <6C8569E6-BE75-44C6-862D-00F5082DADF9@gmail.com>
References: <6C8569E6-BE75-44C6-862D-00F5082DADF9@gmail.com>
Message-ID: <E172DC2D-D602-40A1-B3C2-D10334DE82E1@gmail.com>

Follow up.

On 2007-December-13  , at 10:45 , jiho wrote:
> foo1 <- function(uv="u")
> {
> 	# solution 1: do not use the data argument at all
> 	# 	(forces the use of qplot, could be more elegant)
> 	B = A[A$y<=5,]
> 	qplot(B$x, B$y, fill=B[[uv]], geom="tile")
> }

---> actually this does not even work currently:
	Error in eval(expr, envir, enclos) : object "B" not found
Which only leaves the most inelegant solution: 2

JiHO
---
http://jo.irisson.free.fr/


From ripley at stats.ox.ac.uk  Thu Dec 13 11:16:25 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 13 Dec 2007 10:16:25 +0000 (GMT)
Subject: [R] Rd files with unknown encoding?
In-Reply-To: <4760AFED.5050908@pdf.com>
References: <4760AFED.5050908@pdf.com>
Message-ID: <Pine.LNX.4.64.0712131006040.7003@gannet.stats.ox.ac.uk>

On Wed, 12 Dec 2007, Spencer Graves wrote:

>
>      How can I identify the problem generating a warning in R CMD check
> for "Rd files with unknown encoding"?
>
>      Google identified an email from John Fox with a reply from Brian
> Ripley about this last 12 Jun 2007.

But not on this list:

https://stat.ethz.ch/pipermail/r-devel/2007-June/046055.html

R-devel would have been more appropriate for this too.

>  This suggests that I may have accidentally entered some possibly 
> non-printing character into the offending Rd file.  The message tells me 
> which file, but I don't know which lines in the file.  Is there some way 
> of finding the offending character(s) without laboriously running R CMD 
> check after deleting different portions of the file until I isolate the 
> problem?

I did say so in that thread:

https://stat.ethz.ch/pipermail/r-devel/2007-June/046061.html

You can do much the same in R via iconv("", "C", sub="byte"), provided you 
can read the file in (it may not be representable in your current 
locale, but you could run R in a Latin-1 locale, if your OS has one).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jim at bitwrit.com.au  Thu Dec 13 12:01:54 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 13 Dec 2007 22:01:54 +1100
Subject: [R] Measure of agreement??
In-Reply-To: <2BAF2D3C41D1274E9228E63287F19B7E4FD4B8@mailsrv2.loginmpa.mpa.se>
References: <2BAF2D3C41D1274E9228E63287F19B7E4FD4B8@mailsrv2.loginmpa.mpa.se>
Message-ID: <47611122.9060605@bitwrit.com.au>

?hagen Patrik wrote:
> 
> Dear List,
> 
> Please put me on the right track. I have been searching for the R functions for measures of agreement. Where should I look?
> 
Hi Patrik,
Tried the irr package?

Jim


From tiago17 at gmail.com  Thu Dec 13 12:28:54 2007
From: tiago17 at gmail.com (=?windows-1252?Q?Tiago_R_Magalh=E3es?=)
Date: Thu, 13 Dec 2007 11:28:54 +0000
Subject: [R] Flushing (Reset) 'last.warning'
Message-ID: <47611776.1090001@gmail.com>

Dear members of the mailing list,

I want to fetch warnings() from a series of prop.test calls. I want to
get "none" if no warning is issued, and "warning" if there is a problem.

I have looked (and relooked) at options(warn) and warning(), warnings()
and 'last.warning' but to no avail. I am a biologist, so the "R language
areas" are hard to fully understand.

I cannot flush the warnings before each prop.test calls; hence if there
is no warning, instead of "" I get (obviously) the last.warning from the
previous call. I tried to remove 'last.warning' but it won't let me
(wisely?) touch the base environment.

Code below. And if anyone could give some pointers it would be great.

Thank you, kind list
###############
collect <- vector('character', 3)
vec.1 <- c(10,19); total.1 <- c(24,35)
vec.2 <- c(5,12); total.2 <- c(50,211)

prop.test(vec.1, total.1); collect[1] <- names(warnings())
prop.test(vec.2, total.2); collect[2] <- names(warnings())
prop.test(vec.1, total.1); collect[3] <- names(warnings())

collect
[1] ""
[2] "Chi-squared approximation may be incorrect"
[3] "Chi-squared approximation may be incorrect"

I wanted
[1] ""
[2] "Chi-squared approximation may be incorrect"
[3] ""


From murdoch at stats.uwo.ca  Thu Dec 13 12:48:00 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 13 Dec 2007 06:48:00 -0500
Subject: [R] Flushing (Reset) 'last.warning'
In-Reply-To: <47611776.1090001@gmail.com>
References: <47611776.1090001@gmail.com>
Message-ID: <47611BF0.5090408@stats.uwo.ca>

Tiago R Magalh?es wrote:
> Dear members of the mailing list,
>
> I want to fetch warnings() from a series of prop.test calls. I want to
> get "none" if no warning is issued, and "warning" if there is a problem.
>
> I have looked (and relooked) at options(warn) and warning(), warnings()
> and 'last.warning' but to no avail. I am a biologist, so the "R language
> areas" are hard to fully understand.
>
> I cannot flush the warnings before each prop.test calls; hence if there
> is no warning, instead of "" I get (obviously) the last.warning from the
> previous call. I tried to remove 'last.warning' but it won't let me
> (wisely?) touch the base environment.
>
> Code below. And if anyone could give some pointers it would be great.
>
> Thank you, kind list
> ###############
> collect <- vector('character', 3)
> vec.1 <- c(10,19); total.1 <- c(24,35)
> vec.2 <- c(5,12); total.2 <- c(50,211)
>
> prop.test(vec.1, total.1); collect[1] <- names(warnings())
> prop.test(vec.2, total.2); collect[2] <- names(warnings())
> prop.test(vec.1, total.1); collect[3] <- names(warnings())
>
> collect
> [1] ""
> [2] "Chi-squared approximation may be incorrect"
> [3] "Chi-squared approximation may be incorrect"
>
> I wanted
> [1] ""
> [2] "Chi-squared approximation may be incorrect"
> [3] ""
>   
One workaround:  when you want to clear the old warning, issue a blank 
one.  For example:

warning("")
prop.test(vec.1, total.1); collect[1] <- names(warnings())
warning("")
prop.test(vec.2, total.2); collect[2] <- names(warnings())
warning("")
prop.test(vec.1, total.1); collect[3] <- names(warnings())

This puts a lot of spurious lines like

Warning message:
 
into your output; you might prefer to use text like "Resetting warning 
message" instead of a blank.

Duncan Murdoch


From sw283 at maths.bath.ac.uk  Thu Dec 13 13:04:13 2007
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Thu, 13 Dec 2007 12:04:13 +0000 (GMT)
Subject: [R] Probelms on using gam(mgcv)
In-Reply-To: <2fc17e30712122036w40d46d12hbe7f3ae4fc845612@mail.gmail.com>
References: <2fc17e30712122036w40d46d12hbe7f3ae4fc845612@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0712131158570.23339@archer.maths.bath.ac.uk>

A smooth of `disbinary' doesn't really make sense. It should enter the 
model parametrically. The problem is easy to see in 1D. Consider the 
model
y_i = f(x_i) + error_i
where `f' is a smooth function. If x_i only takes the values 0 and 1 
then you really don't have much of a basis for estimating f.


>- Simon Wood, Mathematical Sciences, University of Bath, Bath BA2 7AY
>-             +44 (0)1225 386603         www.maths.bath.ac.uk/~sw283/


On Thu, 13 Dec 2007, zhijie zhang wrote:

> Dear all,
>   Following the help from gam(mgcv) help page, i tried to analyze my
> dataset with all the default arguments. Unfortunately, it can't be run
> successfully. I list the errors below.
>
> #m.gam<-gam(mark~s(x,y)+s(lstday2004)+s(slope)+s(ndvi2004)+s(elevation)+s(disbinary),family=binomial(logit),data=point)
>
> m.gam<-gam(mark~s(x,y,k=10)+s(lstday2004,k=10)+s(slope,k=10)+s(ndvi2004,k=10)+s(elevation,k=10)+s(disbinary,k=10),family=binomial(logit),data=point)
>
>> m.gam<-gam(mark~s(x.1,y.1,k=10)+s(lstday2004,k=10)+s(slope,k=10)+s(ndvi2004,k=10)+s(elevation,k=10)+s(disbinary,k=10),family=binomial(logit),data=point)
> #??????
> Errors: smooth.construct.tp.smooth.spec(object, data, knots) :
>  A term has fewer unique covariate combinations than specified maximum
> degrees of freedom.
>
> #========================#
>  Descriptions for my dataset
> #========================#
>  There are 1 response variable "mark" (1-case,0-control), and 7 explanatory
> covariables.
> x/y are spatial coordinates,
> lstday2004, slope, ndvi2004, elevation: continuous variables;
> disbinary : binary variable.
>  Is there any problems with the above codes? How should i solve it?
>  Any help or suggestions are greatly appreciated.
> -- 
> With Kind Regards,
>
> oooO:::::::::
> (..):::::::::
> :\.(:::Oooo::
> ::\_)::(..)::
> :::::::)./:::
> ::::::(_/::::
> :::::::::::::
> [***********************************************************************]
> Zhi Jie,Zhang ,PHD
> Tel:+86-21-54237149
> Dept. of Epidemiology,School of Public Health,Fudan University
> Address:No. 138 Yi Xue Yuan Road,Shanghai,China
> Postcode:200032
> Email:epistat at gmail.com
> Website: www.statABC.com <http://www.statabc.com/>
> [***********************************************************************]
> oooO:::::::::
> (..):::::::::
> :\.(:::Oooo::
> ::\_)::(..)::
> :::::::)./:::
> ::::::(_/::::
> :::::::::::::
>


From pbarros at ualg.pt  Thu Dec 13 13:09:11 2007
From: pbarros at ualg.pt (Pedro de Barros)
Date: Thu, 13 Dec 2007 12:09:11 +0000
Subject: [R] ggplot - Setting the y-scale in a bar plot
In-Reply-To: <f8e6ff050712111501i14ef7097j3f7071c1121667fe@mail.gmail.co
 m>
References: <20071211182017.9DEECF8001@smtp3.ualg.pt>
	<f8e6ff050712111315s623d86ma571c690895e25ec@mail.gmail.com>
	<20071211220403.3AA6BF8001@smtp3.ualg.pt>
	<f8e6ff050712111501i14ef7097j3f7071c1121667fe@mail.gmail.com>
Message-ID: <20071213120911.56939F8001@smtp3.ualg.pt>

Hi Hadley,

Thanks for the help.

I found the problem. In the code you sent me earlier to correct the 
scale_y_continuous,  (below) the parameter is "limits", rather than 
"limit", as you write it. If I run my code without introducing the 
change you sent me, it works, but if I first correct it, then I have 
to use the changed spelling.

Thanks again,
Pedro
PS: If you care to explain: why do all parameters in the code below 
have a "." before the name, except precisely "limits"? I know it has 
to do with "proto", but could not find out why this one was different.

=================
[...] The fix will be included in the next version of
ggplot, or you can fix the current version by running this code:

ScaleContinuous$new <- function(., name=NULL, limits=c(NA,NA),
breaks=NULL, labels=NULL, variable, trans="identity", expand=c(0.05,
0)) {
     if (is.null(breaks) && !is.null(labels)) stop("Labels can only be
specified in conjunction with breaks")

     .$proto(name=name, .input=variable, .output=variable,
limits=limits, .breaks = breaks, .labels = labels, .expand=expand)
   }

==============================================================
At 23:01 2007/12/11, you wrote:
>Hi Pedro,
>
>It seems to work for me:
>
>qplot(mpg, wt, data=mtcars)
>qplot(mpg, wt, data=mtcars) + scale_y_continuous(limit=c(4,5))
>
>Maybe I don't understand what you're trying to do.
>
>Hadley
>
>On 12/11/07, Pedro de Barros <pbarros at ualg.pt> wrote:
> > Hi Hadley,
> >
> > Well, the problem seems to be that ggplot is not recognizing the
> > scale when set by scale_y_continuous, the maximum value stays exactly
> > at the data range, irrespective of what I provide as range.
> > I am not familiar yet with proto, therefore I do have some difficulty
> > delving into the code to find out exactly what is wrong, but I hope
> > you can tell if I am doing something pretty stupid, or f it is a
> > feature or a bug...
> >
> > Cheers,
> > Pedro
> > At 21:15 2007/12/11, you wrote:
> > >Hi Pedro,
> > >
> > >What's the problem exactly?  You'll need to compute the range
> > >yourself, and then use scale_y_continuous as you have below.
> > >
> > >(Also you can abbreviate the bar chart plotting command to:
> > >qplot(x, y, data=plotdata, geom="bar", stat="identity"))
> > >
> > >Hadley
> > >
> > >On 12/11/07, Pedro de Barros <pbarros at ualg.pt> wrote:
> > > > Dear All (probably Hadley),
> > > >
> > > > I am now trying to customise some plots using a bar geom.
> > > >
> > > > I do not want to use the default binning statistic, but rather
> > > > calculate the bar heigths separately. I do manage this, but for
> > > > comparison purposes I would like to have a set of plots all with the
> > > > same y-axis height. But I do not seem to find out how to fix the
> > > > scale of the y-axis in this case.
> > > > Any tips?
> > > > Using R 2.6.1 on Windows.
> > > >
> > > > Thanks for any help,
> > > > Pedro
> > > >
> > > > I attach below the code I am using:
> > > > plotdata<-data.frame(x=factor(2:8), y=0.1*(2:8))
> > > > plot1<-ggplot()
> > > > plot1<-plot1+layer(data=plotdata,
> > > > mapping=aes_string(x='x',y='y'),geom='bar', stat='identity')
> > > >
> > > > RangeY <-c(0,1)
> > > > YBreaks <- (0:10)*diff(RangeY)/10
> > > > YTickLabels<- as.character(YBreaks)
> > > >
> > > > plot2 <- plot1 + scale_y_continuous(limits=RangeY, breaks=YBreaks,
> > > > labels=YTickLabels, expand=c(0,0))
> > > > print(plot2)
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > >
> > >--
> > >http://had.co.nz/
> >
> >
>
>
>--
>http://had.co.nz/


From megh700004 at yahoo.com  Thu Dec 13 13:30:51 2007
From: megh700004 at yahoo.com (Megh Dal)
Date: Thu, 13 Dec 2007 04:30:51 -0800 (PST)
Subject: [R] Number of ways to select population members
Message-ID: <738094.29136.qm@web58114.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/2a7ce883/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Thu Dec 13 13:41:46 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 13 Dec 2007 13:41:46 +0100
Subject: [R] Number of ways to select population members
References: <738094.29136.qm@web58114.mail.re3.yahoo.com>
Message-ID: <003f01c83d85$85d96220$0540210a@www.domain>

try something like this:

combn(LETTERS[1:3], 2, paste, collapse = "")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Megh Dal" <megh700004 at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, December 13, 2007 1:30 PM
Subject: [R] Number of ways to select population members


> Hi all,
>
>  Suppose I have a population of 3 alphabets : A, B, C. From this 
> population, number of ways that any 2 can be chosen is 3 i.e. AB, 
> AC, and BC.
>
>  Is there any R function to generalize this process, for any number 
> of alphabets/numbers and for any sub-sample size?
>
>  Thanks and regards,
>
>
>
> ---------------------------------
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From chrysopa at gmail.com  Thu Dec 13 14:16:19 2007
From: chrysopa at gmail.com (Ronaldo Reis Junior)
Date: Thu, 13 Dec 2007 11:16:19 -0200
Subject: [R] Very simple question on plot
Message-ID: <200712131116.19592.chrysopa@gmail.com>

Hi,

I try to make a plot like this:

Y |
  |
  |               o  
  |         o        o
  |     o               o
  |  o                     o
  |o                         o   o
  |--------------------------------
   0 10 20 30 40 50 60 70 80 90 100 (A)
 100 90 80 70 60 50 40 30 20 10 0   (B)

or
		B
  100 90 80 70 60 50 40 30 20 10 0
Y |--------------------------------
  |
  |               o  
  |         o        o
  |     o               o
  |  o                     o
  |o                         o   o
  |--------------------------------
   0 10 20 30 40 50 60 70 80 90 100
		A

A and B are complementary variables.

How is the best way to make this plot?

Thanks
Ronaldo
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. de Biologia Computacional
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8187 | ronaldo.reis em unimontes.br | chrysopa em gmail.com
| http://www.ppgcb.unimontes.br/ | ICQ#: 5692561 | LinuxUser#: 205366


From murdoch at stats.uwo.ca  Thu Dec 13 14:31:34 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 13 Dec 2007 08:31:34 -0500
Subject: [R] Very simple question on plot
In-Reply-To: <200712131116.19592.chrysopa@gmail.com>
References: <200712131116.19592.chrysopa@gmail.com>
Message-ID: <47613436.90106@stats.uwo.ca>

On 12/13/2007 8:16 AM, Ronaldo Reis Junior wrote:
> Hi,
> 
> I try to make a plot like this:
> 
> Y |
>   |
>   |               o  
>   |         o        o
>   |     o               o
>   |  o                     o
>   |o                         o   o
>   |--------------------------------
>    0 10 20 30 40 50 60 70 80 90 100 (A)
>  100 90 80 70 60 50 40 30 20 10 0   (B)
> 
> or
> 		B
>   100 90 80 70 60 50 40 30 20 10 0
> Y |--------------------------------
>   |
>   |               o  
>   |         o        o
>   |     o               o
>   |  o                     o
>   |o                         o   o
>   |--------------------------------
>    0 10 20 30 40 50 60 70 80 90 100
> 		A
> 
> A and B are complementary variables.
> 
> How is the best way to make this plot?
> 

Plot Y versus A, then use the axis() function to add another axis and 
mtext to label it.  For example,

Y <- rnorm(101)
A <- 0:100
plot(A, Y)
B <- 100-A
ticks <- pretty(B)
axis(side=3, at=100-ticks, labels=ticks)
mtext("B", side=3, line=3)

You may need to adjust par(mar=) to get large enough margins for some 
more elaborate versions.

Duncan Murdoch


From david.barron at sbs.ox.ac.uk  Thu Dec 13 14:32:04 2007
From: david.barron at sbs.ox.ac.uk (David Barron)
Date: Thu, 13 Dec 2007 13:32:04 +0000
Subject: [R] Very simple question on plot
In-Reply-To: <200712131116.19592.chrysopa@gmail.com>
References: <200712131116.19592.chrysopa@gmail.com>
Message-ID: <f21f775b0712130532n4ae47931rab3eb0a523781037@mail.gmail.com>

Something like this?

> x<-0:100
> y<-dnorm(x,50,20)
> plot(x,y)
> axis(3,seq(0,100,by=20),labels=seq(100,0,by=-20))


On 12/13/07, Ronaldo Reis Junior <chrysopa at gmail.com> wrote:
> Hi,
>
> I try to make a plot like this:
>
> Y |
>   |
>   |               o
>   |         o        o
>   |     o               o
>   |  o                     o
>   |o                         o   o
>   |--------------------------------
>    0 10 20 30 40 50 60 70 80 90 100 (A)
>  100 90 80 70 60 50 40 30 20 10 0   (B)
>
> or
>                 B
>   100 90 80 70 60 50 40 30 20 10 0
> Y |--------------------------------
>   |
>   |               o
>   |         o        o
>   |     o               o
>   |  o                     o
>   |o                         o   o
>   |--------------------------------
>    0 10 20 30 40 50 60 70 80 90 100
>                 A
>
> A and B are complementary variables.
>
> How is the best way to make this plot?
>
> Thanks
> Ronaldo
> --
> > Prof. Ronaldo Reis J?nior
> |  .''`. UNIMONTES/Depto. Biologia Geral/Lab. de Biologia Computacional
> | : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
> | `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
> |   `- Fone: (38) 3229-8187 | ronaldo.reis at unimontes.br | chrysopa at gmail.com
> | http://www.ppgcb.unimontes.br/ | ICQ#: 5692561 | LinuxUser#: 205366
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
David Barron
Said Business School                 Jesus College
Park End Street                          Oxford
Oxford OX1 1HP                          OX1 3DW
01865 288906                              01865 279684


From pisicandru at hotmail.com  Thu Dec 13 14:41:24 2007
From: pisicandru at hotmail.com (Monica Pisica)
Date: Thu, 13 Dec 2007 13:41:24 +0000
Subject: [R] spliting strings ...
Message-ID: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/438d3e5a/attachment.pl 

From jrkrideau at yahoo.ca  Thu Dec 13 14:54:55 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 13 Dec 2007 08:54:55 -0500 (EST)
Subject: [R] R and Excel Interface
In-Reply-To: <OF9247DAF5.7BFBA0BA-ON862573AF.00729E43-862573AF.00741C05@aphis.usda.gov>
Message-ID: <646710.88166.qm@web32801.mail.mud.yahoo.com>

See the data imput export manual on CRAN
--- Antonio_Paredes at aphis.usda.gov wrote:

> Hello everyone,
> 
> I'll to request some input on what is available for
> use as an R/Excel 
> interface; any help will be appreciated.
> 
> Tony. 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From dimitris.rizopoulos at med.kuleuven.be  Thu Dec 13 14:57:16 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 13 Dec 2007 14:57:16 +0100
Subject: [R] spliting strings ...
References: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
Message-ID: <005a01c83d90$11f2f820$0540210a@www.domain>

one way is the following:

str <- c('aaa bbb', 'cc', 'd eee aa', 'mmm o n')
sapply(strsplit(str, " "), "[", 1)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Monica Pisica" <pisicandru at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, December 13, 2007 2:41 PM
Subject: [R] spliting strings ...


>
> Hi everyone,
>
> I have a vector of strings, each string made up by different number 
> of words. I want to get a new vector which has only the first word 
> of each string in the first vector. I came up with this:
>
> str <- c('aaa bbb', 'cc', 'd eee aa', 'mmm o n')
> str1 <- rep(1, length(str))
> for (i in 1:length(str)) {
>     str1[i] <- strsplit(str, " ")[[i]][1]
> }
> str1
> 'aaa'   'cc'   'd'  'mmm'
>
> Now, is there any way to do this simpler?
>
> Thanks,
>
> Monica
>
> _________________________________________________________________
> Get the power of Windows + Web with the new Windows Live.
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rfrancois at mango-solutions.com  Thu Dec 13 14:52:32 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Thu, 13 Dec 2007 13:52:32 +0000
Subject: [R] spliting strings ...
In-Reply-To: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
References: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
Message-ID: <47613920.9060400@mango-solutions.com>

Hi,

if you only want the first word, then this should do the trick :

R> sub( " +.*", "", str )
[1] "aaa" "cc"  "d"   "mmm"

Cheers,

Romain

Monica Pisica wrote:
> Hi everyone,
>  
> I have a vector of strings, each string made up by different number of words. I want to get a new vector which has only the first word of each string in the first vector. I came up with this:
>  
> str <- c('aaa bbb', 'cc', 'd eee aa', 'mmm o n')
> str1 <- rep(1, length(str))
> for (i in 1:length(str)) {
>      str1[i] <- strsplit(str, " ")[[i]][1]
> }
>  str1
> 'aaa'   'cc'   'd'  'mmm'
>  
> Now, is there any way to do this simpler?
>  
> Thanks,
>  
> Monica
>  
> _________________________________________________________________
> Get the power of Windows + Web with the new Windows Live.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   


-- 
Mango Solutions
data analysis that delivers

Tel:  +44(0) 1249 467 467
Fax:  +44(0) 1249 467 468
Mob:  +44(0) 7813 526 123


From Michael.Suen at inginvestment.com  Thu Dec 13 14:59:42 2007
From: Michael.Suen at inginvestment.com (Suen, Michael)
Date: Thu, 13 Dec 2007 08:59:42 -0500
Subject: [R] How to use R to estimate a model which has two sets of lagged
	time series independent variables
Message-ID: <B2010ADA64EF3F43A729B67C44904439DC6A59@EXC13HFD1P.IAM.INTRANET>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/c498c923/attachment.pl 

From seanpor at acm.org  Thu Dec 13 15:03:04 2007
From: seanpor at acm.org (seanpor)
Date: Thu, 13 Dec 2007 06:03:04 -0800 (PST)
Subject: [R] spliting strings ...
In-Reply-To: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
References: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
Message-ID: <14316361.post@talk.nabble.com>


Good afternoon Monica,

Relying on regular expressions, substituting nothing "" for everything
starting with a space until the end of the "line" (i.e. with a dollar sign) 

str1 <- sub(" .*$", "", str)

Regards,
Sean


Monica Pisica wrote:
> 
> 
> Hi everyone,
>  
> I have a vector of strings, each string made up by different number of
> words. I want to get a new vector which has only the first word of each
> string in the first vector. I came up with this:
>  
> str <- c('aaa bbb', 'cc', 'd eee aa', 'mmm o n')
> str1 <- rep(1, length(str))
> for (i in 1:length(str)) {
>      str1[i] <- strsplit(str, " ")[[i]][1]
> }
>  str1
> 'aaa'   'cc'   'd'  'mmm'
>  
> Now, is there any way to do this simpler?
>  
> Thanks,
>  
> Monica
>  
> _________________________________________________________________
> Get the power of Windows + Web with the new Windows Live.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/spliting-strings-...-tp14316255p14316361.html
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Thu Dec 13 15:04:12 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 13 Dec 2007 14:04:12 +0000 (GMT)
Subject: [R] spliting strings ...
In-Reply-To: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
References: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
Message-ID: <Pine.LNX.4.64.0712131357010.10050@gannet.stats.ox.ac.uk>

On Thu, 13 Dec 2007, Monica Pisica wrote:

>
> Hi everyone,
>
> I have a vector of strings, each string made up by different number of 
> words.

You need to define 'word' and 'first'.  Your solution says the first word 
of " aa" is "", which is not what most people would think.

> I want to get a new vector which has only the first word of each 
> string in the first vector. I came up with this:
>
> str <- c('aaa bbb', 'cc', 'd eee aa', 'mmm o n')
> str1 <- rep(1, length(str))
> for (i in 1:length(str)) {
>     str1[i] <- strsplit(str, " ")[[i]][1]
> }
> str1
> 'aaa'   'cc'   'd'  'mmm'
>
> Now, is there any way to do this simpler?

> sapply(strsplit(str, " "), `[`, 1)
[1] "aaa" "cc"  "d"   "mmm"

or
> sub("([^ ]+).*", "\\1", str)

I don't see how you got your answer: R does not print like that (and never 
has).

>
> Thanks,
>
> Monica
>
> _________________________________________________________________
> Get the power of Windows + Web with the new Windows Live.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Richard.Cotton at hsl.gov.uk  Thu Dec 13 15:04:25 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Thu, 13 Dec 2007 14:04:25 +0000
Subject: [R] Very simple question on plot
In-Reply-To: <200712131116.19592.chrysopa@hsl.gov.uk>
Message-ID: <OFD5098146.7426656F-ON802573B0.004CF3EB-802573B0.004D4CE1@hsl.gov.uk>

> I try to make a plot like this:
> 
> Y |
>   |
>   |               o 
>   |         o        o
>   |     o               o
>   |  o                     o
>   |o                         o   o
>   |--------------------------------
>    0 10 20 30 40 50 60 70 80 90 100 (A)
>  100 90 80 70 60 50 40 30 20 10 0   (B)

x = seq(10,100,10)
y = runif(10)
plot(x,y)
par(mgp = c(3,2,0))
Axis(side=1, at=x, labels=rev(x))

> or
>       B
>   100 90 80 70 60 50 40 30 20 10 0
> Y |--------------------------------
>   |
>   |               o 
>   |         o        o
>   |     o               o
>   |  o                     o
>   |o                         o   o
>   |--------------------------------
>    0 10 20 30 40 50 60 70 80 90 100
>       A

Code as before, but without the par statement, and using Axis(side=3, ...)

Regards,
Richie.

Mathematical Sciences Unit
HSL

------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From rh at family-krueger.com  Thu Dec 13 15:05:32 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Thu, 13 Dec 2007 15:05:32 +0100
Subject: [R] Helpfiles in HTML browser
Message-ID: <47613C2C.9060303@family-krueger.com>

I forgot how to switch between Windows helpfiles and Browser helpfiles.
f.e ?lm should open the browser.
Maybe anybody could give me a hint?

Regards Knut


From P.Dalgaard at biostat.ku.dk  Thu Dec 13 15:06:27 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 13 Dec 2007 15:06:27 +0100
Subject: [R] spliting strings ...
In-Reply-To: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
References: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
Message-ID: <47613C63.2000807@biostat.ku.dk>

Monica Pisica wrote:
> Hi everyone,
>  
> I have a vector of strings, each string made up by different number of words. I want to get a new vector which has only the first word of each string in the first vector. I came up with this:
>  
> str <- c('aaa bbb', 'cc', 'd eee aa', 'mmm o n')
> str1 <- rep(1, length(str))
> for (i in 1:length(str)) {
>      str1[i] <- strsplit(str, " ")[[i]][1]
> }
>  str1
> 'aaa'   'cc'   'd'  'mmm'
>  
> Now, is there any way to do this simpler?
>   
> sapply(strsplit(str, " "), "[", 1)
[1] "aaa" "cc"  "d"   "mmm"

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From yn19832 at msn.com  Thu Dec 13 15:11:50 2007
From: yn19832 at msn.com (livia)
Date: Thu, 13 Dec 2007 06:11:50 -0800 (PST)
Subject: [R]  ls() pattern
Message-ID: <14316765.post@talk.nabble.com>


Hello everyone,

I get some data in the following format and I would like to combine them to
form a dataframe. 

The data is like:

cbcname1 = 0.1,
cbcname2= 0.2,
cbcname3=0.3,...

name1, name2, name2 are just some random names. I would like to achieve sth
like:
(cbcname1=0.1, cbcname2=0.2, cbcname3=0.3,......)

I am using the following codes
do.call(cbind, ls(pat=paste("cbc",*,"=")))

But it seems like the ls() part does not work. 

Could anyone give me some advice? Many thanks.


-- 
View this message in context: http://www.nabble.com/ls%28%29-pattern-tp14316765p14316765.html
Sent from the R help mailing list archive at Nabble.com.


From pisicandru at hotmail.com  Thu Dec 13 15:24:25 2007
From: pisicandru at hotmail.com (Monica Pisica)
Date: Thu, 13 Dec 2007 14:24:25 +0000
Subject: [R] spliting strings ... THANKS!
In-Reply-To: <47613C63.2000807@biostat.ku.dk>
References: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
	<47613C63.2000807@biostat.ku.dk>
Message-ID: <BAY104-W15B83D2EE15BBCD8B26E62C3660@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/9cb72e75/attachment.pl 

From jrkrideau at yahoo.ca  Thu Dec 13 15:38:01 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 13 Dec 2007 09:38:01 -0500 (EST)
Subject: [R] Very simple question on plot
In-Reply-To: <200712131116.19592.chrysopa@gmail.com>
Message-ID: <95432.36896.qm@web32809.mail.mud.yahoo.com>

There is probably a better way but something like this
should do it for your second version.

aa <- 1:20
bb <-  c(5,10,15,20 )
cc <- c(25, 50, 75, 100)
plot(aa)
axis(side=3, labels=cc, at=bb)

--- Ronaldo Reis Junior <chrysopa at gmail.com> wrote:

> Hi,
> 
> I try to make a plot like this:
> 
> Y |
>   |
>   |               o  
>   |         o        o
>   |     o               o
>   |  o                     o
>   |o                         o   o
>   |--------------------------------
>    0 10 20 30 40 50 60 70 80 90 100 (A)
>  100 90 80 70 60 50 40 30 20 10 0   (B)
> 
> or
> 		B
>   100 90 80 70 60 50 40 30 20 10 0
> Y |--------------------------------
>   |
>   |               o  
>   |         o        o
>   |     o               o
>   |  o                     o
>   |o                         o   o
>   |--------------------------------
>    0 10 20 30 40 50 60 70 80 90 100
> 		A
> 
> A and B are complementary variables.
> 
> How is the best way to make this plot?
> 
> Thanks
> Ronaldo
> --
> > Prof. Ronaldo Reis J?nior
> |  .''`. UNIMONTES/Depto. Biologia Geral/Lab. de
> Biologia Computacional
> | : :'  : Campus Universit?rio Prof. Darcy Ribeiro,
> Vila Mauric?ia
> | `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG
> - Brasil
> |   `- Fone: (38) 3229-8187 |
> ronaldo.reis at unimontes.br | chrysopa at gmail.com
> | http://www.ppgcb.unimontes.br/ | ICQ#: 5692561 |
> LinuxUser#: 205366
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From ggrothendieck at gmail.com  Thu Dec 13 15:41:55 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 13 Dec 2007 09:41:55 -0500
Subject: [R] How to use R to estimate a model which has two sets of
	lagged time series independent variables
In-Reply-To: <B2010ADA64EF3F43A729B67C44904439DC6A59@EXC13HFD1P.IAM.INTRANET>
References: <B2010ADA64EF3F43A729B67C44904439DC6A59@EXC13HFD1P.IAM.INTRANET>
Message-ID: <971536df0712130641o2cc62725s326868c4523332d4@mail.gmail.com>

If you don't need a complex error structure then the dyn package
(and also the dynlm package) can do it. Using R's builtin
EuStockMarkets time series:

library(dyn)

z <- as.zoo(EuStockMarkets)
mod1 <- dyn$lm(DAX ~ lag(DAX, -(1:2)) + lag(FTSE, -(0:2)), z)
mod1

# compare to model without FTSE
mod2 <- dyn$lm(DAX ~ lag(DAX, -(1:2)), z)
anova(mod2, mod1)

On Dec 13, 2007 8:59 AM, Suen, Michael <Michael.Suen at inginvestment.com> wrote:
> Hi,
>
> I would like to use R to estimate the following model:
>
> X(t) = a + b1*X(t-1) + b2*X(t-2) + c1*Y(t) + c2*Y(t-1) + c3*Y(t-2)
>
> Is there any R function that performs this type of estimation? I know
> that if I only have one time series (i.e. lagged value of X) on the
> right hand side then there are R functions to do the estimation. I am
> thinking a work around by preparing X(t-1), X(t-2),Y(t),Y(t-1) and
> Y(t-2) as five independent variables and use the lm() function to
> performance the estimation. Please advise. Thanks.
>
> Michael
>
> This e-mail message including any attachments may be legally privileged and confidential under applicable law,
> and is meant only for the intended recipient(s).  If you received this message in error, please reply to the sender,
> adding "SENT IN ERROR" to the subject line, then delete this message.
> Thank you.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Bernhard_Pfaff at fra.invesco.com  Thu Dec 13 15:44:23 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Thu, 13 Dec 2007 14:44:23 -0000
Subject: [R] How to use R to estimate a model which has two sets of
	laggedtime series independent variables
In-Reply-To: <B2010ADA64EF3F43A729B67C44904439DC6A59@EXC13HFD1P.IAM.INTRANET>
References: <B2010ADA64EF3F43A729B67C44904439DC6A59@EXC13HFD1P.IAM.INTRANET>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C30D0FD92E@GBHENXMB02.corp.amvescap.net>

>Hi,
>
>I would like to use R to estimate the following model:
>
>X(t) = a + b1*X(t-1) + b2*X(t-2) + c1*Y(t) + c2*Y(t-1) + c3*Y(t-2)
>
>Is there any R function that performs this type of estimation? I know
>that if I only have one time series (i.e. lagged value of X) on the
>right hand side then there are R functions to do the estimation. I am
>thinking a work around by preparing X(t-1), X(t-2),Y(t),Y(t-1) and
>Y(t-2) as five independent variables and use the lm() function to
>performance the estimation. Please advise. Thanks.
>
>Michael
>

Hello Michael,

you can use the function dynlm() contained in the CRAN-package with the
same name, or you can use the function VAR() contained in the package
vars. Here, you would only need the lm-object belonging to X_T as
lhs-variable.

Best,
Bernhard



>This e-mail message including any attachments may be legally 
>privileged and confidential under applicable law, 
>and is meant only for the intended recipient(s).  If you 
>received this message in error, please reply to the sender, 
>adding "SENT IN ERROR" to the subject line, then delete this message. 
>Thank you.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
*****************************************************************
Confidentiality Note: The information contained in this ...{{dropped:10}}


From h.wickham at gmail.com  Thu Dec 13 15:52:02 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 13 Dec 2007 08:52:02 -0600
Subject: [R] ggplot - Setting the y-scale in a bar plot
In-Reply-To: <20071213120911.56939F8001@smtp3.ualg.pt>
References: <20071211182017.9DEECF8001@smtp3.ualg.pt>
	<f8e6ff050712111315s623d86ma571c690895e25ec@mail.gmail.com>
	<20071211220403.3AA6BF8001@smtp3.ualg.pt>
	<f8e6ff050712111501i14ef7097j3f7071c1121667fe@mail.gmail.com>
	<20071213120911.56939F8001@smtp3.ualg.pt>
Message-ID: <f8e6ff050712130652w7a1324f9h240e1c7b5769412f@mail.gmail.com>

> PS: If you care to explain: why do all parameters in the code below
> have a "." before the name, except precisely "limits"? I know it has
> to do with "proto", but could not find out why this one was different.

Not really - I started off with the convention that variables should
start with a . to distinguish them from functions of the same name.
Unfortunately I forgot about the convention later on, so the code is a
bit confusing!

(And sorry for the confusion of limit vs. limits)

Hadley

> =================
> [...] The fix will be included in the next version of
> ggplot, or you can fix the current version by running this code:
>
> ScaleContinuous$new <- function(., name=NULL, limits=c(NA,NA),
> breaks=NULL, labels=NULL, variable, trans="identity", expand=c(0.05,
> 0)) {
>      if (is.null(breaks) && !is.null(labels)) stop("Labels can only be
> specified in conjunction with breaks")
>
>      .$proto(name=name, .input=variable, .output=variable,
> limits=limits, .breaks = breaks, .labels = labels, .expand=expand)
>    }
>
> ==============================================================
> At 23:01 2007/12/11, you wrote:
> >Hi Pedro,
> >
> >It seems to work for me:
> >
> >qplot(mpg, wt, data=mtcars)
> >qplot(mpg, wt, data=mtcars) + scale_y_continuous(limit=c(4,5))
> >
> >Maybe I don't understand what you're trying to do.
> >
> >Hadley
> >
> >On 12/11/07, Pedro de Barros <pbarros at ualg.pt> wrote:
> > > Hi Hadley,
> > >
> > > Well, the problem seems to be that ggplot is not recognizing the
> > > scale when set by scale_y_continuous, the maximum value stays exactly
> > > at the data range, irrespective of what I provide as range.
> > > I am not familiar yet with proto, therefore I do have some difficulty
> > > delving into the code to find out exactly what is wrong, but I hope
> > > you can tell if I am doing something pretty stupid, or f it is a
> > > feature or a bug...
> > >
> > > Cheers,
> > > Pedro
> > > At 21:15 2007/12/11, you wrote:
> > > >Hi Pedro,
> > > >
> > > >What's the problem exactly?  You'll need to compute the range
> > > >yourself, and then use scale_y_continuous as you have below.
> > > >
> > > >(Also you can abbreviate the bar chart plotting command to:
> > > >qplot(x, y, data=plotdata, geom="bar", stat="identity"))
> > > >
> > > >Hadley
> > > >
> > > >On 12/11/07, Pedro de Barros <pbarros at ualg.pt> wrote:
> > > > > Dear All (probably Hadley),
> > > > >
> > > > > I am now trying to customise some plots using a bar geom.
> > > > >
> > > > > I do not want to use the default binning statistic, but rather
> > > > > calculate the bar heigths separately. I do manage this, but for
> > > > > comparison purposes I would like to have a set of plots all with the
> > > > > same y-axis height. But I do not seem to find out how to fix the
> > > > > scale of the y-axis in this case.
> > > > > Any tips?
> > > > > Using R 2.6.1 on Windows.
> > > > >
> > > > > Thanks for any help,
> > > > > Pedro
> > > > >
> > > > > I attach below the code I am using:
> > > > > plotdata<-data.frame(x=factor(2:8), y=0.1*(2:8))
> > > > > plot1<-ggplot()
> > > > > plot1<-plot1+layer(data=plotdata,
> > > > > mapping=aes_string(x='x',y='y'),geom='bar', stat='identity')
> > > > >
> > > > > RangeY <-c(0,1)
> > > > > YBreaks <- (0:10)*diff(RangeY)/10
> > > > > YTickLabels<- as.character(YBreaks)
> > > > >
> > > > > plot2 <- plot1 + scale_y_continuous(limits=RangeY, breaks=YBreaks,
> > > > > labels=YTickLabels, expand=c(0,0))
> > > > > print(plot2)
> > > > >
> > > > > ______________________________________________
> > > > > R-help at r-project.org mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.
> > > > >
> > > >
> > > >
> > > >--
> > > >http://had.co.nz/
> > >
> > >
> >
> >
> >--
> >http://had.co.nz/
>
>


-- 
http://had.co.nz/


From h.wickham at gmail.com  Thu Dec 13 15:56:50 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 13 Dec 2007 08:56:50 -0600
Subject: [R] use ggplot in a function to which a column name is given
In-Reply-To: <6C8569E6-BE75-44C6-862D-00F5082DADF9@gmail.com>
References: <6C8569E6-BE75-44C6-862D-00F5082DADF9@gmail.com>
Message-ID: <f8e6ff050712130656g7f5b0f72oe33612d044f18df6@mail.gmail.com>

Hi Jiho,

The key to solving this problem is to use aes_string instead of aes.
Instead of the complicated munging that aes does to get the names of
the variables, aes_string works directly with strings, so that:

aes_string(x = "mpg", y = "wt") == aes(x = mpg, y = wt)

So your function would look like:

foo4 <- function(uv="u") {
  ggplot(A, aes_string(x = "x", y= "y", fill = uv)) + geom_tile()
}

Or

ggplot(A, aes(x=x, y=y)) + aes_string(fill=uv) + geom_tile()

Hope that helps!  (And I've made a note to better document aes_string
so you can discover after looking at aes)

Hadley

On 12/13/07, jiho <jo.irisson at gmail.com> wrote:
> Hi everyone, Hi ggplot users in particular,
>
> ggplot makes it very easy to plot things given their names when you
> use it interactively (and therefore can provide the names of the
> columns).
>         qplot(x,foo,data=A) where A has columns (x,y,foo,bar) for example
>
> but I would like to use this from inside a function to which the name
> of the column is given. I cannot find an elegant way to make this
> work. Here are my attempts:
>
> #------------------------------------------------------------
>
> library(ggplot2)
>
> A = data.frame(x=rep(1:10,10), y=rep(1:10,each=10), u=runif(100),
> v=rnorm(100))
>
> # goal: extract values for y<=5 and plot them, either for u or v
>
> foo1 <- function(uv="u")
> {
>         # solution 1: do not use the data argument at all
>         #       (forces the use of qplot, could be more elegant)
>         B = A[A$y<=5,]
>         qplot(B$x, B$y, fill=B[[uv]], geom="tile")
> }
>
> foo2 <- function(uv="u")
> {
>         # solution 2: extract and rename the colums, then use the data argument
>         #       (enables ggplot but could be shorter)
>         B = A[A$y<=5,c("x","y",uv)]
>         names(B)[3] = "value"
>         # rem: cannot use rename since rename(B,c(uv="value")) would not work
>         qplot(x, y, fill=value, data=B, geom="tile")
>         # or
>         # ggplot(B,aes(x=x,y=y,fill=value)) + geom_tile()
> }
>
> foo3 <- function(uv="u")
> {
>         # solution 3: use the data argument and perform the extraction
> directly in it
>         #       (elegant and powerful but can't make it work)
>         ggplot(A[A$y<=5,c("x","y",uv)],aes(x=x,y=y,fill=???)) + geom_tile()
>         # or
>         ggplot(A[A$y<=5,],aes(x=x,y=y,fill=???)) + geom_tile()
>         # or ...
> }
>
> print(foo1("u"))
> print(foo1("v"))
> print(foo2("u"))
> print(foo3("u"))
>
> #------------------------------------------------------------
>
> Any help in making foo3 work would be appreciated. Thanks in advance
> for your expertise.
>
> JiHO
> ---
> http://jo.irisson.free.fr/
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://had.co.nz/


From h.wickham at gmail.com  Thu Dec 13 15:59:59 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 13 Dec 2007 08:59:59 -0600
Subject: [R] ggplot2: can not find "current.grobTree()"
In-Reply-To: <14310411.post@talk.nabble.com>
References: <14310411.post@talk.nabble.com>
Message-ID: <f8e6ff050712130659m4c6e2e7bmc64bc99600bb1229@mail.gmail.com>

> Is the function "current.grobTree()" implemented in ggplot2? According to
> the draft ggplot2 book, on page 43, we can get a list of all grobs with
> current.grobTree(). But when I try that, I get 'Error: could not find
> function "current.grobTree"'.

Ooops - the book is now out of date.  Paul Murrell was kind enough to
write a much nicer version called grid.ls - use that instead

Hadley

-- 
http://had.co.nz/


From guy.brock at louisville.edu  Thu Dec 13 16:09:47 2007
From: guy.brock at louisville.edu (Guy Brock)
Date: Thu, 13 Dec 2007 10:09:47 -0500
Subject: [R] cyclic dependency error
In-Reply-To: <475C8785.26F4.0031.0@gwise.louisville.edu>
References: <4756DA1B020000310001117A@gwise.louisville.edu>
	<475C878302000031000113BA@gwise.louisville.edu>
	<475C8785.26F4.0031.0@gwise.louisville.edu>
Message-ID: <476104EB.26F4.0031.0@gwise.louisville.edu>

I deleted the file zzz.R from the R subdirectory which contained the following:

## startup file for clValid package
#.First.lib <- function(libname, pkgname, where) {
#  if( !require(methods) ) stop("we require methods for package clValid")
#  where <- match(paste("package:", pkgname, sep=""), search())
#  .initClValid(where)
#}

and the problem went away.  I'm not sure, though, why this would have any effect ...

Anyway, problem solved!

Guy  

>>> "Guy Brock" <guy.brock at louisville.edu> 12/10/2007 12:25 AM >>>

Dear all, 
I am encountering a cyclic dependency error when running R CMD check on an R package I wrote (R version 2.6.1, Mac OS X 10.4), see the error message below.    

Creating a new generic function for "print" in "clValid"
Creating a new generic function for "summary" in "clValid"
Creating a new generic function for "plot" in "clValid"
Error in loadNamespace(package, c(which.lib.loc, lib.loc), keep.source = keep.source) : 
        cyclic name space dependencies are not supported
Error : package/namespace load failed for 'clValid'
Error: unable to load R code in package 'clValid'
Execution halted
ERROR: lazy loading failed for package 'clValid'
** Removing '/Users/guybrock/Documents/projects/DattaPackages/Cluster/clValid_0.5-4.Rcheck/clValid'


The NAMESPACE file is below,

.onLoad <- function(lib, pkg) require(methods)
importFrom(graphics, plot)
exportClasses(clValid)
exportMethods(clusters, clusterMethods, nClusters, measNames, measures, optimalScores, plot, print, show, summary)
S3method(print,sota)
S3method(plot,sota)
export(clValid, sota, dunn, connectivity, BHI, BSI, stability, matchGO)

My question is - how can I locate the source of the cyclic dependency error?  This is an updated version of a package which did not previously have this error, so somehow I introduced this in the modifications I made.  Note that R CMD check on Windows (R version 2.6.0) doesn't catch this, although R CMD build --binary fails on Windows with the same error.   

Thanks,
Guy

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From cganduri at gmail.com  Thu Dec 13 14:53:38 2007
From: cganduri at gmail.com (chandrasekhar ganduri)
Date: Thu, 13 Dec 2007 08:53:38 -0500
Subject: [R] Adding data labels to Lattice plots
Message-ID: <1f3dc4a0712130553y266a7fb8n5c79c50256b5592c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/5854a20a/attachment.pl 

From Richard.Cotton at hsl.gov.uk  Thu Dec 13 16:19:56 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Thu, 13 Dec 2007 15:19:56 +0000
Subject: [R] ls() pattern
In-Reply-To: <14316765.post@hsl.gov.uk>
Message-ID: <OF226F0820.022A4C5A-ON802573B0.0053EA2B-802573B0.0054366D@hsl.gov.uk>

> I get some data in the following format and I would like to combine them 
to
> form a dataframe. 
>
> The data is like:
> 
> cbcname1 = 0.1,
> cbcname2= 0.2,
> cbcname3=0.3,...
> 
> name1, name2, name2 are just some random names. I would like to achieve 
sth
> like:
> (cbcname1=0.1, cbcname2=0.2, cbcname3=0.3,......)
> 
> I am using the following codes
> do.call(cbind, ls(pat=paste("cbc",*,"=")))

Try this:

unlist(mget(ls(pattern="cbc"), envir=.GlobalEnv))

The output is an array rather than a data frame, but so would be the 
output from cbind.

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From penel at biomserv.univ-lyon1.fr  Wed Dec 12 15:59:10 2007
From: penel at biomserv.univ-lyon1.fr (Simon Penel)
Date: Wed, 12 Dec 2007 15:59:10 +0100
Subject: [R] [R-pkgs] New version of seqinR  released
Message-ID: <475FF73E.8080606@biomserv.univ-lyon1.fr>

Dear useRs,

the seqinR package contains utilities to import and analyze biological
sequence data. For a general introduction see this document:
http://pbil.univ-lyon1.fr/software/SeqinR//vignette.pdf

Please do not use r-help for questions about seqinR or r-bugs
for bug report about seqinR. Use instead the seqinR diffusion list:
http://pbil.univ-lyon1.fr/software/SeqinR//mailing.php?lang=eng

A new version of seqinR, seqinR 1.1-4, has been released on CRAN.
Here is a summary of changes:

o There is a new chapter to explain how to set up a
  local ACNUC server on Unix-like platforms.

o Function GC() has gained a new argument NA.GC
  defaulting to NA to say what should be returned when the
  GC content cannot be computed from data (for instance with a
  sequence like NNNNNNNNNNNN). The argument oldGC is now
  deprecated and a warning is issued. Functions GC1(),
  GC2(), GC3() are now simple wrappers for the
  more general GCpos() function. The new argument frame
  allows to take the frame into account for CDS.

o Function read.fasta() now supports comment lines
  starting by a semicolon character in FASTA files. An example
  of such a file is provided in sequences/legacy.fasta.
  The argument File is now deprecated. There is
  a new argument seqonly to import just the sequences
  without names, annotations and coercion attempts. There is
  a new argument strip.desc to remove the leading
  '>' character in annotations (as in function readFASTA
  from the Biostrings package). The FASTA file
  example someORF.fsa from Biostrings is also added
  for comparisons.

o Function read.alignment() has gained a new argument
  forceToLower defaulting to TRUE to force lower case in
  the character of the sequence (this is for a smoother interaction
  with the package ape). The argument File is now
  deprecated and a warning is issued when used instead of file.
  The example in the function kaks() has been corrected
  to avoid this warning when reading the example files.

o The details of the socket connection are no more stored in
  the slot socket for objects of class seqAcnucWeb:
  this slot is now deleted. As a consequence, the argument
  socket in function as.SeqAcnucWeb() has been
  removed and there is now a new
  argument socket = "auto" in functions getAnnot(),
  getFrag(), geyKeyword(), getLocation(),
  and getSequence(). The default value "auto" means
  that the details of the socket connection are taken automatically
  when necessary from the last opened bank. The size of local lists
  of sequences is reduced by about a third now as compared to the
  previous version.

o New dataset m16j and waterabs added.

o Generic functions getAnnot(), getFrag(), getKeyword(),
  getLength(), getLocation(), getName(), getSequence() and
  getTrans() have gained methods to handle objects from class list
  and class qaw.

o Functions getAttributsocket() and getNumber.socket()
  are now deprecated, a warning is issued.

o There is a new appendix in which all the examples protected
  by a dontrun statment are forced to be executed.

o New low level utility functions related to an ACNUC server:
  acnucclose(), quitacnuc(), clientid(), countfreelists(),
  knowndbs(), autosocket(), countsubseqs(), savelist(),
  ghelp(), modifylist(), getlistate(), setlistname(),
  residuecount(), isenum(), prettyseq(), gfrag(),
  print.seqAcnucWeb()

o Utility function parser.socket() has been optimized and
  is about four times faster now. This decreases the time
  needed by the query() function.

Best,

the seqinR team

-- 
Simon Penel
Laboratoire de Biometrie et Biologie Evolutive           Bat 711  -   CNRS UMR 5558  -    Universite Lyon 1              43 bd du 11 novembre 1918 69622 Villeurbanne Cedex       Tel:   04 72 43 29 04      Fax:  04 72 43 13 88
http://lbbe.univ-lyon1.fr/-Penel-Simon-.html?lang=fr
http://pbil.univ-lyon1.fr/members/penel

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From jo.irisson at gmail.com  Thu Dec 13 16:24:55 2007
From: jo.irisson at gmail.com (jiho)
Date: Thu, 13 Dec 2007 16:24:55 +0100
Subject: [R] use ggplot in a function to which a column name is given
In-Reply-To: <f8e6ff050712130656g7f5b0f72oe33612d044f18df6@mail.gmail.com>
References: <6C8569E6-BE75-44C6-862D-00F5082DADF9@gmail.com>
	<f8e6ff050712130656g7f5b0f72oe33612d044f18df6@mail.gmail.com>
Message-ID: <5E63BE22-C0A4-4F65-9A49-F0539643609B@gmail.com>


On 2007-December-13  , at 15:56 , hadley wickham wrote:
> Hi Jiho,
>
> The key to solving this problem is to use aes_string instead of aes.
> Instead of the complicated munging that aes does to get the names of
> the variables, aes_string works directly with strings, so that:
>
> aes_string(x = "mpg", y = "wt") == aes(x = mpg, y = wt)
>
> So your function would look like:
>
> foo4 <- function(uv="u") {
>  ggplot(A, aes_string(x = "x", y= "y", fill = uv)) + geom_tile()
> }
>
> Or
>
> ggplot(A, aes(x=x, y=y)) + aes_string(fill=uv) + geom_tile()
>
> Hope that helps!  (And I've made a note to better document aes_string
> so you can discover after looking at aes)

great! I knew you would have thought this through. That's perfect. As  
always there's the trade-off between writing code and documenting the  
code already written. In this case the trade-off turned toward the  
code part I guess.

Autodetection of strings by aes would be even greater but that would  
prevent me to assign the actual strings "u", "x", "y" to an aes  
element, which I don't see as a problem for non text related functions  
though...

Thanks again.

JiHO
---
http://jo.irisson.free.fr/


From h.wickham at gmail.com  Thu Dec 13 16:43:05 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 13 Dec 2007 09:43:05 -0600
Subject: [R] use ggplot in a function to which a column name is given
In-Reply-To: <5E63BE22-C0A4-4F65-9A49-F0539643609B@gmail.com>
References: <6C8569E6-BE75-44C6-862D-00F5082DADF9@gmail.com>
	<f8e6ff050712130656g7f5b0f72oe33612d044f18df6@mail.gmail.com>
	<5E63BE22-C0A4-4F65-9A49-F0539643609B@gmail.com>
Message-ID: <f8e6ff050712130743x63c38444m315deff4a10e411c@mail.gmail.com>

> great! I knew you would have thought this through. That's perfect. As
> always there's the trade-off between writing code and documenting the
> code already written. In this case the trade-off turned toward the
> code part I guess.
>
> Autodetection of strings by aes would be even greater but that would
> prevent me to assign the actual strings "u", "x", "y" to an aes
> element, which I don't see as a problem for non text related functions
> though...

It can come in handy when combining multiple datasets:

mtcarsq <- as.data.frame(mtcars ^ 2)

ggplot(mtcars, aes(x=mpg, y=wt)) + geom_point(aes(colour = "Raw")) +
geom_point(aes(colour="Squared"), data=mtcarsq)

and aes is complicated enough as it is.

Hadley

-- 
http://had.co.nz/


From john.lowen at ns.sympatico.ca  Thu Dec 13 16:49:46 2007
From: john.lowen at ns.sympatico.ca (lowen)
Date: Thu, 13 Dec 2007 07:49:46 -0800 (PST)
Subject: [R] More than one value of Y for each value of X (Model I
 regression)
Message-ID: <14318579.post@talk.nabble.com>


Dear readers,

Is it possible to compute a Model I regression (Sokhal & Rolf 1995) in R
where there is more than one value of Y for each value of X? 

Thanks, John Lowen

-- 
View this message in context: http://www.nabble.com/More-than-one-value-of-Y-for-each-value-of-X-%28Model-I-regression%29-tp14318579p14318579.html
Sent from the R help mailing list archive at Nabble.com.


From Richard.Cotton at hsl.gov.uk  Thu Dec 13 16:51:30 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Thu, 13 Dec 2007 15:51:30 +0000
Subject: [R] ls() pattern
In-Reply-To: <BAY109-W39375B16F2D2C1BFCA89D4E3660@hsl.gov.uk>
Message-ID: <OF7AD8AF48.93C69865-ON802573B0.0057090F-802573B0.00571A4F@hsl.gov.uk>

>   Thank you very much for your reply.I came across with another 
> problem for assigning the variables.
>   in order to achieve cbcname1 = 0.1, I would use sth like:
> 
>   paste("cbc","name1","=",function(),sep="") 
> 
>   But it seems incorrect. Any advice?

This sounds like a different problem.  Do you mean that you want to 
automatically generate variable names and assign something to the 
resultant strings?

May I suggest you read the FAQ on R, 7.21 "How can I turn a string into a 
variable?"

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From donorio at caspur.it  Thu Dec 13 17:50:36 2007
From: donorio at caspur.it (Paolo D'Onorio De Meo)
Date: Thu, 13 Dec 2007 17:50:36 +0100
Subject: [R] RMySQL with Xampp could never work?
Message-ID: <476162DC.3040807@caspur.it>

Hello everybody, i had a big issue with RMySQL,
working on an installation of R on multiple nodes via net file-system.

The point is that we installed Xampp as suite for mysql, perl, apache 
and php on every node, and we need to use R scripts with mysql.
Xampp doesn't compile anything, it gives you pre-compiled binaries all 
linked together, you just extract from a tarball.

At my first try, RMySQL could not find where mysql stuff was.
There are a few mentions of this problem in google, so i  defined the 
LDFLAGS and CPPFLAGS environment variable, and then set the PKG_CPPFLAGS 
and PKG_LIBS environment variables as well.
After this the installation with the "R CMD INSTALL" went fine, but then 
trying to load the library:
/> library(RMySQL)/
unable to load shared library:
RMySQL.so: undefined symbol: mysql_field_count

The command "ldd RMySQL.so" showed that the shared object was not 
pointing to libmysqlclient.so in the Xampp directory.
I tried to modify /etc/ld.so.conf adding Xampp directory, running again 
ldconfig, still same problem.

The only way to have it work was installing on one node the RPM of the 
shared library for Mysql 4.1.
On that machine i installed RMySQL and finally RMySQL.so was pointing to 
libmysqlclient.so.14, and loading the library was successful.

But the funny thing is that now that RMySQL.so works fine on the other 
nodes, without requiring the rpm installation,
i just need to specify the xampp mysql socket address (different from 
the standard /var/mysql) when i use dbConnect.

Is this RMySQL going to work one day on Xampp?
Does anybody know the reason why you cannot specify to RMySQL where 
libmysqlclient should be?

Thanks
Paolo


From Richard.Cotton at hsl.gov.uk  Thu Dec 13 18:10:07 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Thu, 13 Dec 2007 17:10:07 +0000
Subject: [R] ls() pattern
In-Reply-To: <BAY109-W367D96F051DA0C8BDF64A0E3660@hsl.gov.uk>
Message-ID: <OFC0B1C343.6B3B1079-ON802573B0.005E1FF1-802573B0.005E4CEC@hsl.gov.uk>

>    cbcname1 <- rnorm(100,0,1)
>    cbcname2 <- rnorm(100,0.5,1)
>    y <- rnorm(100,0.6,1)
>    lm1 <- lm(y~cbcname1+cbcname2)
> 
>    I have finished the linear regression until here. Now I would 
> like to predict for new data. This is the question part.
> 
>    data <- data.frame(cbcname1=0.1, cbcname2=0.2)
>    predict(lm1,data)

This works fine for me.

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From t.coote at tideway.com  Thu Dec 13 18:11:53 2007
From: t.coote at tideway.com (Tim Coote)
Date: Thu, 13 Dec 2007 17:11:53 -0000
Subject: [R] parsing dates in input file
Message-ID: <B81183566FF73947875C3A207CD60AA608A82F7A@LONEX01.tideway.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/8096c5f9/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Dec 13 18:28:39 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 13 Dec 2007 17:28:39 +0000 (GMT)
Subject: [R] parsing dates in input file
In-Reply-To: <B81183566FF73947875C3A207CD60AA608A82F7A@LONEX01.tideway.com>
References: <B81183566FF73947875C3A207CD60AA608A82F7A@LONEX01.tideway.com>
Message-ID: <Pine.LNX.4.64.0712131725030.12175@gannet.stats.ox.ac.uk>

On Thu, 13 Dec 2007, Tim Coote wrote:

> I've hunted around to try to work this out and cannot find anything
> aposite, although there are exhortations to read News files and the fine
> manual in response to similar queries.  Hope I haven't missed anything
> obvious.
>
> I want to read in csv files that contain dates, or date times. If I read
> them in directly:
> x <- read.csv ("file")
> the relevant dataframe column is of type string. It would be nice to

type 'character', no doubt.

> read in in one go, but not necessary. If I parse the data and then try
> to overwrite the original column of the data frame:
> y <- strptime (x$datefield, "%d/%m/%Y")
> x$datefield <- y
>
> I get an error:
> Error in `$<-.data.frame`(`*tmp*`, "datefield", value = list(sec = c(0,
> :
>  replacement has 9 rows, data has 16
>
> y has 16 values, as does x$datefield.
>
> I'm sure that this is a common problem.  Has anyone got any pointers?

See  ?as.Date : strptime() does not produce Dates.

Also, ?read.csv says there is an argument

colClasses: character.  A vector of classes to be assumed for the
           columns.  Recycled as necessary, or if the character vector
           is named, unspecified values are taken to be 'NA'.

           Possible values are 'NA' (when 'type.convert' is used),
           '"NULL"' (when the column is skipped), one of the atomic
           vector classes (logical, integer, numeric, complex,
           character, raw), or '"factor"', '"Date"' or '"POSIXct"'.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From epistat at gmail.com  Thu Dec 13 18:46:03 2007
From: epistat at gmail.com (zhijie zhang)
Date: Fri, 14 Dec 2007 01:46:03 +0800
Subject: [R] Two repeated warnings when runing gam(mgcv) to analyze my
	dataset?
Message-ID: <2fc17e30712130946x7c553885m6c0640a0f8c47d5c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071214/87d9ac67/attachment.pl 

From thierrydb at gmail.com  Thu Dec 13 18:58:23 2007
From: thierrydb at gmail.com (thierrydb)
Date: Thu, 13 Dec 2007 09:58:23 -0800 (PST)
Subject: [R]  Gaussian Smoothing
Message-ID: <14321313.post@talk.nabble.com>


Hello, 

I'm new to R, and I would like to know if there is a way to smooth a curve
using a Gaussian smoothing with R. 

Thank you very much, 

TDB
-- 
View this message in context: http://www.nabble.com/Gaussian-Smoothing-tp14321313p14321313.html
Sent from the R help mailing list archive at Nabble.com.


From Richard.Cotton at hsl.gov.uk  Thu Dec 13 19:35:11 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Thu, 13 Dec 2007 18:35:11 +0000
Subject: [R] ls() pattern
In-Reply-To: <BAY109-W33C23BAA40F065135FFF19E3660@hsl.gov.uk>
Message-ID: <OF77AD1A2C.66A22348-ON802573B0.00659533-802573B0.006616CD@hsl.gov.uk>

>   Yes, I know this works. I just give it for example to show what I 
> would like to achieve. In fact, I have many variables, you can 
> imagine it is very annoying to construct the data frame by mannually
> assigning the variables and binding them. Therefore, I would like to
> do it automaticlly. So it is back to my first two questions, In 
> order to achieve cbcname1 = 0.1, I tried sth like:(but it does not 
> return the result as desired)
> paste("cbc","name1","=",0.1,sep=""), in this way, I can construct 
> all the variables by a single step.Then I would like to build the 
> data frame automaticlly binding all the "cbcname=.." together. Just 
> like the example.

Ok.  How's this?

#Generate variable names
varnames = paste("cbcname", 1:10, sep="")
#Assign them to values
for(i in 1:10) assign(varnames[i], 0.1*i)
#Form the middle bit of the command
cmd = paste(varnames, "=", mget(varnames, envir=.GlobalEnv), collapse=",")
#Evaluate the command
eval(parse(text=paste("df=data.frame(", cmd, ")", sep="")))

There's a general rule that says if you have to use parse, you're probably 
doing it the wrong way, but it's been a long day, so that's the best way I 
can think of right now.

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From agoralczyk at gmail.com  Thu Dec 13 19:38:26 2007
From: agoralczyk at gmail.com (Armin Goralczyk)
Date: Thu, 13 Dec 2007 19:38:26 +0100
Subject: [R] Function for AUC?
Message-ID: <a695fbee0712131038l3f94dbb0va55fe1fadc29418f@mail.gmail.com>

Hello

Is there an easy way, i.e. a function in a package, to calculate the
area under the curve (AUC) for drug serum levels?

Thanks for any advice
-- 
Armin Goralczyk, M.D.
--
Universit?tsmedizin G?ttingen
Abteilung Allgemein- und Viszeralchirurgie
Rudolf-Koch-Str. 40
39099 G?ttingen
--
Dept. of General Surgery
University of G?ttingen
G?ttingen, Germany
--
http://www.chirurgie-goettingen.de

From Bannho at gmx.de  Thu Dec 13 20:17:47 2007
From: Bannho at gmx.de (creepa1982)
Date: Thu, 13 Dec 2007 11:17:47 -0800 (PST)
Subject: [R]  VARMA in R
Message-ID: <14322697.post@talk.nabble.com>


Hi all, 

does anyone know of a package/function for fitting Vector Autoregressive
Moving Average models? I looked through most of the packages available but
could only find functions to fit a VAR. 

Any help would be appreciated! 

Benjamin
-- 
View this message in context: http://www.nabble.com/VARMA-in-R-tp14322697p14322697.html
Sent from the R help mailing list archive at Nabble.com.


From Jeff_Bardwell at baylor.edu  Thu Dec 13 20:23:31 2007
From: Jeff_Bardwell at baylor.edu (Bardwell, Jeff H)
Date: Thu, 13 Dec 2007 13:23:31 -0600
Subject: [R] Multiple Reponse CART Analysis
References: <D3EBC71C44A1AA4BBA17B0B5AC68A0C501574380@MAIL-I-K.baylor.edu>
	<1197324075.7454.25.camel@graptoleberis.geog.ucl.ac.uk>
Message-ID: <D3EBC71C44A1AA4BBA17B0B5AC68A0C50157438C@MAIL-I-K.baylor.edu>

Thank you for the reply.  With the improved formula, mvpart worked like a charm.
 
Sincerely,
Jeff
 
Jeff H Bardwell, M.S.
Biology Department
ENV 1101 Lab Coordinator
Goebel 115, OH: Thu 1pm-4pm
710-6596 (e-mail preferred)

________________________________

From: Gavin Simpson [mailto:gavin.simpson at ucl.ac.uk]
Sent: Mon 12/10/2007 4:01 PM
To: Bardwell, Jeff H
Cc: r-help at r-project.org
Subject: Re: [R] Multiple Reponse CART Analysis



On Mon, 2007-12-10 at 14:17 -0600, Bardwell, Jeff H wrote:
> Dear R friends-
> 
> I'm attempting to generate a regression tree with one gradient
> predictor and multiple responses, trying to test if change in size
> (turtle.data$Clength) acts as a single predictor of ten multiple diet
> taxa abundances (prey.data)  Neither rpart or mvpart seem to allow me
> to do multiple responses.  (Or if they can, I'm not using the
> functions properly.)
>
> > library(rpart)
> > turtle.rtree<-rpart(prey.data~., data=turtle.data$Clength,
> method="anova", maxsurrogate=0, minsplit=8, minbucket=4, xval=10);
> plot(turtle.rtree); text(turtle.rtree)
> Error in terms.formula(formula, data = data) :
>         '.' in formula and no 'data' argument

rpart doesn't do multiple responses - try package mvpart for a drop-in
replacement. Alternatively look at package party.

Also, you are not using formula correctly. What you should have written
is:

prey.data ~ Clength, data = turtle.data

What R does is look for variables in the formula from within the data
argument, and IIRC data is supposed to be a data frame. If it doesn't
find what it needs in data, it looks in the workspace. This probably
isn't correct - the real answer having to do with environments and
parents etc., but effectively in this case this is what happens.

>
> When I switch response for predictor, it works.  But this is the
> opposite of what I wanted to test and gives me splits at abundance
> values, not carapace length values.
> > turtle.rtree<-rpart(turtle.data$Clength~., data=prey.data,
> method="anova", maxsurrogate=0, minsplit=8, minbucket=4, xval=10);
> plot(turtle.rtree); text(turtle.rtree)
> >

Of course, it has to expand . from data and prey.data is a data frame. R
picks up turtle.data$Clength from the workspace. But this isn't a
multivariate tree. You are confusing the problem above with not being
able to deal with multiple responses.

If mvpart is not working then you need to show why and how it fails.
>From your description, the response is abundances of prey species, this
should be fine in mvpart.

Note though, that mvpart seems to need a data.matrix as the response so
something like this should work:

data.matrix(prey.data) ~ Clength, data = turtle.data

HTH

G

> 
> I've heard polymars recommended for this sort of situation.  I've
> downloaded the polyspline library, but get bogged down in the
> equation.  Also, it doesn't seem like polymars will generate a tree
> even if I do get it working.  Can rpart be modified in some way to
> accomodate multiple response parameters?  If anyone's ever come across
> this situation before, pointers would be much appreciated.  Thanks.
> 
> Sincerely,
> Jeff Bardwell

--
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk <http://www.freshwaters.org.uk/> 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From sbigelow at fs.fed.us  Thu Dec 13 20:36:55 2007
From: sbigelow at fs.fed.us (Seth W Bigelow)
Date: Thu, 13 Dec 2007 11:36:55 -0800
Subject: [R] Overlaying trellis xyplot on contourplot
Message-ID: <OFA430680F.7B901ACA-ON882573B0.006B055B-882573B0.006BBFFD@fs.fed.us>


Friends: I wish to overlay data points on a contour graph.  The following
example produces a nice contour plot, but I have not mastered the concept
of using panel functions to modify plots. Can someone show me how to
overlay the data points (given after contour plot statement) on the
contourplot?
--Seth


model <- function(a,b,c,X1,X2)                        # provide model
function for contour plot
 {(exp(a + b*X1 + c*X2)) / (1 + exp(a + b*X1 + c*X2))}

g <- expand.grid(X1 = seq(0.40, 0.8,0.01), X2 = seq(0.03,0.99,0.03)) #
create gridded data for contour plot
a <- -37.61                               # Assign value to 'a' parameter
b <- 34.88                                # Assign value to 'b' parameter
c <- 28.44                                # Assign value to 'c' parameter
g$z<- model(a, b, c, g$X1,g$X2)                       # Create variable z
using gridded data, model, and variables

contourplot(z ~ X1 * X2,                              # specify the basic
equation for the contour plot
 data=g,                                  # Specify the data frame to be
used
 contour=TRUE,                                  # Make sure it adds
contours
 xlim=c(0.4,0.8), ylim=c(0.401,0.999), zlim=c(0,1),         # Set axis
ranges
 xlab="p(H)", ylab="p(H|H)",                    # Add axis labels
 region = TRUE,                                 # Add nice colors
 cuts=10                                  # Specify number of contour
slices
 )

# Data to superimpose as xyplot on the contourplot....

ph <-c(0.42,0.47,0.59,0.40)                     # Create a vector of values
under variable 'ph'
phh <-c(0.76,0.81,0.82,0.71)                          # Create vector of
values for variable 'phh'
d <- data.frame(ph,phh)                               # Group variables ph
& phh in data frame 'd'




Dr. Seth  W. Bigelow
Biologist, Sierra Nevada Research Center
Pacific Southwest Research Station, USDA Forest Service
Mailing address: 2121 2nd St Suite A101, Davis CA 95616
www.fs.fed.us/psw/programs/snrc/staff/bigelow
www.smbigelow.net
Phone: 530 759 1718
Fax: 530 747 0241


From kemerson at uoregon.edu  Thu Dec 13 21:21:12 2007
From: kemerson at uoregon.edu (Kevin J Emerson)
Date: Thu, 13 Dec 2007 12:21:12 -0800
Subject: [R] multiple ANOVAs
Message-ID: <003101c83dc5$b4c37860$1e4a6920$@edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/845180a1/attachment.pl 

From jmburgos at u.washington.edu  Thu Dec 13 21:33:34 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Thu, 13 Dec 2007 12:33:34 -0800
Subject: [R] [OT] vernacular names for circular diagrams
In-Reply-To: <45e920ef0712122050q63a7cc0w2ac17ea859a2f31a@mail.gmail.com>
References: <mailman.19.1197370804.9403.r-help@r-project.org>
	<p06002019c384ae89478c@192.168.1.10> <475FF149.20103@biostat.ku.dk>
	<05BE78B0CF1BBC4BBA4AA255568D8611029A9A24@EXCHANGE2VS1.campus.mcgill.ca>
	<45e920ef0712122050q63a7cc0w2ac17ea859a2f31a@mail.gmail.com>
Message-ID: <4761971E.2030404@u.washington.edu>

I should say that the name of this chart varies even among 
Spanish-speaking countries.  In Argentina is "diagrama de torta" which 
is something like "cake-chart".

Julian


ahimsa campos-arceiz wrote:
> Two non-eatable examples from Spain and Japan:
> 
> in Spanish we call them "diagrama de sectores" or "gr??fico de sectores". As
> you can imagine it means "sectors diagram (or graph)".
> 
> in Japanese it is called ???????????? (en gurafu), which means "circular graph"
> 
> a link with its name in other languages:
> http://isi.cbs.nl/glossary/term550.htm
> 
> Cheers,
> 
> Ahimsa
> 
> 
> 
> On Dec 13, 2007 3:01 AM, R Heberto Ghezzo, Dr <heberto.ghezzo at mcgill.ca>
> wrote:
> 
>> >From Montreal,
>> Some people here call it the 'pizza diagram'
>> ?some not eatable names?
>> salut
>>
>>
>>
>> -----Original Message-----
>> From: r-help-bounces at r-project.org on behalf of Peter Dalgaard
>> Sent: Wed 12/12/2007 9:33 AM
>> To: Jean lobry
>> Cc: r-help at r-project.org
>> Subject: Re: [R] [OT] vernacular names for circular diagrams
>>
>> Jean lobry wrote:
>>> Dear useRs,
>>>
>>> by a circular diagram representation I mean what you will get by
>> entering
>>> this at your R promt:
>>>
>>> pie(1:5)
>>>
>>> Nice to have R as a lingua franca :-)
>>>
>>> The folowing quote is from page 360 in this very interesting paper:
>>>
>>> @article{SpenceI2005,
>>>      title = {No Humble Pie: The Origins and Usage of a Statistical
>> Chart},
>>>      author = {Spence, I.},
>>>      journal = {Journal of Educational and Behavioral Statistics},
>>>      volume = {30},
>>>      pages = {353-368},
>>>      year = {2005}
>>> }
>>>
>>> QUOTE
>>> Like us, the French employ a gastronomical metaphor when
>>> they refer to Playfair's pie chart, but they have preferred
>>> instead to invoke the name of the wonderful round soft
>>> cheese from Normandy - the camembert. When I spent 4 months
>>> in Paris a few years ago, a friend invited my wife and me to
>>> lunch with her elderly father who lives in Rouen, Normandy,
>>> about an hour North of Paris. Her father inquired -
>>> coincidentally during the cheese course - what work I was
>>> doing in Paris; I replied that I was researching the
>>> activities of a Scot, William Playfair, during the
>>> revolutionary period. I told him that Playfair had invented
>>> several statistical graphs, including the pie chart, which I
>>> referred to, in French, as <<le camembert.>> After a stunned
>>> silence of perhaps a couple of seconds, the distinguished
>>> elderly gentleman looked me in the eye and exclaimed, <<Mon
>>> Dieu ! Notre camembert?>>
>>> UNQUOTE
>>>
>>> So, I'm just curious: how do you refer in your own language to
>>> this kind of graphic? How do you call it?
>>>
>>> Best,
>>>
>>> Jean
>>>
>>>
>> <Grin>
>>
>> In Danish it is "Lagkagediagram" as in the layer cakes that are
>> traditional at birthday parties (and thrown at eachother's faces in
>> slapstick comedy).
>>
>> --
>>   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>>  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>> 35327918
>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>> 35327907
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From deepayan.sarkar at gmail.com  Thu Dec 13 21:52:22 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 13 Dec 2007 12:52:22 -0800
Subject: [R] Overlaying trellis xyplot on contourplot
In-Reply-To: <OFA430680F.7B901ACA-ON882573B0.006B055B-882573B0.006BBFFD@fs.fed.us>
References: <OFA430680F.7B901ACA-ON882573B0.006B055B-882573B0.006BBFFD@fs.fed.us>
Message-ID: <eb555e660712131252i17bfb18dg927f3e94888fe9ff@mail.gmail.com>

On 12/13/07, Seth W Bigelow <sbigelow at fs.fed.us> wrote:
>
> Friends: I wish to overlay data points on a contour graph.  The following
> example produces a nice contour plot, but I have not mastered the concept
> of using panel functions to modify plots. Can someone show me how to
> overlay the data points (given after contour plot statement) on the
> contourplot?

MASS has an example (Figure 4.3); see

file.show(system.file("scripts/ch04.R", package = "MASS"))

-Deepayan


From h.wickham at gmail.com  Thu Dec 13 22:07:25 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 13 Dec 2007 15:07:25 -0600
Subject: [R] Overlaying trellis xyplot on contourplot
In-Reply-To: <OFA430680F.7B901ACA-ON882573B0.006B055B-882573B0.006BBFFD@fs.fed.us>
References: <OFA430680F.7B901ACA-ON882573B0.006B055B-882573B0.006BBFFD@fs.fed.us>
Message-ID: <f8e6ff050712131307v556df33es81f0be1971428b03@mail.gmail.com>

Hi Seth,

An alternative would be to use ggplot2, http://had.co.nz/ggplot2:

model <- function(a,b,c,X1,X2)  {
  (exp(a + b*X1 + c*X2)) / (1 + exp(a + b*X1 + c*X2))
}

g <- expand.grid(X1 = seq(0.40, 0.8,0.01), X2 = seq(0.03,0.99,0.03))
a <- -37.61
b <- 34.88
c <- 28.44
g$z<- model(a, b, c, g$X1,g$X2)

ph <-c(0.42,0.47,0.59,0.40)
phh <-c(0.76,0.81,0.82,0.71)
d <- data.frame(ph,phh)

library(ggplot2)
qplot(X1, X2, data = g, fill = z, geom="tile", xlab="p(H)", ylab="p(H|H)")+
 geom_contour(aes(z=z)) +
 geom_point(aes(x = ph, y = phh, fill = NULL), data=d)

Plots in ggplot2 have multiple layers which can have different data sources.

Hadley


On 12/13/07, Seth W Bigelow <sbigelow at fs.fed.us> wrote:
>
> Friends: I wish to overlay data points on a contour graph.  The following
> example produces a nice contour plot, but I have not mastered the concept
> of using panel functions to modify plots. Can someone show me how to
> overlay the data points (given after contour plot statement) on the
> contourplot?
> --Seth
>
>
> model <- function(a,b,c,X1,X2)                        # provide model
> function for contour plot
>  {(exp(a + b*X1 + c*X2)) / (1 + exp(a + b*X1 + c*X2))}
>
> g <- expand.grid(X1 = seq(0.40, 0.8,0.01), X2 = seq(0.03,0.99,0.03)) #
> create gridded data for contour plot
> a <- -37.61                               # Assign value to 'a' parameter
> b <- 34.88                                # Assign value to 'b' parameter
> c <- 28.44                                # Assign value to 'c' parameter
> g$z<- model(a, b, c, g$X1,g$X2)                       # Create variable z
> using gridded data, model, and variables
>
> contourplot(z ~ X1 * X2,                              # specify the basic
> equation for the contour plot
>  data=g,                                  # Specify the data frame to be
> used
>  contour=TRUE,                                  # Make sure it adds
> contours
>  xlim=c(0.4,0.8), ylim=c(0.401,0.999), zlim=c(0,1),         # Set axis
> ranges
>  xlab="p(H)", ylab="p(H|H)",                    # Add axis labels
>  region = TRUE,                                 # Add nice colors
>  cuts=10                                  # Specify number of contour
> slices
>  )
>
> # Data to superimpose as xyplot on the contourplot....
>
> ph <-c(0.42,0.47,0.59,0.40)                     # Create a vector of values
> under variable 'ph'
> phh <-c(0.76,0.81,0.82,0.71)                          # Create vector of
> values for variable 'phh'
> d <- data.frame(ph,phh)                               # Group variables ph
> & phh in data frame 'd'
>
>
>
>
> Dr. Seth  W. Bigelow
> Biologist, Sierra Nevada Research Center
> Pacific Southwest Research Station, USDA Forest Service
> Mailing address: 2121 2nd St Suite A101, Davis CA 95616
> www.fs.fed.us/psw/programs/snrc/staff/bigelow
> www.smbigelow.net
> Phone: 530 759 1718
> Fax: 530 747 0241
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://had.co.nz/


From sbigelow at fs.fed.us  Thu Dec 13 22:20:42 2007
From: sbigelow at fs.fed.us (Seth W Bigelow)
Date: Thu, 13 Dec 2007 13:20:42 -0800
Subject: [R] Overlaying trellis xyplot on contourplot
In-Reply-To: <eb555e660712131252i17bfb18dg927f3e94888fe9ff@mail.gmail.com>
Message-ID: <OF7B7F749C.92CC1311-ON882573B0.0074D4A6-882573B0.007540B8@fs.fed.us>

Deepayan:

 Very nice, thanks for introducing me to a new resource. I will include the
entire, functioning example in the event others may find it useful.

--Seth

###### Sample code for overlaying data points on a contour graph, using
xyplot and contourplot ##################

library(lattice)

model <- function(a,b,c,X1,X2)            # provide model function for
contour plot
 {(exp(a + b*X1 + c*X2)) / (1 + exp(a + b*X1 + c*X2))}

g <- expand.grid(X1 = seq(0.38, 0.8,0.01), X2 = seq(0.03,0.99,0.03)) #
create gridded data for contour plot
a <- -37.61                               # Assign value to 'a' parameter
b <- 34.88                                # Assign value to 'b' parameter
c <- 28.44                                # Assign value to 'c' parameter
g$z<- model(a, b, c, g$X1,g$X2)           # Create variable z using gridded
data, model, and variables

# Data to superimpose as xyplot on the contourplot....
ph <-c(0.42,0.47,0.59,0.40)               # Create a vector of values under
variable 'ph'
phh <-c(0.76,0.81,0.82,0.71)              # Create vector of values for
variable 'phh'
d <- data.frame(ph,phh)                   # Group variables ph & phh in
data frame 'd'

contourplot(z ~ X1 * X2,
 data=g,
 contour=TRUE,
 xlim=c(0.38,0.8), ylim=c(0.401,0.999), zlim=c(0,1), # Set Axis Ranges
 xlab="p(H)", ylab="p(H|H)",              # Set axis labels
 region = TRUE,
 cuts=10,
 panel = function(x,y,subscripts,...){
 panel.contourplot(x,y,subscripts,...)
 panel.xyplot(d$ph,d$phh)}
 )

#### End ##############################


From tom.soyer at gmail.com  Thu Dec 13 22:27:13 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Thu, 13 Dec 2007 15:27:13 -0600
Subject: [R] counting weekday in a month in R
Message-ID: <65cc7bdf0712131327x3bbe01d7of384f454528b7232@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/64ecb6c2/attachment.pl 

From GPetris at uark.edu  Thu Dec 13 22:31:33 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Thu, 13 Dec 2007 15:31:33 -0600 (CST)
Subject: [R] VARMA in R
In-Reply-To: <14322697.post@talk.nabble.com> (message from creepa1982 on Thu, 
	13 Dec 2007 11:17:47 -0800 (PST))
References: <14322697.post@talk.nabble.com>
Message-ID: <200712132131.lBDLVXIb021302@definetti.ddns.uark.edu>


You may want to check package dlm and, possibly, dse. 

In dlm you can cast a VARMA model in state space form (dlmModARMA) and
estimate unknown parameters by maximum likelihood (dlmMLE). 


Best,
Giovanni

> Date: Thu, 13 Dec 2007 11:17:47 -0800 (PST)
> From: creepa1982 <Bannho at gmx.de>
> Sender: r-help-bounces at r-project.org
> Precedence: list
> 
> 
> Hi all, 
> 
> does anyone know of a package/function for fitting Vector Autoregressive
> Moving Average models? I looked through most of the packages available but
> could only find functions to fit a VAR. 
> 
> Any help would be appreciated! 
> 
> Benjamin
> -- 
> View this message in context: http://www.nabble.com/VARMA-in-R-tp14322697p14322697.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 

Giovanni Petris  <GPetris at uark.edu>
Department of Mathematical Sciences
University of Arkansas - Fayetteville, AR 72701
Ph: (479) 575-6324, 575-8630 (fax)
http://definetti.uark.edu/~gpetris/


From NordlDJ at dshs.wa.gov  Thu Dec 13 22:38:49 2007
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Thu, 13 Dec 2007 13:38:49 -0800
Subject: [R] counting weekday in a month in R
In-Reply-To: <65cc7bdf0712131327x3bbe01d7of384f454528b7232@mail.gmail.com>
References: <65cc7bdf0712131327x3bbe01d7of384f454528b7232@mail.gmail.com>
Message-ID: <941871A13165C2418EC144ACB212BDB04E1459@dshsmxoly1504g.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of tom soyer
> Sent: Thursday, December 13, 2007 1:27 PM
> To: r-help at r-project.org
> Subject: [R] counting weekday in a month in R
> 
> Hi,
> 
> I am trying to count weekday of the month using R. For 
> example, 1/4/2001
> is the 4th weekday of Jan, and 1/5/2001 is the 5th weekday of 
> the month, and
> 1/8/2001 is the 6th weekday of the month, etc. I get as far 
> as extracting
> the weekdays from a sequence of dates (see below). But I have not yet
> figured out a fast way of counting without using a For Loop. 
> Does anyone
> know how to do such counting efficiently in R?
> 
> Thanks!
> 
> library(chron)
> dts=seq(dates("1/4/01"),dates("3/31/01"),1)
> wkday=dts[!is.weekend(dts)]
> weekdays(wkday)
>  [1] Thu Fri Mon Tue Wed Thu Fri Mon Tue Wed Thu Fri Mon Tue 
> Wed Thu Fri Mon
> Tue Wed
> [21] Thu Fri Mon Tue Wed Thu Fri Mon Tue Wed Thu Fri Mon Tue 
> Wed Thu Fri Mon
> Tue Wed
> [41] Thu Fri Mon Tue Wed Thu Fri Mon Tue Wed Thu Fri Mon Tue 
> Wed Thu Fri Mon
> Tue Wed
> [61] Thu Fri
> Levels: Sun < Mon < Tue < Wed < Thu < Fri < Sat
> 
> 
> 
> -- 
> Tom

How about length(wkday) ?

Hope this is helpful,

Dan

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204
 
 


From anditopark at hotmail.com  Thu Dec 13 22:40:21 2007
From: anditopark at hotmail.com (Andrew Park)
Date: Thu, 13 Dec 2007 16:40:21 -0500
Subject: [R] Calculating Rsquared values in rpart
Message-ID: <BAY127-W24D76891A88364C2ACFD93DE660@phx.gbl>


Hi there

In rpart, one can get a graph of R-squared (using rsq.rpart (fit)), in which the x axis is the number of splits, and which contains two lines - an "apparent" R squared and an Rsquared based on the x error.

I would like to caclulate these R-squared values, but cannot work out from the output how it is done. Is there any way to access the values that underpin this graph? Alternatively, is there any way to calculate them from the summary data?

Thanks in advance,

Andy Park









_________________________________________________________________
[[replacing trailing spam]]


From f.harrell at vanderbilt.edu  Thu Dec 13 22:47:07 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 13 Dec 2007 15:47:07 -0600
Subject: [R] Calculating Rsquared values in rpart
In-Reply-To: <BAY127-W24D76891A88364C2ACFD93DE660@phx.gbl>
References: <BAY127-W24D76891A88364C2ACFD93DE660@phx.gbl>
Message-ID: <4761A85B.2090300@vanderbilt.edu>

Andrew Park wrote:
> Hi there
> 
> In rpart, one can get a graph of R-squared (using rsq.rpart (fit)), in which the x axis is the number of splits, and which contains two lines - an "apparent" R squared and an Rsquared based on the x error.
> 
> I would like to caclulate these R-squared values, but cannot work out from the output how it is done. Is there any way to access the values that underpin this graph? Alternatively, is there any way to calculate them from the summary data?
> 
> Thanks in advance,
> 
> Andy Park

Beware.  Yi in his JASA paper about generalized degrees of freedom 
showed that to get an unbiased estimate of R^2 from recursive 
partitioning you have to use the formula for adjusted R^2 with number of 
parameters far exceeding the number of final splits.  He showed how to 
estimate the d.f.   Recursive partitioning seems to result in simple 
prediction models but this is mainly an illusion.

Frank Harrell

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From wkimwkim at googlemail.com  Thu Dec 13 22:48:43 2007
From: wkimwkim at googlemail.com (Wan Kyu Kim)
Date: Thu, 13 Dec 2007 15:48:43 -0600
Subject: [R] Average Precision (APR)
Message-ID: <f2cb12530712131348n61c9e52ey71e910ce5826136d@mail.gmail.com>

Hi,

Is there a R library (code) to calculate Average Precision?
I have been using ROCR to calculate other measure like AUC, F-measure etc.
But ARP (average precision) was not available although it provides
precision/recall.

I am almost sure that someone already implemented it.

wan


From hpbenton at scripps.edu  Thu Dec 13 22:52:51 2007
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Thu, 13 Dec 2007 13:52:51 -0800
Subject: [R] RMySQL and free text variable
Message-ID: <4761A9B3.60107@scripps.edu>

Hi all,

    I have a quick question, I want to send a select statement to the 
mysql server. With the code below I connect and ask my select statement.
As you can see from the code I want to have a mode = "+". The '+' needs 
to be free text and hence the "\"" around it. But it doesn't work. If i 
put just plus in instead of the 'mode' I get a table. How can I give the 
'select' statement a variable and put it in free text ?

cheers,

Paul

library(RMySQL)
 >con<- dbConnect(MySQL(), user, password, host, dbname)
 > mode<-"+"
 > query <- paste("SELECT * FROM data WHERE molNUM =", 165 , " AND mode 
=", "\",mode,\"" ) ## my statement
 > scans <- dbGetQuery(con, query)
 > scans
NULL data frame with 0 row

 > query <- paste("Select * FROM metabolite_msms WHERE molid =", 165 , " 
AND mode =", "\"+\"" )##only difference is that mode is replace by +
 > scans <- dbGetQuery(con, query)
 > scans
   molNUM       mz  intensity    fwhm     mode adduct
1  165    385.5790   0.492911 0.0975147    +
2  165    111.1130   0.497857 0.3894250    +
3  165    229.2050   0.499505 0.4097280    +
4  165    302.8850   0.514342 0.0521955    +
5  165    186.9430   0.524233 0.1347370    +
6  165    125.1010   0.530828 0.0571061    +
7  165    207.2150   0.535773 0.0586751    +
8  165    217.2020   0.552258 0.0612605    +
9  165    171.1100   0.560501 0.0823234    +

 > mode<-'+' ## ok double quote didn't work SO...
 > scans <- dbGetQuery(con, query)
 > scans
NULL data frame with 0 rows


From ileyer at yahoo.de  Thu Dec 13 23:15:31 2007
From: ileyer at yahoo.de (Ilona Leyer)
Date: Thu, 13 Dec 2007 23:15:31 +0100 (CET)
Subject: [R] convergence error code in mixed effects models
Message-ID: <580233.74151.qm@web26210.mail.ukl.yahoo.com>

Dear All,
I want to analyse treatment effects with time series
data:  I measured e.g. leaf number (five replicate
plants) in relation to two soil pH - after 2,4,6,8
weeks. I used mixed effects models, but some analyses
didn?t work. It seems for me as if this is a randomly
occurring problem since sometimes the same model works
sometimes not.

An example:
> names(test)
[1] "rep"    "treat"  "leaf"   "week"  
> library (lattice)
> library (nlme)
> test<-groupedData(leaf~week|rep,outer=~treat,test)
> model<-lme(leaf~treat,random=~leaf|rep)
Error in lme.formula(leaf~ treat, random = ~week|rep)
: 
        nlminb problem, convergence error code = 1;
message = iteration limit reached without convergence
(9)

Has anybody an idea to solve this problem?
Thanks

Ilona

Ilona Leyer
Conservation Biology
University of Marburg
Germany
e-mail: ileyer at yahoo.de


From bates at stat.wisc.edu  Thu Dec 13 23:26:47 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 13 Dec 2007 16:26:47 -0600
Subject: [R] convergence error code in mixed effects models
In-Reply-To: <580233.74151.qm@web26210.mail.ukl.yahoo.com>
References: <580233.74151.qm@web26210.mail.ukl.yahoo.com>
Message-ID: <40e66e0b0712131426h6e702ed2pa45b5786c7b29d36@mail.gmail.com>

On Dec 13, 2007 4:15 PM, Ilona Leyer <ileyer at yahoo.de> wrote:
> Dear All,
> I want to analyse treatment effects with time series
> data:  I measured e.g. leaf number (five replicate
> plants) in relation to two soil pH - after 2,4,6,8
> weeks. I used mixed effects models, but some analyses
> didn?t work. It seems for me as if this is a randomly
> occurring problem since sometimes the same model works
> sometimes not.
>
> An example:
> > names(test)
> [1] "rep"    "treat"  "leaf"   "week"
> > library (lattice)
> > library (nlme)
> > test<-groupedData(leaf~week|rep,outer=~treat,test)
> > model<-lme(leaf~treat,random=~leaf|rep)
> Error in lme.formula(leaf~ treat, random = ~week|rep)

Really!? You gave lme a model with random = ~ leaf | rep (and no data
specification) and it tried to fit a model with random = ~ week | rep?
Are you sure that is an exact transcript?

> :
>         nlminb problem, convergence error code = 1;
> message = iteration limit reached without convergence
> (9)

> Has anybody an idea to solve this problem?

Oh, I have lots of ideas but without a reproducible example I can't
hope to decide what might be the problem.

It appears that the model may be over-parameterized.  Assuming that
there are 4 different values of week then ~ week | rep requires
fitting 10 variance-covariance parameters. That's a lot.
The error code indicates that the optimizer is taking


From juryef at yahoo.com  Thu Dec 13 23:35:00 2007
From: juryef at yahoo.com (Judith Flores)
Date: Thu, 13 Dec 2007 14:35:00 -0800 (PST)
Subject: [R] Different display of same graphs on different screens
Message-ID: <615767.13284.qm@web34708.mail.mud.yahoo.com>

Dear R-experts,

   I need to run a R code on different computers (MACs
and PCs) on different monitos sizes. The code
generates graphs that are displayed in the same page.
But there are some elements of the graphs that appear
at the right position on a PC, but they don't appear
at all when I run it on a MAC. I have R v 2.5.1
installed in both types of systems.

  I specify windows(height=10, width=10) at the
beggining of the code.

I am guessing it has to do with devices, but I am not
sure what to specify.

Thank you,

Judith


      ____________________________________________________________________________________
Never miss a thing.  Make Yahoo your home page.


From dylan.beaudette at gmail.com  Thu Dec 13 23:47:52 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Thu, 13 Dec 2007 14:47:52 -0800
Subject: [R] Different display of same graphs on different screens
In-Reply-To: <615767.13284.qm@web34708.mail.mud.yahoo.com>
References: <615767.13284.qm@web34708.mail.mud.yahoo.com>
Message-ID: <200712131447.53002.dylan.beaudette@gmail.com>

On Thursday 13 December 2007, Judith Flores wrote:
> Dear R-experts,
>
>    I need to run a R code on different computers (MACs
> and PCs) on different monitos sizes. The code
> generates graphs that are displayed in the same page.
> But there are some elements of the graphs that appear
> at the right position on a PC, but they don't appear
> at all when I run it on a MAC. I have R v 2.5.1
> installed in both types of systems.
>
>   I specify windows(height=10, width=10) at the
> beggining of the code.
>

how are you specifying the size on the macs? 

quartz(height=10, width=10)

?




-- 
Dylan Beaudette
Soil Resource Laboratory
http://casoilresource.lawr.ucdavis.edu/
University of California at Davis
530.754.7341


From dylan.beaudette at gmail.com  Thu Dec 13 23:50:58 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Thu, 13 Dec 2007 14:50:58 -0800
Subject: [R] Calculating Rsquared values in rpart
In-Reply-To: <4761A85B.2090300@vanderbilt.edu>
References: <BAY127-W24D76891A88364C2ACFD93DE660@phx.gbl>
	<4761A85B.2090300@vanderbilt.edu>
Message-ID: <200712131450.58932.dylan.beaudette@gmail.com>

On Thursday 13 December 2007, Frank E Harrell Jr wrote:
> Andrew Park wrote:
> > Hi there
> >
> > In rpart, one can get a graph of R-squared (using rsq.rpart (fit)), in
> > which the x axis is the number of splits, and which contains two lines -
> > an "apparent" R squared and an Rsquared based on the x error.
> >
> > I would like to caclulate these R-squared values, but cannot work out
> > from the output how it is done. Is there any way to access the values
> > that underpin this graph? Alternatively, is there any way to calculate
> > them from the summary data?
> >
> > Thanks in advance,
> >
> > Andy Park
>
> Beware.  Yi in his JASA paper about generalized degrees of freedom
> showed that to get an unbiased estimate of R^2 from recursive
> partitioning you have to use the formula for adjusted R^2 with number of
> parameters far exceeding the number of final splits.  He showed how to
> estimate the d.f.   Recursive partitioning seems to result in simple
> prediction models but this is mainly an illusion.
>
> Frank Harrell

Hi Frank and others,

hapen to have a link / citation for that paper? 

thanks!

-- 
Dylan Beaudette
Soil Resource Laboratory
http://casoilresource.lawr.ucdavis.edu/
University of California at Davis
530.754.7341


From m_olshansky at yahoo.com  Thu Dec 13 23:59:10 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Thu, 13 Dec 2007 14:59:10 -0800 (PST)
Subject: [R] ls() pattern
In-Reply-To: <14316765.post@talk.nabble.com>
Message-ID: <994147.10866.qm@web32210.mail.mud.yahoo.com>

Hi Livia,

Below is my solution. I believe that other list
members will post a better one.

Regards,

Moshe.

> rm(list=ls())
>
eval(parse(text=paste(paste("x",1:5,sep=""),paste("<-1.",1:5,";",sep=""))))
>
eval(parse(text=paste(paste("y",1:5,sep=""),paste("<-2.",1:5,";",sep=""))))
>
eval(parse(text=paste(paste("xy",1:5,sep=""),paste("<-12.",1:5,";",sep=""))))
> ls()
 [1] "x1"  "x2"  "x3"  "x4"  "x5"  "xy1" "xy2" "xy3"
"xy4" "xy5" "y1"  "y2"  "y3"  "y4"  "y5" 
> ls(pattern="xy")
[1] "xy1" "xy2" "xy3" "xy4" "xy5"
> a <- ls(pattern="xy")
> b <-
paste("df<-data.frame(",paste(a,"=",a,",",sep="",collapse=""),")",sep="")
> b
[1]
"df<-data.frame(xy1=xy1,xy2=xy2,xy3=xy3,xy4=xy4,xy5=xy5,)"
> substr(b,nchar(b)-1,nchar(b)-1) <- " "
> b
[1]
"df<-data.frame(xy1=xy1,xy2=xy2,xy3=xy3,xy4=xy4,xy5=xy5
)"
> eval(parse(text=b))
> df
   xy1  xy2  xy3  xy4  xy5
1 12.1 12.2 12.3 12.4 12.5
> 


--- livia <yn19832 at msn.com> wrote:

> 
> Hello everyone,
> 
> I get some data in the following format and I would
> like to combine them to
> form a dataframe. 
> 
> The data is like:
> 
> cbcname1 = 0.1,
> cbcname2= 0.2,
> cbcname3=0.3,...
> 
> name1, name2, name2 are just some random names. I
> would like to achieve sth
> like:
> (cbcname1=0.1, cbcname2=0.2, cbcname3=0.3,......)
> 
> I am using the following codes
> do.call(cbind, ls(pat=paste("cbc",*,"=")))
> 
> But it seems like the ls() part does not work. 
> 
> Could anyone give me some advice? Many thanks.
> 
> 
> -- 
> View this message in context:
>
http://www.nabble.com/ls%28%29-pattern-tp14316765p14316765.html
> Sent from the R help mailing list archive at
> Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From rvaradhan at jhmi.edu  Fri Dec 14 00:06:56 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 13 Dec 2007 18:06:56 -0500
Subject: [R] Calculating Rsquared values in rpart
In-Reply-To: <200712131450.58932.dylan.beaudette@gmail.com>
References: <BAY127-W24D76891A88364C2ACFD93DE660@phx.gbl>
	<4761A85B.2090300@vanderbilt.edu>
	<200712131450.58932.dylan.beaudette@gmail.com>
Message-ID: <001e01c83ddc$dbd29690$7c94100a@win.ad.jhu.edu>

The author is actually "Ye", and not "Yi".  It is titled "On Measuring and
correcting the effects of data mining and model selection" by Jianming Ye,
JASA(1998).

Here is link from JSTOR:

http://www.jstor.org/view/01621459/di015668/01p00145/0?currentResult=0162145
9%2bdi015668%2b01p00145%2b0%2cFF15&searchUrl=http%3A%2F%2Fwww.jstor.org%2Fse
arch%2FAdvancedResults%3Fhp%3D25%26si%3D1%26q0%3DYe%2Bdata%2Bmining%26f0%3D%
26c0%3DAND%26wc%3Don%26sd%3D%26ed%3D%26la%3D%26dc%3DStatistics


Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Dylan Beaudette
Sent: Thursday, December 13, 2007 5:51 PM
To: r-help at r-project.org
Cc: Andrew Park
Subject: Re: [R] Calculating Rsquared values in rpart

On Thursday 13 December 2007, Frank E Harrell Jr wrote:
> Andrew Park wrote:
> > Hi there
> >
> > In rpart, one can get a graph of R-squared (using rsq.rpart (fit)), in
> > which the x axis is the number of splits, and which contains two lines -
> > an "apparent" R squared and an Rsquared based on the x error.
> >
> > I would like to caclulate these R-squared values, but cannot work out
> > from the output how it is done. Is there any way to access the values
> > that underpin this graph? Alternatively, is there any way to calculate
> > them from the summary data?
> >
> > Thanks in advance,
> >
> > Andy Park
>
> Beware.  Yi in his JASA paper about generalized degrees of freedom
> showed that to get an unbiased estimate of R^2 from recursive
> partitioning you have to use the formula for adjusted R^2 with number of
> parameters far exceeding the number of final splits.  He showed how to
> estimate the d.f.   Recursive partitioning seems to result in simple
> prediction models but this is mainly an illusion.
>
> Frank Harrell

Hi Frank and others,

hapen to have a link / citation for that paper? 

thanks!

-- 
Dylan Beaudette
Soil Resource Laboratory
http://casoilresource.lawr.ucdavis.edu/
University of California at Davis
530.754.7341

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From m_olshansky at yahoo.com  Fri Dec 14 00:30:10 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Thu, 13 Dec 2007 15:30:10 -0800 (PST)
Subject: [R] spliting strings ...
In-Reply-To: <BAY104-W9FF9FF2025B1340E226FDC3660@phx.gbl>
Message-ID: <324427.62589.qm@web32215.mail.mud.yahoo.com>

Hi Monica,

Try 

sapply(as.list(str),function(x) unlist(strsplit(x,"
"))[1])

--- Monica Pisica <pisicandru at hotmail.com> wrote:

> 
> Hi everyone,
>  
> I have a vector of strings, each string made up by
> different number of words. I want to get a new
> vector which has only the first word of each string
> in the first vector. I came up with this:
>  
> str <- c('aaa bbb', 'cc', 'd eee aa', 'mmm o n')
> str1 <- rep(1, length(str))
> for (i in 1:length(str)) {
>      str1[i] <- strsplit(str, " ")[[i]][1]
> }
>  str1
> 'aaa'   'cc'   'd'  'mmm'
>  
> Now, is there any way to do this simpler?
>  
> Thanks,
>  
> Monica
>  
>
_________________________________________________________________
> Get the power of Windows + Web with the new Windows
> Live.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From gerifalte28 at hotmail.com  Fri Dec 14 01:46:57 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 13 Dec 2007 17:46:57 -0700
Subject: [R] Helpfiles in HTML browser
In-Reply-To: <47613C2C.9060303@family-krueger.com>
References: <47613C2C.9060303@family-krueger.com>
Message-ID: <4761D281.6000401@hotmail.com>

Go to your Rprofile file (in the etc directory) and add the following line:

options(htmlhelp=TRUE)

I hope this helps

Francisco

Knut Krueger wrote:
> I forgot how to switch between Windows helpfiles and Browser helpfiles.
> f.e ?lm should open the browser.
> Maybe anybody could give me a hint?
> 
> Regards Knut
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gerifalte28 at hotmail.com  Fri Dec 14 01:46:57 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 13 Dec 2007 17:46:57 -0700
Subject: [R] Helpfiles in HTML browser
In-Reply-To: <47613C2C.9060303@family-krueger.com>
References: <47613C2C.9060303@family-krueger.com>
Message-ID: <4761D281.6000401@hotmail.com>

Go to your Rprofile file (in the etc directory) and add the following line:

options(htmlhelp=TRUE)

I hope this helps

Francisco

Knut Krueger wrote:
> I forgot how to switch between Windows helpfiles and Browser helpfiles.
> f.e ?lm should open the browser.
> Maybe anybody could give me a hint?
> 
> Regards Knut
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From hpbenton at scripps.edu  Fri Dec 14 01:51:16 2007
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Thu, 13 Dec 2007 16:51:16 -0800
Subject: [R] RMySQL and free text variable
In-Reply-To: <4761D068.2020304@justemail.net>
References: <4761D068.2020304@justemail.net>
Message-ID: <4761D384.2090503@scripps.edu>

Yea that worked !!
Did
paste(SELECT * FROM MET WHERE molid=", molid, "AND mode=", "\'" mode, 
"\'" , sep="")
I guess all I needed was the sep="" which I totally forgot about.

Cheers,

Paul

Eric wrote:
> Does this work for you?
>
> "SELECT * FROM data WHERE molNUM = 165 AND mode = '+'"
>
> Possibly related: when using the paste function, remember that you 
> might need explicitly set sep = "" and add your own padding where needed.
>
> Please re-post to the list if it works.
>
>
>
>


From thomas.pujol at yahoo.com  Fri Dec 14 01:58:59 2007
From: thomas.pujol at yahoo.com (Thomas Pujol)
Date: Thu, 13 Dec 2007 16:58:59 -0800 (PST)
Subject: [R] RODBC, optimizing memory,
	"Error: cannot allocate vector of size 522 Kb".
Message-ID: <795408.86294.qm@web59304.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/163d5c7a/attachment.pl 

From nmprista at fc.ul.pt  Fri Dec 14 02:56:09 2007
From: nmprista at fc.ul.pt (Nuno Prista)
Date: Thu, 13 Dec 2007 20:56:09 -0500
Subject: [R] Help! - boxcox transformations
Message-ID: <003801c83df4$8602e3b0$c0685280@ts.odu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/79ac57c3/attachment.pl 

From fjbuch at gmail.com  Fri Dec 14 03:03:51 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Thu, 13 Dec 2007 21:03:51 -0500
Subject: [R] Analyzing Publications from Pubmed via XML
Message-ID: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/f2d4b9db/attachment.pl 

From ashoka.polpitiya at gmail.com  Fri Dec 14 03:07:35 2007
From: ashoka.polpitiya at gmail.com (Ashoka Polpitiya)
Date: Thu, 13 Dec 2007 18:07:35 -0800
Subject: [R] multiple ANOVAs
In-Reply-To: <003101c83dc5$b4c37860$1e4a6920$@edu>
References: <003101c83dc5$b4c37860$1e4a6920$@edu>
Message-ID: <fb74d7d50712131807mf7d120xa20f71f4e640fd9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/15c315a4/attachment.pl 

From dunn at usq.edu.au  Fri Dec 14 03:02:59 2007
From: dunn at usq.edu.au (Peter Dunn)
Date: Fri, 14 Dec 2007 12:02:59 +1000
Subject: [R] Help! - boxcox transformations
In-Reply-To: <003801c83df4$8602e3b0$c0685280@ts.odu.edu>
References: <003801c83df4$8602e3b0$c0685280@ts.odu.edu>
Message-ID: <200712141202.59715.dunn@usq.edu.au>

> I am trying to detrend and transform variables to achieve
> normality and stationarity (for time series use, namely spectral
> analysis). I am using the boxcox transformations.
>
>
>
> As my dataset contains zeros, I found I need to add a constant to
> it in order to run "boxcox". 

If your data are continuous with exact zeros, and contain a lot of 
zeros, no transformation may help.  You simply map a stack of zeros 
to some other value.  Box--Cox transforms are not really designed 
for this.

If your data really is continuous with exact zeros, other options 
exist such as hurdle models, Tweedie glms (package  tweedie) and so 
forth.

P.

-- 
Dr Peter Dunn  |  dunn <at> usq.edu.au
Faculty of Sciences, USQ; http://www.sci.usq.edu.au/staff/dunn
Aust. Centre for Sustainable Catchments: www.usq.edu.au/acsc

This email (including any attached files) is confidentia...{{dropped:15}}


From rguha at indiana.edu  Fri Dec 14 03:12:43 2007
From: rguha at indiana.edu (Rajarshi Guha)
Date: Thu, 13 Dec 2007 21:12:43 -0500
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
Message-ID: <AC9C43A9-C918-42D7-A397-FC77E43AC3BC@indiana.edu>


On Dec 13, 2007, at 9:03 PM, Farrel Buchinsky wrote:

> I would like to track in which journals articles about a particular  
> disease
> are being published. Creating a pubmed search is trivial. The search
> provides data but obviously not as an R dataframe. I can get the  
> search to
> export the data as an xml feed and the xml package seems to be able  
> to read
> it.
>
> xmlTreeParse("
> http://eutils.ncbi.nlm.nih.gov/entrez/eutils/erss.cgi? 
> rss_guid=0_JYbpsax0ZAAPnOd7nFAX-29fXDpTk5t8M4hx9ytT-
> ",isURL=TRUE)
>
> But getting from there to a dataframe in which one column would be  
> the name
> of the journal and another column would be the year (to keep things  
> simple)
> seems to be beyond my capabilities.

If you're comfortable with Python (or Perl, Ruby etc), it'd be easier  
to just extract the required stuff from the raw feed - using  
ElementTree in Python makes this a trivial task

Once you have the raw data you can read it into R

-------------------------------------------------------------------
Rajarshi Guha  <rguha at indiana.edu>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04  06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
A committee is a group that keeps the minutes and loses hours.
	-- Milton Berle


From fjbuch at gmail.com  Fri Dec 14 03:16:31 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Thu, 13 Dec 2007 21:16:31 -0500
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <AC9C43A9-C918-42D7-A397-FC77E43AC3BC@indiana.edu>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<AC9C43A9-C918-42D7-A397-FC77E43AC3BC@indiana.edu>
Message-ID: <bd93cdad0712131816t3dca1dfftbea82a5257ea9c17@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/8610572a/attachment.pl 

From ggrothendieck at gmail.com  Fri Dec 14 03:42:52 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 13 Dec 2007 21:42:52 -0500
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
Message-ID: <971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>

On Dec 13, 2007 9:03 PM, Farrel Buchinsky <fjbuch at gmail.com> wrote:
> I would like to track in which journals articles about a particular disease
> are being published. Creating a pubmed search is trivial. The search
> provides data but obviously not as an R dataframe. I can get the search to
> export the data as an xml feed and the xml package seems to be able to read
> it.
>
> xmlTreeParse("
> http://eutils.ncbi.nlm.nih.gov/entrez/eutils/erss.cgi?rss_guid=0_JYbpsax0ZAAPnOd7nFAX-29fXDpTk5t8M4hx9ytT-
> ",isURL=TRUE)
>
> But getting from there to a dataframe in which one column would be the name
> of the journal and another column would be the year (to keep things simple)
> seems to be beyond my capabilities.
>
> Has anyone ever done this and could you share your script? Are there any
> published examples where the end result is a dataframe.
>
> I guess what I am looking for is an easy and simple way to parse the feed
> and extract the data. Alternatively how does one turn an RSS feed into a CSV
> file?

Try this:

library(XML)
doc <-
xmlTreeParse("http://eutils.ncbi.nlm.nih.gov/entrez/eutils/erss.cgi?rss_guid=0_JYbpsax0ZAAPnOd7nFAX-29fXDpTk5t8M4hx9ytT-",
isURL = TRUE, useInternalNodes = TRUE)
sapply(c("//author", "//category"), xpathApply, doc = doc, fun = xmlValue)


From rguha at indiana.edu  Fri Dec 14 03:44:42 2007
From: rguha at indiana.edu (Rajarshi Guha)
Date: Thu, 13 Dec 2007 21:44:42 -0500
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <bd93cdad0712131816t3dca1dfftbea82a5257ea9c17@mail.gmail.com>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<AC9C43A9-C918-42D7-A397-FC77E43AC3BC@indiana.edu>
	<bd93cdad0712131816t3dca1dfftbea82a5257ea9c17@mail.gmail.com>
Message-ID: <AEBB91A7-0F3A-476D-AF3B-67352630661A@indiana.edu>


On Dec 13, 2007, at 9:16 PM, Farrel Buchinsky wrote:

> I am afraid not! The only thing I know about Python (or Perl, Ruby  
> etc) is that they exist and that I have been able to download some  
> amazing freeware or open source software thanks to their existence.
> The XML package and specifically the xmlTreeParse function looks as  
> if it is begging to do the task for me. Is that not true?


Certainly - probably as a better Python programmer than an R  
programmer, it's faster and neater for me to do it in Python:

from elementtree.ElementTree import XML
import urllib

url = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/erss.cgi? 
rss_guid=0_JYbpsax0ZAAPnOd7nFAX-29fXDpTk5t8M4hx9ytT-'
con = urllib.urlopen(url)
dat = con.read()
root = XML(dat)
items = root.findall("channel/item")
for item in items:
     category = item.find("category")
     print category.text

The problem is that the RSS feed you linked to, does not contain the  
year of the article in an easily accessible XML element. Rather you  
have to process the HTML content of the description element - which,  
is something R could do, but you'd be using the wrong tool for the job.

In general, if you're planning to analyze article data from Pubmed  
I'd suggest going through the Entrez CGI's (ESearch and EFetch)   
which will give you all the details of the articles in an XML format  
which can then be easily parsed in your language of choice.

This is something that can be done in R (the rpubchem package  
contains functions to process XML files from Pubchem, which might  
provide some pointers)

-------------------------------------------------------------------
Rajarshi Guha  <rguha at indiana.edu>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04  06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Writing software is more fun than working.


From T.METZ at CGIAR.ORG  Fri Dec 14 04:45:06 2007
From: T.METZ at CGIAR.ORG (Metz, Thomas (IRRI))
Date: Fri, 14 Dec 2007 11:45:06 +0800
Subject: [R] RJDBC to OpenOffice Calc as RODBC to MS Excel
Message-ID: <A9688B3789579A498D5CBF96EDBC9D0F011143C6@HERMES>

Under Windows, I have used RODBC to connect to Excel spreadsheets as per
the example below: 

library(RODBC);
connect = odbcConnectExcel("testdata.xls");
query = "SELECT [data$.ethn], [data$.sex], [data$.age], 
                [data$.height], [data$.weight], 
                [label$.label]
         FROM [data$], [label$] 
         WHERE [data$.ethn] = [label$.ethn];"
data = sqlQuery(connect, query);
odbcClose(connect);

[data$] and [label$] are two named sheets in the Excel spreadsheet
testdata.xls. [.ethn], [.sex], [.age], [.height], [.weight], and
[.label] are cloumn names that appear in the first row in the sheets. I
can also have UNION queries that allow me to overcome the spreadsheet
row limitation of a single sheet. The idea is to allow normalization of
data in a spreadsheet and leveraging the power of SQL, without using a
database. 

Can the same be done under Windows (Linux?) with OpenOffice Calc using
RJDBC? Are there ODBC drivers for OpenOffice Calc? 

I know that the right solution would be to use a database, but this is
outside the comfort zone of many users who rely mainly on spreadsheets
to collect, manipulate and analyze their data.

Thomas Metz 
International Rice Research Institute
Philippines


From rgentlem at fhcrc.org  Fri Dec 14 05:35:27 2007
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Thu, 13 Dec 2007 20:35:27 -0800
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
Message-ID: <4762080F.8070606@fhcrc.org>

or just try looking in the annotate package from Bioconductor


Gabor Grothendieck wrote:
> On Dec 13, 2007 9:03 PM, Farrel Buchinsky <fjbuch at gmail.com> wrote:
>> I would like to track in which journals articles about a particular disease
>> are being published. Creating a pubmed search is trivial. The search
>> provides data but obviously not as an R dataframe. I can get the search to
>> export the data as an xml feed and the xml package seems to be able to read
>> it.
>>
>> xmlTreeParse("
>> http://eutils.ncbi.nlm.nih.gov/entrez/eutils/erss.cgi?rss_guid=0_JYbpsax0ZAAPnOd7nFAX-29fXDpTk5t8M4hx9ytT-
>> ",isURL=TRUE)
>>
>> But getting from there to a dataframe in which one column would be the name
>> of the journal and another column would be the year (to keep things simple)
>> seems to be beyond my capabilities.
>>
>> Has anyone ever done this and could you share your script? Are there any
>> published examples where the end result is a dataframe.
>>
>> I guess what I am looking for is an easy and simple way to parse the feed
>> and extract the data. Alternatively how does one turn an RSS feed into a CSV
>> file?
> 
> Try this:
> 
> library(XML)
> doc <-
> xmlTreeParse("http://eutils.ncbi.nlm.nih.gov/entrez/eutils/erss.cgi?rss_guid=0_JYbpsax0ZAAPnOd7nFAX-29fXDpTk5t8M4hx9ytT-",
> isURL = TRUE, useInternalNodes = TRUE)
> sapply(c("//author", "//category"), xpathApply, doc = doc, fun = xmlValue)
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Robert Gentleman, PhD
Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M2-B876
PO Box 19024
Seattle, Washington 98109-1024
206-667-7700
rgentlem at fhcrc.org


From ripley at stats.ox.ac.uk  Fri Dec 14 08:04:36 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Dec 2007 07:04:36 +0000 (GMT)
Subject: [R] RODBC, optimizing memory,
 "Error: cannot allocate vector of size 522 Kb".
In-Reply-To: <795408.86294.qm@web59304.mail.re1.yahoo.com>
References: <795408.86294.qm@web59304.mail.re1.yahoo.com>
Message-ID: <Pine.LNX.4.64.0712140658510.21961@gannet.stats.ox.ac.uk>

This question is nothing to do with RODBC.

You need to study the rw-FAQ Q2.9: I believe you should be able to get up 
to (almost) 3GB on that system.  (BTW, you seem confused about units: I 
hope you have 3GB of RAM, where G means 1024^3.)


On Thu, 13 Dec 2007, Thomas Pujol wrote:

> I am using RODBC and "odbcConnect".  I have successfully used 
> odbcConnect to extract "modest" amounts of data from SQL.  For 
> convenience, (and maybe speed?) I wish, if possible, to extract larger 
> amounts of data in a single query.
>
>  (I am running R2.6.0 under a machine running Windows Small Business 
> Server with 3mb of RAM.  I run gc() prior to attempting the query.  I 
> have attempted to maximize the memory R uses by running the command 
> "memory.size(4095)")
>
>  After attempting my "odbcConnect" query, I receive the following error message:
>   "Error: cannot allocate vector of size 522 Kb".
>
>  After I received the message, I obtained the following statistics re my memory use:
>   memory.limit(size = NA)/1000 #reports memory size
> [1] 4.095
>> memory.size(max = F)/1000 #reports amount of memory currently in use
> [1] 1.930705
>> memory.size(max = T)/1000 #reports maximum amount of memory obtained from the OS
> [1] 1.93925
>
>  Before I give up and go back to running many queries to extract my data, I wanted to ask if there were any suggestions. (I really do wish to extract all this data for local storage as R-files on my hard-drive, it is just a question of the easiest and fastest process.)  Thanks.
>
>
>
>
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Dec 14 08:31:09 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Dec 2007 07:31:09 +0000 (GMT)
Subject: [R] RJDBC to OpenOffice Calc as RODBC to MS Excel
In-Reply-To: <A9688B3789579A498D5CBF96EDBC9D0F011143C6@HERMES>
References: <A9688B3789579A498D5CBF96EDBC9D0F011143C6@HERMES>
Message-ID: <Pine.LNX.4.64.0712140718220.21961@gannet.stats.ox.ac.uk>

On Fri, 14 Dec 2007, Metz, Thomas (IRRI) wrote:

> Under Windows, I have used RODBC to connect to Excel spreadsheets as per
> the example below:
>
> library(RODBC);
> connect = odbcConnectExcel("testdata.xls");
> query = "SELECT [data$.ethn], [data$.sex], [data$.age],
>                [data$.height], [data$.weight],
>                [label$.label]
>         FROM [data$], [label$]
>         WHERE [data$.ethn] = [label$.ethn];"
> data = sqlQuery(connect, query);
> odbcClose(connect);
>
> [data$] and [label$] are two named sheets in the Excel spreadsheet
> testdata.xls. [.ethn], [.sex], [.age], [.height], [.weight], and
> [.label] are cloumn names that appear in the first row in the sheets. I
> can also have UNION queries that allow me to overcome the spreadsheet
> row limitation of a single sheet. The idea is to allow normalization of
> data in a spreadsheet and leveraging the power of SQL, without using a
> database.
>
> Can the same be done under Windows (Linux?) with OpenOffice Calc using
> RJDBC? Are there ODBC drivers for OpenOffice Calc?

An awful lot of that is Microsoft warts on SQL, so it will not be 
portable.  But in a more standard syntax (drop the [] and $) it should be 
doable over any connection that supports SQL queries.

The question is whether OO calc has suitable drivers as an ODBC/JDBC 
server.  Not a question for this list!  (I suspect the answer is no: 
Microsoft's drivers effectively use the Access engine to work with 
spreadsheet files and even plain text.  I don't even see drivers for OO 
base.)

> I know that the right solution would be to use a database, but this is
> outside the comfort zone of many users who rely mainly on spreadsheets
> to collect, manipulate and analyze their data.
>
> Thomas Metz
> International Rice Research Institute
> Philippines

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jasonshi510 at hotmail.com  Fri Dec 14 10:09:59 2007
From: jasonshi510 at hotmail.com (Xin)
Date: Fri, 14 Dec 2007 09:09:59 -0000
Subject: [R] can I solve this equation in
	R----29.040x+1=327.727x^(355.768x/(1-x))
Message-ID: <BAY141-DAV241281A93A369D45678CEF0670@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071214/ab7d7cf4/attachment.pl 

From robert.ptacnik at niva.no  Fri Dec 14 10:47:09 2007
From: robert.ptacnik at niva.no (robert.ptacnik at niva.no)
Date: Fri, 14 Dec 2007 10:47:09 +0100
Subject: [R] termplot reference
Message-ID: <OF791A61BE.259B3B8D-ONC12573B1.0034A17A-C12573B1.0035C185@niva.no>

Dear Group,
Is there a specific reference for how termplot partitionates residuals
(part=T)? I want to use figures from termplot in a publication and wonder
about an appropriate reference (nothing in the help-file)
Thanks
Robert



----------------------------------------------------------------------------------------------------------------------
NIVAs hovedkontor har flyttet til nye lokaler i CIENS - Forskningssenter
for milj? og samfunn; Gaustadall?en 21, 0349 Oslo. Meld deg p? v?rt
nyhetsbrev p? www.niva.no

From ligges at statistik.uni-dortmund.de  Fri Dec 14 10:49:33 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 14 Dec 2007 10:49:33 +0100
Subject: [R] can I solve this equation
	in	R----29.040x+1=327.727x^(355.768x/(1-x))
In-Reply-To: <BAY141-DAV241281A93A369D45678CEF0670@phx.gbl>
References: <BAY141-DAV241281A93A369D45678CEF0670@phx.gbl>
Message-ID: <476251AD.2020501@statistik.uni-dortmund.de>

Well, numerically by re-writing

-29.040x+1 = 327.727x^(355.768x/(1-x))
0 = 327.727x^(355.768x/(1-x)) - (-29.040x+1)
0^2 = (327.727x^(355.768x/(1-x)) - (-29.040x+1))^2

and then minimizing the right hand side:

optimize(function(x)
         (327.727 * x^(355.768*x / (1-x)) - (-29.040*x + 1))^2,
     interval = c(0, 1))


Uwe Ligges



Xin wrote:
> -29.040x+1=327.727x^(355.768x/(1-x))
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ileyer at yahoo.de  Fri Dec 14 11:40:55 2007
From: ileyer at yahoo.de (Ilona Leyer)
Date: Fri, 14 Dec 2007 11:40:55 +0100 (CET)
Subject: [R] convergence error code in mixed effects models
In-Reply-To: <40e66e0b0712131426h6e702ed2pa45b5786c7b29d36@mail.gmail.com>
Message-ID: <83439.66132.qm@web26213.mail.ukl.yahoo.com>


Here an simple example:

rep	treat	heightfra	leaffra	leafvim	week
ID1	pHf	1.54	4	4	4
ID2	pHf	1.49	4	4	4
ID3	pHf	1.57	4	5	4
ID4	pHf	1.48	4	4	4
ID5	pHf	1.57	4	4	4
ID6	pHs	1.29	4	5	4
ID7	pHs	0.97	4	5	4
ID8	pHs	2.06	4	4	4
ID9	pHs	0.88	4	4	4
ID10	pHs	1.47	4	4	4
ID1	pHf	3.53	5	6	6
ID2	pHf	4.08	6	6	6
ID3	pHf	3.89	6	6	6
ID4	pHf	3.78	5	6	6
ID5	pHf	3.92	6	6	6
ID6	pHs	2.76	5	5	6
ID7	pHs	3.31	6	7	6
ID8	pHs	4.46	6	7	6
ID9	pHs	2.19	5	5	6
ID10	pHs	3.83	5	5	6
ID1	pHf	5.07	7	7	9
ID2	pHf	6.42	7	8	9
ID3	pHf	5.43	6	8	9
ID4	pHf	6.83	6	8	9
ID5	pHf	6.26	6	8	9
ID6	pHs	4.57	6	9	9
ID7	pHs	5.05	6	7	9
ID8	pHs	6.27	6	8	9
ID9	pHs	3.37	5	7	9
ID10	pHs	5.38	6	8	9
ID1	pHf	5.58	7	9	12
ID2	pHf	7.43	8	9	12
ID3	pHf	6.18	8	10	12
ID4	pHf	6.91	7	10	12
ID5	pHf	6.78	7	10	12
ID6	pHs	4.99	6	13	12
ID7	pHs	5.50	7	8	12
ID8	pHs	6.56	7	10	12
ID9	pHs	3.72	6	10	12
ID10	pHs	5.94	6	10	12


I used the procedure described in Crawley?s new R
Book.
For two of the tree response variables
(heightfra,leaffra) it doesn?t work, while it worked
with leafvim (but in another R session, yesterday,
leaffra worked as well...).

Here the commands:

> attach(test)
> names(test)
[1] "week"      "rep"       "treat"     "heightfra"
"leaffra"   "leafvim"  
> library(nlme)
>
test<-groupedData(heightfra~week|rep,outer=~treat,test)
> model1<-lme(heightfra~treat,random=~week|rep)
Error in lme.formula(heightfra ~ treat, random = ~week
| rep) : 
        nlminb problem, convergence error code = 1;
message = iteration limit reached without convergence
(9)

>
test<-groupedData(leaffra~week|rep,outer=~treat,test)
> model2<-lme(leaffra~treat,random=~week|rep)
Error in lme.formula(leaffra ~ treat, random = ~week |
rep) : 
        nlminb problem, convergence error code = 1;
message = iteration limit reached without convergence
(9)

>
test<-groupedData(leafvim~week|rep,outer=~treat,test)
> model3<-lme(leafvim~treat,random=~week|rep)
> summary(model)
Error in summary(model) : object "model" not found
> summary(model3)
Linear mixed-effects model fit by REML
 Data: NULL 
       AIC      BIC    logLik
  129.6743 139.4999 -58.83717

Random effects:
 Formula: ~week | rep
 Structure: General positive-definite, Log-Cholesky
parametrization
            StdDev    Corr  
(Intercept) 4.4110478 (Intr)
week        0.7057311 -0.999
Residual    0.5976143       

Fixed effects: leafvim ~ treat 
               Value Std.Error DF  t-value p-value
(Intercept) 5.924659 0.1653596 30 35.82893  0.0000
treatpHs    0.063704 0.2338538  8  0.27241  0.7922
 Correlation: 
         (Intr)
treatpHs -0.707

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3       
 Max 
-1.34714254 -0.53042878 -0.01769195  0.40644540 
2.29301560 

Number of Observations: 40
Number of Groups: 10 

Is there a solution for this problem?

Thanks!!!

Ilona

--- Douglas Bates <bates at stat.wisc.edu> schrieb:

> On Dec 13, 2007 4:15 PM, Ilona Leyer
> <ileyer at yahoo.de> wrote:
> > Dear All,
> > I want to analyse treatment effects with time
> series
> > data:  I measured e.g. leaf number (five replicate
> > plants) in relation to two soil pH - after 2,4,6,8
> > weeks. I used mixed effects models, but some
> analyses
> > didn?t work. It seems for me as if this is a
> randomly
> > occurring problem since sometimes the same model
> works
> > sometimes not.
> >
> > An example:
> > > names(test)
> > [1] "rep"    "treat"  "leaf"   "week"
> > > library (lattice)
> > > library (nlme)
> > >
> test<-groupedData(leaf~week|rep,outer=~treat,test)
> > > model<-lme(leaf~treat,random=~leaf|rep)
> > Error in lme.formula(leaf~ treat, random =
> ~week|rep)
> 
> Really!? You gave lme a model with random = ~ leaf |
> rep (and no data
> specification) and it tried to fit a model with
> random = ~ week | rep?
> Are you sure that is an exact transcript?
> 
> > :
> >         nlminb problem, convergence error code =
> 1;
> > message = iteration limit reached without
> convergence
> > (9)
> 
> > Has anybody an idea to solve this problem?
> 
> Oh, I have lots of ideas but without a reproducible
> example I can't
> hope to decide what might be the problem.
> 
> It appears that the model may be over-parameterized.
>  Assuming that
> there are 4 different values of week then ~ week |
> rep requires
> fitting 10 variance-covariance parameters. That's a
> lot.
> The error code indicates that the optimizer is
> taking
>


From bgreen at dyson.brisnet.org.au  Fri Dec 14 13:05:19 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Fri, 14 Dec 2007 22:05:19 +1000
Subject: [R] calculating the number of days from dates
In-Reply-To: <mailman.27.1197457204.24367.r-help@r-project.org>
References: <mailman.27.1197457204.24367.r-help@r-project.org>
Message-ID: <20071214120132.9A9785955DD@borg.st.net.au>


Hello,

I gather variants of this question have been asked previously. I have 
done some reading but only became more confused, as I suspect what I 
am trying to do is more basic than other applications.

The following code readily calculates the difference in days between two dates:

  newdays <- ISOdate(2005, 5,12) - ISOdate(2006, 12, 22)

However, I wanted to be able to deduct the dates in one variable from 
the dates in another variable, resulting in a new variable - e.g the 
difference in days between the two dates. Below is a sample of my 
data. My questions:

1. I tried changing the data to dates via as.Date. Is this necessary 
or do I need to alter the date format itself, e.g to 12/12/78 or some 
other format?
2. I gather there are various packages as well as date formats.What 
is the most straight forward approach to calculate the difference 
between two dates, as below.


 > dates <- read.csv("c:\\dates.csv",header=T)
 > dates
           v1         v2
1 12/12/1978 12/12/2005
2 23/01/1965 23/09/2001
3 24/12/2004 16/03/2007
4  3/03/2003  4/04/2004
5  8/11/2006  1/05/2007

 > class(dates$v1)
[1] "factor"
 > class(dates$v2)
[1] "factor"

 > dates <- read.csv("c:\\dates.csv",header=T, 
as.Date(as.character(dates) "%d/%m/%Y"))
Error: syntax error, unexpected STR_CONST, expecting ',' in "dates <- 
read.csv("c:\\dates.csv",header=T, as.Date(as.character(dates) "%d/%m/%Y""
 >

Any assistance is much appreciated,

Bob


From asb at nih.gov  Fri Dec 14 14:16:23 2007
From: asb at nih.gov (Alan Barnett)
Date: Fri, 14 Dec 2007 08:16:23 -0500
Subject: [R] Plot question
Message-ID: <47628227.8060104@nih.gov>

I have some data consisting of multiple trials of an experiment with 
different values of an independent variable.  If I run
R> plot(var,result)
I get a scatterplot of result versus the independent variable var.
If I run
R> plot(as.factor(var),result)
I get a boxplot of the distribution of result for each value of var.  In 
this plot, each boxplot is labeled by the corresponding value of var, 
but the absissas are evenly spaced.
Is it possible to generate a boxplot with the absissas of each boxplot 
equal to the corresponding value of var?

-- 
Alan Barnett, PhD
Imaging Physicist
National Institutes of Health
NIMH/CBDB
301 402 3507


From kelvin.lam at ices.on.ca  Thu Dec 13 18:25:05 2007
From: kelvin.lam at ices.on.ca (Lam, Kelvin)
Date: Thu, 13 Dec 2007 12:25:05 -0500
Subject: [R] Bivariate Survival Analysis
Message-ID: <69E8946004ED8243A9E1554F7401424F0257A98F@ices10.ices.on.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071213/83ba6402/attachment.pl 

From daniel at umd.edu  Thu Dec 13 19:17:36 2007
From: daniel at umd.edu (Daniel Malter)
Date: Thu, 13 Dec 2007 13:17:36 -0500
Subject: [R] Gaussian Smoothing
In-Reply-To: <14321313.post@talk.nabble.com>
Message-ID: <200712131816.CIS32123@md2.mail.umd.edu>

The gam() function in library mgcv could be what you are looking for.

Cheers,
Daniel

-------------------------
cuncta stricte discussurus
-------------------------

-----Urspr?ngliche Nachricht-----
Von: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Im
Auftrag von thierrydb
Gesendet: Thursday, December 13, 2007 12:58 PM
An: r-help at r-project.org
Betreff: [R] Gaussian Smoothing


Hello, 

I'm new to R, and I would like to know if there is a way to smooth a curve
using a Gaussian smoothing with R. 

Thank you very much, 

TDB
--
View this message in context:
http://www.nabble.com/Gaussian-Smoothing-tp14321313p14321313.html
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bawazer_3 at yahoo.com  Fri Dec 14 00:47:57 2007
From: bawazer_3 at yahoo.com (jIowa)
Date: Thu, 13 Dec 2007 15:47:57 -0800 (PST)
Subject: [R]  High Frequency Financial Calculations
Message-ID: <14327315.post@talk.nabble.com>


Does anyone know of a package for High Frequency Data analysis.
Particularly I'd like to find something over any of the following topics:
     ACD (Autoregressive Conditional Duration) - Engle
     HARCH - Muller
     UHF-GARCH (Ultra High Frequency GARCH) - Engle

thanks!
-- 
View this message in context: http://www.nabble.com/High-Frequency-Financial-Calculations-tp14327315p14327315.html
Sent from the R help mailing list archive at Nabble.com.


From elke.moons at uhasselt.be  Fri Dec 14 14:17:19 2007
From: elke.moons at uhasselt.be (Elke Moons)
Date: Fri, 14 Dec 2007 14:17:19 +0100
Subject: [R] kernel density in space
Message-ID: <005801c83e53$a7e26960$f60ebec1@ibis.luc.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071214/e4da418d/attachment.pl 

From someone29_7 at yahoo.de  Fri Dec 14 13:39:02 2007
From: someone29_7 at yahoo.de (Peter Paul)
Date: Fri, 14 Dec 2007 12:39:02 -0000
Subject: [R] Conflating categories
Message-ID: <000e01c83e4e$75a9e9a0$0200a8c0@PC2>



Hi,

I think this is a pretty basic question. I still couldn#t find the answer to 
it, though.

I have some data loaded into R, which looks like this:

> data()
...
38358     Advice      Article
38359     Advice      Article
38360    GeneralInfo  List
38361    GeneralInfo      Article
38362     Purchase   Paragraphs
38363     Purchase         List
38364     Purchase   Paragraphs
...


I now would like to edit the data in such a way that every "Advice" is 
changed into "GeneralInfo".
Is there some easy way how I can do this?

Thanks a lot!


From marc_schwartz at comcast.net  Fri Dec 14 14:28:23 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 14 Dec 2007 07:28:23 -0600
Subject: [R] RJDBC to OpenOffice Calc as RODBC to MS Excel
In-Reply-To: <Pine.LNX.4.64.0712140718220.21961@gannet.stats.ox.ac.uk>
References: <A9688B3789579A498D5CBF96EDBC9D0F011143C6@HERMES>
	<Pine.LNX.4.64.0712140718220.21961@gannet.stats.ox.ac.uk>
Message-ID: <1197638903.3134.16.camel@Bellerophon.localdomain>


On Fri, 2007-12-14 at 07:31 +0000, Prof Brian Ripley wrote:
> On Fri, 14 Dec 2007, Metz, Thomas (IRRI) wrote:
> 
> > Under Windows, I have used RODBC to connect to Excel spreadsheets as per
> > the example below:
> >
> > library(RODBC);
> > connect = odbcConnectExcel("testdata.xls");
> > query = "SELECT [data$.ethn], [data$.sex], [data$.age],
> >                [data$.height], [data$.weight],
> >                [label$.label]
> >         FROM [data$], [label$]
> >         WHERE [data$.ethn] = [label$.ethn];"
> > data = sqlQuery(connect, query);
> > odbcClose(connect);
> >
> > [data$] and [label$] are two named sheets in the Excel spreadsheet
> > testdata.xls. [.ethn], [.sex], [.age], [.height], [.weight], and
> > [.label] are cloumn names that appear in the first row in the sheets. I
> > can also have UNION queries that allow me to overcome the spreadsheet
> > row limitation of a single sheet. The idea is to allow normalization of
> > data in a spreadsheet and leveraging the power of SQL, without using a
> > database.
> >
> > Can the same be done under Windows (Linux?) with OpenOffice Calc using
> > RJDBC? Are there ODBC drivers for OpenOffice Calc?
> 
> An awful lot of that is Microsoft warts on SQL, so it will not be 
> portable.  But in a more standard syntax (drop the [] and $) it should be 
> doable over any connection that supports SQL queries.
> 
> The question is whether OO calc has suitable drivers as an ODBC/JDBC 
> server.  Not a question for this list!  (I suspect the answer is no: 
> Microsoft's drivers effectively use the Access engine to work with 
> spreadsheet files and even plain text.  I don't even see drivers for OO 
> base.)

There are none to my knowledge.  Most of OO.org's ODBC/JDBC integration
is one-way. That is, it can connect within the suite and to external
sources, but does not appear to provide connectivity to enable external
applications to acquire data stored within OO.org's apps.

Base is an embedded version of HSQLDB (http://www.hsqldb.org), which is
a java based application. In theory, it would support a JDBC interface,
but I have seen none and when the subject comes up on the OO.org lists,
no solutions are forthcoming. It's too bad, at least within this
context, that the OO.org folks elected to use HSQLDB rather than SQLite,
which was the the other option under consideration. Perhaps Sun's
influence, vis-a-vis Java, won the day here.

Bearing in mind that OO.org's Write and Calc documents are just 'zipped'
XML files, it would be possible to parse the data stored within such
documents. I suspect Max Kuhn has spent much time on this for odfWeave.

There are Perl modules that can provide a level of interaction here. For
example, OpenOffice::Parse::SXC
(http://search.cpan.org/~dclee/OpenOffice-Parse-SXC-0.03/SXC.pm)
provides the means to parse a Calc file directly, without needing the
OO.org API.

One could wrap that module in an R function via a system() call and then
interact with a Calc file directly.

HTH,

Marc Schwartz


From KKIII at Indiana.Edu  Thu Dec 13 20:13:39 2007
From: KKIII at Indiana.Edu (Ken Kelley)
Date: Thu, 13 Dec 2007 14:13:39 -0500
Subject: [R] [R-pkgs] New version of MBESS released
Message-ID: <86b533e90712131113y3104de23x9028456d2f7d8466@mail.gmail.com>

Hello useRs,

MBESS (Methods for the Behavioral, Educational, and Social Sciences)
has recently been released and should be on all of the mirrors by now
(with binaries for Mac and Windows:
http://cran.r-project.org/src/contrib/Descriptions/MBESS.html).

The major contribution of MBESS is confidence intervals for
noncentrality parameters (t, F, and chi-square) and standardized
effect sizes (e.g., the standardized mean and mean difference, R^2 for
random or fixed effects, the coefficient of variation, the root mean
square error of approximation, standardized regression coefficients)
as well as sample size planning from the accuracy in parameter
estimation (AIPE) approach, where the width of the observed confidence
intervals is of interest (in addition to or instead of the power
analytic approach to sample size planning).

This is the 10th release of MBESS and it is version number is now 1.0.0.

Detailed information about MBESS is available in the current issue of
Behavior Research Methods
(http://www.psychonomic.org/BRMIC/contents.htm) as well as Journal of
Statistical Software (http://www.jstatsoft.org/v20/i08).

Take care,
Ken

-- 
Ken Kelley, Ph.D.
Indiana University
Inquiry Methodology Program
201 North Rose Avenue, Suite 4000
Bloomington, Indiana 47405

Phone: 812-856-8330 / Fax: 812-856-8333
Email: KKIII at Indiana.Edu
Internet: http://www.indiana.edu/~kenkel

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From petr.pikal at precheza.cz  Fri Dec 14 14:54:06 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Fri, 14 Dec 2007 14:54:06 +0100
Subject: [R] Odp:  Conflating categories
In-Reply-To: <000e01c83e4e$75a9e9a0$0200a8c0@PC2>
Message-ID: <OF5A85D677.79B6B4E5-ONC12573B1.004C433D-C12573B1.004C5DA6@precheza.cz>

Hi

r-help-bounces at r-project.org napsal dne 14.12.2007 13:39:02:

> 
> 
> Hi,
> 
> I think this is a pretty basic question. I still couldn#t find the 
answer to 
> it, though.
> 
> I have some data loaded into R, which looks like this:
> 
> > data()
> ...
> 38358     Advice      Article
> 38359     Advice      Article
> 38360    GeneralInfo  List
> 38361    GeneralInfo      Article
> 38362     Purchase   Paragraphs
> 38363     Purchase         List
> 38364     Purchase   Paragraphs
> ...

If it is factor you can change its levels

> temp$V1
[1] A01 A01 A01 A02 A02 A03 A03 A04
Levels: A01 A02 A03 A04
> levels(temp$V1)
[1] "A01" "A02" "A03" "A04"
> levels(temp$V1)[1:2]<-"one"
> temp$V1
[1] one one one one one A03 A03 A04
Levels: one A03 A04

Regards
Petr



> 
> 
> I now would like to edit the data in such a way that every "Advice" is 
> changed into "GeneralInfo".
> Is there some easy way how I can do this?
> 
> Thanks a lot!
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Fri Dec 14 14:59:44 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 14 Dec 2007 08:59:44 -0500
Subject: [R] Conflating categories
In-Reply-To: <000e01c83e4e$75a9e9a0$0200a8c0@PC2>
References: <000e01c83e4e$75a9e9a0$0200a8c0@PC2>
Message-ID: <47628C50.2030301@stats.uwo.ca>

On 12/14/2007 7:39 AM, Peter Paul wrote:
> 
> Hi,
> 
> I think this is a pretty basic question. I still couldn#t find the answer to 
> it, though.
> 
> I have some data loaded into R, which looks like this:
> 
>> data()
> ...
> 38358     Advice      Article
> 38359     Advice      Article
> 38360    GeneralInfo  List
> 38361    GeneralInfo      Article
> 38362     Purchase   Paragraphs
> 38363     Purchase         List
> 38364     Purchase   Paragraphs
> ...
> 
> 
> I now would like to edit the data in such a way that every "Advice" is 
> changed into "GeneralInfo".
> Is there some easy way how I can do this?

This is a little tricky, because R has probably converted those columns 
into factors.  Let's assume that column 2 is named Type and the dataset 
is named dat.

Then you replace it as

dat$Type[dat$Type == "Advice"] <- "GeneralInfo"

You'd think the following would work:

dat$Type <- ifelse(dat$Type == "Advice", "GeneralInfo", dat$Type)

but it doesn't, because ifelse() loses the factor levels.  Factors are 
one of the most error-prone features of the S language.

Duncan Murdoch


From mxkuhn at gmail.com  Fri Dec 14 15:19:24 2007
From: mxkuhn at gmail.com (Max Kuhn)
Date: Fri, 14 Dec 2007 09:19:24 -0500
Subject: [R] RJDBC to OpenOffice Calc as RODBC to MS Excel
In-Reply-To: <1197638903.3134.16.camel@Bellerophon.localdomain>
References: <A9688B3789579A498D5CBF96EDBC9D0F011143C6@HERMES>
	<Pine.LNX.4.64.0712140718220.21961@gannet.stats.ox.ac.uk>
	<1197638903.3134.16.camel@Bellerophon.localdomain>
Message-ID: <6731304c0712140619q545c7b82l44e0b4a05c3b5eae@mail.gmail.com>

On Dec 14, 2007 8:28 AM, Marc Schwartz <marc_schwartz at comcast.net> wrote:
>
> Bearing in mind that OO.org's Write and Calc documents are just 'zipped'
> XML files, it would be possible to parse the data stored within such
> documents. I suspect Max Kuhn has spent much time on this for odfWeave.
>

I haven't played much with Calc, but it if the Calc file has formulas
or anything other than raw data, this would probably be a mess to
parse.

The first thing that I would try is to use command line tools (like
ooconvert or jodconverter) to convert the file to csv and then read it
in. These tools were written for Write documents, but they may also be
good at converting other types of od* documents.

Good luck,

Max


From marc_schwartz at comcast.net  Fri Dec 14 15:24:30 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 14 Dec 2007 08:24:30 -0600
Subject: [R] Plot question
In-Reply-To: <47628227.8060104@nih.gov>
References: <47628227.8060104@nih.gov>
Message-ID: <1197642270.3134.23.camel@Bellerophon.localdomain>


On Fri, 2007-12-14 at 08:16 -0500, Alan Barnett wrote:
> I have some data consisting of multiple trials of an experiment with 
> different values of an independent variable.  If I run
> R> plot(var,result)
> I get a scatterplot of result versus the independent variable var.
> If I run
> R> plot(as.factor(var),result)
> I get a boxplot of the distribution of result for each value of var.  In 
> this plot, each boxplot is labeled by the corresponding value of var, 
> but the absissas are evenly spaced.
> Is it possible to generate a boxplot with the absissas of each boxplot 
> equal to the corresponding value of var?

If you want boxplots, you could do something like this:

CV <- rnorm(100)
FV <- sample(c(1, 3, 6), 100, replace = TRUE)

boxplot(CV ~ factor(FV, levels = seq(max(FV))))


This essentially 'fills in' the missing values (levels) of the factor
variable so that they are included in the plot.

HTH,

Marc Schwartz


From mark_difford at yahoo.co.uk  Fri Dec 14 15:25:28 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Fri, 14 Dec 2007 06:25:28 -0800 (PST)
Subject: [R] Plot question
In-Reply-To: <47628227.8060104@nih.gov>
References: <47628227.8060104@nih.gov>
Message-ID: <14336622.post@talk.nabble.com>


Hi Alan,

Yes it is, but you need to do a bit work.  There are different approaches. 
Look at the at= option under

?bxp

and draw your boxplots with something like:

boxplot(y ~ as.numeric(as.factor(grp)), at=c(0.5, 2, 2.5, 3), xaxt="n", ...)
axis(side=1, at=c(0.5, 2, 2.5, 3), labels=c("0.5","2","2.5","3"))

This should do it, and is perhaps the easiest route.  There may be something
missing, because I don't have a concrete example.  Anyhow, you should be
able to fill in the gaps.

HTH,

Mark.


Alan Barnett wrote:
> 
> I have some data consisting of multiple trials of an experiment with 
> different values of an independent variable.  If I run
> R> plot(var,result)
> I get a scatterplot of result versus the independent variable var.
> If I run
> R> plot(as.factor(var),result)
> I get a boxplot of the distribution of result for each value of var.  In 
> this plot, each boxplot is labeled by the corresponding value of var, 
> but the absissas are evenly spaced.
> Is it possible to generate a boxplot with the absissas of each boxplot 
> equal to the corresponding value of var?
> 
> -- 
> Alan Barnett, PhD
> Imaging Physicist
> National Institutes of Health
> NIMH/CBDB
> 301 402 3507
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Plot-question-tp14335709p14336622.html
Sent from the R help mailing list archive at Nabble.com.


From lieven.desmet at wis.kuleuven.be  Fri Dec 14 15:31:59 2007
From: lieven.desmet at wis.kuleuven.be (Lieven Desmet)
Date: Fri, 14 Dec 2007 15:31:59 +0100
Subject: [R] discrepancy between periodogram implementations ? per and
 spec.pgram
In-Reply-To: <Pine.LNX.4.64.0712121532050.9158@gannet.stats.ox.ac.uk>
References: <475FF863.6030403@wis.kuleuven.be>
	<Pine.LNX.4.64.0712121532050.9158@gannet.stats.ox.ac.uk>
Message-ID: <476293DF.3020007@wis.kuleuven.be>

Prof Brian Ripley wrote:

> There are several definitions of a periodgram.  Note that
>
>> log(2*pi)
>
> [1] 1.837877
>
> See the comments in ?spectrum about scalings.
>
> I think the comments in ?per incorrectly ignore the scaling issues: 
> per() does not take the base frequency into account and has an extra 
> divisor of 2*pi.  E.g.
>
>> x <- rnorm(64)
>> spec.pgram(x, taper=0, detrend=F)$spec/per(x)[-1]
>
>  [1] 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 
> 6.283185
>  [9] 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 
> 6.283185
> [17] 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 
> 6.283185
> [25] 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 6.283185 
> 6.283185
>
>
> On Wed, 12 Dec 2007, Lieven Desmet wrote:
>
>> hello,
>>
>> I have been using the per function in package longmemo to obtain a
>> simple raw periodogram.
>> I am considering to switch to the function spec.pgram since I want to be
>> able to do tapering.
>> To compare both I used spec.pgram with the options as suggested in the
>> documentation of per {longmemo} to make them correspond.
>>
>> Now I have found on a variety of examples that there is a shift between
>> the log of the periodogram with per and that with spec.pgram. This
>> vertical shift amounts to  approx. 1.8  on the log scale  (the graph of
>> spec.pgram being above the one from per).
>>
>> Is there some explanation for this ? Is the one from spec.pgram the
>> better one as suggested in the documentation of per {longmemo}? Finally
>> how are these related to an estimate of the spectral density obtained
>> from spec.arima ?
>
>
> What is spec.arima?  If you meant spec.ar, that is on the same scale 
> as spec.pgram for series with base frequency 1 (and for all series for 
> R >= 2.7.0).
>
>
>> Many thanks for help and clarification.
>>
>> Lieven Desmet
>
>
Dear Prof. Ripley,

thanks very much for a quick and helpful response. In the last question 
I wanted to hint at
specARIMA which I am using to get the theoretical spectral density of an 
ARMA process.

This works very well in general, however, in a simple example

X_t=0.7*X_{t-1}+epsilon_t

I obtain a value 1.768253 for funscaled[1] ( the first Fourier frequency 
0.003141593)

using

str(f<-specARIMA(eta=c(H=0.5,phi=c(0.7),psi=c()),p=1,q=0,m=2000))
funscaled<-numeric(length(f$freq))
funscaled<-f$spec*f$theta1

where the theoretical value should be 0.901878 with

b<-0.7
omega<-0.003141593
1/(2*pi)*(1-b^2)/(1+b^2-2*b*cos(omega))
[1] 0.9018088


using the formula (2.40) in Fan and Yao, Nonlinear Time Series ( 
Springer 2003 ), page 54-55

Is there also a simple explanation for this ? am I overlooking something ?

Thanks and best regards,

Lieven Desmet,

maths dept - KULeuven - Belgium

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From wwwhsd at gmail.com  Fri Dec 14 15:41:29 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 14 Dec 2007 12:41:29 -0200
Subject: [R] calculating the number of days from dates
In-Reply-To: <20071214120132.9A9785955DD@borg.st.net.au>
References: <mailman.27.1197457204.24367.r-help@r-project.org>
	<20071214120132.9A9785955DD@borg.st.net.au>
Message-ID: <da79af330712140641n516def37r55112656ceb9cc2e@mail.gmail.com>

Try this:

dates <- read.csv("c:\\dates.csv",header=T)
dates[,1] <- as.Date(dates[,1], "%d/%m/%Y")
dates[,2] <- as.Date(dates[,2], "%d/%m/%Y")
transform(dates,
     Dif=V2-V1)

On 14/12/2007, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
>
> Hello,
>
> I gather variants of this question have been asked previously. I have
> done some reading but only became more confused, as I suspect what I
> am trying to do is more basic than other applications.
>
> The following code readily calculates the difference in days between two dates:
>
>  newdays <- ISOdate(2005, 5,12) - ISOdate(2006, 12, 22)
>
> However, I wanted to be able to deduct the dates in one variable from
> the dates in another variable, resulting in a new variable - e.g the
> difference in days between the two dates. Below is a sample of my
> data. My questions:
>
> 1. I tried changing the data to dates via as.Date. Is this necessary
> or do I need to alter the date format itself, e.g to 12/12/78 or some
> other format?
> 2. I gather there are various packages as well as date formats.What
> is the most straight forward approach to calculate the difference
> between two dates, as below.
>
>
>  > dates <- read.csv("c:\\dates.csv",header=T)
>  > dates
>           v1         v2
> 1 12/12/1978 12/12/2005
> 2 23/01/1965 23/09/2001
> 3 24/12/2004 16/03/2007
> 4  3/03/2003  4/04/2004
> 5  8/11/2006  1/05/2007
>
>  > class(dates$v1)
> [1] "factor"
>  > class(dates$v2)
> [1] "factor"
>
>  > dates <- read.csv("c:\\dates.csv",header=T,
> as.Date(as.character(dates) "%d/%m/%Y"))
> Error: syntax error, unexpected STR_CONST, expecting ',' in "dates <-
> read.csv("c:\\dates.csv",header=T, as.Date(as.character(dates) "%d/%m/%Y""
>  >
>
> Any assistance is much appreciated,
>
> Bob
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From rmh at temple.edu  Fri Dec 14 15:56:11 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 14 Dec 2007 09:56:11 -0500
Subject: [R] Plot question
In-Reply-To: <47628227.8060104@nih.gov>
References: <47628227.8060104@nih.gov>
Message-ID: <000201c83e61$77ec64f0$67c52ed0$@edu>

The easiest way would be to use the HH package, which you can get from CRAN.

Marc's example is limited to integers on the X axis.  Using positioned(),
which is
an extension to ordered(), allows arbitrary values.  Building on that
example

require(HH)

CV <- rnorm(100)
FV <- sample(c(-1.4, 3.2, 5), 100, replace = TRUE)

bwplot(CV ~ positioned(FV))



-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Alan Barnett
Sent: Friday, December 14, 2007 08:16 AM
To: R-Help
Subject: [R] Plot question

I have some data consisting of multiple trials of an experiment with 
different values of an independent variable.  If I run
R> plot(var,result)
I get a scatterplot of result versus the independent variable var.
If I run
R> plot(as.factor(var),result)
I get a boxplot of the distribution of result for each value of var.  In 
this plot, each boxplot is labeled by the corresponding value of var, 
but the absissas are evenly spaced.
Is it possible to generate a boxplot with the absissas of each boxplot 
equal to the corresponding value of var?

-- 
Alan Barnett, PhD
Imaging Physicist
National Institutes of Health
NIMH/CBDB
301 402 3507

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From shaun.hughes at sgcib.com  Fri Dec 14 16:05:05 2007
From: shaun.hughes at sgcib.com (shaun.hughes at sgcib.com)
Date: Fri, 14 Dec 2007 16:05:05 +0100
Subject: [R] options("defaultPackages") was not found [C1]
Message-ID: <OFF448CA42.418A6448-ONC12573B1.00526234-C12573B1.0052E1EF@fr.world.socgen>



hello, i'm having a problem creating a custom package.
i've found other posts on the web that describe a similar problem but i
haven't found any solution.  please advise.

the error in the build process is
checking DESCRIPTION meta-information ... ERROR
During startup - Warning messages:
' in: library(package, lib.loc = lib.loc, character.only = TRUE, logical =
TRUE,
 in options("defaultPackages") was not found



my r source (in the file dlog.r)

dlog <- function(X,h=1) {
list(res=(as.matrix(diff(log(X),lag = h, differences = 1))), res2=h)
}

1.  in RGui
source("C:\\shaun\\projects\\rpackagecreation\\dlog.r")

then

package.skeleton(name="DLOGPackage", list=ls(), .GlobalEnv,
"C:\\shaun\\projects\\rpackagecreation", TRUE)

2.  in the R install\bin directory i ran
rcmd build --force --binary --no-vignettes --docs=none
c:/shaun/projects/rpackagecreation/DLOGPackage

and i get the following message

* checking for file
'c:/shaun/projects/rpackagecreation/DLOGPackage/DESCRIPTION' ... OK
* preparing 'c:/shaun/projects/rpackagecreation/DLOGPackage':
* checking DESCRIPTION meta-information ... ERROR
During startup - Warning messages:
' in: library(package, lib.loc = lib.loc, character.only = TRUE, logical =
TRUE,
 in options("defaultPackages") was not found

i modified the DESCRIPTION file as follows

Package: DLOGPackage
Type: Package
Title: A test package
Version: 1.0
Date: 2007-12-14
Author: Shaun HUGHES
Maintainer: Shaun HUGHES <shaun.hughes at sgcib.com>
Description: Just a test custom package
Depends: R (>= 2.5.1)
License: GPL (Version 3 or later)



Version : R version 2.5.1 (2007-06-27)
OS Windows XP SP2



Cordialement,
--------------------------------------------------
Shaun HUGHES
ITEC/GED/DAI/RTM
+33 (0)1 58 98 27 54
--------------------------------------------------
*************************************************************************
This message and any attachments (the "message") are con...{{dropped:10}}


From gilhamto at gmail.com  Fri Dec 14 16:52:33 2007
From: gilhamto at gmail.com (G Ilhamto)
Date: Fri, 14 Dec 2007 10:52:33 -0500
Subject: [R] train nnet
Message-ID: <bebd16360712140752l43a4ceb5xedb80f8bdc658f00@mail.gmail.com>

Hi R-helpers,

Can some one tell me how to train 'mynn' of this type?:
mynn <- nnet(y ~ x1 + ..+ x8, data = lgist, size = 2, rang = 0.1,
decay = 5e-4, maxit = 200)

I assume that this nn is untrained, and to train I have to split the
original data into train:test data set,
do leave-one-out refitting to refine the weights (please straighten
this up if I was wrong).

I just don't know how to do it in R. Is 'training' and
'training.reports' in (AMORE) able to do it?

Thank you for any light on this.

Ilh


From rolf.fankhauser at gepdata.ch  Fri Dec 14 18:06:52 2007
From: rolf.fankhauser at gepdata.ch (Rolf Fankhauser)
Date: Fri, 14 Dec 2007 17:06:52 +0000
Subject: [R] How to convert Datetime numbers from Excel to POSIXt objects
Message-ID: <4762B82C.9040708@gepdata.ch>

Hi all,

I need to compare time series data files of different time formats. I 
had no problems with text format using strptime.
But how can I convert datetime numbers from Excel (days since 30.12.1899 
00:00:00) into POSIXt objects?
For example 29770.375 should be converted to  "03.07.1981 09:00:00"

I tried the following code and encountered strange results:

t1-t0 gives 29770.33 (should be 29770.375 in my opinion)
t1-t2 and t1-t3 are ok
t1-t4 gives 183.3333 (should be 183.375)
Are these rounding errors?

The R-code:

t1 <- strptime("3.7.1981 09:00:00","%d.%m.%Y %H:%M:%S")
t0 <- strptime("30.12.1899 00:00:00","%d.%m.%Y %H:%M:%S")
t2 <- strptime("3.7.1981 00:00:00","%d.%m.%Y %H:%M:%S")
t3 <- strptime("1.7.1981 00:00:00","%d.%m.%Y %H:%M:%S")
t4 <- strptime("1.1.1981 00:00:00","%d.%m.%Y %H:%M:%S")
t1 - t0
t1 - t2
difftime(t1,t2,units="days")
t1 - t3
t1 - t4

Thanks for any help or clarifications

Rolf


From mxkuhn at gmail.com  Fri Dec 14 17:14:07 2007
From: mxkuhn at gmail.com (Max Kuhn)
Date: Fri, 14 Dec 2007 11:14:07 -0500
Subject: [R] train nnet
In-Reply-To: <bebd16360712140752l43a4ceb5xedb80f8bdc658f00@mail.gmail.com>
References: <bebd16360712140752l43a4ceb5xedb80f8bdc658f00@mail.gmail.com>
Message-ID: <6731304c0712140814l79111fcbt11bd9c97f368c04c@mail.gmail.com>

On Dec 14, 2007 10:52 AM, G Ilhamto <gilhamto at gmail.com> wrote:
> Hi R-helpers,
>
> Can some one tell me how to train 'mynn' of this type?:
> mynn <- nnet(y ~ x1 + ..+ x8, data = lgist, size = 2, rang = 0.1,
> decay = 5e-4, maxit = 200)
>

nnet will estimate model parameters for the model that you have specified.

If you want to understand what parameters (e.g. size, decay) should be
used, the train function in the caret package uses a variety of
resampling methods to help pick those parameters (and will refit the
model based on that). There are similar functions in other packages
(like e1071).

Install the caret package and use vignette("caretTrain") to see the details.

-- 

Max


From willy.wynant.chum at ssss.gouv.qc.ca  Fri Dec 14 17:20:32 2007
From: willy.wynant.chum at ssss.gouv.qc.ca (willy.wynant.chum at ssss.gouv.qc.ca)
Date: Fri, 14 Dec 2007 11:20:32 -0500
Subject: [R]  problem with coxph
Message-ID: <OF4B4F019C.C1152F24-ON852573B1.0059B8E9-852573B1.0059A820@ssss.gouv.qc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071214/31836afd/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Fri Dec 14 17:23:55 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 14 Dec 2007 17:23:55 +0100
Subject: [R] How to convert Datetime numbers from Excel to POSIXt objects
In-Reply-To: <4762B82C.9040708@gepdata.ch>
References: <4762B82C.9040708@gepdata.ch>
Message-ID: <4762AE1B.70604@biostat.ku.dk>

Rolf Fankhauser wrote:
> Hi all,
>
> I need to compare time series data files of different time formats. I 
> had no problems with text format using strptime.
> But how can I convert datetime numbers from Excel (days since 30.12.1899 
> 00:00:00) into POSIXt objects?
> For example 29770.375 should be converted to  "03.07.1981 09:00:00"
>
> I tried the following code and encountered strange results:
>
> t1-t0 gives 29770.33 (should be 29770.375 in my opinion)
> t1-t2 and t1-t3 are ok
> t1-t4 gives 183.3333 (should be 183.375)
> Are these rounding errors?
>   
> 1/(.375 - .333333)
[1] 23.99981

So your expectation is off by 1/24th of a day.

Can you think of something that might affect time differences by that
amount, depending on which times of the year you are comparing?

> The R-code:
>
> t1 <- strptime("3.7.1981 09:00:00","%d.%m.%Y %H:%M:%S")
> t0 <- strptime("30.12.1899 00:00:00","%d.%m.%Y %H:%M:%S")
> t2 <- strptime("3.7.1981 00:00:00","%d.%m.%Y %H:%M:%S")
> t3 <- strptime("1.7.1981 00:00:00","%d.%m.%Y %H:%M:%S")
> t4 <- strptime("1.1.1981 00:00:00","%d.%m.%Y %H:%M:%S")
> t1 - t0
> t1 - t2
> difftime(t1,t2,units="days")
> t1 - t3
> t1 - t4
>
> Thanks for any help or clarifications
>
> Rolf
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Greg.Snow at imail.org  Fri Dec 14 17:48:23 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Fri, 14 Dec 2007 09:48:23 -0700
Subject: [R] multiple ANOVAs
In-Reply-To: <003101c83dc5$b4c37860$1e4a6920$@edu>
References: <003101c83dc5$b4c37860$1e4a6920$@edu>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBDB67A1@LP-EXCHVS07.CO.IHC.COM>

Look at ?by and if that is not enough then look at the doBy package.  If
neither of those give you what you want, then give us more detail to
help you with.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Kevin J Emerson
> Sent: Thursday, December 13, 2007 1:21 PM
> To: r-help at r-project.org
> Subject: [R] multiple ANOVAs
> 
> Hello R help-ers,
> 
> 
> I have a basic question, but I have been playing with it for 
> a while and haven't quite gotten a hang of how to get it 
> working.  I want to perform multiple one-way ANOVAs on 
> subsets of data and am not sure how to do it in an automated 
> way.  I am thinking of doing something similar to 'aggregate'
> but I would like to collect all of the ANOVA results in a way 
> in which I can get to the information from each of the 
> independent ANOVAs.  In effect, I would like a data frame 
> with each of the subset identifiers, F and P values in the 
> rows.  Or a list of all of the ANOVA summaries or something like that.
> 
>  
> 
> I would like to do an anova of the form aov(var ~ day) for 
> each subset of data corresponding to "temp" and "line".  I 
> have lots of temps and lines and would like to do this in an 
> automated fashion.  I know of the aggregate function for 
> doing this kind of thing to compute means etc, but I am not 
> sure how to call an anova in this way.  I have looked at "by" 
> and "apply"
> and have tried various things but am unable to get it to work.  
> 
>  
> 
> Any help  would be greatly appreciated.  A sample of the 
> dataset is below.
> 
>  
> 
> Thank you for your time,
> 
> Kevin
> 
>  
> 
>  
> 
>    temp  line       day                 var 
> 
>      12 WMG 1        LD               70.59
> 
>      12 WMG 1        LD              100.00
> 
>      12 WMG 1        LD              100.00
> 
>      12 WMG 1        SD               85.00
> 
>      12 WMG 1        SD               75.00
> 
>      12 WMG 1        SD               90.00
> 
>      12 WMG 2        LD              100.00
> 
>      12 WMG 2        LD               83.33
> 
>      12 WMG 2        LD              100.00
> 
>      12 WMG 2        SD               91.67
> 
>      14 WMG 1        LD               76.03
> 
> .
> 
>  
> 
>    
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From David.Chosid at state.ma.us  Fri Dec 14 17:50:12 2007
From: David.Chosid at state.ma.us (Chosid, David (FWE))
Date: Fri, 14 Dec 2007 11:50:12 -0500
Subject: [R] Adjusting axis by groups within xyplot
Message-ID: <3CCC4D52A4CF6F4DA92F3F322D696D5E9F1F3F@ES-MSG-002.es.govt.state.ma.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071214/a2db30e5/attachment.pl 

From cberry at tajo.ucsd.edu  Fri Dec 14 17:59:27 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 14 Dec 2007 08:59:27 -0800
Subject: [R] problem with coxph
In-Reply-To: <OF4B4F019C.C1152F24-ON852573B1.0059B8E9-852573B1.0059A820@ssss.gouv.qc.ca>
References: <OF4B4F019C.C1152F24-ON852573B1.0059B8E9-852573B1.0059A820@ssss.gouv.qc.ca>
Message-ID: <Pine.LNX.4.64.0712140851100.12151@tajo.ucsd.edu>

On Fri, 14 Dec 2007, willy.wynant.chum at ssss.gouv.qc.ca wrote:

> Hi everyone,
>
> I encountered a problem using the coxph function for the conditional
> logistic regression. I am trying to do some simulations and I really don???t
> understand a mistake which happened maybe only 1 time among more than
> 1,000 simulations.
>
> What appeared on the screen is the following:
>
> Error in fitter(X,Y,strats,offset,init,control,weights=weights,:
> NA/NaN/Inf in a foreign function (arg 6)
> Warning message:
> Ran out of iterations and did not converge in:
> fiiter(X,Y,strats,offset,init,control,weights=weights,
>
> I don???t know if the warning message could have caused the error or vice
> versa.
>
> Does anyone have just an idea of what kind of error it can be? I don???t put

I suspect that the data set in that run has 'issues'.  These might include 
having a near dependency among the regressors - at least when weighting as 
to form the likelihood. Perhaps, (some of) the data are separable or 
nearly so.

To get at this I would wrap the call in 'try' or one of its relatives and 
save out the data when the error is triggered to see what gives.

Another possibility is to run interaactively and set

 	options( error=recover )

to try to take things apart.

HTH,

Chuck

> all the code because it is very long. But I guess that this error message
> may have an "answer" whatever the code is?
>
> Thanks so much in advance!
>
> Willy
>
> 	[[alternative HTML version deleted]]
>
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From ggrothendieck at gmail.com  Fri Dec 14 18:12:46 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 14 Dec 2007 12:12:46 -0500
Subject: [R] How to convert Datetime numbers from Excel to POSIXt objects
In-Reply-To: <4762B82C.9040708@gepdata.ch>
References: <4762B82C.9040708@gepdata.ch>
Message-ID: <971536df0712140912k6a897409u74f9ba5a8a11f785@mail.gmail.com>

Since you are getting the date times from Excel clearly you don't need
time zones, etc. so you can use chron.  See R News 4/1 for more.

> library(chron)
> dd <- c(t0 = "30.12.1899 00:00:00", t1 = "3.7.1981 09:00:00",
+  t2 = "3.7.1981 00:00:00", t3 = "1.7.1981 00:00:00", t4 = "1.1.1981 00:00:00")
>
> x <- chron(sub(" .*", "", dd), sub(".* ", "", dd), format = c("D.M.Y", "H:M:S"))
>
> diff(x)
Time in days:
    t1 t1     t2 t2     t3 t3     t4 t4
29770.375    -0.375    -2.000  -181.000


On Dec 14, 2007 12:06 PM, Rolf Fankhauser <rolf.fankhauser at gepdata.ch> wrote:
> Hi all,
>
> I need to compare time series data files of different time formats. I
> had no problems with text format using strptime.
> But how can I convert datetime numbers from Excel (days since 30.12.1899
> 00:00:00) into POSIXt objects?
> For example 29770.375 should be converted to  "03.07.1981 09:00:00"
>
> I tried the following code and encountered strange results:
>
> t1-t0 gives 29770.33 (should be 29770.375 in my opinion)
> t1-t2 and t1-t3 are ok
> t1-t4 gives 183.3333 (should be 183.375)
> Are these rounding errors?
>
> The R-code:
>
> t1 <- strptime("3.7.1981 09:00:00","%d.%m.%Y %H:%M:%S")
> t0 <- strptime("30.12.1899 00:00:00","%d.%m.%Y %H:%M:%S")
> t2 <- strptime("3.7.1981 00:00:00","%d.%m.%Y %H:%M:%S")
> t3 <- strptime("1.7.1981 00:00:00","%d.%m.%Y %H:%M:%S")
> t4 <- strptime("1.1.1981 00:00:00","%d.%m.%Y %H:%M:%S")
> t1 - t0
> t1 - t2
> difftime(t1,t2,units="days")
> t1 - t3
> t1 - t4
>
> Thanks for any help or clarifications
>
> Rolf
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From roland.rproject at gmail.com  Fri Dec 14 18:13:22 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Fri, 14 Dec 2007 12:13:22 -0500
Subject: [R] Result depends on previous result; easy with a loop;
 but without a loop?
Message-ID: <4762B9B2.80704@gmail.com>

Dear all,

I am pretty sure that this has been discussed before. Unfortunately, I 
can not find anything in the archives -- probably because I am 
"RSiteSearching" for the wrong terms. If I remember correctly, I think I 
even asked this question a few years ago. But I cannot even find this.

The basic problem is that a result depends on a previous result. This is 
easy with a loop--but how can I do this without a loop?

Lets give an example:

initial.matrix <- rbind(rep(1,4), matrix(0,ncol=4,nrow=5))
the.other.matrix <- matrix(runif(20), ncol=4, nrow=5)

the initial matrix should be filled according to the following 
(pseudo-code) rule:
if (row==1) initial.matrix[1,] <- 1
if (row>1) initial.matrix[x,] <- initial.matrix[x-1,] * 
the.other.matrix[x-1,]

as I said this is easy to do with a loop:
for (i in 2:(nrow(initial.matrix))) {
   initial.matrix[i,] <- initial.matrix[i-1,]*the.other.matrix[i-1,]
}
initial.matrix

But how can I do this without a loop?

Thank you already in advance!
Roland


From jluo.rhelp at gmail.com  Fri Dec 14 18:21:54 2007
From: jluo.rhelp at gmail.com (Jack Luo)
Date: Fri, 14 Dec 2007 12:21:54 -0500
Subject: [R] detailed calculation of two way anova with unbalanced design
Message-ID: <124ea520712140921k382e3cv432a588ce29529a6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071214/6da54a0d/attachment.pl 

From tlumley at u.washington.edu  Fri Dec 14 19:01:19 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 14 Dec 2007 10:01:19 -0800 (PST)
Subject: [R] termplot reference
In-Reply-To: <OF791A61BE.259B3B8D-ONC12573B1.0034A17A-C12573B1.0035C185@niva.no>
References: <OF791A61BE.259B3B8D-ONC12573B1.0034A17A-C12573B1.0035C185@niva.no>
Message-ID: <Pine.LNX.4.64.0712141000440.23614@homer24.u.washington.edu>

On Fri, 14 Dec 2007, robert.ptacnik at niva.no wrote:

> Dear Group,
> Is there a specific reference for how termplot partitionates residuals
> (part=T)? I want to use figures from termplot in a publication and wonder
> about an appropriate reference (nothing in the help-file)

References for types of residuals, including partial residuals, are in the 
help pages for residuals.glm and residuals.lm.

 	-thomas


From dusa.adrian at gmail.com  Fri Dec 14 19:55:15 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Fri, 14 Dec 2007 20:55:15 +0200
Subject: [R] connecting RMySQL to and external server
Message-ID: <200712142055.15148.dusa.adrian@gmail.com>


Dear list,

I learned how to connect R to a local MySQL server, using:
drv <- dbDriver("MySQL")
con <- dbConnect(drv, user="root", password="mypass", dbname="mydb")

Is it possible to connect R in this way to an external server (on a different 
machine, with a different IP)?

I read the documentation on ?dbConnect (and everything I could find on the 
internet), but I failed to find some other relevant arguments. For example, 
one needs to first connect to the external machine and only after that to the 
MySQL server on that machine. Is this possible from within R?

Thank you in advance,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From pgilbert at bank-banque-canada.ca  Fri Dec 14 20:29:36 2007
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 14 Dec 2007 14:29:36 -0500
Subject: [R] VARMA in R
In-Reply-To: <200712132131.lBDLVXIb021302@definetti.ddns.uark.edu>
References: <14322697.post@talk.nabble.com>
	<200712132131.lBDLVXIb021302@definetti.ddns.uark.edu>
Message-ID: <4762D9A0.1030709@bank-banque-canada.ca>



Giovanni Petris wrote:
> You may want to check package dlm and, possibly, dse. 
> 
Yes, you can also do this in dse, either in the ARMA specification or as 
an equivalent state-space model. There is an example in the Users' Guide 
distributed with the package.

Paul

> In dlm you can cast a VARMA model in state space form (dlmModARMA) and
> estimate unknown parameters by maximum likelihood (dlmMLE). 
> 
> 
> Best,
> Giovanni
> 
>> Date: Thu, 13 Dec 2007 11:17:47 -0800 (PST)
>> From: creepa1982 <Bannho at gmx.de>
>> Sender: r-help-bounces at r-project.org
>> Precedence: list
>>
>>
>> Hi all, 
>>
>> does anyone know of a package/function for fitting Vector Autoregressive
>> Moving Average models? I looked through most of the packages available but
>> could only find functions to fit a VAR. 
>>
>> Any help would be appreciated! 
>>
>> Benjamin
>> -- 
>> View this message in context: http://www.nabble.com/VARMA-in-R-tp14322697p14322697.html
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> 
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential in...{{dropped:26}}


From roland.rproject at gmail.com  Fri Dec 14 20:33:04 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Fri, 14 Dec 2007 14:33:04 -0500
Subject: [R] Result depends on previous result; easy with a loop;
 but without a loop?
In-Reply-To: <4762B9B2.80704@gmail.com>
References: <4762B9B2.80704@gmail.com>
Message-ID: <4762DA70.1020807@gmail.com>

Dear all,

in the meantime, I found a solution -- thank to a suggestion sent by 
Mark Leeds to me off-list.

All the best,
Roland

set.seed(1234)
initial.matrix <- rbind(rep(1,4), matrix(0,ncol=4,nrow=5))
the.other.matrix <- matrix(runif(20), ncol=4, nrow=5)
for (i in 2:(nrow(initial.matrix))) {
   initial.matrix[i,] <- initial.matrix[i-1,]*the.other.matrix[i-1,]
}
### that is how it should look like:
initial.matrix


### this is Mark's suggestion (if I understood it correctly)
initial.matrix2 <- rbind(rep(1,4), matrix(1,ncol=4,nrow=5))
initial.matrix2[-1,] <- sapply(1:ncol(initial.matrix2),
                                function(.col) {
cumprod(initial.matrix2[-(nrow(initial.matrix2)),.col]
                                          * the.other.matrix[,.col])
                                }
                                )
## and it works!!!
initial.matrix2
if (all(initial.matrix==initial.matrix2)) cat("Good\n") else cat("Bad\n")
## yes, I know, such comparisons of floats are notoriously problematic

Roland Rau wrote:
> Dear all,
> 
> I am pretty sure that this has been discussed before. Unfortunately, I 
> can not find anything in the archives -- probably because I am 
> "RSiteSearching" for the wrong terms. If I remember correctly, I think I 
> even asked this question a few years ago. But I cannot even find this.
> 
> The basic problem is that a result depends on a previous result. This is 
> easy with a loop--but how can I do this without a loop?
> 
> Lets give an example:
> 
> initial.matrix <- rbind(rep(1,4), matrix(0,ncol=4,nrow=5))
> the.other.matrix <- matrix(runif(20), ncol=4, nrow=5)
> 
> the initial matrix should be filled according to the following 
> (pseudo-code) rule:
> if (row==1) initial.matrix[1,] <- 1
> if (row>1) initial.matrix[x,] <- initial.matrix[x-1,] * 
> the.other.matrix[x-1,]
> 
> as I said this is easy to do with a loop:
> for (i in 2:(nrow(initial.matrix))) {
>    initial.matrix[i,] <- initial.matrix[i-1,]*the.other.matrix[i-1,]
> }
> initial.matrix
> 
> But how can I do this without a loop?
> 
> Thank you already in advance!
> Roland
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cpaulsen at u.washington.edu  Fri Dec 14 20:35:48 2007
From: cpaulsen at u.washington.edu (Caroline Paulsen)
Date: Fri, 14 Dec 2007 11:35:48 -0800
Subject: [R] flagging glm models with warnings
Message-ID: <002e01c83e88$8947af30$780aa8c0@cpaulsen>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071214/2b267f68/attachment.pl 

From rvaradhan at jhmi.edu  Fri Dec 14 20:56:16 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 14 Dec 2007 14:56:16 -0500
Subject: [R] Result depends on previous result; easy with a loop;
	but without a loop?
In-Reply-To: <4762DA70.1020807@gmail.com>
References: <4762B9B2.80704@gmail.com> <4762DA70.1020807@gmail.com>
Message-ID: <000d01c83e8b$6360be80$7c94100a@win.ad.jhu.edu>

Roland,

You can test for the "sameness" of floating-point results as follows:

all.equal(initial.matrix, initial.matrix2)

By default, it uses a tolerance = .Machine$double.eps ^ 0.5 (roughly,
1.e-08).  You can decrease this if you want a more stringent test for
sameness.

all.identical(initial.matrix, initial.matrix2) 

This tests for "identicality", which, of course, is not appropriate for
floating point computations.

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Roland Rau
Sent: Friday, December 14, 2007 2:33 PM
To: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Result depends on previous result; easy with a loop; but
without a loop?

Dear all,

in the meantime, I found a solution -- thank to a suggestion sent by 
Mark Leeds to me off-list.

All the best,
Roland

set.seed(1234)
initial.matrix <- rbind(rep(1,4), matrix(0,ncol=4,nrow=5))
the.other.matrix <- matrix(runif(20), ncol=4, nrow=5)
for (i in 2:(nrow(initial.matrix))) {
   initial.matrix[i,] <- initial.matrix[i-1,]*the.other.matrix[i-1,]
}
### that is how it should look like:
initial.matrix


### this is Mark's suggestion (if I understood it correctly)
initial.matrix2 <- rbind(rep(1,4), matrix(1,ncol=4,nrow=5))
initial.matrix2[-1,] <- sapply(1:ncol(initial.matrix2),
                                function(.col) {
cumprod(initial.matrix2[-(nrow(initial.matrix2)),.col]
                                          * the.other.matrix[,.col])
                                }
                                )
## and it works!!!
initial.matrix2
if (all(initial.matrix==initial.matrix2)) cat("Good\n") else cat("Bad\n")
## yes, I know, such comparisons of floats are notoriously problematic

Roland Rau wrote:
> Dear all,
> 
> I am pretty sure that this has been discussed before. Unfortunately, I 
> can not find anything in the archives -- probably because I am 
> "RSiteSearching" for the wrong terms. If I remember correctly, I think I 
> even asked this question a few years ago. But I cannot even find this.
> 
> The basic problem is that a result depends on a previous result. This is 
> easy with a loop--but how can I do this without a loop?
> 
> Lets give an example:
> 
> initial.matrix <- rbind(rep(1,4), matrix(0,ncol=4,nrow=5))
> the.other.matrix <- matrix(runif(20), ncol=4, nrow=5)
> 
> the initial matrix should be filled according to the following 
> (pseudo-code) rule:
> if (row==1) initial.matrix[1,] <- 1
> if (row>1) initial.matrix[x,] <- initial.matrix[x-1,] * 
> the.other.matrix[x-1,]
> 
> as I said this is easy to do with a loop:
> for (i in 2:(nrow(initial.matrix))) {
>    initial.matrix[i,] <- initial.matrix[i-1,]*the.other.matrix[i-1,]
> }
> initial.matrix
> 
> But how can I do this without a loop?
> 
> Thank you already in advance!
> Roland
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From stenka1 at go.com  Fri Dec 14 20:58:48 2007
From: stenka1 at go.com (stephen bond)
Date: Fri, 14 Dec 2007 19:58:48 +0000 (UTC)
Subject: [R] rcom close Excel problem
Message-ID: <2983390.1197662328437.JavaMail.?@fh128.dia.he.tucows.com>

Hello,

I just discovered that I cannot close the Excel application and task
manager shows numerous copies of Excel.exe

I tried both

x$Quit() # shown in the rcom archive

and

x$Exit()

and Excel refuses to die.
Thank you very much.
S.

"You can't kill me, I will not die" Mojo Nixon


I also have a problem with saving. It produces a pop-up dialog and 
does
not take my second parameter:

x<-comCreateObject("Excel.Application")
wb<-comInvoke(comGetProperty(x,"Workbooks"),"Open","G:
/MR/Stephen/repo.
xls", "0")
sh<-comGetProperty(wb,"Worksheets","Market Data")
range1 <- comGetProperty(sh,"Range","C10","I11")
vals <- comGetProperty(range1,"Value")
comInvoke(wb,"Close","G:/MR/Stephen/repo.xls","True") # True is 
ignored

Thank you All.
Stephen


From fjbuch at gmail.com  Fri Dec 14 21:04:22 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Fri, 14 Dec 2007 15:04:22 -0500
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <AEBB91A7-0F3A-476D-AF3B-67352630661A@indiana.edu>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<AC9C43A9-C918-42D7-A397-FC77E43AC3BC@indiana.edu>
	<bd93cdad0712131816t3dca1dfftbea82a5257ea9c17@mail.gmail.com>
	<AEBB91A7-0F3A-476D-AF3B-67352630661A@indiana.edu>
Message-ID: <bd93cdad0712141204j5cf2e10axf8a47f337ae7002c@mail.gmail.com>

> The problem is that the RSS feed you linked to, does not contain the
> year of the article in an easily accessible XML element. Rather you
> have to process the HTML content of the description element - which,
> is something R could do, but you'd be using the wrong tool for the job.
>

Yes. I have noticed that there two sorts of xml that pubmed will
provide. The kind I had hooked into was an rss feed which provides a
lot of the information simply as a formatted table for viewing in a
rss reader. There is another way to get the xml to come out with more
tags. However, I found the best way to do this is probably through the
bioconductor annotate package

x <- pubmed("18046565", "17978930", "17975511")
a <- xmlRoot(x)
numAbst <- length(xmlChildren(a))
absts <- list()
for (i in 1:numAbst) {
absts[[i]] <- buildPubMedAbst(a[[i]])
   }

I am now trying to work through that approach to see what I can come up with.
-- 
Farrel Buchinsky


From fjbuch at gmail.com  Fri Dec 14 21:16:59 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Fri, 14 Dec 2007 15:16:59 -0500
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <4762080F.8070606@fhcrc.org>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
Message-ID: <bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>

On Dec 13, 2007 11:35 PM, Robert Gentleman <rgentlem at fhcrc.org> wrote:
> or just try looking in the annotate package from Bioconductor
>

Yip. annotate seems to be the most streamlined way to do this.
1) How does one turn the list that is created into a dataframe whose
column names are along the lines of date, title, journal, authors etc
2) I have already created a standing search in pubmed using MyNCBI.
There are many ways I can feed those results to the pubmed() function.
The most brute force way of doing it is by running the search and
outputing the data as a UI List and getting that into the pubmed
brackets. A way that involved more finesse would allow me to create a
rss feed based on my search and then give the rss feed url to the
pubmed function. Or perhaps once could just plop the query inside the
pubmed functions
pubmed(somefunction("Laryngeal Neoplasms"[MeSH] AND "Papilloma"[MeSH])
OR ((("recurrence"[TIAB] NOT Medline[SB]) OR "recurrence"[MeSH Terms]
OR recurrent[Text Word]) AND respiratory[All Fields] AND
(("papilloma"[TIAB] NOT Medline[SB]) OR "papilloma"[MeSH Terms] OR
papillomatosis[Text Word])))

Does "somefunction" exist?

If there are any further questions do you think I should migrate this
conversation to the bioconductor mailing list?



Farrel Buchinsky


From bates at stat.wisc.edu  Fri Dec 14 21:27:18 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 14 Dec 2007 14:27:18 -0600
Subject: [R] convergence error code in mixed effects models
In-Reply-To: <83439.66132.qm@web26213.mail.ukl.yahoo.com>
References: <40e66e0b0712131426h6e702ed2pa45b5786c7b29d36@mail.gmail.com>
	<83439.66132.qm@web26213.mail.ukl.yahoo.com>
Message-ID: <40e66e0b0712141227s6ba38a72iebdb8fcc8c720a43@mail.gmail.com>

Thank you for sending the data.  It is very helpful in understanding
the nature of the problem.

For example, in your original description of your study you referred
to week as a factor, which is a completely reasonable term, but I
mistakenly thought that you meant an object of class "factor" and that
was why I replied that you would be estimating too many variances and
covariances.

I can tell you why you are having problems fitting a mixed-effects
model.  Strangely it is because there is too little variability in the
patterns across the replicates, especially at the early times.  The
leaf number is discrete with a small range (all the leaffra
observations in this example are 4, 5, 6, 7 or 8) and non-decreasing
over time.  (I assume the nature of the experiment is such that the
leaf number is necessarily non-decreasing.)  That doesn't allow for
many patterns.  I'm sure some clever person reading this will be able
to tell us exactly how many different such patterns you could get but
I will simply say "not many".

Notice that the first 10 lines show that the leaffra is 4 at week 4 in
*every* replicate.  There isn't a whole lot of variation here for the
random effects to model.

The best place to start is with a plot of the data.  I changed the
levels of the rep factor to "f1"-"f5" and "s1"-"s5" to indicate that
each rep is at one level of the treat. (Those who are playing along at
home should be careful of the ordering of the original levels because
ID10, which I now call s5, occurs between ID1 and ID2.)

With this set of labels the cross-tabulation and treat should be

> xtabs(~ rep + treat, leaf)
    treat
rep  pHf pHs
  f1   4   0
  f2   4   0
  f3   4   0
  f4   4   0
  f5   4   0
  s1   0   4
  s2   0   4
  s3   0   4
  s4   0   4
  s5   0   4

Now look at lattice plots such as

library(lattice)
xyplot(heightfra ~ week | rep, leaf, type = c("g", "p", "r"), layout =
c(5,2), aspect = 'xy', groups = treat)

(I enclose PDF files of these plots for each of the three responses.)
First you can see that there is very little variation at the low end.
Strangely enough, this causes a problem in fitting mixed-effects
models because the mle's of the variances of  the random effects for
the intercept will be zero.  The lme function does not handle this
gracefully.  The lmer function from the lme4 package does a better job
on this type of model.

Also, note that the pattern of heightfra over time is not linear.  It
is consistently concave down.  Thuso a mixed-effects model that is
linear in week will miss much of the structure in the data.

The point of R is to encourage you to explore your data rather than
subjecting it to a "canned" analysis.  You could try fitting a
mixed-effects model to these data in SAS PROC MIXED or SPSS MIXED and
I have no doubt that those packages would give you estimates (not to
mention p-values, something that the author of lmer has been woefully
negligent in not providing :-) but you probably won't get much of a
hint that the model doesn't make sense.  I would prefer to start with
the plot and see what the data have to say.


The technical problem with convergence in lme is that the mle of the
variance of the intercept term is zero.  You can see that if you use
lmer from the lme4 package instead to fit the model.
the random effect for the intercept is estimate
On Dec 14, 2007 4:40 AM, Ilona Leyer <ileyer at yahoo.de> wrote:
>
> Here an simple example:
>
> rep     treat   heightfra       leaffra leafvim week
> ID1     pHf     1.54    4       4       4
> ID2     pHf     1.49    4       4       4
> ID3     pHf     1.57    4       5       4
> ID4     pHf     1.48    4       4       4
> ID5     pHf     1.57    4       4       4
> ID6     pHs     1.29    4       5       4
> ID7     pHs     0.97    4       5       4
> ID8     pHs     2.06    4       4       4
> ID9     pHs     0.88    4       4       4
> ID10    pHs     1.47    4       4       4
> ID1     pHf     3.53    5       6       6
> ID2     pHf     4.08    6       6       6
> ID3     pHf     3.89    6       6       6
> ID4     pHf     3.78    5       6       6
> ID5     pHf     3.92    6       6       6
> ID6     pHs     2.76    5       5       6
> ID7     pHs     3.31    6       7       6
> ID8     pHs     4.46    6       7       6
> ID9     pHs     2.19    5       5       6
> ID10    pHs     3.83    5       5       6
> ID1     pHf     5.07    7       7       9
> ID2     pHf     6.42    7       8       9
> ID3     pHf     5.43    6       8       9
> ID4     pHf     6.83    6       8       9
> ID5     pHf     6.26    6       8       9
> ID6     pHs     4.57    6       9       9
> ID7     pHs     5.05    6       7       9
> ID8     pHs     6.27    6       8       9
> ID9     pHs     3.37    5       7       9
> ID10    pHs     5.38    6       8       9
> ID1     pHf     5.58    7       9       12
> ID2     pHf     7.43    8       9       12
> ID3     pHf     6.18    8       10      12
> ID4     pHf     6.91    7       10      12
> ID5     pHf     6.78    7       10      12
> ID6     pHs     4.99    6       13      12
> ID7     pHs     5.50    7       8       12
> ID8     pHs     6.56    7       10      12
> ID9     pHs     3.72    6       10      12
> ID10    pHs     5.94    6       10      12
>
>
> I used the procedure described in Crawley?s new R
> Book.
> For two of the tree response variables
> (heightfra,leaffra) it doesn?t work, while it worked
> with leafvim (but in another R session, yesterday,
> leaffra worked as well...).
>
> Here the commands:
>
> > attach(test)
> > names(test)
> [1] "week"      "rep"       "treat"     "heightfra"
> "leaffra"   "leafvim"
> > library(nlme)
> >
> test<-groupedData(heightfra~week|rep,outer=~treat,test)
> > model1<-lme(heightfra~treat,random=~week|rep)
> Error in lme.formula(heightfra ~ treat, random = ~week
> | rep) :
>         nlminb problem, convergence error code = 1;
> message = iteration limit reached without convergence
> (9)
>
> >
> test<-groupedData(leaffra~week|rep,outer=~treat,test)
> > model2<-lme(leaffra~treat,random=~week|rep)
> Error in lme.formula(leaffra ~ treat, random = ~week |
> rep) :
>         nlminb problem, convergence error code = 1;
> message = iteration limit reached without convergence
> (9)
>
> >
> test<-groupedData(leafvim~week|rep,outer=~treat,test)
> > model3<-lme(leafvim~treat,random=~week|rep)
> > summary(model)
> Error in summary(model) : object "model" not found
> > summary(model3)
> Linear mixed-effects model fit by REML
>  Data: NULL
>        AIC      BIC    logLik
>   129.6743 139.4999 -58.83717
>
> Random effects:
>  Formula: ~week | rep
>  Structure: General positive-definite, Log-Cholesky
> parametrization
>             StdDev    Corr
> (Intercept) 4.4110478 (Intr)
> week        0.7057311 -0.999
> Residual    0.5976143
>
> Fixed effects: leafvim ~ treat
>                Value Std.Error DF  t-value p-value
> (Intercept) 5.924659 0.1653596 30 35.82893  0.0000
> treatpHs    0.063704 0.2338538  8  0.27241  0.7922
>  Correlation:
>          (Intr)
> treatpHs -0.707
>
> Standardized Within-Group Residuals:
>         Min          Q1         Med          Q3
>  Max
> -1.34714254 -0.53042878 -0.01769195  0.40644540
> 2.29301560
>
> Number of Observations: 40
> Number of Groups: 10
>
> Is there a solution for this problem?
>
> Thanks!!!
>
> Ilona
>
> --- Douglas Bates <bates at stat.wisc.edu> schrieb:
>
>
> > On Dec 13, 2007 4:15 PM, Ilona Leyer
> > <ileyer at yahoo.de> wrote:
> > > Dear All,
> > > I want to analyse treatment effects with time
> > series
> > > data:  I measured e.g. leaf number (five replicate
> > > plants) in relation to two soil pH - after 2,4,6,8
> > > weeks. I used mixed effects models, but some
> > analyses
> > > didn?t work. It seems for me as if this is a
> > randomly
> > > occurring problem since sometimes the same model
> > works
> > > sometimes not.
> > >
> > > An example:
> > > > names(test)
> > > [1] "rep"    "treat"  "leaf"   "week"
> > > > library (lattice)
> > > > library (nlme)
> > > >
> > test<-groupedData(leaf~week|rep,outer=~treat,test)
> > > > model<-lme(leaf~treat,random=~leaf|rep)
> > > Error in lme.formula(leaf~ treat, random =
> > ~week|rep)
> >
> > Really!? You gave lme a model with random = ~ leaf |
> > rep (and no data
> > specification) and it tried to fit a model with
> > random = ~ week | rep?
> > Are you sure that is an exact transcript?
> >
> > > :
> > >         nlminb problem, convergence error code =
> > 1;
> > > message = iteration limit reached without
> > convergence
> > > (9)
> >
> > > Has anybody an idea to solve this problem?
> >
> > Oh, I have lots of ideas but without a reproducible
> > example I can't
> > hope to decide what might be the problem.
> >
> > It appears that the model may be over-parameterized.
> >  Assuming that
> > there are 4 different values of week then ~ week |
> > rep requires
> > fitting 10 variance-covariance parameters. That's a
> > lot.
> > The error code indicates that the optimizer is
> > taking
> >
>
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: hf.pdf
Type: application/pdf
Size: 22554 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20071214/e8cff557/attachment.pdf 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: lf.pdf
Type: application/pdf
Size: 23715 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20071214/e8cff557/attachment-0001.pdf 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: lv.pdf
Type: application/pdf
Size: 23776 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20071214/e8cff557/attachment-0002.pdf 

From ggrothendieck at gmail.com  Fri Dec 14 21:36:17 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 14 Dec 2007 15:36:17 -0500
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <bd93cdad0712141204j5cf2e10axf8a47f337ae7002c@mail.gmail.com>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<AC9C43A9-C918-42D7-A397-FC77E43AC3BC@indiana.edu>
	<bd93cdad0712131816t3dca1dfftbea82a5257ea9c17@mail.gmail.com>
	<AEBB91A7-0F3A-476D-AF3B-67352630661A@indiana.edu>
	<bd93cdad0712141204j5cf2e10axf8a47f337ae7002c@mail.gmail.com>
Message-ID: <971536df0712141236u450d977fj82cbdb6040de66c4@mail.gmail.com>

On Dec 14, 2007 3:04 PM, Farrel Buchinsky <fjbuch at gmail.com> wrote:
> > The problem is that the RSS feed you linked to, does not contain the
> > year of the article in an easily accessible XML element. Rather you
> > have to process the HTML content of the description element - which,
> > is something R could do, but you'd be using the wrong tool for the job.
> >
>
> Yes. I have noticed that there two sorts of xml that pubmed will
> provide. The kind I had hooked into was an rss feed which provides a
> lot of the information simply as a formatted table for viewing in a
> rss reader. There is another way to get the xml to come out with more
> tags. However, I found the best way to do this is probably through the
> bioconductor annotate package
>
> x <- pubmed("18046565", "17978930", "17975511")
> a <- xmlRoot(x)
> numAbst <- length(xmlChildren(a))
> absts <- list()
> for (i in 1:numAbst) {
> absts[[i]] <- buildPubMedAbst(a[[i]])
>   }
>
> I am now trying to work through that approach to see what I can come up with.

Note that the lines after a<-xmlRoot(x) could be reduced to:

xmlSApply(a, buildPubMedAbst)


From dieter.menne at menne-biomed.de  Fri Dec 14 21:49:20 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 14 Dec 2007 20:49:20 +0000 (UTC)
Subject: [R] flagging glm models with warnings
References: <002e01c83e88$8947af30$780aa8c0@cpaulsen>
Message-ID: <loom.20071214T204701-394@post.gmane.org>

Caroline Paulsen <cpaulsen <at> u.washington.edu> writes:

> 
> I'm attempting to run 250 permutations of a negative binomial GLM model for
> data on fish counts. Many of the models are fit appropriately, but some
> issue warnings such as "convergence not reached" or "step size truncated due
> to divergence." 
....
> I'd like to figure out a way to flag the models that have warnings and
> output them into either a table or into R console. Then I could evaluate the
> problems associated with these models and know not choose them as the best
> models for fitting the data.

You could promote the options to errors (?warning) and use try()

Dieter


From jmburgos at u.washington.edu  Fri Dec 14 22:20:04 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Fri, 14 Dec 2007 13:20:04 -0800
Subject: [R] detailed calculation of two way anova with unbalanced design
In-Reply-To: <124ea520712140921k382e3cv432a588ce29529a6@mail.gmail.com>
References: <124ea520712140921k382e3cv432a588ce29529a6@mail.gmail.com>
Message-ID: <4762F384.7060103@u.washington.edu>

Hi Jack,

Any intro stats book should have them.  See for example chapter 11 in 
Sokal and Rohlf (2nd ed., 1981).

Julian

Jack Luo wrote:
> Dear list,
>
> Could someone show me where can I find the detailed formula on how to
> calculate the two way anova with unbalanced design? Say, if I have 2*2
> design with 10,20,30,40 samples in each of the 2*2 cells. Most of the places
> I've googled only show how to calculate using software such as R, but not
> clear the detailed formula for calculating this.
>
> Thanks,
>
> Jack
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   

-- 
Julian M. Burgos

Fisheries Acoustics Research Lab
School of Aquatic and Fishery Science
University of Washington

1122 NE Boat Street
Seattle, WA  98105 

Phone: 206-221-6864


From duncan at wald.ucdavis.edu  Fri Dec 14 23:07:21 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Sat, 15 Dec 2007 11:07:21 +1300
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <bd93cdad0712141204j5cf2e10axf8a47f337ae7002c@mail.gmail.com>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>	<AC9C43A9-C918-42D7-A397-FC77E43AC3BC@indiana.edu>	<bd93cdad0712131816t3dca1dfftbea82a5257ea9c17@mail.gmail.com>	<AEBB91A7-0F3A-476D-AF3B-67352630661A@indiana.edu>
	<bd93cdad0712141204j5cf2e10axf8a47f337ae7002c@mail.gmail.com>
Message-ID: <4762FE99.3090501@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



Farrel Buchinsky wrote:
>> The problem is that the RSS feed you linked to, does not contain the
>> year of the article in an easily accessible XML element. Rather you
>> have to process the HTML content of the description element - which,
>> is something R could do, but you'd be using the wrong tool for the job.
>>
> 
> Yes. I have noticed that there two sorts of xml that pubmed will
> provide. The kind I had hooked into was an rss feed which provides a
> lot of the information simply as a formatted table for viewing in a
> rss reader. There is another way to get the xml to come out with more
> tags. However, I found the best way to do this is probably through the
> bioconductor annotate package
> 
> x <- pubmed("18046565", "17978930", "17975511")
> a <- xmlRoot(x)
> numAbst <- length(xmlChildren(a))
> absts <- list()
> for (i in 1:numAbst) {
> absts[[i]] <- buildPubMedAbst(a[[i]])
>    }

You can simplify the final 5 lines to

   absts = xmlApply(a, buildPubMedAbst)

which is shorter, fractionally faster and handles cases where there are
no abstracts.


> 
> I am now trying to work through that approach to see what I can come up with.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFHYv6Z9p/Jzwa2QP4RAp0NAJ4pfGS7Jy9nwHMOGpT1jVM+IMedywCeOZPG
9GER8GI62Y24a+cQT7KbW08=
=4TVP
-----END PGP SIGNATURE-----


From thomas.pujol at yahoo.com  Fri Dec 14 23:10:52 2007
From: thomas.pujol at yahoo.com (Thomas Pujol)
Date: Fri, 14 Dec 2007 14:10:52 -0800 (PST)
Subject: [R] way to check if the evaluation of an expression returns an
	error?
Message-ID: <178702.47894.qm@web59316.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071214/25c6ee8f/attachment.pl 

From A.Robinson at ms.unimelb.edu.au  Fri Dec 14 23:33:36 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sat, 15 Dec 2007 09:33:36 +1100
Subject: [R] way to check if the evaluation of an expression returns an
	error?
In-Reply-To: <178702.47894.qm@web59316.mail.re1.yahoo.com>
References: <178702.47894.qm@web59316.mail.re1.yahoo.com>
Message-ID: <20071214223336.GO93868@ms.unimelb.edu.au>

?try

Cheers,

Andrew

On Fri, Dec 14, 2007 at 02:10:52PM -0800, Thomas Pujol wrote:
> Is there a recommended or "good" way to check if the evaluation of an expression returns an error?
>    
>   e.g. 
> var(NA)    
>   I wish  var(NA) to return NA or "err", or some other value, even a text-string, but not an error message.
>    
>    
>   I am using a loop to load many samples of data and to perform analysis on each sample.  On occasion, the function I execute returns an error message.  (e.g. when all data is NA, etc).   I wish for the loop to continue to execute, and to have the function return an NA or NULL rather then an error message.
>    
> 
>        
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> This message has been scanned for viruses and
> dangerous content by MailScanner, and is
> believed to be clean.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From Dan.Kelley at dal.ca  Fri Dec 14 23:51:45 2007
From: Dan.Kelley at dal.ca (dankelley)
Date: Fri, 14 Dec 2007 14:51:45 -0800 (PST)
Subject: [R]  how to add an index to a vignette?
Message-ID: <14339838.post@talk.nabble.com>


Is it possible to include index items in vignettes?  I tried doing the usual
latex thing with 

\index{the item} 

but but the vignette created by 

R CMD build  

has no index.  Do I need to do something more? 

-- 
View this message in context: http://www.nabble.com/how-to-add-an-index-to-a-vignette--tp14339838p14339838.html
Sent from the R help mailing list archive at Nabble.com.


From cskiadas at gmail.com  Sat Dec 15 03:57:04 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Fri, 14 Dec 2007 21:57:04 -0500
Subject: [R] rcom close Excel problem
In-Reply-To: <2983390.1197662328437.JavaMail.?@fh128.dia.he.tucows.com>
References: <2983390.1197662328437.JavaMail.?@fh128.dia.he.tucows.com>
Message-ID: <7D66C553-F2E0-4D47-BB08-B631D26C9821@gmail.com>

I know it won't answer your question exactly, but using comGetObject  
instead of comCreateObject won't create new Excel instances, so at  
least you won't have more than one processes running, so this might  
solve some of your problems.

As for your second problem, I would venture to guess you need your  
paths with double backslashes instead of slashes. The following just  
worked over here:

  wb<-comInvoke(comGetProperty(obj,"Workbooks"),"Open", "C:\ 
\Documents and Settings\\Haris\\Desktop\\test1.xlsx")

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


On Dec 14, 2007, at 2:58 PM, stephen bond wrote:

> Hello,
>
> I just discovered that I cannot close the Excel application and task
> manager shows numerous copies of Excel.exe
>
> I tried both
>
> x$Quit() # shown in the rcom archive
>
> and
>
> x$Exit()
>
> and Excel refuses to die.
> Thank you very much.
> S.
>
> "You can't kill me, I will not die" Mojo Nixon
>
>
> I also have a problem with saving. It produces a pop-up dialog and
> does
> not take my second parameter:
>
> x<-comCreateObject("Excel.Application")
> wb<-comInvoke(comGetProperty(x,"Workbooks"),"Open","G:
> /MR/Stephen/repo.
> xls", "0")
> sh<-comGetProperty(wb,"Worksheets","Market Data")
> range1 <- comGetProperty(sh,"Range","C10","I11")
> vals <- comGetProperty(range1,"Value")
> comInvoke(wb,"Close","G:/MR/Stephen/repo.xls","True") # True is
> ignored
>
> Thank you All.
> Stephen


From mark_difford at yahoo.co.uk  Sat Dec 15 04:26:21 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Fri, 14 Dec 2007 19:26:21 -0800 (PST)
Subject: [R] convergence error code in mixed effects models
In-Reply-To: <83439.66132.qm@web26213.mail.ukl.yahoo.com>
References: <580233.74151.qm@web26210.mail.ukl.yahoo.com>
	<40e66e0b0712131426h6e702ed2pa45b5786c7b29d36@mail.gmail.com>
	<83439.66132.qm@web26213.mail.ukl.yahoo.com>
Message-ID: <14340592.post@talk.nabble.com>


Hi Ilona,

>> Is there a solution for this problem?

If there is, then Professor Bates (the gentleman who replied to your
question) will have tried to find it, and fix it, for you.

Professor Bates wrote/co-wrote the software package (nlme) you are using. 
And while I have nothing against Crawley's book, you are usually much better
off going to primary sources first, to solve this kind of problem (which, of
course you have done, though may not have been aware of it ;)

Mixed-Effects Models in S and S-PLUS, by: Pinheiro, Jos?, Bates, Douglas
http://www.springer.com/west/home/statistics/computational?SGWID=4-10130-22-2102822-0

Hope this speeds you on your way...

Regards, Mark.


Ilona Leyer wrote:
> 
> 
> Here an simple example:
> 
> rep	treat	heightfra	leaffra	leafvim	week
> ID1	pHf	1.54	4	4	4
> ID2	pHf	1.49	4	4	4
> ID3	pHf	1.57	4	5	4
> ID4	pHf	1.48	4	4	4
> ID5	pHf	1.57	4	4	4
> ID6	pHs	1.29	4	5	4
> ID7	pHs	0.97	4	5	4
> ID8	pHs	2.06	4	4	4
> ID9	pHs	0.88	4	4	4
> ID10	pHs	1.47	4	4	4
> ID1	pHf	3.53	5	6	6
> ID2	pHf	4.08	6	6	6
> ID3	pHf	3.89	6	6	6
> ID4	pHf	3.78	5	6	6
> ID5	pHf	3.92	6	6	6
> ID6	pHs	2.76	5	5	6
> ID7	pHs	3.31	6	7	6
> ID8	pHs	4.46	6	7	6
> ID9	pHs	2.19	5	5	6
> ID10	pHs	3.83	5	5	6
> ID1	pHf	5.07	7	7	9
> ID2	pHf	6.42	7	8	9
> ID3	pHf	5.43	6	8	9
> ID4	pHf	6.83	6	8	9
> ID5	pHf	6.26	6	8	9
> ID6	pHs	4.57	6	9	9
> ID7	pHs	5.05	6	7	9
> ID8	pHs	6.27	6	8	9
> ID9	pHs	3.37	5	7	9
> ID10	pHs	5.38	6	8	9
> ID1	pHf	5.58	7	9	12
> ID2	pHf	7.43	8	9	12
> ID3	pHf	6.18	8	10	12
> ID4	pHf	6.91	7	10	12
> ID5	pHf	6.78	7	10	12
> ID6	pHs	4.99	6	13	12
> ID7	pHs	5.50	7	8	12
> ID8	pHs	6.56	7	10	12
> ID9	pHs	3.72	6	10	12
> ID10	pHs	5.94	6	10	12
> 
> 
> I used the procedure described in Crawley?s new R
> Book.
> For two of the tree response variables
> (heightfra,leaffra) it doesn?t work, while it worked
> with leafvim (but in another R session, yesterday,
> leaffra worked as well...).
> 
> Here the commands:
> 
>> attach(test)
>> names(test)
> [1] "week"      "rep"       "treat"     "heightfra"
> "leaffra"   "leafvim"  
>> library(nlme)
>>
> test<-groupedData(heightfra~week|rep,outer=~treat,test)
>> model1<-lme(heightfra~treat,random=~week|rep)
> Error in lme.formula(heightfra ~ treat, random = ~week
> | rep) : 
>         nlminb problem, convergence error code = 1;
> message = iteration limit reached without convergence
> (9)
> 
>>
> test<-groupedData(leaffra~week|rep,outer=~treat,test)
>> model2<-lme(leaffra~treat,random=~week|rep)
> Error in lme.formula(leaffra ~ treat, random = ~week |
> rep) : 
>         nlminb problem, convergence error code = 1;
> message = iteration limit reached without convergence
> (9)
> 
>>
> test<-groupedData(leafvim~week|rep,outer=~treat,test)
>> model3<-lme(leafvim~treat,random=~week|rep)
>> summary(model)
> Error in summary(model) : object "model" not found
>> summary(model3)
> Linear mixed-effects model fit by REML
>  Data: NULL 
>        AIC      BIC    logLik
>   129.6743 139.4999 -58.83717
> 
> Random effects:
>  Formula: ~week | rep
>  Structure: General positive-definite, Log-Cholesky
> parametrization
>             StdDev    Corr  
> (Intercept) 4.4110478 (Intr)
> week        0.7057311 -0.999
> Residual    0.5976143       
> 
> Fixed effects: leafvim ~ treat 
>                Value Std.Error DF  t-value p-value
> (Intercept) 5.924659 0.1653596 30 35.82893  0.0000
> treatpHs    0.063704 0.2338538  8  0.27241  0.7922
>  Correlation: 
>          (Intr)
> treatpHs -0.707
> 
> Standardized Within-Group Residuals:
>         Min          Q1         Med          Q3       
>  Max 
> -1.34714254 -0.53042878 -0.01769195  0.40644540 
> 2.29301560 
> 
> Number of Observations: 40
> Number of Groups: 10 
> 
> Is there a solution for this problem?
> 
> Thanks!!!
> 
> Ilona
> 
> --- Douglas Bates <bates at stat.wisc.edu> schrieb:
> 
>> On Dec 13, 2007 4:15 PM, Ilona Leyer
>> <ileyer at yahoo.de> wrote:
>> > Dear All,
>> > I want to analyse treatment effects with time
>> series
>> > data:  I measured e.g. leaf number (five replicate
>> > plants) in relation to two soil pH - after 2,4,6,8
>> > weeks. I used mixed effects models, but some
>> analyses
>> > didn?t work. It seems for me as if this is a
>> randomly
>> > occurring problem since sometimes the same model
>> works
>> > sometimes not.
>> >
>> > An example:
>> > > names(test)
>> > [1] "rep"    "treat"  "leaf"   "week"
>> > > library (lattice)
>> > > library (nlme)
>> > >
>> test<-groupedData(leaf~week|rep,outer=~treat,test)
>> > > model<-lme(leaf~treat,random=~leaf|rep)
>> > Error in lme.formula(leaf~ treat, random =
>> ~week|rep)
>> 
>> Really!? You gave lme a model with random = ~ leaf |
>> rep (and no data
>> specification) and it tried to fit a model with
>> random = ~ week | rep?
>> Are you sure that is an exact transcript?
>> 
>> > :
>> >         nlminb problem, convergence error code =
>> 1;
>> > message = iteration limit reached without
>> convergence
>> > (9)
>> 
>> > Has anybody an idea to solve this problem?
>> 
>> Oh, I have lots of ideas but without a reproducible
>> example I can't
>> hope to decide what might be the problem.
>> 
>> It appears that the model may be over-parameterized.
>>  Assuming that
>> there are 4 different values of week then ~ week |
>> rep requires
>> fitting 10 variance-covariance parameters. That's a
>> lot.
>> The error code indicates that the optimizer is
>> taking
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/convergence-error-code-in-mixed-effects-models-tp14325990p14340592.html
Sent from the R help mailing list archive at Nabble.com.


From code.monkey91 at yahoo.com  Sat Dec 15 05:05:50 2007
From: code.monkey91 at yahoo.com (R. W.)
Date: Fri, 14 Dec 2007 20:05:50 -0800 (PST)
Subject: [R] Simulating MATCHED Case Control Data
Message-ID: <152345.57819.qm@web45805.mail.sp1.yahoo.com>

Dear R-Help-List,

A few days ago I asked for help simulating
case-control data.  I got a great answer to help me
with my code, but I am having trouble modifying it for
1:M matched case-control data.  Does anyone have any
guidance/pointers for simulating 1:M matched data?.

Thank you,
-R




> Dear R-Help-List,
>
> I was wondering if anyone had experience simulating
> case-control data in R?

I think the only simple method that allows you to
specify any arbitrary
 
population distribution of predictors and does not
rely on the logistic
 
regression model being true is to simulate cohorts and
then take a 
case-control sample from each one

Eg for a case-control sample of 500 cases and 1000
controls where there
 is 
about a 1% cumulative incidence
1. Generate all your predictor variables for a cohort
of 50,000 people,
 
from any distributions you want
2. Specify the disease model. This could be logistic
     logit(p(Y=1))=eta = b0+b1x1+b2x2+...
     p = exp(eta)/(1+exp(eta))
   or it could be anything else.
3. Now sum(p) gives the expected number of cases.
Adjust b0 so that
 this 
is a bit bigger than your desired number, eg 550.
4. Generate Y for the population by rbinom(50000,1,p)
5. Choose 500 cases and 1000 controls using sample().




      ____________________________________________________________________________________
Looking for last minute shopping deals?


From tplate at acm.org  Sat Dec 15 06:45:46 2007
From: tplate at acm.org (Tony Plate)
Date: Fri, 14 Dec 2007 22:45:46 -0700
Subject: [R] what does cut(data, breaks=n) actually do?
Message-ID: <47636A0A.9070805@acm.org>

Peter Dalgaard wrote:
> melissa cline wrote:
>> Hello,
>>
>> I'm trying to bin a quantity into 2-3 bins for calculating entropy and
>> mutual information.  One of the approaches I'm exploring is the cut()
>> function, which is what the mutualInfo function in binDist uses.  When it's
>> called in the format cut(data, breaks=n), it somehow splits the data into n
>> distinct bins.  Can anyone tell me how cut() decides where to cut?
>>
>>   
> This is one case where reading the actual R code is easier that 
> explaining what it does.  From cut.default
> 
>     if (length(breaks) == 1) {
>         if (is.na(breaks) | breaks < 2)
>             stop("invalid number of intervals")
>         nb <- as.integer(breaks + 1)
>         dx <- diff(rx <- range(x, na.rm = TRUE))
>         if (dx == 0)
>             dx <- rx[1]
>         breaks <- seq.int(rx[1] - dx/1000, rx[2] + dx/1000, length.out = nb)
>     }
> 
> so basically it takes the range, extends it a bit and splits in into 
> <breaks> equally long segments.
> 
> (For the sometimes more attractive option of splitting into groups of 
> roughly equal size, there is cut2 in the Hmisc package, or use quantile())
> 

It can be a bit dangerous to use quantile() to provide breaks for cut(),
because quantiles can be non-unique, which cut() doesn't like:
> x1 <- c(1,1,1,1,1,1,1,1,1,2)
> cut(x1, breaks=quantile(x1, (0:2)/2))
Error in cut.default(x1, breaks = quantile(x1, (0:2)/2)) :
   'breaks' are not unique
>

However, cut2() in Hmisc handles this situation gracefully:
> library(Hmisc)
Attaching package: 'Hmisc'
        The following object(s) are masked from package:base :
          format.pval,
          round.POSIXt,
          trunc.POSIXt,
          units
> cut2(x1, g=2)
  [1] 1 1 1 1 1 1 1 1 1 2
Levels: 1 2
>

(Additionally, a potentially dangerous peculiarity of quantile() for 
this kind of purpose is that its return values can be out of order 
(i.e., diff(quantile(...))<0, at rounding error level), but this doesn't 
actually upset cut() in R because cut() sorts the breaks prior to using 
them.)

-- Tony Plate


From rolf.fankhauser at gepdata.ch  Sat Dec 15 08:22:11 2007
From: rolf.fankhauser at gepdata.ch (Rolf Fankhauser)
Date: Sat, 15 Dec 2007 07:22:11 +0000
Subject: [R] How to convert Datetime numbers from Excel to POSIXt objects
In-Reply-To: <4762AE1B.70604@biostat.ku.dk>
References: <4762B82C.9040708@gepdata.ch> <4762AE1B.70604@biostat.ku.dk>
Message-ID: <476380A3.8050603@gepdata.ch>

Ok, I see, the difference comes from summer and winter time. Thanks for 
the hint!
Stupid not to bear that in mind!!

Peter Dalgaard wrote:

>Rolf Fankhauser wrote:
>  
>
>>Hi all,
>>
>>I need to compare time series data files of different time formats. I 
>>had no problems with text format using strptime.
>>But how can I convert datetime numbers from Excel (days since 30.12.1899 
>>00:00:00) into POSIXt objects?
>>For example 29770.375 should be converted to  "03.07.1981 09:00:00"
>>
>>I tried the following code and encountered strange results:
>>
>>t1-t0 gives 29770.33 (should be 29770.375 in my opinion)
>>t1-t2 and t1-t3 are ok
>>t1-t4 gives 183.3333 (should be 183.375)
>>Are these rounding errors?
>>  
>>1/(.375 - .333333)
>>    
>>
>[1] 23.99981
>
>So your expectation is off by 1/24th of a day.
>
>Can you think of something that might affect time differences by that
>amount, depending on which times of the year you are comparing?
>
>  
>
>>The R-code:
>>
>>t1 <- strptime("3.7.1981 09:00:00","%d.%m.%Y %H:%M:%S")
>>t0 <- strptime("30.12.1899 00:00:00","%d.%m.%Y %H:%M:%S")
>>t2 <- strptime("3.7.1981 00:00:00","%d.%m.%Y %H:%M:%S")
>>t3 <- strptime("1.7.1981 00:00:00","%d.%m.%Y %H:%M:%S")
>>t4 <- strptime("1.1.1981 00:00:00","%d.%m.%Y %H:%M:%S")
>>t1 - t0
>>t1 - t2
>>difftime(t1,t2,units="days")
>>t1 - t3
>>t1 - t4
>>
>>Thanks for any help or clarifications
>>
>>Rolf
>>
>>______________________________________________
>>R-help at r-project.org mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>  
>>    
>>
>
>
>  
>


From rh at family-krueger.com  Sat Dec 15 08:39:02 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Sat, 15 Dec 2007 08:39:02 +0100
Subject: [R] calculating the number of days from dates
In-Reply-To: <20071214120132.9A9785955DD@borg.st.net.au>
References: <mailman.27.1197457204.24367.r-help@r-project.org>
	<20071214120132.9A9785955DD@borg.st.net.au>
Message-ID: <47638496.9070905@family-krueger.com>

Bob Green schrieb:
>
>  > dates <- read.csv("c:\\dates.csv",header=T)
>  > dates
>            v1         v2
> 1 12/12/1978 12/12/2005
> 2 23/01/1965 23/09/2001
> 3 24/12/2004 16/03/2007
> 4  3/03/2003  4/04/2004
> 5  8/11/2006  1/05/2007
>
>  > class(dates$v1)
> [1] "factor"
>  > class(dates$v2)
> [1] "factor"
>
>   
What about chron library:

dts <- dates(c("02/27/92", "02/27/92", "01/14/92",
               "02/28/92", "02/01/92"))
dts
# [1] 02/27/92 02/27/92 01/14/92 02/28/92 02/01/92
tms <- times(c("23:03:20", "22:29:56", "01:03:30",
               "18:21:03", "16:56:26"))
tms
# [1] 23:03:20 22:29:56 01:03:30 18:21:03 16:56:26
x <- chron(dates = dts, times = tms)
x
# [1] (02/27/92 23:03:19) (02/27/92 22:29:56) (01/14/92 01:03:30)
# [4] (02/28/92 18:21:03) (02/01/92 16:56:26)

# We can add or subtract scalars (representing days) to dates or
# chron objects:
c(dts[1], dts[1] + 10)
# [1] 02/27/92 03/08/92
dts[1] - 31
# [1] 01/27/92


Knut


From rh at family-krueger.com  Sat Dec 15 08:39:26 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Sat, 15 Dec 2007 08:39:26 +0100
Subject: [R] RMySQL installation problem
In-Reply-To: <Pine.LNX.4.64.0711152120050.7199@gannet.stats.ox.ac.uk>
References: <BAY120-W88A17C859B8C463333777C7820@phx.gbl>
	<Pine.LNX.4.64.0711152120050.7199@gannet.stats.ox.ac.uk>
Message-ID: <476384AE.5010500@family-krueger.com>

Prof Brian Ripley schrieb:
> On Thu, 15 Nov 2007, Tao Shi wrote:
>   
> The 'specified module' was specified in the popup!  (Windows' users should 
> be used to the arcaneness of the error messages.)
>
> Assuming you actually have MySQL installed, you need to make sure 
> libmysql.dll is on the PATH.  It's in the mysql\bin directory, so 
> installing MySQL would normally put it on the PATH.
>   

I tried to install the package too, and I can confirm that there is the
same error.
the ...\library\mysql\libs is in the path
I copied an executable file into the directory named test123.exe
and used Start-> execute test123.exe
The program was executed

controll test:
I deleted the exeutable file
and used Start-> execute test123.exe  again
result
file not found.

so the path is ok but it isn't working

Did I miss something else?
Regards Knut


From rh at family-krueger.com  Sat Dec 15 08:39:42 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Sat, 15 Dec 2007 08:39:42 +0100
Subject: [R] Helpfiles in HTML browser
In-Reply-To: <4761D281.6000401@hotmail.com>
References: <47613C2C.9060303@family-krueger.com>
	<4761D281.6000401@hotmail.com>
Message-ID: <476384BE.5090000@family-krueger.com>

Francisco J. Zagmutt schrieb:
> Go to your Rprofile file (in the etc directory) and add the following line:
>
> options(htmlhelp=TRUE)
>
> I hope this helps
>   
yes, thank you very much
...
and a hint for others:
set
# to prefer Compiled HTML help
options(chmhelp=TRUE)
to
# to prefer Compiled HTML help
# options(chmhelp=TRUE)

if both is active the the HTML help doesn't work.


Regards Knut


From ripley at stats.ox.ac.uk  Sat Dec 15 09:10:08 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 15 Dec 2007 08:10:08 +0000 (GMT)
Subject: [R] connecting RMySQL to and external server
In-Reply-To: <200712142055.15148.dusa.adrian@gmail.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
Message-ID: <Pine.LNX.4.64.0712141908030.26362@gannet.stats.ox.ac.uk>

It is trivial with RODBC (I know that is not what you asked, but it is the 
solution we found first).


For RMySQL, note first that the MySQL configuration file is consulted, so 
the default host is specified in the client section, which is like

[client]

port=3306

[mysql]

default-character-set=latin1

Add host=foo under [client] to change the default host.

And ?dbConnect mentions a 'host' argument under '...'.  That seems to work 
for me (provided of course the server allows connections from other 
machines): on my home network from Windows laptop to Linux server

> library(RMySQL)
Loading required package: DBI
> drv <- dbDriver("MySQL")
> con <- dbConnect(drv, user="ripley", host="auk", dbname="ripley")

This mixture of using the *local* configuration file overridden by 
arguments is a bit dangerous: RMySQL seems not really designed for 
client-server operation and there are some things that definitely do not 
work.  (As I recall, that included dbWrite as that imports a file which is 
on the local machine.)

On Fri, 14 Dec 2007, Adrian Dusa wrote:

> Dear list,
>
> I learned how to connect R to a local MySQL server, using:
> drv <- dbDriver("MySQL")
> con <- dbConnect(drv, user="root", password="mypass", dbname="mydb")
>
> Is it possible to connect R in this way to an external server (on a different
> machine, with a different IP)?
>
> I read the documentation on ?dbConnect (and everything I could find on the
> internet), but I failed to find some other relevant arguments. For example,
> one needs to first connect to the external machine and only after that to the
> MySQL server on that machine. Is this possible from within R?
>
> Thank you in advance,
> Adrian
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From seikibunpu at yahoo.co.jp  Fri Dec 14 21:12:25 2007
From: seikibunpu at yahoo.co.jp (Kazuhiko SHINKI)
Date: Sat, 15 Dec 2007 05:12:25 +0900 (JST)
Subject: [R] garch function in tseries package
Message-ID: <20071214201225.91095.qmail@web3212.mail.kcd.yahoo.co.jp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071215/5f1beb5c/attachment.pl 

From thierrydb at gmail.com  Fri Dec 14 16:19:56 2007
From: thierrydb at gmail.com (thierrydb)
Date: Fri, 14 Dec 2007 07:19:56 -0800 (PST)
Subject: [R]  Function built with segments
Message-ID: <14337780.post@talk.nabble.com>


Hello,


I'm new to R. If I have a set of 10 points (X(i), Y(i)), is there an elegant
way to build a function y=f(x) that would be build out of the consecutive
segments of X(j),Y(j) points (with X(j) sorted)?


Thank you very much

TDB


-- 
View this message in context: http://www.nabble.com/Function-built-with-segments-tp14337780p14337780.html
Sent from the R help mailing list archive at Nabble.com.


From dmmtchll at gmail.com  Fri Dec 14 19:31:17 2007
From: dmmtchll at gmail.com (dave mitchell)
Date: Fri, 14 Dec 2007 12:31:17 -0600
Subject: [R] Array dimnames
Message-ID: <ea7af0460712141031n5c5df115y55e4d116615e7474@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071214/7a641b5e/attachment.pl 

From ravi.longia at gmail.com  Sat Dec 15 00:32:15 2007
From: ravi.longia at gmail.com (Ravi Longia)
Date: Fri, 14 Dec 2007 23:32:15 +0000
Subject: [R] Truncated normal distribution
Message-ID: <bac068d10712141532r150fdb39s3d2127b53dfb8223@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071214/7d11dab5/attachment.pl 

From gygulyas at gmail.com  Sat Dec 15 06:51:21 2007
From: gygulyas at gmail.com (Gyula)
Date: Fri, 14 Dec 2007 21:51:21 -0800 (PST)
Subject: [R] rcom close Excel problem
In-Reply-To: <2983390.1197662328437.JavaMail.?@fh128.dia.he.tucows.com>
References: <2983390.1197662328437.JavaMail.?@fh128.dia.he.tucows.com>
Message-ID: <d6d71138-196d-4240-ac32-c9c6de3b21ec@e10g2000prf.googlegroups.com>

First, you will likely have to use Ctrl-Alt-Delete - Task Manager - to
kill the Excel processes.

you could also try

 wb[["Saved"]]<-TRUE  # trick Excel to think that the workbook is
saved
 x$Quit() # close Excel
 rm(list=ls()) # remove all objects attached to environment

Gyula

On Dec 14, 11:58 am, stephen bond <sten... at go.com> wrote:
> Hello,
>
> I just discovered that I cannot close the Excel application and task
> manager shows numerous copies of Excel.exe
>
> I tried both
>
> x$Quit() # shown in the rcom archive
>
> and
>
> x$Exit()
>
> and Excel refuses to die.
> Thank you very much.
> S.
>
> "You can't kill me, I will not die" Mojo Nixon
>
> I also have a problem with saving. It produces a pop-up dialog and
> does
> not take my second parameter:
>
> x<-comCreateObject("Excel.Application")
> wb<-comInvoke(comGetProperty(x,"Workbooks"),"Open","G:
> /MR/Stephen/repo.
> xls", "0")
> sh<-comGetProperty(wb,"Worksheets","Market Data")
> range1 <- comGetProperty(sh,"Range","C10","I11")
> vals <- comGetProperty(range1,"Value")
> comInvoke(wb,"Close","G:/MR/Stephen/repo.xls","True") # True is
> ignored
>
> Thank you All.
> Stephen
>
> ______________________________________________
> R-h... at r-project.org mailing listhttps://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Sat Dec 15 11:32:22 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 15 Dec 2007 11:32:22 +0100
Subject: [R] Function built with segments
In-Reply-To: <14337780.post@talk.nabble.com>
References: <14337780.post@talk.nabble.com>
Message-ID: <18275.44342.310433.560984@ada-stat.math.ethz.ch>

>>>>> "t" == thierrydb  <thierrydb at gmail.com>
>>>>>     on Fri, 14 Dec 2007 07:19:56 -0800 (PST) writes:

    t> Hello,


    t> I'm new to R. If I have a set of 10 points (X(i), Y(i)), is there an elegant
    t> way to build a function y=f(x) that would be build out of the consecutive
    t> segments of X(j),Y(j) points (with X(j) sorted)?

Yes.
approxfun() !

E.g.

 set.seed(1); x <- runif(10); y <- 3*x + rnorm(10)/10;  plot(x,y)
 myF <- approxfun(x,y)

 myF(0.1)
 ## [1] 0.3734227

and then

  plot(myF, add=TRUE, col=2) ## using plot() method for functions

or simpler to understand:

  xx <- seq(0,1, length=100)
  lines(xx, myF(xx), col=2)


    t> Thank you very much

    t> TDB

you're welcome.
Martin Maechler, ETH Zurich


From maechler at stat.math.ethz.ch  Sat Dec 15 11:37:29 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 15 Dec 2007 11:37:29 +0100
Subject: [R] calculating the number of days from dates
In-Reply-To: <47638496.9070905@family-krueger.com>
References: <mailman.27.1197457204.24367.r-help@r-project.org>
	<20071214120132.9A9785955DD@borg.st.net.au>
	<47638496.9070905@family-krueger.com>
Message-ID: <18275.44649.487754.926370@ada-stat.math.ethz.ch>

>>>>> "KK" == Knut Krueger <rh at family-krueger.com>
>>>>>     on Sat, 15 Dec 2007 08:39:02 +0100 writes:

    KK> Bob Green schrieb:
    >> 
    >> > dates <- read.csv("c:\\dates.csv",header=T)
    >> > dates
    >> v1         v2
    >> 1 12/12/1978 12/12/2005
    >> 2 23/01/1965 23/09/2001
    >> 3 24/12/2004 16/03/2007
    >> 4  3/03/2003  4/04/2004
    >> 5  8/11/2006  1/05/2007
    >> 
    >> > class(dates$v1)
    >> [1] "factor"
    >> > class(dates$v2)
    >> [1] "factor"
    >> 
    >> 
    KK> What about chron library:

it's  a  >> package <<  , not a library, please!

and as Henrique  has shown it's really not needed for the question.
There's the "Date" (S3) class, and even a "difftime" one
for time *differences*
See
	?as.Date
	?difftime
and also note the output of
     	methods(class = "Date")

Martin

    KK> dts <- dates(c("02/27/92", "02/27/92", "01/14/92",
    KK> "02/28/92", "02/01/92"))
    KK> dts
    KK> # [1] 02/27/92 02/27/92 01/14/92 02/28/92 02/01/92
    KK> tms <- times(c("23:03:20", "22:29:56", "01:03:30",
    KK> "18:21:03", "16:56:26"))
    KK> tms
    KK> # [1] 23:03:20 22:29:56 01:03:30 18:21:03 16:56:26
    KK> x <- chron(dates = dts, times = tms)
    KK> x
    KK> # [1] (02/27/92 23:03:19) (02/27/92 22:29:56) (01/14/92 01:03:30)
    KK> # [4] (02/28/92 18:21:03) (02/01/92 16:56:26)

    KK> # We can add or subtract scalars (representing days) to dates or
    KK> # chron objects:
    KK> c(dts[1], dts[1] + 10)
    KK> # [1] 02/27/92 03/08/92
    KK> dts[1] - 31
    KK> # [1] 01/27/92


    KK> Knut

    KK> ______________________________________________
    KK> R-help at r-project.org mailing list
    KK> https://stat.ethz.ch/mailman/listinfo/r-help
    KK> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    KK> and provide commented, minimal, self-contained, reproducible code.


From ysapir at bgu.ac.il  Sat Dec 15 12:34:49 2007
From: ysapir at bgu.ac.il (Yuval Sapir)
Date: Sat, 15 Dec 2007 11:34:49 GMT
Subject: [R] modify a data.frame within a function
In-Reply-To: <mailman.25.1197716404.14713.r-help@r-project.org>
References: <mailman.25.1197716404.14713.r-help@r-project.org>
Message-ID: <fa15e0c38bf9.4763bbd9@bgu.ac.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071215/afd816d9/attachment.pl 

From Bannho at gmx.de  Sat Dec 15 13:16:51 2007
From: Bannho at gmx.de (creepa1982)
Date: Sat, 15 Dec 2007 04:16:51 -0800 (PST)
Subject: [R] VARMA in R
In-Reply-To: <200712132131.lBDLVXIb021302@definetti.ddns.uark.edu>
References: <14322697.post@talk.nabble.com>
	<200712132131.lBDLVXIb021302@definetti.ddns.uark.edu>
Message-ID: <14350275.post@talk.nabble.com>


Hey Giovanni, 

thanks a lot for the help. I tried out combining the two functions
dlmModARMA and dlmMLE and it works. The only problem I have right now is
this. When I pass on the information about the starting parameters (param)
in the dlmMLE function I can only input one parameter vector. However, for a
VARMA I have a matrix of coefficients for both the AR part and MA part. How
can I signal to dlmModARMA which part of the passed on vector is supposed to
be the AR input, the MA input and so on? 

Thanks again! 

Benjamin



Giovanni Petris wrote:
> 
> 
> You may want to check package dlm and, possibly, dse. 
> 
> In dlm you can cast a VARMA model in state space form (dlmModARMA) and
> estimate unknown parameters by maximum likelihood (dlmMLE). 
> 
> 
> Best,
> Giovanni
> 
>> Date: Thu, 13 Dec 2007 11:17:47 -0800 (PST)
>> From: creepa1982 <Bannho at gmx.de>
>> Sender: r-help-bounces at r-project.org
>> Precedence: list
>> 
>> 
>> Hi all, 
>> 
>> does anyone know of a package/function for fitting Vector Autoregressive
>> Moving Average models? I looked through most of the packages available
>> but
>> could only find functions to fit a VAR. 
>> 
>> Any help would be appreciated! 
>> 
>> Benjamin
>> -- 
>> View this message in context:
>> http://www.nabble.com/VARMA-in-R-tp14322697p14322697.html
>> Sent from the R help mailing list archive at Nabble.com.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> -- 
> 
> Giovanni Petris  <GPetris at uark.edu>
> Department of Mathematical Sciences
> University of Arkansas - Fayetteville, AR 72701
> Ph: (479) 575-6324, 575-8630 (fax)
> http://definetti.uark.edu/~gpetris/
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/VARMA-in-R-tp14322697p14350275.html
Sent from the R help mailing list archive at Nabble.com.


From mi2kelgrum at yahoo.com  Sat Dec 15 14:37:28 2007
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sat, 15 Dec 2007 05:37:28 -0800 (PST)
Subject: [R] truncated fields with RODBC
Message-ID: <300923.64529.qm@web60221.mail.yahoo.com>

For the record, the problem with truncated fields below was solved by increasing the Max LongVarChar variable in the data source settings page 1 from 8190 to 32760. So it was a psqlODBC problem not an RODBC problem. The command nchar(Grids$Grids) helped me see how large the fields actually were and what size of number I was looking for. 

cheers,
Mikkel


----- Original Message ----
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
To: Mikkel Grum <mi2kelgrum at yahoo.com>
Cc: r-help at stat.math.ethz.ch
Sent: Sunday, November 25, 2007 2:05:37 PM
Subject: Re: [R] truncated fields with RODBC


You need to study the RODBC documentation: you haven't set the type of
 the 
character fields in the database table correctly (in fact, you seem not
 
to have set them at all, hence will get the default of varchar(255)).

The 64k limit is for reading, not writing.

As ever, full details and a reproducible example are needed for people
 to 
help you fully.

On Sat, 24 Nov 2007, Mikkel Grum wrote:

> I'm changing some functions from storing data in
> SQLite (using RSQLite) to storing it in PostgreSQL
> (using RODBC). When trying to store very long
> character fields I get the following message:
>
>>    sqlSave(pg, Grids, rownames = FALSE, append =
> TRUE)
> Warning messages:
> 1: In odbcUpdate(channel, query, mydata, paramdata,
> test = test, verbose = verbose,  :
>  character data truncated in column 'grids'
> 2: In odbcUpdate(channel, query, mydata, paramdata,
> test = test, verbose = verbose,  :
>  character data truncated in column 'grids'
> 3: In odbcUpdate(channel, query, mydata, paramdata,
> test = test, verbose = verbose,  :
>  character data truncated in column 'grids'
>
> The structure of the dataframe that I'm trying to
> store looks like this:
>> str(Grids)
> 'data.frame':   9 obs. of  4 variables:
> $ ScoutDate: chr  "2007-10-11" "2007-10-11"
> "2007-10-11" "2007-10-11" ...
> $ SectorId : int  93 93 93 93 93 93 93 93 93
> $ Trait    : chr  "eTop" "eMB" "nTop" "nMB" ...
> $ Grids    : chr  "0 0 0 0 0 0 0 53 6064 2364 61 0 0
> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 74 482
> 524 51 0 0 157 316 0 0 0 0 0 0 0 0 0 0 0"|
> __truncated__ "45 45 45 45 45 45 45 1 0 0 0 45 45 45
> 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45
> 45 45 45 45 50 68 70 49 46 46 0 0 3"| __truncated__ "0
> 0 0 0 0 0 0 84 18766 7266 111 0 0 0 0 0 0 0 0 0 0 0 0
> 0 0 0 0 0 0 0 0 0 0 0 0 0 192 1628 1777 112 0 0 409
> 903 0 0 0 0 0 0 0 0"| __truncated__ "94 94 94 94 94 94
> 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94
> 94 94 94 94 94 94 94 94 94 94 94 94 137 312 331 128 94
> "| __truncated__ ...
>
> The same fields could be copied from SQLite into
> PostgreSQL through a | delimited file without any
> error message, so it is not PostgreSQL that is the
> limitation. dbWriteTable in RSQLite was also able to
> handle this without truncating the data. I think these
> fields are 4-5000 characters wide, but don't actually
> know how to get the exact figure.
>
> The offending field is set as a text field in
> PostgreSQL. I'm using psqlODBC on Windows Server 2003
> and R-2.6.0.
>
> Have I missed an argument somewhere that could solve
> the problem? I've read that RODBC has a field length
> limit of 64k. This could be the problem. Is there
> somewhere I could change this in the source code?
> Would that just give me other problems?
>
> Any assistance highly appreciated.
>
> cheers,
> Mikkel

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595





      ____________________________________________________________________________________
Be a better friend, newshound, and


From ripley at stats.ox.ac.uk  Sat Dec 15 15:24:02 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 15 Dec 2007 14:24:02 +0000 (GMT)
Subject: [R] garch function in tseries package
In-Reply-To: <20071214201225.91095.qmail@web3212.mail.kcd.yahoo.co.jp>
References: <20071214201225.91095.qmail@web3212.mail.kcd.yahoo.co.jp>
Message-ID: <Pine.LNX.4.64.0712151420020.23036@gannet.stats.ox.ac.uk>

This has been reported several times.  I posted a workaround earlier this 
week:

https://stat.ethz.ch/pipermail/r-help/2007-December/148290.html

so your searching of the archives was not very thorough!

Another workaround is to use rterm instead of Rgui.


On Sat, 15 Dec 2007, Kazuhiko SHINKI wrote:

>
>  I am wondering how to run 'garch' function of 'tseries' package in R2.6.1.
>  I installed R2.3.1 and R2.6.1 in my PC (Windows XP Home) and run a
>  following simple GARCH function in both versions:
>
>  >garch(dSP[1:300], order = c(1,1))
>
>  where 'dSP' is daily return series of a stock index.
>  R2.6.1 can not finish calculation and also I can not stop the calculation
>  with the 'stop' button, while R2.3.1 can finish it within a few seconds.
>
>  Thank you,
>
>  Kazuhiko Shinki
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mark at wardle.org  Sat Dec 15 16:13:25 2007
From: mark at wardle.org (Mark Wardle)
Date: Sat, 15 Dec 2007 15:13:25 +0000
Subject: [R] modify a data.frame within a function
In-Reply-To: <fa15e0c38bf9.4763bbd9@bgu.ac.il>
References: <mailman.25.1197716404.14713.r-help@r-project.org>
	<fa15e0c38bf9.4763bbd9@bgu.ac.il>
Message-ID: <b59a37130712150713m4a3bdce5me6eff495f4f257a6@mail.gmail.com>

You need to read about scope.

Try using return() from within your function to return the modified
data frame to the caller.

Best wishes,

Mark

On 15/12/2007, Yuval Sapir <ysapir at bgu.ac.il> wrote:
>  Hello all,
> I'm trying to modify a single column of a data frame to remove randomly half of the values. I want to do it within a function, but can not assign the modified column back into the data frame. It was easy and successful without a function, so I suspect the problem is the call of the single column within the function.
>
> removedata<-function(datafull,var.removed)
> {
> attach(datafull)
> NArandom<-rnorm(n=length(var.removed), mean=0, sd=1)
> for (i in 1:length(var.removed))
>   if(NArandom[i]>0)
>     var.removed[i]<-"NA"
> datamiss<<-datafull
> detach(datafull)
> }
>
> -- Yuval Sapir, PhDResearcherInstitute of Evolution, Haifa UniversityHaifa, Israel 3190?
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
> ______________________________________________________________________
> This email has been scanned by the MessageLabs Email Security System.
> For more information please visit http://www.messagelabs.com/email
> ______________________________________________________________________
>


-- 
Dr. Mark Wardle
Specialist registrar, Neurology
Cardiff, UK

From chengshanliu at gmail.com  Sat Dec 15 16:14:50 2007
From: chengshanliu at gmail.com (Chengshan Liu)
Date: Sat, 15 Dec 2007 23:14:50 +0800
Subject: [R] How to analyze observations list-wise deleted?
Message-ID: <4763EF6A.2090003@gmail.com>

I am using lrm in Design package for a project using logit analysis.
I think lrm is very useful for providing information about the number of
missing values due to the inclusion of each variable.

My first questions is: How to explore those observations that are
automatically deleted from the lrm?
For example, in a data I have more than 6,000 observations but after
list-wise deletion I have only 2,411 observations, where there are 583
observations for the dependent variable of value 1 and 1,828 for the
value 0.

One independent variable of the model "cDisagLW" accounts for the
deletion of 2,598 observations and I think this could cause problem in
regression result. So, could you show me a way to find out "who" are
those 2,598 observations?

A further question is, how to make a subset of the original data that
contains these 2,411 valid observations?
With this dataset, I like to explore the underlying distribution of the
regression data and contrast to the original data of 6,000 observations.

Thank you in advance.

Frank

-- 
Frank C.S. Liu
Assistant Professor,                      
Graduate Institute of Political Science   E-mail: frankcsliu at gmail.com
National Sun Yat-sen University (NYSYU)   Office:+886.7.525.2000 #5555
Kaohsiung, Taiwan 804, R.O.C.             FAX:+886.7.525.5540


From ripley at stats.ox.ac.uk  Sat Dec 15 16:37:16 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 15 Dec 2007 15:37:16 +0000 (GMT)
Subject: [R] garch function in tseries package
In-Reply-To: <Pine.LNX.4.64.0712151420020.23036@gannet.stats.ox.ac.uk>
References: <20071214201225.91095.qmail@web3212.mail.kcd.yahoo.co.jp>
	<Pine.LNX.4.64.0712151420020.23036@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0712151503270.24648@gannet.stats.ox.ac.uk>

On Sat, 15 Dec 2007, Prof Brian Ripley wrote:

> This has been reported several times.  I posted a workaround earlier this
> week:
>
> https://stat.ethz.ch/pipermail/r-help/2007-December/148290.html

The promised update, tseries_0.10-13.tar.gz, is on the CRAN master in 
source form.  Expect a binary Windows build to propagate through CRAN in 
the next few days (it seems to have missed today's updates).  AFAIK this 
fixes the problems under Rgui.

> so your searching of the archives was not very thorough!
>
> Another workaround is to use rterm instead of Rgui.
>
>
> On Sat, 15 Dec 2007, Kazuhiko SHINKI wrote:
>
>>
>>  I am wondering how to run 'garch' function of 'tseries' package in R2.6.1.
>>  I installed R2.3.1 and R2.6.1 in my PC (Windows XP Home) and run a
>>  following simple GARCH function in both versions:
>>
>> >garch(dSP[1:300], order = c(1,1))
>>
>>  where 'dSP' is daily return series of a stock index.
>>  R2.6.1 can not finish calculation and also I can not stop the calculation
>>  with the 'stop' button, while R2.3.1 can finish it within a few seconds.
>>
>>  Thank you,
>>
>>  Kazuhiko Shinki
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From affysnp at gmail.com  Sat Dec 15 16:56:46 2007
From: affysnp at gmail.com (affy snp)
Date: Sat, 15 Dec 2007 10:56:46 -0500
Subject: [R] how to threshold a matrix
Message-ID: <5032046e0712150756n73980c22w85b5f8e282c7b607@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071215/1174b2ba/attachment.pl 

From ggrothendieck at gmail.com  Sat Dec 15 17:10:06 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 15 Dec 2007 11:10:06 -0500
Subject: [R] how to threshold a matrix
In-Reply-To: <5032046e0712150756n73980c22w85b5f8e282c7b607@mail.gmail.com>
References: <5032046e0712150756n73980c22w85b5f8e282c7b607@mail.gmail.com>
Message-ID: <971536df0712150810j74712b20m2643aa406d2f0f04@mail.gmail.com>

Try:

(M >= 0.3) - (M <= 0.3)

On Dec 15, 2007 10:56 AM, affy snp <affysnp at gmail.com> wrote:
> Dear list,
>
> I have a matrix M (2500 rows and 9 columns). It looks like
>
> 2.2     0.1      2.6    3.6 ......
> 0.4     1.9      2.7    4.2......
> 1.8     2.5      4.3    2.2.......
> .....................
>
> If I want to do:
>
> (1) if M[i,j]>=0.3, M[i,j]=1
> (2) if M[i,j]<=-0.3, M[i,j]=-1
> (3) Otherwise, M[i,j]=0
>
> Is there a way to ceil and floor the data?
>
> Thanks a lot!
>
> Allen
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dusa.adrian at gmail.com  Sat Dec 15 17:33:04 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sat, 15 Dec 2007 18:33:04 +0200
Subject: [R] connecting RMySQL to and external server
In-Reply-To: <Pine.LNX.4.64.0712141908030.26362@gannet.stats.ox.ac.uk>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<Pine.LNX.4.64.0712141908030.26362@gannet.stats.ox.ac.uk>
Message-ID: <200712151833.04227.dusa.adrian@gmail.com>


Indeed, I noticed the "host" argument but the server demands an username and a 
password for the machine first, and only after that for the MySQL server. 
Those were the arguments I was looking for.
I will study the RODBC package then, if it solves the problem.

Thank you very much,
Adrian


On Saturday 15 December 2007, Prof Brian Ripley wrote:
> It is trivial with RODBC (I know that is not what you asked, but it is the
> solution we found first).
>
>
> For RMySQL, note first that the MySQL configuration file is consulted, so
> the default host is specified in the client section, which is like
>
> [client]
>
> port=3306
>
> [mysql]
>
> default-character-set=latin1
>
> Add host=foo under [client] to change the default host.
>
> And ?dbConnect mentions a 'host' argument under '...'.  That seems to work
> for me (provided of course the server allows connections from other
> machines): on my home network from Windows laptop to Linux server
>
> > library(RMySQL)
>
> Loading required package: DBI
>
> > drv <- dbDriver("MySQL")
> > con <- dbConnect(drv, user="ripley", host="auk", dbname="ripley")
>
> This mixture of using the *local* configuration file overridden by
> arguments is a bit dangerous: RMySQL seems not really designed for
> client-server operation and there are some things that definitely do not
> work.  (As I recall, that included dbWrite as that imports a file which is
> on the local machine.)
>
> On Fri, 14 Dec 2007, Adrian Dusa wrote:
> > Dear list,
> >
> > I learned how to connect R to a local MySQL server, using:
> > drv <- dbDriver("MySQL")
> > con <- dbConnect(drv, user="root", password="mypass", dbname="mydb")
> >
> > Is it possible to connect R in this way to an external server (on a
> > different machine, with a different IP)?
> >
> > I read the documentation on ?dbConnect (and everything I could find on
> > the internet), but I failed to find some other relevant arguments. For
> > example, one needs to first connect to the external machine and only
> > after that to the MySQL server on that machine. Is this possible from
> > within R?
> >
> > Thank you in advance,
> > Adrian



-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From ripley at stats.ox.ac.uk  Sat Dec 15 17:46:01 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 15 Dec 2007 16:46:01 +0000 (GMT)
Subject: [R] connecting RMySQL to and external server
In-Reply-To: <200712151833.04227.dusa.adrian@gmail.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<Pine.LNX.4.64.0712141908030.26362@gannet.stats.ox.ac.uk>
	<200712151833.04227.dusa.adrian@gmail.com>
Message-ID: <Pine.LNX.4.64.0712151637340.7933@gannet.stats.ox.ac.uk>

On Sat, 15 Dec 2007, Adrian Dusa wrote:

>
> Indeed, I noticed the "host" argument but the server demands an username and a
> password for the machine first,

But you said 'connect to', not 'log in to', so how were we to know that?

> and only after that for the MySQL server.
> Those were the arguments I was looking for.
> I will study the RODBC package then, if it solves the problem.

I am afraid I don't understand your setup. MySQL works by listening on 
port 3306: user accounts don't come into that.  With our bastion servers 
all such ports are blocked and can only be accessed via tunnels 
(implemented by stunnel).  I think you need to discuss this with your 
sysadmins: if it works under mysql or for ODBC (isql) it will work with 
the corresponding R packages.

>
> Thank you very much,
> Adrian
>
>
> On Saturday 15 December 2007, Prof Brian Ripley wrote:
>> It is trivial with RODBC (I know that is not what you asked, but it is the
>> solution we found first).
>>
>>
>> For RMySQL, note first that the MySQL configuration file is consulted, so
>> the default host is specified in the client section, which is like
>>
>> [client]
>>
>> port=3306
>>
>> [mysql]
>>
>> default-character-set=latin1
>>
>> Add host=foo under [client] to change the default host.
>>
>> And ?dbConnect mentions a 'host' argument under '...'.  That seems to work
>> for me (provided of course the server allows connections from other
>> machines): on my home network from Windows laptop to Linux server
>>
>>> library(RMySQL)
>>
>> Loading required package: DBI
>>
>>> drv <- dbDriver("MySQL")
>>> con <- dbConnect(drv, user="ripley", host="auk", dbname="ripley")
>>
>> This mixture of using the *local* configuration file overridden by
>> arguments is a bit dangerous: RMySQL seems not really designed for
>> client-server operation and there are some things that definitely do not
>> work.  (As I recall, that included dbWrite as that imports a file which is
>> on the local machine.)
>>
>> On Fri, 14 Dec 2007, Adrian Dusa wrote:
>>> Dear list,
>>>
>>> I learned how to connect R to a local MySQL server, using:
>>> drv <- dbDriver("MySQL")
>>> con <- dbConnect(drv, user="root", password="mypass", dbname="mydb")
>>>
>>> Is it possible to connect R in this way to an external server (on a
>>> different machine, with a different IP)?
>>>
>>> I read the documentation on ?dbConnect (and everything I could find on
>>> the internet), but I failed to find some other relevant arguments. For
>>> example, one needs to first connect to the external machine and only
>>> after that to the MySQL server on that machine. Is this possible from
>>> within R?
>>>
>>> Thank you in advance,
>>> Adrian
>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Sat Dec 15 17:54:20 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 15 Dec 2007 11:54:20 -0500
Subject: [R] connecting RMySQL to and external server
In-Reply-To: <200712151833.04227.dusa.adrian@gmail.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<Pine.LNX.4.64.0712141908030.26362@gannet.stats.ox.ac.uk>
	<200712151833.04227.dusa.adrian@gmail.com>
Message-ID: <971536df0712150854n55d5c15eoaf300527cd8ce844@mail.gmail.com>

Use ssh forwarding to forward local port 3307 to remote port 3306
specifying the remote account and password.  Then if you use local port
3306 you can access your local version of MySQL and if you
use port 3307 you can access the remote version.   There is some
info on the MySQL site.  First test it out by running the mysql command
line program accessing the remote data base via port 3307 and once
that works you know its ok and you can try RMySQL or RODBC packages.

On Dec 15, 2007 11:33 AM, Adrian Dusa <dusa.adrian at gmail.com> wrote:
>
> Indeed, I noticed the "host" argument but the server demands an username and a
> password for the machine first, and only after that for the MySQL server.
> Those were the arguments I was looking for.
> I will study the RODBC package then, if it solves the problem.
>
> Thank you very much,
> Adrian
>
>
>
> On Saturday 15 December 2007, Prof Brian Ripley wrote:
> > It is trivial with RODBC (I know that is not what you asked, but it is the
> > solution we found first).
> >
> >
> > For RMySQL, note first that the MySQL configuration file is consulted, so
> > the default host is specified in the client section, which is like
> >
> > [client]
> >
> > port=3306
> >
> > [mysql]
> >
> > default-character-set=latin1
> >
> > Add host=foo under [client] to change the default host.
> >
> > And ?dbConnect mentions a 'host' argument under '...'.  That seems to work
> > for me (provided of course the server allows connections from other
> > machines): on my home network from Windows laptop to Linux server
> >
> > > library(RMySQL)
> >
> > Loading required package: DBI
> >
> > > drv <- dbDriver("MySQL")
> > > con <- dbConnect(drv, user="ripley", host="auk", dbname="ripley")
> >
> > This mixture of using the *local* configuration file overridden by
> > arguments is a bit dangerous: RMySQL seems not really designed for
> > client-server operation and there are some things that definitely do not
> > work.  (As I recall, that included dbWrite as that imports a file which is
> > on the local machine.)
> >
> > On Fri, 14 Dec 2007, Adrian Dusa wrote:
> > > Dear list,
> > >
> > > I learned how to connect R to a local MySQL server, using:
> > > drv <- dbDriver("MySQL")
> > > con <- dbConnect(drv, user="root", password="mypass", dbname="mydb")
> > >
> > > Is it possible to connect R in this way to an external server (on a
> > > different machine, with a different IP)?
> > >
> > > I read the documentation on ?dbConnect (and everything I could find on
> > > the internet), but I failed to find some other relevant arguments. For
> > > example, one needs to first connect to the external machine and only
> > > after that to the MySQL server on that machine. Is this possible from
> > > within R?
> > >
> > > Thank you in advance,
> > > Adrian
>
>
>
> --
> Adrian Dusa
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>          +40 21 3120210 / int.101
>
> ______________________________________________
>
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ajung at gfz-potsdam.de  Sat Dec 15 18:22:48 2007
From: ajung at gfz-potsdam.de (Andre Jung)
Date: Sat, 15 Dec 2007 18:22:48 +0100
Subject: [R] X11 uses font size 25 although 28 was requested
Message-ID: <47640D68.6070501@gfz-potsdam.de>

Hi,

I'm not sure if the real English warning is the same. I translated it
from German. R came up with the following message:

-------------------------------------------------------
Warning message:
In title(main = sinc.exp) :
X11 nutzt Schriftgr??e 25 obwohl 28 angefordert war
[engl.: X11 uses font size 25 although 28 was requested]
-------------------------------------------------------

system:
R version 2.6.1 (2007-11-26)
on openSuSE 10.3
installed via YaST from
http://download.opensuse.org/repositories/home:/dsteuer/
(provided by Detlef Steuer)

any suggestions?
Thanks!
andre


From dusa.adrian at gmail.com  Sat Dec 15 18:37:41 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sat, 15 Dec 2007 19:37:41 +0200
Subject: [R] connecting RMySQL to and external server
In-Reply-To: <Pine.LNX.4.64.0712151637340.7933@gannet.stats.ox.ac.uk>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<200712151833.04227.dusa.adrian@gmail.com>
	<Pine.LNX.4.64.0712151637340.7933@gannet.stats.ox.ac.uk>
Message-ID: <200712151937.41271.dusa.adrian@gmail.com>

On Saturday 15 December 2007, Prof Brian Ripley wrote:
> On Sat, 15 Dec 2007, Adrian Dusa wrote:
> > Indeed, I noticed the "host" argument but the server demands an username
> > and a password for the machine first,
>
> But you said 'connect to', not 'log in to', so how were we to know that?
>
> > and only after that for the MySQL server.
> > Those were the arguments I was looking for.
> > I will study the RODBC package then, if it solves the problem.
>
> I am afraid I don't understand your setup. MySQL works by listening on
> port 3306: user accounts don't come into that.  With our bastion servers
> all such ports are blocked and can only be accessed via tunnels
> (implemented by stunnel).  I think you need to discuss this with your
> sysadmins: if it works under mysql or for ODBC (isql) it will work with
> the corresponding R packages.

Of course, most definitely. I usually log in to the external machine using 
ssh, then access the MySQL server using the MySQL username and password.
I probably asked for too much from R, since logging to an external machine 
needs a secure connection.
I'll talk to our sysadmin for local advice, thanks again.

Adrian


-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From dwinsemius at comcast.net  Sat Dec 15 18:31:37 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 15 Dec 2007 17:31:37 +0000 (UTC)
Subject: [R] Analyzing Publications from Pubmed via XML
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
Message-ID: <Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>

"Farrel Buchinsky" <fjbuch at gmail.com> wrote in
news:bd93cdad0712141216s23071d27n17d87a487ad06950 at mail.gmail.com: 

> On Dec 13, 2007 11:35 PM, Robert Gentleman <rgentlem at fhcrc.org> wrote:
>> or just try looking in the annotate package from Bioconductor
>>
> 
> Yip. annotate seems to be the most streamlined way to do this.
> 1) How does one turn the list that is created into a dataframe whose
> column names are along the lines of date, title, journal, authors etc

Gabor's example already did that task.

> 2) I have already created a standing search in pubmed using MyNCBI.
> There are many ways I can feed those results to the pubmed() function.
> The most brute force way of doing it is by running the search and
> outputing the data as a UI List and getting that into the pubmed
> brackets. A way that involved more finesse would allow me to create a
> rss feed based on my search and then give the rss feed url to the
> pubmed function. Or perhaps once could just plop the query inside the
> pubmed functions
> pubmed(somefunction("Laryngeal Neoplasms"[MeSH] AND "Papilloma"[MeSH])
> OR ((("recurrence"[TIAB] NOT Medline[SB]) OR "recurrence"[MeSH Terms]
> OR recurrent[Text Word]) AND respiratory[All Fields] AND
> (("papilloma"[TIAB] NOT Medline[SB]) OR "papilloma"[MeSH Terms] OR
> papillomatosis[Text Word])))
> 
> Does "somefunction" exist?

I could not find it. The pubmed function appears to assume that you will 
already have a list of PMIDs. When I set up a function to take an 
arbitrary  PubMed search string (quoted by the user) and return the 
PMIDs, I had success by following Gabor's example:

> pm.srch<- function (){
   srch.stem <-"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term="
   query <-as.character(scan(file="",what="character"))
   doc <-xmlTreeParse(paste(srch.stem,query,sep=""),isURL = TRUE, 
         useInternalNodes = TRUE)
   sapply(c("//Id"), xpathApply, doc = doc, fun = xmlValue)
     }
> pm.srch()
1: "laryngeal neoplasms[mh]"
2: 
Read 1 item
      //Id      
 [1,] "18042931"
 [2,] "18038886"
 [3,] "17978930"
 [4,] "17974987"
 [5,] "17972507"
 [6,] "17970149"
 [7,] "17967299"
 [8,] "17962724"
 [9,] "17954109"
[10,] "17942038"
[11,] "17940076"
[12,] "17848290"
[13,] "17848288"
[14,] "17848287"
[15,] "17848278"
[16,] "17938330"
[17,] "17938329"
[18,] "17918311"
[19,] "17910347"
[20,] "17908862"

Emboldened by that minor success, I pushed on. Pubmed said your example 
was malformed and I took their suggested modification:
("Laryngeal Neoplasms"[MeSH] AND "Papilloma"[MeSH]) OR (("recurrence"[TIAB] NOT Medline[SB]) OR "recurrence"[MeSH Terms] OR recurrent[Text Word]) AND respiratory[All Fields] AND (("papilloma"[TIAB] NOT Medline[SB]) OR "papilloma"[MeSH Terms] OR papillomatosis[Text Word]) 

That returned 400+ citations, and I put it into a text file.

After quite a bit of hacking (in the sense of ineffective chopping with 
a dull ax), I finally came up with:

pm.srch<- function (){
  srch.stem<-"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term="
  query<-readLines(con=file.choose())
  query<-gsub("\\\"","",x=query)
  doc<-xmlTreeParse(paste(srch.stem,query,sep=""),isURL = TRUE, 
                     useInternalNodes = TRUE)
  return(sapply(c("//Id"), xpathApply, doc = doc, fun = xmlValue) )
     }

pm.srch()  #choosing the search-file
      //Id      
 [1,] "18046565"
 [2,] "17978930"
 [3,] "17975511"
 [4,] "17935912"
 [5,] "17851940"
 [6,] "17765779"
 [7,] "17688640"
 [8,] "17638782"
 [9,] "17627059"
[10,] "17599582"
[11,] "17589729"
[12,] "17585283"
[13,] "17568846"
[14,] "17560665"
[15,] "17547971"
[16,] "17428551"
[17,] "17419899"
[18,] "17419519"
[19,] "17385606"
[20,] "17366752"

-- 
David Winsemius


From Dan.Kelley at dal.ca  Sat Dec 15 18:53:23 2007
From: Dan.Kelley at dal.ca (dankelley)
Date: Sat, 15 Dec 2007 09:53:23 -0800 (PST)
Subject: [R] how to add an index to a vignette?
In-Reply-To: <14339838.post@talk.nabble.com>
References: <14339838.post@talk.nabble.com>
Message-ID: <14353154.post@talk.nabble.com>


Oh, I see.  The answer is just to do as usual, e.g. as follows, and R CMD
build simply takes care of it.

<code>
...
\usepackage{makeidx} 
\makeindex
...
\begin{document}
...
\index{something}
...
\printindex
\end{document}
</code>
-- 
View this message in context: http://www.nabble.com/how-to-add-an-index-to-a-vignette--tp14339838p14353154.html
Sent from the R help mailing list archive at Nabble.com.


From dusa.adrian at gmail.com  Sat Dec 15 19:19:54 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sat, 15 Dec 2007 20:19:54 +0200
Subject: [R] connecting RMySQL to and external server
In-Reply-To: <971536df0712150854n55d5c15eoaf300527cd8ce844@mail.gmail.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<200712151833.04227.dusa.adrian@gmail.com>
	<971536df0712150854n55d5c15eoaf300527cd8ce844@mail.gmail.com>
Message-ID: <200712152019.54927.dusa.adrian@gmail.com>

On Saturday 15 December 2007, you wrote:
> Use ssh forwarding to forward local port 3307 to remote port 3306
> specifying the remote account and password. ?Then if you use local port
> 3306 you can access your local version of MySQL and if you
> use port 3307 you can access the remote version. ? There is some
> info on the MySQL site. ?First test it out by running the mysql command
> line program accessing the remote data base via port 3307 and once
> that works you know its ok and you can try RMySQL or RODBC packages.

Thanks Gabor, it is a little bit of a foreign language for me (at the moment) 
but I'm sure your hints will be relevant to our sysadmin.
I do want to understand this stuff myself, just need more digging in the 
manuals.

Cheers,
Adrian


-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
? ? ? ? ? +40 21 3120210 / int.101


From vistocco at unicas.it  Sat Dec 15 19:24:07 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Sat, 15 Dec 2007 19:24:07 +0100
Subject: [R] Array dimnames
In-Reply-To: <ea7af0460712141031n5c5df115y55e4d116615e7474@mail.gmail.com>
References: <ea7af0460712141031n5c5df115y55e4d116615e7474@mail.gmail.com>
Message-ID: <47641BC7.7040709@unicas.it>

dave mitchell wrote:
> Dear all,
> Possibly a rudimentary question, however any help is greatly appreciated.  I
> am sorting a large matrix into an array of dim(p(i),q,3).  I put each entry
> into a corresponding matrix (1 of the 3) based on some criteria.  I figure
> this will assist me in condensing code as I can loop through the 3rd
> dimension of the array instead of generating 3 separate matrices and using
> the same block of code 3 times.  My question is how to get the colnames of
> the 3 nested matrices in the array to match the colnames of the data
> matrix.  In other words...
>   
Denoting with array3d the array and with matrix2d the data matrix:

colnames(array3d)=colnames(array3d)

Otherwise, using dimnames:
dimnames(array3d)=list(NULL,colnames(array3d),NULL)

You can operate using the "[" operator, that is:
array3d[,"region",] to extract the region columns from each dimension.

You obtain the same effect using the column number, i.e.
array3d[,2,]

domenico
> DATA:
>    Exp   region   Qty   Ct  ...q
> 1   S      CB     3.55  2.15  .
> 2   S      TG     4.16  2.18  .
> 3   C      OO     2.36  3.65  .
> 4   C   .           .         .
> .   .     .           .       .
> .   .       .           .     .
> .   .         .           .   .
> p   ...........................
>
>
>
> ARRAY
> 1
>    [,1]   [,2]    [,3] [,4]...q
> 1   SOME DATA WILL FILL THIS   .
> 2   .  .              .        .
> 3   .   .              .       .
> 4   .    .              .      .
> .   .     .              .     .
> .   .      .              .    .
> .   .       .              .   .
> P(1) ...........................
>
> 2
>    [,1]   [,2]    [,3] [,4]...q
> 1   SOME DATA WILL FILL THIS   .
> 2   .  .              .        .
> 3   .   .              .       .
> 4   .    .              .      .
> .   .     .              .     .
> .   .      .              .    .
> .   .       .              .   .
> P(2) ...........................
> 3
>    [,1]   [,2]    [,3] [,4]...q
> 1   SOME DATA WILL FILL THIS   .
> 2   .  .              .        .
> 3   .   .              .       .
> 4   .    .              .      .
> .   .     .              .     .
> .   .      .              .    .
> .   .       .              .   .
> P(3) ...........................
>
> Again, how to get those [,1], [,2]... to read (and operate) in the same
> fashion as the column names in the data matrix?  Also, am I interpreting the
> dimensions of the array incorrectly?  Please feel free to post any helpful
> links on the subject, as I have found "dimnames" and "array" in the R-help
> documentation unhelpful.  Any help is greatly appreciated.
>
> Dave Mitchell
> Undergraduate: Statistics and Mathematics,
> University of Illinois, Urbana-Champaign
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From vistocco at unicas.it  Sat Dec 15 19:30:44 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Sat, 15 Dec 2007 19:30:44 +0100
Subject: [R] Array dimnames
In-Reply-To: <47641BC7.7040709@unicas.it>
References: <ea7af0460712141031n5c5df115y55e4d116615e7474@mail.gmail.com>
	<47641BC7.7040709@unicas.it>
Message-ID: <47641D54.9050908@unicas.it>

Sorry, there were mistakes in variable names... (I realized only after 
pressed the send button)

Domenico Vistocco wrote:
> dave mitchell wrote:
>   
>> Dear all,
>> Possibly a rudimentary question, however any help is greatly appreciated.  I
>> am sorting a large matrix into an array of dim(p(i),q,3).  I put each entry
>> into a corresponding matrix (1 of the 3) based on some criteria.  I figure
>> this will assist me in condensing code as I can loop through the 3rd
>> dimension of the array instead of generating 3 separate matrices and using
>> the same block of code 3 times.  My question is how to get the colnames of
>> the 3 nested matrices in the array to match the colnames of the data
>> matrix.  In other words...
>>   
>>     
> Denoting with array3d the array and with matrix2d the data matrix:
>
> colnames(array3d)=colnames(array3d)
>   
colnames(array3d)=colnames(matrix2d)
> Otherwise, using dimnames:
> dimnames(array3d)=list(NULL,colnames(array3d),NULL)
>   
dimnames(array3d)=list(NULL,colnames(matrix2d),NULL)
> You can operate using the "[" operator, that is:
> array3d[,"region",] to extract the region columns from each dimension.
>
> You obtain the same effect using the column number, i.e.
> array3d[,2,]
>
> domenico
>   
>> DATA:
>>    Exp   region   Qty   Ct  ...q
>> 1   S      CB     3.55  2.15  .
>> 2   S      TG     4.16  2.18  .
>> 3   C      OO     2.36  3.65  .
>> 4   C   .           .         .
>> .   .     .           .       .
>> .   .       .           .     .
>> .   .         .           .   .
>> p   ...........................
>>
>>
>>
>> ARRAY
>> 1
>>    [,1]   [,2]    [,3] [,4]...q
>> 1   SOME DATA WILL FILL THIS   .
>> 2   .  .              .        .
>> 3   .   .              .       .
>> 4   .    .              .      .
>> .   .     .              .     .
>> .   .      .              .    .
>> .   .       .              .   .
>> P(1) ...........................
>>
>> 2
>>    [,1]   [,2]    [,3] [,4]...q
>> 1   SOME DATA WILL FILL THIS   .
>> 2   .  .              .        .
>> 3   .   .              .       .
>> 4   .    .              .      .
>> .   .     .              .     .
>> .   .      .              .    .
>> .   .       .              .   .
>> P(2) ...........................
>> 3
>>    [,1]   [,2]    [,3] [,4]...q
>> 1   SOME DATA WILL FILL THIS   .
>> 2   .  .              .        .
>> 3   .   .              .       .
>> 4   .    .              .      .
>> .   .     .              .     .
>> .   .      .              .    .
>> .   .       .              .   .
>> P(3) ...........................
>>
>> Again, how to get those [,1], [,2]... to read (and operate) in the same
>> fashion as the column names in the data matrix?  Also, am I interpreting the
>> dimensions of the array incorrectly?  Please feel free to post any helpful
>> links on the subject, as I have found "dimnames" and "array" in the R-help
>> documentation unhelpful.  Any help is greatly appreciated.
>>
>> Dave Mitchell
>> Undergraduate: Statistics and Mathematics,
>> University of Illinois, Urbana-Champaign
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>     
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From p.dalgaard at biostat.ku.dk  Sat Dec 15 19:41:13 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sat, 15 Dec 2007 19:41:13 +0100
Subject: [R] X11 uses font size 25 although 28 was requested
In-Reply-To: <47640D68.6070501@gfz-potsdam.de>
References: <47640D68.6070501@gfz-potsdam.de>
Message-ID: <47641FC9.2060009@biostat.ku.dk>

Andre Jung wrote:
> Hi,
>
> I'm not sure if the real English warning is the same. I translated it
> from German. R came up with the following message:
>
> -------------------------------------------------------
> Warning message:
> In title(main = sinc.exp) :
> X11 nutzt Schriftgr??e 25 obwohl 28 angefordert war
> [engl.: X11 uses font size 25 although 28 was requested]
> -------------------------------------------------------
>
> system:
> R version 2.6.1 (2007-11-26)
> on openSuSE 10.3
> installed via YaST from
> http://download.opensuse.org/repositories/home:/dsteuer/
> (provided by Detlef Steuer)
>
> any suggestions?
>   
It's only a warning!

It usually means that your system is set up with non-scalable versions 
of the adobe fonts, so that some fonts are substituted. This can be 
considered a good thing: The fixed-size bitmap fonts have been carefully 
designed for maximum legibility, whereas the scaled fonts look awful... 
However, the former are only available in pixel sizes 8, 10, 11, 12, 14, 
17, 18, 20, 24, 25, 34, if you install both the 75 dpi and  100dpi font 
sets.

> Thanks!
> andre
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bhatticr at tulane.edu  Sat Dec 15 20:52:44 2007
From: bhatticr at tulane.edu (Chad R. Bhatti)
Date: Sat, 15 Dec 2007 13:52:44 -0600 (CST)
Subject: [R] R memory limits and good memory management
Message-ID: <Pine.LNX.4.63.0712151342090.29270@lax.math.tulane.edu>

Hello,

I am not fluent in computer hardware or software development so I will try 
to be as precise as I can.  I am going to build some R routines to analyze 
large intraday financial data sets.  For this project I think that I 
need be concerned 
about R memory limits and good memory management/practices.  The following 
R help 
page states that the limit on individual objects (a vector) is around 200 
million elements.

http://sekhon.berkeley.edu/base/html/Memory-limits.html

What about data frames? And I suppose that I need to be aware of the 
objects in the working memory (overwrite as many objects as possible)?

Note, I am new to the issue of good memory management.  Any specific 
details on R memory and good practical advice are welcomed.

Thanks,

Chad R. Bhatti


From pedrosmarques at portugalmail.pt  Sat Dec 15 16:13:34 2007
From: pedrosmarques at portugalmail.pt (pedrosmarques at portugalmail.pt)
Date: Sat, 15 Dec 2007 15:13:34 +0000
Subject: [R] Using boxplot in a daily time series
Message-ID: <1197731614.4763ef1e5b2f3@gold3.portugalmail.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071215/4b52b617/attachment.pl 

From vistocco at unicas.it  Sat Dec 15 22:56:04 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Sat, 15 Dec 2007 22:56:04 +0100
Subject: [R] Using boxplot in a daily time series
In-Reply-To: <1197731614.4763ef1e5b2f3@gold3.portugalmail.pt>
References: <1197731614.4763ef1e5b2f3@gold3.portugalmail.pt>
Message-ID: <47644D74.1030301@unicas.it>

 From your post it is not clear how the data are organized. Supposing 
they are in a data frame you could use the ~ sintax.

For example:
timeColumn=as.Date("01-01-1970") + 1:500
timeSeries=rnorm(500)
df=data.frame(time=timeColumn, index=timeSeries)
> > head(df)
>      time      index
> 1 1-01-20 -0.6382554
> 2 1-01-21 -2.0346649
> 3 1-01-22 -0.4900213
> 4 1-01-23  0.7311806
> 5 1-01-24  0.9386528
> 6 1-01-25  0.7868129
boxplot(index~months(time), data=df)

Perhaps you would order the 12 boxes according to the months:
monthColumn=ordered(months(df$time), levels= c("January", "February", 
"March",
            "April", "May", "June", "July", "August", "September", 
"October", "November", "December"))
boxplot(df$index~monthColumn)

domenico

pedrosmarques at portugalmail.pt wrote:
> Hi all, 
>
> I'm trying to plot my daily time series, with 3650 observations( 10 years), with boxplot but I want 12 boxes, one for each month, can anyone help me doing that.
>
> Best regards
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From dylan.beaudette at gmail.com  Sat Dec 15 23:02:02 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Sat, 15 Dec 2007 14:02:02 -0800
Subject: [R] Using boxplot in a daily time series
In-Reply-To: <1197731614.4763ef1e5b2f3@gold3.portugalmail.pt>
References: <1197731614.4763ef1e5b2f3@gold3.portugalmail.pt>
Message-ID: <200712151402.02416.dylan.beaudette@gmail.com>

On Saturday 15 December 2007 07:13:34 am pedrosmarques at portugalmail.pt wrote:
> Hi all,
>
> I'm trying to plot my daily time series, with 3650 observations( 10 years),
> with boxplot but I want 12 boxes, one for each month, can anyone help me
> doing that.
>
> Best regards
>

Sure,

there are several approaches to this, but in general thinking about your data 
in terms of 'grouping' factors can be helpful

example:

# 1 year's worth of dates:
d <- strptime(1:365, format="%j")

# some simulated data
x <- rlnorm(365)

# combine them into a DF 
dx <- data.frame(date=d, value=x)

# plot the data, note that x-axis is in dates:
plot(dx)

# now generate a grouping factor. how about months:
dx$month <- format(dx$date, format="%B")

# box and whisker plot for data *grouped* by month
boxplot(value ~ month, data=dx, las=3)


cheers,

Dylan


From manuel.castejon at unileon.es  Sat Dec 15 23:12:07 2007
From: manuel.castejon at unileon.es (Manuel =?iso-8859-1?Q?Castej=F3n_Limas?=)
Date: Sat, 15 Dec 2007 23:12:07 +0100 (CET)
Subject: [R] train nnet
In-Reply-To: <mailman.24.1197716404.14713.r-help@r-project.org>
References: <mailman.24.1197716404.14713.r-help@r-project.org>
Message-ID: <15822.81.172.111.38.1197756727.squirrel@www.unileon.es>


Dear Ilh

I was planning to dedicate a few hours this christmas to improve the AMORE
package. I would like to explore the possibilities of the openMP library
and to  programm a few more training algorithms. Right now the rbf
extension is ready but under testing.

I think that what you ask for would not need much time to get done. If you
provide me a detailed definition of your needs I can include them in my
to-do list. For that purpose it could be adequate to send me an email to
mcasl at unileon.es

Best wishes.

Manuel



> Hi R-helpers,
>
> Can some one tell me how to train 'mynn' of this type?:
> mynn <- nnet(y ~ x1 + ..+ x8, data = lgist, size = 2, rang = 0.1,
> decay = 5e-4, maxit = 200)
>
> I assume that this nn is untrained, and to train I have to split the
> original data into train:test data set,
> do leave-one-out refitting to refine the weights (please straighten
> this up if I was wrong).
>
> I just don't know how to do it in R. Is 'training' and
> 'training.reports' in (AMORE) able to do it?
>
> Thank you for any light on this.
>
> Ilh
>
>
>


-- 
Manuel Castej?n Limas
?rea de Proyectos de Ingenier?a
Departamento de Ingenier?as Mec?nica, Inform?tica y Aeroespacial
Universidad de Le?n.
e-mail: manuel.castejon en unileon.es

Escuela de Ingenier?as Industrial e Inform?tica
Campus de Vegazana, sn. 24007. Le?n


From patrick at pdrechsler.de  Sun Dec 16 01:04:43 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Sun, 16 Dec 2007 01:04:43 +0100
Subject: [R] [HH] changing font size in interaction2wt
Message-ID: <87lk7vqzc4.fsf@pdrechsler.de>

Hi,

I would like to change font sizes (more generally the cex) in plots
produced by the function "interaction2wt" of the "HH" package.

Here is an example (from the interaction2wt example) of what I have
tried:

--8<---------------cut here---------------start------------->8---
rm(list = ls(all = TRUE))
rm(list = c(ls()))

library("HH")

## From the example:
vulcan <- read.table(hh("datasets/vulcan.dat"), header = TRUE)

## I would like to change the size (cex) of the axes description,
## labels and the overall line widths.
trellis.par.set(par.xlab.text = list(cex = 0.3),
                par.ylab.text = list(cex = 0.3),
                axis.text = list(cex = 0.3),
                par.main.text = list(cex = 0.3),
                par.sub.text = list(cex = 0.3),
                add.text = list(cex = 0.3))

foo <- interaction2wt(wear ~ filler + pretreat + raw, data = vulcan)
plot(foo)
--8<---------------cut here---------------end--------------->8---

Thankful for any pointers,

Patrick


From spencer.graves at pdf.com  Sun Dec 16 02:29:57 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 15 Dec 2007 17:29:57 -0800
Subject: [R] Rd files with unknown encoding?
In-Reply-To: <Pine.LNX.4.64.0712131006040.7003@gannet.stats.ox.ac.uk>
References: <4760AFED.5050908@pdf.com>
	<Pine.LNX.4.64.0712131006040.7003@gannet.stats.ox.ac.uk>
Message-ID: <47647F95.10505@pdf.com>

Dear Prof. Ripley: 

      Thanks very much.  I did as you suggested, which I'll outline here 
to make it easier for anyone else who might have a similar problem: 

           * Read the offending *.Rd file in R using 'readLines' 

           * Applied 'iconv' to the character vector, following the last 
example in the help file.  This translated all offending characters into 
a multi-character sequence starting with '<'. 

           * Used 'regexpr' to find all occurrences of '<'. 

      The latter identified other uses of '<' but produced a 
sufficiently short list that I was able to find the problems fairly 
easily. 

      Thanks again.
      Spencer Graves   
p.s.  And in the future, I will refer 'Rd' questions to 'R-devel', per 
your suggestion. 

Prof Brian Ripley wrote:
> On Wed, 12 Dec 2007, Spencer Graves wrote:
>
>   
>>      How can I identify the problem generating a warning in R CMD check
>> for "Rd files with unknown encoding"?
>>
>>      Google identified an email from John Fox with a reply from Brian
>> Ripley about this last 12 Jun 2007.
>>     
>
> But not on this list:
>
> https://stat.ethz.ch/pipermail/r-devel/2007-June/046055.html
>
> R-devel would have been more appropriate for this too.
>
>   
>>  This suggests that I may have accidentally entered some possibly 
>> non-printing character into the offending Rd file.  The message tells me 
>> which file, but I don't know which lines in the file.  Is there some way 
>> of finding the offending character(s) without laboriously running R CMD 
>> check after deleting different portions of the file until I isolate the 
>> problem?
>>     
>
> I did say so in that thread:
>
> https://stat.ethz.ch/pipermail/r-devel/2007-June/046061.html
>
> You can do much the same in R via iconv("", "C", sub="byte"), provided you 
> can read the file in (it may not be representable in your current 
> locale, but you could run R in a Latin-1 locale, if your OS has one).
>
>


From dwinsemius at comcast.net  Sun Dec 16 04:13:52 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 16 Dec 2007 03:13:52 +0000 (UTC)
Subject: [R] Analyzing Publications from Pubmed via XML
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
Message-ID: <Xns9A07E22B53BDAdNOTwinscomcast@80.91.229.13>

David Winsemius <dwinsemius at comcast.net> wrote in
news:Xns9A077F740B4A0dNOTwinscomcast at 80.91.229.13: 

> "Farrel Buchinsky" <fjbuch at gmail.com> wrote in
> news:bd93cdad0712141216s23071d27n17d87a487ad06950 at mail.gmail.com: 
> 
>> On Dec 13, 2007 11:35 PM, Robert Gentleman <rgentlem at fhcrc.org>
>> wrote: 
>>> or just try looking in the annotate package from Bioconductor
>>>
>> 
>> Yip. annotate seems to be the most streamlined way to do this.
>> 1) How does one turn the list that is created into a dataframe whose
>> column names are along the lines of date, title, journal, authors etc
> 
> Gabor's example already did that task.
> 

Actually the object returned by Gabor's method was a list of lists. Here 
is one way (probably very inefficient) of getting "doc" into a 
data.frame:

colvals <-sapply(c("//title", "//author", "//category"), xpathApply, 
           doc = doc, fun = xmlValue)

titles=as.vector(unlist(colvals[1])[3:17])

# needed to drop extraneous titles for search name and an NCBI header
#>str(colvals)
#List of 3
# $ //title   :List of 17
#  ..$ : chr "PubMed: (\"Laryngeal Neoplasm..."
#  ..$ : chr "NCBI PubMed"

authors=colvals[[2]]
jrnls=colvals[[3]]

# not sure why, but trying to do it in one step failed:
#  cites<-data.frame(titles=as.vector(unlist(colvals[1])[3:17]),  
#                     authors=colvals[[2]],jnrls=colvals[[3]])
# Error in data.frame(titles = as.vector(unlist(colvals[1])[3:17]), 
# authors = colvals[[2]],  : 
#  arguments imply differing number of rows: 15, 1
# but the following worked

 cites<-data.frame(titles=as.vector(titles))
 cites$author<-authors
 cites$jrnls<-jrnls
 cites

I am still wondering how to extract material that does not have an XML 
tag.  Each item looks like:

 <item>
   <title>Gastroesophageal reflux in patients with recurrent laryngeal 
papillomatosis.</title>
   <link>http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?
tmpl=NoSidebarfile&amp;db=PubMed&amp;cmd=Retrieve&amp;list_uids=17589729
&amp;dopt=Abstract</link>
   <description>
    <![CDATA[
    <table border="0" width="100%"><tr><td align="left"><a 
href="http://www.scielo.br/scielo.php?script=sci_arttext&amp;pid=S0034-
72992007000200011&amp;lng=en&amp;nrm=iso&amp;tlng=en"><img 
src="http://www.ncbi.nlm.nih.gov/entrez/query/egifs/http:--www.scielo.br-
img-scielo_en.gif" border="0"/></a> </td><td align="right"><a 
href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?
db=PubMed&amp;cmd=Display&amp;dopt=PubMed_PubMed&amp;from_uid=17589729">
Related Articles</a></td></tr></table>
        <p><b>Gastroesophageal reflux in patients with recurrent 
laryngeal papillomatosis.</b></p>
        <p>Rev Bras Otorrinolaringol (Engl Ed). 2007 Mar-Apr;73(2):210-4
</p>
        <p>Authors:  Pignatari SS, Liriano RY, Avelino MA, Testa JR, 
Fujita R, De Marco EK</p>
        <p>Evidence of a relation between gastroesophaeal reflux and 
pediatric respiratory disorders increases every year. Many respiratory 
symptoms and clinical conditions such as stridor, chronic cough, and 
recurrent pneumonia and bronchitis appear to be related to 
gastroesophageal reflux. Some studies have also suggested that 
gastroesophageal reflux may be associated with recurrent laryngeal 
papillomatosis, contributing to its recurrence and severity. AIM: the aim 
of this study was to verify the frequency and intensity of 
gastroesophageal reflux in children with recurrent laryngeal 
papillomatosis. MATERIAL AND METHODS: ten children of both genders, aged 
between 3 and 12 years, presenting laryngeal papillomatosis, were 
included in this study. The children underwent 24-hour double-probe pH-
metry. RESULTS: fifty percent of the patients had evidence of 
gastroesophageal reflux at the distal sphincter; 90% presented reflux at 
the proximal sphincter. CONCLUSION: the frequency of proximal 
gastroesophageal reflux is significantly increased in patients with 
recurrent laryngeal papillomatosis.</p>
        <p>PMID: 17589729 [PubMed - in process]</p>    ]]>
   </description>
   <author>Pignatari SS, Liriano RY, Avelino MA, Testa JR, Fujita R, De 
Marco EK</author>
   <category>Rev Bras Otorrinolaringol (Engl Ed)</category>
   <guid isPermaLink="false">PubMed:17589729</guid>
  </item>

I would like to access, for instance, the PMID or the abstract within the 
<description> element, but I do not think that they have names in the the 
same way that <author> or <category> have xml named nodes. I suspect that 
getting the output in a different format, say as MEDLINE, might produce 
output that was tagged more completely.

-- 
David Winsemius


From deleeuw at stat.ucla.edu  Sun Dec 16 05:04:46 2007
From: deleeuw at stat.ucla.edu (Jan de Leeuw)
Date: Sat, 15 Dec 2007 20:04:46 -0800
Subject: [R] not a package (yet): derivatives of generalized eigen/singular
	pairs
Message-ID: <4D95AB5D-7E49-4791-88BE-52F50B32D447@stat.ucla.edu>

but maybe of use to some:

http://gifi.stat.ucla.edu/psychoR/derivatives

Computes generalized eigenvalue solutions Ax=\lambda Bx
and generalized singular value solutions Rz=\gamma Px
and R'x=\gamma Qy for matrices that are differentiable
functions of a vector of parameters. Along with the
decomposition the code returns arrays with all first-order
partial derivatives of the values/vector wrt the parameters.
This makes it easy to apply the delta method to CA, CCA, MCA,
PCA and friends. But on a really large example you may be
in for an unpleasant surprise.

There is also a paper with the formulas in the same directory.

Note that Patrick Mair and I are in the process of converting some of  
the
psychoR repository into CRAN packages and JSS papers/snippets.

===
Jan de Leeuw; Distinguished Professor and Chair, UCLA Department of  
Statistics;
Director: UCLA Center for Environmental Statistics (CES);
Editor: Journal of Multivariate Analysis, Journal of Statistical  
Software;
US mail: 8125 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095-1554
phone (310)-825-9550;  fax (310)-206-5658;  email: deleeuw at stat.ucla.edu
.mac: jdeleeuw +++ skype: j_deleeuw +++ homepage: http://www.cuddyvalley.org
   
-------------------------------------------------------------------------------------------------
           No matter where you go, there you are. --- Buckaroo Banzai
                    http://gifi.stat.ucla.edu/sounds/nomatter.au


From jrbeamer at gmail.com  Sun Dec 16 05:31:50 2007
From: jrbeamer at gmail.com (John Beamer)
Date: Sun, 16 Dec 2007 06:31:50 +0200
Subject: [R] Changing the origin in polar.plot in plotrix package
Message-ID: <506c08d50712152031s1b1dc911v57974528f8af6f87@mail.gmail.com>

I am trying to draw a polar plot, which is easy enough to do in the
plotrix package through the polar.plot function.

However I would like to change the origin of the length vector. For
instance all my length values are between 75 and 85, so instead of
having the origin as 0 (the default) I'd like it to be, say, 50.

Is there any way do to this in the polar.plot function, or if not is
there an additional package that will accomplish this.

I am running R 2.5 and the latest version of plotrix.

Thanks for your help
John Beamer
(http://www.hardballtimes.com)


From p_connolly at slingshot.co.nz  Sun Dec 16 05:32:16 2007
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Sun, 16 Dec 2007 17:32:16 +1300
Subject: [R] connecting RMySQL to and external server
In-Reply-To: <200712142055.15148.dusa.adrian@gmail.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
Message-ID: <20071216043216.GS6584@slingshot.co.nz>

On Fri, 14-Dec-2007 at 08:55PM +0200, Adrian Dusa wrote:

|> 
|> Dear list,
|> 
|> I learned how to connect R to a local MySQL server, using:
|> drv <- dbDriver("MySQL")
|> con <- dbConnect(drv, user="root", password="mypass", dbname="mydb")
|> 
|> Is it possible to connect R in this way to an external server (on a different 
|> machine, with a different IP)?

Do you use a ~/.my.cnf file?

There are many settings that can be made there.  Talk to your sysadmin
about those.

HTH

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From ggrothendieck at gmail.com  Sun Dec 16 05:39:30 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 15 Dec 2007 23:39:30 -0500
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <Xns9A07E22B53BDAdNOTwinscomcast@80.91.229.13>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<Xns9A07E22B53BDAdNOTwinscomcast@80.91.229.13>
Message-ID: <971536df0712152039r7d41c950w773da384b941332d@mail.gmail.com>

If we can assume that the abstract is always the 4th paragraph then we
can try something like this:

library(XML)
doc <- xmlTreeParse("http://eutils.ncbi.nlm.nih.gov/entrez/eutils/erss.cgi?rss_guid=0_JYbpsax0ZAAPnOd7nFAX-29fXDpTk5t8M4hx9ytT-",
isURL = TRUE, useInternalNodes = TRUE, trim = TRUE)

out <- cbind(
	Author = unlist(xpathApply(doc, "//author", xmlValue)),
	PMID = gsub(".*:", "", unlist(xpathApply(doc, "//guid", xmlValue))),
	Abstract = unlist(xpathApply(doc, "//description",
		function(x) {
			on.exit(free(doc2))
			doc2 <- htmlTreeParse(xmlValue(x)[[1]], asText = TRUE,
				useInternalNodes = TRUE, trim = TRUE)
			xpathApply(doc2, "//p[4]", xmlValue)
		}
	)))
free(doc)
substring(out, 1, 25) # display first 25 chars of each field


The last line produces (it may look messed up in this email):

> substring(out, 1, 25) # display it
      Author                      PMID       Abstract
 [1,] " Goon P, Sonnex C, Jani P" "18046565" "Human papillomaviruses (H"
 [2,] " Rad MH, Alizadeh E, Ilkh" "17978930" "Recurrent laryngeal papil"
 [3,] " Lee LA, Cheng AJ, Fang T" "17975511" "OBJECTIVES:: Papillomas o"
 [4,] " Gerein V, Schmandt S, Ba" "17935912" "BACKGROUND: Human papillo"
 [5,] " Hopp R, Natarajan N, Lew" "17908862" ""
 [6,] " Preuss SF, Klussmann JP," "17851940" "CONCLUSIONS: The presente"
 [7,] " Mouadeb DA, Belafsky PC"  "17765779" "OBJECTIVES: The 585nm pul"
 [8,] " Thompson L"               "17702311" ""
 [9,] " Schaffer A, Brotherton J" "17688640" ""
[10,] " Stephen JK, Vaught LE, C" "17638782" "OBJECTIVE: To investigate"
[11,] " Shah KV, Westra WH"       "17627059" ""
[12,] " Koufman JA, Rees CJ, Fra" "17599582" "BACKGROUND: Unsedated off"
[13,] " Akst LM, Broadhurst MS, " "17592395" ""
[14,] " Pignatari SS, Liriano RY" "17589729" "Evidence of a relation be"


On Dec 15, 2007 10:13 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> David Winsemius <dwinsemius at comcast.net> wrote in
> news:Xns9A077F740B4A0dNOTwinscomcast at 80.91.229.13:
>
> > "Farrel Buchinsky" <fjbuch at gmail.com> wrote in
> > news:bd93cdad0712141216s23071d27n17d87a487ad06950 at mail.gmail.com:
> >
> >> On Dec 13, 2007 11:35 PM, Robert Gentleman <rgentlem at fhcrc.org>
> >> wrote:
> >>> or just try looking in the annotate package from Bioconductor
> >>>
> >>
> >> Yip. annotate seems to be the most streamlined way to do this.
> >> 1) How does one turn the list that is created into a dataframe whose
> >> column names are along the lines of date, title, journal, authors etc
> >
> > Gabor's example already did that task.
> >
>
> Actually the object returned by Gabor's method was a list of lists. Here
> is one way (probably very inefficient) of getting "doc" into a
> data.frame:
>
> colvals <-sapply(c("//title", "//author", "//category"), xpathApply,
>           doc = doc, fun = xmlValue)
>
> titles=as.vector(unlist(colvals[1])[3:17])
>
> # needed to drop extraneous titles for search name and an NCBI header
> #>str(colvals)
> #List of 3
> # $ //title   :List of 17
> #  ..$ : chr "PubMed: (\"Laryngeal Neoplasm..."
> #  ..$ : chr "NCBI PubMed"
>
> authors=colvals[[2]]
> jrnls=colvals[[3]]
>
> # not sure why, but trying to do it in one step failed:
> #  cites<-data.frame(titles=as.vector(unlist(colvals[1])[3:17]),
> #                     authors=colvals[[2]],jnrls=colvals[[3]])
> # Error in data.frame(titles = as.vector(unlist(colvals[1])[3:17]),
> # authors = colvals[[2]],  :
> #  arguments imply differing number of rows: 15, 1
> # but the following worked
>
>  cites<-data.frame(titles=as.vector(titles))
>  cites$author<-authors
>  cites$jrnls<-jrnls
>  cites
>
> I am still wondering how to extract material that does not have an XML
> tag.  Each item looks like:
>
>  <item>
>   <title>Gastroesophageal reflux in patients with recurrent laryngeal
> papillomatosis.</title>
>   <link>http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?
> tmpl=NoSidebarfile&amp;db=PubMed&amp;cmd=Retrieve&amp;list_uids=17589729
> &amp;dopt=Abstract</link>
>   <description>
>    <![CDATA[
>    <table border="0" width="100%"><tr><td align="left"><a
> href="http://www.scielo.br/scielo.php?script=sci_arttext&amp;pid=S0034-
> 72992007000200011&amp;lng=en&amp;nrm=iso&amp;tlng=en"><img
> src="http://www.ncbi.nlm.nih.gov/entrez/query/egifs/http:--www.scielo.br-
> img-scielo_en.gif" border="0"/></a> </td><td align="right"><a
> href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?
> db=PubMed&amp;cmd=Display&amp;dopt=PubMed_PubMed&amp;from_uid=17589729">
> Related Articles</a></td></tr></table>
>        <p><b>Gastroesophageal reflux in patients with recurrent
> laryngeal papillomatosis.</b></p>
>        <p>Rev Bras Otorrinolaringol (Engl Ed). 2007 Mar-Apr;73(2):210-4
> </p>
>        <p>Authors:  Pignatari SS, Liriano RY, Avelino MA, Testa JR,
> Fujita R, De Marco EK</p>
>        <p>Evidence of a relation between gastroesophaeal reflux and
> pediatric respiratory disorders increases every year. Many respiratory
> symptoms and clinical conditions such as stridor, chronic cough, and
> recurrent pneumonia and bronchitis appear to be related to
> gastroesophageal reflux. Some studies have also suggested that
> gastroesophageal reflux may be associated with recurrent laryngeal
> papillomatosis, contributing to its recurrence and severity. AIM: the aim
> of this study was to verify the frequency and intensity of
> gastroesophageal reflux in children with recurrent laryngeal
> papillomatosis. MATERIAL AND METHODS: ten children of both genders, aged
> between 3 and 12 years, presenting laryngeal papillomatosis, were
> included in this study. The children underwent 24-hour double-probe pH-
> metry. RESULTS: fifty percent of the patients had evidence of
> gastroesophageal reflux at the distal sphincter; 90% presented reflux at
> the proximal sphincter. CONCLUSION: the frequency of proximal
> gastroesophageal reflux is significantly increased in patients with
> recurrent laryngeal papillomatosis.</p>
>        <p>PMID: 17589729 [PubMed - in process]</p>    ]]>
>   </description>
>   <author>Pignatari SS, Liriano RY, Avelino MA, Testa JR, Fujita R, De
> Marco EK</author>
>   <category>Rev Bras Otorrinolaringol (Engl Ed)</category>
>   <guid isPermaLink="false">PubMed:17589729</guid>
>  </item>
>
> I would like to access, for instance, the PMID or the abstract within the
> <description> element, but I do not think that they have names in the the
> same way that <author> or <category> have xml named nodes. I suspect that
> getting the output in a different format, say as MEDLINE, might produce
> output that was tagged more completely.
>
>
> --
> David Winsemius
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrbeamer at gmail.com  Sun Dec 16 05:54:21 2007
From: jrbeamer at gmail.com (John Beamer)
Date: Sun, 16 Dec 2007 06:54:21 +0200
Subject: [R] Changing the origin in polar.plot in plotrix package
In-Reply-To: <506c08d50712152031s1b1dc911v57974528f8af6f87@mail.gmail.com>
References: <506c08d50712152031s1b1dc911v57974528f8af6f87@mail.gmail.com>
Message-ID: <506c08d50712152054w464ef704ib408ff9ea0dfb8f@mail.gmail.com>

I am trying to draw a polar plot, which is easy enough to do in the
plotrix package through the polar.plot function.

However I would like to change the origin of the length vector. For
instance all my length values are between 75 and 85, so instead of
having the origin as 0 (the default) I'd like it to be, say, 50.

Is there any way do to this in the polar.plot function, or if not is
there an additional package that will accomplish this.

I am running R 2.5 and the latest version of plotrix.

Thanks for your help
John Beamer
(http://www.hardballtimes.com)


From h.wickham at gmail.com  Sun Dec 16 06:37:03 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 15 Dec 2007 23:37:03 -0600
Subject: [R] Changing the origin in polar.plot in plotrix package
In-Reply-To: <506c08d50712152031s1b1dc911v57974528f8af6f87@mail.gmail.com>
References: <506c08d50712152031s1b1dc911v57974528f8af6f87@mail.gmail.com>
Message-ID: <f8e6ff050712152137n399fee26xfd1ac05fcfb70103@mail.gmail.com>

Hi John,

One alternative would be to use ggplot2 with a polar coordinate system:

library(ggplot2)
qplot(mpg, wt, data=mtcars) + coord_polar()
qplot(mpg, wt, data=mtcars, ylim=c(3,4)) + coord_polar()

etc.  You can see more examples of polar coordinates at
http://had.co.nz/ggplot2/coord_polar.html

although I'm still working on good axes etc for plots in polar coordinates.

Hadley

On 12/15/07, John Beamer <jrbeamer at gmail.com> wrote:
> I am trying to draw a polar plot, which is easy enough to do in the
> plotrix package through the polar.plot function.
>
> However I would like to change the origin of the length vector. For
> instance all my length values are between 75 and 85, so instead of
> having the origin as 0 (the default) I'd like it to be, say, 50.
>
> Is there any way do to this in the polar.plot function, or if not is
> there an additional package that will accomplish this.
>
> I am running R 2.5 and the latest version of plotrix.
>
> Thanks for your help
> John Beamer
> (http://www.hardballtimes.com)
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://had.co.nz/


From p_connolly at slingshot.co.nz  Sun Dec 16 10:02:29 2007
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Sun, 16 Dec 2007 22:02:29 +1300
Subject: [R] Importing Large Dataset from Excel
In-Reply-To: <475FB968.4020907@biostat.ku.dk>
References: <1197429329.475f525155179@webmail.fas.harvard.edu>
	<644e1f320712111924y17a0d367t6876a49c53010c7e@mail.gmail.com>
	<475F9BE0.6030503@sciviews.org> <475FB968.4020907@biostat.ku.dk>
Message-ID: <20071216090229.GT6584@slingshot.co.nz>

On Wed, 12-Dec-2007 at 11:35AM +0100, Peter Dalgaard wrote:

|> Philippe Grosjean wrote:
|> > The problem is often a misspecification of the comment.char argument. 
|> > For read.table(), it defaults to '#'. This means that everywhere you 
|> > have a '#' char in your Excel sheet, the rest of the line is ignored. 
|> > This results in a different number of items per line.
|> >
|> > You should better use read.csv() which provides better default arguments 
|> > for your particular problem.
|> > Best,
|> >
|> >   
|> Or read.delim/read.delim2, which should be even better at TAB-separated
|> files.
|> 
|> In general, be very suspicious of read.table() with such files, not only
|> because of the '#' but also because it expects columns separated by
|> _arbitrary_ amounts of whitespace. I.e., n TABs  counts as one, so empty
|> fields are skipped over.

I don't recall that happening with TABs, but a problem can arise when
the last (rightmost) column has more than a few empty cells.
Occasionally, I've had to resort to adding a dummy column on the
right, but as Peter suggests, read.delim is usually less involved.


-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From bgreen at dyson.brisnet.org.au  Sun Dec 16 10:14:29 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Sun, 16 Dec 2007 19:14:29 +1000
Subject: [R] improving a bar graph
In-Reply-To: <mailman.15.1184320808.31491.r-help@stat.math.ethz.ch>
References: <mailman.15.1184320808.31491.r-help@stat.math.ethz.ch>
Message-ID: <20071216091012.7644F595551@borg.st.net.au>

Hello,

Below is the code for a basic bar graph. I was seeking advice 
regarding the following:

(a)  For each time period there are values from 16 people. How I can 
change the colour value so that each person has a different colour, 
which recurs across each of the three graphs/tie epriods?

(b) I have seen much more sophisticated examples using lattice (e.g 
each person has a separate panel/plot). I am open to alternative code 
as to how I could present this data.

Time1 <- c(9.0,6.0,1.0,5.0,7.0,9.0,5.0,7.5,6.0,8.0,5.0,5.0,9.0,4.0,5.0,5.0)
Time2 <- c (10,5,3,3,3,6,7,8,5,8,7,7,9,8,5,3)
Time3 <- c (10,0,3,0,0,6,0,0,0,0,0,0,0,0,0,0)
df  <- rbind  (Time1, Time2, Time3)
dft <- (t(df))
dft
barplot(dft, beside = TRUE, main= "Risk score by assessment", xlab = 
" Score", ylab = "frequency", col="blue")


Any assistance is much appreciated,

regards

Bob Green


From jim at bitwrit.com.au  Sun Dec 16 10:29:40 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sun, 16 Dec 2007 20:29:40 +1100
Subject: [R] Changing the origin in polar.plot in plotrix package
In-Reply-To: <506c08d50712152054w464ef704ib408ff9ea0dfb8f@mail.gmail.com>
References: <506c08d50712152031s1b1dc911v57974528f8af6f87@mail.gmail.com>
	<506c08d50712152054w464ef704ib408ff9ea0dfb8f@mail.gmail.com>
Message-ID: <4764F004.1090209@bitwrit.com.au>

John Beamer wrote:
> I am trying to draw a polar plot, which is easy enough to do in the
> plotrix package through the polar.plot function.
> 
> However I would like to change the origin of the length vector. For
> instance all my length values are between 75 and 85, so instead of
> having the origin as 0 (the default) I'd like it to be, say, 50.
> 
> Is there any way do to this in the polar.plot function, or if not is
> there an additional package that will accomplish this.
> 
> I am running R 2.5 and the latest version of plotrix.
> 
Hi John,
The short answer is no.

The long answer is:

radial.plot<-function(lengths,radial.pos=NULL,
  labels=NA,label.pos=NULL,start=0,clockwise=FALSE,
  rp.type="r",label.prop=1.1,main="",xlab="",ylab="",
  line.col=par("fg"),mar=c(2,2,3,2),show.grid=TRUE,
  show.radial.grid=TRUE,grid.col="gray",
  grid.bg="transparent",grid.left=FALSE,
  point.symbols=NULL,point.col=NULL,
  show.centroid=FALSE,radial.lim=NULL,...) {

  if(is.null(radial.lim)) radial.lim<-range(lengths)
  length.dim<-dim(lengths)
  if(is.null(length.dim)) {
   npoints<-length(lengths)
   nsets<-1
   lengths<-matrix(lengths,nrow=1)
  }
  else {
   npoints<-length.dim[2]
   nsets<-length.dim[1]
   lengths<-as.matrix(lengths)
  }
  lengths<-lengths-radial.lim[1]
  lengths[lengths<0]<-NA
  if(is.null(radial.pos))
   radial.pos<-seq(0,pi*(2-2/npoints),length=npoints)
  radial.pos.dim<-dim(radial.pos)
  if(is.null(radial.pos.dim))
   radial.pos<-matrix(rep(radial.pos,nsets),
    nrow=nsets,byrow=TRUE)
  else radial.pos<-as.matrix(radial.pos)
  if(clockwise) radial.pos<--radial.pos
  if(start) radial.pos<-radial.pos+start
  if(show.grid) {
   grid.pos<-pretty(radial.lim)
   if(grid.pos[1] <= radial.lim[1])
    grid.pos<-grid.pos[-1]
   maxlength<-max(grid.pos-radial.lim[1])
   angles<-seq(0,1.96*pi,by=0.04*pi)
  }
  else {
   grid.pos<-NA
   maxlength<-diff(radial.lim)
  }
  oldpar<-par("xpd","mar","pty")
  par(mar=mar,pty="s")
  plot(c(-maxlength,maxlength),c(-maxlength,maxlength),
   type="n",axes=FALSE,
   main=main,xlab=xlab,ylab=ylab)
  par(xpd=TRUE)
  if(length(line.col) < nsets) line.col<-1:nsets
  rp.type<-unlist(strsplit(rp.type,""))
  if(match("s",rp.type,0)) {
   if(is.null(point.symbols)) point.symbols<-1:nsets
   if(length(point.symbols)<nsets)
    point.symbols<-rep(point.symbols,length.out=nsets)
   if(is.null(point.col)) point.col<-1:nsets
   if(length(point.col)<nsets)
    point.col<-rep(point.col,length.out=nsets)
  }
  # split up rp.type if there is a combination of displays
  for(i in 1:nsets) {
   # get the vector of x positions
   xpos<-cos(radial.pos[i,])*lengths[i,]
   # get the vector of y positions
   ypos<-sin(radial.pos[i,])*lengths[i,]
   # plot radial lines if rp.type == "r"
   if(match("r",rp.type,0))
    segments(0,0,xpos,ypos,col=line.col[i],...)
   if(match("p",rp.type,0))
    polygon(xpos,ypos,border=line.col[i],col=NA,...)
   if(match("s",rp.type,0))
    points(xpos,ypos,pch=point.symbols[i],
     col=point.col[i],...)
   if(show.centroid)
    points(mean(xpos),mean(ypos),col=point.col[i],
     pch=point.symbols[i],cex=2,...)
  }
  if(missing(labels)) {
   label.pos<-seq(0,1.8*pi,length=9)
   labels<-as.character(round(label.pos,2))
  }
  if(clockwise) label.pos<--label.pos
  if(start) label.pos<-label.pos+start
  xpos<-cos(label.pos)*maxlength
  ypos<-sin(label.pos)*maxlength
  if(show.radial.grid) segments(0,0,xpos,ypos,col=grid.col)
  xpos<-cos(label.pos)*maxlength*label.prop
  ypos<-sin(label.pos)*maxlength*label.prop
  boxed.labels(xpos,ypos,labels,ypad=0.7,border=FALSE)
  if(show.grid) {
   print(grid.pos)
   for(i in seq(length(grid.pos),1,by=-1)) {
    xpos<-cos(angles)*(grid.pos[i]-radial.lim[1])
    ypos<-sin(angles)*(grid.pos[i]-radial.lim[1])
    polygon(xpos,ypos,border=grid.col,col=grid.bg)
   }
   ypos<-rep(-maxlength/15,length(grid.pos))
   boxed.labels(grid.pos-radial.lim[1],ypos,
    as.character(grid.pos),border=FALSE)
  }
  par(oldpar)
}

But you will have to test it, and I would appreciate knowing if anything 
breaks. Thanks.

Jim


From jim at bitwrit.com.au  Sun Dec 16 10:50:39 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sun, 16 Dec 2007 20:50:39 +1100
Subject: [R] improving a bar graph
In-Reply-To: <20071216091012.7644F595551@borg.st.net.au>
References: <mailman.15.1184320808.31491.r-help@stat.math.ethz.ch>
	<20071216091012.7644F595551@borg.st.net.au>
Message-ID: <4764F4EF.40405@bitwrit.com.au>

Bob Green wrote:
> Hello,
> 
> Below is the code for a basic bar graph. I was seeking advice 
> regarding the following:
> 
> (a)  For each time period there are values from 16 people. How I can 
> change the colour value so that each person has a different colour, 
> which recurs across each of the three graphs/tie epriods?
> 
> (b) I have seen much more sophisticated examples using lattice (e.g 
> each person has a separate panel/plot). I am open to alternative code 
> as to how I could present this data.
> 
> Time1 <- c(9.0,6.0,1.0,5.0,7.0,9.0,5.0,7.5,6.0,8.0,5.0,5.0,9.0,4.0,5.0,5.0)
> Time2 <- c (10,5,3,3,3,6,7,8,5,8,7,7,9,8,5,3)
> Time3 <- c (10,0,3,0,0,6,0,0,0,0,0,0,0,0,0,0)
> df  <- rbind  (Time1, Time2, Time3)
> dft <- (t(df))
> dft
> barplot(dft, beside = TRUE, main= "Risk score by assessment", xlab = 
> " Score", ylab = "frequency", col="blue")
> 
Hi Bob,

library(plotrix)
barplot(dft, beside = TRUE, main= "Risk score by assessment", xlab =
  " Score", ylab = "frequency",
  col=color.scale(1:16,c(1,0.5,0),c(0,1,0),c(0,0.5,1)))
legend(44,9,1:16,
  fill=color.scale(1:16,c(1,0.5,0),c(0,1,0),c(0,0.5,1)))

Jim


From ravi.longia at gmail.com  Sun Dec 16 13:02:07 2007
From: ravi.longia at gmail.com (Ravi Longia)
Date: Sun, 16 Dec 2007 12:02:07 +0000
Subject: [R] Using mean of truncated normal
Message-ID: <bac068d10712160402m178a90c8t7e5dec4a871efab4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071216/f0734022/attachment.pl 

From jrbeamer at gmail.com  Sun Dec 16 14:08:11 2007
From: jrbeamer at gmail.com (John Beamer)
Date: Sun, 16 Dec 2007 15:08:11 +0200
Subject: [R] Changing the origin in polar.plot in plotrix package
In-Reply-To: <4764F004.1090209@bitwrit.com.au>
References: <506c08d50712152031s1b1dc911v57974528f8af6f87@mail.gmail.com>
	<506c08d50712152054w464ef704ib408ff9ea0dfb8f@mail.gmail.com>
	<4764F004.1090209@bitwrit.com.au>
Message-ID: <506c08d50712160508q77fe34b4w6f92e569af66c123@mail.gmail.com>

Thanks Jim.

That works well, thanks. Is there a way I can specify the range as it
seems to adjust automatically. Also I was hoping to plot multiple
datasets on the same chart but with different colours.

In the normal plot() I can do this with col =  .... I was hoping to do
the same with points.col but it doesn't seem to work.

Thanks a lot for your help
John

On 16/12/2007, Jim Lemon <jim at bitwrit.com.au> wrote:
> John Beamer wrote:
> > I am trying to draw a polar plot, which is easy enough to do in the
> > plotrix package through the polar.plot function.
> >
> > However I would like to change the origin of the length vector. For
> > instance all my length values are between 75 and 85, so instead of
> > having the origin as 0 (the default) I'd like it to be, say, 50.
> >
> > Is there any way do to this in the polar.plot function, or if not is
> > there an additional package that will accomplish this.
> >
> > I am running R 2.5 and the latest version of plotrix.
> >
> Hi John,
> The short answer is no.
>
> The long answer is:
>
> radial.plot<-function(lengths,radial.pos=NULL,
>   labels=NA,label.pos=NULL,start=0,clockwise=FALSE,
>   rp.type="r",label.prop=1.1,main="",xlab="",ylab="",
>   line.col=par("fg"),mar=c(2,2,3,2),show.grid=TRUE,
>   show.radial.grid=TRUE,grid.col="gray",
>   grid.bg="transparent",grid.left=FALSE,
>   point.symbols=NULL,point.col=NULL,
>   show.centroid=FALSE,radial.lim=NULL,...) {
>
>   if(is.null(radial.lim)) radial.lim<-range(lengths)
>   length.dim<-dim(lengths)
>   if(is.null(length.dim)) {
>    npoints<-length(lengths)
>    nsets<-1
>    lengths<-matrix(lengths,nrow=1)
>   }
>   else {
>    npoints<-length.dim[2]
>    nsets<-length.dim[1]
>    lengths<-as.matrix(lengths)
>   }
>   lengths<-lengths-radial.lim[1]
>   lengths[lengths<0]<-NA
>   if(is.null(radial.pos))
>    radial.pos<-seq(0,pi*(2-2/npoints),length=npoints)
>   radial.pos.dim<-dim(radial.pos)
>   if(is.null(radial.pos.dim))
>    radial.pos<-matrix(rep(radial.pos,nsets),
>     nrow=nsets,byrow=TRUE)
>   else radial.pos<-as.matrix(radial.pos)
>   if(clockwise) radial.pos<--radial.pos
>   if(start) radial.pos<-radial.pos+start
>   if(show.grid) {
>    grid.pos<-pretty(radial.lim)
>    if(grid.pos[1] <= radial.lim[1])
>     grid.pos<-grid.pos[-1]
>    maxlength<-max(grid.pos-radial.lim[1])
>    angles<-seq(0,1.96*pi,by=0.04*pi)
>   }
>   else {
>    grid.pos<-NA
>    maxlength<-diff(radial.lim)
>   }
>   oldpar<-par("xpd","mar","pty")
>   par(mar=mar,pty="s")
>   plot(c(-maxlength,maxlength),c(-maxlength,maxlength),
>    type="n",axes=FALSE,
>    main=main,xlab=xlab,ylab=ylab)
>   par(xpd=TRUE)
>   if(length(line.col) < nsets) line.col<-1:nsets
>   rp.type<-unlist(strsplit(rp.type,""))
>   if(match("s",rp.type,0)) {
>    if(is.null(point.symbols)) point.symbols<-1:nsets
>    if(length(point.symbols)<nsets)
>     point.symbols<-rep(point.symbols,length.out=nsets)
>    if(is.null(point.col)) point.col<-1:nsets
>    if(length(point.col)<nsets)
>     point.col<-rep(point.col,length.out=nsets)
>   }
>   # split up rp.type if there is a combination of displays
>   for(i in 1:nsets) {
>    # get the vector of x positions
>    xpos<-cos(radial.pos[i,])*lengths[i,]
>    # get the vector of y positions
>    ypos<-sin(radial.pos[i,])*lengths[i,]
>    # plot radial lines if rp.type == "r"
>    if(match("r",rp.type,0))
>     segments(0,0,xpos,ypos,col=line.col[i],...)
>    if(match("p",rp.type,0))
>     polygon(xpos,ypos,border=line.col[i],col=NA,...)
>    if(match("s",rp.type,0))
>     points(xpos,ypos,pch=point.symbols[i],
>      col=point.col[i],...)
>    if(show.centroid)
>     points(mean(xpos),mean(ypos),col=point.col[i],
>      pch=point.symbols[i],cex=2,...)
>   }
>   if(missing(labels)) {
>    label.pos<-seq(0,1.8*pi,length=9)
>    labels<-as.character(round(label.pos,2))
>   }
>   if(clockwise) label.pos<--label.pos
>   if(start) label.pos<-label.pos+start
>   xpos<-cos(label.pos)*maxlength
>   ypos<-sin(label.pos)*maxlength
>   if(show.radial.grid) segments(0,0,xpos,ypos,col=grid.col)
>   xpos<-cos(label.pos)*maxlength*label.prop
>   ypos<-sin(label.pos)*maxlength*label.prop
>   boxed.labels(xpos,ypos,labels,ypad=0.7,border=FALSE)
>   if(show.grid) {
>    print(grid.pos)
>    for(i in seq(length(grid.pos),1,by=-1)) {
>     xpos<-cos(angles)*(grid.pos[i]-radial.lim[1])
>     ypos<-sin(angles)*(grid.pos[i]-radial.lim[1])
>     polygon(xpos,ypos,border=grid.col,col=grid.bg)
>    }
>    ypos<-rep(-maxlength/15,length(grid.pos))
>    boxed.labels(grid.pos-radial.lim[1],ypos,
>     as.character(grid.pos),border=FALSE)
>   }
>   par(oldpar)
> }
>
> But you will have to test it, and I would appreciate knowing if anything
> breaks. Thanks.
>
> Jim
>


From ggrothendieck at gmail.com  Sun Dec 16 15:00:10 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Dec 2007 09:00:10 -0500
Subject: [R] improving a bar graph
In-Reply-To: <20071216091012.7644F595551@borg.st.net.au>
References: <mailman.15.1184320808.31491.r-help@stat.math.ethz.ch>
	<20071216091012.7644F595551@borg.st.net.au>
Message-ID: <971536df0712160600g2e79dd29l14942731cf9d7d1e@mail.gmail.com>

Try this:

matplot(t(dft), type = "o", xlab = "Time", ylab = "Score")


On Dec 16, 2007 4:14 AM, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
> Hello,
>
> Below is the code for a basic bar graph. I was seeking advice
> regarding the following:
>
> (a)  For each time period there are values from 16 people. How I can
> change the colour value so that each person has a different colour,
> which recurs across each of the three graphs/tie epriods?
>
> (b) I have seen much more sophisticated examples using lattice (e.g
> each person has a separate panel/plot). I am open to alternative code
> as to how I could present this data.
>
> Time1 <- c(9.0,6.0,1.0,5.0,7.0,9.0,5.0,7.5,6.0,8.0,5.0,5.0,9.0,4.0,5.0,5.0)
> Time2 <- c (10,5,3,3,3,6,7,8,5,8,7,7,9,8,5,3)
> Time3 <- c (10,0,3,0,0,6,0,0,0,0,0,0,0,0,0,0)
> df  <- rbind  (Time1, Time2, Time3)
> dft <- (t(df))
> dft
> barplot(dft, beside = TRUE, main= "Risk score by assessment", xlab =
> " Score", ylab = "frequency", col="blue")
>
>
> Any assistance is much appreciated,
>
> regards
>
> Bob Green
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ravi.longia at gmail.com  Sun Dec 16 15:22:13 2007
From: ravi.longia at gmail.com (ravirrrr)
Date: Sun, 16 Dec 2007 06:22:13 -0800 (PST)
Subject: [R] Truncated normal distribution
In-Reply-To: <bac068d10712141532r150fdb39s3d2127b53dfb8223@mail.gmail.com>
References: <bac068d10712141532r150fdb39s3d2127b53dfb8223@mail.gmail.com>
Message-ID: <14361967.post@talk.nabble.com>


I have the following code, where we need to solve for mu and sigma, when we
have mut and sdt. Can you suggest how to use a solve function in R to do
that? I am new to R and am not sure how to go from defining the functions,
to solving for them. 

Thanks


truncated <- function(x)
{

mu=x[1];
sigma=x[2];


f <- function(x) (1/(sigma*sqrt(2*pi)))*exp(-(x-mu)^2/(2*sigma^2));

pdf.fun <- function(x) x*f(x);

sd.fun <- function(x) (x)^2*f(x);

st=integrate(sd.fun,lower=-Inf,upper=1)$value;

a=integrate(pdf.fun,lower=-Inf,upper=1)$value;

a1=integrate(f,lower=-Inf,upper=1)$value;

mut <- a/a1;
sdt <- sqrt((st/a1)-(a/a1)^2);

} 


-- 
View this message in context: http://www.nabble.com/Truncated-normal-distribution-tp14348951p14361967.html
Sent from the R help mailing list archive at Nabble.com.


From tom.soyer at gmail.com  Sun Dec 16 15:23:19 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Sun, 16 Dec 2007 08:23:19 -0600
Subject: [R] question about the aggregate function with respect to order of
	levels of grouping elements
Message-ID: <65cc7bdf0712160623k20f69089xf1d6efad05040588@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071216/fa75d37b/attachment.pl 

From cgenolin at u-paris10.fr  Sun Dec 16 15:25:20 2007
From: cgenolin at u-paris10.fr (cgenolin at u-paris10.fr)
Date: Sun, 16 Dec 2007 15:25:20 +0100
Subject: [R] clean programming
In-Reply-To: <mailman.15.1197802803.31564.r-help@r-project.org>
References: <mailman.15.1197802803.31564.r-help@r-project.org>
Message-ID: <20071216152520.qo8jnpahvk0k0w4k@icare.u-paris10.fr>

Hello the list,

I am trying to write a "cleanProgramming" function to test the 
procedure I use. For example, I want to be sure that I am not using 
globals variables. The function "findGlobals" detect that.

To list the globals used in function "fun", the syntax is : 
"findGlobals(fun,FALSE)$variable"

My problem is that I want to use it in a function, something like :

cleanProg <- function(name){
  if(length(findGlobals(name,FALSE)$variable>0){
    cat("Warnings: there is globals is function ",name,"\a\n")
  }
}

But findGlobals take a function as first argument, not a variable 
containing a function name.

Anyway to solve that?

Christophe


----------------------------------------------------------------
Ce message a ete envoye par IMP, grace a l'Universite Paris 10 Nanterre


From ggrothendieck at gmail.com  Sun Dec 16 15:41:18 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Dec 2007 09:41:18 -0500
Subject: [R] clean programming
In-Reply-To: <20071216152520.qo8jnpahvk0k0w4k@icare.u-paris10.fr>
References: <mailman.15.1197802803.31564.r-help@r-project.org>
	<20071216152520.qo8jnpahvk0k0w4k@icare.u-paris10.fr>
Message-ID: <971536df0712160641q35edc1d9v2b1f1b4833bf88f5@mail.gmail.com>

Its a FAQ

http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-can-I-turn-a-string-into-a-variable_003f

On Dec 16, 2007 9:25 AM,  <cgenolin at u-paris10.fr> wrote:
> Hello the list,
>
> I am trying to write a "cleanProgramming" function to test the
> procedure I use. For example, I want to be sure that I am not using
> globals variables. The function "findGlobals" detect that.
>
> To list the globals used in function "fun", the syntax is :
> "findGlobals(fun,FALSE)$variable"
>
> My problem is that I want to use it in a function, something like :
>
> cleanProg <- function(name){
>  if(length(findGlobals(name,FALSE)$variable>0){
>    cat("Warnings: there is globals is function ",name,"\a\n")
>  }
> }
>
> But findGlobals take a function as first argument, not a variable
> containing a function name.
>
> Anyway to solve that?
>
> Christophe
>
>
> ----------------------------------------------------------------
> Ce message a ete envoye par IMP, grace a l'Universite Paris 10 Nanterre
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Sun Dec 16 15:44:04 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 16 Dec 2007 09:44:04 -0500
Subject: [R] clean programming
In-Reply-To: <20071216152520.qo8jnpahvk0k0w4k@icare.u-paris10.fr>
References: <mailman.15.1197802803.31564.r-help@r-project.org>
	<20071216152520.qo8jnpahvk0k0w4k@icare.u-paris10.fr>
Message-ID: <476539B4.7050302@stats.uwo.ca>

On 16/12/2007 9:25 AM, cgenolin at u-paris10.fr wrote:
> Hello the list,
> 
> I am trying to write a "cleanProgramming" function to test the 
> procedure I use. For example, I want to be sure that I am not using 
> globals variables. The function "findGlobals" detect that.
> 
> To list the globals used in function "fun", the syntax is : 
> "findGlobals(fun,FALSE)$variable"
> 
> My problem is that I want to use it in a function, something like :
> 
> cleanProg <- function(name){
>   if(length(findGlobals(name,FALSE)$variable>0){
>     cat("Warnings: there is globals is function ",name,"\a\n")
>   }
> }
> 
> But findGlobals take a function as first argument, not a variable 
> containing a function name.
> 
> Anyway to solve that?

Use get() to find an object with a given name.  You need to be careful 
to specify where it should look (the envir argument); typically 
parent.frame() is appropriate, but your cleanProg function should allow 
the user to override this.

Duncan Murdoch


From ggrothendieck at gmail.com  Sun Dec 16 15:50:15 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Dec 2007 09:50:15 -0500
Subject: [R] question about the aggregate function with respect to order
	of levels of grouping elements
In-Reply-To: <65cc7bdf0712160623k20f69089xf1d6efad05040588@mail.gmail.com>
References: <65cc7bdf0712160623k20f69089xf1d6efad05040588@mail.gmail.com>
Message-ID: <971536df0712160650i41394691l778a5a3e608c76f6@mail.gmail.com>

This does look strange.  Note that aggregate.zoo in the zoo package
would work here:

> library(zoo)
> aggregate(zoo(rnum, dts), as.yearmon, sum)
   Jan 2001    Feb 2001    Mar 2001    Apr 2001    May 2001    Jun 2001
 4.43610085  0.49842227  7.52139932  1.47917343 10.64459923 -1.22530586
   Jul 2001    Aug 2001    Sep 2001    Oct 2001    Nov 2001    Dec 2001
 8.19563685  1.57626974  1.28842871  2.50540074  0.71156951  0.54118342
   Jan 2002    Feb 2002    Mar 2002    Apr 2002    May 2002    Jun 2002
-0.41292840 -2.41301496  3.23783551  0.63914807 -1.46357402  2.91651492
   Jul 2002    Aug 2002    Sep 2002    Oct 2002    Nov 2002    Dec 2002
 2.17263290 -2.30981022 -9.60701788  1.16504368 -3.07038254  1.38281927
   Jan 2003    Feb 2003    Mar 2003    Apr 2003    May 2003    Jun 2003
 4.48761479  2.42455090 -0.03743888  1.11223001 -4.07988016 -1.15116293
   Jul 2003    Aug 2003    Sep 2003    Oct 2003    Nov 2003    Dec 2003
-7.15292576 -2.34231702 -0.48132751 11.74252191  2.51063034 -4.35801058


On Dec 16, 2007 9:23 AM, tom soyer <tom.soyer at gmail.com> wrote:
> Hi,
>
> I am using aggregate() to add up groups of data according to year and month.
> It seems that the function aggregate() automatically sorts the levels of
> factors of the grouping elements, even if the order of the levels of factors
> is supplied. I am wondering if this is a bug, or if I missed something
> important. Below is an example that shows what I mean. Does anyone know if
> this is just the way the aggregate function works, or are there ways
> to force aggregate() to keep the order of levels of factors supplied by the
> grouping elements? Thanks!
>
> library(chron)
> dts=seq.dates("1/1/01","12/31/03")
> rnum=rnorm(1:length(dts))
> df=data.frame(date=dts,obs=rnum)
> agg=aggregate(df[,2],list(year=years(df[,1]),month=months(df[,1])),sum)
> levels(agg$month) # aggregate() automatically generates levels sorted by
> alphabet.
>
> [1] "Apr" "Aug" "Dec" "Feb" "Jan" "Jul" "Jun" "Mar" "May" "Nov" "Oct" "Sep"
>
> fmonth=factor(months(df[,1]))
> levels(fmonth) # factor() automatically generates the correct order of
> levels.
>
> [1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct" "Nov" "Dec"
>
>
> agg2=aggregate(df[,2],list(year=years(df[,1]),month=fmonth),sum)
> levels(agg2$month) # even if a factor with levels in the correct order is
> supplied, aggregate(), sortsthe levels by alphabet regardless.
>
> [1] "Apr" "Aug" "Dec" "Feb" "Jan" "Jul" "Jun" "Mar" "May" "Nov" "Oct" "Sep"
>
>
> --
> Tom
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Sun Dec 16 15:54:50 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Dec 2007 09:54:50 -0500
Subject: [R] question about the aggregate function with respect to order
	of levels of grouping elements
In-Reply-To: <971536df0712160650i41394691l778a5a3e608c76f6@mail.gmail.com>
References: <65cc7bdf0712160623k20f69089xf1d6efad05040588@mail.gmail.com>
	<971536df0712160650i41394691l778a5a3e608c76f6@mail.gmail.com>
Message-ID: <971536df0712160654n7cfeeb70y8d0be60590d52109@mail.gmail.com>

In fact, even ordinary aggegate works ok with zoo's as.yearmon:

> aggregate(rnum, list(dts = as.yearmon(dts)), sum)
        dts           x
1  Jan 2001  4.43610085
2  Feb 2001  0.49842227
3  Mar 2001  7.52139932
4  Apr 2001  1.47917343
5  May 2001 10.64459923
6  Jun 2001 -1.22530586
7  Jul 2001  8.19563685
8  Aug 2001  1.57626974
9  Sep 2001  1.28842871
10 Oct 2001  2.50540074
11 Nov 2001  0.71156951
12 Dec 2001  0.54118342
13 Jan 2002 -0.41292840
14 Feb 2002 -2.41301496
15 Mar 2002  3.23783551
16 Apr 2002  0.63914807
17 May 2002 -1.46357402
18 Jun 2002  2.91651492
19 Jul 2002  2.17263290
20 Aug 2002 -2.30981022
21 Sep 2002 -9.60701788
22 Oct 2002  1.16504368
23 Nov 2002 -3.07038254
24 Dec 2002  1.38281927
25 Jan 2003  4.48761479
26 Feb 2003  2.42455090
27 Mar 2003 -0.03743888
28 Apr 2003  1.11223001
29 May 2003 -4.07988016
30 Jun 2003 -1.15116293
31 Jul 2003 -7.15292576
32 Aug 2003 -2.34231702
33 Sep 2003 -0.48132751
34 Oct 2003 11.74252191
35 Nov 2003  2.51063034
36 Dec 2003 -4.35801058


On Dec 16, 2007 9:50 AM, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> This does look strange.  Note that aggregate.zoo in the zoo package
> would work here:
>
> > library(zoo)
> > aggregate(zoo(rnum, dts), as.yearmon, sum)
>   Jan 2001    Feb 2001    Mar 2001    Apr 2001    May 2001    Jun 2001
>  4.43610085  0.49842227  7.52139932  1.47917343 10.64459923 -1.22530586
>   Jul 2001    Aug 2001    Sep 2001    Oct 2001    Nov 2001    Dec 2001
>  8.19563685  1.57626974  1.28842871  2.50540074  0.71156951  0.54118342
>   Jan 2002    Feb 2002    Mar 2002    Apr 2002    May 2002    Jun 2002
> -0.41292840 -2.41301496  3.23783551  0.63914807 -1.46357402  2.91651492
>   Jul 2002    Aug 2002    Sep 2002    Oct 2002    Nov 2002    Dec 2002
>  2.17263290 -2.30981022 -9.60701788  1.16504368 -3.07038254  1.38281927
>   Jan 2003    Feb 2003    Mar 2003    Apr 2003    May 2003    Jun 2003
>  4.48761479  2.42455090 -0.03743888  1.11223001 -4.07988016 -1.15116293
>   Jul 2003    Aug 2003    Sep 2003    Oct 2003    Nov 2003    Dec 2003
> -7.15292576 -2.34231702 -0.48132751 11.74252191  2.51063034 -4.35801058
>
>
>
> On Dec 16, 2007 9:23 AM, tom soyer <tom.soyer at gmail.com> wrote:
> > Hi,
> >
> > I am using aggregate() to add up groups of data according to year and month.
> > It seems that the function aggregate() automatically sorts the levels of
> > factors of the grouping elements, even if the order of the levels of factors
> > is supplied. I am wondering if this is a bug, or if I missed something
> > important. Below is an example that shows what I mean. Does anyone know if
> > this is just the way the aggregate function works, or are there ways
> > to force aggregate() to keep the order of levels of factors supplied by the
> > grouping elements? Thanks!
> >
> > library(chron)
> > dts=seq.dates("1/1/01","12/31/03")
> > rnum=rnorm(1:length(dts))
> > df=data.frame(date=dts,obs=rnum)
> > agg=aggregate(df[,2],list(year=years(df[,1]),month=months(df[,1])),sum)
> > levels(agg$month) # aggregate() automatically generates levels sorted by
> > alphabet.
> >
> > [1] "Apr" "Aug" "Dec" "Feb" "Jan" "Jul" "Jun" "Mar" "May" "Nov" "Oct" "Sep"
> >
> > fmonth=factor(months(df[,1]))
> > levels(fmonth) # factor() automatically generates the correct order of
> > levels.
> >
> > [1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct" "Nov" "Dec"
> >
> >
> > agg2=aggregate(df[,2],list(year=years(df[,1]),month=fmonth),sum)
> > levels(agg2$month) # even if a factor with levels in the correct order is
> > supplied, aggregate(), sortsthe levels by alphabet regardless.
> >
> > [1] "Apr" "Aug" "Dec" "Feb" "Jan" "Jul" "Jun" "Mar" "May" "Nov" "Oct" "Sep"
> >
> >
> > --
> > Tom
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From jholtman at gmail.com  Sun Dec 16 16:04:25 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 16 Dec 2007 10:04:25 -0500
Subject: [R] question about the aggregate function with respect to order
	of levels of grouping elements
In-Reply-To: <65cc7bdf0712160623k20f69089xf1d6efad05040588@mail.gmail.com>
References: <65cc7bdf0712160623k20f69089xf1d6efad05040588@mail.gmail.com>
Message-ID: <644e1f320712160704w1e5532eco6cabb2b6211f4c05@mail.gmail.com>

What version of R are you using?  Here is the output I got with 2.6.1:

> library(chron)
> dts=seq.dates("1/1/01","12/31/03")
> rnum=rnorm(1:length(dts))
> df=data.frame(date=dts,obs=rnum)
> agg=aggregate(df[,2],list(year=years(df[,1]),month=months(df[,1])),sum)
> levels(agg$month) # aggregate() automatically generates levels sorted by alphabet.
 [1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct" "Nov" "Dec"
>
> fmonth=factor(months(df[,1]))
> levels(fmonth) # factor() automatically generates the correct order of  levels.
 [1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct" "Nov" "Dec"
> agg2=aggregate(df[,2],list(year=years(df[,1]),month=fmonth),sum)
> levels(agg2$month) # even if a factor with levels in the correct order is supplied, aggregate(), sortsthe levels by alphabet regardless.
 [1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct" "Nov" "Dec"
>
>

Order seems to be correct.

On Dec 16, 2007 9:23 AM, tom soyer <tom.soyer at gmail.com> wrote:
> Hi,
>
> I am using aggregate() to add up groups of data according to year and month.
> It seems that the function aggregate() automatically sorts the levels of
> factors of the grouping elements, even if the order of the levels of factors
> is supplied. I am wondering if this is a bug, or if I missed something
> important. Below is an example that shows what I mean. Does anyone know if
> this is just the way the aggregate function works, or are there ways
> to force aggregate() to keep the order of levels of factors supplied by the
> grouping elements? Thanks!
>
> library(chron)
> dts=seq.dates("1/1/01","12/31/03")
> rnum=rnorm(1:length(dts))
> df=data.frame(date=dts,obs=rnum)
> agg=aggregate(df[,2],list(year=years(df[,1]),month=months(df[,1])),sum)
> levels(agg$month) # aggregate() automatically generates levels sorted by
> alphabet.
>
> [1] "Apr" "Aug" "Dec" "Feb" "Jan" "Jul" "Jun" "Mar" "May" "Nov" "Oct" "Sep"
>
> fmonth=factor(months(df[,1]))
> levels(fmonth) # factor() automatically generates the correct order of
> levels.
>
> [1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct" "Nov" "Dec"
>
>
> agg2=aggregate(df[,2],list(year=years(df[,1]),month=fmonth),sum)
> levels(agg2$month) # even if a factor with levels in the correct order is
> supplied, aggregate(), sortsthe levels by alphabet regardless.
>
> [1] "Apr" "Aug" "Dec" "Feb" "Jan" "Jul" "Jun" "Mar" "May" "Nov" "Oct" "Sep"
>
>
> --
> Tom
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From anupa.fabian at yahoo.com  Sun Dec 16 13:34:15 2007
From: anupa.fabian at yahoo.com (Anupa Fabian)
Date: Sun, 16 Dec 2007 19:34:15 +0700 (ICT)
Subject: [R] Extracting Year (only) information from Non-Standard Dates
Message-ID: <298698.13553.qm@web45702.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071216/229065ed/attachment.pl 

From tom.soyer at gmail.com  Sun Dec 16 03:22:50 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Sat, 15 Dec 2007 20:22:50 -0600
Subject: [R] format numbers in a contingency table
Message-ID: <65cc7bdf0712151822k705a2a48g4031b8abde095701@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071215/4ca232e0/attachment.pl 

From ferri.leberl at gmx.at  Sun Dec 16 14:54:17 2007
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Sun, 16 Dec 2007 14:54:17 +0100
Subject: [R] Working with ranges of a list
Message-ID: <1197813257.9479.8.camel@localhost>

Dear everybody!
Please find attached a tiny R-program. It returns:
      [,1] [,2]
[1,] 53.55   NA
[2,] 53.55   NA
[3,] 53.55   NA
How can I manage the first column to show the second component not only
of the first list in k??ste but of the second component of every list in
k??ste respectively, such as to get the following array returned:
      [,1] [,2]
[1,] 53.55   NA
[2,] 53.87   NA
[3,] 53.87   NA
Thank you in advance.
yours,
Mag. Ferri Leberl
-------------- n?chster Teil --------------
#Programm zur Erstellung eines Wasserwegenetzplanes

#St??dte
#TLD.Stadtk??rzel<-list("Stadtname",Breite,L??nge)

de.cux<-list("Cuxhaven",53.87,8.7)

de.hl<-list("L??beck",53.87,10.69)
de.hb.br<-list("Bremerhaven",53.55,8.58)

see<-function(orte){
lang<-length(orte)
punkte<-array(,c(lang,2))
punkte[,1]<-orte[[1]][[2[1]]]
punkte
}
k??ste<-(list(de.hb.br,de.cux,de.hl))
see(k??ste)

From jholtman at gmail.com  Sun Dec 16 16:32:41 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 16 Dec 2007 10:32:41 -0500
Subject: [R] Working with ranges of a list
In-Reply-To: <1197813257.9479.8.camel@localhost>
References: <1197813257.9479.8.camel@localhost>
Message-ID: <644e1f320712160732w1202892o15016f2571e8e182@mail.gmail.com>

Is this what you want as output:

> do.call('rbind', kaste)
     [,1]          [,2]  [,3]
[1,] "Bremerhaven" 53.55 8.58
[2,] "Cuxhaven"    53.87 8.7
[3,] "L??beck"     53.87 10.69



On Dec 16, 2007 8:54 AM, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> Dear everybody!
> Please find attached a tiny R-program. It returns:
>      [,1] [,2]
> [1,] 53.55   NA
> [2,] 53.55   NA
> [3,] 53.55   NA
> How can I manage the first column to show the second component not only
> of the first list in k?ste but of the second component of every list in
> k?ste respectively, such as to get the following array returned:
>      [,1] [,2]
> [1,] 53.55   NA
> [2,] 53.87   NA
> [3,] 53.87   NA
> Thank you in advance.
> yours,
> Mag. Ferri Leberl
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From milton_ruser at yahoo.com.br  Sun Dec 16 16:47:16 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sun, 16 Dec 2007 07:47:16 -0800 (PST)
Subject: [R] reading data on code script.
Message-ID: <865708.15733.qm@web56011.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071216/5eda447c/attachment.pl 

From ggrothendieck at gmail.com  Sun Dec 16 16:48:22 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Dec 2007 10:48:22 -0500
Subject: [R] format numbers in a contingency table
In-Reply-To: <65cc7bdf0712151822k705a2a48g4031b8abde095701@mail.gmail.com>
References: <65cc7bdf0712151822k705a2a48g4031b8abde095701@mail.gmail.com>
Message-ID: <971536df0712160748t77db6babu8ac52feb0970f26e@mail.gmail.com>

Try:

print(x, digits = 3)

On Dec 15, 2007 9:22 PM, tom soyer <tom.soyer at gmail.com> wrote:
> Hi,
>
> I am constructing a contingency table using xtabs. The function works great:
>      mo
> yr               Sep           Oct           Nov           Dec
>  1950 -7.164486e-02  3.152674e-02 -1.283389e-02  1.570382e-01
>  1951  3.054293e-02  4.665234e-02 -2.445499e-04  8.720204e-02
>  1952  3.937034e-02 -4.790636e-02  5.022616e-02  1.180279e-01
>
> but I wonder if there is an argument I can pass to xtabs to format the
> numbers in the table. For example, -0.0716 instead of -7.164486e-02. Does
> anyone know?
>
> Thanks!
>
> --
> Tom
>
>
> --
> Tom
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Sun Dec 16 16:50:03 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Dec 2007 10:50:03 -0500
Subject: [R] reading data on code script.
In-Reply-To: <865708.15733.qm@web56011.mail.re3.yahoo.com>
References: <865708.15733.qm@web56011.mail.re3.yahoo.com>
Message-ID: <971536df0712160750r3ee1b1a0w982302b63b23c6ff@mail.gmail.com>

Try this:

Lines <- "freqesperado
117.5
147.5
47.5
17.5
"
read.table(textConnection(Lines), header = TRUE)

On Dec 16, 2007 10:47 AM, Milton Cezar Ribeiro
<milton_ruser at yahoo.com.br> wrote:
> Dear All,
>
> It there a way of I read my data tab-separated on the own script, without read from a external file and without type the data?
>
> I would like something like
>
> mydata<- read.data(head=T, sep="\t")
> freqesperado
> 117.5
> 147.5
> 47.5
> 17.5
>
> ##END OF DATA
>
> Many thanks, miltinho
>
>
>
>  para armazenamento!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Sun Dec 16 16:50:03 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Dec 2007 10:50:03 -0500
Subject: [R] reading data on code script.
In-Reply-To: <865708.15733.qm@web56011.mail.re3.yahoo.com>
References: <865708.15733.qm@web56011.mail.re3.yahoo.com>
Message-ID: <971536df0712160750r3ee1b1a0w982302b63b23c6ff@mail.gmail.com>

Try this:

Lines <- "freqesperado
117.5
147.5
47.5
17.5
"
read.table(textConnection(Lines), header = TRUE)

On Dec 16, 2007 10:47 AM, Milton Cezar Ribeiro
<milton_ruser at yahoo.com.br> wrote:
> Dear All,
>
> It there a way of I read my data tab-separated on the own script, without read from a external file and without type the data?
>
> I would like something like
>
> mydata<- read.data(head=T, sep="\t")
> freqesperado
> 117.5
> 147.5
> 47.5
> 17.5
>
> ##END OF DATA
>
> Many thanks, miltinho
>
>
>
>  para armazenamento!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ferri.leberl at gmx.at  Sun Dec 16 16:54:47 2007
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Sun, 16 Dec 2007 16:54:47 +0100
Subject: [R] Working with ranges of a list
In-Reply-To: <644e1f320712160732w1202892o15016f2571e8e182@mail.gmail.com>
References: <1197813257.9479.8.camel@localhost>
	<644e1f320712160732w1202892o15016f2571e8e182@mail.gmail.com>
Message-ID: <1197820488.15829.10.camel@localhost>

Thank you for the attempt.
On your advice I am working with lists to avoid the numbers (which are
geographical coordinates) becoming strings. The call you suggest does
not take care of that. Now I am trying to extract the coordinates from
the list efficiently. Of course I could make a loop, e.g.

punkte[,1]<-orte[[1]][[2[1]]]

being replaced by

for(n in 1:lang){
punkte[n,1]<-orte[[n]][[2[1]]]
punkte[n,2]<-orte[[n]][[3[1]]]
}

but I wonder if there was no more efficient way.
Faithfully,
Mag. Ferri Leberl

Am Sonntag, den 16.12.2007, 10:32 -0500 schrieb jim holtman:
> Is this what you want as output:
> 
> > do.call('rbind', kaste)
>      [,1]          [,2]  [,3]
> [1,] "Bremerhaven" 53.55 8.58
> [2,] "Cuxhaven"    53.87 8.7
> [3,] "L??beck"     53.87 10.69
> 
> 
> 
> On Dec 16, 2007 8:54 AM, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> > Dear everybody!
> > Please find attached a tiny R-program. It returns:
> >      [,1] [,2]
> > [1,] 53.55   NA
> > [2,] 53.55   NA
> > [3,] 53.55   NA
> > How can I manage the first column to show the second component not only
> > of the first list in k?ste but of the second component of every list in
> > k?ste respectively, such as to get the following array returned:
> >      [,1] [,2]
> > [1,] 53.55   NA
> > [2,] 53.87   NA
> > [3,] 53.87   NA
> > Thank you in advance.
> > yours,
> > Mag. Ferri Leberl
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> 
> 
>


From jholtman at gmail.com  Sun Dec 16 17:00:57 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 16 Dec 2007 11:00:57 -0500
Subject: [R] Working with ranges of a list
In-Reply-To: <1197820488.15829.10.camel@localhost>
References: <1197813257.9479.8.camel@localhost>
	<644e1f320712160732w1202892o15016f2571e8e182@mail.gmail.com>
	<1197820488.15829.10.camel@localhost>
Message-ID: <644e1f320712160800uc0ad806yb216026207e5f5f1@mail.gmail.com>

You can use the following to get the data from that call:

> x <- do.call('rbind', kaste)
> as.matrix(x[,2:3])
     [,1]  [,2]
[1,] 53.55 8.58
[2,] 53.87 8.7
[3,] 53.87 10.69


On Dec 16, 2007 10:54 AM, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> Thank you for the attempt.
> On your advice I am working with lists to avoid the numbers (which are
> geographical coordinates) becoming strings. The call you suggest does
> not take care of that. Now I am trying to extract the coordinates from
> the list efficiently. Of course I could make a loop, e.g.
>
> punkte[,1]<-orte[[1]][[2[1]]]
>
> being replaced by
>
> for(n in 1:lang){
> punkte[n,1]<-orte[[n]][[2[1]]]
> punkte[n,2]<-orte[[n]][[3[1]]]
> }
>
> but I wonder if there was no more efficient way.
> Faithfully,
> Mag. Ferri Leberl
>
> Am Sonntag, den 16.12.2007, 10:32 -0500 schrieb jim holtman:
>
> > Is this what you want as output:
> >
> > > do.call('rbind', kaste)
> >      [,1]          [,2]  [,3]
> > [1,] "Bremerhaven" 53.55 8.58
> > [2,] "Cuxhaven"    53.87 8.7
> > [3,] "L??beck"     53.87 10.69
> >
> >
> >
> > On Dec 16, 2007 8:54 AM, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> > > Dear everybody!
> > > Please find attached a tiny R-program. It returns:
> > >      [,1] [,2]
> > > [1,] 53.55   NA
> > > [2,] 53.55   NA
> > > [3,] 53.55   NA
> > > How can I manage the first column to show the second component not only
> > > of the first list in k?ste but of the second component of every list in
> > > k?ste respectively, such as to get the following array returned:
> > >      [,1] [,2]
> > > [1,] 53.55   NA
> > > [2,] 53.87   NA
> > > [3,] 53.87   NA
> > > Thank you in advance.
> > > yours,
> > > Mag. Ferri Leberl
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> >
> >
> >
>
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Sun Dec 16 17:07:56 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 16 Dec 2007 11:07:56 -0500
Subject: [R] Extracting Year (only) information from Non-Standard Dates
In-Reply-To: <298698.13553.qm@web45702.mail.sp1.yahoo.com>
References: <298698.13553.qm@web45702.mail.sp1.yahoo.com>
Message-ID: <644e1f320712160807q3ecd93a5sf15c078d7f83e6f7@mail.gmail.com>

try this:

> dates <- c("January 12, 1988", "March 4, 2006", "1958")
> gsub(".*, *(.*)", "\\1", dates) # notice the it is comma blank asterisk in the string
[1] "1988" "2006" "1958"



On Dec 16, 2007 7:34 AM, Anupa Fabian <anupa.fabian at yahoo.com> wrote:
> I have some data whose date column consists of two types of date entries:
> (a) year-only entries (eg "1983") and
> (b) full date info (eg September 12, 1962).
>
> Here's what the non-standard date info looks like:
> > mode(non.standard.dates)
> [1] "numeric"
> > head(non.standard.dates)
> [1] July 15, 1925     February 13, 1923 July 10, 1988     February 24, 1931
> [5] 1952     January 9, 1957
> 1638 Levels: 1732 1735 1736 1740 1745 1748 1749 1750 1752 1754 1757 1760 ... September 9, 1898
>
> I just want to extract out the year information (viz in the example above: 1925, 1923, 1988, 1931, 1952 and 1957) and create a "year" column. Have tried using the R date libraries without success, but they seem more designed for working with dates which are standardized the same way.... I'd really appreciate suggestions from more experienced R folks. Thank you.
>
>
>
>
>      ____________________________________________________________________________________
> Never miss a thing.  Make Yahoo your home page.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jrkrideau at yahoo.ca  Sun Dec 16 17:12:19 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 16 Dec 2007 11:12:19 -0500 (EST)
Subject: [R] improving a bar graph
In-Reply-To: <20071216091012.7644F595551@borg.st.net.au>
Message-ID: <15322.73558.qm@web32808.mail.mud.yahoo.com>

Ugly brute-force approach:  col=1:16 .   Jim Lemon's
approach with Plotrix is much nicer.  You might also
want to have a look at RColorBrewer though I am not
sure how easily it can handle 16 different colours.


barplot(dft, beside = TRUE, main= "Risk score by
assessment", xlab = " Score", ylab = "frequency",
col=1:16)

--- Bob Green <bgreen at dyson.brisnet.org.au> wrote:

> Hello,
> 
> Below is the code for a basic bar graph. I was
> seeking advice 
> regarding the following:
> 
> (a)  For each time period there are values from 16
> people. How I can 
> change the colour value so that each person has a
> different colour, 
> which recurs across each of the three graphs/tie
> epriods?
> 
> (b) I have seen much more sophisticated examples
> using lattice (e.g 
> each person has a separate panel/plot). I am open to
> alternative code 
> as to how I could present this data.
> 
> Time1 <-
>
c(9.0,6.0,1.0,5.0,7.0,9.0,5.0,7.5,6.0,8.0,5.0,5.0,9.0,4.0,5.0,5.0)
> Time2 <- c (10,5,3,3,3,6,7,8,5,8,7,7,9,8,5,3)
> Time3 <- c (10,0,3,0,0,6,0,0,0,0,0,0,0,0,0,0)
> df  <- rbind  (Time1, Time2, Time3)
> dft <- (t(df))
> dft
> barplot(dft, beside = TRUE, main= "Risk score by
> assessment", xlab = 
> " Score", ylab = "frequency", col="blue")
> 
> 
> Any assistance is much appreciated,
> 
> regards
> 
> Bob Green
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From milton_ruser at yahoo.com.br  Sun Dec 16 17:30:25 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sun, 16 Dec 2007 08:30:25 -0800 (PST)
Subject: [R] polar orientation significance test
Message-ID: <876515.18778.qm@web56004.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071216/881af85a/attachment.pl 

From h.wickham at gmail.com  Sun Dec 16 18:19:09 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 16 Dec 2007 11:19:09 -0600
Subject: [R] improving a bar graph
In-Reply-To: <20071216091012.7644F595551@borg.st.net.au>
References: <mailman.15.1184320808.31491.r-help@stat.math.ethz.ch>
	<20071216091012.7644F595551@borg.st.net.au>
Message-ID: <f8e6ff050712160919g4793dfecx61928f85ec709b49@mail.gmail.com>

On 12/16/07, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
> Hello,
>
> Below is the code for a basic bar graph. I was seeking advice
> regarding the following:
>
> (a)  For each time period there are values from 16 people. How I can
> change the colour value so that each person has a different colour,
> which recurs across each of the three graphs/tie epriods?
>
> (b) I have seen much more sophisticated examples using lattice (e.g
> each person has a separate panel/plot). I am open to alternative code
> as to how I could present this data.

Why not use a line plot?  It would be much easier to see how an
individual is changing over time.  Here's a simple way to do that
using ggplot2:

df  <- data.frame(Time1, Time2, Time3)
df$id <- 1:nrow(df)
dfm <- melt(df, id="id")

qplot(variable, value, data=dfm, group=id, geom="line")

Hadley

-- 
http://had.co.nz/


From dieter.menne at menne-biomed.de  Sun Dec 16 18:22:31 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 16 Dec 2007 17:22:31 +0000 (UTC)
Subject: [R] polar orientation significance test
References: <876515.18778.qm@web56004.mail.re3.yahoo.com>
Message-ID: <loom.20071216T172136-125@post.gmane.org>

Milton Cezar Ribeiro <milton_ruser <at> yahoo.com.br> writes:

> I would like to test if are there some significant orientation of frequencies
on a polar analysis.
> 
Check packages CircStats and circular.

Dieter


From ripley at stats.ox.ac.uk  Sun Dec 16 18:43:38 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 16 Dec 2007 17:43:38 +0000 (GMT)
Subject: [R] reading data on code script.
In-Reply-To: <865708.15733.qm@web56011.mail.re3.yahoo.com>
References: <865708.15733.qm@web56011.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.64.0712161730340.14716@gannet.stats.ox.ac.uk>

Yes, for some ways of running a script (and you haven't told us how your 
intend to run this).  E.g.

% Rscript foo.R

works, where

% cat foo.R
mydata <- read.table(stdin(), header=TRUE, sep="\t", nrow=6)
freqesperado
117.5
147.5
47.5
17.5
27.7
16.3

mydata

But, e.g. you can't use source() (it parses the input before running it), 
amongst others.


Milton Cezar Ribeiro wrote:

> Dear All,
>
> It there a way of I read my data tab-separated on the own script, 
> without read from a external file and without type the data?
>
> I would like something like
>
> mydata<- read.data(head=T, sep="\t")
> freqesperado
> 117.5
> 147.5
> 47.5
> 17.5
>
> ##END OF DATA
>
> Many thanks, miltinho
>
>
>
> para armazenamento!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do, and avoid sending HTML as requested.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bates at stat.wisc.edu  Sun Dec 16 19:20:51 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 16 Dec 2007 12:20:51 -0600
Subject: [R] convergence error code in mixed effects models
In-Reply-To: <14340592.post@talk.nabble.com>
References: <580233.74151.qm@web26210.mail.ukl.yahoo.com>
	<40e66e0b0712131426h6e702ed2pa45b5786c7b29d36@mail.gmail.com>
	<83439.66132.qm@web26213.mail.ukl.yahoo.com>
	<14340592.post@talk.nabble.com>
Message-ID: <40e66e0b0712161020w5244e731j9fb511c0335183@mail.gmail.com>

On Dec 14, 2007 9:26 PM, Mark Difford <mark_difford at yahoo.co.uk> wrote:

> >> Is there a solution for this problem?
>
> If there is, then Professor Bates (the gentleman who replied to your
> question) will have tried to find it, and fix it, for you.

What I was trying to indicate in my replies is that "this problem" is
the user attempting to fit a model that is not appropriate for the
data.

I feel, and I hope that Jos? Pinheiro and I conveyed in our book, that
analysis of longitudinal data should always begin with plots of the
response versus time by experimental unit.  Much of the wonderful work
that Deepayan Sarkar did in developing the lattice package was
motivated by the desire to produce exactly those plots.  Because the
purpose of the analysis is to look at the behavior within units over
time and see how these patterns differ between units, it is clear that
you should always begin with such a plot.

It is disappointing to have users feel that the software is somehow
inferior because it doesn't mindlessly produce estimates for
inappropriate models when it is so simple for the user to check the
patterns in the data and decide if the model is appropriate.

As I said, I think that SAS and SPSS are better tools for fitting
"the" repeated measures model or "the" longitudinal data model when
you don't want to be bothered with actually looking at your data and
thinking about the model.  (And yes, I am being a trifle sarcastic in
saying that.  Please don't quote me as having endorsed the SAS and
SPSS mixed model software.)

By the way, I received a personal reply from a friend who asked what I
meant when I said that the model fit by SAS PROC MIXED to these data
may not make sense.  I haven't used SAS PROC MIXED in a long time (I
can never get past the "CARDS;" statement and still take the software
seriously) but I strongly suspect that it would produce one of those
mixed models that occurs in SAS computation and nowhere else.  If you
allow correlated random effects for the intercept and slope on these
data you will probably get the ML or REML estimate of the variance of
the intercept random effects being driven to zero while the estimate
of the covariance of the intercept and slope random effects stays
decidedly nonzero.  Apparently mathematical impossibility is not an
impediment to parameter estimation in such cases.  In fact, Singer and
Willett claim on p. 154 of their 2003 book "Applied Longitudinal Data
Analysis" that one can go further and invoke an option to allow for
negative variance estimates.  The mind boggles.


> Professor Bates wrote/co-wrote the software package (nlme) you are using.
> And while I have nothing against Crawley's book, you are usually much better
> off going to primary sources first, to solve this kind of problem (which, of
> course you have done, though may not have been aware of it ;)
>
> Mixed-Effects Models in S and S-PLUS, by: Pinheiro, Jos?, Bates, Douglas
> http://www.springer.com/west/home/statistics/computational?SGWID=4-10130-22-2102822-0
>
> Hope this speeds you on your way...
>
> Regards, Mark.
>
>
>
> Ilona Leyer wrote:
> >
> >
> > Here an simple example:
> >
> > rep   treat   heightfra       leaffra leafvim week
> > ID1   pHf     1.54    4       4       4
> > ID2   pHf     1.49    4       4       4
> > ID3   pHf     1.57    4       5       4
> > ID4   pHf     1.48    4       4       4
> > ID5   pHf     1.57    4       4       4
> > ID6   pHs     1.29    4       5       4
> > ID7   pHs     0.97    4       5       4
> > ID8   pHs     2.06    4       4       4
> > ID9   pHs     0.88    4       4       4
> > ID10  pHs     1.47    4       4       4
> > ID1   pHf     3.53    5       6       6
> > ID2   pHf     4.08    6       6       6
> > ID3   pHf     3.89    6       6       6
> > ID4   pHf     3.78    5       6       6
> > ID5   pHf     3.92    6       6       6
> > ID6   pHs     2.76    5       5       6
> > ID7   pHs     3.31    6       7       6
> > ID8   pHs     4.46    6       7       6
> > ID9   pHs     2.19    5       5       6
> > ID10  pHs     3.83    5       5       6
> > ID1   pHf     5.07    7       7       9
> > ID2   pHf     6.42    7       8       9
> > ID3   pHf     5.43    6       8       9
> > ID4   pHf     6.83    6       8       9
> > ID5   pHf     6.26    6       8       9
> > ID6   pHs     4.57    6       9       9
> > ID7   pHs     5.05    6       7       9
> > ID8   pHs     6.27    6       8       9
> > ID9   pHs     3.37    5       7       9
> > ID10  pHs     5.38    6       8       9
> > ID1   pHf     5.58    7       9       12
> > ID2   pHf     7.43    8       9       12
> > ID3   pHf     6.18    8       10      12
> > ID4   pHf     6.91    7       10      12
> > ID5   pHf     6.78    7       10      12
> > ID6   pHs     4.99    6       13      12
> > ID7   pHs     5.50    7       8       12
> > ID8   pHs     6.56    7       10      12
> > ID9   pHs     3.72    6       10      12
> > ID10  pHs     5.94    6       10      12
> >
> >
> > I used the procedure described in Crawley?s new R
> > Book.
> > For two of the tree response variables
> > (heightfra,leaffra) it doesn?t work, while it worked
> > with leafvim (but in another R session, yesterday,
> > leaffra worked as well...).
> >
> > Here the commands:
> >
> >> attach(test)
> >> names(test)
> > [1] "week"      "rep"       "treat"     "heightfra"
> > "leaffra"   "leafvim"
> >> library(nlme)
> >>
> > test<-groupedData(heightfra~week|rep,outer=~treat,test)
> >> model1<-lme(heightfra~treat,random=~week|rep)
> > Error in lme.formula(heightfra ~ treat, random = ~week
> > | rep) :
> >         nlminb problem, convergence error code = 1;
> > message = iteration limit reached without convergence
> > (9)
> >
> >>
> > test<-groupedData(leaffra~week|rep,outer=~treat,test)
> >> model2<-lme(leaffra~treat,random=~week|rep)
> > Error in lme.formula(leaffra ~ treat, random = ~week |
> > rep) :
> >         nlminb problem, convergence error code = 1;
> > message = iteration limit reached without convergence
> > (9)
> >
> >>
> > test<-groupedData(leafvim~week|rep,outer=~treat,test)
> >> model3<-lme(leafvim~treat,random=~week|rep)
> >> summary(model)
> > Error in summary(model) : object "model" not found
> >> summary(model3)
> > Linear mixed-effects model fit by REML
> >  Data: NULL
> >        AIC      BIC    logLik
> >   129.6743 139.4999 -58.83717
> >
> > Random effects:
> >  Formula: ~week | rep
> >  Structure: General positive-definite, Log-Cholesky
> > parametrization
> >             StdDev    Corr
> > (Intercept) 4.4110478 (Intr)
> > week        0.7057311 -0.999
> > Residual    0.5976143
> >
> > Fixed effects: leafvim ~ treat
> >                Value Std.Error DF  t-value p-value
> > (Intercept) 5.924659 0.1653596 30 35.82893  0.0000
> > treatpHs    0.063704 0.2338538  8  0.27241  0.7922
> >  Correlation:
> >          (Intr)
> > treatpHs -0.707
> >
> > Standardized Within-Group Residuals:
> >         Min          Q1         Med          Q3
> >  Max
> > -1.34714254 -0.53042878 -0.01769195  0.40644540
> > 2.29301560
> >
> > Number of Observations: 40
> > Number of Groups: 10
> >
> > Is there a solution for this problem?
> >
> > Thanks!!!
> >
> > Ilona
> >
> > --- Douglas Bates <bates at stat.wisc.edu> schrieb:
> >
> >> On Dec 13, 2007 4:15 PM, Ilona Leyer
> >> <ileyer at yahoo.de> wrote:
> >> > Dear All,
> >> > I want to analyse treatment effects with time
> >> series
> >> > data:  I measured e.g. leaf number (five replicate
> >> > plants) in relation to two soil pH - after 2,4,6,8
> >> > weeks. I used mixed effects models, but some
> >> analyses
> >> > didn?t work. It seems for me as if this is a
> >> randomly
> >> > occurring problem since sometimes the same model
> >> works
> >> > sometimes not.
> >> >
> >> > An example:
> >> > > names(test)
> >> > [1] "rep"    "treat"  "leaf"   "week"
> >> > > library (lattice)
> >> > > library (nlme)
> >> > >
> >> test<-groupedData(leaf~week|rep,outer=~treat,test)
> >> > > model<-lme(leaf~treat,random=~leaf|rep)
> >> > Error in lme.formula(leaf~ treat, random =
> >> ~week|rep)
> >>
> >> Really!? You gave lme a model with random = ~ leaf |
> >> rep (and no data
> >> specification) and it tried to fit a model with
> >> random = ~ week | rep?
> >> Are you sure that is an exact transcript?
> >>
> >> > :
> >> >         nlminb problem, convergence error code =
> >> 1;
> >> > message = iteration limit reached without
> >> convergence
> >> > (9)
> >>
> >> > Has anybody an idea to solve this problem?
> >>
> >> Oh, I have lots of ideas but without a reproducible
> >> example I can't
> >> hope to decide what might be the problem.
> >>
> >> It appears that the model may be over-parameterized.
> >>  Assuming that
> >> there are 4 different values of week then ~ week |
> >> rep requires
> >> fitting 10 variance-covariance parameters. That's a
> >> lot.
> >> The error code indicates that the optimizer is
> >> taking
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
> --
> View this message in context: http://www.nabble.com/convergence-error-code-in-mixed-effects-models-tp14325990p14340592.html
> Sent from the R help mailing list archive at Nabble.com.
>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ozric at web.de  Sun Dec 16 20:16:58 2007
From: ozric at web.de (Christian Schulz)
Date: Sun, 16 Dec 2007 20:16:58 +0100
Subject: [R] paste dependent variable in  formula (rpart)?
Message-ID: <476579AA.2020604@web.de>

Hello,

i'm trying to replace  different  target variables in rpart with a 
function. The data.frame getting always the target variable as last column.
Try below, i get the target variable in the explained variables, too!?  
Have anybody an advice to avoid this.

rp1 <- rpart(eval(parse(text=paste(names(train[length(train)])))) ~ . , 
data=train,cp=0.0001)

regards & many thanks
Christian


From dwinsemius at comcast.net  Sun Dec 16 20:53:49 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 16 Dec 2007 19:53:49 +0000 (UTC)
Subject: [R] Analyzing Publications from Pubmed via XML
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<Xns9A07E22B53BDAdNOTwinscomcast@80.91.229.13>
	<971536df0712152039r7d41c950w773da384b941332d@mail.gmail.com>
Message-ID: <Xns9A0897913BC5DdNOTwinscomcast@80.91.229.13>

On 15 Dec 2007, you wrote in gmane.comp.lang.r.general:

> If we can assume that the abstract is always the 4th paragraph then we
> can try something like this:
> 
> library(XML)
> doc <-
> xmlTreeParse("http://eutils.ncbi.nlm.nih.gov/entrez/eutils/erss.cgi?rss
> _guid=0_JYbpsax0ZAAPnOd7nFAX-29fXDpTk5t8M4hx9ytT-", isURL = TRUE,
> useInternalNodes = TRUE, trim = TRUE) 
> 
> out <- cbind(
>      Author = unlist(xpathApply(doc, "//author", xmlValue)),
>      PMID = gsub(".*:", "", unlist(xpathApply(doc, "//guid",
>      xmlValue))), 
>      Abstract = unlist(xpathApply(doc, "//description",
>           function(x) {
>                on.exit(free(doc2))
>                doc2 <- htmlTreeParse(xmlValue(x)[[1]], asText = TRUE,
>                     useInternalNodes = TRUE, trim = TRUE)
>                xpathApply(doc2, "//p[4]", xmlValue)
>           }
>      )))
> free(doc)
> substring(out, 1, 25) # display first 25 chars of each field
> 
> 
> The last line produces (it may look messed up in this email):
> 
>> substring(out, 1, 25) # display it
>       Author                      PMID       Abstract
 [1,] " Goon P, Sonnex C, Jani P" "18046565" "Human papillomaviruses (H"
 [2,] " Rad MH, Alizadeh E, Ilkh" "17978930" "Recurrent laryngeal papil"
 [3,] " Lee LA, Cheng AJ, Fang T" "17975511" "OBJECTIVES:: Papillomas o"
 [4,] " Gerein V, Schmandt S, Ba" "17935912" "BACKGROUND: Human papillo"
snip
> 
> 

It looked beautifully regular in my newsreader. It is helpful to see an 
example showing the indexed access to nodes. It was also helpful to see the 
example of substring for column display. Thank you (for this and all of 
your other contributions.)

I find upon further browsing that the pmfetch access point is obsolete. 
Experimentation with the PubMed eFetch server access point results in fully 
xml-tagged results:

e.fetch.doc<- function (){
   fetch.stem <-
        "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"
   src.mode <- "db=pubmed&retmode=xml&"
   request <- "id=11045395"
   doc<-xmlTreeParse(paste(fetch.stem,src.mode,request,sep=""),
                          isURL = TRUE, useInternalNodes = TRUE)
     }
# in the debugging phase I needed to set useInternalNodes = TRUE to see the  
tags. Never did find a way to "print" them when internal.

doc<-e.fetch.doc()
get.info<- function(doc){
         df<-cbind(
 	Abstract = unlist(xpathApply(doc, "//AbstractText", xmlValue)),
 	Journal =  unlist(xpathApply(doc, "//Title", xmlValue)),
 	Pmid =  unlist(xpathApply(doc, "//PMID", xmlValue))
                   )
   return(df)
   } 

# this works
> substring(get.info(doc), 1, 25)
     Abstract                    Journal                     Pmid      
[1,] "We studied the prevalence" "Pediatric nephrology (Ber" "11045395"


-- 
David Winsemius


From dusa.adrian at gmail.com  Sun Dec 16 21:09:22 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 16 Dec 2007 22:09:22 +0200
Subject: [R] read.table and double quotes in strings
Message-ID: <200712162209.22705.dusa.adrian@gmail.com>


Dear all,

Some very wise data entry person gave me about an hour of a headache, trying 
to find out why a 2000x500 dataframe won't be read into R.
After much trial and error, I pinpointed the problem to an accidentally 
inserted double quote into a string variable (some comments from an open 
question). This can be replicated by:

aa <- data.frame(id=1:2, var1=c("some \" quote", "without quote"))
> aa
  id          var1
1  1  some " quote
2  2 without quote

Saving this with R:
write.table(aa, "aa.dat", sep="\t", row.names=F)

creates the following ASCII file (between #s)

### R export
"id"	"var1"
1	"some \" quote"
2	"without quote"
###

which throws an error when trying to load it back:

> bb <- read.table("aa.dat", sep="\t", header=T)
Warning message:
In read.table("aa.dat", sep = "\t", header = T) :
  incomplete final line found by readTableHeader on 'aa.dat'

The dataframe was initially an SPSS file, which saved it as tab delimited in 
this format:

### SPSS export
"id"	"var1"
1	"some " quote"
2	"without quote"
###

which of course thrown the same obvious error.

StatTransfer was the only software that solved the problem of exporting the 
SPSS file in a tab delimited file that could finally be imported in R, and 
the saved file looks like this:

### StatTransfer export
"id"	"var1"
1	"some "" quote"
2	"without quote"
###

Given these examples, I have two questions:
1. What is the correct syntax to import the R-exported file
2. What can I do to prevent these situations from happening?
(besides whipping the data entry person :), I am referring to R procedures to 
detect and correct such things)

Thank you,
Adrian


-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From dusa.adrian at gmail.com  Sun Dec 16 21:18:29 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 16 Dec 2007 22:18:29 +0200
Subject: [R] read.table and double quotes in strings
In-Reply-To: <200712162209.22705.dusa.adrian@gmail.com>
References: <200712162209.22705.dusa.adrian@gmail.com>
Message-ID: <200712162218.29844.dusa.adrian@gmail.com>

On Sunday 16 December 2007, Adrian Dusa wrote:
> Dear all,
>
> [...]
>
> Given these examples, I have two questions:
> 1. What is the correct syntax to import the R-exported file
> 2. What can I do to prevent these situations from happening?
> (besides whipping the data entry person :), I am referring to R procedures
> to detect and correct such things)

Trying to answer question number 2: would the usage of qmethod="double" 
argument in write.table solve the problem *in general*?

What are the situations that specifically need escaping the quote character?

Thank you,
Adrian



-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From ggrothendieck at gmail.com  Sun Dec 16 21:26:16 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Dec 2007 15:26:16 -0500
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <Xns9A0897913BC5DdNOTwinscomcast@80.91.229.13>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<Xns9A07E22B53BDAdNOTwinscomcast@80.91.229.13>
	<971536df0712152039r7d41c950w773da384b941332d@mail.gmail.com>
	<Xns9A0897913BC5DdNOTwinscomcast@80.91.229.13>
Message-ID: <971536df0712161226j2cddb7c6qa99992ae7366ed63@mail.gmail.com>

On Dec 16, 2007 2:53 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> # in the debugging phase I needed to set useInternalNodes = TRUE to see the
> tags. Never did find a way to "print" them when internal.

I assume you mean FALSE.  See:
?saveXML


From brothberg at gmail.com  Sun Dec 16 18:56:39 2007
From: brothberg at gmail.com (Burton Rothberg)
Date: Sun, 16 Dec 2007 12:56:39 -0500
Subject: [R] eee pc
Message-ID: <9402113c0712160956o7fdb786drcc5d24cbe3ac57b9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071216/d18b2ee2/attachment.pl 

From tom.soyer at gmail.com  Sun Dec 16 21:33:04 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Sun, 16 Dec 2007 14:33:04 -0600
Subject: [R] question about the aggregate function with respect to order
	of levels of grouping elements
In-Reply-To: <644e1f320712160704w1e5532eco6cabb2b6211f4c05@mail.gmail.com>
References: <65cc7bdf0712160623k20f69089xf1d6efad05040588@mail.gmail.com>
	<644e1f320712160704w1e5532eco6cabb2b6211f4c05@mail.gmail.com>
Message-ID: <65cc7bdf0712161233i3f227d69q43a5d58b3d34319c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071216/a32df678/attachment.pl 

From csardi at rmki.kfki.hu  Sun Dec 16 21:37:31 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Sun, 16 Dec 2007 21:37:31 +0100
Subject: [R] eee pc
In-Reply-To: <9402113c0712160956o7fdb786drcc5d24cbe3ac57b9@mail.gmail.com>
References: <9402113c0712160956o7fdb786drcc5d24cbe3ac57b9@mail.gmail.com>
Message-ID: <20071216203730.GA18505@localdomain>

You can easily install ubuntu on it (although it might require
an external drive):
http://hup.hu/node/48116
So running R should not be problem.

G.

On Sun, Dec 16, 2007 at 12:56:39PM -0500, Burton Rothberg wrote:
> I'm thinking of getting one of these lightweight linux laptops for
> traveling. Does anyone know if / how R runs on it?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From dwinsemius at comcast.net  Sun Dec 16 22:11:04 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 16 Dec 2007 21:11:04 +0000 (UTC)
Subject: [R] Analyzing Publications from Pubmed via XML
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<Xns9A07E22B53BDAdNOTwinscomcast@80.91.229.13>
	<971536df0712152039r7d41c950w773da384b941332d@mail.gmail.com>
	<Xns9A0897913BC5DdNOTwinscomcast@80.91.229.13>
	<971536df0712161226j2cddb7c6qa99992ae7366ed63@mail.gmail.com>
Message-ID: <Xns9A08A4AA154FFdNOTwinscomcast@80.91.229.13>

"Gabor Grothendieck" <ggrothendieck at gmail.com> wrote in
news:971536df0712161226j2cddb7c6qa99992ae7366ed63 at mail.gmail.com: 

> On Dec 16, 2007 2:53 PM, David Winsemius <dwinsemius at comcast.net>
> wrote: 
>> # in the debugging phase I needed to set useInternalNodes = TRUE to
>> see the tags. Never did find a way to "print" them when internal.
> 
> I assume you mean FALSE.  See:
> ?saveXML

You're correct, yet again; I did a copy/paste/forget-to-edit. And thanks 
for the further tip.

-- 
David


From dwinsemius at comcast.net  Sun Dec 16 22:23:38 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 16 Dec 2007 21:23:38 +0000 (UTC)
Subject: [R] Analyzing Publications from Pubmed via XML
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<Xns9A07E22B53BDAdNOTwinscomcast@80.91.229.13>
	<971536df0712152039r7d41c950w773da384b941332d@mail.gmail.com>
	<Xns9A0897913BC5DdNOTwinscomcast@80.91.229.13>
	<971536df0712161226j2cddb7c6qa99992ae7366ed63@mail.gmail.com>
Message-ID: <Xns9A08A6CB395DEdNOTwinscomcast@80.91.229.13>

"Gabor Grothendieck" <ggrothendieck at gmail.com> wrote in
news:971536df0712161226j2cddb7c6qa99992ae7366ed63 at mail.gmail.com: 

> On Dec 16, 2007 2:53 PM, David Winsemius <dwinsemius at comcast.net>
> wrote: 
>> # Never did find a way to "print" them when internal.

> ?saveXML

And now I understand where that odd "\n      <text>" originated before I 
changed the searched-for node name from \\Abstract to \\AbstractText. It's 
a remnant from the pretty-printing of the XML tree after excising the 
intervening node name.

-- 
David Winsemius


From paul.boutros at utoronto.ca  Sun Dec 16 23:13:36 2007
From: paul.boutros at utoronto.ca (Paul Boutros)
Date: Sun, 16 Dec 2007 17:13:36 -0500
Subject: [R] levelplot border and dendrogram width
Message-ID: <20071216171336.8ic5nl5og0soocgw@webmail.utoronto.ca>

Hello,

I'm trying to learn how to use lattice and levelplot in particular.   
There are three elements of customizing the plots I'm stuck with:

a) Is there a way to put borders around each "cell" within a  
level-plot.  I'm trying to do something like the  
colsep/rowsep/sepcolor/sepwidth parameters of heatmap.2 in gplots

b) Can I alter the line-width of dendrogram added to the levelplot  
using latticeExtra?

c) Can I alter the spacing between the dendrogram and a row of  
coloured rectanges added to a dendrogram using the add option?

The basic code-example that I've been trying to tweak is below, any  
suggestions and hints are very much welcome!!  In particular, I'm not  
sure where to find documentation on parameters for these, so hints on  
where to look would be great!

Paul



library(lattice);
library(latticeExtra);

tmp <- rnorm(100);
tmp <- matrix(data = tmp, nrow = 10, ncol = 10);

dd.row <- as.dendrogram( hclust( dist( tmp ) ) );

levelplot(
	tmp[order.dendrogram(dd.row), ],
	aspect = "fill",
	scales = list(
		x = list(rot = 90)
		),
	colorkey = list(
		space = "left"
		),
	legend = list(
		top = list(
			fun = dendrogramGrob,
			args = list(
				x = dd.row,
				ord = order.dendrogram(dd.row),
				side = "top",
				add = list(
					rect = list(
						col = "black",
						fill = c(
							rep("blue", 5),
							rep("green",5)
							),
						lwd = 3
						),
					type = "rectangle"
					),
				size.add = 1,
				size = 10,
				type = "rectangle"
				)
			)
		)
	);


From ligges at statistik.uni-dortmund.de  Mon Dec 17 00:26:45 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 17 Dec 2007 00:26:45 +0100
Subject: [R] paste dependent variable in  formula (rpart)?
In-Reply-To: <476579AA.2020604@web.de>
References: <476579AA.2020604@web.de>
Message-ID: <4765B435.4030000@statistik.uni-dortmund.de>



Christian Schulz wrote:
> Hello,
> 
> i'm trying to replace  different  target variables in rpart with a 
> function. The data.frame getting always the target variable as last column.
> Try below, i get the target variable in the explained variables, too!?  
> Have anybody an advice to avoid this.
> 
> rp1 <- rpart(eval(parse(text=paste(names(train[length(train)])))) ~ . , 

I guess you want something along the following example:

   train <- iris
   form <- as.formula(paste(names(train)[length(train)], "~ ."))
   rpart(form, data = iris)

or some data.frame method for rpart....

Uwe Ligges


> data=train,cp=0.0001)
> 
> regards & many thanks
> Christian
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From duncan at wald.ucdavis.edu  Mon Dec 17 01:28:24 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Mon, 17 Dec 2007 13:28:24 +1300
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <Xns9A0897913BC5DdNOTwinscomcast@80.91.229.13>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>	<4762080F.8070606@fhcrc.org>	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>	<Xns9A07E22B53BDAdNOTwinscomcast@80.91.229.13>	<971536df0712152039r7d41c950w773da384b941332d@mail.gmail.com>
	<Xns9A0897913BC5DdNOTwinscomcast@80.91.229.13>
Message-ID: <4765C2A8.9070801@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



David Winsemius wrote:
> On 15 Dec 2007, you wrote in gmane.comp.lang.r.general:
> 
>> If we can assume that the abstract is always the 4th paragraph then we
>> can try something like this:
>>
>> library(XML)
>> doc <-
>> xmlTreeParse("http://eutils.ncbi.nlm.nih.gov/entrez/eutils/erss.cgi?rss
>> _guid=0_JYbpsax0ZAAPnOd7nFAX-29fXDpTk5t8M4hx9ytT-", isURL = TRUE,
>> useInternalNodes = TRUE, trim = TRUE) 
>>
>> out <- cbind(
>>      Author = unlist(xpathApply(doc, "//author", xmlValue)),
>>      PMID = gsub(".*:", "", unlist(xpathApply(doc, "//guid",
>>      xmlValue))), 
>>      Abstract = unlist(xpathApply(doc, "//description",
>>           function(x) {
>>                on.exit(free(doc2))
>>                doc2 <- htmlTreeParse(xmlValue(x)[[1]], asText = TRUE,
>>                     useInternalNodes = TRUE, trim = TRUE)
>>                xpathApply(doc2, "//p[4]", xmlValue)
>>           }
>>      )))
>> free(doc)
>> substring(out, 1, 25) # display first 25 chars of each field
>>
>>
>> The last line produces (it may look messed up in this email):
>>
>>> substring(out, 1, 25) # display it
>>       Author                      PMID       Abstract
>  [1,] " Goon P, Sonnex C, Jani P" "18046565" "Human papillomaviruses (H"
>  [2,] " Rad MH, Alizadeh E, Ilkh" "17978930" "Recurrent laryngeal papil"
>  [3,] " Lee LA, Cheng AJ, Fang T" "17975511" "OBJECTIVES:: Papillomas o"
>  [4,] " Gerein V, Schmandt S, Ba" "17935912" "BACKGROUND: Human papillo"
> snip
>>
> 
> It looked beautifully regular in my newsreader. It is helpful to see an 
> example showing the indexed access to nodes. It was also helpful to see the 
> example of substring for column display. Thank you (for this and all of 
> your other contributions.)
> 
> I find upon further browsing that the pmfetch access point is obsolete. 
> Experimentation with the PubMed eFetch server access point results in fully 
> xml-tagged results:
> 
> e.fetch.doc<- function (){
>    fetch.stem <-
>         "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"
>    src.mode <- "db=pubmed&retmode=xml&"
>    request <- "id=11045395"
>    doc<-xmlTreeParse(paste(fetch.stem,src.mode,request,sep=""),
>                           isURL = TRUE, useInternalNodes = TRUE)
>      }
> # in the debugging phase I needed to set useInternalNodes = TRUE to see the  
> tags. Never did find a way to "print" them when internal.

saveXML(node)

will return a string giving the XML content of that node as tree.


> 
> doc<-e.fetch.doc()
> get.info<- function(doc){
>          df<-cbind(
>  	Abstract = unlist(xpathApply(doc, "//AbstractText", xmlValue)),
>  	Journal =  unlist(xpathApply(doc, "//Title", xmlValue)),
>  	Pmid =  unlist(xpathApply(doc, "//PMID", xmlValue))
>                    )
>    return(df)
>    } 
> 
> # this works
>> substring(get.info(doc), 1, 25)
>      Abstract                    Journal                     Pmid      
> [1,] "We studied the prevalence" "Pediatric nephrology (Ber" "11045395"
> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFHZcKo9p/Jzwa2QP4RAnu3AJ9ucFyb17rm48PLQaPTw6VWyrZWSQCdG0rT
zdLB6mkNPFh5lWgNgb70sDc=
=SR2E
-----END PGP SIGNATURE-----


From bolker at ufl.edu  Mon Dec 17 02:14:22 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Sun, 16 Dec 2007 17:14:22 -0800 (PST)
Subject: [R] Truncated normal distribution
In-Reply-To: <14361967.post@talk.nabble.com>
References: <bac068d10712141532r150fdb39s3d2127b53dfb8223@mail.gmail.com>
	<14361967.post@talk.nabble.com>
Message-ID: <14368641.post@talk.nabble.com>




ravirrrr wrote:
> 
> I have the following code, where we need to solve for mu and sigma, when
> we have mut and sdt. Can you suggest how to use a solve function in R to
> do that? I am new to R and am not sure how to go from defining the
> functions, to solving for them. 
> 
> Thanks
> 
> 
> truncated <- function(x)
> {
> 
> mu=x[1];
> sigma=x[2];
> 
> 
> f <- function(x) (1/(sigma*sqrt(2*pi)))*exp(-(x-mu)^2/(2*sigma^2));
> 
> pdf.fun <- function(x) x*f(x);
> 
> sd.fun <- function(x) (x)^2*f(x);
> 
> st=integrate(sd.fun,lower=-Inf,upper=1)$value;
> 
> a=integrate(pdf.fun,lower=-Inf,upper=1)$value;
> 
> a1=integrate(f,lower=-Inf,upper=1)$value;
> 
> mut <- a/a1;
> sdt <- sqrt((st/a1)-(a/a1)^2);
> 
> } 
> 

truncated <- function(x)
{
  mu <- x[1]
  sigma <- x[2]
  pdf.fun <- function(x) x*dnorm(x,mu,sigma)
  sd.fun <- function(x) x^2*dnorm(x,mu,sigma)
  st <- integrate(sd.fun,lower=-Inf,upper=1)$value;
  a <- integrate(pdf.fun,lower=-Inf,upper=1)$value;
  a1 <- pnorm(1,mu,sigma)
  mut <- a/a1;
  sdt <- sqrt((st/a1)-(a/a1)^2)
  c(mut,sdt)
}

truncated(c(0,1))  ## sensible: mean <0, sd<1
truncated(c(0,0.1)) ## sensible: approx 0,1
## trouble for small values 
truncated(c(0,0.001))
truncated(c(0,0.0001))

optfun <- function(p,target=c(0,1)) {
  sum((truncated(p)-target)^2)
}

target <- c(mu=-0.5,sd=2)
fit1 <- optim(fn=optfun,
      par=c(-0.5,2),
      target=target)
fit1

truncated(fit1$par) ## didn't succeed

## let's do something easier -- can we
## work backward to a known value?

t1 <- truncated(c(0,1))

optim(fn=optfun,
      par=c(0,1),
      target=t1)
          

optim(fn=optfun,
      par=c(0.5,2),
      target=t1)

-- 
View this message in context: http://www.nabble.com/Truncated-normal-distribution-tp14348951p14368641.html
Sent from the R help mailing list archive at Nabble.com.


From chainsawtiney at gmail.com  Mon Dec 17 02:50:52 2007
From: chainsawtiney at gmail.com (C.H.)
Date: Mon, 17 Dec 2007 09:50:52 +0800
Subject: [R] eee pc
In-Reply-To: <20071216203730.GA18505@localdomain>
References: <9402113c0712160956o7fdb786drcc5d24cbe3ac57b9@mail.gmail.com>
	<20071216203730.GA18505@localdomain>
Message-ID: <30d7ea360712161750w26212364sb68192a3291f30c@mail.gmail.com>

You can using the default Linux (Xandros, Debian Based) and enable
Debian Etch Repo in eeepc to install R via apt-get.

http://lnxg.ca/?Hardware:Asus_Eeepc_701:EeePC_Tips

I can install R, LaTeX.

Regards,
C


On Dec 17, 2007 4:37 AM, Gabor Csardi <csardi at rmki.kfki.hu> wrote:
> You can easily install ubuntu on it (although it might require
> an external drive):
> http://hup.hu/node/48116
> So running R should not be problem.
>
> G.
>
>
> On Sun, Dec 16, 2007 at 12:56:39PM -0500, Burton Rothberg wrote:
> > I'm thinking of getting one of these lightweight linux laptops for
> > traveling. Does anyone know if / how R runs on it?
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK
>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
CH Chan
Research Assistant - KWH
http://www.macgrass.com


From weigand.stephen at gmail.com  Mon Dec 17 02:58:51 2007
From: weigand.stephen at gmail.com (Stephen Weigand)
Date: Sun, 16 Dec 2007 19:58:51 -0600
Subject: [R] Function for AUC?
In-Reply-To: <a695fbee0712131038l3f94dbb0va55fe1fadc29418f@mail.gmail.com>
References: <a695fbee0712131038l3f94dbb0va55fe1fadc29418f@mail.gmail.com>
Message-ID: <bc47d3330712161758j1cc895f1r51238fdd9cce3595@mail.gmail.com>

RSiteSearch("AUC")

would lead you to

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46416.html

On Dec 13, 2007 12:38 PM, Armin Goralczyk <agoralczyk at gmail.com> wrote:
> Hello
>
> Is there an easy way, i.e. a function in a package, to calculate the
> area under the curve (AUC) for drug serum levels?
>
> Thanks for any advice
> --
> Armin Goralczyk, M.D.
> --
> Universit?tsmedizin G?ttingen
> Abteilung Allgemein- und Viszeralchirurgie
> Rudolf-Koch-Str. 40
> 39099 G?ttingen
> --
> Dept. of General Surgery
> University of G?ttingen
> G?ttingen, Germany
> --
> http://www.chirurgie-goettingen.de


From spencer.graves at pdf.com  Mon Dec 17 03:44:53 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 16 Dec 2007 18:44:53 -0800
Subject: [R] more structure than 'str'?
Message-ID: <4765E2A5.4010809@pdf.com>

      How can I see more of the structure than displayed by 'str'?  
Consider the following: 


tstDF <- data.frame(a=1, row.names='b')
 > str(tstDF)
'data.frame':   1 obs. of  1 variable:
 $ a: num 1

     
      The object 'tstDF' has row.names, but I have to suspect they are 
there -- AND know a function like 'row.names' or 'dimnames' -- to see 
them. 

      I've found 'str' extremely valuable for understanding and 
explaining to others the internal structure of an R object.   In many 
cases, it has helped me find fairly simple ways to do things with R 
objects that might have been much more difficult and perhaps infeasible 
without 'str' -- and without access to the right expert, who may not be 
available in the time I have to solve a particular problem. 

      Thanks again to Martin Maechler, who wrote 'str', and to everyone 
else who has replied to questions from me over the years. 

      Best Wishes,
      Spencer Graves
p.s.  'methods(class="data.frame")' would help in this example.  
However, 'methods' won't always find all available and recommended 
extractor functions.  For example, 'methods(class="lm")' was not able to 
find 'AIC' for me just now.


From cberry at tajo.ucsd.edu  Mon Dec 17 04:08:00 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Sun, 16 Dec 2007 19:08:00 -0800
Subject: [R] more structure than 'str'?
In-Reply-To: <4765E2A5.4010809@pdf.com>
References: <4765E2A5.4010809@pdf.com>
Message-ID: <Pine.LNX.4.64.0712161904260.30215@tajo.ucsd.edu>

On Sun, 16 Dec 2007, Spencer Graves wrote:

>      How can I see more of the structure than displayed by 'str'?

str is generic.

> methods(str)
[1] str.data.frame* str.default*    str.dendrogram* str.logLik*     str.POSIXt*

    Non-visible functions are asterisked
>

As you see there is a data.frame method that chooses not to report on the 
"row.names" attribute.

You can get the default behavior by removing the class attribute:

> str(unclass(tstDF))
List of 1
  $ a: num 1
  - attr(*, "row.names")= chr "b"
>

HTH,

Chuck


> Consider the following:
>
>
> tstDF <- data.frame(a=1, row.names='b')
> > str(tstDF)
> 'data.frame':   1 obs. of  1 variable:
> $ a: num 1
>
>
>      The object 'tstDF' has row.names, but I have to suspect they are
> there -- AND know a function like 'row.names' or 'dimnames' -- to see
> them.
>
>      I've found 'str' extremely valuable for understanding and
> explaining to others the internal structure of an R object.   In many
> cases, it has helped me find fairly simple ways to do things with R
> objects that might have been much more difficult and perhaps infeasible
> without 'str' -- and without access to the right expert, who may not be
> available in the time I have to solve a particular problem.
>
>      Thanks again to Martin Maechler, who wrote 'str', and to everyone
> else who has replied to questions from me over the years.
>
>      Best Wishes,
>      Spencer Graves
> p.s.  'methods(class="data.frame")' would help in this example.
> However, 'methods' won't always find all available and recommended
> extractor functions.  For example, 'methods(class="lm")' was not able to
> find 'AIC' for me just now.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From ggrothendieck at gmail.com  Mon Dec 17 05:08:35 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Dec 2007 23:08:35 -0500
Subject: [R] more structure than 'str'?
In-Reply-To: <4765E2A5.4010809@pdf.com>
References: <4765E2A5.4010809@pdf.com>
Message-ID: <971536df0712162008m1815b799pfb93bf71907b9489@mail.gmail.com>

On Dec 16, 2007 9:44 PM, Spencer Graves <spencer.graves at pdf.com> wrote:
>      How can I see more of the structure than displayed by 'str'?
> Consider the following:
>
>
> tstDF <- data.frame(a=1, row.names='b')
>  > str(tstDF)
> 'data.frame':   1 obs. of  1 variable:
>  $ a: num 1
>
>
>      The object 'tstDF' has row.names, but I have to suspect they are
> there -- AND know a function like 'row.names' or 'dimnames' -- to see
> them.
>
>      I've found 'str' extremely valuable for understanding and
> explaining to others the internal structure of an R object.   In many
> cases, it has helped me find fairly simple ways to do things with R
> objects that might have been much more difficult and perhaps infeasible
> without 'str' -- and without access to the right expert, who may not be
> available in the time I have to solve a particular problem.
>
>      Thanks again to Martin Maechler, who wrote 'str', and to everyone
> else who has replied to questions from me over the years.
>
>      Best Wishes,
>      Spencer Graves


Try this:

> dput(tstDF)
structure(list(a = 1), .Names = "a", row.names = "b", class = "data.frame")


From jasonshi510 at hotmail.com  Mon Dec 17 06:43:18 2007
From: jasonshi510 at hotmail.com (Xin)
Date: Mon, 17 Dec 2007 05:43:18 -0000
Subject: [R] can R solve these paired equations
Message-ID: <BAY141-DAV8BDEFDAA6D93C575226E3F0620@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/6155b2b8/attachment.pl 

From cskiadas at gmail.com  Mon Dec 17 07:39:24 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Mon, 17 Dec 2007 01:39:24 -0500
Subject: [R] can R solve these paired equations
In-Reply-To: <BAY141-DAV8BDEFDAA6D93C575226E3F0620@phx.gbl>
References: <BAY141-DAV8BDEFDAA6D93C575226E3F0620@phx.gbl>
Message-ID: <338E5ABA-9EDE-41C7-A14F-4A92DFDDED0D@gmail.com>

Turn your problem into an optimization one and use the various  
optimization abilities of R, for instance have a look at nlm:

?nlm

As an example to solve the rather simple:
x-y=0
x+y=2

We could do:

f <- function(vals) {
	x <- vals[1]
	y <- vals[2]
	sum(c(x-y,x+y-2)^2)
}
nlm(f, c(2,2))

PS: A google search for "rhelp solving nonlinear system of equations"  
would likely have given you some starting points.
PS2: Perhaps someone more expert could comment on the differences  
between optim, nlm and nlminb.

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


On Dec 17, 2007, at 12:43 AM, Xin wrote:

> Dear:
>
>
>
> I have a paired equation below. Can I solve (x,y) using R.
>
>
>
> Thanks!
>
>
>
> Xin
>
>
>
> A=327.727
>
> B=9517.336
>
> p=0.114^10
>
>
>
> (1-p)*y*(1-x)/x/(1-x^y)=A
>
> A(1+(1-x)*(1+y)/x-A))=B


From rh at family-krueger.com  Mon Dec 17 08:10:58 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Mon, 17 Dec 2007 08:10:58 +0100
Subject: [R] calculating the number of days from dates
In-Reply-To: <18275.44649.487754.926370@ada-stat.math.ethz.ch>
References: <mailman.27.1197457204.24367.r-help@r-project.org>	<20071214120132.9A9785955DD@borg.st.net.au>	<47638496.9070905@family-krueger.com>
	<18275.44649.487754.926370@ada-stat.math.ethz.ch>
Message-ID: <47662102.7050200@family-krueger.com>


> it's  a  >> package <<  , not a library, please!
>
>   
Sorry for using library instead package, but

library() is one command for using packages.

Therefore I (and it seems that i am not the only one) used library instead package.

Knut


From r.hankin at noc.soton.ac.uk  Mon Dec 17 09:24:10 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 17 Dec 2007 08:24:10 +0000
Subject: [R] can R solve these paired equations
In-Reply-To: <BAY141-DAV8BDEFDAA6D93C575226E3F0620@phx.gbl>
References: <BAY141-DAV8BDEFDAA6D93C575226E3F0620@phx.gbl>
Message-ID: <365C228A-78EC-453A-BDFA-3B8DF14F6DD3@noc.soton.ac.uk>

Hello

[An answer was posted just now using numerical ideas;
here is an answer from a symbolic perspective]

These equations involve x^y in more than one unknown,
so inverse functions cannot be used.

I do not think you will be able to characterize even the number
of solutions, let alone their nature.

To see the difficulty, look at the Lambert W function.

My advice would be to simplify, simplify, simplify
your problem as far as possible; remove terms
successively until you are left with a trivial
system, then add *one* term and see if
this produces any insights.

HTH

rksh



On 17 Dec 2007, at 05:43, Xin wrote:

> Dear:
>
>
>
> I have a paired equation below. Can I solve (x,y) using R.
>
>
>
> Thanks!
>
>
>
> Xin
>
>
>
> A=327.727
>
> B=9517.336
>
> p=0.114^10
>
>
>
> (1-p)*y*(1-x)/x/(1-x^y)=A
>
> A(1+(1-x)*(1+y)/x-A))=B
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst and Neutral Theorist,
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From ileyer at yahoo.de  Mon Dec 17 09:31:01 2007
From: ileyer at yahoo.de (Ilona Leyer)
Date: Mon, 17 Dec 2007 09:31:01 +0100 (CET)
Subject: [R] convergence error code in mixed effects models
In-Reply-To: <40e66e0b0712141227s6ba38a72iebdb8fcc8c720a43@mail.gmail.com>
Message-ID: <514845.7750.qm@web26202.mail.ukl.yahoo.com>

Thank you for your helpful answer. The only thing
obout what I still wonder is that some of the analyses
work with R some time and some time not without any
changes in the data or commands.
Your text (from both emails) is cut at the end, so
that  your information about the convergence error
problem is  not complete. Would you send it again?
Than you for your help!

Ilona


--- Douglas Bates <bates at stat.wisc.edu> schrieb:

> Thank you for sending the data.  It is very helpful
> in understanding
> the nature of the problem.
> 
> For example, in your original description of your
> study you referred
> to week as a factor, which is a completely
> reasonable term, but I
> mistakenly thought that you meant an object of class
> "factor" and that
> was why I replied that you would be estimating too
> many variances and
> covariances.
> 
> I can tell you why you are having problems fitting a
> mixed-effects
> model.  Strangely it is because there is too little
> variability in the
> patterns across the replicates, especially at the
> early times.  The
> leaf number is discrete with a small range (all the
> leaffra
> observations in this example are 4, 5, 6, 7 or 8)
> and non-decreasing
> over time.  (I assume the nature of the experiment
> is such that the
> leaf number is necessarily non-decreasing.)  That
> doesn't allow for
> many patterns.  I'm sure some clever person reading
> this will be able
> to tell us exactly how many different such patterns
> you could get but
> I will simply say "not many".
> 
> Notice that the first 10 lines show that the leaffra
> is 4 at week 4 in
> *every* replicate.  There isn't a whole lot of
> variation here for the
> random effects to model.
> 
> The best place to start is with a plot of the data. 
> I changed the
> levels of the rep factor to "f1"-"f5" and "s1"-"s5"
> to indicate that
> each rep is at one level of the treat. (Those who
> are playing along at
> home should be careful of the ordering of the
> original levels because
> ID10, which I now call s5, occurs between ID1 and
> ID2.)
> 
> With this set of labels the cross-tabulation and
> treat should be
> 
> > xtabs(~ rep + treat, leaf)
>     treat
> rep  pHf pHs
>   f1   4   0
>   f2   4   0
>   f3   4   0
>   f4   4   0
>   f5   4   0
>   s1   0   4
>   s2   0   4
>   s3   0   4
>   s4   0   4
>   s5   0   4
> 
> Now look at lattice plots such as
> 
> library(lattice)
> xyplot(heightfra ~ week | rep, leaf, type = c("g",
> "p", "r"), layout =
> c(5,2), aspect = 'xy', groups = treat)
> 
> (I enclose PDF files of these plots for each of the
> three responses.)
> First you can see that there is very little
> variation at the low end.
> Strangely enough, this causes a problem in fitting
> mixed-effects
> models because the mle's of the variances of  the
> random effects for
> the intercept will be zero.  The lme function does
> not handle this
> gracefully.  The lmer function from the lme4 package
> does a better job
> on this type of model.
> 
> Also, note that the pattern of heightfra over time
> is not linear.  It
> is consistently concave down.  Thuso a mixed-effects
> model that is
> linear in week will miss much of the structure in
> the data.
> 
> The point of R is to encourage you to explore your
> data rather than
> subjecting it to a "canned" analysis.  You could try
> fitting a
> mixed-effects model to these data in SAS PROC MIXED
> or SPSS MIXED and
> I have no doubt that those packages would give you
> estimates (not to
> mention p-values, something that the author of lmer
> has been woefully
> negligent in not providing :-) but you probably
> won't get much of a
> hint that the model doesn't make sense.  I would
> prefer to start with
> the plot and see what the data have to say.
> 
> 
> The technical problem with convergence in lme is
> that the mle of the
> variance of the intercept term is zero.  You can see
> that if you use
> lmer from the lme4 package instead to fit the model.
> the random effect for the intercept is estimate
> On Dec 14, 2007 4:40 AM, Ilona Leyer
> <ileyer at yahoo.de> wrote:
> >
> > Here an simple example:
> >
> > rep     treat   heightfra       leaffra leafvim
> week
> > ID1     pHf     1.54    4       4       4
> > ID2     pHf     1.49    4       4       4
> > ID3     pHf     1.57    4       5       4
> > ID4     pHf     1.48    4       4       4
> > ID5     pHf     1.57    4       4       4
> > ID6     pHs     1.29    4       5       4
> > ID7     pHs     0.97    4       5       4
> > ID8     pHs     2.06    4       4       4
> > ID9     pHs     0.88    4       4       4
> > ID10    pHs     1.47    4       4       4
> > ID1     pHf     3.53    5       6       6
> > ID2     pHf     4.08    6       6       6
> > ID3     pHf     3.89    6       6       6
> > ID4     pHf     3.78    5       6       6
> > ID5     pHf     3.92    6       6       6
> > ID6     pHs     2.76    5       5       6
> > ID7     pHs     3.31    6       7       6
> > ID8     pHs     4.46    6       7       6
> > ID9     pHs     2.19    5       5       6
> > ID10    pHs     3.83    5       5       6
> > ID1     pHf     5.07    7       7       9
> > ID2     pHf     6.42    7       8       9
> > ID3     pHf     5.43    6       8       9
> > ID4     pHf     6.83    6       8       9
> > ID5     pHf     6.26    6       8       9
> > ID6     pHs     4.57    6       9       9
> > ID7     pHs     5.05    6       7       9
> > ID8     pHs     6.27    6       8       9
> > ID9     pHs     3.37    5       7       9
> > ID10    pHs     5.38    6       8       9
> > ID1     pHf     5.58    7       9       12
> > ID2     pHf     7.43    8       9       12
> > ID3     pHf     6.18    8       10      12
> > ID4     pHf     6.91    7       10      12
> > ID5     pHf     6.78    7       10      12
> > ID6     pHs     4.99    6       13      12
> > ID7     pHs     5.50    7       8       12
> > ID8     pHs     6.56    7       10      12
> > ID9     pHs     3.72    6       10      12
> > ID10    pHs     5.94    6       10      12
> >
> >
> > I used the procedure described in Crawley?s new R
> > Book.
> > For two of the tree response variables
> > (heightfra,leaffra) it doesn?t work, while it
> worked
> > with leafvim (but in another R session, yesterday,
> > leaffra worked as well...).
> >
> > Here the commands:
> >
> > > attach(test)
> > > names(test)
> > [1] "week"      "rep"       "treat"    
> "heightfra"
> > "leaffra"   "leafvim"
> > > library(nlme)
> > >
> >
>
test<-groupedData(heightfra~week|rep,outer=~treat,test)
> > > model1<-lme(heightfra~treat,random=~week|rep)
> > Error in lme.formula(heightfra ~ treat, random =
> ~week
> > | rep) :
> >         nlminb problem, convergence error code =
> 1;
> > message = iteration limit reached without
> convergence
> > (9)
> >
> > >
> >
>
test<-groupedData(leaffra~week|rep,outer=~treat,test)
> > > model2<-lme(leaffra~treat,random=~week|rep)
> 
=== message truncated ===


From leffgh at 163.com  Mon Dec 17 09:37:46 2007
From: leffgh at 163.com (Bin Yue)
Date: Mon, 17 Dec 2007 00:37:46 -0800 (PST)
Subject: [R] re store pictures in an automatically named files using loops
Message-ID: <14370153.post@talk.nabble.com>


Dear all:
     I hope that the file in which the picture is stored has the same name
as the " main" title of the picture . But it turns out that , the name of
the file is :names(spl.sp)[i].bmp,while the main title of the picture is
names(spl.sp)[i+1]
 It seems that the problem is related to the "device". But I am finding it
very difficult to understand .
  Do you have any idea about that?

Here is my function:
 
sp.drawK<-function(spl.sp){
env<-list() 
for(i in 1:length(spl.sp)){
if(spl.sp[[i]]$n==1) next
else{
env[[i]]<-envelope(spl.sp[[i]],Kest)
  plot(env[[i]],main=names(spl.sp)[i])
  legend("topleft",lty=1:4,col=1:4,legend=c("obs","theo","hi","lo"))
  text(2,500,cex=0.8, paste("n=",spl.sp[[i]]$n,sep=""))
  bmp(paste(names(spl.sp)[i],".bmp",sep=""))

  }
}
}

   Best wishes,
  Bin Yue


-----
Best regards,
Bin Yue

*************
student for a Master program in South Botanical Garden , CAS

-- 
View this message in context: http://www.nabble.com/restore--pictures-in-an-automatically-named-files-using-loops-tp14370153p14370153.html
Sent from the R help mailing list archive at Nabble.com.


From rh at family-krueger.com  Mon Dec 17 09:42:03 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Mon, 17 Dec 2007 09:42:03 +0100
Subject: [R] RMySQL installation problem - partially solved
In-Reply-To: <476384AE.5010500@family-krueger.com>
References: <BAY120-W88A17C859B8C463333777C7820@phx.gbl>	<Pine.LNX.4.64.0711152120050.7199@gannet.stats.ox.ac.uk>
	<476384AE.5010500@family-krueger.com>
Message-ID: <4766365B.7010707@family-krueger.com>

It seems to be an 2.6.1 Version problem.
I tried to use
http://umfragen.sowi.uni-mainz.de/CRAN/bin/windows/contrib/2.2/DBI_0.1-10.zip
with 2.6.1 and 2.2.0
It is working with 2.2.0 (build under R Version 2.2.1)

Knut


From andreas at maunz.de  Mon Dec 17 09:42:14 2007
From: andreas at maunz.de (Andreas Maunz)
Date: Mon, 17 Dec 2007 09:42:14 +0100
Subject: [R] kernlab and gram matrix
Message-ID: <47663666.8000707@maunz.de>

Hi, this is a question about the R package kernlab.

I use kernlab as a library in a C++ program. The host application 
defines a graph kernel (defined by me), generates a gram matrix and 
trains kernlab directly on this gram matrix, like this:

regm<-ksvm(K,y,kernel="matrix"),

where K is the n x n gram kernelMatrix of my kernel, and y is the 
R-vector of quantitative target values.
So, to make sure you got it: I don't want kernlab to compute the kernel 
values by itself. Rather, this is a task for the host application.

Learning (see above) works well, but how do I predict a new instance? I 
couldn't find any information in this respect in the manual. The only 
examples for prediction were concerned with data from the input space, 
which i don't have, since my input space consists of graphs. I tried the 
following:

predict(regm,x,type="response")

where x is the 1xn R-matrix containing kernel values between the 
instance to be predicted and my training points. This won't work:

Error in as.matrix(Z) : object "Z" not found.

I'm using the current CRAN version of kernlab. Any help by kernlab users 
who had a similar task to do would be appreciated.

Best regards,
Andreas Maunz
-- 
http://www.maunz.de

       Yoda of Borg are we: Futile is resistance. Assimilate you, we will.


From n.lapidus at gmail.com  Mon Dec 17 09:57:49 2007
From: n.lapidus at gmail.com (N. Lapidus)
Date: Mon, 17 Dec 2007 09:57:49 +0100
Subject: [R] Function for AUC?
In-Reply-To: <bc47d3330712161758j1cc895f1r51238fdd9cce3595@mail.gmail.com>
References: <a695fbee0712131038l3f94dbb0va55fe1fadc29418f@mail.gmail.com>
	<bc47d3330712161758j1cc895f1r51238fdd9cce3595@mail.gmail.com>
Message-ID: <70a3a4850712170057i104bc793qcbecffecaa310bd7@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20071217/1e517255/attachment.pl 

From s.wood at bath.ac.uk  Mon Dec 17 09:54:29 2007
From: s.wood at bath.ac.uk (Simon Wood)
Date: Mon, 17 Dec 2007 08:54:29 +0000
Subject: [R] Two repeated warnings when runing gam(mgcv) to analyze my
	dataset?
In-Reply-To: <2fc17e30712130946x7c553885m6c0640a0f8c47d5c@mail.gmail.com>
References: <2fc17e30712130946x7c553885m6c0640a0f8c47d5c@mail.gmail.com>
Message-ID: <200712170854.29191.s.wood@bath.ac.uk>

What mgcv version are you running (and on what platform)?

n Thursday 13 December 2007 17:46, zhijie zhang wrote:
> Dear all,
>  I run the GAMs (generalized additive models) in gam(mgcv) using the
> following codes.
>
> m.gam
> <-gam(mark~s(x)+s(y)+s(lstday2004)+s(ndvi2004)+s(slope)+s(elevation)+disbin
>ary,family=binomial(logit),data=point)
>
>  And two repeated warnings appeared.
> Warnings?
> 1: In gam.fit(G, family = G$family, control = control, gamma = gamma,  ...
> : Algorithm did not converge
>
> 2: In gam.fit(G, family = G$family, control = control, gamma = gamma,  ...
> : fitted probabilities numerically 0 or 1 occurred
>
> Q1: For warning1, could it be solved by changing the value of
> mgcv.toloptions for
> gam.control(mgcv.tol=1e-7)?
>
> Q1: For warning2, is there any impact for the results if the "fitted
> probabilities numerically 0 or 1 occurred" ?  How can i solve it?
>
>  I didn't try the possible solutions for them, because it took such a
> longer time to run the whole programs.
>  Could anybody suggest their solutions?
>  Any help or suggestions are greatly appreciated.
>   Thanks.

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283 


From ozric at web.de  Mon Dec 17 10:51:00 2007
From: ozric at web.de (Christian Schulz)
Date: Mon, 17 Dec 2007 10:51:00 +0100
Subject: [R] paste dependent variable in  formula (rpart)?
In-Reply-To: <4765B435.4030000@statistik.uni-dortmund.de>
References: <476579AA.2020604@web.de>
	<4765B435.4030000@statistik.uni-dortmund.de>
Message-ID: <47664684.7070108@web.de>


> Christian Schulz wrote:
>   
>> Hello,
>>
>> i'm trying to replace  different  target variables in rpart with a 
>> function. The data.frame getting always the target variable as last column.
>> Try below, i get the target variable in the explained variables, too!?  
>> Have anybody an advice to avoid this.
>>
>> rp1 <- rpart(eval(parse(text=paste(names(train[length(train)])))) ~ . , 
>>     
>
> I guess you want something along the following example:
>
>    train <- iris
>    form <- as.formula(paste(names(train)[length(train)], "~ ."))
>    rpart(form, data = iris)
>
> or some data.frame method for rpart....
>   
yes ,many thanks!
Christian

>   
> Uwe Ligges
>
>
>   
>> data=train,cp=0.0001)
>>
>> regards & many thanks
>> Christian
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>     
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From agoralczyk at gmail.com  Mon Dec 17 11:07:25 2007
From: agoralczyk at gmail.com (Armin Goralczyk)
Date: Mon, 17 Dec 2007 11:07:25 +0100
Subject: [R] Function for AUC?
In-Reply-To: <70a3a4850712170057i104bc793qcbecffecaa310bd7@mail.gmail.com>
References: <a695fbee0712131038l3f94dbb0va55fe1fadc29418f@mail.gmail.com>
	<bc47d3330712161758j1cc895f1r51238fdd9cce3595@mail.gmail.com>
	<70a3a4850712170057i104bc793qcbecffecaa310bd7@mail.gmail.com>
Message-ID: <a695fbee0712170207n110b18c1gb237d0e68faa7d39@mail.gmail.com>

Hi all

On Dec 17, 2007 9:57 AM, N. Lapidus <n.lapidus at gmail.com> wrote:
> Hi Armin,
> Do you know the rocr package ? This is very easy to draw ROC curves and to
> calculate AUC with it.
> http://rocr.bioinf.mpi-sb.mpg.de/
> Hope this will help.
>

I know ROCR although I am not familiar with ROC. I have seen the AUC
function of ROCR, but are you sure that this is applicable to the
stated problem, i.e. calculate AUC of serum levels of a drug over
time?! ROC seems to handle completely different problems.

I have been reading the following post (thanks for the hint Stephen):

> On Dec 17, 2007 2:58 AM, Stephen Weigand <weigand.stephen at gmail.com> wrote:
>
> > RSiteSearch("AUC")
> >
> > would lead you to
> >
> > http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46416.html
> >

I tried it:

> y<-c(1,2,3,4,5);x<-c(10,15,10,5,0)
> trap.rule <- function(x,y) sum(diff(x)*(y[-1]+y[-length(y)]))/2
> trap.rule(x,y)
[1] -45

It is not the correct value, but the formula seems applicable and I
changed it to

> auc <- function(x,y) sum((x[-length(x)] + x[-1]) * (y[-1]-y[-length(y)]))/2
> auc(x,y)
[1] 35

which seems to be correct. I hope everyone agees.
I didn't know it's that simple. I guess I don't need another function.
Thank's for all the help and all the suggestions.

-- 
Armin Goralczyk, M.D.
--
Universit?tsmedizin G?ttingen
Abteilung Allgemein- und Viszeralchirurgie
Rudolf-Koch-Str. 40
39099 G?ttingen
--
Dept. of General Surgery
University of G?ttingen
G?ttingen, Germany
--
http://www.chirurgie-goettingen.de

From ripley at stats.ox.ac.uk  Mon Dec 17 11:08:57 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 17 Dec 2007 10:08:57 +0000 (GMT)
Subject: [R] Rd files with unknown encoding?
In-Reply-To: <47647F95.10505@pdf.com>
References: <4760AFED.5050908@pdf.com>
	<Pine.LNX.4.64.0712131006040.7003@gannet.stats.ox.ac.uk>
	<47647F95.10505@pdf.com>
Message-ID: <Pine.LNX.4.64.0712171004100.28418@auk.stats>

Here's a slightly cleaner version:

showNonASCII <- function(x)
{
     ind <- is.na(iconv(x, "latin1", "ASCII"))
     xxx <- iconv(x[ind], "latin1", "ASCII", sub="byte")
     if(any(ind)) cat(which(ind), ": ", xxx, "\n", sep="")
}

used as

> showNonASCII(readLines("foo.Rd"))

On Sat, 15 Dec 2007, Spencer Graves wrote:

> Dear Prof. Ripley:
>     Thanks very much.  I did as you suggested, which I'll outline here to 
> make it easier for anyone else who might have a similar problem:
>          * Read the offending *.Rd file in R using 'readLines'
>          * Applied 'iconv' to the character vector, following the last 
> example in the help file.  This translated all offending characters into a 
> multi-character sequence starting with '<'.
>          * Used 'regexpr' to find all occurrences of '<'.
>     The latter identified other uses of '<' but produced a sufficiently 
> short list that I was able to find the problems fairly easily.
>     Thanks again.
>     Spencer Graves   p.s.  And in the future, I will refer 'Rd' questions to 
> 'R-devel', per your suggestion. 
> Prof Brian Ripley wrote:
>> On Wed, 12 Dec 2007, Spencer Graves wrote:
>>
>>
>>>      How can I identify the problem generating a warning in R CMD check
>>> for "Rd files with unknown encoding"?
>>>
>>>      Google identified an email from John Fox with a reply from Brian
>>> Ripley about this last 12 Jun 2007.
>>> 
>> 
>> But not on this list:
>> 
>> https://stat.ethz.ch/pipermail/r-devel/2007-June/046055.html
>> 
>> R-devel would have been more appropriate for this too.
>>
>>
>>>  This suggests that I may have accidentally entered some possibly 
>>> non-printing character into the offending Rd file.  The message tells me 
>>> which file, but I don't know which lines in the file.  Is there some way 
>>> of finding the offending character(s) without laboriously running R CMD 
>>> check after deleting different portions of the file until I isolate the 
>>> problem?
>>> 
>> 
>> I did say so in that thread:
>> 
>> https://stat.ethz.ch/pipermail/r-devel/2007-June/046061.html
>> 
>> You can do much the same in R via iconv("", "C", sub="byte"), provided you 
>> can read the file in (it may not be representable in your current locale, 
>> but you could run R in a Latin-1 locale, if your OS has one).
>>
>>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ozric at web.de  Mon Dec 17 11:32:59 2007
From: ozric at web.de (Christian Schulz)
Date: Mon, 17 Dec 2007 11:32:59 +0100
Subject: [R] RMySQL installation problem - partially solved
In-Reply-To: <4766365B.7010707@family-krueger.com>
References: <BAY120-W88A17C859B8C463333777C7820@phx.gbl>	<Pine.LNX.4.64.0711152120050.7199@gannet.stats.ox.ac.uk>	<476384AE.5010500@family-krueger.com>
	<4766365B.7010707@family-krueger.com>
Message-ID: <4766505B.1060407@web.de>

I think you should use the newest DBI Version with 2.6.1.
regards, christian
> It seems to be an 2.6.1 Version problem.
> I tried to use
> http://umfragen.sowi.uni-mainz.de/CRAN/bin/windows/contrib/2.2/DBI_0.1-10.zip
> with 2.6.1 and 2.2.0
> It is working with 2.2.0 (build under R Version 2.2.1)
>
> Knut
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From yn19832 at msn.com  Mon Dec 17 11:54:31 2007
From: yn19832 at msn.com (livia)
Date: Mon, 17 Dec 2007 02:54:31 -0800 (PST)
Subject: [R]  Parse Expression
Message-ID: <14370218.post@talk.nabble.com>


Hello everyone, I would like to construct a datafram with the following
command. 

eval(parse(text=paste("df=data.frame(", cmd, ")", sep="")))

But it comes out " Error in parse(file, n, text, prompt, srcfile, encoding)
: 
        syntax error, unexpected $undefined, expecting ',' in
"df=data.frame(cbcDummy10to12 = 1,cbcForeWrld_Ret = 1,cbcYC10-2_"


"cmd" is a character string I obtained before, it is like:
 
cmd
[1] "cbcDummy10to12 = 1,cbcForeWrld_Ret = 1,cbcYC10-2_wld = 1"

Could anyone give me some advice? Many thanks.


-- 
View this message in context: http://www.nabble.com/Parse-Expression-tp14370218p14370218.html
Sent from the R help mailing list archive at Nabble.com.


From rh at family-krueger.com  Mon Dec 17 12:10:51 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Mon, 17 Dec 2007 12:10:51 +0100
Subject: [R] RMySQL installation problem - partially solved
In-Reply-To: <4766505B.1060407@web.de>
References: <BAY120-W88A17C859B8C463333777C7820@phx.gbl>	<Pine.LNX.4.64.0711152120050.7199@gannet.stats.ox.ac.uk>	<476384AE.5010500@family-krueger.com>
	<4766365B.7010707@family-krueger.com> <4766505B.1060407@web.de>
Message-ID: <4766593B.2060907@family-krueger.com>

Christian Schulz schrieb:
>
> I think you should use the newest DBI Version with 2.6.1.
> regards, christian
I run *actualice packages*,
 this should update to the newest DBI, shouldn't it?
Knut


From Richard.Cotton at hsl.gov.uk  Mon Dec 17 12:20:51 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Mon, 17 Dec 2007 11:20:51 +0000
Subject: [R] ls() pattern
In-Reply-To: <BAY109-W270B8BA4544CA9ADF9E41AE3670@hsl.gov.uk>
Message-ID: <OFB545B3E4.09E03B83-ON802573B4.003E04BE-802573B4.003E5139@hsl.gov.uk>

>    I am sorry I have to bother you again. I am trying your conmmands
> and until the "cmd" part, it looks fine.
>    So I call "cmd", it gives the following output:
> 
>   [1] "cbcDummy10to12 = 1,cbcForeWrld_Ret = 0.04,cbcYC10-2_wld = 64,
> cbcEP_us = 0.5,cbcDelta102YC_wld = 8,cbcGold = 2"
> 
>    But after that, if I call: eval(parse(text=paste("df=data.
> frame(", cmd, ")", sep="")))
> 
>    It says: Error in parse(file, n, text, prompt, srcfile, encoding) : 
>     syntax error, unexpected $undefined, expecting ',' in "df=data.
> frame(cbcDummy10to12 = 1,cbcForeWrld_Ret = 0.04,cbcYC10-2_"

cbcYC10-2_wld is an invalid variable name, since it contains a minus sign.

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From S.Ellison at lgc.co.uk  Mon Dec 17 12:47:47 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Mon, 17 Dec 2007 11:47:47 +0000
Subject: [R] RMySQL installation problem - partially solved
Message-ID: <s76661eb.052@tedmail.lgc.co.uk>

.. but don;t forget to add checkBuilt=TRUE , otherwise you may find (as
I did) that you update packages for your current version, but not those
built for a previous version of R.


>>> Knut Krueger <rh at family-krueger.com> 17/12/2007 11:10:51 >>>
Christian Schulz schrieb:
>
> I think you should use the newest DBI Version with 2.6.1.
> regards, christian
I run *actualice packages*,
 this should update to the newest DBI, shouldn't it?
Knut

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Mon Dec 17 12:50:58 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 17 Dec 2007 06:50:58 -0500
Subject: [R] Parse Expression
In-Reply-To: <14370218.post@talk.nabble.com>
References: <14370218.post@talk.nabble.com>
Message-ID: <476662A2.5000708@stats.uwo.ca>

livia wrote:
> Hello everyone, I would like to construct a datafram with the following
> command. 
>
> eval(parse(text=paste("df=data.frame(", cmd, ")", sep="")))
>
> But it comes out " Error in parse(file, n, text, prompt, srcfile, encoding)
> : 
>         syntax error, unexpected $undefined, expecting ',' in
> "df=data.frame(cbcDummy10to12 = 1,cbcForeWrld_Ret = 1,cbcYC10-2_"
>
>
> "cmd" is a character string I obtained before, it is like:
>  
> cmd
> [1] "cbcDummy10to12 = 1,cbcForeWrld_Ret = 1,cbcYC10-2_wld = 1"
>
> Could anyone give me some advice? Many thanks.
>   
You can't have a hyphen "-" in a variable name, i.e. cbcYC10-2_wld is 
not legal.  It is treated as a subtraction.
I think the "$undefined" refers to 2_wld; the parser has no idea what 
that is supposed to be. 

Duncan Murdoch


>
>


From epistat at gmail.com  Mon Dec 17 12:53:54 2007
From: epistat at gmail.com (zhijie zhang)
Date: Mon, 17 Dec 2007 19:53:54 +0800
Subject: [R] Two repeated warnings when runing gam(mgcv) to analyze my
	dataset?
In-Reply-To: <200712170854.29191.s.wood@bath.ac.uk>
References: <2fc17e30712130946x7c553885m6c0640a0f8c47d5c@mail.gmail.com>
	<200712170854.29191.s.wood@bath.ac.uk>
Message-ID: <2fc17e30712170353j5837068bx8fecb645978df2d1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/1b13bedd/attachment.pl 

From f.harrell at vanderbilt.edu  Mon Dec 17 13:16:45 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 17 Dec 2007 06:16:45 -0600
Subject: [R] eee pc
In-Reply-To: <9402113c0712160956o7fdb786drcc5d24cbe3ac57b9@mail.gmail.com>
References: <9402113c0712160956o7fdb786drcc5d24cbe3ac57b9@mail.gmail.com>
Message-ID: <476668AD.4000405@vanderbilt.edu>

Burton Rothberg wrote:
> I'm thinking of getting one of these lightweight linux laptops for
> traveling. Does anyone know if / how R runs on it?

The  eee is getting good reviews but an in-depth review in 
linuxtoday.com yesterday makes me want to wait.  Battery life and heat 
production are problematic among other things.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From rh at family-krueger.com  Mon Dec 17 13:17:56 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Mon, 17 Dec 2007 13:17:56 +0100
Subject: [R] RMySQL installation problem - partially solved
In-Reply-To: <s76661eb.051@tedmail.lgc.co.uk>
References: <s76661eb.051@tedmail.lgc.co.uk>
Message-ID: <476668F4.7090404@family-krueger.com>

S Ellison schrieb:
> .. but don;t forget to add checkBuilt=TRUE , otherwise you may find (as
> I did) that you update packages for your current version, but not those
> built for a previous version of R.
Thanks, adiidional packages were loaded but
I tried this in 2.2.0  / 2.4.0 and 2.6.1 ist only found in RMySQL.dll in
2.2.0

Knut


From leffgh at 163.com  Mon Dec 17 13:20:11 2007
From: leffgh at 163.com (Bin Yue)
Date: Mon, 17 Dec 2007 04:20:11 -0800 (PST)
Subject: [R] re store pictures in an automatically named files using
 loops
In-Reply-To: <14370153.post@talk.nabble.com>
References: <14370153.post@talk.nabble.com>
Message-ID: <14370262.post@talk.nabble.com>


 I put bmp() before the plot() syntax ,
 then the code worked.



Bin Yue wrote:
> 
> Dear all:
>      I hope that the file in which the picture is stored has the same name
> as the " main" title of the picture . But it turns out that , the name of
> the file is :names(spl.sp)[i].bmp,while the main title of the picture is
> names(spl.sp)[i+1]
>  It seems that the problem is related to the "device". But I am finding it
> very difficult to understand .
>   Do you have any idea about that?
> 
> Here is my function:
>  
> sp.drawK<-function(spl.sp){
> env<-list() 
> for(i in 1:length(spl.sp)){
> if(spl.sp[[i]]$n==1) next
> else{
> env[[i]]<-envelope(spl.sp[[i]],Kest)
>   plot(env[[i]],main=names(spl.sp)[i])
>   legend("topleft",lty=1:4,col=1:4,legend=c("obs","theo","hi","lo"))
>   text(2,500,cex=0.8, paste("n=",spl.sp[[i]]$n,sep=""))
>   bmp(paste(names(spl.sp)[i],".bmp",sep=""))
> 
>   }
> }
> }
> 
>    Best wishes,
>   Bin Yue
> 
> 


-----
Best regards,
Bin Yue

*************
student for a Master program in South Botanical Garden , CAS

-- 
View this message in context: http://www.nabble.com/restore--pictures-in-an-automatically-named-files-using-loops-tp14370153p14370262.html
Sent from the R help mailing list archive at Nabble.com.


From elke.moons at uhasselt.be  Mon Dec 17 14:33:29 2007
From: elke.moons at uhasselt.be (Elke Moons)
Date: Mon, 17 Dec 2007 14:33:29 +0100
Subject: [R] polygon class in splancs package
Message-ID: <007b01c840b1$6979fb10$3d1ec454@ibis.luc.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/8db9740e/attachment.pl 

From jonas.malmros at gmail.com  Mon Dec 17 15:08:09 2007
From: jonas.malmros at gmail.com (Jonas Malmros)
Date: Mon, 17 Dec 2007 15:08:09 +0100
Subject: [R] How to create a mixed col.names?
Message-ID: <fd3c7adf0712170608r444e009djc63c489eb2627e78@mail.gmail.com>

Hello,

I have a vector of names, say :

names <- c("Factor 1", "Factor 2", Factor 3")

I am creating a dataframe and I want the column names to be mixed like this:
"Factor 1" " Sign Factor 1" "Factor 2" "Sign Factor 2" "Factor 3"
"Sign Factor 3"
How can I automate the creation of such a mixed vector? I tried with
rep but did not succeed.

Could someone please suggest a solution to this problem?
Thanks in advance!

Regards,
JM
-- 
Jonas Malmros
Stockholm University
Stockholm, Sweden


From agoralczyk at gmail.com  Mon Dec 17 15:18:03 2007
From: agoralczyk at gmail.com (Armin Goralczyk)
Date: Mon, 17 Dec 2007 15:18:03 +0100
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
Message-ID: <a695fbee0712170618t74888388sc303646454ee65db@mail.gmail.com>

On Dec 15, 2007 6:31 PM, David Winsemius <dwinsemius at comcast.net> wrote:

> After quite a bit of hacking (in the sense of ineffective chopping with
> a dull ax), I finally came up with:
>
> pm.srch<- function (){
>   srch.stem<-"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term="
>   query<-readLines(con=file.choose())
>   query<-gsub("\\\"","",x=query)
>   doc<-xmlTreeParse(paste(srch.stem,query,sep=""),isURL = TRUE,
>                      useInternalNodes = TRUE)
>   return(sapply(c("//Id"), xpathApply, doc = doc, fun = xmlValue) )
>      }
>
> pm.srch()  #choosing the search-file
>       //Id
>  [1,] "18046565"
>  [2,] "17978930"
>  [3,] "17975511"
>  [4,] "17935912"
>  [5,] "17851940"
>  [6,] "17765779"
>  [7,] "17688640"
>  [8,] "17638782"
>  [9,] "17627059"
> [10,] "17599582"
> [11,] "17589729"
> [12,] "17585283"
> [13,] "17568846"
> [14,] "17560665"
> [15,] "17547971"
> [16,] "17428551"
> [17,] "17419899"
> [18,] "17419519"
> [19,] "17385606"
> [20,] "17366752"

I tried the example above, but only the first 20 PMIDs will be
returned. How can I circumvent this (I guesss its a restraint from
pubmed)?
-- 
Armin Goralczyk, M.D.
--
Universit?tsmedizin G?ttingen
Abteilung Allgemein- und Viszeralchirurgie
Rudolf-Koch-Str. 40
39099 G?ttingen
--
Dept. of General Surgery
University of G?ttingen
G?ttingen, Germany
--
http://www.chirurgie-goettingen.de

From csardi at rmki.kfki.hu  Mon Dec 17 15:22:01 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Mon, 17 Dec 2007 15:22:01 +0100
Subject: [R] How to create a mixed col.names?
In-Reply-To: <fd3c7adf0712170608r444e009djc63c489eb2627e78@mail.gmail.com>
References: <fd3c7adf0712170608r444e009djc63c489eb2627e78@mail.gmail.com>
Message-ID: <20071217142201.GC20681@localdomain>

paste(rep(c("Factor", "Sign Factor"), 5), rep(1:5, each=2))

Replace '5' with the desired number,
Gabor

On Mon, Dec 17, 2007 at 03:08:09PM +0100, Jonas Malmros wrote:
> Hello,
> 
> I have a vector of names, say :
> 
> names <- c("Factor 1", "Factor 2", Factor 3")
> 
> I am creating a dataframe and I want the column names to be mixed like this:
> "Factor 1" " Sign Factor 1" "Factor 2" "Sign Factor 2" "Factor 3"
> "Sign Factor 3"
> How can I automate the creation of such a mixed vector? I tried with
> rep but did not succeed.
> 
> Could someone please suggest a solution to this problem?
> Thanks in advance!
> 
> Regards,
> JM
> -- 
> Jonas Malmros
> Stockholm University
> Stockholm, Sweden
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From ptit_bleu at yahoo.fr  Mon Dec 17 15:34:50 2007
From: ptit_bleu at yahoo.fr (Ptit_Bleu)
Date: Mon, 17 Dec 2007 06:34:50 -0800 (PST)
Subject: [R] Must be obvious but not to me : problem with regular expression
Message-ID: <14370723.post@talk.nabble.com>


Hi,

I have a vector called nfichiers of 138 names of file whose extension is .P0
or P1 ... to P8.
The script is not the same when the extension is P0 or P(1 to 8).

Examples of file names :
[128] "Output0.P0"       
[129] "Output0.P1"       
[130] "Output0.P2"       
[131] "Output01102007.P0"
[132] "Output01102007.P1"
[133] "Output01102007.P2"
[134] "Output01102007.P3"
[135] "Output01102007.P4"


To extract the names of file with .P0 extension I wrote :
nfichiers[grep(".P0", nfichiers)]
For the other extensions :
nfichiers[grep(".P[^0]", nfichiers)]

But for the last, I get a length of 138 that is the length of the initial
vector although I have 130 files with .P0 extension.

So I tried "manually" with a small vector :
> s
[1] "aa.P0" "bb.P0" "cc.P1" "dd.P2"
> s[grep(".P[^0]", s)]
[1] "cc.P1" "dd.P2"

It works !!!

Has someone an idea to solve this small problem ?
Thanks in advance,
Ptit Bleu.


-- 
View this message in context: http://www.nabble.com/Must-be-obvious-but-not-to-me-%3A-problem-with-regular-expression-tp14370723p14370723.html
Sent from the R help mailing list archive at Nabble.com.


From stenka1 at go.com  Mon Dec 17 15:35:50 2007
From: stenka1 at go.com (stephen bond)
Date: Mon, 17 Dec 2007 14:35:50 +0000 (UTC)
Subject: [R] rcom close Excel problem
Message-ID: <7998345.1197902150135.JavaMail.?@fh127.dia.he.tucows.com>

Thank you, getobject is a roundabout, but solves the issue. BTW I 
discovered that

 system("taskkill /f /im Excel.exe") # kills the process just fine.

The second problem is not due to the backslash vs slash, you can try 
and see that using the forward slash works fine from ESS. The q. is how 
to submit a second argument to "Close"?
Submitting a second argument to Open works fine as shown by the "0" 
below, but neither "1" nor "-1" worked for Close. Very strange.


----Original Message----
From: cskiadas at gmail.com
Date: 12/14/2007 18:57 
To: "stephen bond"<stenka1 at go.com>
Cc: <r-help at r-project.org>
Subj: Re: [R] rcom close Excel problem

I know it won't answer your question exactly, but using comGetObject  
instead of comCreateObject won't create new Excel instances, so at  
least you won't have more than one processes running, so this might  
solve some of your problems.

As for your second problem, I would venture to guess you need your  
paths with double backslashes instead of slashes. The following just  
worked over here:

  wb<-comInvoke(comGetProperty(obj,"Workbooks"),"Open", "C:\ 
\Documents and Settings\\Haris\\Desktop\\test1.xlsx")

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


On Dec 14, 2007, at 2:58 PM, stephen bond wrote:

> Hello,
>
> I just discovered that I cannot close the Excel application and task
> manager shows numerous copies of Excel.exe
>
> I tried both
>
> x$Quit() # shown in the rcom archive
>
> and
>
> x$Exit()
>
> and Excel refuses to die.
> Thank you very much.
> S.
>
> "You can't kill me, I will not die" Mojo Nixon
>
>
> I also have a problem with saving. It produces a pop-up dialog and
> does
> not take my second parameter:
>
> x<-comCreateObject("Excel.Application")
> wb<-comInvoke(comGetProperty(x,"Workbooks"),"Open","G:
> /MR/Stephen/repo.
> xls", "0")
> sh<-comGetProperty(wb,"Worksheets","Market Data")
> range1 <- comGetProperty(sh,"Range","C10","I11")
> vals <- comGetProperty(range1,"Value")
> comInvoke(wb,"Close","G:/MR/Stephen/repo.xls","True") # True is
> ignored
>
> Thank you All.
> Stephen


From bradtimm at gmail.com  Mon Dec 17 15:41:49 2007
From: bradtimm at gmail.com (Brad Timm)
Date: Mon, 17 Dec 2007 09:41:49 -0500
Subject: [R] Memory problem using predict function
Message-ID: <4eef39b90712170641s77cbb929y8503702f2bd300dc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/22762203/attachment.pl 

From murdoch at stats.uwo.ca  Mon Dec 17 15:46:17 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 17 Dec 2007 09:46:17 -0500
Subject: [R] Must be obvious but not to me : problem with regular
	expression
In-Reply-To: <14370723.post@talk.nabble.com>
References: <14370723.post@talk.nabble.com>
Message-ID: <47668BB9.3090709@stats.uwo.ca>

On 12/17/2007 9:34 AM, Ptit_Bleu wrote:
> Hi,
> 
> I have a vector called nfichiers of 138 names of file whose extension is .P0
> or P1 ... to P8.
> The script is not the same when the extension is P0 or P(1 to 8).
> 
> Examples of file names :
> [128] "Output0.P0"       
> [129] "Output0.P1"       
> [130] "Output0.P2"       
> [131] "Output01102007.P0"
> [132] "Output01102007.P1"
> [133] "Output01102007.P2"
> [134] "Output01102007.P3"
> [135] "Output01102007.P4"
> 
> 
> To extract the names of file with .P0 extension I wrote :
> nfichiers[grep(".P0", nfichiers)]
> For the other extensions :
> nfichiers[grep(".P[^0]", nfichiers)]
> 
> But for the last, I get a length of 138 that is the length of the initial
> vector although I have 130 files with .P0 extension.

One problem above is that "." is special in regular expressions.  I'd 
also suggest adding $ at the end, to force the match to the end of the 
string.  That is, code as

grep("\\.P0$", nfichiers)

and

grep("\\.P[^0]$", nfichiers)

I don't know what false matches you were seeing, but this should 
eliminate some.

Duncan Murdoch

> 
> So I tried "manually" with a small vector :
>> s
> [1] "aa.P0" "bb.P0" "cc.P1" "dd.P2"
>> s[grep(".P[^0]", s)]
> [1] "cc.P1" "dd.P2"
> 
> It works !!!
> 
> Has someone an idea to solve this small problem ?
> Thanks in advance,
> Ptit Bleu.
> 
>


From pisicandru at hotmail.com  Mon Dec 17 15:51:35 2007
From: pisicandru at hotmail.com (Monica Pisica)
Date: Mon, 17 Dec 2007 14:51:35 +0000
Subject: [R] cor.test formula
Message-ID: <BAY104-W13332EF7443472301AD1EEC3620@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/d7274994/attachment.pl 

From ligges at statistik.uni-dortmund.de  Mon Dec 17 16:01:10 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 17 Dec 2007 16:01:10 +0100
Subject: [R] Must be obvious but not to me : problem with regular
	expression
In-Reply-To: <14370723.post@talk.nabble.com>
References: <14370723.post@talk.nabble.com>
Message-ID: <47668F36.9090407@statistik.uni-dortmund.de>



Ptit_Bleu wrote:
> Hi,
> 
> I have a vector called nfichiers of 138 names of file whose extension is .P0
> or P1 ... to P8.
> The script is not the same when the extension is P0 or P(1 to 8).
> 
> Examples of file names :
> [128] "Output0.P0"       
> [129] "Output0.P1"       
> [130] "Output0.P2"       
> [131] "Output01102007.P0"
> [132] "Output01102007.P1"
> [133] "Output01102007.P2"
> [134] "Output01102007.P3"
> [135] "Output01102007.P4"
> 
> 
> To extract the names of file with .P0 extension I wrote :
> nfichiers[grep(".P0", nfichiers)]
> For the other extensions :
> nfichiers[grep(".P[^0]", nfichiers)]
> 
> But for the last, I get a length of 138 that is the length of the initial
> vector although I have 130 files with .P0 extension.
> 
> So I tried "manually" with a small vector :
>> s
> [1] "aa.P0" "bb.P0" "cc.P1" "dd.P2"
>> s[grep(".P[^0]", s)]
> [1] "cc.P1" "dd.P2"


I guess you want
     grep("\\.P0$", nfichiers)
Otherwise you get "XP0X" as a positive as well.

And for the others:
   grep("\\.P[^0]$", nfichiers)
with ".P[^0]", you'd get "XPXX" as positive, for example...
because you are looking for something that contains a P that is preceded 
by any character and followed by some non-zero character.

Uwe Ligges


> It works !!!
> 
> Has someone an idea to solve this small problem ?
> Thanks in advance,
> Ptit Bleu.
> 
>


From cganduri at gmail.com  Mon Dec 17 16:01:15 2007
From: cganduri at gmail.com (chandrasekhar ganduri)
Date: Mon, 17 Dec 2007 10:01:15 -0500
Subject: [R] Adding data labels to Lattice plots
Message-ID: <1f3dc4a0712170701h2f8b1172q56c91d59231cc493@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/8fcb0c7c/attachment.pl 

From x1lin at ucsd.edu  Mon Dec 17 16:16:53 2007
From: x1lin at ucsd.edu (xinyi lin)
Date: Mon, 17 Dec 2007 23:16:53 +0800
Subject: [R] Capture warning messages from coxph()
Message-ID: <b7e16700712170716x2c7a0580yfca8df05245efcd7@mail.gmail.com>

Hi,

I want to fit multiple cox models using the coxph() function. To do
this, I use a for-loop and save the relevant results in a separate
matrix. In the example below, only two models are fitted (my actual
matrix has many more columns), one gives a warning message, while the
other does not. Right now, I see all the warning message(s) after the
for-loop is completed but have no idea which model gave the warning
message. Is there a way in which the warning message can be captured
and saved (i.e. as a binary variable, having value 1 if there was a
warning message and 0 otherwise)? I can't possibly fit the models one
by one (and see if they give a warning message) as I have many of them
to fit.


> library(survival)
Loading required package: splines
> time= c(4,3,1,1,2,2,3,3,2)
> status=c(1,0,0,0,1,1,1,1,1)
> TIME=Surv(time,status)
> x= cbind(c(0,2,1,1,0,0,0,2,0),c(0,2,1,1,0,0,0,0,0))
>
> results=matrix(NA,ncol=3,nrow=ncol(x))
> colnames(results)=c("coef","se","p")
>
> for(i in 1:ncol(x)){
+ fit=summary(coxph(TIME~x[,i]))
+ results[i,1]=fit$coef[1]
+ results[i,2]=fit$coef[3]
+ results[i,3]=fit$coef[5]
+ rm(fit)
+ }
Warning message:
Loglik converged before variable  1 ; beta may be infinite.  in:
fitter(X, Y, strats, offset, init, control, weights = weights,
>
> results
            coef           se    p
[1,]  -0.5117033 5.647385e-01 0.36
[2,] -10.2256937 1.146168e+04 1.00
>
> #To see which model gave the warning message
> coxph(TIME~x[,1])
Call:
coxph(formula = TIME ~ x[, 1])


         coef exp(coef) se(coef)      z    p
x[, 1] -0.512       0.6    0.565 -0.906 0.36

Likelihood ratio test=0.97  on 1 df, p=0.324  n= 9
> coxph(TIME~x[,2])
Call:
coxph(formula = TIME ~ x[, 2])


        coef exp(coef) se(coef)         z        p
x[, 2] -10.2  3.62e-05    11462 -0.000892 1

Likelihood ratio test=2.51  on 1 df, p=0.113  n= 9
Warning message:
Loglik converged before variable  1 ; beta may be infinite.  in:
fitter(X, Y, strats, offset, init, control, weights = weights,


Thank you,
Cindy Lin


From darteta001 at ikasle.ehu.es  Mon Dec 17 16:27:52 2007
From: darteta001 at ikasle.ehu.es (darteta001 at ikasle.ehu.es)
Date: Mon, 17 Dec 2007 16:27:52 +0100 (CET)
Subject: [R] Reproducibility of experiment
Message-ID: <9564983215darteta001@ikasle.ehu.es>

Dear Marc and R-list,

thanks for your help. I 
have checked Bland-Altman help 
page about repeatability, and I learnt that instead of 
reproducibility, 
I was talking about repeatability. Although I am not sure whether they 
only 
focuse on agreement of two different measurement methods, and not 
on 
repeatability of one single method. 

To explain further on my topic, I have repeated ten times an 
experiment involving protein quantification(i.e. how much protein I 
have), 
giving me ten continuous values. All experimental settings are similar 
so there should be no variability due to day of experiment, operator
or any batch effect. My aim is to know whether these ten observations 
are good enough so that I can conclude that the repeatability of my 
detection technique is good. But as I have learnt from Altman?s page, 
it is not possible to set a threshold to the repeatability score to 
say my experiment is "repeatable".  
I guess I can obtain a 95% confidence interval for the protein 
quantification values, 
but I am not sure this will show how well my experiment performs. 
Putting it differently, 
something I would like to know is whether I can estimate
beforehand how many times I need to run an experiment in order to be 
confident that it is "repeatable".


Thanks for your comments

David

> 
> On Wed, 2007-12-12 at 17:23 +0100, darteta001 at ikasle.ehu.es wrote:
> > Dear list,
> > 
> > I have an experiment that I have run 10 times in order to find out 
its 
> > reproducibility. I wonder if there is any function that I can use 
for 
> > obtaining a significance value of reproducibility or agreement of 
> > measurements. I thought of coefficient of variation but, as far as 
I 
> > know, I would have to set a threshold for saying the experiment is 
not 
> > reproducible. Any pointers to something more "objective" would be 
very 
> > helpful.
> > 
> > Thanks
> > 
> > David
> 
> I suspect that you are going to have to be more specific regarding 
the
> subject matter and the experimental design so that those with the
> requisite expertise could comment.
> 
> If this is looking at a continuous measure (ie. instrumentation
> measurements), you could look at Bland-Altman methods. More 
information
> here:
> 
>   http://www-users.york.ac.uk/~mb55/meas/meas.htm
> 
> Otherwise, given that Google returns almost a million hits with the
> phrase "reproducibility of experiment"...
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> 


From rmh at temple.edu  Mon Dec 17 16:50:52 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 17 Dec 2007 10:50:52 -0500 (EST)
Subject: [R] fortune warning
Message-ID: <20071217105052.CXS23311@po-d.temple.edu>

fortune("help")  ## or any quoted string

gives a warning
Warning message:
In grep(which, fort, useBytes = TRUE) :
  argument 'useBytes = TRUE' will be ignored

in version.string R version 2.6.1 (2007-11-26)


From ccleland at optonline.net  Mon Dec 17 16:50:30 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 17 Dec 2007 10:50:30 -0500
Subject: [R] cor.test formula
In-Reply-To: <BAY104-W13332EF7443472301AD1EEC3620@phx.gbl>
References: <BAY104-W13332EF7443472301AD1EEC3620@phx.gbl>
Message-ID: <47669AC6.5040100@optonline.net>

Monica Pisica wrote:
> Hi everybody,
>  
> I am interested in seeing how the p value is calculated for a t test for a correlation coefficient. I know that cor.test delivers the correlation coefficient and the t-test, p-value and the 95 confidence interval. I am interested  in how the p-value is calculated.
>  
> Usually if i type the name of the function i get explicitly the coding of that function, but if i type
>  
>> cor.testfunction (x, ...) UseMethod("cor.test")<environment: namespace:stats>
>  
> So .... How can i get the coding to find out how the p-value is calculated for this function?

  The following Google search finds coding for cor.test:

cor.test site:https://svn.r-project.org/R/trunk/src/

  From those 6 hits, it is a short trip to the following link:

https://svn.r-project.org/R/trunk/src/library/stats/R/cor.test.R

> Thanks,
>  
> Monica
> _________________________________________________________________
> [[replacing trailing spam]]
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From pisicandru at hotmail.com  Mon Dec 17 16:54:58 2007
From: pisicandru at hotmail.com (Monica Pisica)
Date: Mon, 17 Dec 2007 15:54:58 +0000
Subject: [R] cor.test formula
In-Reply-To: <47669AC6.5040100@optonline.net>
References: <BAY104-W13332EF7443472301AD1EEC3620@phx.gbl>
	<47669AC6.5040100@optonline.net>
Message-ID: <BAY104-W38D97E39103579E4737583C3620@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/41750360/attachment.pl 

From jholtman at gmail.com  Mon Dec 17 16:55:08 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 17 Dec 2007 10:55:08 -0500
Subject: [R] Capture warning messages from coxph()
In-Reply-To: <b7e16700712170716x2c7a0580yfca8df05245efcd7@mail.gmail.com>
References: <b7e16700712170716x2c7a0580yfca8df05245efcd7@mail.gmail.com>
Message-ID: <644e1f320712170755j2f87038fi1caa335372d2d590@mail.gmail.com>

One way is to turn the 'warnings' into 'errors' and then trap the error:

> library(survival)
>
> time= c(4,3,1,1,2,2,3,3,2)
> status=c(1,0,0,0,1,1,1,1,1)
> TIME=Surv(time,status)
> x= cbind(c(0,2,1,1,0,0,0,2,0),c(0,2,1,1,0,0,0,0,0))
>
> results=matrix(NA,ncol=3,nrow=ncol(x))
> colnames(results)=c("coef","se","p")
>
> old.warn <- options(warn=2)
> for(i in 1:ncol(x)){
+
+     aa <- try(fit <- summary(coxph(TIME~x[,i])))
+     if (class(aa) == "try-error"){
+         print(paste("i =", i, "had error"))
+         next   # skip iteration
+     }
+
+     results[i,1]=fit$coef[1]
+     results[i,2]=fit$coef[3]
+     results[i,3]=fit$coef[5]
+     rm(fit)
+ }
Error in fitter(X, Y, strats, offset, init, control, weights = weights,  :
  (converted from warning) Loglik converged before variable  1 ; beta
may be infinite.
[1] "i = 2 had error"
> options(old.warn)
>
>


On Dec 17, 2007 10:16 AM, xinyi lin <x1lin at ucsd.edu> wrote:
> Hi,
>
> I want to fit multiple cox models using the coxph() function. To do
> this, I use a for-loop and save the relevant results in a separate
> matrix. In the example below, only two models are fitted (my actual
> matrix has many more columns), one gives a warning message, while the
> other does not. Right now, I see all the warning message(s) after the
> for-loop is completed but have no idea which model gave the warning
> message. Is there a way in which the warning message can be captured
> and saved (i.e. as a binary variable, having value 1 if there was a
> warning message and 0 otherwise)? I can't possibly fit the models one
> by one (and see if they give a warning message) as I have many of them
> to fit.
>
>
> > library(survival)
> Loading required package: splines
> > time= c(4,3,1,1,2,2,3,3,2)
> > status=c(1,0,0,0,1,1,1,1,1)
> > TIME=Surv(time,status)
> > x= cbind(c(0,2,1,1,0,0,0,2,0),c(0,2,1,1,0,0,0,0,0))
> >
> > results=matrix(NA,ncol=3,nrow=ncol(x))
> > colnames(results)=c("coef","se","p")
> >
> > for(i in 1:ncol(x)){
> + fit=summary(coxph(TIME~x[,i]))
> + results[i,1]=fit$coef[1]
> + results[i,2]=fit$coef[3]
> + results[i,3]=fit$coef[5]
> + rm(fit)
> + }
> Warning message:
> Loglik converged before variable  1 ; beta may be infinite.  in:
> fitter(X, Y, strats, offset, init, control, weights = weights,
> >
> > results
>            coef           se    p
> [1,]  -0.5117033 5.647385e-01 0.36
> [2,] -10.2256937 1.146168e+04 1.00
> >
> > #To see which model gave the warning message
> > coxph(TIME~x[,1])
> Call:
> coxph(formula = TIME ~ x[, 1])
>
>
>         coef exp(coef) se(coef)      z    p
> x[, 1] -0.512       0.6    0.565 -0.906 0.36
>
> Likelihood ratio test=0.97  on 1 df, p=0.324  n= 9
> > coxph(TIME~x[,2])
> Call:
> coxph(formula = TIME ~ x[, 2])
>
>
>        coef exp(coef) se(coef)         z        p
> x[, 2] -10.2  3.62e-05    11462 -0.000892 1
>
> Likelihood ratio test=2.51  on 1 df, p=0.113  n= 9
> Warning message:
> Loglik converged before variable  1 ; beta may be infinite.  in:
> fitter(X, Y, strats, offset, init, control, weights = weights,
>
>
> Thank you,
> Cindy Lin
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ken.williams at thomson.com  Mon Dec 17 16:56:40 2007
From: ken.williams at thomson.com (Ken Williams)
Date: Mon, 17 Dec 2007 09:56:40 -0600
Subject: [R] Extending data.frame
Message-ID: <C38BF858.15DCC%ken.williams@thomson.com>

Hi,

I've got a long-running project whose data fits nicely into data.frame
objects in R.  As I accumulate more and more functions, I decided to switch
to an OO approach so I can organize things better.

Looking around at the various approaches to OO R, I came across R.oo, which
seems nice.

Where I'm currently stuck is getting my objects to be mutable.  For example,
in the following toy code, the addStuff() method has no effect:

> library(R.oo)
R.oo v1.3.0 (2006-08-29) successfully loaded. See ?R.oo for help.
> setConstructorS3("Foo", function(...) {
+   frame <- data.frame(foo=4, bar=3:5)
+   extend(frame, "Foo")
+ })
> setMethodS3("addStuff", "Foo", function(this, ...) { this$field <- 5 })
> f <- Foo(); f
  foo bar
1   4   3
2   4   4
3   4   5
> addStuff(f); f
  foo bar
1   4   3
2   4   4
3   4   5


Can anyone offer any advice?  I'm open to using a different OO system if
that's deemed advisable, I'm not very familiar with any of them.

Note that in my real (non-toy) application, I'll need arbitrary methods to
be able to read & write data to the object, so simple getField() and
setField() accessors won't be sufficient.

Thanks.


-- 
Ken Williams
Research Scientist
The Thomson Corporation
Eagan, MN


From murdoch at stats.uwo.ca  Mon Dec 17 16:57:12 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 17 Dec 2007 10:57:12 -0500
Subject: [R] fortune warning
In-Reply-To: <20071217105052.CXS23311@po-d.temple.edu>
References: <20071217105052.CXS23311@po-d.temple.edu>
Message-ID: <47669C58.5030009@stats.uwo.ca>

On 12/17/2007 10:50 AM, Richard M. Heiberger wrote:
> fortune("help")  ## or any quoted string
> 
> gives a warning
> Warning message:
> In grep(which, fort, useBytes = TRUE) :
>   argument 'useBytes = TRUE' will be ignored
> 
> in version.string R version 2.6.1 (2007-11-26)

This is a problem in a contributed package; I've cc'd the maintainer 
(Achim).

Duncan Murdoch


From Achim.Zeileis at R-project.org  Mon Dec 17 17:30:44 2007
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Mon, 17 Dec 2007 17:30:44 +0100 (CET)
Subject: [R] fortune warning
In-Reply-To: <47669C58.5030009@stats.uwo.ca>
Message-ID: <Pine.LNX.4.44.0712171729290.24326-100000@disco.wu-wien.ac.at>

On Mon, 17 Dec 2007, Duncan Murdoch wrote:

> On 12/17/2007 10:50 AM, Richard M. Heiberger wrote:
> > fortune("help")  ## or any quoted string
> >
> > gives a warning
> > Warning message:
> > In grep(which, fort, useBytes = TRUE) :
> >   argument 'useBytes = TRUE' will be ignored
> >
> > in version.string R version 2.6.1 (2007-11-26)
>
> This is a problem in a contributed package; I've cc'd the maintainer
> (Achim).

Thanks for the pointer (which was already reported privately previously),
but I didn't get round to fix it, yet. I'll try to fix it asap.
Z

> Duncan Murdoch
>
>
>


From gomes at eva.mpg.de  Mon Dec 17 17:28:51 2007
From: gomes at eva.mpg.de (Cristina Gomes)
Date: Mon, 17 Dec 2007 17:28:51 +0100
Subject: [R] Identity link in tweedie
Message-ID: <4766A3C3.5070900@eva.mpg.de>

Hi there,
I'm using the tweedie distribution package and I cant figure out how to 
run a model using an identity link. I know I can use a log link by 
having link.power=0 and I think identity would be link.power=1, but I'm 
not sure. Furthermore when I try running it with link.power=1 it 
requires starting values which I cant manage to give appropriately so 
I'm not sure if its actually an identity link its using. I can't find 
this info on the help page and I was wondering if somebody could give me 
a hand.
Thanks a lot.
Cheers,
Cristina.


From bacaro at unisi.it  Mon Dec 17 18:03:34 2007
From: bacaro at unisi.it (giovanni bacaro)
Date: Mon, 17 Dec 2007 18:03:34 +0100
Subject: [R] convert table
Message-ID: <000001c840ce$c2971120$554a10ac@fisso>

Dear R user, a very simple question:
I have a table like this:

coor	v1	v2	v3
x1	12	33	123	
x2	1	123
x3	12	
x4	33	1

and I'd like to tranform this matrix in presence/absence data.frame

coor	1	12	33	123
x1	0	1	1	1	
x2	1	0	0	1
x3	0	1	0	0	
x4	1	0	1	0

Could you suggest me a direct way to do this?
Thank you
Giovanni


From marc_schwartz at comcast.net  Mon Dec 17 18:02:40 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 17 Dec 2007 11:02:40 -0600
Subject: [R] Reproducibility of experiment
In-Reply-To: <9564983215darteta001@ikasle.ehu.es>
References: <9564983215darteta001@ikasle.ehu.es>
Message-ID: <1197910960.3128.21.camel@Bellerophon.localdomain>


On Mon, 2007-12-17 at 16:27 +0100, darteta001 at ikasle.ehu.es wrote:
> Dear Marc and R-list,
> 
> thanks for your help. I 
> have checked Bland-Altman help 
> page about repeatability, and I learnt that instead of 
> reproducibility, 
> I was talking about repeatability. Although I am not sure whether they 
> only 
> focuse on agreement of two different measurement methods, and not 
> on 
> repeatability of one single method. 
> 
> To explain further on my topic, I have repeated ten times an 
> experiment involving protein quantification(i.e. how much protein I 
> have), 
> giving me ten continuous values. All experimental settings are similar 
> so there should be no variability due to day of experiment, operator
> or any batch effect. My aim is to know whether these ten observations 
> are good enough so that I can conclude that the repeatability of my 
> detection technique is good. But as I have learnt from Altman?s page, 
> it is not possible to set a threshold to the repeatability score to 
> say my experiment is "repeatable".  
> I guess I can obtain a 95% confidence interval for the protein 
> quantification values, 
> but I am not sure this will show how well my experiment performs. 
> Putting it differently, 
> something I would like to know is whether I can estimate
> beforehand how many times I need to run an experiment in order to be 
> confident that it is "repeatable".
> 
> 
> Thanks for your comments
> 
> David

<snip>

David,

There is information on Prof. Bland's pages pertaining to the questions
you ask. If you have not reviewed his FAQ, please do so as it covers
issues such as sample size calculations, etc.

If the 10 measures are all of the same quantity, then a simple one
sample t-test is all you need to determine whether or not the measured
values are significantly different than a presumably known correct value
and to get confidence intervals for the mean measurement.

However, if all 10 values are of the same quantity, you will not answer
the questions as to whether or not any measurement error is
constant/linear over the range of possible values and whether that error
is within "acceptable limits". This is what the Bland-Altman methods
address.

My recommendation would be to solicit local expertise in the design of
such studies, as in reality, all of this should have been specified a
priori.

In addition, both Profs. Bland and Altman participate in the MedStats
group and that would be a better forum for your queries. More
information here:

 http://groups.google.com/group/MedStats

HTH,

Marc Schwartz


From Richard.Cotton at hsl.gov.uk  Mon Dec 17 18:13:13 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Mon, 17 Dec 2007 17:13:13 +0000
Subject: [R] convert table
In-Reply-To: <000001c840ce$c2971120$554a10ac@hsl.gov.uk>
Message-ID: <OFA64168E3.F38DB752-ON802573B4.005E69A4-802573B4.005E93AF@hsl.gov.uk>

> I have a table like this:
> 
> coor   v1   v2   v3
> x1   12   33   123 
> x2   1   123
> x3   12 
> x4   33   1
> 
> and I'd like to tranform this matrix in presence/absence data.frame
> 
> coor   1   12   33   123
> x1   0   1   1   1 
> x2   1   0   0   1
> x3   0   1   0   0 
> x4   1   0   1   0

#This uses the reshape package
df = data.frame(coor = paste("x", 1:4, sep=""), v1=c(12,1,12,33), 
v2=c(33,123,NA,1), v3=c(1,NA,NA,NA))
mdf = melt(df)
with(mdf, table(coor, value))

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From P.Dalgaard at biostat.ku.dk  Mon Dec 17 18:30:01 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 17 Dec 2007 18:30:01 +0100
Subject: [R] convert table
In-Reply-To: <OFA64168E3.F38DB752-ON802573B4.005E69A4-802573B4.005E93AF@hsl.gov.uk>
References: <OFA64168E3.F38DB752-ON802573B4.005E69A4-802573B4.005E93AF@hsl.gov.uk>
Message-ID: <4766B219.7080206@biostat.ku.dk>

Richard.Cotton at hsl.gov.uk wrote:
>> I have a table like this:
>>
>> coor   v1   v2   v3
>> x1   12   33   123 
>> x2   1   123
>> x3   12 
>> x4   33   1
>>
>> and I'd like to tranform this matrix in presence/absence data.frame
>>
>> coor   1   12   33   123
>> x1   0   1   1   1 
>> x2   1   0   0   1
>> x3   0   1   0   0 
>> x4   1   0   1   0
>>     
>
> #This uses the reshape package
> df = data.frame(coor = paste("x", 1:4, sep=""), v1=c(12,1,12,33), 
> v2=c(33,123,NA,1), v3=c(1,NA,NA,NA))
> mdf = melt(df)
> with(mdf, table(coor, value))
>   
Plain reshape() too:

> df = data.frame(coor = paste("x", 1:4, sep=""), v1=c(12,1,12,33),
+ v2=c(33,123,NA,1), v3=c(123,NA,NA,NA))
> mdf <- reshape(df, direction="long", varying=c("v1","v2","v3"), sep="")
> with(mdf, table(coor, v))
    v
coor 1 12 33 123
  x1 0  1  1   1
  x2 1  0  0   1
  x3 0  1  0   0
  x4 1  0  1   0

(actually, varying=-1 also works.)


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From wojciech at gmail.com  Mon Dec 17 18:34:32 2007
From: wojciech at gmail.com (Wojciech Gryc)
Date: Mon, 17 Dec 2007 12:34:32 -0500
Subject: [R] read.table() and precision?
Message-ID: <6d32d7830712170934m4f9aa742je29da95b7eaab869@mail.gmail.com>

Hi,

I'm currently working with data that has values as large as 99,000,000
but is accurate to 6 decimal places. Unfortunately, when I load the
data using read.table(), it rounds everything to the nearest integer.
Is there any way for me to preserve the information or work with
arbitrarily large floating point numbers?

Thank you,
Wojciech

-- 

Five Minutes to Midnight:
Youth on human rights and current affairs
http://www.fiveminutestomidnight.org/


From mtmorgan at fhcrc.org  Mon Dec 17 19:03:09 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Mon, 17 Dec 2007 10:03:09 -0800
Subject: [R] Analyzing Publications from Pubmed via XML
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<a695fbee0712170618t74888388sc303646454ee65db@mail.gmail.com>
Message-ID: <6ph1w9lmc6a.fsf@gopher4.fhcrc.org>

Hi Armin -- 

See the help page for esearch

http://www.ncbi.nlm.nih.gov/entrez/query/static/esearch_help.html

especially the 'retmax' key.

A couple of other thoughts on this thread...

1) using the full path, e.g.,

ids <- xpathApply(doc, "/eSearchResult/IdList/Id", xmlValue)

is likely to lead to less grief in the long run, as you'll only select
elements of the node you're interested in, rather than any element,
anywhere in the document, labeled 'Id'

2) From a different post in the thread, things like

On Dec 16, 2007 2:53 PM, David Winsemius <dwinsemius at comcast.net> wrote:
[snip]
> get.info<- function(doc){
>          df<-cbind(
>  	Abstract = unlist(xpathApply(doc, "//AbstractText", xmlValue)),
>  	Journal =  unlist(xpathApply(doc, "//Title", xmlValue)),
>  	Pmid =  unlist(xpathApply(doc, "//PMID", xmlValue))
>                    )
>    return(df)
>    } 

will lead to more trouble, because they assume that AbstractText, etc
occur exactly once in each record. It would seem better to extract the
relevant node, and query that, probably defining appropriate
defaults. I started with

xpath_or_na <- function(doc, q) {
    res <- xpathApply(doc, q, xmlValue)
    if (length(res)==1) res[[1]]
    else NA_character_
}

citn <- function(citation){
 	Abstract <- xpath_or_na(citation,
                           "/MedlineCitation/Article/Abstract/AbstractText")
 	Journal <- xpath_or_na(citation,
                          "/MedlineCitation/Article/Journal/Title")
 	Pmid <- xpath_or_na(citation,
                       "/MedlineCitation/PMID")
    c(Abstract=Abstract, Journal=Journal, Pmid=Pmid)
}

medline_q <- "/PubmedArticleSet/PubmedArticle/MedlineCitation"
res <- xpathApply(doc, medline_q, citn)

One would still have to coerce res into a data.frame. Also worth
thinking about each of the lines in citn -- e.g., clearly only applies
to Journals.  Eventually one wants to consult the DTD (basically, the
contract spelling out the content) of the document, confirm that the
xpath queries will perform correctly, and verify that the document
actually conforms to its DTD.

Following my own advice, I quickly found that doing things 'more
right' becomes quite complicated, and suddenly became satisfied with
the information I can get out of the 'annotate' package.

Martin

"Armin Goralczyk" <agoralczyk at gmail.com> writes:

> On Dec 15, 2007 6:31 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> After quite a bit of hacking (in the sense of ineffective chopping with
>> a dull ax), I finally came up with:
>>
>> pm.srch<- function (){
>>   srch.stem<-"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term="
>>   query<-readLines(con=file.choose())
>>   query<-gsub("\\\"","",x=query)
>>   doc<-xmlTreeParse(paste(srch.stem,query,sep=""),isURL = TRUE,
>>                      useInternalNodes = TRUE)
>>   return(sapply(c("//Id"), xpathApply, doc = doc, fun = xmlValue) )
>>      }
>>
>> pm.srch()  #choosing the search-file
>>       //Id
>>  [1,] "18046565"
>>  [2,] "17978930"
>>  [3,] "17975511"
>>  [4,] "17935912"
>>  [5,] "17851940"
>>  [6,] "17765779"
>>  [7,] "17688640"
>>  [8,] "17638782"
>>  [9,] "17627059"
>> [10,] "17599582"
>> [11,] "17589729"
>> [12,] "17585283"
>> [13,] "17568846"
>> [14,] "17560665"
>> [15,] "17547971"
>> [16,] "17428551"
>> [17,] "17419899"
>> [18,] "17419519"
>> [19,] "17385606"
>> [20,] "17366752"
>
> I tried the example above, but only the first 20 PMIDs will be
> returned. How can I circumvent this (I guesss its a restraint from
> pubmed)?
> -- 
> Armin Goralczyk, M.D.
> --
> Universit?tsmedizin G?ttingen
> Abteilung Allgemein- und Viszeralchirurgie
> Rudolf-Koch-Str. 40
> 39099 G?ttingen
> --
> Dept. of General Surgery
> University of G?ttingen
> G?ttingen, Germany
> --
> http://www.chirurgie-goettingen.de
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Computational Biology / Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M2 B169
Phone: (206) 667-2793


From P.Dalgaard at biostat.ku.dk  Mon Dec 17 19:09:57 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 17 Dec 2007 19:09:57 +0100
Subject: [R] read.table() and precision?
In-Reply-To: <6d32d7830712170934m4f9aa742je29da95b7eaab869@mail.gmail.com>
References: <6d32d7830712170934m4f9aa742je29da95b7eaab869@mail.gmail.com>
Message-ID: <4766BB75.6090801@biostat.ku.dk>

Wojciech Gryc wrote:
> Hi,
>
> I'm currently working with data that has values as large as 99,000,000
> but is accurate to 6 decimal places. Unfortunately, when I load the
> data using read.table(), it rounds everything to the nearest integer.
> Is there any way for me to preserve the information or work with
> arbitrarily large floating point numbers?
>
> Thank you,
> Wojciech
>
>   
Are you sure?

To my knowledge, read.table doesn't round anything, except when running
out of bits to store the values in, and 13 decimal places should fit in
ordinary double precision variables.

Printing the result is another matter. Try playing with the
print(mydata, digits=15) and the like.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From rh at family-krueger.com  Mon Dec 17 19:10:06 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Mon, 17 Dec 2007 19:10:06 +0100
Subject: [R] read.table() and precision?
In-Reply-To: <6d32d7830712170934m4f9aa742je29da95b7eaab869@mail.gmail.com>
References: <6d32d7830712170934m4f9aa742je29da95b7eaab869@mail.gmail.com>
Message-ID: <4766BB7E.9020002@family-krueger.com>

Did you set the  the character used in the file for decimal points?

dec = "." or  dec = ","


Knut


From tring at gvdnet.dk  Mon Dec 17 19:21:39 2007
From: tring at gvdnet.dk (Troels Ring)
Date: Mon, 17 Dec 2007 19:21:39 +0100
Subject: [R] regression towards the mean, AS paper November 2007
Message-ID: <4766BE33.1030806@gvdnet.dk>

Dear friends, regression towards the mean is interesting in medical 
circles, and a very recent paper (The American Statistician November 
2007;61:302-307 by Krause and Pinheiro) treats it at length. An initial 
example specifies (p 303):
"Consider the following example: we draw 100 samples from a bivariate 
Normal distribution with X0~N(0,1), X1~N(0,1) and cov(X0,X1)=0.7, We 
then calculate the p value for the null hypothesis that the means of X0 
and X1 are equal, using a paired Student's t test. The procedure is 
repeated 1000 times, producing 1000 simulated p values. Because X0 and 
X1 have identical marginal distributions, the simulated p values behave 
like independent Uniform(0,1) random variables." This I did not 
understand, and simulating like shown below produced far from uniform 
(0,1) p values - but I fail to see how it is wrong. I contacted the 
authors of the paper but they did not answer. So, please, doesn?t the 
code below specify a bivariate N(0,1) with covariance 0.7? I get p 
values = 1 all over - not interesting, but how wrong?
Best wishes
Troels

library(MASS)
Sigma <- matrix(c(1,0.7,0.7,1),2,2)
Sigma
res <- NULL
for (i in 1:1000){
ff <-(mvrnorm(n=100, rep(0, 2), Sigma, empirical = TRUE))
res[i] <- t.test(ff[,1],ff[,2],paired=TRUE)$p.value}

-- 

Troels Ring - -
Department of nephrology - - 
Aalborg Hospital 9100 Aalborg, Denmark - -
+45 99326629 - -
tring at gvdnet.dk


From tplate at acm.org  Mon Dec 17 19:27:37 2007
From: tplate at acm.org (Tony Plate)
Date: Mon, 17 Dec 2007 11:27:37 -0700
Subject: [R] Array dimnames
In-Reply-To: <ea7af0460712141031n5c5df115y55e4d116615e7474@mail.gmail.com>
References: <ea7af0460712141031n5c5df115y55e4d116615e7474@mail.gmail.com>
Message-ID: <4766BF99.3080601@acm.org>

I can't quite understand what you're having difficulty with (is it 
constructing the array, or coping with the different 'matrices' having 
different column names, or something else?)

However, your sample data looks like it has a mixture of factor (region) 
and numeric data (Qty), so you're probably storing it in a data frame. 
AFAIK, there is no 3d object in R that can store mixed-type data like a 
data frame can.  An array object in R has to have the same data type for 
every column etc.

-- Tony Plate

dave mitchell wrote:
> Dear all,
> Possibly a rudimentary question, however any help is greatly appreciated.  I
> am sorting a large matrix into an array of dim(p(i),q,3).  I put each entry
> into a corresponding matrix (1 of the 3) based on some criteria.  I figure
> this will assist me in condensing code as I can loop through the 3rd
> dimension of the array instead of generating 3 separate matrices and using
> the same block of code 3 times.  My question is how to get the colnames of
> the 3 nested matrices in the array to match the colnames of the data
> matrix.  In other words...
> 
> DATA:
>    Exp   region   Qty   Ct  ...q
> 1   S      CB     3.55  2.15  .
> 2   S      TG     4.16  2.18  .
> 3   C      OO     2.36  3.65  .
> 4   C   .           .         .
> .   .     .           .       .
> .   .       .           .     .
> .   .         .           .   .
> p   ...........................
> 
> 
> 
> ARRAY
> 1
>    [,1]   [,2]    [,3] [,4]...q
> 1   SOME DATA WILL FILL THIS   .
> 2   .  .              .        .
> 3   .   .              .       .
> 4   .    .              .      .
> .   .     .              .     .
> .   .      .              .    .
> .   .       .              .   .
> P(1) ...........................
> 
> 2
>    [,1]   [,2]    [,3] [,4]...q
> 1   SOME DATA WILL FILL THIS   .
> 2   .  .              .        .
> 3   .   .              .       .
> 4   .    .              .      .
> .   .     .              .     .
> .   .      .              .    .
> .   .       .              .   .
> P(2) ...........................
> 3
>    [,1]   [,2]    [,3] [,4]...q
> 1   SOME DATA WILL FILL THIS   .
> 2   .  .              .        .
> 3   .   .              .       .
> 4   .    .              .      .
> .   .     .              .     .
> .   .      .              .    .
> .   .       .              .   .
> P(3) ...........................
> 
> Again, how to get those [,1], [,2]... to read (and operate) in the same
> fashion as the column names in the data matrix?  Also, am I interpreting the
> dimensions of the array incorrectly?  Please feel free to post any helpful
> links on the subject, as I have found "dimnames" and "array" in the R-help
> documentation unhelpful.  Any help is greatly appreciated.
> 
> Dave Mitchell
> Undergraduate: Statistics and Mathematics,
> University of Illinois, Urbana-Champaign
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From P.Dalgaard at biostat.ku.dk  Mon Dec 17 19:31:06 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 17 Dec 2007 19:31:06 +0100
Subject: [R] regression towards the mean, AS paper November 2007
In-Reply-To: <4766BE33.1030806@gvdnet.dk>
References: <4766BE33.1030806@gvdnet.dk>
Message-ID: <4766C06A.7060805@biostat.ku.dk>

Troels Ring wrote:
> Dear friends, regression towards the mean is interesting in medical 
> circles, and a very recent paper (The American Statistician November 
> 2007;61:302-307 by Krause and Pinheiro) treats it at length. An initial 
> example specifies (p 303):
> "Consider the following example: we draw 100 samples from a bivariate 
> Normal distribution with X0~N(0,1), X1~N(0,1) and cov(X0,X1)=0.7, We 
> then calculate the p value for the null hypothesis that the means of X0 
> and X1 are equal, using a paired Student's t test. The procedure is 
> repeated 1000 times, producing 1000 simulated p values. Because X0 and 
> X1 have identical marginal distributions, the simulated p values behave 
> like independent Uniform(0,1) random variables." This I did not 
> understand, and simulating like shown below produced far from uniform 
> (0,1) p values - but I fail to see how it is wrong. I contacted the 
> authors of the paper but they did not answer. So, please, doesn?t the 
> code below specify a bivariate N(0,1) with covariance 0.7? I get p 
> values = 1 all over - not interesting, but how wrong?
> Best wishes
> Troels
>
> library(MASS)
> Sigma <- matrix(c(1,0.7,0.7,1),2,2)
> Sigma
> res <- NULL
> for (i in 1:1000){
> ff <-(mvrnorm(n=100, rep(0, 2), Sigma, empirical = TRUE))
> res[i] <- t.test(ff[,1],ff[,2],paired=TRUE)$p.value}
>
>   
You do not want empirical=TRUE in the mvrnorm call. This pegs the
empirical means to exactly (0,0) which are obviously never significantly
different.

BTW, there's a function called replicate()...

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From murdoch at stats.uwo.ca  Mon Dec 17 19:32:32 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 17 Dec 2007 13:32:32 -0500
Subject: [R] regression towards the mean, AS paper November 2007
In-Reply-To: <4766BE33.1030806@gvdnet.dk>
References: <4766BE33.1030806@gvdnet.dk>
Message-ID: <4766C0C0.2080509@stats.uwo.ca>

On 12/17/2007 1:21 PM, Troels Ring wrote:
> Dear friends, regression towards the mean is interesting in medical 
> circles, and a very recent paper (The American Statistician November 
> 2007;61:302-307 by Krause and Pinheiro) treats it at length. An initial 
> example specifies (p 303):
> "Consider the following example: we draw 100 samples from a bivariate 
> Normal distribution with X0~N(0,1), X1~N(0,1) and cov(X0,X1)=0.7, We 
> then calculate the p value for the null hypothesis that the means of X0 
> and X1 are equal, using a paired Student's t test. The procedure is 
> repeated 1000 times, producing 1000 simulated p values. Because X0 and 
> X1 have identical marginal distributions, the simulated p values behave 
> like independent Uniform(0,1) random variables." This I did not 
> understand, and simulating like shown below produced far from uniform 
> (0,1) p values - but I fail to see how it is wrong. I contacted the 
> authors of the paper but they did not answer. So, please, doesn?t the 
> code below specify a bivariate N(0,1) with covariance 0.7? I get p 
> values = 1 all over - not interesting, but how wrong?
> Best wishes
> Troels
> 
> library(MASS)
> Sigma <- matrix(c(1,0.7,0.7,1),2,2)
> Sigma
> res <- NULL
> for (i in 1:1000){
> ff <-(mvrnorm(n=100, rep(0, 2), Sigma, empirical = TRUE))
> res[i] <- t.test(ff[,1],ff[,2],paired=TRUE)$p.value}

Specifying empirical=TRUE means that your sampled values are not 
independent, the means are guaranteed to match exactly, and the mean 
difference is exactly zero.  Thus all of the t statistics are exactly 
zero, and the p-values are exactly 1.

Set empirical=FALSE (the default), and you'll see more reasonable results.

Duncan Murdoch


From ozric at web.de  Mon Dec 17 19:41:59 2007
From: ozric at web.de (Christian Schulz)
Date: Mon, 17 Dec 2007 19:41:59 +0100
Subject: [R] RMySQL installation problem - partially solved
In-Reply-To: <4766593B.2060907@family-krueger.com>
References: <BAY120-W88A17C859B8C463333777C7820@phx.gbl>	<Pine.LNX.4.64.0711152120050.7199@gannet.stats.ox.ac.uk>	<476384AE.5010500@family-krueger.com>	<4766365B.7010707@family-krueger.com>
	<4766505B.1060407@web.de> <4766593B.2060907@family-krueger.com>
Message-ID: <4766C2F7.3020302@web.de>

Knut Krueger wrote:
> Christian Schulz schrieb:
>   
>> I think you should use the newest DBI Version with 2.6.1.
>> regards, christian
>>     
> I run *actualice packages*,
>  this should update to the newest DBI, shouldn't it?
> Knut
>
>   
I'm not sure, perhaps you are on an old "branch"? Try  for 2.6.1:
http://umfragen.sowi.uni-mainz.de/CRAN/bin/windows/contrib/2.6/DBI_0.2-4.zip
http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.6/RMySQL_0.6-0.zip

good luck,
Christian


From jonas.malmros at gmail.com  Mon Dec 17 19:47:46 2007
From: jonas.malmros at gmail.com (Jonas Malmros)
Date: Mon, 17 Dec 2007 19:47:46 +0100
Subject: [R] Cannot grasp how to apply "by" here...
Message-ID: <fd3c7adf0712171047v128897fek622ae9bbd9b9ed1c@mail.gmail.com>

I have a data frame named "database" with panel data, a little piece
of which looks like this:

  Symbol               Name             Trial        Factor1  Factor2
   External
1 548140                 A                  1            -3.87
-0.32         0.01
2 547400                 B                  1            12.11
-0.68         0.40
3 547173                 C                  1             4.50
0.71        -1.36
4 546832                 D                  1             2.59
0.00         0.09
5 548140                 A                  2             2.41
0.50        -1.04
6 547400                 B                  2             1.87
0.32         0.39

What I want to do is to calculate correlation between each factor and
external for each Symbol, and record the corr. estimate, the p.value,
the name and number of observations in a vector named "vector", then
rbind these vectors together in "results". When there are fewer than 5
observations for a particular symbol I want to put NAs in each column
of "vector".

I tried with the following code, making assumption that by splits
database into sort of smaller dataframes for each Symbol (that's the
"x"):

factor.names <- c("Factor1", "Factor2")
factor.pvalue <- c("SigF1", "SigF2")
results <- numeric()
vector <- matrix(0, ncol=(length(factor.names)*2+2), nrow=1)
colnames(vector) <- c("No.obs", factor.names, factor.pvalue)

application <- function(x){

    rownames(vector) <- x$Name

    for(i in 1:length(factor.names)){

        if(dim(x)[1]>=5){
            vector[1] <- dim(x)[1]
            vector[i+1] <- cor.test(x$External, x[,factor.names[i]],
method="kendall")$estimate
            vector[i+3] <- cor.test(x$External, x[,factor.names[i]],
method="kendall")$p.value
        } else {
            vector <- rep(NA, length(vector))
        }
    }
    results <- rbind(results, vector)
}

by(database, database$Symbol, application)

This did not work. I get :
"Error in dimnames(x) <- dn :
  length of 'dimnames' [1] not equal to array extent"

I used browser() and I see that the Name is not assigned to the row
name of vector and then dim(x)[1] does not work.

What am I doing wrong? Do not understand. :-(

Thank you in advance for your help.

Regards,
JM

-- 
Jonas Malmros
Stockholm University
Stockholm, Sweden


From jonas.malmros at gmail.com  Mon Dec 17 19:48:40 2007
From: jonas.malmros at gmail.com (Jonas Malmros)
Date: Mon, 17 Dec 2007 19:48:40 +0100
Subject: [R] How to create a mixed col.names?
In-Reply-To: <20071217142201.GC20681@localdomain>
References: <fd3c7adf0712170608r444e009djc63c489eb2627e78@mail.gmail.com>
	<20071217142201.GC20681@localdomain>
Message-ID: <fd3c7adf0712171048p4cfb7a4eybab8237cefe4212b@mail.gmail.com>

Thanks Gabor!

On Dec 17, 2007 3:22 PM, Gabor Csardi <csardi at rmki.kfki.hu> wrote:
> paste(rep(c("Factor", "Sign Factor"), 5), rep(1:5, each=2))
>
> Replace '5' with the desired number,
> Gabor
>
>
> On Mon, Dec 17, 2007 at 03:08:09PM +0100, Jonas Malmros wrote:
> > Hello,
> >
> > I have a vector of names, say :
> >
> > names <- c("Factor 1", "Factor 2", Factor 3")
> >
> > I am creating a dataframe and I want the column names to be mixed like this:
> > "Factor 1" " Sign Factor 1" "Factor 2" "Sign Factor 2" "Factor 3"
> > "Sign Factor 3"
> > How can I automate the creation of such a mixed vector? I tried with
> > rep but did not succeed.
> >
> > Could someone please suggest a solution to this problem?
> > Thanks in advance!
> >
> > Regards,
> > JM
> > --
> > Jonas Malmros
> > Stockholm University
> > Stockholm, Sweden
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK
>



-- 
Jonas Malmros
Stockholm University
Stockholm, Sweden


From jonas.malmros at gmail.com  Mon Dec 17 20:03:07 2007
From: jonas.malmros at gmail.com (Jonas Malmros)
Date: Mon, 17 Dec 2007 20:03:07 +0100
Subject: [R] Cannot grasp how to apply "by" here...
Message-ID: <fd3c7adf0712171103yc4571e8k4d8649d510181ded@mail.gmail.com>

Obviously, A cannot assign a row name because the dimensions do not
agree. I can use rownames(vector) <- x$Name[1] though.

then things get calculated (I saw it with browser()) but rbind does
not do what I want it to do, "results" remains numeric().

why?

-- 
Jonas Malmros
Stockholm University
Stockholm, Sweden


From cskiadas at gmail.com  Mon Dec 17 20:02:40 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Mon, 17 Dec 2007 14:02:40 -0500
Subject: [R] Cannot grasp how to apply "by" here...
In-Reply-To: <fd3c7adf0712171047v128897fek622ae9bbd9b9ed1c@mail.gmail.com>
References: <fd3c7adf0712171047v128897fek622ae9bbd9b9ed1c@mail.gmail.com>
Message-ID: <DE06CB2C-EDD2-4249-8AC9-5FF128FDA6B5@gmail.com>

On Dec 17, 2007, at 1:47 PM, Jonas Malmros wrote:

> factor.names <- c("Factor1", "Factor2")
> factor.pvalue <- c("SigF1", "SigF2")
> results <- numeric()
> vector <- matrix(0, ncol=(length(factor.names)*2+2), nrow=1)
> colnames(vector) <- c("No.obs", factor.names, factor.pvalue)

If you look at "vector" you'll see it has column dimension 6. You are  
trying to assign to it 5 colnames, which is not going to work. That's  
exactly what the error tells you, and it happens on the line above,  
the rest of your code is irrelevant to it.

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From jonas.malmros at gmail.com  Mon Dec 17 20:07:08 2007
From: jonas.malmros at gmail.com (Jonas Malmros)
Date: Mon, 17 Dec 2007 20:07:08 +0100
Subject: [R] Cannot grasp how to apply "by" here...
In-Reply-To: <fd3c7adf0712171105n4ff41077x685ca45123c6e414@mail.gmail.com>
References: <fd3c7adf0712171047v128897fek622ae9bbd9b9ed1c@mail.gmail.com>
	<DE06CB2C-EDD2-4249-8AC9-5FF128FDA6B5@gmail.com>
	<fd3c7adf0712171105n4ff41077x685ca45123c6e414@mail.gmail.com>
Message-ID: <fd3c7adf0712171107w304cb8f1n1fc5753348a8df58@mail.gmail.com>

Dear Charilaos,
Thanks, I see what you mean, but I just simplified the real code here
and I made a mistake by putting +2, it is +1. :-)

JM


 On Dec 17, 2007 8:02 PM, Charilaos Skiadas <cskiadas at gmail.com> wrote:
 > On Dec 17, 2007, at 1:47 PM, Jonas Malmros wrote:
 >
 > > factor.names <- c("Factor1", "Factor2")
 > > factor.pvalue <- c("SigF1", "SigF2")
 > > results <- numeric()
 > > vector <- matrix(0, ncol=(length(factor.names)*2+2), nrow=1)
 > > colnames(vector) <- c("No.obs", factor.names, factor.pvalue)
 >
> > If you look at "vector" you'll see it has column dimension 6. You are
> > trying to assign to it 5 colnames, which is not going to work. That's
> > exactly what the error tells you, and it happens on the line above,
> > the rest of your code is irrelevant to it.
> >
> > Haris Skiadas
> > Department of Mathematics and Computer Science
> > Hanover College
> >
> >
> >
> >
> >
>
>
>
>
> --
> Jonas Malmros
> Stockholm University
> Stockholm, Sweden
>



-- 
Jonas Malmros
Stockholm University
Stockholm, Sweden


From r.turner at auckland.ac.nz  Mon Dec 17 20:24:53 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 18 Dec 2007 08:24:53 +1300
Subject: [R] regression towards the mean, AS paper November 2007
In-Reply-To: <4766C0C0.2080509@stats.uwo.ca>
References: <4766BE33.1030806@gvdnet.dk> <4766C0C0.2080509@stats.uwo.ca>
Message-ID: <36F293BA-497C-43D7-8943-7AFD3C8648A4@auckland.ac.nz>


On 18/12/2007, at 7:32 AM, Duncan Murdoch wrote:

> On 12/17/2007 1:21 PM, Troels Ring wrote:
>> Dear friends, regression towards the mean is interesting in medical
>> circles, and a very recent paper (The American Statistician November
>> 2007;61:302-307 by Krause and Pinheiro) treats it at length. An  
>> initial
>> example specifies (p 303):
>> "Consider the following example: we draw 100 samples from a bivariate
>> Normal distribution with X0~N(0,1), X1~N(0,1) and cov(X0,X1)=0.7, We
>> then calculate the p value for the null hypothesis that the means  
>> of X0
>> and X1 are equal, using a paired Student's t test. The procedure is
>> repeated 1000 times, producing 1000 simulated p values. Because X0  
>> and
>> X1 have identical marginal distributions, the simulated p values  
>> behave
>> like independent Uniform(0,1) random variables." This I did not
>> understand, and simulating like shown below produced far from uniform
>> (0,1) p values - but I fail to see how it is wrong. I contacted the
>> authors of the paper but they did not answer. So, please, doesn?t the
>> code below specify a bivariate N(0,1) with covariance 0.7? I get p
>> values = 1 all over - not interesting, but how wrong?
>> Best wishes
>> Troels
>>
>> library(MASS)
>> Sigma <- matrix(c(1,0.7,0.7,1),2,2)
>> Sigma
>> res <- NULL
>> for (i in 1:1000){
>> ff <-(mvrnorm(n=100, rep(0, 2), Sigma, empirical = TRUE))
>> res[i] <- t.test(ff[,1],ff[,2],paired=TRUE)$p.value}
>
> Specifying empirical=TRUE means that your sampled values are not
> independent, the means are guaranteed to match exactly, and the mean
> difference is exactly zero.  Thus all of the t statistics are exactly
> zero, and the p-values are exactly 1.
>
> Set empirical=FALSE (the default), and you'll see more reasonable  
> results.

	This has nothing to do really with the question that Troels asked,
	but the exposition quoted from the AA paper is unnecessarily confusing.
	The phrase ``Because X0 and X1 have identical marginal  
distributions ...''
	throws the reader off the track.  The identical marginal distributions
	are irrelevant.  All one needs is that the ***means*** of X0 and X1
	be the same, and then the null hypothesis tested by a paired t-test
	is true and so the p-values are (asymptotically) Uniform[0,1].  With
	a sample size of 100, the ``asymptotically'' bit can be safely ignored
	for any ``decent'' joint distribution of X0 and X1.  If one further
	assumes that X0 - X1 is Gaussian (which has nothing to do with X0 and
	X1 having identical marginal distributions) then ``asymptotically''
	turns into ``exactly''.

				cheers,

					Rolf Turner
######################################################################
Attention: 
This e-mail message is privileged and confidential. If you are not the 
intended recipient please delete the message and notify the sender. 
Any views or opinions presented are solely those of the author.

This e-mail has been scanned and cleared by MailMarshal 
www.marshalsoftware.com
######################################################################


From tkremund98 at hotmail.com  Mon Dec 17 20:29:09 2007
From: tkremund98 at hotmail.com (Todd Remund)
Date: Mon, 17 Dec 2007 12:29:09 -0700
Subject: [R] Axes limits in rgl.surface.
Message-ID: <BAY121-W150613A3C43E86740A20C1D4620@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/9f68d4d1/attachment.pl 

From johannes at huesing.name  Mon Dec 17 20:38:36 2007
From: johannes at huesing.name (Johannes Hsing)
Date: Mon, 17 Dec 2007 20:38:36 +0100
Subject: [R] Function for AUC?
In-Reply-To: <a695fbee0712170207n110b18c1gb237d0e68faa7d39@mail.gmail.com>
References: <a695fbee0712131038l3f94dbb0va55fe1fadc29418f@mail.gmail.com>
	<bc47d3330712161758j1cc895f1r51238fdd9cce3595@mail.gmail.com>
	<70a3a4850712170057i104bc793qcbecffecaa310bd7@mail.gmail.com>
	<a695fbee0712170207n110b18c1gb237d0e68faa7d39@mail.gmail.com>
Message-ID: <20071217193836.GA5331@huesing.name>

Armin Goralczyk <agoralczyk at gmail.com> [Mon, Dec 17, 2007 at 11:07:25AM CET]:
[AUC]
> 
> I tried it:
> 
> > y<-c(1,2,3,4,5);x<-c(10,15,10,5,0)

Are you sure you don't have x and y wrong? Normally the x values
should be monotonically increasing.

-- 
Johannes H?sing               There is something fascinating about science. 
                              One gets such wholesale returns of conjecture 
mailto:johannes at huesing.name  from such a trifling investment of fact.                
http://derwisch.wikidot.com         (Mark Twain, "Life on the Mississippi")


From r.turner at auckland.ac.nz  Mon Dec 17 20:49:29 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 18 Dec 2007 08:49:29 +1300
Subject: [R] polygon class in splancs package
In-Reply-To: <007b01c840b1$6979fb10$3d1ec454@ibis.luc.ac.be>
References: <007b01c840b1$6979fb10$3d1ec454@ibis.luc.ac.be>
Message-ID: <C1184E9D-E31F-489A-816A-11764DC50AFD@auckland.ac.nz>


On 18/12/2007, at 2:33 AM, Elke Moons wrote:

> Dear forum,
>
>
>
> I would like to use the kernel2d or spkernel2d in the Splancs- 
> package, but
> it does not recognize my polygon data.
>
>
>
> "Error in kernel2d(as.points(ptsbin), polygonprov, h0 = 2, nx =  
> 100,  : " is
> the error message.
>
>         Invalid poly argument
>
>
>
> The data are defined as follows:
>
> polgonprov<-list(x=polyprov$X, y=polyprov$Y) with X and Y  
> coordinates in the
> Lambert1972 notation. The points are defined in the same coordinate  
> system
> and it does recognize them, so I don't suppose that is the problem.
>
>
>
> I can also draw the province polygon by:
>
> plot(c(192800,254100),c(154100,221800),type="n")
>
> polygon(polyprov$X,polyprov$Y)
>
>
>
> Can someone help me with this? Or explain to me how exactly is the  
> polygon
> class defined in splancs? I also tried with
>
> Polygonprov<-Polygon(list(x=polyprov$X,y:polyprov$Y)) but that does  
> not seem
> to work either.

	Not absolutely sure, but I infer from experimentation that ``poly''  
should
	be a (2-column) matrix, not a list:

	plot(0:1,0:1,type="n",xlab="x",ylab="y")
	melvin <- getpoly() # Clickety-clickety-clickety.
	str(melvin)
	clyde <- csr(melvin,100)
	irving <- kernel2d(clyde,melvin,h0=2)
	image(irving)

	HTH.

		cheers,

			Rolf Turner

######################################################################
Attention:\ This e-mail message is privileged and confid...{{dropped:9}}


From aiminy at iastate.edu  Mon Dec 17 20:50:41 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Mon, 17 Dec 2007 13:50:41 -0600
Subject: [R] gene shaving method
Message-ID: <6.1.2.0.2.20071217134901.031c9e98@aiminy.mail.iastate.edu>

Does anyone know if Hastie's gene shaving method is implemented in R

Thanks,

Aimin


From murdoch at stats.uwo.ca  Mon Dec 17 21:07:03 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 17 Dec 2007 15:07:03 -0500
Subject: [R] Axes limits in rgl.surface.
In-Reply-To: <BAY121-W150613A3C43E86740A20C1D4620@phx.gbl>
References: <BAY121-W150613A3C43E86740A20C1D4620@phx.gbl>
Message-ID: <4766D6E7.908@stats.uwo.ca>

On 12/17/2007 2:29 PM, Todd Remund wrote:
> I have looked through the documentation and have not been able to find a way of using an xlim, ylim, or zlim type option on rgl.surface.  I know that persp3d has the option, but seems to only be able to expand the axes not reduce them.  Is there anyone who has an idea of how to do this?  Thank you for your time.

You'll need to do the trimming yourself.  There isn't currently any 
support for user-controlled clipping regions in rgl.

That is:  for the usual case where x and y are vectors, to limit x or y 
to a certain range, just take a subset of those values, and a subset of 
the rows or columns of z.  To limit the range of z, set out of range 
entries to NA.  It will probably look ugly because it will get a very 
ragged edge.

It appears that the NA handling for the case where x or y is a matrix 
leaves something to be desired.

Duncan Murdoch


From albert.greinoecker at uibk.ac.at  Mon Dec 17 21:10:08 2007
From: albert.greinoecker at uibk.ac.at (Albert Greinoecker)
Date: Mon, 17 Dec 2007 21:10:08 +0100
Subject: [R] margin between plot region and axes
Message-ID: <1197922208.6147.25.camel@localhost>

Hi useRs,

in the following graphic...
http://www.survey4all.org/tmp/with_margin.png
I drawed a function and added two axes afterwards with "axis". I could
not find a way to erase the margin between the axes and the plotting
region, so I solved the problem with "text" and "segments", which looks
like this: 
http://www.survey4all.org/tmp/no_margin.png

My question: is there a way to add axes the usual way (as tried for the
first graphic), but to erase the margin, so that the axes start at point
(0/0) in my case.

thanks in advance for any help,
Albert


From scionforbai at gmail.com  Mon Dec 17 21:22:06 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Mon, 17 Dec 2007 21:22:06 +0100
Subject: [R] margin between plot region and axes
In-Reply-To: <1197922208.6147.25.camel@localhost>
References: <1197922208.6147.25.camel@localhost>
Message-ID: <e9ee1f0a0712171222ve37cda3t98f396a60cf21e69@mail.gmail.com>

> My question: is there a way to add axes the usual way (as tried for the
> first graphic), but to erase the margin, so that the axes start at point
> (0/0) in my case.

Not really sure if this is what you ask, but maybe you should call
your first plot() with xaxs="i" and yaxs="i". It reduces the default
4% increase on data range on both axis.


From agoralczyk at gmail.com  Mon Dec 17 21:38:19 2007
From: agoralczyk at gmail.com (Armin Goralczyk)
Date: Mon, 17 Dec 2007 21:38:19 +0100
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
Message-ID: <a695fbee0712171238g4995040x579e58f52f83376e@mail.gmail.com>

On Dec 15, 2007 6:31 PM, David Winsemius <dwinsemius at comcast.net> wrote:

> > pm.srch<- function (){
>    srch.stem <-"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term="
>    query <-as.character(scan(file="",what="character"))
>    doc <-xmlTreeParse(paste(srch.stem,query,sep=""),isURL = TRUE,
>          useInternalNodes = TRUE)
>    sapply(c("//Id"), xpathApply, doc = doc, fun = xmlValue)
>      }
> > pm.srch()
> 1: "laryngeal neoplasms[mh]"
> 2:
> Read 1 item
>       //Id
>  [1,] "18042931"
>  [2,] "18038886"
>  [3,] "17978930"
>  [4,] "17974987"
>  [5,] "17972507"
>  [6,] "17970149"
>  [7,] "17967299"
>  [8,] "17962724"
>  [9,] "17954109"
> [10,] "17942038"
> [11,] "17940076"
> [12,] "17848290"
> [13,] "17848288"
> [14,] "17848287"
> [15,] "17848278"
> [16,] "17938330"
> [17,] "17938329"
> [18,] "17918311"
> [19,] "17910347"
> [20,] "17908862"
>
>

I tried the above function with simple search terms and it worked fine
for me (also more output thanks to Martin's post) but when I use
search terms attributed to certain fields, i.e. with [au] or [ta], I
get the following error message:

> pm.srch()
1: "laryngeal neoplasms[mh]"
2:
Read 1 item
Fehler in .Call("RS_XML_ParseTree", as.character(file), handlers,
as.logical(ignoreBlanks),  :
  error in creating parser for
http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=laryngeal
neoplasms[mh]
I/O warning : failed to load external entity
"http%3A//eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi%3Fdb=pubmed&term=laryngeal%20neoplasms%5Bmh%5D"
>

What's wrong?
Thanks for any help
-- 
Armin Goralczyk, M.D.
--
Universit?tsmedizin G?ttingen
Abteilung Allgemein- und Viszeralchirurgie
Rudolf-Koch-Str. 40
39099 G?ttingen
--
Dept. of General Surgery
University of G?ttingen
G?ttingen, Germany
--
http://www.chirurgie-goettingen.de

From joe_retzer at yahoo.com  Mon Dec 17 21:50:52 2007
From: joe_retzer at yahoo.com (Joseph Retzer)
Date: Mon, 17 Dec 2007 12:50:52 -0800 (PST)
Subject: [R] Cluster Package - Clara w/ categorical variables
In-Reply-To: <14370262.post@talk.nabble.com>
Message-ID: <496958.54402.qm@web60323.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/213c080d/attachment.pl 

From lee.kitty at yahoo.com  Mon Dec 17 22:01:49 2007
From: lee.kitty at yahoo.com (Kitty Lee)
Date: Mon, 17 Dec 2007 13:01:49 -0800 (PST)
Subject: [R] Dual Core vs Quad Core
Message-ID: <674056.74408.qm@web36304.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/e332ce15/attachment.pl 

From WinkelD at BATTELLE.ORG  Mon Dec 17 22:03:14 2007
From: WinkelD at BATTELLE.ORG (Winkel, David)
Date: Mon, 17 Dec 2007 16:03:14 -0500
Subject: [R] bar plot colors
Message-ID: <81EA3460BED3AB4483920E05F13B12DD07D79189@WS-BCO-MSE8.milky-way.battelle.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/9a4f2699/attachment.pl 

From dylan.beaudette at gmail.com  Mon Dec 17 22:13:22 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Mon, 17 Dec 2007 13:13:22 -0800
Subject: [R] odd error messages coming from val.prob() {Design}
Message-ID: <200712171313.22872.dylan.beaudette@gmail.com>

Hi,

after upgrading my R install from 2.5 -> 2.6.1 and performing multiple 
iterations of update.packages(), I am getting an odd error when trying to 
plot a calibration curve from the val.prob() function in package Design.

when running this function (which used to work) I get the following error 
message:

Error in .C("lowess", x = as.double(xy$x[o]), as.double(xy$y[o]), n, 
as.double(f),  : 
  C symbol name "lowess" not in DLL for package "base"

Here is my relevent session info:

R version 2.6.1 (2007-11-26) 
i686-pc-linux-gnu 

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] splines   stats     graphics  grDevices utils     datasets  methods  
[8] base     

other attached packages:
 [1] e1071_1.5-17   class_7.2-38   lattice_0.17-2 Cairo_1.3-5    ROCR_1.0-2    
 [6] gplots_2.3.2   gdata_2.3.1    gtools_2.4.0   Design_2.1-1   survival_2.34 
[11] Hmisc_3.4-3   

loaded via a namespace (and not attached):
[1] cluster_1.11.9  grid_2.6.1      rcompgen_0.1-17


Any ideas?

cheers,

Dylan



-- 
Dylan Beaudette
Soil Resource Laboratory
http://casoilresource.lawr.ucdavis.edu/
University of California at Davis
530.754.7341


From h.wickham at gmail.com  Mon Dec 17 22:10:32 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 17 Dec 2007 15:10:32 -0600
Subject: [R] regression towards the mean, AS paper November 2007
In-Reply-To: <36F293BA-497C-43D7-8943-7AFD3C8648A4@auckland.ac.nz>
References: <4766BE33.1030806@gvdnet.dk> <4766C0C0.2080509@stats.uwo.ca>
	<36F293BA-497C-43D7-8943-7AFD3C8648A4@auckland.ac.nz>
Message-ID: <f8e6ff050712171310l381a275eq75da61305c640573@mail.gmail.com>

>         This has nothing to do really with the question that Troels asked,
>         but the exposition quoted from the AA paper is unnecessarily confusing.
>         The phrase ``Because X0 and X1 have identical marginal
> distributions ...''
>         throws the reader off the track.  The identical marginal distributions
>         are irrelevant.  All one needs is that the ***means*** of X0 and X1
>         be the same, and then the null hypothesis tested by a paired t-test
>         is true and so the p-values are (asymptotically) Uniform[0,1].  With
>         a sample size of 100, the ``asymptotically'' bit can be safely ignored
>         for any ``decent'' joint distribution of X0 and X1.  If one further
>         assumes that X0 - X1 is Gaussian (which has nothing to do with X0 and
>         X1 having identical marginal distributions) then ``asymptotically''
>         turns into ``exactly''.

Another related issue is that uniform distributions don't look very uniform:

hist(runif(100))
hist(runif(1000))
hist(runif(10000))

Be sure to calibrate your eyes (and your bin width) before rejecting
the hypothesis that the distribution is uniform.

Hadley

-- 
http://had.co.nz/


From h.wickham at gmail.com  Mon Dec 17 22:14:12 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 17 Dec 2007 15:14:12 -0600
Subject: [R] bar plot colors
In-Reply-To: <81EA3460BED3AB4483920E05F13B12DD07D79189@WS-BCO-MSE8.milky-way.battelle.org>
References: <81EA3460BED3AB4483920E05F13B12DD07D79189@WS-BCO-MSE8.milky-way.battelle.org>
Message-ID: <f8e6ff050712171314n1c6374bcoe93a34ec5b86caff@mail.gmail.com>

> I have a question regarding colors in bar plots.  I want to stack a
> total of 18 cost values in each bar. Basically, it is six cost types and
> each cost type has three components- direct, indirect, and induced
> costs.  I would like to use both solid color bars and bars with the
> slanted lines (using the density parameter).  The colors would
> distinguish cost types and the lines would distinguish
> direct/indirect/induced.  I want the cost types (i.e. colors) to be
> stacked together for each cost type.  In other words, I don't want all
> of the solid bars at the bottom and all of the slanted lines at the top.

What are you trying to achieve with such a plot?  A stacked bar chart
only allows easy comparisons of cumulative totals (eg. cost 1, or cost
1 + 2, or cost 1 + 2 + 3) and it will be very difficult to compare
individual cost types or components within a type.  You might want to
think about a series of line plots instead.

Hadley

-- 
http://had.co.nz/


From albert.greinoecker at uibk.ac.at  Mon Dec 17 22:19:37 2007
From: albert.greinoecker at uibk.ac.at (Albert Greinoecker)
Date: Mon, 17 Dec 2007 22:19:37 +0100
Subject: [R] margin between plot region and axes
In-Reply-To: <1197926256.6147.36.camel@localhost>
References: <1197922208.6147.25.camel@localhost>
	<e9ee1f0a0712171222ve37cda3t98f396a60cf21e69@mail.gmail.com>
	<1197926256.6147.36.camel@localhost>
Message-ID: <1197926378.6147.39.camel@localhost>

Am Montag, den 17.12.2007, 21:22 +0100 schrieb Scionforbai:
> > My question: is there a way to add axes the usual way (as tried for
the
> > first graphic), but to erase the margin, so that the axes start at
point
> > (0/0) in my case.
> 
> Not really sure if this is what you ask, but maybe you should call
> your first plot() with xaxs="i" and yaxs="i". It reduces the default
> 4% increase on data range on both axis.

thanks for your answer and sorry for the weak explanation of the
problem.
To put it another way:
consider the following simple statement:
hist(rnorm(100))
how can I influence drawing of the axis so that they lie directly on the
edges of the bars? The solution on this I think could directly be
applied to my problem.

regards,
Albert


From vogranovich at jumptrading.com  Mon Dec 17 22:43:24 2007
From: vogranovich at jumptrading.com (Vadim Ogranovich)
Date: Mon, 17 Dec 2007 15:43:24 -0600 (CST)
Subject: [R] names in Rscript -e
Message-ID: <969899.7371197927804405.JavaMail.root@jumpmail1.w2k.jumptrading.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/a2a3ed9c/attachment.pl 

From vogranovich at jumptrading.com  Mon Dec 17 23:03:03 2007
From: vogranovich at jumptrading.com (Vadim Ogranovich)
Date: Mon, 17 Dec 2007 16:03:03 -0600 (CST)
Subject: [R] names in Rscript -e
In-Reply-To: <969899.7371197927804405.JavaMail.root@jumpmail1.w2k.jumptrading.com>
Message-ID: <5955499.7521197928983650.JavaMail.root@jumpmail1.w2k.jumptrading.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/80728590/attachment.pl 

From milton_ruser at yahoo.com.br  Mon Dec 17 23:33:41 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Mon, 17 Dec 2007 14:33:41 -0800 (PST)
Subject: [R] Res:  convert table
Message-ID: <985099.16512.qm@web56002.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/c5f83826/attachment.pl 

From dunn at usq.edu.au  Mon Dec 17 23:38:52 2007
From: dunn at usq.edu.au (Peter Dunn)
Date: Tue, 18 Dec 2007 08:38:52 +1000
Subject: [R] Identity link in tweedie
In-Reply-To: <4766A3C3.5070900@eva.mpg.de>
References: <4766A3C3.5070900@eva.mpg.de>
Message-ID: <200712180838.52338.dunn@usq.edu.au>

Cristina

> I'm using the tweedie distribution package and I cant figure out
> how to run a model using an identity link. I know I can use a log
> link by having link.power=0 and I think identity would be
> link.power=1, but I'm not sure. 

Yes, that is correct.

> Furthermore when I try running it 
> with link.power=1 it requires starting values which I cant manage
> to give appropriately 

Without any code to see, I will guess at what is happening.  The 
trouble with using an identity link is that the predicted means, 
say mu, may be negative, where as the Tweedie distributions have mu 
greater than zero.  In using the identity link, you may be asking R 
to fit the impossible.

Perhaps the question you need to ask is this:  Why do you need an 
identity link?

P.

-- 
Dr Peter Dunn  |  dunn <at> usq.edu.au
Faculty of Sciences, USQ; http://www.sci.usq.edu.au/staff/dunn
Aust. Centre for Sustainable Catchments: www.usq.edu.au/acsc

This email (including any attached files) is confidentia...{{dropped:15}}


From murdoch at stats.uwo.ca  Tue Dec 18 00:12:14 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 17 Dec 2007 18:12:14 -0500
Subject: [R] margin between plot region and axes
In-Reply-To: <1197926378.6147.39.camel@localhost>
References: <1197922208.6147.25.camel@localhost>	<e9ee1f0a0712171222ve37cda3t98f396a60cf21e69@mail.gmail.com>	<1197926256.6147.36.camel@localhost>
	<1197926378.6147.39.camel@localhost>
Message-ID: <4767024E.9080008@stats.uwo.ca>

On 17/12/2007 4:19 PM, Albert Greinoecker wrote:
> Am Montag, den 17.12.2007, 21:22 +0100 schrieb Scionforbai:
>>> My question: is there a way to add axes the usual way (as tried for
> the
>>> first graphic), but to erase the margin, so that the axes start at
> point
>>> (0/0) in my case.
>> Not really sure if this is what you ask, but maybe you should call
>> your first plot() with xaxs="i" and yaxs="i". It reduces the default
>> 4% increase on data range on both axis.
> 
> thanks for your answer and sorry for the weak explanation of the
> problem.
> To put it another way:
> consider the following simple statement:
> hist(rnorm(100))
> how can I influence drawing of the axis so that they lie directly on the
> edges of the bars? The solution on this I think could directly be
> applied to my problem.

That's what he was getting at:

 > par(xaxs="i", yaxs="i")
 > hist(rnorm(100))

Duncan Murdoch


From albert.greinoecker at uibk.ac.at  Tue Dec 18 00:19:56 2007
From: albert.greinoecker at uibk.ac.at (Albert Greinoecker)
Date: Tue, 18 Dec 2007 00:19:56 +0100
Subject: [R] margin between plot region and axes
In-Reply-To: <4767024E.9080008@stats.uwo.ca>
References: <1197922208.6147.25.camel@localhost>
	<e9ee1f0a0712171222ve37cda3t98f396a60cf21e69@mail.gmail.com>
	<1197926256.6147.36.camel@localhost>
	<1197926378.6147.39.camel@localhost>
	<4767024E.9080008@stats.uwo.ca>
Message-ID: <1197933596.6147.50.camel@localhost>

yes, that's it, thank's a lot to all respondents!

Am Montag, den 17.12.2007, 18:12 -0500 schrieb Duncan Murdoch:
> On 17/12/2007 4:19 PM, Albert Greinoecker wrote:
> > Am Montag, den 17.12.2007, 21:22 +0100 schrieb Scionforbai:
> >>> My question: is there a way to add axes the usual way (as tried for
> > the
> >>> first graphic), but to erase the margin, so that the axes start at
> > point
> >>> (0/0) in my case.
> >> Not really sure if this is what you ask, but maybe you should call
> >> your first plot() with xaxs="i" and yaxs="i". It reduces the default
> >> 4% increase on data range on both axis.
> > 
> > thanks for your answer and sorry for the weak explanation of the
> > problem.
> > To put it another way:
> > consider the following simple statement:
> > hist(rnorm(100))
> > how can I influence drawing of the axis so that they lie directly on the
> > edges of the bars? The solution on this I think could directly be
> > applied to my problem.
> 
> That's what he was getting at:
> 
>  > par(xaxs="i", yaxs="i")
>  > hist(rnorm(100))
> 
> Duncan Murdoch


From jonas.malmros at gmail.com  Tue Dec 18 01:52:51 2007
From: jonas.malmros at gmail.com (Jonas Malmros)
Date: Tue, 18 Dec 2007 01:52:51 +0100
Subject: [R] Why is conversion not working?
Message-ID: <fd3c7adf0712171652h4d48da4fqd237ac3711bb6b73@mail.gmail.com>

I have a data frame, where two last columns - "month" and "year" - are
character vectors. The "year" vector is made of two numbers (i.e. "97"
for 1997, "07" for 2007, etc)
What I want to do is to create a variable "Year" that is mode numeric
and where each record is a four-figure number (1997, 2007,...)
I have about 40000 rows in the dataframe, the observations are for 10
years (so there are multiple rows for each year).
I tried the following, but the program runs and runs, and if I abort
it all the years in "Year" are 1997:

for(i in 1:dim(database)[1]){
    if(database$year[i]>90) {
           database$Year[i] <- as.numeric(database$year[i])+1900 } else {
           database$Year[i] <- as.numeric(database$year[i])+2000
    }
}

Thanks in advance for explanations.

Regards,
JM

-- 
Jonas Malmros
Stockholm University
Stockholm, Sweden


From dwinsemius at comcast.net  Tue Dec 18 01:53:09 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 18 Dec 2007 00:53:09 +0000 (UTC)
Subject: [R] Analyzing Publications from Pubmed via XML
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<a695fbee0712171238g4995040x579e58f52f83376e@mail.gmail.com>
Message-ID: <Xns9A09CA51DB1E4dNOTwinscomcast@80.91.229.13>

"Armin Goralczyk" <agoralczyk at gmail.com> wrote in
news:a695fbee0712171238g4995040x579e58f52f83376e at mail.gmail.com: 

> On Dec 15, 2007 6:31 PM, David Winsemius <dwinsemius at comcast.net>
> wrote: 
>> > pm.srch<- function (){
>>    srch.stem
>>    <-"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pub
>>    med&term=" query <-as.character(scan(file="",what="character"))
>>    doc <-xmlTreeParse(paste(srch.stem,query,sep=""),isURL = TRUE,
>>          useInternalNodes = TRUE)
>>    sapply(c("//Id"), xpathApply, doc = doc, fun = xmlValue)
>>      }
>> > pm.srch()
>> 1: "laryngeal neoplasms[mh]"
>> 2:
>> Read 1 item
>>       //Id
>>  [1,] "18042931"
snipped list of IDs
>>
>>
> I tried the above function with simple search terms and it worked fine
> for me (also more output thanks to Martin's post) but when I use
> search terms attributed to certain fields, i.e. with [au] or [ta], I
> get the following error message:
>> pm.srch()
> 1: "laryngeal neoplasms[mh]"
> 2:
> Read 1 item
> Fehler in .Call("RS_XML_ParseTree", as.character(file), handlers,
> as.logical(ignoreBlanks),  :
>   error in creating parser for
> http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&ter
> m=laryngeal neoplasms[mh]
> I/O warning : failed to load external entity
> "http%3A//eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi%3Fdb=pubme
> d&term=laryngeal%20neoplasms%5Bmh%5D" 
>>
> What's wrong?

I'm not sure. You included my simple example. rather than your search string 
that provoked an error. This is an example search that one can find on 
the how-to page for literature searches with /esearch:

http://www.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=PNAS[ta]+AND+97[vi]&retstart=6&retmax=6&tool=biomed3

I am wondering if you used spaces, rather than "+"'s? If so then you may 
want your function to do more gsub-processing of the input string.

When I use the search terms in NCBI's example I get:

> pm.srch<- function (){
+    srch.stem<-"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term="
+              query<-as.character(scan(file="",what="character"))
+              doc<-xmlTreeParse(paste(srch.stem,query,sep=""),isURL = TRUE, useInternalNodes = TRUE)
+              sapply(c("//Id"), xpathApply, doc = doc, fun = xmlValue)
+      }
> doc.xml<-pm.srch()
1: "PNAS[ta]+AND+97[vi]"
2: 
Read 1 item
> doc.xml
      //Id      
 [1,] "16578858"
 [2,] "11186225"
 [3,] "11121081"
 [4,] "11121080"
 [5,] "11121079"
 [6,] "11121078"
 [7,] "11121077"
 [8,] "11121076"
 [9,] "11121075"
[10,] "11121074"
[11,] "11121073"
[12,] "11121072"
[13,] "11121071"
[14,] "11121070"
[15,] "11121069"
[16,] "11121068"
[17,] "11121067"
[18,] "11121066"
[19,] "11121065"
[20,] "11121064"


-- 
David Winsemius, MD


> Thanks for any help
> -- 
> Armin Goralczyk, M.D.


From jason.law at BES.CI.PORTLAND.OR.US  Tue Dec 18 02:13:47 2007
From: jason.law at BES.CI.PORTLAND.OR.US (Law, Jason)
Date: Mon, 17 Dec 2007 17:13:47 -0800
Subject: [R] Why is conversion not working?
Message-ID: <59F44D74269BD711898900B0D0491AD80B15F80E@miranda.bes.city>

Without actually running the code, one problem seems to be that the line
if(database$year[i]>90)
is comparing a character vector to 90 and not the numbers that you are
expecting.

You could try this instead:
database$year <- as.numeric(database$year)
ifelse(database$year > 90, database$year + 1900, database$year + 2000)

Jason Law
Statistician
City of Portland, Bureau of Environmental Services
Water Pollution Control Laboratory
6543 N Burlington Avenue
Portland, OR 97203 -5452
jlaw at bes.ci.portland.or.us



-----Original Message-----
From: r-help-bounces at r-project.org
[mailto:r-help-bounces at r-project.org]On Behalf Of Jonas Malmros
Sent: Monday, December 17, 2007 4:53 PM
To: r-help at r-project.org
Subject: [R] Why is conversion not working?


I have a data frame, where two last columns - "month" and "year" - are
character vectors. The "year" vector is made of two numbers (i.e. "97"
for 1997, "07" for 2007, etc)
What I want to do is to create a variable "Year" that is mode numeric
and where each record is a four-figure number (1997, 2007,...)
I have about 40000 rows in the dataframe, the observations are for 10
years (so there are multiple rows for each year).
I tried the following, but the program runs and runs, and if I abort
it all the years in "Year" are 1997:

for(i in 1:dim(database)[1]){
    if(database$year[i]>90) {
           database$Year[i] <- as.numeric(database$year[i])+1900 } else {
           database$Year[i] <- as.numeric(database$year[i])+2000
    }
}

Thanks in advance for explanations.

Regards,
JM

-- 
Jonas Malmros
Stockholm University
Stockholm, Sweden

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Tue Dec 18 02:11:37 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 17 Dec 2007 20:11:37 -0500
Subject: [R] Why is conversion not working?
In-Reply-To: <fd3c7adf0712171652h4d48da4fqd237ac3711bb6b73@mail.gmail.com>
References: <fd3c7adf0712171652h4d48da4fqd237ac3711bb6b73@mail.gmail.com>
Message-ID: <644e1f320712171711g27e6dbcdt8b0045f1fcef6be9@mail.gmail.com>

try this:

> x <- data.frame(month=as.character(sample(1:12,1000,TRUE)),
+     year=as.character(sample(c(0:7, 95:99), 1000, TRUE)),
stringsAsFactors=FALSE)
> # convert to integer
> x$Year <- as.integer(x$year)
> # now add the century
> x$Year <- ifelse(x$Year < 10, x$Year+2000, x$Year+1900)
>
>
> head(x)
  month year Year
1    10   96 1996
2     9    5 2005
3     8   95 1995
4     1    7 2007
5    11    3 2003
6     2   99 1999
>


On Dec 17, 2007 7:52 PM, Jonas Malmros <jonas.malmros at gmail.com> wrote:
> I have a data frame, where two last columns - "month" and "year" - are
> character vectors. The "year" vector is made of two numbers (i.e. "97"
> for 1997, "07" for 2007, etc)
> What I want to do is to create a variable "Year" that is mode numeric
> and where each record is a four-figure number (1997, 2007,...)
> I have about 40000 rows in the dataframe, the observations are for 10
> years (so there are multiple rows for each year).
> I tried the following, but the program runs and runs, and if I abort
> it all the years in "Year" are 1997:
>
> for(i in 1:dim(database)[1]){
>    if(database$year[i]>90) {
>           database$Year[i] <- as.numeric(database$year[i])+1900 } else {
>           database$Year[i] <- as.numeric(database$year[i])+2000
>    }
> }
>
> Thanks in advance for explanations.
>
> Regards,
> JM
>
> --
> Jonas Malmros
> Stockholm University
> Stockholm, Sweden
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From clists at perrin.socsci.unc.edu  Tue Dec 18 02:13:16 2007
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Mon, 17 Dec 2007 20:13:16 -0500 (EST)
Subject: [R] Dual Core vs Quad Core
In-Reply-To: <674056.74408.qm@web36304.mail.mud.yahoo.com>
References: <674056.74408.qm@web36304.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0712172011390.8627@perrin.socsci.unc.edu>

On Mon, 17 Dec 2007, Kitty Lee wrote:

> Dear R-users,
>
> I use R to run spatial stuff and it takes up a lot of ram. Runs can take hours or days. I am thinking of getting a new desktop. Can R take advantage of the dual-core system?
>
> I have a dual-core computer at work. But it seems that right now R is using only one processor.
>
> The new computers feature quad core with 3GB of RAM. Can R take advantage of the 4 chips? Or am I better off getting a dual core with faster processing speed per chip?
>
> Thanks! Any advice would be really appreciated!
>
> K.

If I have my information right, R will use dual- or quad-cores if it's 
doing two (or four) things at once. The second core will help a little bit 
insofar as whatever else your machine is doing won't interfere with the 
one core on which it's running, but generally things that take a single 
thread will remain on a single core.

As for RAM, if you're doing memory-bound work you should certainly be 
using a 64-bit machine and OS so you can utilize the larger memory space.


----------------------------------------------------------------------
Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
Associate Professor of Sociology; Book Review Editor, _Social Forces_
University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA


From wgavioli at fas.harvard.edu  Tue Dec 18 02:14:23 2007
From: wgavioli at fas.harvard.edu (Wayne Aldo Gavioli)
Date: Mon, 17 Dec 2007 20:14:23 -0500
Subject: [R] Scatterplot Showing All Points
Message-ID: <1197940463.47671eef4045e@webmail.fas.harvard.edu>



Hello all,


I'm trying to graph a scatterplot of a large (5,000 x,y coordinates) of data
with the caveat that many of the data points overlap with each other (share the
same x AND y coordinates).  In using the usual "plot" command,


> plot(education, xlab="etc", ylab="etc")


it seems that the overlap of points is not shown in the graph.  Namely, there
are 5,000 points that should be plotted, as I mentioned above, but because so
many of the points overlap with each other exactly, only about 50-60 points are
actually plotted on the graph.  Thus, there's no indication that Point A shares
its coordinates with 200 other pieces of data and thus is very common while
Point B doesn't share its coordinates with any other pieces of data and thus
isn't common at all.  Is there anyway to indicate the frequency of such points
on such a graph?  Should I be using a different command than "plot"?


Thanks,


Wayne


From spencer.graves at pdf.com  Tue Dec 18 02:46:58 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 17 Dec 2007 17:46:58 -0800
Subject: [R] more structure than 'str'?
In-Reply-To: <971536df0712162008m1815b799pfb93bf71907b9489@mail.gmail.com>
References: <4765E2A5.4010809@pdf.com>
	<971536df0712162008m1815b799pfb93bf71907b9489@mail.gmail.com>
Message-ID: <47672692.4080406@pdf.com>

Hi, Gabor and Charles: 

      Thanks very much for two simple alternatives to 'str(obj)': 

            str(unclass(obj))
            dput(obj) 

      Best Wishes,
      Spencer

Gabor Grothendieck wrote:
> On Dec 16, 2007 9:44 PM, Spencer Graves <spencer.graves at pdf.com> wrote:
>   
>>      How can I see more of the structure than displayed by 'str'?
>> Consider the following:
>>
>>
>> tstDF <- data.frame(a=1, row.names='b')
>>  > str(tstDF)
>> 'data.frame':   1 obs. of  1 variable:
>>  $ a: num 1
>>
>>
>>      The object 'tstDF' has row.names, but I have to suspect they are
>> there -- AND know a function like 'row.names' or 'dimnames' -- to see
>> them.
>>
>>      I've found 'str' extremely valuable for understanding and
>> explaining to others the internal structure of an R object.   In many
>> cases, it has helped me find fairly simple ways to do things with R
>> objects that might have been much more difficult and perhaps infeasible
>> without 'str' -- and without access to the right expert, who may not be
>> available in the time I have to solve a particular problem.
>>
>>      Thanks again to Martin Maechler, who wrote 'str', and to everyone
>> else who has replied to questions from me over the years.
>>
>>      Best Wishes,
>>      Spencer Graves
>>     
>
>
> Try this:
>
>   
>> dput(tstDF)
>>     
> structure(list(a = 1), .Names = "a", row.names = "b", class = "data.frame")
>
>


From jholtman at gmail.com  Tue Dec 18 02:49:58 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 17 Dec 2007 20:49:58 -0500
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
References: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
Message-ID: <644e1f320712171749k7e8b05d9n81ec99150400609e@mail.gmail.com>

Use 'hexbin' from bioconductor to show how many points are in a grid
on the graph.

On Dec 17, 2007 8:14 PM, Wayne Aldo Gavioli <wgavioli at fas.harvard.edu> wrote:
>
>
> Hello all,
>
>
> I'm trying to graph a scatterplot of a large (5,000 x,y coordinates) of data
> with the caveat that many of the data points overlap with each other (share the
> same x AND y coordinates).  In using the usual "plot" command,
>
>
> > plot(education, xlab="etc", ylab="etc")
>
>
> it seems that the overlap of points is not shown in the graph.  Namely, there
> are 5,000 points that should be plotted, as I mentioned above, but because so
> many of the points overlap with each other exactly, only about 50-60 points are
> actually plotted on the graph.  Thus, there's no indication that Point A shares
> its coordinates with 200 other pieces of data and thus is very common while
> Point B doesn't share its coordinates with any other pieces of data and thus
> isn't common at all.  Is there anyway to indicate the frequency of such points
> on such a graph?  Should I be using a different command than "plot"?
>
>
> Thanks,
>
>
> Wayne
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From dxc13 at health.state.ny.us  Tue Dec 18 04:10:35 2007
From: dxc13 at health.state.ny.us (dxc13)
Date: Mon, 17 Dec 2007 19:10:35 -0800 (PST)
Subject: [R]  creating a database
Message-ID: <14375875.post@talk.nabble.com>


useR's,

I am writing a program in which the input can be multidimensional.  As of
now, to hold the input, I have created an n by m matrix where n is the
number of observations and m is the number of variables.  The data that I
could potentially use can contain well over 20,000 observations.  

Can a simple matrix be used for this or would it be better and more
efficient to create an external database to hold the data.  If so, should
the database be created using C and how would I do this (seeing as that I
have never programmed in C)?  

Any help would be greatly appreciated.  Thank you

Derek
-- 
View this message in context: http://www.nabble.com/creating-a-database-tp14375875p14375875.html
Sent from the R help mailing list archive at Nabble.com.


From murdoch at stats.uwo.ca  Tue Dec 18 04:43:50 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 17 Dec 2007 22:43:50 -0500
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
References: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
Message-ID: <476741F6.7040005@stats.uwo.ca>

On 17/12/2007 8:14 PM, Wayne Aldo Gavioli wrote:
> 
> Hello all,
> 
> 
> I'm trying to graph a scatterplot of a large (5,000 x,y coordinates) of data
> with the caveat that many of the data points overlap with each other (share the
> same x AND y coordinates).  In using the usual "plot" command,
> 
> 
>> plot(education, xlab="etc", ylab="etc")
> 
> 
> it seems that the overlap of points is not shown in the graph.  Namely, there
> are 5,000 points that should be plotted, as I mentioned above, but because so
> many of the points overlap with each other exactly, only about 50-60 points are
> actually plotted on the graph.  Thus, there's no indication that Point A shares
> its coordinates with 200 other pieces of data and thus is very common while
> Point B doesn't share its coordinates with any other pieces of data and thus
> isn't common at all.  Is there anyway to indicate the frequency of such points
> on such a graph?  Should I be using a different command than "plot"?

The jitter() function can add a bit of noise to your data, so that 
repeated points show up as groupings instead of isolated points.

Duncan Murdoch


From s.blomberg1 at uq.edu.au  Tue Dec 18 04:49:48 2007
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Tue, 18 Dec 2007 13:49:48 +1000
Subject: [R] Dual Core vs Quad Core
In-Reply-To: <Pine.LNX.4.64.0712172011390.8627@perrin.socsci.unc.edu>
References: <674056.74408.qm@web36304.mail.mud.yahoo.com>
	<Pine.LNX.4.64.0712172011390.8627@perrin.socsci.unc.edu>
Message-ID: <1197949788.9904.4.camel@sib-sblomber01d.sib.uq.edu.au>

I've been running R on a quad-core using Debian Gnu/Linux since March
this year, and I am very pleased with the performance.

Simon.


On Mon, 2007-12-17 at 20:13 -0500, Andrew Perrin wrote:
> On Mon, 17 Dec 2007, Kitty Lee wrote:
> 
> > Dear R-users,
> >
> > I use R to run spatial stuff and it takes up a lot of ram. Runs can take hours or days. I am thinking of getting a new desktop. Can R take advantage of the dual-core system?
> >
> > I have a dual-core computer at work. But it seems that right now R is using only one processor.
> >
> > The new computers feature quad core with 3GB of RAM. Can R take advantage of the 4 chips? Or am I better off getting a dual core with faster processing speed per chip?
> >
> > Thanks! Any advice would be really appreciated!
> >
> > K.
> 
> If I have my information right, R will use dual- or quad-cores if it's 
> doing two (or four) things at once. The second core will help a little bit 
> insofar as whatever else your machine is doing won't interfere with the 
> one core on which it's running, but generally things that take a single 
> thread will remain on a single core.
> 
> As for RAM, if you're doing memory-bound work you should certainly be 
> using a 64-bit machine and OS so you can utilize the larger memory space.
> 
> 
> ----------------------------------------------------------------------
> Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
> Associate Professor of Sociology; Book Review Editor, _Social Forces_
> University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat. 
Lecturer and Consultant Statistician 
Faculty of Biological and Chemical Sciences 
The University of Queensland 
St. Lucia Queensland 4072 
Australia
Room 320 Goddard Building (8)
T: +61 7 3365 2506 
email: S.Blomberg1_at_uq.edu.au

Policies:
1.  I will NOT analyse your data for you.
2.  Your deadline is your problem.

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer can 
be extracted from a given body of data. - John Tukey.


From joe_retzer at yahoo.com  Tue Dec 18 04:52:28 2007
From: joe_retzer at yahoo.com (Joseph Retzer)
Date: Mon, 17 Dec 2007 19:52:28 -0800 (PST)
Subject: [R] Cluster Package - Clara w/ categorical variables
Message-ID: <567459.15903.qm@web60323.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/8ae7d443/attachment.pl 

From jholtman at gmail.com  Tue Dec 18 04:53:04 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 17 Dec 2007 22:53:04 -0500
Subject: [R] creating a database
In-Reply-To: <14375875.post@talk.nabble.com>
References: <14375875.post@talk.nabble.com>
Message-ID: <644e1f320712171953m3c510fe6xe4ed5cdeea9cb9b6@mail.gmail.com>

What are you intending to do with the data?  How big is 'm'?  How do
you want to access the data?  You can always put it in a SQL database
that R can access and then pull out the rows that you are interested
in.  If 'm' is 100, then if you are just keeping numeric data, this
will only require 16MB of memory, so you can just keep it in memory.

Some more information about the characteristics of the data and what
you want to do with it are required to determine what might be the
appropriate method for storing/accessing it.

On Dec 17, 2007 10:10 PM, dxc13 <dxc13 at health.state.ny.us> wrote:
>
> useR's,
>
> I am writing a program in which the input can be multidimensional.  As of
> now, to hold the input, I have created an n by m matrix where n is the
> number of observations and m is the number of variables.  The data that I
> could potentially use can contain well over 20,000 observations.
>
> Can a simple matrix be used for this or would it be better and more
> efficient to create an external database to hold the data.  If so, should
> the database be created using C and how would I do this (seeing as that I
> have never programmed in C)?
>
> Any help would be greatly appreciated.  Thank you
>
> Derek
> --
> View this message in context: http://www.nabble.com/creating-a-database-tp14375875p14375875.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From johannes at huesing.name  Tue Dec 18 04:58:27 2007
From: johannes at huesing.name (Johannes Hsing)
Date: Tue, 18 Dec 2007 04:58:27 +0100
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
References: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
Message-ID: <20071218035827.GA5517@huesing.name>

Wayne Aldo Gavioli <wgavioli at fas.harvard.edu> [Tue, Dec 18, 2007 at 02:14:23AM CET]:
> Is there anyway to indicate the frequency of such points
> on such a graph?  Should I be using a different command than "plot"?

?sunflowerplot

-- 
Johannes H?sing               There is something fascinating about science. 
                              One gets such wholesale returns of conjecture 
mailto:johannes at huesing.name  from such a trifling investment of fact.                
http://derwisch.wikidot.com         (Mark Twain, "Life on the Mississippi")


From m_olshansky at yahoo.com  Tue Dec 18 05:06:35 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Mon, 17 Dec 2007 20:06:35 -0800 (PST)
Subject: [R] read.table() and precision?
In-Reply-To: <6d32d7830712170934m4f9aa742je29da95b7eaab869@mail.gmail.com>
Message-ID: <366618.15295.qm@web32208.mail.mud.yahoo.com>

If x is the result of your read.table, it is a double
precision number (matrix, data.frame, etc.), but by
default only up to 7 decimal digits of x are printed,
so you do not see the rest of x. 
Try for example
options(digits=15) 
and see how your x look then.

--- Wojciech Gryc <wojciech at gmail.com> wrote:

> Hi,
> 
> I'm currently working with data that has values as
> large as 99,000,000
> but is accurate to 6 decimal places. Unfortunately,
> when I load the
> data using read.table(), it rounds everything to the
> nearest integer.
> Is there any way for me to preserve the information
> or work with
> arbitrarily large floating point numbers?
> 
> Thank you,
> Wojciech
> 
> -- 
> 
> Five Minutes to Midnight:
> Youth on human rights and current affairs
> http://www.fiveminutestomidnight.org/
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From jporzak at gmail.com  Tue Dec 18 05:26:13 2007
From: jporzak at gmail.com (Jim Porzak)
Date: Mon, 17 Dec 2007 20:26:13 -0800
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
References: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
Message-ID: <2a9c000c0712172026l4b38d69ct40b17f541adedd8b@mail.gmail.com>

Wayne,

I am fond of the bagplot (think 2D box plot) to replace scatter plots
for large N. See
http://www.wiwi.uni-bielefeld.de/~wolf/software/aplpack/ and aplpack
in CRAN.

-- 
HTH,
Jim Porzak
Responsys, Inc.
San Francisco, CA
http://www.linkedin.com/in/jimporzak

On Dec 17, 2007 5:14 PM, Wayne Aldo Gavioli <wgavioli at fas.harvard.edu> wrote:
>
>
> Hello all,
>
>
> I'm trying to graph a scatterplot of a large (5,000 x,y coordinates) of data
> with the caveat that many of the data points overlap with each other (share the
> same x AND y coordinates).  In using the usual "plot" command,
>
>
> > plot(education, xlab="etc", ylab="etc")
>
>
> it seems that the overlap of points is not shown in the graph.  Namely, there
> are 5,000 points that should be plotted, as I mentioned above, but because so
> many of the points overlap with each other exactly, only about 50-60 points are
> actually plotted on the graph.  Thus, there's no indication that Point A shares
> its coordinates with 200 other pieces of data and thus is very common while
> Point B doesn't share its coordinates with any other pieces of data and thus
> isn't common at all.  Is there anyway to indicate the frequency of such points
> on such a graph?  Should I be using a different command than "plot"?
>
>
> Thanks,
>
>
> Wayne
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From milton_ruser at yahoo.com.br  Tue Dec 18 05:36:23 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Mon, 17 Dec 2007 20:36:23 -0800 (PST)
Subject: [R] Res:  Scatterplot Showing All Points
Message-ID: <731100.79960.qm@web56005.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071217/25821fc5/attachment.pl 

From dwinsemius at comcast.net  Tue Dec 18 05:37:51 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 18 Dec 2007 04:37:51 +0000 (UTC)
Subject: [R] Analyzing Publications from Pubmed via XML
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<a695fbee0712171238g4995040x579e58f52f83376e@mail.gmail.com>
	<Xns9A09CA51DB1E4dNOTwinscomcast@80.91.229.13>
Message-ID: <Xns9A09F06AFFDDDdNOTwinscomcast@80.91.229.13>

David Winsemius <dwinsemius at comcast.net> wrote in
news:Xns9A09CA51DB1E4dNOTwinscomcast at 80.91.229.13: 

> "Armin Goralczyk" <agoralczyk at gmail.com> wrote in
> news:a695fbee0712171238g4995040x579e58f52f83376e at mail.gmail.com: 

>> I tried the above function with simple search terms and it worked
>> fine for me (also more output thanks to Martin's post) but when I use
>> search terms attributed to certain fields, i.e. with [au] or [ta], I
>> get the following error message:
>>> pm.srch()
>> 1: "laryngeal neoplasms[mh]"
>> 2:

> I am wondering if you used spaces, rather than "+"'s? If so then you
> may want your function to do more gsub-processing of the input string.

I tried my theory that one would need "+"'s instead of spaces, but 
disproved it. Spaces in the input string seems to produce acceptable 
results on my WinXP/R.2.6.1/RGui system even with more complex search 
strings. 

-- 
David Winsemius


From m_olshansky at yahoo.com  Tue Dec 18 08:02:03 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Mon, 17 Dec 2007 23:02:03 -0800 (PST)
Subject: [R] read.table() and precision?
In-Reply-To: <6d32d7830712170934m4f9aa742je29da95b7eaab869@mail.gmail.com>
Message-ID: <484073.27472.qm@web32207.mail.mud.yahoo.com>

Dear List,

Following the below question I have a question of my
own:
Suppose that I have large matrices which are produced
sequentially and must be used sequentially in the
reverse order. I do not have enough memory to store
them and so I would like to write them to disk and
then read them. This raises two questions:
1) what is the fastest (and the most economic
space-wise) way to do this?
2) functions like write, write.table, etc. write the
data the way it is printed and this may result in a
loss of accuracy. Is there any way to prevent this,
except for setting the "digits" option to a higher
value or using format prior to writing the data? Is it
possible to write binary files (similar to Fortran)?

Any suggestion will be greatly appreciated.

--- Wojciech Gryc <wojciech at gmail.com> wrote:

> Hi,
> 
> I'm currently working with data that has values as
> large as 99,000,000
> but is accurate to 6 decimal places. Unfortunately,
> when I load the
> data using read.table(), it rounds everything to the
> nearest integer.
> Is there any way for me to preserve the information
> or work with
> arbitrarily large floating point numbers?
> 
> Thank you,
> Wojciech
> 
> -- 
> 
> Five Minutes to Midnight:
> Youth on human rights and current affairs
> http://www.fiveminutestomidnight.org/
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From r.otasuke at gmail.com  Tue Dec 18 08:07:20 2007
From: r.otasuke at gmail.com (Kunio takezawa)
Date: Tue, 18 Dec 2007 16:07:20 +0900
Subject: [R] "gam()" in "gam" package
Message-ID: <4b2e15cd0712172307m1d3f2cb4hb9b7dd2a390d8521@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/23ae19de/attachment.pl 

From sabunime at gmail.com  Tue Dec 18 08:18:14 2007
From: sabunime at gmail.com (Saeed Abu Nimeh)
Date: Tue, 18 Dec 2007 01:18:14 -0600
Subject: [R] Dual Core vs Quad Core
In-Reply-To: <1197949788.9904.4.camel@sib-sblomber01d.sib.uq.edu.au>
References: <674056.74408.qm@web36304.mail.mud.yahoo.com>	<Pine.LNX.4.64.0712172011390.8627@perrin.socsci.unc.edu>
	<1197949788.9904.4.camel@sib-sblomber01d.sib.uq.edu.au>
Message-ID: <47677436.8090303@gmail.com>

I ran a bayesian simulation sometime ago and it took me 1 week to finish
on a debian box (Dell PE 2850  Dual Intel Xeon at 3.6GHz  6GB). I think it
depends on the setting of the experiment and whether the code can be
parallelized.

Simon Blomberg wrote:
> I've been running R on a quad-core using Debian Gnu/Linux since March
> this year, and I am very pleased with the performance.
> 
> Simon.
> 
> 
> On Mon, 2007-12-17 at 20:13 -0500, Andrew Perrin wrote:
>> On Mon, 17 Dec 2007, Kitty Lee wrote:
>>
>>> Dear R-users,
>>>
>>> I use R to run spatial stuff and it takes up a lot of ram. Runs can take hours or days. I am thinking of getting a new desktop. Can R take advantage of the dual-core system?
>>>
>>> I have a dual-core computer at work. But it seems that right now R is using only one processor.
>>>
>>> The new computers feature quad core with 3GB of RAM. Can R take advantage of the 4 chips? Or am I better off getting a dual core with faster processing speed per chip?
>>>
>>> Thanks! Any advice would be really appreciated!
>>>
>>> K.
>> If I have my information right, R will use dual- or quad-cores if it's 
>> doing two (or four) things at once. The second core will help a little bit 
>> insofar as whatever else your machine is doing won't interfere with the 
>> one core on which it's running, but generally things that take a single 
>> thread will remain on a single core.
>>
>> As for RAM, if you're doing memory-bound work you should certainly be 
>> using a 64-bit machine and OS so you can utilize the larger memory space.
>>
>>
>> ----------------------------------------------------------------------
>> Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
>> Associate Professor of Sociology; Book Review Editor, _Social Forces_
>> University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From r.otasuke at gmail.com  Tue Dec 18 08:46:26 2007
From: r.otasuke at gmail.com (Kunio takezawa)
Date: Tue, 18 Dec 2007 16:46:26 +0900
Subject: [R] R-users
Message-ID: <4b2e15cd0712172346j190df99co9f55de28085eb55f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/0f0e7617/attachment.pl 

From jim at bitwrit.com.au  Tue Dec 18 09:10:22 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 18 Dec 2007 19:10:22 +1100
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
References: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
Message-ID: <4767806E.5030305@bitwrit.com.au>

Wayne Aldo Gavioli wrote:
> 
> Hello all,
> 
> 
> I'm trying to graph a scatterplot of a large (5,000 x,y coordinates) of data
> with the caveat that many of the data points overlap with each other (share the
> same x AND y coordinates).  In using the usual "plot" command,
> 
> 
> 
>>plot(education, xlab="etc", ylab="etc")
> 
> 
> 
> it seems that the overlap of points is not shown in the graph.  Namely, there
> are 5,000 points that should be plotted, as I mentioned above, but because so
> many of the points overlap with each other exactly, only about 50-60 points are
> actually plotted on the graph.  Thus, there's no indication that Point A shares
> its coordinates with 200 other pieces of data and thus is very common while
> Point B doesn't share its coordinates with any other pieces of data and thus
> isn't common at all.  Is there anyway to indicate the frequency of such points
> on such a graph?  Should I be using a different command than "plot"?
> 
Hi Wayne,
While this is not a really pretty picture, you can get a viewable plot 
with count.overplot if the first two elements of "education" are named 
"x" and "y" and they are the coordinates you want to plot. Otherwise, 
pass the x and y coordinates separately.

library(plotrix)
count.overplot(education,
  tol=c(diff(range(education$x))/10,
  diff(range(education$y))/10))

Jim


From jari.oksanen at oulu.fi  Tue Dec 18 09:22:36 2007
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Tue, 18 Dec 2007 08:22:36 +0000 (UTC)
Subject: [R] Scatterplot Showing All Points
References: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
Message-ID: <loom.20071218T081616-231@post.gmane.org>

Wayne Aldo Gavioli <wgavioli <at> fas.harvard.edu> writes:

> 
> 
> Hello all,
> 
> I'm trying to graph a scatterplot of a large (5,000 x,y coordinates) of data
> with the caveat that many of the data points overlap with each other (share the
> same x AND y coordinates).  In using the usual "plot" command,
> 
> > plot(education, xlab="etc", ylab="etc")
> 
> it seems that the overlap of points is not shown in the graph.  Namely, there
> are 5,000 points that should be plotted, as I mentioned above, but because so
> many of the points overlap with each other exactly, only about 50-60 points are
> actually plotted on the graph.  Thus, there's no indication that Point A shares
> its coordinates with 200 other pieces of data and thus is very common while
> Point B doesn't share its coordinates with any other pieces of data and thus
> isn't common at all.  Is there anyway to indicate the frequency of such points
> on such a graph?  Should I be using a different command than "plot"?
> 
> 
One suggestion seems to be still missing: 'sunflowerplot' of base R. May look
taggy, though, if you have 200 "petals". 

Actually the documentation of sunflowerplot is wrong in botanical sense.
Sunflowers have composite flowers in capitula, and the things called 'petals' in
documentation are ligulate, sterile ray-florets (each with vestigial petals
which are not easily visible in sunflower, but in some other species you may see
three (occasionally two) teeth). 

cheers, jari oksanen


From jim at bitwrit.com.au  Tue Dec 18 09:31:52 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 18 Dec 2007 19:31:52 +1100
Subject: [R] bar plot colors
In-Reply-To: <81EA3460BED3AB4483920E05F13B12DD07D79189@WS-BCO-MSE8.milky-way.battelle.org>
References: <81EA3460BED3AB4483920E05F13B12DD07D79189@WS-BCO-MSE8.milky-way.battelle.org>
Message-ID: <47678578.8060802@bitwrit.com.au>

Winkel, David wrote:
> All,
> 
>  
> 
> I have a question regarding colors in bar plots.  I want to stack a
> total of 18 cost values in each bar. Basically, it is six cost types and
> each cost type has three components- direct, indirect, and induced
> costs.  I would like to use both solid color bars and bars with the
> slanted lines (using the density parameter).  The colors would
> distinguish cost types and the lines would distinguish
> direct/indirect/induced.  I want the cost types (i.e. colors) to be
> stacked together for each cost type.  In other words, I don't want all
> of the solid bars at the bottom and all of the slanted lines at the top.
> 
> 
> So far, I have made a bar plot with all solid colors and then tried to
> overwrite that bar plot by calling barplot() again and putting the white
> slanted lines across the bars.  However, I can't get this method to work
> while still grouping the cost types together.
> 
>  
Hi David,
This is a real challenge:

heights<-matrix(sample(10:70,54),ncol=3)
bar.colors<-rep(rep(2:7,each=3),3)
bar.densities<-rep(10,54)
bar.angles<-matrix(rep(rep(c(45,90,135),6),3),ncol=3)
barplot(heights,col=bar.colors)
barplot(heights,angle=bar.angles,add=TRUE,density=bar.densities)

Jim


From pbarros at ualg.pt  Mon Dec 17 23:54:47 2007
From: pbarros at ualg.pt (Pedro de Barros)
Date: Mon, 17 Dec 2007 22:54:47 +0000
Subject: [R] ggplot-How to define fill colours?
Message-ID: <20071218082810.3B2D5F8003@smtp3.ualg.pt>

Dear R's (most likely Hadley),

I want to build a stacked bar plot where I would like to define which 
colours will be used for each of the groups. However, I do not seem 
to find a way to do this, even if I've been looking over many places.

I have tried several variations, and my final try was this code, but 
I still do not manage to get the colours as I pre-define. Any hints 
about how to get this?

Thanks in advance,
Pedro
============================================
my code:
 >plotdata1<-data.frame(x=rep(factor(1:4),4), y=rep(0.1*(1:4),4), 
+group=as.character(rep(c('white', 'red', 'blue', 'green'),rep(4,4))))
 >plot0<-ggplot()
 >plot3<-plot0+layer(data=plotdata1, mapping=aes_string(x='x',y='y', 
+fill='group'),geom='bar', stat='identity', position='stack')
 >print(plot3)


From dlavecchia at tiscali.it  Mon Dec 17 17:02:44 2007
From: dlavecchia at tiscali.it (dlavecchia at tiscali.it)
Date: Mon, 17 Dec 2007 17:02:44 +0100 (CET)
Subject: [R] integration
Message-ID: <272297.1197907364917.JavaMail.root@ps11>

Dear All,
I need to perform a numerical integration of one dimensional 
fucntions. The extrems of integration are both finite and the functions 
I'm working on are quite complicated. I have already tried both area() 
and integrate(), but they do not perform well: area() is very slow and 
integrate() does not converge. Are in R other functions for numerical 
integration of one dimentional functions?

Thanks in advance 

Davide





____________________________________________________________
Tiscali.Fax: il tuo fax online in promo fino al 31 dicembre, 
paghi 15? e ricarichi 20? 


From g.barbati at yahoo.com  Tue Dec 18 09:34:43 2007
From: g.barbati at yahoo.com (Giulia Barbati)
Date: Tue, 18 Dec 2007 00:34:43 -0800 (PST)
Subject: [R] hazard ratio of interaction Cox model
Message-ID: <74404.16205.qm@web45614.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/fe9ce8c1/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Tue Dec 18 09:57:53 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 18 Dec 2007 09:57:53 +0100
Subject: [R] R-users
In-Reply-To: <4b2e15cd0712172346j190df99co9f55de28085eb55f@mail.gmail.com>
References: <4b2e15cd0712172346j190df99co9f55de28085eb55f@mail.gmail.com>
Message-ID: <47678B91.5000705@biostat.ku.dk>

Kunio takezawa wrote:
> R-users
> E-mail: r-help at r-project.org
>
>    I have a quenstion on "gam()" in "gam" package.
> The help of gam() says:
>       'gam' uses the _backfitting
>      algorithm_ to combine different smoothing or fitting methods.
>
> On the other hand,  lm.wfit(), which is a routine of gam.fit() contains:
>
>         z <- .Fortran("dqrls", qr = x * wts, n = n, p = p, y = y *
>         wts, ny = ny, tol = as.double(tol), coefficients = mat.or.vec(p,
>         ny), residuals = y, effects = mat.or.vec(n, ny), rank = integer(1),
>         pivot = 1:p, qraux = double(p), work = double(2 * p),
>         PACKAGE = "base")
> It may indicate that QR decomposition is used to derive an additive model
> instead of backfitting.
>    I am wondering if my guess is correct, or this "the _backfitting
> algorithm"
> has another meaning.
>   
Please don't ask the same question multiple times!

And no, backfitting and QR are unrelated concepts. You need to read up
on the theory, there are two fundamental books: Hastie & Tibshirani (gam
package) and Simon Wood (mgcv package). Both are a bit much to ask to
have summarized in email.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From maura.monville at gmail.com  Tue Dec 18 11:00:13 2007
From: maura.monville at gmail.com (Maura E Monville)
Date: Tue, 18 Dec 2007 04:00:13 -0600
Subject: [R] R command "leap"
Message-ID: <36d691950712180200q75a98becyf04b762bdb9fc9ee@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/36b796b9/attachment.pl 

From hoffmann at giub.uni-bonn.de  Tue Dec 18 11:06:06 2007
From: hoffmann at giub.uni-bonn.de (Thomas Hoffmann)
Date: Tue, 18 Dec 2007 11:06:06 +0100
Subject: [R] axis names in triangle.plot
Message-ID: <47679B8E.2090005@giub.uni-bonn.de>

Hi folks,

I am using a data.frame with sediment grain sizes:

 > grain
     sand  silt  clay
OAT 10.03 56.77 18.25
OAT 10.40 57.40 17.94
WG1 50.03 20.68 12.57
WG1 43.20 25.69 13.41
WG1 33.89 31.10 14.48
WG2  2.84 62.81 20.79
WG2  2.79 60.46 19.16
WG2 16.27 33.04  6.48
WG2  1.39 57.90  9.13
WG3  4.54 52.91 17.20
WG3  5.20 50.55 15.65
WG3  7.71 49.13 10.80
WG3  4.43 50.03 11.83
WG3  1.72 57.53 14.20
WG3  1.51 58.99 13.96

I would like to do a trinagle.plot with labeled axis-names "sand" "silt" 
and "clay". However using the command:

tringle.plot(grain)

from ade4-apckage) does not plot the axis names and there is no paramter 
to set the labels like "xlab" in the plot command. Does anybody has a 
good advice?

Thanks in advance
Thomas


From ripley at stats.ox.ac.uk  Tue Dec 18 11:20:24 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Dec 2007 10:20:24 +0000 (GMT)
Subject: [R] read.table() and precision?
In-Reply-To: <484073.27472.qm@web32207.mail.mud.yahoo.com>
References: <484073.27472.qm@web32207.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0712181002150.8324@gannet.stats.ox.ac.uk>

On Mon, 17 Dec 2007, Moshe Olshansky wrote:

> Dear List,
>
> Following the below question I have a question of my
> own:
> Suppose that I have large matrices which are produced
> sequentially and must be used sequentially in the
> reverse order. I do not have enough memory to store
> them and so I would like to write them to disk and
> then read them. This raises two questions:
> 1) what is the fastest (and the most economic
> space-wise) way to do this?

Using save/load is the simplest.  Don't worry about finding better 
solutions until you know those are not good enough.  (serialize / 
unserialize is another interface to the same underlying idea.)

> 2) functions like write, write.table, etc. write the
> data the way it is printed and this may result in a
> loss of accuracy. Is there any way to prevent this,
> except for setting the "digits" option to a higher
> value or using format prior to writing the data?

Do please read the help before making false claims. ?write.table says

      Real and complex numbers are written to the maximal possible
      precision.

OTOH, ?write says it is a wrapper for cat, whose help says

      'cat' converts numeric/complex elements in the same way as 'print'
      (and not in the same way as 'as.character' which is used by the S
      equivalent), so 'options' '"digits"' and '"scipen"' are relevant.
      However, it uses the minimum field width necessary for each
      element, rather than the same field width for all elements.

so this hints as.character() might be a useful preprocessor.

> Is it possible to write binary files (similar to Fortran)?

See ?writeBin.  save/load by default write binary files, but use of 
writeBin can be faster (and less flexible).

> Any suggestion will be greatly appreciated.

Somehow you have missed a great deal of information about R I/O.
Try help.start() and reading the sections the search engine shows you 
that look relevant.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From S.Ellison at lgc.co.uk  Tue Dec 18 11:19:22 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Tue, 18 Dec 2007 10:19:22 +0000
Subject: [R] Res:  Scatterplot Showing All Points
Message-ID: <s7679eba.049@tedmail.lgc.co.uk>

?jitter is simpler:

x<-rep(1:10,10)
y<-x
plot(x,y)     	#100 points, only 10 show
plot(jitter(x),jitter(y))	#overlap removed.



>>> Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> 18/12/2007 04:36
>>>
Hi Wayne,

I have two suggestion to you.
1. You add some random noise on both x and y data or
2. You graph bubble points, where the size is proportional to the
frequence of the xy combination.

x<-sample(1:10,10000,replace=T)
y<-sample(1:10,10000,replace=T)
xy<-cbind(x,y)
x11(1400,800)
par(mfrow=c(1,3))
plot(xy)
xy.random<-xy+rnorm(20000,0.1,0.1) 
plot(xy.random,cex=0.1)
xy.tab<-data.frame(table(x,y))
xy.tab$x<-as.numeric(as.character(xy.tab$x))
xy.tab$y<-as.numeric(as.character(xy.tab$y))
min.freq<-min(xy.tab$Freq)
max.freq<-max(xy.tab$Freq)
plot(xy.tab$x,xy.tab$y,cex=(xy.tab$Freq-min.freq)/(max.freq-min.freq)*5)

Kind regards,

Miltinho
Brazil


----- Mensagem original ----
De: Wayne Aldo Gavioli <wgavioli at fas.harvard.edu>
Para: r-help at r-project.org
Enviadas: Segunda-feira, 17 de Dezembro de 2007 22:14:23
Assunto: [R] Scatterplot Showing All Points



Hello all,


I'm trying to graph a scatterplot of a large (5,000 x,y coordinates) of
data
with the caveat that many of the data points overlap with each other
(share the
same x AND y coordinates).  In using the usual "plot" command,


> plot(education, xlab="etc", ylab="etc")


it seems that the overlap of points is not shown in the graph.  Namely,
there
are 5,000 points that should be plotted, as I mentioned above, but
because so
many of the points overlap with each other exactly, only about 50-60
points are
actually plotted on the graph.  Thus, there's no indication that Point
A shares
its coordinates with 200 other pieces of data and thus is very common
while
Point B doesn't share its coordinates with any other pieces of data and
thus
isn't common at all.  Is there anyway to indicate the frequency of such
points
on such a graph?  Should I be using a different command than "plot"?


Thanks,


Wayne

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.



 para armazenamento!

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From dray at biomserv.univ-lyon1.fr  Tue Dec 18 11:31:07 2007
From: dray at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?St=E9phane_Dray?=)
Date: Tue, 18 Dec 2007 11:31:07 +0100
Subject: [R] axis names in triangle.plot
In-Reply-To: <47679B8E.2090005@giub.uni-bonn.de>
References: <47679B8E.2090005@giub.uni-bonn.de>
Message-ID: <4767A16B.8070606@biomserv.univ-lyon1.fr>

Hi Thomas,
This looks quite strange. By default, the function use the column names 
as labels.
What is grain ? A data.frame ? Why have you duplicated row.names? Please 
return the results of class(grain) and names(grain)

Cheers,
PS: If you have questions related to ade4, you can use the adelist 
(http://listes.univ-lyon1.fr/wws/info/adelist)


Thomas Hoffmann wrote:
> Hi folks,
>
> I am using a data.frame with sediment grain sizes:
>
>  > grain
>      sand  silt  clay
> OAT 10.03 56.77 18.25
> OAT 10.40 57.40 17.94
> WG1 50.03 20.68 12.57
> WG1 43.20 25.69 13.41
> WG1 33.89 31.10 14.48
> WG2  2.84 62.81 20.79
> WG2  2.79 60.46 19.16
> WG2 16.27 33.04  6.48
> WG2  1.39 57.90  9.13
> WG3  4.54 52.91 17.20
> WG3  5.20 50.55 15.65
> WG3  7.71 49.13 10.80
> WG3  4.43 50.03 11.83
> WG3  1.72 57.53 14.20
> WG3  1.51 58.99 13.96
>
> I would like to do a trinagle.plot with labeled axis-names "sand" "silt" 
> and "clay". However using the command:
>
> tringle.plot(grain)
>
> from ade4-apckage) does not plot the axis names and there is no paramter 
> to set the labels like "xlab" in the plot command. Does anybody has a 
> good advice?
>
> Thanks in advance
> Thomas
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>   


-- 
St?phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://biomserv.univ-lyon1.fr/~dray/


From Thierry.ONKELINX at inbo.be  Tue Dec 18 11:35:31 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 18 Dec 2007 11:35:31 +0100
Subject: [R] ggplot-How to define fill colours?
In-Reply-To: <20071218082810.3B2D5F8003@smtp3.ualg.pt>
References: <20071218082810.3B2D5F8003@smtp3.ualg.pt>
Message-ID: <2E9C414912813E4EB981326983E0A1040404BCFC@inboexch.inbo.be>

Pedro,

I've had a similar problem. See this post for the solution:
http://thread.gmane.org/gmane.comp.lang.r.general/100649/focus=100673

Cheers,

Thierry 


------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be 
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: r-help-bounces op r-project.org [mailto:r-help-bounces op r-project.org]
Namens Pedro de Barros
Verzonden: maandag 17 december 2007 23:55
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] ggplot-How to define fill colours?
Urgentie: Hoog

Dear R's (most likely Hadley),

I want to build a stacked bar plot where I would like to define which 
colours will be used for each of the groups. However, I do not seem 
to find a way to do this, even if I've been looking over many places.

I have tried several variations, and my final try was this code, but 
I still do not manage to get the colours as I pre-define. Any hints 
about how to get this?

Thanks in advance,
Pedro
============================================
my code:
 >plotdata1<-data.frame(x=rep(factor(1:4),4), y=rep(0.1*(1:4),4), 
+group=as.character(rep(c('white', 'red', 'blue', 'green'),rep(4,4))))
 >plot0<-ggplot()
 >plot3<-plot0+layer(data=plotdata1, mapping=aes_string(x='x',y='y', 
+fill='group'),geom='bar', stat='identity', position='stack')
 >print(plot3)

______________________________________________
R-help op r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From S.Ellison at lgc.co.uk  Tue Dec 18 11:38:57 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Tue, 18 Dec 2007 10:38:57 +0000
Subject: [R] Dual Core vs Quad Core
Message-ID: <s767a34e.042@tedmail.lgc.co.uk>

Hiding in the windows faq is the observation that "R's computation is
single-threaded, and so it cannot use more than one CPU". So multi-core
should make no difference other than allowing R to run with less
interruption from other tasks. That is often a significant advantage,
though.




>>> Andrew Perrin <clists at perrin.socsci.unc.edu> 18/12/2007 01:13 >>>
On Mon, 17 Dec 2007, Kitty Lee wrote:

> Dear R-users,
>
> I use R to run spatial stuff and it takes up a lot of ram. Runs can
take hours or days. I am thinking of getting a new desktop. Can R take
advantage of the dual-core system?
>
> I have a dual-core computer at work. But it seems that right now R is
using only one processor.
>
> The new computers feature quad core with 3GB of RAM. Can R take
advantage of the 4 chips? Or am I better off getting a dual core with
faster processing speed per chip?
>
> Thanks! Any advice would be really appreciated!
>
> K.

If I have my information right, R will use dual- or quad-cores if it's

doing two (or four) things at once. The second core will help a little
bit 
insofar as whatever else your machine is doing won't interfere with the

one core on which it's running, but generally things that take a single

thread will remain on a single core.

As for RAM, if you're doing memory-bound work you should certainly be 
using a 64-bit machine and OS so you can utilize the larger memory
space.


----------------------------------------------------------------------
Andrew J Perrin - andrew_perrin (at) unc.edu -
http://perrin.socsci.unc.edu 
Associate Professor of Sociology; Book Review Editor, _Social Forces_
University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From Heather.Turner at warwick.ac.uk  Tue Dec 18 11:40:34 2007
From: Heather.Turner at warwick.ac.uk (Turner, Heather)
Date: Tue, 18 Dec 2007 10:40:34 -0000
Subject: [R] gene shaving method
In-Reply-To: <6.1.2.0.2.20071217134901.031c9e98@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20071217134901.031c9e98@aiminy.mail.iastate.edu>
Message-ID: <1072002710EB6047A212400EEB65F006AE78EA@ELDER.ads.warwick.ac.uk>

Gene shaving is implemented in the GeneClust package for R which you can
download from
http://odin.mdacc.tmc.edu/~kim/geneclust/
For more details see "The Analysis of Gene Expression Data: Methods and
Software" edited by Giovanni Parmiagiani, Elizabeth S. Garett, Rafael A.
Irizarry and Scott L. Zeger (you can get a preview on Google Book Search
http://books.google.com/books?id=r9gROQvdelcC&pg=PA352&lpg=PA352&dq=gene
clust&source=web&ots=3FO3jQlfrp&sig=BeOgUK2cgfuv7d12vWsvpRXOkBU#PPA353,M
1)

Best wishes,

Heather

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Aimin Yan
Sent: 17 December 2007 19:51
To: r-help at r-project.org
Subject: [R] gene shaving method

Does anyone know if Hastie's gene shaving method is implemented in R

Thanks,

Aimin

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From born.to.b.wyld at gmail.com  Tue Dec 18 12:01:41 2007
From: born.to.b.wyld at gmail.com (born.to.b.wyld at gmail.com)
Date: Tue, 18 Dec 2007 05:01:41 -0600
Subject: [R] accessing dimension names
Message-ID: <da3995450712180301s4ffd0a33u635bb6f85f0f84db@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/8ed65f6e/attachment.pl 

From s.wood at bath.ac.uk  Tue Dec 18 11:54:50 2007
From: s.wood at bath.ac.uk (Simon Wood)
Date: Tue, 18 Dec 2007 10:54:50 +0000
Subject: [R] Two repeated warnings when running gam(mgcv) to analyse my
	dataset?
In-Reply-To: <2fc17e30712170353j5837068bx8fecb645978df2d1@mail.gmail.com>
References: <2fc17e30712130946x7c553885m6c0640a0f8c47d5c@mail.gmail.com>
	<200712170854.29191.s.wood@bath.ac.uk>
	<2fc17e30712170353j5837068bx8fecb645978df2d1@mail.gmail.com>
Message-ID: <200712181054.50986.s.wood@bath.ac.uk>

The model here is just a penalised GLM, and the warnings relate to the GLM 
fitting process. Fitted probabilities of 0 or 1 can be perfectly appropriate, 
but do indicate that the linear predictor is not really uniquely defined, and 
that some care may be needed in interpreting results (for example, if the 
fitted probabilities are zero or one, then a CI for the corresponding linear 
predictor will depend more on the prior assumptions about smoothness than 
anything else). This problem is not really GAM specific, it relates to any 
`logistic regression' model. 

Similarly, the GLM fitting IRLS iterations are not guaranteed to converge, and 
can fail, especially for overly flexible logistic regression models. Try 
this, for example....

x <- 1:10
y <- c(0,0,0,0,0,1,1,1,1,1)
glm(y~x,family=binomial)

I get...
...
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = 
etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = 
etastart,  :
  fitted probabilities numerically 0 or 1 occurred

...as models become more complex the scope for this sort of thing to happen 
increases, and some simplification may be appropriate. 

That said, mgcv::gam fitting with all smoothing parameters fixed, is slightly 
more likely to fail in this way than `glm' or `mgcv::gam' with some smoothing 
parameters  estimated, because of the steps taken to stabilise divergent fit 
iterations. When all smoothing parameters are fixed, mgcv uses older fitting 
routines that don't try as hard to stabilise a divergent fit as the newer 
fitting routines. This is a bit of an anomaly and I'll try and fix it for a 
future release. 

best,
Simon



On Monday 17 December 2007 11:53, zhijie zhang wrote:
> Dear Simon,
> Sorry for an incomplete listing of the question.
> #mgcv version is  1.3-29, R 2.6.1, windows XP
> #m.gam<-gam(mark~s(x)+s(y)+s(lstday2004)+s(ndvi2004)+s(slope)+s(elevation)+
>disbinary,family=binomial(logit),data=point) The above program's the core
> codes in my following loop programs.
>  It works well if i run the above codes only one time for my dataset, but
> warnings will occur if i run many times for the following loop.
>
> > while (j<1001) {
>
> +  index=sample(ID, replace=F)
> +  m.data$x=coords[index,]$x
> +  m.data$y=coords[index,]$y
> +  # For each permutation, we run the GAM using the optimal span for the
> above model m.gam
> +  s.gam
> <-gam(mark~s(x)+s(y)+s(lstday2004)+s(ndvi2004)+s(slope)+s(elevation)+disbin
>ary,,sp=c( 5.582647e-07,4.016504e-02,2.300424e-04,1.274065e+03,9.558236e-09,
> 1.868827e-08),family=binomial(logit),data=m.data)
> +  permresults[,i]=predict.gam(s.gam)
> +  i=i+1
> +  if (j%%100==0) print(i)
> +  j=j+1
> +  }
> [1] 101
> [1] 201
> [1] 301
> [1] 401
> [1] 501
> [1] 601
> [1] 701
> [1] 801
> [1] 901
> [1] 1001
> warnings() over 50
>
> > warnings()
>
> 1: In gam.fit(G, family = G$family, control = control, gamma = gamma,  ...
> : fitted probabilities numerically 0 or 1 occurred
> ......................................
> 14: In gam.fit(G, family = G$family, control = control, gamma = gamma,  ...
>
>   Algorithm did not converge
> ..........................
>
> On Dec 17, 2007 4:54 PM, Simon Wood <s.wood at bath.ac.uk> wrote:
> > What mgcv version are you running (and on what platform)?
> >
> > n Thursday 13 December 2007 17:46, zhijie zhang wrote:
> > > Dear all,
> > >  I run the GAMs (generalized additive models) in gam(mgcv) using the
> > > following codes.
> > >
> > > m.gam
> >
> > <-gam(mark~s(x)+s(y)+s(lstday2004)+s(ndvi2004)+s(slope)+s(elevation)+disb
> >in
> >
> > >ary,family=binomial(logit),data=point)
> > >
> > >  And two repeated warnings appeared.
> > > Warnings?
> > > 1: In gam.fit(G, family = G$family, control = control, gamma = gamma,
> >
> >  ...
> >
> > > : Algorithm did not converge
> > >
> > > 2: In gam.fit(G, family = G$family, control = control, gamma = gamma,
> >
> >  ...
> >
> > > : fitted probabilities numerically 0 or 1 occurred
> > >
> > > Q1: For warning1, could it be solved by changing the value of
> > > mgcv.toloptions for
> > > gam.control(mgcv.tol=1e-7)?
> > >
> > > Q1: For warning2, is there any impact for the results if the "fitted
> > > probabilities numerically 0 or 1 occurred" ?  How can i solve it?
> > >
> > >  I didn't try the possible solutions for them, because it took such a
> > > longer time to run the whole programs.
> > >  Could anybody suggest their solutions?
> > >  Any help or suggestions are greatly appreciated.
> > >   Thanks.
> >
> > --
> >
> > > Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> > > +44 1225 386603  www.maths.bath.ac.uk/~sw283

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283 


From cgenolin at u-paris10.fr  Tue Dec 18 12:20:12 2007
From: cgenolin at u-paris10.fr (cgenolin at u-paris10.fr)
Date: Tue, 18 Dec 2007 12:20:12 +0100
Subject: [R] clean programming
In-Reply-To: <971536df0712160641q35edc1d9v2b1f1b4833bf88f5@mail.gmail.com>
References: <mailman.15.1197802803.31564.r-help@r-project.org>
	<20071216152520.qo8jnpahvk0k0w4k@icare.u-paris10.fr>
	<971536df0712160641q35edc1d9v2b1f1b4833bf88f5@mail.gmail.com>
Message-ID: <20071218122012.cbp2ex0rkg8s840k@icare.u-paris10.fr>

Gabor Grothendieck <ggrothendieck at gmail.com> a ??crit? :

> Its a FAQ

Oups... Sorry for that.
Just to close the topic :

cleanProg <- function(name,tolerance){
if(length(findGlobals(get(name),FALSE)$variables) > tolerance){
   cat("More than",tolerance,"global variable(s) in ",name,"\a\n")
}
}
cleanProg(fun,0)

----------------------------------------------------------------
Ce message a ete envoye par IMP, grace a l'Universite Paris 10 Nanterre


From ripley at stats.ox.ac.uk  Tue Dec 18 12:29:17 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Dec 2007 11:29:17 +0000 (GMT)
Subject: [R] Dual Core vs Quad Core
In-Reply-To: <s767a34e.042@tedmail.lgc.co.uk>
References: <s767a34e.042@tedmail.lgc.co.uk>
Message-ID: <Pine.LNX.4.64.0712181049380.9580@gannet.stats.ox.ac.uk>

On Tue, 18 Dec 2007, S Ellison wrote:

> Hiding in the windows faq is the observation that "R's computation is
> single-threaded, and so it cannot use more than one CPU". So multi-core
> should make no difference other than allowing R to run with less
> interruption from other tasks. That is often a significant advantage,
> though.

Yes, but that is Windows-specific.

On most other platforms you can benefit from using a multi-threaded BLAS, 
such as ATLAS, ACML or Dr Goto's.  The speedup for linear algebra can be 
substantial (although sometimes it will slow things down).  Luke Tierney 
has an experimental package to make use of parallel threads for some basic 
R computations which may appear in R 2.7.0.

It should be possible to use a multi-threaded BLAS under Windows, but I 
know no one who has done it.  There is a viable pthreads implementation 
for Windows, and I've tested Luke's experimental package using it.

Some compilers' runtimes will be able to use parallel threads for other 
tasks.  Since all the examples I am aware of are expensive commercial 
compilers, I suspect R will make limited use of them.  (In particular, 
base R does not use the Fortran 9x vector operations at which many of 
these features are targeted: we probably would if we routinely used such 
compilers.)

I've had dual-CPU desktops for more than ten years.  Given how little 
speedup you are likely to get via parallel processing (only under ideal 
conditions do the optimized BLASes run >1.5x faster using two CPUs), the 
most effective way to make use of multiple CPUs has been to run multiple 
jobs: I typically run 3-4 at once to keep the CPUs fully used.

One way to run multiple R processes to cooperate on a single task is to 
use a package such as snow to distribute the load.



>>>> Andrew Perrin <clists at perrin.socsci.unc.edu> 18/12/2007 01:13 >>>
> On Mon, 17 Dec 2007, Kitty Lee wrote:
>
>> Dear R-users,
>>
>> I use R to run spatial stuff and it takes up a lot of ram. Runs can
> take hours or days. I am thinking of getting a new desktop. Can R take
> advantage of the dual-core system?
>>
>> I have a dual-core computer at work. But it seems that right now R is
> using only one processor.
>>
>> The new computers feature quad core with 3GB of RAM. Can R take
> advantage of the 4 chips? Or am I better off getting a dual core with
> faster processing speed per chip?
>>
>> Thanks! Any advice would be really appreciated!
>>
>> K.
>
> If I have my information right, R will use dual- or quad-cores if it's
> doing two (or four) things at once. The second core will help a little
> bit
> insofar as whatever else your machine is doing won't interfere with the
> one core on which it's running, but generally things that take a single
> thread will remain on a single core.
>
> As for RAM, if you're doing memory-bound work you should certainly be
> using a 64-bit machine and OS so you can utilize the larger memory
> space.

They only have 3GB of RAM, which 32-bit OSes can address.  The benefits 
really come with more than that.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From hoffmann at giub.uni-bonn.de  Tue Dec 18 12:48:48 2007
From: hoffmann at giub.uni-bonn.de (Thomas Hoffmann)
Date: Tue, 18 Dec 2007 12:48:48 +0100
Subject: [R] axis names in triangle.plot
In-Reply-To: <4767A16B.8070606@biomserv.univ-lyon1.fr>
References: <47679B8E.2090005@giub.uni-bonn.de>
	<4767A16B.8070606@biomserv.univ-lyon1.fr>
Message-ID: <4767B3A0.1030002@giub.uni-bonn.de>

Thanks for this advice.

grain was not a data.frame but a matrix. Now it works:-)

Cheers
Thomas


St?phane Dray schrieb:
> Hi Thomas,
> This looks quite strange. By default, the function use the column 
> names as labels.
> What is grain ? A data.frame ? Why have you duplicated row.names? 
> Please return the results of class(grain) and names(grain)
>
> Cheers,
> PS: If you have questions related to ade4, you can use the adelist 
> (http://listes.univ-lyon1.fr/wws/info/adelist)
>
>
> Thomas Hoffmann wrote:
>> Hi folks,
>>
>> I am using a data.frame with sediment grain sizes:
>>
>>  > grain
>>      sand  silt  clay
>> OAT 10.03 56.77 18.25
>> OAT 10.40 57.40 17.94
>> WG1 50.03 20.68 12.57
>> WG1 43.20 25.69 13.41
>> WG1 33.89 31.10 14.48
>> WG2  2.84 62.81 20.79
>> WG2  2.79 60.46 19.16
>> WG2 16.27 33.04  6.48
>> WG2  1.39 57.90  9.13
>> WG3  4.54 52.91 17.20
>> WG3  5.20 50.55 15.65
>> WG3  7.71 49.13 10.80
>> WG3  4.43 50.03 11.83
>> WG3  1.72 57.53 14.20
>> WG3  1.51 58.99 13.96
>>
>> I would like to do a trinagle.plot with labeled axis-names "sand" 
>> "silt" and "clay". However using the command:
>>
>> tringle.plot(grain)
>>
>> from ade4-apckage) does not plot the axis names and there is no 
>> paramter to set the labels like "xlab" in the plot command. Does 
>> anybody has a good advice?
>>
>> Thanks in advance
>> Thomas
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>   
>
>


From pbarros at ualg.pt  Tue Dec 18 12:53:22 2007
From: pbarros at ualg.pt (Pedro de Barros)
Date: Tue, 18 Dec 2007 11:53:22 +0000
Subject: [R] ggplot-How to define fill colours?
In-Reply-To: <2E9C414912813E4EB981326983E0A1040404BCFC@inboexch.inbo.be>
References: <20071218082810.3B2D5F8003@smtp3.ualg.pt>
	<2E9C414912813E4EB981326983E0A1040404BCFC@inboexch.inbo.be>
Message-ID: <20071218115322.E6EA0F800F@smtp3.ualg.pt>

Thanks Thierry,
Rigth on target.

Cheers,
Pedro
At 10:35 2007/12/18, you wrote:
>Pedro,
>
>I've had a similar problem. See this post for the solution:
>http://thread.gmane.org/gmane.comp.lang.r.general/100649/focus=100673
>
>Cheers,
>
>Thierry
>
>
>------------------------------------------------------------------------
>----
>ir. Thierry Onkelinx
>Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>and Forest
>Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
>methodology and quality assurance
>Gaverstraat 4
>9500 Geraardsbergen
>Belgium
>tel. + 32 54/436 185
>Thierry.Onkelinx at inbo.be
>www.inbo.be
>
>Do not put your faith in what statistics say until you have carefully
>considered what they do not say.  ~William W. Watt
>A statistical analysis, properly conducted, is a delicate dissection of
>uncertainties, a surgery of suppositions. ~M.J.Moroney
>
>-----Oorspronkelijk bericht-----
>Van: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
>Namens Pedro de Barros
>Verzonden: maandag 17 december 2007 23:55
>Aan: r-help at stat.math.ethz.ch
>Onderwerp: [R] ggplot-How to define fill colours?
>Urgentie: Hoog
>
>Dear R's (most likely Hadley),
>
>I want to build a stacked bar plot where I would like to define which
>colours will be used for each of the groups. However, I do not seem
>to find a way to do this, even if I've been looking over many places.
>
>I have tried several variations, and my final try was this code, but
>I still do not manage to get the colours as I pre-define. Any hints
>about how to get this?
>
>Thanks in advance,
>Pedro
>============================================
>my code:
>  >plotdata1<-data.frame(x=rep(factor(1:4),4), y=rep(0.1*(1:4),4),
>+group=as.character(rep(c('white', 'red', 'blue', 'green'),rep(4,4))))
>  >plot0<-ggplot()
>  >plot3<-plot0+layer(data=plotdata1, mapping=aes_string(x='x',y='y',
>+fill='group'),geom='bar', stat='identity', position='stack')
>  >print(plot3)
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Tue Dec 18 13:00:29 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 18 Dec 2007 07:00:29 -0500
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <loom.20071218T081616-231@post.gmane.org>
References: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
	<loom.20071218T081616-231@post.gmane.org>
Message-ID: <4767B65D.5030602@stats.uwo.ca>

Jari Oksanen wrote:
> Wayne Aldo Gavioli <wgavioli <at> fas.harvard.edu> writes:
>
>   
>> Hello all,
>>
>> I'm trying to graph a scatterplot of a large (5,000 x,y coordinates) of data
>> with the caveat that many of the data points overlap with each other (share the
>> same x AND y coordinates).  In using the usual "plot" command,
>>
>>     
>>> plot(education, xlab="etc", ylab="etc")
>>>       
>> it seems that the overlap of points is not shown in the graph.  Namely, there
>> are 5,000 points that should be plotted, as I mentioned above, but because so
>> many of the points overlap with each other exactly, only about 50-60 points are
>> actually plotted on the graph.  Thus, there's no indication that Point A shares
>> its coordinates with 200 other pieces of data and thus is very common while
>> Point B doesn't share its coordinates with any other pieces of data and thus
>> isn't common at all.  Is there anyway to indicate the frequency of such points
>> on such a graph?  Should I be using a different command than "plot"?
>>
>>
>>     
> One suggestion seems to be still missing: 'sunflowerplot' of base R. May look
> taggy, though, if you have 200 "petals". 
>
> Actually the documentation of sunflowerplot is wrong in botanical sense.
> Sunflowers have composite flowers in capitula, and the things called 'petals' in
> documentation are ligulate, sterile ray-florets (each with vestigial petals
> which are not easily visible in sunflower, but in some other species you may see
> three (occasionally two) teeth). 
>   
Could you please put together a patch that replaces "petals" with 
"ligulate, sterile ray-florets" in appropriate places?

;-)

Duncan Murdoch
> cheers, jari oksanen
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From petr.pikal at precheza.cz  Tue Dec 18 13:26:19 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Tue, 18 Dec 2007 13:26:19 +0100
Subject: [R] Odp:  accessing dimension names
In-Reply-To: <da3995450712180301s4ffd0a33u635bb6f85f0f84db@mail.gmail.com>
Message-ID: <OF0BD17FB0.B06B94E1-ONC12573B5.0044102D-C12573B5.004450A6@precheza.cz>

Hi

r-help-bounces at r-project.org napsal dne 18.12.2007 12:01:41:

> I have a matrix y:
> 
> > dimnames(y)
> $x93
> [1] "1" "2"
> 
> $x94
> [1] "0" "1" "2"
> .................. so on  (there are other dimensions as well)
> 
> 
> 
> I need to access a particular dimension, but a random mechanism tells me
> which dimension it would. So, sometimes I might need to access
> dimnames(y)$x93, some other time it would be dimnames(y)$x94.. and so 
on.
> Now let that random dimension be idx, then 
dimnames(y)$paste('x',idx,sep='')
> doesn't work.

Why not

dimnames(y)[idx]

Regards
Petr


> 
> Can anyone help?
> 
> Thanks!
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From unwin at math.uni-augsburg.de  Tue Dec 18 13:31:22 2007
From: unwin at math.uni-augsburg.de (Antony Unwin)
Date: Tue, 18 Dec 2007 13:31:22 +0100
Subject: [R] Scatterplot Showing All Points
Message-ID: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/ec6f8b88/attachment.pl 

From f.harrell at vanderbilt.edu  Tue Dec 18 13:41:18 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 18 Dec 2007 06:41:18 -0600
Subject: [R] hazard ratio of interaction Cox model
In-Reply-To: <74404.16205.qm@web45614.mail.sp1.yahoo.com>
References: <74404.16205.qm@web45614.mail.sp1.yahoo.com>
Message-ID: <4767BFEE.3030000@vanderbilt.edu>

Giulia Barbati wrote:
> Dear Forum,
> I have a question about interaction estimate in the Cox model: 
> why the hazard ratio of the interaction is not produced in the summary of the model?
> (Instead, the estimate of the coefficient is given in the print of the model.) 

The 'hazard ratio of the interaction' is not well defined.  Decide what 
hazard ratio you want to estimate, then ask summary to compute that, e.g.

summary(modINT, XX_PR=c(1,3), XX_DISF=2)

will estimate the 3:1 XX_PR hazard ratio at XX_DISF=2

Frank Harrell

> 
> 
> # Example:
> 
> modINT <-cph( Surv(T_BASE, T_FIN,STATUS)~ NYHA + ASINI + RFP + FE_REC + XX_PR*XX_DISF)
> 
> print(modINT)
> 
> 
>                   coef se(coef)     z        p
> NYHA=2       1.254    0.584  2.15 0.031767
> ASINI           0.665    0.409  1.62 0.104247
> RFP=2            0.725    0.704  1.03 0.302578
> FE_REC=2        -1.637    0.810 -2.02 0.043331
> XX_PR            2.189    0.649  3.37 0.000748
> XX_DISF          3.233    1.000  3.23 0.001222
> XX_PR * XX_DISF -2.852    1.280 -2.23 0.025852
> 
> summary(modINT)
> 
>  Effects              Response : Surv(T_BASE, T_FIN, STATUS) 
> 
>  Factor         Low    High Diff.  Effect S.E. Lower 0.95 Upper 0.95
>  ASINI         2.0725 2.85 0.7775  0.52  0.32 -0.11        1.14    
>   Hazard Ratio  2.0725 2.85 0.7775  1.68    NA  0.90        3.13    
>  XX_PR          0.0000 1.00 1.0000  2.19  0.65  0.92        3.46    
>   Hazard Ratio  0.0000 1.00 1.0000  8.92    NA  2.50       31.86    
>  XX_DISF        0.0000 1.00 1.0000  3.23  1.00  1.27        5.19    
>   Hazard Ratio  0.0000 1.00 1.0000 25.35    NA  3.57      179.88    
>  NYHA - 2:1 1.0000 2.00     NA  1.25  0.58  0.11        2.40    
>   Hazard Ratio  1.0000 2.00     NA  3.50    NA  1.12       11.00    
>  RFP - 2:1      1.0000 2.00     NA  0.73  0.70 -0.65        2.10    
>   Hazard Ratio  1.0000 2.00     NA  2.07    NA  0.52        8.20    
>  FE_REC - 2:1   1.0000 2.00     NA -1.64  0.81 -3.23       -0.05    
>   Hazard Ratio  1.0000 2.00     NA  0.19    NA  0.04        0.95    
> 
> Adjusted to: XX_PR=0 XX_DISF=0  
> 
> 
> 
> 
>       ____________________________________________________________________________________
> Be a better friend, newshound, and 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From S.Ellison at lgc.co.uk  Tue Dec 18 14:29:27 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Tue, 18 Dec 2007 13:29:27 +0000
Subject: [R] Scatterplot Showing All Points
Message-ID: <s767cb46.060@tedmail.lgc.co.uk>



>> Antony Unwin <unwin at math.uni-augsburg.de> >>
>I must admit to being very surprised that jittering and sunflower  
>plots have been suggested for a dataset of 5000 points.  Do those who 

>mentioned these methods have examples on that scale where they are  
>effective?)

You have a point. haha. 
But check the microarray literature; scatterplots have been used -
often - to display microarray data with 10000 observations at a time.
And in their defence, even on screen, a 600x600 pixel plot window holds
360000 pixels - 5000 is not a large fraction of that. Jittering has
visible effects on data at that resolution. Compare the two plots in 

library(MASS)
Sigma <- matrix(c(10,4,4,2),2,2)
xy<- round(mvrnorm(n=5000, rep(0, 2), Sigma), 1)
plot(xy,pch=".")
plot(jitter(xy, factor=2),pch=".")

But you're of course right to question how sensible this is. The best
you can get is a visual impression of the 'shape' of the data with a
greater perceived density at multiple observations which otherwise
overlapped. 

S.


From therneau at mayo.edu  Tue Dec 18 14:37:05 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Tue, 18 Dec 2007 07:37:05 -0600 (CST)
Subject: [R] Capture warning messages from coxph()
Message-ID: <200712181337.lBIDb4F05711@hsrnfs-101.mayo.edu>


Xinyi Li asked how to keep track of which coxph models, called from within a
loop, were responsible for warning messges.  One solution is to modify the
coxph code so that those models are marked in the return coxph object.  Below
is a set of changes to the final 40 lines of coxph.fit, that will cause the
component "infinite.warn" to be added to the result whenever a warning was
generated; it will be a vector of T/F showing which component(s) of the
coefficient vector generated the warning.
  Change the code for coxph.fit.s, then do
	> source('coxph.fit.revised.s')    #or whatever you called it
	> coxph <- source('coxph.s')
	> coxph.wtest <- survival:::coxph.wtest

Line 2 causes you to have a local version of coxph, otherwise, due
to name spaces, the original version of coxph.fit will still be used.  Line 3
is another consequence of name spaces.  Then code such as

	for(i in 1:ncol(x)){
	    fit=coxph(TIME~x[,i])
 	    if (!is.null(fit$infinite.warn)) cat("Waring in fit", i, "\n")
	    sfit <- summary(fit)
	    results[i,1]=sfit$coef[1]
	    results[i,2]=sfit$coef[3]
	    results[i,3]=sfit$coef[5]
 	 }
Will report out the models with a warning.  

Notes
  1. The warning message is not completely reliable

  2. Name spaces protect a package from accidental overrides, when a user or 
some other package reuses a function name.  With its hundreds of packages, they
are a necessity for R.  But sometimes you really do want to override a
function; then they are a bit of a pain.  Others with a better grasp of R
internals may be able to suggest a better override than I have done here.

	Terry Therneau

-------------------------


	infs <- abs(coxfit$u %*% var)
        keep.infs <- F                    # new line
	if (maxiter >1) {
	    if (coxfit$flag == 1000)
		   warning("Ran out of iterations and did not converge")
	    else {
		infs <- ((infs > control$eps) & 
			 infs > control$toler.inf*abs(coef))
		if (any(infs))
		warning(paste("Loglik converged before variable ",
			  paste((1:nvar)[infs],collapse=","),
			  "; beta may be infinite. "))
                keep.infs <- T          #new line
		}
	    }

	names(coef) <- dimnames(x)[[2]]
	lp <- c(x %*% coef) + offset - sum(coef*coxfit$means)
	score <- exp(lp[sorted])
	coxres <- .C("coxmart", as.integer(n),
				as.integer(method=='efron'),
				stime,
				sstat,
				newstrat,
				as.double(score),
				as.double(weights),
				resid=double(n))
	resid <- double(n)
	resid[sorted] <- coxres$resid
	names(resid) <- rownames
	coef[which.sing] <- NA

	temp <- list(coefficients  = coef,  #modified line
		    var    = var,
		    loglik = coxfit$loglik,
		    score  = coxfit$sctest,
		    iter   = coxfit$iter,
		    linear.predictors = as.vector(lp),
		    residuals = resid,
		    means = coxfit$means,
		    method='coxph')
        if (keep.infs) temp$infinite.warn <- infs   #new line
        temp				            #new line
	}
    }


From murdoch at stats.uwo.ca  Tue Dec 18 14:42:19 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 18 Dec 2007 08:42:19 -0500
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>
References: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>
Message-ID: <4767CE3B.7070603@stats.uwo.ca>

On 18/12/2007 7:31 AM, Antony Unwin wrote:
> Wayne,
> 
> Try the iplot command in iPlots.  You can then vary both the  
> pointsize and the transparency of your scatterplot interactively and  
> decide which scatterplot conveys the information best.  Sometimes  
> it's helpful to use more than one scatterplot when presenting your  
> results.
> 
> (I must admit to being very surprised that jittering and sunflower  
> plots have been suggested for a dataset of 5000 points.  Do those who  
> mentioned these methods have examples on that scale where they are  
> effective?)

Sure.  The original post said there were about 50-60 unique locations. 
This plot:

x <- rbinom(5000, 20, 0.15)
y <- rbinom(5000, 20, 0.15)
plot(x,y)

has a few more unique locations; tune those probabilities if you want it 
closer.  Due to the overlap, the distribution is very unclear.  But this 
plot

plot(jitter(x), jitter(y))

makes the distribution quite clear.

I wouldn't use the default pch if I had 50000 points, but with pch=".", 
it's not so bad even in that case.

Duncan Murdoch


From rh at family-krueger.com  Tue Dec 18 14:52:55 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Tue, 18 Dec 2007 14:52:55 +0100
Subject: [R] GLM and factor in forular
Message-ID: <4767D0B7.1050906@family-krueger.com>

I used a GLM with a factor variable and wondered about that the first 
factor is missing in the results.
means there is no result for Y1
Is it wrong to use factors in GLM or is there a statistical reason that 
there is no Y1 result ?

X<-rnorm(31:40)
Y<-factor(c(1:10))
glm(X~Y)

Knut


From petr.pikal at precheza.cz  Tue Dec 18 14:54:56 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Tue, 18 Dec 2007 14:54:56 +0100
Subject: [R] accessing dimension names
In-Reply-To: <da3995450712180525jdd43093h1818ef9639d5ae65@mail.gmail.com>
Message-ID: <OF22595B02.296DB5AB-ONC12573B5.004BC449-C12573B5.004C6DAB@precheza.cz>

Hard to help as i do not have "y" and it definitelly is not a matrix as 
you tried to pretend. 

1.      Try to look at structure of your y object by str(y)
2.      Try to learn about how to extract parts of objects e.g. by reading 
?"["
3.      Try to use what you learned on your y object
4.      If you still does not get what you want then make some example 
which can be reproduced and ask again

> mat<-matrix(rnorm(12),3,4)
> dmat<-data.frame(mat)
> dimnames(dmat)
[[1]]
[1] "1" "2" "3"

[[2]]
[1] "X1" "X2" "X3" "X4"

> dimnames(dmat)[1]
[[1]]
[1] "1" "2" "3"

> dimnames(dmat)[1][1]
[[1]]
[1] "1" "2" "3"

> dimnames(dmat)[[1]][1]
[1] "1"

Regards

Petr
petr.pikal at precheza.cz

born.to.b.wyld at gmail.com napsal dne 18.12.2007 14:25:06:

> Thanks. Actually, I need something else as well.
> 
> I need to get as.numeric(dimnames(y)$x93[1]), which in this case is 1. I 
tried
> as.numeric(dimnames(y)$paste('x',idx,sep='')[1]), and it did not work.
> 
> Please help.
> 
> 
> 

> On Dec 18, 2007 6:26 AM, Petr PIKAL <petr.pikal at precheza.cz> wrote:
> Hi
> 
> r-help-bounces at r-project.org napsal dne 18.12.2007 12:01:41:
> 
> > I have a matrix y:
> >
> > > dimnames(y)
> > $x93
> > [1] "1" "2"
> >
> > $x94
> > [1] "0" "1" "2"
> > .................. so on  (there are other dimensions as well)
> >
> >
> >
> > I need to access a particular dimension, but a random mechanism tells 
me
> > which dimension it would. So, sometimes I might need to access
> > dimnames(y)$x93, some other time it would be dimnames(y)$x94.. and so 
> on.
> > Now let that random dimension be idx, then
> dimnames(y)$paste('x',idx,sep='')
> > doesn't work.

> Why not
> 
> dimnames(y)[idx]
> 
> Regards
> Petr
> 
> 
> >
> > Can anyone help?
> >
> > Thanks!
> >
> >    [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From b.jacobs at pandora.be  Tue Dec 18 15:07:58 2007
From: b.jacobs at pandora.be (Bert Jacobs)
Date: Tue, 18 Dec 2007 15:07:58 +0100
Subject: [R] Reshape Dataframe
In-Reply-To: <OF22595B02.296DB5AB-ONC12573B5.004BC449-C12573B5.004C6DAB@precheza.cz>
Message-ID: <20071218140641.5234C67DB2@agave.telenet-ops.be>


Hi,

I'm having a bit of problems in creating a new dataframe. 
Below you'll find a description of the current dataframe and of the
dataframe that needs to be created.
Can someone help me out on this one?
Thx in advance.
Bert

Current Dataframe

Var1	Var2	Var3	Var4
A	Fa	W1	1
A	Si	W1	2
A	Fa	W2	3
A	Si	W3	4
B	Si	W1	5
C	La	W2	6
C	Do	W4	7

New Dataframe

Var1	Var2	W1	W2	W3	W4
A	Fa	1	3		
A	Si	2		4
A	La
A	Do			
B	Fa				
B	Si	5		
B	La
B	Do
C	Fa				
C	Si			
C	La		6
C	Do				7


From h.wickham at gmail.com  Tue Dec 18 15:07:27 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 18 Dec 2007 08:07:27 -0600
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <2a9c000c0712172026l4b38d69ct40b17f541adedd8b@mail.gmail.com>
References: <1197940463.47671eef4045e@webmail.fas.harvard.edu>
	<2a9c000c0712172026l4b38d69ct40b17f541adedd8b@mail.gmail.com>
Message-ID: <f8e6ff050712180607p302f12d3y7b87d449937be440@mail.gmail.com>

On 12/17/07, Jim Porzak <jporzak at gmail.com> wrote:
> Wayne,
>
> I am fond of the bagplot (think 2D box plot) to replace scatter plots
> for large N. See
> http://www.wiwi.uni-bielefeld.de/~wolf/software/aplpack/ and aplpack
> in CRAN.

The big drawback of the bagplot, like the boxplot, is that it's
difficult to see multimodality.

Hadley

-- 
http://had.co.nz/


From h.wickham at gmail.com  Tue Dec 18 15:15:46 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 18 Dec 2007 08:15:46 -0600
Subject: [R] Reshape Dataframe
In-Reply-To: <20071218140641.5234C67DB2@agave.telenet-ops.be>
References: <OF22595B02.296DB5AB-ONC12573B5.004BC449-C12573B5.004C6DAB@precheza.cz>
	<20071218140641.5234C67DB2@agave.telenet-ops.be>
Message-ID: <f8e6ff050712180615j19ddb38y33bd599e1340c9b6@mail.gmail.com>

On 12/18/07, Bert Jacobs <b.jacobs at pandora.be> wrote:
>
> Hi,
>
> I'm having a bit of problems in creating a new dataframe.
> Below you'll find a description of the current dataframe and of the
> dataframe that needs to be created.
> Can someone help me out on this one?

library(reshape)
dfm <- melt(df, id = 1:3)
cast(dfm, ... ~ Var3)

You can find out more about the reshape package at http://had.co.nz/reshape

Hadley

-- 
http://had.co.nz/


From pbarros at ualg.pt  Tue Dec 18 15:41:34 2007
From: pbarros at ualg.pt (Pedro de Barros)
Date: Tue, 18 Dec 2007 14:41:34 +0000
Subject: [R] ggplot2 - getting at the grobs
Message-ID: <20071218144135.4364EF800B@smtp3.ualg.pt>

Dear All,

I continue trying to get several of my plotting functions to use 
ggplot, because I really do like the concept of the graphical 
objects, and working with them in the abstract.
I am now trying to access the grobs to manipulate using grid. 
However, until now all I managed was to get the plot as a gTree 
object, and manipulate it as a gTree from there. The problem is that 
then it is no longer a ggplot, and thus I can no longer use the 
ggplot functions.
How to get at the grobs, without converting the ggplot into a gTree?

Thanks,
Pedro


From rvaradhan at jhmi.edu  Tue Dec 18 15:46:47 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 18 Dec 2007 09:46:47 -0500
Subject: [R] integration
In-Reply-To: <272297.1197907364917.JavaMail.root@ps11>
References: <272297.1197907364917.JavaMail.root@ps11>
Message-ID: <001401c84184$d1248860$7c94100a@win.ad.jhu.edu>

Hi Davide,

It is difficult to say what the problem is without knowing more about the nature of the integrand.  So, you should do a couple of preliminary things before attempting compute the integral.

First, is the integral is finite? You should establish this.  Second, plot the integrand over the entire interval.  Then you need to think about the following: Is the integrand unimodal, with the mass concentrated over a small region? Or is it multimodal? Does it have thick tail?

Assuming that the integral is finite, you could try a few things:
1.  Divide the interval of integration into several small intervals (say, 10 or 100), and then use integrate() on each and then add up the results.  You can make this process more efficient if you know where the mass is concentrated.
2.  Transform the integrand.
3.  Try a simple trapezoidal rule quadrature.

Ravi.

-----------------------------------------------------------------------------------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html



------------------------------------------------------------------------------------


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On Behalf Of dlavecchia at tiscali.it
Sent: Monday, December 17, 2007 11:03 AM
To: r-help at r-project.org
Subject: [R] integration

Dear All,
I need to perform a numerical integration of one dimensional 
fucntions. The extrems of integration are both finite and the functions 
I'm working on are quite complicated. I have already tried both area() 
and integrate(), but they do not perform well: area() is very slow and 
integrate() does not converge. Are in R other functions for numerical 
integration of one dimentional functions?

Thanks in advance 

Davide





____________________________________________________________
Tiscali.Fax: il tuo fax online in promo fino al 31 dicembre, 
paghi 15? e ricarichi 20? 

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Tue Dec 18 15:54:08 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 18 Dec 2007 09:54:08 -0500
Subject: [R] Reshape Dataframe
In-Reply-To: <20071218140641.5234C67DB2@agave.telenet-ops.be>
References: <OF22595B02.296DB5AB-ONC12573B5.004BC449-C12573B5.004C6DAB@precheza.cz>
	<20071218140641.5234C67DB2@agave.telenet-ops.be>
Message-ID: <971536df0712180654t741c94f1q6072fc8a0b6120c6@mail.gmail.com>

On Dec 18, 2007 9:07 AM, Bert Jacobs <b.jacobs at pandora.be> wrote:
>
> Hi,
>
> I'm having a bit of problems in creating a new dataframe.
> Below you'll find a description of the current dataframe and of the
> dataframe that needs to be created.
> Can someone help me out on this one?
> Thx in advance.
> Bert
>
> Current Dataframe
>
> Var1    Var2    Var3    Var4
> A       Fa      W1      1
> A       Si      W1      2
> A       Fa      W2      3
> A       Si      W3      4
> B       Si      W1      5
> C       La      W2      6
> C       Do      W4      7
>
> New Dataframe
>
> Var1    Var2    W1      W2      W3      W4
> A       Fa      1       3
> A       Si      2               4
> A       La
> A       Do
> B       Fa
> B       Si      5
> B       La
> B       Do
> C       Fa
> C       Si
> C       La              6
> C       Do                              7

Try this:

out <- ftable(xtabs(Var4 ~ Var1 + Var2 + Var3, DF))
out[out == 0] <- NA

Omit the last line is 0 fill is what you had wanted.

This will do it except that it will eliminate all rows
without data:

out2 <- reshape(DF, dir = "wide", timevar = "Var3", idvar = c("Var1", "Var2"))
out2[is.na(out2)] <- 0

Omit the last line if NA fill is what you wanted.

The reshape package melt/cast routines (see Hadley's solution in this
thread) can be used
to give a similar result to the reshape command above (i.e. all
missing rows are not
included) except that cast is a bit more flexible since it has a fill= argument.


From unwin at math.uni-augsburg.de  Tue Dec 18 16:01:40 2007
From: unwin at math.uni-augsburg.de (Antony Unwin)
Date: Tue, 18 Dec 2007 16:01:40 +0100
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <4767CE3B.7070603@stats.uwo.ca>
References: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>
	<4767CE3B.7070603@stats.uwo.ca>
Message-ID: <6CC742D2-40D9-4A49-A79B-D05F5969DE25@math.uni-augsburg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/7970ea19/attachment.pl 

From jmacdon at med.umich.edu  Tue Dec 18 16:02:00 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 18 Dec 2007 10:02:00 -0500
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <4767CE3B.7070603@stats.uwo.ca>
References: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>
	<4767CE3B.7070603@stats.uwo.ca>
Message-ID: <4767E0E8.3050800@med.umich.edu>

Duncan Murdoch wrote:
> On 18/12/2007 7:31 AM, Antony Unwin wrote:
>> Wayne,
>>
>> Try the iplot command in iPlots.  You can then vary both the  
>> pointsize and the transparency of your scatterplot interactively and  
>> decide which scatterplot conveys the information best.  Sometimes  
>> it's helpful to use more than one scatterplot when presenting your  
>> results.
>>
>> (I must admit to being very surprised that jittering and sunflower  
>> plots have been suggested for a dataset of 5000 points.  Do those who  
>> mentioned these methods have examples on that scale where they are  
>> effective?)
> 
> Sure.  The original post said there were about 50-60 unique locations. 
> This plot:
> 
> x <- rbinom(5000, 20, 0.15)
> y <- rbinom(5000, 20, 0.15)
> plot(x,y)
> 
> has a few more unique locations; tune those probabilities if you want it 
> closer.  Due to the overlap, the distribution is very unclear.  But this 
> plot
> 
> plot(jitter(x), jitter(y))

Another alternative is smoothscatter() in the geneplotter package from 
Bioconductor, which does a pretty reasonable job with these example data.

Best,

Jim


> 
> makes the distribution quite clear.
> 
> I wouldn't use the default pch if I had 50000 points, but with pch=".", 
> it's not so bad even in that case.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


From agoralczyk at gmail.com  Tue Dec 18 16:02:52 2007
From: agoralczyk at gmail.com (Armin Goralczyk)
Date: Tue, 18 Dec 2007 16:02:52 +0100
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <Xns9A09F06AFFDDDdNOTwinscomcast@80.91.229.13>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<a695fbee0712171238g4995040x579e58f52f83376e@mail.gmail.com>
	<Xns9A09CA51DB1E4dNOTwinscomcast@80.91.229.13>
	<Xns9A09F06AFFDDDdNOTwinscomcast@80.91.229.13>
Message-ID: <a695fbee0712180702k1a351b5cxca54d45b81096166@mail.gmail.com>

On 12/18/07, David Winsemius <dwinsemius at comcast.net> wrote:
> David Winsemius <dwinsemius at comcast.net> wrote in
> news:Xns9A09CA51DB1E4dNOTwinscomcast at 80.91.229.13:
>
> > "Armin Goralczyk" <agoralczyk at gmail.com> wrote in
> > news:a695fbee0712171238g4995040x579e58f52f83376e at mail.gmail.com:
>
> >> I tried the above function with simple search terms and it worked
> >> fine for me (also more output thanks to Martin's post) but when I use
> >> search terms attributed to certain fields, i.e. with [au] or [ta], I
> >> get the following error message:
> >>> pm.srch()
> >> 1: "laryngeal neoplasms[mh]"
> >> 2:
>
> > I am wondering if you used spaces, rather than "+"'s? If so then you
> > may want your function to do more gsub-processing of the input string.
>
> I tried my theory that one would need "+"'s instead of spaces, but
> disproved it. Spaces in the input string seems to produce acceptable
> results on my WinXP/R.2.6.1/RGui system even with more complex search
> strings.
>
> --
>

It's not the spaces, the problem is the tag (sorry that I didn't
specify this), or maybe the string []. I am working on a Mac OS X 10.4
with R version 2.6. Is it maybe a string conversion problem? In the
following warning strings in the html adress seem to be different:

Fehler in .Call("RS_XML_ParseTree", as.character(file), handlers,
as.logical(ignoreBlanks),  :
 error in creating parser for
http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=laryngeal
neoplasms[mh]
I/O warning : failed to load external entity
"http%3A//eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi%3Fdb=pubmed&term=laryngeal%20neoplasms%5Bmh%5D"

-- 
Armin Goralczyk, M.D.
--
Universit?tsmedizin G?ttingen
Abteilung Allgemein- und Viszeralchirurgie
Rudolf-Koch-Str. 40
39099 G?ttingen
--
Dept. of General Surgery
University of G?ttingen
G?ttingen, Germany
--
http://www.chirurgie-goettingen.de

From ggrothendieck at gmail.com  Tue Dec 18 16:03:53 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 18 Dec 2007 10:03:53 -0500
Subject: [R] Reshape Dataframe
In-Reply-To: <971536df0712180654t741c94f1q6072fc8a0b6120c6@mail.gmail.com>
References: <OF22595B02.296DB5AB-ONC12573B5.004BC449-C12573B5.004C6DAB@precheza.cz>
	<20071218140641.5234C67DB2@agave.telenet-ops.be>
	<971536df0712180654t741c94f1q6072fc8a0b6120c6@mail.gmail.com>
Message-ID: <971536df0712180703v283f86ccl8a1f716ef4a047b5@mail.gmail.com>

On Dec 18, 2007 9:54 AM, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
> On Dec 18, 2007 9:07 AM, Bert Jacobs <b.jacobs at pandora.be> wrote:
> >
> > Hi,
> >
> > I'm having a bit of problems in creating a new dataframe.
> > Below you'll find a description of the current dataframe and of the
> > dataframe that needs to be created.
> > Can someone help me out on this one?
> > Thx in advance.
> > Bert
> >
> > Current Dataframe
> >
> > Var1    Var2    Var3    Var4
> > A       Fa      W1      1
> > A       Si      W1      2
> > A       Fa      W2      3
> > A       Si      W3      4
> > B       Si      W1      5
> > C       La      W2      6
> > C       Do      W4      7
> >
> > New Dataframe
> >
> > Var1    Var2    W1      W2      W3      W4
> > A       Fa      1       3
> > A       Si      2               4
> > A       La
> > A       Do
> > B       Fa
> > B       Si      5
> > B       La
> > B       Do
> > C       Fa
> > C       Si
> > C       La              6
> > C       Do                              7
>
> Try this:
>
> out <- ftable(xtabs(Var4 ~ Var1 + Var2 + Var3, DF))
> out[out == 0] <- NA
>
> Omit the last line is 0 fill is what you had wanted.
>
> This will do it except that it will eliminate all rows
> without data:
>
> out2 <- reshape(DF, dir = "wide", timevar = "Var3", idvar = c("Var1", "Var2"))
> out2[is.na(out2)] <- 0
>
> Omit the last line if NA fill is what you wanted.
>
> The reshape package melt/cast routines (see Hadley's solution in this
> thread) can be used
> to give a similar result to the reshape command above (i.e. all
> missing rows are not
> included) except that cast is a bit more flexible since it has a fill= argument.

Just one correction.  The cast function in reshape has an add.missing= argument
that can control this so actually any of the solutions could be
obtained with cast
using the fill= and add.missing= arguments to control which one you want.

>


From b.jacobs at pandora.be  Tue Dec 18 16:31:32 2007
From: b.jacobs at pandora.be (Bert Jacobs)
Date: Tue, 18 Dec 2007 16:31:32 +0100
Subject: [R] Reshape Dataframe
In-Reply-To: <f8e6ff050712180615j19ddb38y33bd599e1340c9b6@mail.gmail.com>
Message-ID: <20071218153027.B665C67DE1@agave.telenet-ops.be>


Thx Hadley,
It works, but I need some finetuning.

If I use the following expression:
Newdf <-reshape(df, timevar="Var3", idvar=c("Var1","Var2"),direction="wide")

Newdf
Var1	Var2	Var3.W1	Var3.W2	Var3.W3	var3.W4
A	Fa	1	  	3		
A	Si	2				4
B	Si	5		
C	La			6
C	Do							7

Is there an option so that for each Var1 all possible combinations of Var2
are listed (i.e. creation of blanco lines).
Is it possible to name the columns with the values of the original Var3
variable, so that the name Var3.W1 changes to W1? 

Var1	Var2	W1	W2	W3	W4
A	Fa	1	3		
A	Si	2		4
A	La
A	Do			
B	Fa				
B	Si	5		
B	La
B	Do
C	Fa				
C	Si			
C	La		6
C	Do				7


Thx,
Bert

-----Original Message-----
From: hadley wickham [mailto:h.wickham at gmail.com] 
Sent: 18 December 2007 15:16
To: Bert Jacobs
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Reshape Dataframe

On 12/18/07, Bert Jacobs <b.jacobs at pandora.be> wrote:
>
> Hi,
>
> I'm having a bit of problems in creating a new dataframe.
> Below you'll find a description of the current dataframe and of the
> dataframe that needs to be created.
> Can someone help me out on this one?

library(reshape)
dfm <- melt(df, id = 1:3)
cast(dfm, ... ~ Var3)

You can find out more about the reshape package at http://had.co.nz/reshape

Hadley

-- 
http://had.co.nz/


From murdoch at stats.uwo.ca  Tue Dec 18 16:49:50 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 18 Dec 2007 10:49:50 -0500
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <6CC742D2-40D9-4A49-A79B-D05F5969DE25@math.uni-augsburg.de>
References: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>
	<4767CE3B.7070603@stats.uwo.ca>
	<6CC742D2-40D9-4A49-A79B-D05F5969DE25@math.uni-augsburg.de>
Message-ID: <4767EC1E.3010902@stats.uwo.ca>

On 18/12/2007 10:01 AM, Antony Unwin wrote:
> On 18 Dec 2007, at 2:42 pm, Duncan Murdoch wrote:
> 
>>> (I must admit to being very surprised that jittering and  
>>> sunflower  plots have been suggested for a dataset of 5000  
>>> points.  Do those who  mentioned these methods have examples on  
>>> that scale where they are  effective?)
>> Sure.  The original post said there were about 50-60 unique  
>> locations. This plot:
>>
>> x <- rbinom(5000, 20, 0.15)
>> y <- rbinom(5000, 20, 0.15)
>> plot(x,y)
>>
>> has a few more unique locations; tune those probabilities if you  
>> want it closer.  Due to the overlap, the distribution is very  
>> unclear.  But this plot
>>
>> plot(jitter(x), jitter(y))
>>
>> makes the distribution quite clear.
> 
> No it doesn't!  It makes it moderately clearer than the plot without  
> jittering.  One good alternative here is the fluctuation diagram  
> variant of a mosaic plot:
> 
> xx<-as.factor(x)
> yy<-as.factor(y)
> imosaic(xx,yy, type="f")

That plot is better than jittering, but there's the problem in the 
mosaic plot of understanding the scale of the rectangles:  is it area or 
diameter that encodes the count?  With a jittered plot, you lose 
resolution when the number of points gets too high because you just see 
a mess of ink, but at least you only require the viewer to count in 
order to get a close numerical reading from the plot.

I could also claim that while imperfect, at least jittering is widely 
applicable.  For example, if the data were not on a regular grid, 
perhaps because they had been generated like this:

xloc <- rnorm(50)
yloc <- rnorm(50)
index <- sample(1:50, 5000, rep=TRUE, prob = abs(xloc))
x <- xloc[index]
y <- yloc[index]

then jittering still works as well (or as poorly), but the imosaic would 
not work at all.  There are better plots than jittering available, but 
jittering is easy.

(Actually, with this dataset, plot(jitter(x), jitter(y)) is really poor, 
because jitter() chooses a bad amount of jittering.  But with manual 
tuning (e.g.  plot(jitter(x, a=0.1), jitter(y, a=0.1), pch=".")) it's 
not too bad.  So I'd say jittering worked, but the R implementation of 
it may need improvement).

> Using jittering for categorical data is really not to be recommended  
> and will certainly degrade in performance as the dataset gets bigger.

Yes, I probably wouldn't recommend jittering if there were more than a 
few hundred replications at any point, or more than a few hundred unique 
points.

Duncan Murdoch

P.S. iplots 1.1-1 may have an init problem in Windows: in my first 
attempt, the plot made the boxes too large to fit in their cells, but it 
fixed itself when I resized the window, and the bug doesn't seem to be 
repeatable.


> 
> Antony Unwin
> Professor of Computer-Oriented Statistics and Data Analysis,
> University of Augsburg,
> Germany


From murdoch at stats.uwo.ca  Tue Dec 18 16:58:09 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 18 Dec 2007 10:58:09 -0500
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <4767E0E8.3050800@med.umich.edu>
References: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>	<4767CE3B.7070603@stats.uwo.ca>
	<4767E0E8.3050800@med.umich.edu>
Message-ID: <4767EE11.8030104@stats.uwo.ca>

On 18/12/2007 10:02 AM, James W. MacDonald wrote:
> Duncan Murdoch wrote:
>> On 18/12/2007 7:31 AM, Antony Unwin wrote:
>>> Wayne,
>>>
>>> Try the iplot command in iPlots.  You can then vary both the  
>>> pointsize and the transparency of your scatterplot interactively and  
>>> decide which scatterplot conveys the information best.  Sometimes  
>>> it's helpful to use more than one scatterplot when presenting your  
>>> results.
>>>
>>> (I must admit to being very surprised that jittering and sunflower  
>>> plots have been suggested for a dataset of 5000 points.  Do those who  
>>> mentioned these methods have examples on that scale where they are  
>>> effective?)
>> Sure.  The original post said there were about 50-60 unique locations. 
>> This plot:
>>
>> x <- rbinom(5000, 20, 0.15)
>> y <- rbinom(5000, 20, 0.15)
>> plot(x,y)
>>
>> has a few more unique locations; tune those probabilities if you want it 
>> closer.  Due to the overlap, the distribution is very unclear.  But this 
>> plot
>>
>> plot(jitter(x), jitter(y))
> 
> Another alternative is smoothscatter() in the geneplotter package from 
> Bioconductor, which does a pretty reasonable job with these example data.

Yes, I agree.  (As an aside, there's actually a capital S in 
smoothScatter(), and it's a bit of a pain to install, because 
geneplotter depends on something that depends on DBI, which is not so 
easily available these days.)

Duncan Murdoch


From jholtman at gmail.com  Tue Dec 18 17:08:23 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 18 Dec 2007 11:08:23 -0500
Subject: [R] accessing dimension names
In-Reply-To: <da3995450712180301s4ffd0a33u635bb6f85f0f84db@mail.gmail.com>
References: <da3995450712180301s4ffd0a33u635bb6f85f0f84db@mail.gmail.com>
Message-ID: <644e1f320712180808l54294814n5a4e436f87f020dc@mail.gmail.com>

dimnames(y)[[paste('x', idx, sep="")]]

On Dec 18, 2007 6:01 AM,  <born.to.b.wyld at gmail.com> wrote:
> I have a matrix y:
>
> > dimnames(y)
> $x93
> [1] "1" "2"
>
> $x94
> [1] "0" "1" "2"
> .................. so on  (there are other dimensions as well)
>
>
>
> I need to access a particular dimension, but a random mechanism tells me
> which dimension it would. So, sometimes I might need to access
> dimnames(y)$x93, some other time it would be dimnames(y)$x94.. and so on.
> Now let that random dimension be idx, then dimnames(y)$paste('x',idx,sep='')
> doesn't work.
>
> Can anyone help?
>
> Thanks!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ahenningsen at email.uni-kiel.de  Tue Dec 18 17:15:57 2007
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 18 Dec 2007 17:15:57 +0100
Subject: [R] [R-pkgs] New version of systemfit (not backward compatible)
Message-ID: <200712181715.57597.ahenningsen@email.uni-kiel.de>

Dear R users,

the systemfit package contains functions for fitting systems of simultaneous 
equations by various estimation methods (e.g. OLS, SUR, 2SLS, 3SLS). 
Currently version 0.8 of systemfit is available on CRAN. However, shortly we 
will upload version 1.0, which is NOT BACKWARD COMPATIBLE. The changes that 
broke backward compatibility were necessary to make systemfit() more similar 
to standard regression tools in R such as lm(). We hope that the usage of 
systemfit() is more intuitive for R users now. We will continue to maintain 
the 0.8 branch so that users can still use the old version if they do not 
want to update their R scripts. Both versions are and will be available for 
download from systemfit's website:
   http://www.systemfit.org/
which is a shortcut to 
   http://www.uni-kiel.de/agrarpol/ahenningsen/systemfit/

A paper that describes the (new version of the) systemfit package is 
forthcoming in the Journal of Statistical Software (JSS). A preprint of this 
paper is available on systemfit's website:
   http://www.systemfit.org/systemfit_paper_1.0.pdf

The following list summarizes the most important changes 
from version 0.8 to 1.0:
- some names of systemfit()'s arguments have changed to make it more 
  similar to standard regression tools in R
- the order of systemfit()'s arguments has changed to make it more 
  similar to standard regression tools in R
- the names of the elements in the object returned by systemfit() have 
  changed to make it more similar to lm()
- added several methods for systemfit objects that are generally 
  available for standard regression tools in R
- restrictions on the coefficients can be specified symbolically now
- the functionality of systemfitClassic() has been integrated into systemfit()
- replaced ftest.systemfit() and waldtest.systemfit() by the method
  linear.hypothesis()
- systemfit now uses the "Matrix" package for matrix calculations (this 
  makes the estimation of large models and large data sets much faster)
- improved checking of the arguments so that error messages are more 
  helpful now

We thank two anonymous referees of the JSS, Achim Zeileis, John Fox, William 
H. Greene, Ott Toomet, Duncan Murdoch, Martin Maechler, Duglas Bates and 
several (other) systemfit users for their answers, comments, and/or 
suggestions that helped us to improve the systemfit package.

Feedback is always welcome!
Arne & Jeff

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445 or +49-4349-914871
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From jmacdon at med.umich.edu  Tue Dec 18 17:21:28 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 18 Dec 2007 11:21:28 -0500
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <4767EE11.8030104@stats.uwo.ca>
References: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>	<4767CE3B.7070603@stats.uwo.ca>
	<4767E0E8.3050800@med.umich.edu> <4767EE11.8030104@stats.uwo.ca>
Message-ID: <4767F388.1050101@med.umich.edu>

Duncan Murdoch wrote:
> Yes, I agree.  (As an aside, there's actually a capital S in 
> smoothScatter(), and it's a bit of a pain to install, because 
> geneplotter depends on something that depends on DBI, which is not so 
> easily available these days.)

Somehow I always forget the capital S and wonder if I have loaded the 
correct package ;-D

As for installing the required dependencies, I believe this is actually 
quite straightforward:

source("http://www.bioconductor.org/biocLite.R")
biocLite("geneplotter")

Should install geneplotter and all required dependencies.

Best,

Jim


> 
> Duncan Murdoch

-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


From jrkrideau at yahoo.ca  Tue Dec 18 17:24:05 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 18 Dec 2007 11:24:05 -0500 (EST)
Subject: [R] bar plot colors
In-Reply-To: <81EA3460BED3AB4483920E05F13B12DD07D79189@WS-BCO-MSE8.milky-way.battelle.org>
Message-ID: <794058.27400.qm@web32815.mail.mud.yahoo.com>

I think you're going to find that barchart with that
many values in a bar is going to be pretty well
uninterpretable.  

Jim Lemon gives the desired barchart but it is very
difficult to read.  

Stealing his code to create the same matrix I'd
suggest may be looking at a dotchart.  I'm not sure if
this is even close to an optimal solution but I do
think it's a bit better than a barchart approach
======================================================
heights<-matrix(sample(10:70,54),ncol=3)
bar.colors<-rep(rep(2:7,each=3),3)
cost.types <- c("Direct", "Indirec", "Induced")
colnames(heights) <-  c("A", "B", "C")
rownames(heights) <- c(rep(cost.types, 6))

dotchart(heights, col=bar.colors, pch=16, cex=.6)

=======================================================
--- "Winkel, David" <WinkelD at BATTELLE.ORG> wrote:

> All,
> 
>  
> 
> I have a question regarding colors in bar plots.  I
> want to stack a
> total of 18 cost values in each bar. Basically, it
> is six cost types and
> each cost type has three components- direct,
> indirect, and induced
> costs.  I would like to use both solid color bars
> and bars with the
> slanted lines (using the density parameter).  The
> colors would
> distinguish cost types and the lines would
> distinguish
> direct/indirect/induced.  I want the cost types
> (i.e. colors) to be
> stacked together for each cost type.  In other
> words, I don't want all
> of the solid bars at the bottom and all of the
> slanted lines at the top.
> 
> 
> So far, I have made a bar plot with all solid colors
> and then tried to
> overwrite that bar plot by calling barplot() again
> and putting the white
> slanted lines across the bars.  However, I can't get
> this method to work
> while still grouping the cost types together.
> 
>  
> 
> Thanks in advance for any help you can provide.
> 
>  
> 
> David Winkel
> 
> Applied Biology and Aerosol Technology
> 
> Battelle Memorial Institute
> 
> 505 King Ave.
> 
> Columbus, Ohio 43201
> 
> 614.424.3513
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From kw.statr at gmail.com  Tue Dec 18 18:02:51 2007
From: kw.statr at gmail.com (Kevin Wright)
Date: Tue, 18 Dec 2007 11:02:51 -0600
Subject: [R] regression towards the mean, AS paper November 2007
In-Reply-To: <f8e6ff050712171310l381a275eq75da61305c640573@mail.gmail.com>
References: <4766BE33.1030806@gvdnet.dk> <4766C0C0.2080509@stats.uwo.ca>
	<36F293BA-497C-43D7-8943-7AFD3C8648A4@auckland.ac.nz>
	<f8e6ff050712171310l381a275eq75da61305c640573@mail.gmail.com>
Message-ID: <c968588d0712180902n70776aefnb8fc222638b8e23a@mail.gmail.com>

On Dec 17, 2007 3:10 PM, hadley wickham <h.wickham at gmail.com> wrote:
> >         This has nothing to do really with the question that Troels asked,
> >         but the exposition quoted from the AA paper is unnecessarily confusing.
> >         The phrase ``Because X0 and X1 have identical marginal
> > distributions ...''
> >         throws the reader off the track.  The identical marginal distributions
> >         are irrelevant.  All one needs is that the ***means*** of X0 and X1
> >         be the same, and then the null hypothesis tested by a paired t-test
> >         is true and so the p-values are (asymptotically) Uniform[0,1].  With
> >         a sample size of 100, the ``asymptotically'' bit can be safely ignored
> >         for any ``decent'' joint distribution of X0 and X1.  If one further
> >         assumes that X0 - X1 is Gaussian (which has nothing to do with X0 and
> >         X1 having identical marginal distributions) then ``asymptotically''
> >         turns into ``exactly''.
>
> Another related issue is that uniform distributions don't look very uniform:
>
> hist(runif(100))
> hist(runif(1000))
> hist(runif(10000))
>
> Be sure to calibrate your eyes (and your bin width) before rejecting
> the hypothesis that the distribution is uniform.
>
> Hadley

Thanks for the example, Hadley.  To me, this suggests we should stop
teaching histograms in Stat 101 and instead use quantile plots, which
give excellent results for n=100 and even surprisingly good results
for n=10:

par(mfrow=c(2,2))
for(i in c(10, 100, 1000, 10000)) {
  qqplot(runif(i), qunif(seq(1/i, 1, length=i)), main=i,
         xlim=c(0,1), ylim=c(0,1),
         xlab="runif", ylab="Uniform distribution quantiles")
  abline(0,1,col="lightgray")
}

Kevin (drifting even further off topic)


From unwin at math.uni-augsburg.de  Tue Dec 18 18:44:23 2007
From: unwin at math.uni-augsburg.de (Antony Unwin)
Date: Tue, 18 Dec 2007 18:44:23 +0100
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <4767EC1E.3010902@stats.uwo.ca>
References: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>
	<4767CE3B.7070603@stats.uwo.ca>
	<6CC742D2-40D9-4A49-A79B-D05F5969DE25@math.uni-augsburg.de>
	<4767EC1E.3010902@stats.uwo.ca>
Message-ID: <071992A4-9AF6-4F14-972B-F0F4114EC8CD@math.uni-augsburg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/ee30dcde/attachment.pl 

From br44114 at gmail.com  Tue Dec 18 18:46:51 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 18 Dec 2007 12:46:51 -0500
Subject: [R] calculating the number of days from dates
Message-ID: <8d5a36350712180946j3e2f153bx250e9c44834ccfb1@mail.gmail.com>

> Sorry for using library instead package, but
> library() is one command for using packages.

... which is why all efforts to make folks say "package" instead of >>
"library" << are doomed to fail, IMHO. Besides, in English, "library"
also means "a collection of software or data usually reflecting a
specific theme or application" (#9 on the list from
http://dictionary.reference.com/ ). Therefore:
   > "library" == "package"
   [1] TRUE!
and just about the only way to clear up the "confusion" would be to
rename library() to package(), and replace "library" with "folder" or
"directory".



> -----Original Message-----
> From: r-help-bounces at r-project.org
> [mailto:r-help-bounces at r-project.org] On Behalf Of Knut Krueger
> Sent: Monday, December 17, 2007 2:11 AM
> To: 'R R-help'
> Subject: Re: [R] calculating the number of days from dates
>
>
> > it's  a  >> package <<  , not a library, please!
> >
> >
> Sorry for using library instead package, but
>
> library() is one command for using packages.
>
> Therefore I (and it seems that i am not the only one) used
> library instead package.
>
> Knut
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Dietrich.Trenkler at uni-osnabrueck.de  Tue Dec 18 18:52:09 2007
From: Dietrich.Trenkler at uni-osnabrueck.de (Dietrich Trenkler)
Date: Tue, 18 Dec 2007 18:52:09 +0100
Subject: [R] Sweave and Scientific Workplace
Message-ID: <476808C9.80002@uni-osnabrueck.de>

Dear HelpeRs,

a colleague of mine uses Scientific Workplace to write his LaTeX documents.
I made his mouth water mentioning the advantages of using Sweave.

Not using SW myself I wonder if anyone out there has gathered some 
experiences
in using the combination of both.

Thank you in advance

Dietrich

-- 
Dietrich Trenkler c/o Universitaet Osnabrueck 
Rolandstr. 8; D-49069 Osnabrueck, Germany    
email: Dietrich.Trenkler at Uni-Osnabrueck.de


From h.wickham at gmail.com  Tue Dec 18 18:54:43 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 18 Dec 2007 11:54:43 -0600
Subject: [R] regression towards the mean, AS paper November 2007
In-Reply-To: <c968588d0712180902n70776aefnb8fc222638b8e23a@mail.gmail.com>
References: <4766BE33.1030806@gvdnet.dk> <4766C0C0.2080509@stats.uwo.ca>
	<36F293BA-497C-43D7-8943-7AFD3C8648A4@auckland.ac.nz>
	<f8e6ff050712171310l381a275eq75da61305c640573@mail.gmail.com>
	<c968588d0712180902n70776aefnb8fc222638b8e23a@mail.gmail.com>
Message-ID: <f8e6ff050712180954y60d7780ew10c1994c4fbc7117@mail.gmail.com>

> Thanks for the example, Hadley.  To me, this suggests we should stop
> teaching histograms in Stat 101 and instead use quantile plots, which
> give excellent results for n=100 and even surprisingly good results
> for n=10:

It all depends on what you're trying to do - I don't think histograms
are particularly good as density estimators, but that's not what
you're using them for most of the time! You're using them as an
exploratory tool to try and understand what's going on in your data -
often you'll need to use very small bin widths which help find
unexpected gaps and patterns in your data.   It's helpful to have some
feel for what common distributions look like.

Hadley

-- 
http://had.co.nz/


From h.wickham at gmail.com  Tue Dec 18 18:56:22 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 18 Dec 2007 11:56:22 -0600
Subject: [R] ggplot2 - getting at the grobs
In-Reply-To: <20071218144135.4364EF800B@smtp3.ualg.pt>
References: <20071218144135.4364EF800B@smtp3.ualg.pt>
Message-ID: <f8e6ff050712180956h253db245n3e4fbc99c967a39a@mail.gmail.com>

Hi Pedro,

Could you be a bit more explicit about what you're trying to do?  Have
you read the last chapter of the draft ggplot book?

Hadley

On Dec 18, 2007 8:41 AM, Pedro de Barros <pbarros at ualg.pt> wrote:
> Dear All,
>
> I continue trying to get several of my plotting functions to use
> ggplot, because I really do like the concept of the graphical
> objects, and working with them in the abstract.
> I am now trying to access the grobs to manipulate using grid.
> However, until now all I managed was to get the plot as a gTree
> object, and manipulate it as a gTree from there. The problem is that
> then it is no longer a ggplot, and thus I can no longer use the
> ggplot functions.
> How to get at the grobs, without converting the ggplot into a gTree?
>
> Thanks,
> Pedro
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
http://had.co.nz/


From murdoch at stats.uwo.ca  Tue Dec 18 18:57:36 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 18 Dec 2007 12:57:36 -0500
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <4767F388.1050101@med.umich.edu>
References: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>	<4767CE3B.7070603@stats.uwo.ca>
	<4767E0E8.3050800@med.umich.edu> <4767EE11.8030104@stats.uwo.ca>
	<4767F388.1050101@med.umich.edu>
Message-ID: <47680A10.6040103@stats.uwo.ca>

On 12/18/2007 11:21 AM, James W. MacDonald wrote:
> Duncan Murdoch wrote:
>> Yes, I agree.  (As an aside, there's actually a capital S in 
>> smoothScatter(), and it's a bit of a pain to install, because 
>> geneplotter depends on something that depends on DBI, which is not so 
>> easily available these days.)
> 
> Somehow I always forget the capital S and wonder if I have loaded the 
> correct package ;-D
> 
> As for installing the required dependencies, I believe this is actually 
> quite straightforward:
> 
> source("http://www.bioconductor.org/biocLite.R")
> biocLite("geneplotter")
> 
> Should install geneplotter and all required dependencies.

Yes, that works.  Not sure why DBI was unavailable for a simple install 
of geneplotter from the Windows Rgui; when I try it now (on a different 
PC, maybe using a different mirror) it's there.

Duncan Murdoch


From murdoch at stats.uwo.ca  Tue Dec 18 19:05:03 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 18 Dec 2007 13:05:03 -0500
Subject: [R] Scatterplot Showing All Points
In-Reply-To: <071992A4-9AF6-4F14-972B-F0F4114EC8CD@math.uni-augsburg.de>
References: <E1AFA099-E78C-4595-BD75-570B65E59766@math.uni-augsburg.de>
	<4767CE3B.7070603@stats.uwo.ca>
	<6CC742D2-40D9-4A49-A79B-D05F5969DE25@math.uni-augsburg.de>
	<4767EC1E.3010902@stats.uwo.ca>
	<071992A4-9AF6-4F14-972B-F0F4114EC8CD@math.uni-augsburg.de>
Message-ID: <47680BCF.6040607@stats.uwo.ca>

On 12/18/2007 12:44 PM, Antony Unwin wrote:
> On 18 Dec 2007, at 4:49 pm, Duncan Murdoch wrote:
> 
>>> One good alternative here is the fluctuation diagram  variant of a  
>>> mosaic plot:
>>> xx<-as.factor(x)
>>> yy<-as.factor(y)
>>> imosaic(xx,yy, type="f")
>>
>> That plot is better than jittering, but there's the problem in the  
>> mosaic plot of understanding the scale of the rectangles:  is it  
>> area or diameter that encodes the count?
> 
> Area is used.
> 
>> With a jittered plot, you lose resolution when the number of points  
>> gets too high because you just see a mess of ink, but at least you  
>> only require the viewer to count in order to get a close numerical  
>> reading from the plot.
> 
> If someone needs a count, they should be given a table.   Graphics  
> are for qualitative conclusions not details.  Anyway, counting will  
> only work for really small datasets.
> 
>> I could also claim that while imperfect, at least jittering is  
>> widely applicable.  For example, if the data were not on a regular  
>> grid, perhaps because they had been generated like this:
>>
>> xloc <- rnorm(50)
>> yloc <- rnorm(50)
>> index <- sample(1:50, 5000, rep=TRUE, prob = abs(xloc))
>> x <- xloc[index]
>> y <- yloc[index]
>>
>> then jittering still works as well (or as poorly), but the imosaic  
>> would not work at all.
> 
> That's right and that's (almost) the sort of example I was thinking  
> of.  For a limited number of locations like this a bubble plot would  
> be best (which has already been suggested in this thread, I think).   
> For many locations and few replications I would still go for varying  
> pointsize and transparency.
> 
> Incidentally, to check your suggestion I ran your code and discovered  
> that the transparency in iplot does not seem to like replications.   
> Very strange, we'll have to check why.  I then looked closely at the  
> numbers of replications generated and discovered that case 25 was  
> picked 325 times and case 40 only once.  Rather too extreme for my  
> liking!  Running it again gave very similar results, though not  
> exactly the same: this time it was 325 times for case 25 and case 40  
> was not picked at all.  Other numbers varied slightly.  This is not  
> what I expected, any ideas?

abs(xloc) typically varies by a factor of about 100 from smallest to 
largest, but sometimes the small end is really small, and so the ratio 
is really big.

Duncan Murdoch

> 
>> P.S. iplots 1.1-1 may have an init problem in Windows: in my first  
>> attempt, the plot made the boxes too large to fit in their cells,  
>> but it fixed itself when I resized the window, and the bug doesn't  
>> seem to be repeatable.
> 
> Thanks.  This happens occasionally on the Mac too.  Refreshing solves  
> it in practice, but we need to find out why it can happen (and stop  
> it happening!).
> 
> Antony Unwin
> Professor of Computer-Oriented Statistics and Data Analysis,
> University of Augsburg,
> Germany


From Marc.Moragues at scri.ac.uk  Tue Dec 18 19:14:00 2007
From: Marc.Moragues at scri.ac.uk (Marc Moragues)
Date: Tue, 18 Dec 2007 18:14:00 -0000
Subject: [R] R brakes when submitting a query to MySQL
Message-ID: <50E41C3A73F46B4D876BD13F5E80264A0127DF54@exchange3.sims.scri.sari.ac.uk>

Hello,

I would like to retrieve data stored in MySQL database, so I installed
RMySQL package.
I can successfully connect with the my database using the following code

> dvr<-dbDriver("MySQL")
> con2<-dbConnect(dvr,group="exbardiv")
> mysqlDescribeConnection(con2)

<MySQLConnection:(972,0)> 
  User: mmorag 
  Host: localhost 
  Dbname: exbardiv 
  Connection type: localhost via TCP/IP 
  No resultSet available

I can even see the tables in the database

> dbListTables(con2)
[1] "agoueb"    "high_ld"   "rescue"    "sjlc_info" "sjlc_ld"   "temp"

[7] "temp_snp1" "temp_snp2"

However, when I try to query the database, R breakes.

res<-dbSendQuery(con,'select * from sjlc_ld')

Can anyone help me tune up the connection between R and MySQL?

Thank you,
Marc.
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

SCRI, Invergowrie, Dundee, DD2 5DA.  
The Scottish Crop Research Institute is a charitable company limited by guarantee. 
Registered in Scotland No: SC 29367.
Recognised by the Inland Revenue as a Scottish Charity No: SC 006662.


DISCLAIMER:

This email is from the Scottish Crop Research Institute, but the views 
expressed by the sender are not necessarily the views of SCRI and its 
subsidiaries.  This email and any files transmitted with it are confidential 
to the intended recipient at the e-mail address to which it has been 
addressed.  It may not be disclosed or used by any other than that addressee.
If you are not the intended recipient you are requested to preserve this 
confidentiality and you must not use, disclose, copy, print or rely on this 
e-mail in any way. Please notify postmaster at scri.ac.uk quoting the 
name of the sender and delete the email from your system.

Although SCRI has taken reasonable precautions to ensure no viruses are 
present in this email, neither the Institute nor the sender accepts any 
responsibility for any viruses, and it is your responsibility to scan the email 
and the attachments (if any).



From bethany.johnson at citi.com  Tue Dec 18 19:14:05 2007
From: bethany.johnson at citi.com (Johnson, Bethany )
Date: Tue, 18 Dec 2007 13:14:05 -0500
Subject: [R] PCA - "cov.wt(z) : 'x' must contain finite values only"
Message-ID: <7FCC4AA5D6E2824DB8A32B8C9E76C7AB11E5D2@EXNJMB12.nam.nsroot.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/94e12ae7/attachment.pl 

From kzembowe at jhuccp.org  Tue Dec 18 19:18:15 2007
From: kzembowe at jhuccp.org (Zembower, Kevin)
Date: Tue, 18 Dec 2007 13:18:15 -0500
Subject: [R] R brakes when submitting a query to MySQL
In-Reply-To: <50E41C3A73F46B4D876BD13F5E80264A0127DF54@exchange3.sims.scri.sari.ac.uk>
References: <50E41C3A73F46B4D876BD13F5E80264A0127DF54@exchange3.sims.scri.sari.ac.uk>
Message-ID: <2E8AE992B157C0409B18D0225D0B476306CD9253@XCH-VN01.sph.ad.jhsph.edu>

Is it your use of 'con' rather than 'con2' in dbSendQuery? -Kevin

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Marc Moragues
Sent: Tuesday, December 18, 2007 1:14 PM
To: r-help at r-project.org
Subject: [R] R brakes when submitting a query to MySQL

Hello,

I would like to retrieve data stored in MySQL database, so I installed
RMySQL package.
I can successfully connect with the my database using the following code

> dvr<-dbDriver("MySQL")
> con2<-dbConnect(dvr,group="exbardiv")
> mysqlDescribeConnection(con2)

<MySQLConnection:(972,0)> 
  User: mmorag 
  Host: localhost 
  Dbname: exbardiv 
  Connection type: localhost via TCP/IP 
  No resultSet available

I can even see the tables in the database

> dbListTables(con2)
[1] "agoueb"    "high_ld"   "rescue"    "sjlc_info" "sjlc_ld"   "temp"

[7] "temp_snp1" "temp_snp2"

However, when I try to query the database, R breakes.

res<-dbSendQuery(con,'select * from sjlc_ld')

Can anyone help me tune up the connection between R and MySQL?

Thank you,
Marc.
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _

SCRI, Invergowrie, Dundee, DD2 5DA.  
The Scottish Crop Research Institute is a charitable company limited by
guarantee. 
Registered in Scotland No: SC 29367.
Recognised by the Inland Revenue as a Scottish Charity No: SC 006662.


DISCLAIMER:\ \ This email is from the Scottish Crop Rese...{{dropped:30}}


From br44114 at gmail.com  Tue Dec 18 19:21:55 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 18 Dec 2007 13:21:55 -0500
Subject: [R] Scatterplot Showing All Points
Message-ID: <8d5a36350712181021rf33a14cs381f175c5172e708@mail.gmail.com>

Another approach which I'm pleased with but was not suggested so far
is jitter + kde2d from MASS:

plot(jitter(x), jitter(y))
if (!exists("kde2d")) require(MASS)
kdesamp <- 20000  #depending on your RAM
forkde <- if (kdesamp < length(x)) sample(1:length(x), kdesamp,
replace=FALSE) else 1:length(x)
d <- kde2d(x[forkde], y[forkde])
contour(d, add=TRUE)



> -----Original Message-----
> From: r-help-bounces at r-project.org
> Subject: Re: [R] Scatterplot Showing All Points
>


From charpent at bacbuc.dyndns.org  Tue Dec 18 19:22:00 2007
From: charpent at bacbuc.dyndns.org (Emmanuel Charpentier)
Date: Tue, 18 Dec 2007 19:22:00 +0100
Subject: [R] R brakes when submitting a query to MySQL
In-Reply-To: <50E41C3A73F46B4D876BD13F5E80264A0127DF54@exchange3.sims.scri.sari.ac.uk>
References: <50E41C3A73F46B4D876BD13F5E80264A0127DF54@exchange3.sims.scri.sari.ac.uk>
Message-ID: <fk934a$75a$1@ger.gmane.org>

Marc Moragues a ?crit :
> Hello,
> 
> I would like to retrieve data stored in MySQL database, so I installed
> RMySQL package.
> I can successfully connect with the my database using the following code
> 
>> dvr<-dbDriver("MySQL")
>> con2<-dbConnect(dvr,group="exbardiv")
>> mysqlDescribeConnection(con2)
> 
> <MySQLConnection:(972,0)> 
>   User: mmorag 
>   Host: localhost 
>   Dbname: exbardiv 
>   Connection type: localhost via TCP/IP 
>   No resultSet available
> 
> I can even see the tables in the database
> 
>> dbListTables(con2)
> [1] "agoueb"    "high_ld"   "rescue"    "sjlc_info" "sjlc_ld"   "temp"
> 
> [7] "temp_snp1" "temp_snp2"
> 
> However, when I try to query the database, R breakes.

What does *that* means ? You should be a bit more descriptive...

> res<-dbSendQuery(con,'select * from sjlc_ld')

require(MindeReaderAlpha).

Hmmmm ... Isn't the "breakage" just a loooong wait with no answer ? Or
maybe a timeout ?

In which case I'd try to put a semicolon (";")  at the end of the SQL
query, thus making it syntactically valid SQL...

HTH,

					Emmanuel Charpentier

> Can anyone help me tune up the connection between R and MySQL?
> 
> Thank you,
> Marc.
> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
> 
> SCRI, Invergowrie, Dundee, DD2 5DA.  
> The Scottish Crop Research Institute is a charitable company limited by guarantee. 
> Registered in Scotland No: SC 29367.
> Recognised by the Inland Revenue as a Scottish Charity No: SC 006662.
> 
> 
> DISCLAIMER:
> 
> This email is from the Scottish Crop Research Institute, but the views 
> expressed by the sender are not necessarily the views of SCRI and its 
> subsidiaries.  This email and any files transmitted with it are confidential 
> to the intended recipient at the e-mail address to which it has been 
> addressed.  It may not be disclosed or used by any other than that addressee.
> If you are not the intended recipient you are requested to preserve this 
> confidentiality and you must not use, disclose, copy, print or rely on this 
> e-mail in any way. Please notify postmaster at scri.ac.uk quoting the 
> name of the sender and delete the email from your system.
> 
> Although SCRI has taken reasonable precautions to ensure no viruses are 
> present in this email, neither the Institute nor the sender accepts any 
> responsibility for any viruses, and it is your responsibility to scan the email 
> and the attachments (if any).
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Marc.Moragues at scri.ac.uk  Tue Dec 18 19:35:38 2007
From: Marc.Moragues at scri.ac.uk (Marc Moragues)
Date: Tue, 18 Dec 2007 18:35:38 -0000
Subject: [R] R brakes when submitting a query to MySQL
In-Reply-To: <2E8AE992B157C0409B18D0225D0B476306CD9253@XCH-VN01.sph.ad.jhsph.edu>
References: <50E41C3A73F46B4D876BD13F5E80264A0127DF54@exchange3.sims.scri.sari.ac.uk>
	<2E8AE992B157C0409B18D0225D0B476306CD9253@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <50E41C3A73F46B4D876BD13F5E80264A0127DF55@exchange3.sims.scri.sari.ac.uk>

You are right, it was a mistake copying and pasting the code. There is
no error message from R when I run con2. I get a Windows error message
saying "R for windows terminal front-end has encountered a problem and
need to close".

The error signature is:

AppName: rterm.exe	 AppVer: 2.60.43063.0	 ModName: msvcrt.dll
ModVer: 7.0.2600.2180	 Offset: 000378c0

Marc. 

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Zembower, Kevin
Sent: 18 December 2007 18:18
To: r-help at r-project.org
Subject: Re: [R] R brakes when submitting a query to MySQL

Is it your use of 'con' rather than 'con2' in dbSendQuery? -Kevin

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Marc Moragues
Sent: Tuesday, December 18, 2007 1:14 PM
To: r-help at r-project.org
Subject: [R] R brakes when submitting a query to MySQL

Hello,

I would like to retrieve data stored in MySQL database, so I installed
RMySQL package.
I can successfully connect with the my database using the following code

> dvr<-dbDriver("MySQL")
> con2<-dbConnect(dvr,group="exbardiv")
> mysqlDescribeConnection(con2)

<MySQLConnection:(972,0)>
  User: mmorag
  Host: localhost
  Dbname: exbardiv
  Connection type: localhost via TCP/IP
  No resultSet available

I can even see the tables in the database

> dbListTables(con2)
[1] "agoueb"    "high_ld"   "rescue"    "sjlc_info" "sjlc_ld"   "temp"

[7] "temp_snp1" "temp_snp2"

However, when I try to query the database, R breakes.

res<-dbSendQuery(con,'select * from sjlc_ld')

Can anyone help me tune up the connection between R and MySQL?

Thank you,
Marc.
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
_ _

SCRI, Invergowrie, Dundee, DD2 5DA.  
The Scottish Crop Research Institute is a charitable company limited by
guarantee. 
Registered in Scotland No: SC 29367.
Recognised by the Inland Revenue as a Scottish Charity No: SC 006662.


DISCLAIMER:\ \ This email is from the Scottish Crop
Rese...{{dropped:30}}

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

SCRI, Invergowrie, Dundee, DD2 5DA.  
The Scottish Crop Research Institute is a charitable company limited by guarantee. 
Registered in Scotland No: SC 29367.
Recognised by the Inland Revenue as a Scottish Charity No: SC 006662.


DISCLAIMER:

This email is from the Scottish Crop Research Institute, but the views 
expressed by the sender are not necessarily the views of SCRI and its 
subsidiaries.  This email and any files transmitted with it are confidential 
to the intended recipient at the e-mail address to which it has been 
addressed.  It may not be disclosed or used by any other than that addressee.
If you are not the intended recipient you are requested to preserve this 
confidentiality and you must not use, disclose, copy, print or rely on this 
e-mail in any way. Please notify postmaster at scri.ac.uk quoting the 
name of the sender and delete the email from your system.

Although SCRI has taken reasonable precautions to ensure no viruses are 
present in this email, neither the Institute nor the sender accepts any 
responsibility for any viruses, and it is your responsibility to scan the email 
and the attachments (if any).



From rvaradhan at jhmi.edu  Tue Dec 18 19:51:40 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 18 Dec 2007 13:51:40 -0500
Subject: [R] PCA - "cov.wt(z) : 'x' must contain finite values only"
In-Reply-To: <7FCC4AA5D6E2824DB8A32B8C9E76C7AB11E5D2@EXNJMB12.nam.nsroot.net>
References: <7FCC4AA5D6E2824DB8A32B8C9E76C7AB11E5D2@EXNJMB12.nam.nsroot.net>
Message-ID: <000d01c841a7$06bd9210$7c94100a@win.ad.jhu.edu>

The problem is the missing values.  The argument "na.action" is not active
in princomp(), which I think is a bug, even though the help page claims that
"factory fresh" default is na.omit.

So, you need to either get rid of the rows with any missing values in them,
or use a PCA code that can deal with missing values by somehow imputing
them.

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Johnson, Bethany 
Sent: Tuesday, December 18, 2007 1:14 PM
To: r-help at r-project.org
Subject: [R] PCA - "cov.wt(z) : 'x' must contain finite values only"

I am trying to run PCA on a matrix (the first column and row are
headers).  There are several cells with NA's.  When I run PCA with the
following code:
______________________________________
setwd("I:/PCA")
AsianProp<-read.csv("Matrix.csv", sep=",", header=T, row.names=1)
attach(AsianProp)
AsianProp
AsianProp.pca<-princomp(AsianProp, na.omit)
_____________________________________

I get the error message:

cov.wt(z) : 'x' must contain finite values only

What am I doing wrong?  

Thanks very much!

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mnevill at exitcheck.net  Tue Dec 18 20:02:18 2007
From: mnevill at exitcheck.net (Max)
Date: Tue, 18 Dec 2007 11:02:18 -0800
Subject: [R] Scatterplot3d model reporting question
Message-ID: <mn.92967d7c97244a0f.83239@exitcheck.net>

I've used the scatterplot3d function to graph some data and had it 
graph a "smooth" fit. Is there a way to actualy find out the function 
of the surface? I've looked through the help and figured out how to get 
it to report the following:

Family: gaussian
Link function: identity

Formula:
y ~ s(x, z)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  0.20750    0.01223   16.97   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Approximate significance of smooth terms:
         edf Est.rank     F  p-value
s(x,z) 8.403       17 9.729 1.76e-14 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.684   Deviance explained = 70.9%
GCV score = 0.017692   Scale est. = 0.016151  n = 108

But I'm still not really sure what I'm looking at, either that or 
"smooth" means something different than I thought. Any help would be 
great!

thanks,

-Max


From gosink at scripps.edu  Tue Dec 18 20:30:55 2007
From: gosink at scripps.edu (Mark Gosink)
Date: Tue, 18 Dec 2007 14:30:55 -0500
Subject: [R] comparing poisson distributions
Message-ID: <BD808F22EEB72143BD185545452D0A787F23A2@FLMAIL1.fl.ad.scripps.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/67ead02d/attachment.pl 

From the13thday at yahoo.com  Tue Dec 18 20:32:55 2007
From: the13thday at yahoo.com (Brad B)
Date: Tue, 18 Dec 2007 11:32:55 -0800 (PST)
Subject: [R] 3d plotting
Message-ID: <760496.43158.qm@web32902.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/1ae59ea6/attachment.pl 

From Pedro.Rodriguez at sungard.com  Tue Dec 18 20:34:15 2007
From: Pedro.Rodriguez at sungard.com (Pedro.Rodriguez at sungard.com)
Date: Tue, 18 Dec 2007 14:34:15 -0500
Subject: [R] Import GAUSS .FMT files
Message-ID: <DD14FDFE1F072D47B2F1F4D890DFB2C501E85132@VOO-EXCHANGE01.internal.sungard.corp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/93a1160c/attachment.pl 

From francogrex at mail.com  Tue Dec 18 20:35:14 2007
From: francogrex at mail.com (francogrex)
Date: Tue, 18 Dec 2007 11:35:14 -0800 (PST)
Subject: [R]  Forestplot
Message-ID: <14404133.post@talk.nabble.com>


I know there is a function forestplot from rmeta package and also the
plot.meta from the meta package and maybe others, but they are rather
complicated with extra plot parameters that I do not need and also they
process only objects created with other package functions. 
But I wonder if anyone has a much simpler function using the basic plot to
make a forestplot with only a median (or mean) and just the confidence
intervals, something like the data below in graphics. thanks

Events	2.50%	50%	97.50%
A	0.33	2.49	24.96
B	0.25	1.9	19.56
C	0.34	1.28	5.35
D	1.58	2.94	5.54
E	0.82	1.94	4.71
F	1.04	3.18	10.32
G	0.58	1.44	3.72
H	0.04	0.48	3.79
I	0.17	0.67	2.52

-- 
View this message in context: http://www.nabble.com/Forestplot-tp14404133p14404133.html
Sent from the R help mailing list archive at Nabble.com.


From scionforbai at gmail.com  Tue Dec 18 20:58:54 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Tue, 18 Dec 2007 20:58:54 +0100
Subject: [R] 3d plotting
In-Reply-To: <760496.43158.qm@web32902.mail.mud.yahoo.com>
References: <760496.43158.qm@web32902.mail.mud.yahoo.com>
Message-ID: <e9ee1f0a0712181158p1eadcd61i9f1d521e24c5547d@mail.gmail.com>

> 60,000

I hope that you actually haven't got any comma to separate the
thousands... it separates fields in a csv files (as the "Comma
Separated Values" name may suggest). If so, get rid of the commas.

>   the 3dplot function returns this error,
>  (list) object cannot be coerced to 'double'

> td<-read.csv("td.csv", header=TRUE)
> price<-read.csv("price.csv", header=TRUE)
> contractdate<-read.csv("contractdate.csv", header=TRUE)

You have to coerce the 1-column dataframes created by read.csv to
numeric vectors or to a 60000x3 dataframe.
solution 1:

myData <- cbind(td,contractdate,price)

> library(rgl)

plot3d(mydata)

solution 2:

td <- as.numeric(td)
...
price <- as.numeric(price)
plot3d(td,contractdate,price)

Bye,

ScionForbai


From caholt at u.washington.edu  Tue Dec 18 21:02:46 2007
From: caholt at u.washington.edu (Catherine A. Holt)
Date: Tue, 18 Dec 2007 12:02:46 -0800 (PST)
Subject: [R] Specifying starting values in lme (nlme package) using msScale
Message-ID: <Pine.LNX.4.43.0712181202460.11807@hymn11.u.washington.edu>


I am using package nlme and would like to specify initial values for a linear mixed-effects model to help with convergence. I am trying to specify those initial values using the msScale option under ?control? in the lme() function:

lme(Y ~ X1,  random= ~ X1|X2, control=list(msScale=lmeScale))

where, (as far as I understand), lmeScale is a function that can take initial values for parameters. However, I am unsure about how to input those starting values (e.g., what names lme will recognize for fixed and random effects, in what format, and if a partial list of initial values would be acceptable?).

Any advice or examples of code inputting starting values would be extremely helpful. I have been unable to find examples myself online. Note, although it may be easier to do this in the lme4 package, I would prefer to use nlme.

Thank you for your attention.
Sincerely, 
Carrie Holt, Ph.D., M.Sc., B.Sc.(Honours)
University of Washington
School of Aquatic & Fishery Sciences
Box 355020
Seattle, WA 98195


From jfox at mcmaster.ca  Tue Dec 18 21:04:30 2007
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 18 Dec 2007 15:04:30 -0500
Subject: [R] Scatterplot3d model reporting question
In-Reply-To: <mn.92967d7c97244a0f.83239@exitcheck.net>
References: <mn.92967d7c97244a0f.83239@exitcheck.net>
Message-ID: <000e01c841b1$34f9c4a0$9eed4de0$@ca>

Dear Max,

I'm guessing that you're actually using scatter3d() in the Rcmdr package rather than scatterplot3d(), since the latter, I believe, doesn't fit regression surfaces.

If I'm right, then as it says in ?scatter3d, the smooth surface is fit by the gam() function in the mgcv package using a smoothing spline and there is no explicit equation to examine. See ?gam for more information.

I hope this helps,
 John

--------------------------------
John Fox, Professor
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox

> -----Original Message-----
> From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-
> project.org] On Behalf Of Max
> Sent: December-18-07 2:02 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Scatterplot3d model reporting question
> 
> I've used the scatterplot3d function to graph some data and had it
> graph a "smooth" fit. Is there a way to actualy find out the function
> of the surface? I've looked through the help and figured out how to get
> it to report the following:
> 
> Family: gaussian
> Link function: identity
> 
> Formula:
> y ~ s(x, z)
> 
> Parametric coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)  0.20750    0.01223   16.97   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Approximate significance of smooth terms:
>          edf Est.rank     F  p-value
> s(x,z) 8.403       17 9.729 1.76e-14 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> R-sq.(adj) =  0.684   Deviance explained = 70.9%
> GCV score = 0.017692   Scale est. = 0.016151  n = 108
> 
> But I'm still not really sure what I'm looking at, either that or
> "smooth" means something different than I thought. Any help would be
> great!
> 
> thanks,
> 
> -Max
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dkowalske at umassd.edu  Tue Dec 18 21:06:16 2007
From: dkowalske at umassd.edu (dkowalske at umassd.edu)
Date: Tue, 18 Dec 2007 15:06:16 -0500 (EST)
Subject: [R] plotting magnitude
Message-ID: <44513.134.88.252.70.1198008376.squirrel@email.umassd.edu>

I am plotting fishing vessel positions and want these points to be
relative in size to the catch at that point.  Is this possible? I am just
begining to use R and my search of the help section didnt help in this
area.  Heres what Im using so far

xyplot(data$latdeg~data$londeg |vessek , groups=vessek,
xlim=rev(range(69:77)),ylim=(range(35:42)), data=data,
	main=list ("Mackerel catches", cex=1.0),
		ylab="latitude", notch=T, varwidth=T,
		xlab="longitude", cex.axis=0.5,)
any info would be appreciated


From the13thday at yahoo.com  Tue Dec 18 21:21:36 2007
From: the13thday at yahoo.com (Brad B)
Date: Tue, 18 Dec 2007 12:21:36 -0800 (PST)
Subject: [R] 3d plotting
In-Reply-To: <e9ee1f0a0712181158p1eadcd61i9f1d521e24c5547d@mail.gmail.com>
Message-ID: <292164.40559.qm@web32912.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/1ec8c61f/attachment.pl 

From jfox at mcmaster.ca  Tue Dec 18 21:25:17 2007
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 18 Dec 2007 15:25:17 -0500
Subject: [R] Scatterplot3d model reporting question
In-Reply-To: <mn.92967d7c97244a0f.83239@exitcheck.net>
References: <mn.92967d7c97244a0f.83239@exitcheck.net>
Message-ID: <000601c841b4$1c652ad0$552f8070$@ca>

Dear Max,

I'm guessing that you're actually using scatter3d() in the Rcmdr package rather than scatterplot3d(), since the latter, I believe, doesn't fit regression surfaces.

If I'm right, then as it says in ?scatter3d, the smooth surface is fit by the gam() function in the mgcv package using a regression spline and there is no explicit equation to examine.

I hope this helps,
 John

--------------------------------
John Fox, Professor
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox

> -----Original Message-----
> From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-
> project.org] On Behalf Of Max
> Sent: December-18-07 2:02 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Scatterplot3d model reporting question
> 
> I've used the scatterplot3d function to graph some data and had it
> graph a "smooth" fit. Is there a way to actualy find out the function
> of the surface? I've looked through the help and figured out how to get
> it to report the following:
> 
> Family: gaussian
> Link function: identity
> 
> Formula:
> y ~ s(x, z)
> 
> Parametric coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)  0.20750    0.01223   16.97   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Approximate significance of smooth terms:
>          edf Est.rank     F  p-value
> s(x,z) 8.403       17 9.729 1.76e-14 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> R-sq.(adj) =  0.684   Deviance explained = 70.9%
> GCV score = 0.017692   Scale est. = 0.016151  n = 108
> 
> But I'm still not really sure what I'm looking at, either that or
> "smooth" means something different than I thought. Any help would be
> great!
> 
> thanks,
> 
> -Max
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Tue Dec 18 21:39:36 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 18 Dec 2007 20:39:36 +0000 (UTC)
Subject: [R] Analyzing Publications from Pubmed via XML
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<a695fbee0712171238g4995040x579e58f52f83376e@mail.gmail.com>
	<Xns9A09CA51DB1E4dNOTwinscomcast@80.91.229.13>
	<Xns9A09F06AFFDDDdNOTwinscomcast@80.91.229.13>
	<a695fbee0712180702k1a351b5cxca54d45b81096166@mail.gmail.com>
Message-ID: <Xns9A0A9F55BFB52dNOTwinscomcast@80.91.229.13>

"Armin Goralczyk" <agoralczyk at gmail.com> wrote in
news:a695fbee0712180702k1a351b5cxca54d45b81096166 at mail.gmail.com: 

> On 12/18/07, David Winsemius <dwinsemius at comcast.net> wrote:
>> David Winsemius <dwinsemius at comcast.net> wrote in
>> news:Xns9A09CA51DB1E4dNOTwinscomcast at 80.91.229.13:
>>
>> > "Armin Goralczyk" <agoralczyk at gmail.com> wrote in
>> > news:a695fbee0712171238g4995040x579e58f52f83376e at mail.gmail.com:
>>
>> >> I tried the above function with simple search terms and it worked
>> >> fine for me (also more output thanks to Martin's post) but when I
>> >> use search terms attributed to certain fields, i.e. with [au] or
>> >> [ta], I get the following error message:
>> >>> pm.srch()
>> >> 1: "laryngeal neoplasms[mh]"
>> >> 2:
>>
>> > I am wondering if you used spaces, rather than "+"'s? If so then
>> > you may want your function to do more gsub-processing of the input
>> > string. 
>>
>> I tried my theory that one would need "+"'s instead of spaces, but
>> disproved it. Spaces in the input string seems to produce acceptable
>> results on my WinXP/R.2.6.1/RGui system even with more complex search
>> strings.
>>
>> --
>>
> It's not the spaces, the problem is the tag (sorry that I didn't
> specify this), or maybe the string []. I am working on a Mac OS X 10.4
> with R version 2.6. Is it maybe a string conversion problem? In the
> following warning strings in the html adress seem to be different:
> Fehler in .Call("RS_XML_ParseTree", as.character(file), handlers,
> as.logical(ignoreBlanks),  :
>  error in creating parser for
> http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&ter
> m=laryngeal neoplasms[mh]
> I/O warning : failed to load external entity
> "http%3A//eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi%3Fdb=pubme
> d&term=laryngeal%20neoplasms%5Bmh%5D" 

I do not have an up-to-date version of R on my Mac, since I have not yet 
upgraded to OSX10.4. I can try with my older version of R, but failure 
(or even success) with versions OSX-10.2/R-2.0 is not likely to be very 
informative. If you will post an example of the input that is resulting 
in the error, I can try it on my WinXP machine. If we cannot reproduce it 
there, then it may be more appropriate to take further questions to the 
Mac-R mailing list. The error message suggests to me that the fault lies 
in the connection phase of the task. 

-- 
David Winsemius


From johannes_graumann at web.de  Tue Dec 18 23:40:37 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Tue, 18 Dec 2007 23:40:37 +0100
Subject: [R] All anchored series from a vector?
Message-ID: <fk9bal$53p$1@ger.gmane.org>

Hi all,

What may be a smart, efficient way to get the following result:

myvector <- c("A","B","C","D","E")
myseries <- miracle(myvector)
myseries
[1]
[[1]] "A"
[2]
[[1]] "A" "B"
[3]
[[1]] "A" "B"
[4]
[[1]] "A" "B" "C"
[5]
[[1]] "A" "B" "C" "D"
[6]
[[1]] "A" "B" "C" "D" "E"

Thanks for any hints,

Joh


From johannes_graumann at web.de  Tue Dec 18 23:47:05 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Tue, 18 Dec 2007 23:47:05 +0100
Subject: [R] All anchored series from a vector?
References: <fk9bal$53p$1@ger.gmane.org>
Message-ID: <fk9bmp$53p$2@ger.gmane.org>

Should have been:
> myvector <- c("A","B","C","D","E")
> myseries <- miracle(myvector)
> myseries
> [1]
> [[1]] "A"
> [2]
> [[1]] "A" "B"
> [3]
> [[1]] "A" "B" "C"
> [4]
> [[1]] "A" "B" "C" "D"
> [5]
> [[1]] "A" "B" "C" "D" "E"

Sorry, Joh

Johannes Graumann wrote:

> Hi all,
> 
> What may be a smart, efficient way to get the following result:
> 
> myvector <- c("A","B","C","D","E")
> myseries <- miracle(myvector)
> myseries
> [1]
> [[1]] "A"
> [2]
> [[1]] "A" "B"
> [3]
> [[1]] "A" "B"
> [4]
> [[1]] "A" "B" "C"
> [5]
> [[1]] "A" "B" "C" "D"
> [6]
> [[1]] "A" "B" "C" "D" "E"
> 
> Thanks for any hints,
> 
> Joh
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From markleeds at verizon.net  Tue Dec 18 21:50:52 2007
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 18 Dec 2007 14:50:52 -0600 (CST)
Subject: [R] All anchored series from a vector?
Message-ID: <29947460.1591198011052527.JavaMail.root@vms062.mailsrvcs.net>

>From: Johannes Graumann <johannes_graumann at web.de>
>Date: 2007/12/18 Tue PM 04:40:37 CST
>To: r-help at stat.math.ethz.ch
>Subject: [R] All anchored series from a vector?

lapply(1:length(myvector) function(.length) {
c(myvector[1}:myvector[.length])
})

but test it because i didn't.



>Hi all,
>
>What may be a smart, efficient way to get the following result:
>
>myvector <- c("A","B","C","D","E")
>myseries <- miracle(myvector)
>myseries
>[1]
>[[1]] "A"
>[2]
>[[1]] "A" "B"
>[3]
>[[1]] "A" "B"
>[4]
>[[1]] "A" "B" "C"
>[5]
>[[1]] "A" "B" "C" "D"
>[6]
>[[1]] "A" "B" "C" "D" "E"
>
>Thanks for any hints,
>
>Joh
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From markleeds at verizon.net  Tue Dec 18 21:58:15 2007
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 18 Dec 2007 14:58:15 -0600 (CST)
Subject: [R] All anchored series from a vector?
Message-ID: <9896208.4381198011495135.JavaMail.root@vms062.mailsrvcs.net>

>From: markleeds at verizon.net
>Date: 2007/12/18 Tue PM 02:50:52 CST
>To: Johannes Graumann <johannes_graumann at web.de>
>Cc: r-help at r-project.org
>Subject: Re: [R] All anchored series from a vector?

i'm sorry. i tested it afterwards and of course
it had some problems. below is the working version.


myvector<-c("A","B","C","D","E")

result<- lapply(1:length(myvector), function(.length) {
myvector[1:.length]
})


print(result)




>>From: Johannes Graumann <johannes_graumann at web.de>
>>Date: 2007/12/18 Tue PM 04:40:37 CST
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] All anchored series from a vector?
>
>lapply(1:length(myvector) function(.length) {
>c(myvector[1}:myvector[.length])
>})
>
>but test it because i didn't.
>
>
>
>>Hi all,
>>
>>What may be a smart, efficient way to get the following result:
>>
>>myvector <- c("A","B","C","D","E")
>>myseries <- miracle(myvector)
>>myseries
>>[1]
>>[[1]] "A"
>>[2]
>>[[1]] "A" "B"
>>[3]
>>[[1]] "A" "B"
>>[4]
>>[[1]] "A" "B" "C"
>>[5]
>>[[1]] "A" "B" "C" "D"
>>[6]
>>[[1]] "A" "B" "C" "D" "E"
>>
>>Thanks for any hints,
>>
>>Joh
>>
>>______________________________________________
>>R-help at r-project.org mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dylan.beaudette at gmail.com  Tue Dec 18 22:08:28 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Tue, 18 Dec 2007 13:08:28 -0800
Subject: [R] plotting magnitude
In-Reply-To: <44513.134.88.252.70.1198008376.squirrel@email.umassd.edu>
References: <44513.134.88.252.70.1198008376.squirrel@email.umassd.edu>
Message-ID: <200712181308.28280.dylan.beaudette@gmail.com>

On Tuesday 18 December 2007, dkowalske at umassd.edu wrote:
> I am plotting fishing vessel positions and want these points to be
> relative in size to the catch at that point.  Is this possible? I am just
> begining to use R and my search of the help section didnt help in this
> area.  Heres what Im using so far
>
> xyplot(data$latdeg~data$londeg |vessek , groups=vessek,
> xlim=rev(range(69:77)),ylim=(range(35:42)), data=data,
> 	main=list ("Mackerel catches", cex=1.0),
> 		ylab="latitude", notch=T, varwidth=T,
> 		xlab="longitude", cex.axis=0.5,)
> any info would be appreciated
>

how about scaling your plotting symbols by the sqrt() of their value. or 
see ?bubble in the gstat package.

cheers,

Dylan


-- 
Dylan Beaudette
Soil Resource Laboratory
http://casoilresource.lawr.ucdavis.edu/
University of California at Davis
530.754.7341


From csardi at rmki.kfki.hu  Tue Dec 18 22:00:00 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Tue, 18 Dec 2007 22:00:00 +0100
Subject: [R] All anchored series from a vector?
In-Reply-To: <fk9bal$53p$1@ger.gmane.org>
References: <fk9bal$53p$1@ger.gmane.org>
Message-ID: <20071218205949.GA26165@localdomain>

miracle <- function(x) { lapply(seq(along=x), function(y) x[1:y]) }

Gabor

On Tue, Dec 18, 2007 at 11:40:37PM +0100, Johannes Graumann wrote:
> Hi all,
> 
> What may be a smart, efficient way to get the following result:
> 
> myvector <- c("A","B","C","D","E")
> myseries <- miracle(myvector)
> myseries
> [1]
> [[1]] "A"
> [2]
> [[1]] "A" "B"
> [3]
> [[1]] "A" "B"
> [4]
> [[1]] "A" "B" "C"
> [5]
> [[1]] "A" "B" "C" "D"
> [6]
> [[1]] "A" "B" "C" "D" "E"
> 
> Thanks for any hints,
> 
> Joh
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From johannes_graumann at web.de  Wed Dec 19 00:01:25 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Wed, 19 Dec 2007 00:01:25 +0100
Subject: [R] All anchored series from a vector?
References: <29947460.1591198011052527.JavaMail.root@vms062.mailsrvcs.net>
Message-ID: <fk9chl$a72$1@ger.gmane.org>

Debugged version:
lapply(1:length(myvector), function(.length) {
myvector[1:.length]
})

Thanks for showing the direction!

Joh

markleeds at verizon.net wrote:

>>From: Johannes Graumann <johannes_graumann at web.de>
>>Date: 2007/12/18 Tue PM 04:40:37 CST
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] All anchored series from a vector?
> 
> lapply(1:length(myvector) function(.length) {
> c(myvector[1}:myvector[.length])
> })
> 
> but test it because i didn't.
> 
> 
> 
>>Hi all,
>>
>>What may be a smart, efficient way to get the following result:
>>
>>myvector <- c("A","B","C","D","E")
>>myseries <- miracle(myvector)
>>myseries
>>[1]
>>[[1]] "A"
>>[2]
>>[[1]] "A" "B"
>>[3]
>>[[1]] "A" "B"
>>[4]
>>[[1]] "A" "B" "C"
>>[5]
>>[[1]] "A" "B" "C" "D"
>>[6]
>>[[1]] "A" "B" "C" "D" "E"
>>
>>Thanks for any hints,
>>
>>Joh
>>
>>______________________________________________
>>R-help at r-project.org mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html and provide commented,
>>minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From johannes_graumann at web.de  Wed Dec 19 00:09:04 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Wed, 19 Dec 2007 00:09:04 +0100
Subject: [R] All anchored series from a vector?
References: <fk9bal$53p$1@ger.gmane.org> <20071218205949.GA26165@localdomain>
Message-ID: <fk9d00$a72$2@ger.gmane.org>

Elegant. Thanks to you too.

Joh

Gabor Csardi wrote:

> miracle <- function(x) { lapply(seq(along=x), function(y) x[1:y]) }
> 
> Gabor
> 
> On Tue, Dec 18, 2007 at 11:40:37PM +0100, Johannes Graumann wrote:
>> Hi all,
>> 
>> What may be a smart, efficient way to get the following result:
>> 
>> myvector <- c("A","B","C","D","E")
>> myseries <- miracle(myvector)
>> myseries
>> [1]
>> [[1]] "A"
>> [2]
>> [[1]] "A" "B"
>> [3]
>> [[1]] "A" "B"
>> [4]
>> [[1]] "A" "B" "C"
>> [5]
>> [[1]] "A" "B" "C" "D"
>> [6]
>> [[1]] "A" "B" "C" "D" "E"
>> 
>> Thanks for any hints,
>> 
>> Joh
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
>


From csardi at rmki.kfki.hu  Tue Dec 18 22:10:37 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Tue, 18 Dec 2007 22:10:37 +0100
Subject: [R] All anchored series from a vector?
In-Reply-To: <fk9chl$a72$1@ger.gmane.org>
References: <29947460.1591198011052527.JavaMail.root@vms062.mailsrvcs.net>
	<fk9chl$a72$1@ger.gmane.org>
Message-ID: <20071218211037.GB26165@localdomain>

On Wed, Dec 19, 2007 at 12:01:25AM +0100, Johannes Graumann wrote:
> Debugged version:
> lapply(1:length(myvector), function(.length) {
> myvector[1:.length]
> })
> 
> Thanks for showing the direction!
> 
> Joh

Note that this fails if length(myvector)==0. 
Good to know the corner cases.

Gabor

> 
> markleeds at verizon.net wrote:
> 
> >>From: Johannes Graumann <johannes_graumann at web.de>
> >>Date: 2007/12/18 Tue PM 04:40:37 CST
> >>To: r-help at stat.math.ethz.ch
> >>Subject: [R] All anchored series from a vector?
> > 
> > lapply(1:length(myvector) function(.length) {
> > c(myvector[1}:myvector[.length])
> > })
> > 
> > but test it because i didn't.
> > 
> > 
> > 
> >>Hi all,
> >>
> >>What may be a smart, efficient way to get the following result:
> >>
> >>myvector <- c("A","B","C","D","E")
> >>myseries <- miracle(myvector)
> >>myseries
> >>[1]
> >>[[1]] "A"
> >>[2]
> >>[[1]] "A" "B"
> >>[3]
> >>[[1]] "A" "B"
> >>[4]
> >>[[1]] "A" "B" "C"
> >>[5]
> >>[[1]] "A" "B" "C" "D"
> >>[6]
> >>[[1]] "A" "B" "C" "D" "E"
> >>
> >>Thanks for any hints,
> >>
> >>Joh
> >>
> >>______________________________________________
> >>R-help at r-project.org mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html and provide commented,
> >>minimal, self-contained, reproducible code.
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From johannes_graumann at web.de  Wed Dec 19 00:09:51 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Wed, 19 Dec 2007 00:09:51 +0100
Subject: [R] All anchored series from a vector?
References: <9896208.4381198011495135.JavaMail.root@vms062.mailsrvcs.net>
Message-ID: <fk9d1f$a72$3@ger.gmane.org>

Nothing to be sorry about. You suggested a viable solution untested ... my
job to figure it out ;0)

Joh

markleeds at verizon.net wrote:

>>From: markleeds at verizon.net
>>Date: 2007/12/18 Tue PM 02:50:52 CST
>>To: Johannes Graumann <johannes_graumann at web.de>
>>Cc: r-help at r-project.org
>>Subject: Re: [R] All anchored series from a vector?
> 
> i'm sorry. i tested it afterwards and of course
> it had some problems. below is the working version.
> 
> 
> myvector<-c("A","B","C","D","E")
> 
> result<- lapply(1:length(myvector), function(.length) {
> myvector[1:.length]
> })
> 
> 
> print(result)
> 
> 
> 
> 
>>>From: Johannes Graumann <johannes_graumann at web.de>
>>>Date: 2007/12/18 Tue PM 04:40:37 CST
>>>To: r-help at stat.math.ethz.ch
>>>Subject: [R] All anchored series from a vector?
>>
>>lapply(1:length(myvector) function(.length) {
>>c(myvector[1}:myvector[.length])
>>})
>>
>>but test it because i didn't.
>>
>>
>>
>>>Hi all,
>>>
>>>What may be a smart, efficient way to get the following result:
>>>
>>>myvector <- c("A","B","C","D","E")
>>>myseries <- miracle(myvector)
>>>myseries
>>>[1]
>>>[[1]] "A"
>>>[2]
>>>[[1]] "A" "B"
>>>[3]
>>>[[1]] "A" "B"
>>>[4]
>>>[[1]] "A" "B" "C"
>>>[5]
>>>[[1]] "A" "B" "C" "D"
>>>[6]
>>>[[1]] "A" "B" "C" "D" "E"
>>>
>>>Thanks for any hints,
>>>
>>>Joh
>>>
>>>______________________________________________
>>>R-help at r-project.org mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide
>>>http://www.R-project.org/posting-guide.html and provide commented,
>>>minimal, self-contained, reproducible code.
>>
>>______________________________________________
>>R-help at r-project.org mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html and provide commented,
>>minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From luke at stat.uiowa.edu  Tue Dec 18 22:29:28 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Tue, 18 Dec 2007 15:29:28 -0600 (CST)
Subject: [R] Dual Core vs Quad Core
In-Reply-To: <Pine.LNX.4.64.0712181049380.9580@gannet.stats.ox.ac.uk>
References: <s767a34e.042@tedmail.lgc.co.uk>
	<Pine.LNX.4.64.0712181049380.9580@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0712181520470.18642@nokomis.stat.uiowa.edu>

On Tue, 18 Dec 2007, Prof Brian Ripley wrote:

> On Tue, 18 Dec 2007, S Ellison wrote:
>
>> Hiding in the windows faq is the observation that "R's computation is
>> single-threaded, and so it cannot use more than one CPU". So multi-core
>> should make no difference other than allowing R to run with less
>> interruption from other tasks. That is often a significant advantage,
>> though.
>
> Yes, but that is Windows-specific.
>
> On most other platforms you can benefit from using a multi-threaded BLAS,
> such as ATLAS, ACML or Dr Goto's.  The speedup for linear algebra can be
> substantial (although sometimes it will slow things down).  Luke Tierney
> has an experimental package to make use of parallel threads for some basic
> R computations which may appear in R 2.7.0.

There are two experimental packages available in
http://www.stat.uiowa.edu/~luke/R/experimental: pnmath, based on
OpenMP, and pnmath0, based on basic pthreads. These packages provide
parallelized versions of many of the R vectorized math functions. The
README files in these packages give more details.  OpenMP is I think
the way we want to go in the longer term; there are a few configu
issues that need sorting out and so in the interim a non OpenMP
version might be useful.

Best,

luke

>
> It should be possible to use a multi-threaded BLAS under Windows, but I
> know no one who has done it.  There is a viable pthreads implementation
> for Windows, and I've tested Luke's experimental package using it.
>
> Some compilers' runtimes will be able to use parallel threads for other
> tasks.  Since all the examples I am aware of are expensive commercial
> compilers, I suspect R will make limited use of them.  (In particular,
> base R does not use the Fortran 9x vector operations at which many of
> these features are targeted: we probably would if we routinely used such
> compilers.)
>
> I've had dual-CPU desktops for more than ten years.  Given how little
> speedup you are likely to get via parallel processing (only under ideal
> conditions do the optimized BLASes run >1.5x faster using two CPUs), the
> most effective way to make use of multiple CPUs has been to run multiple
> jobs: I typically run 3-4 at once to keep the CPUs fully used.
>
> One way to run multiple R processes to cooperate on a single task is to
> use a package such as snow to distribute the load.
>
>
>
>>>>> Andrew Perrin <clists at perrin.socsci.unc.edu> 18/12/2007 01:13 >>>
>> On Mon, 17 Dec 2007, Kitty Lee wrote:
>>
>>> Dear R-users,
>>>
>>> I use R to run spatial stuff and it takes up a lot of ram. Runs can
>> take hours or days. I am thinking of getting a new desktop. Can R take
>> advantage of the dual-core system?
>>>
>>> I have a dual-core computer at work. But it seems that right now R is
>> using only one processor.
>>>
>>> The new computers feature quad core with 3GB of RAM. Can R take
>> advantage of the 4 chips? Or am I better off getting a dual core with
>> faster processing speed per chip?
>>>
>>> Thanks! Any advice would be really appreciated!
>>>
>>> K.
>>
>> If I have my information right, R will use dual- or quad-cores if it's
>> doing two (or four) things at once. The second core will help a little
>> bit
>> insofar as whatever else your machine is doing won't interfere with the
>> one core on which it's running, but generally things that take a single
>> thread will remain on a single core.
>>
>> As for RAM, if you're doing memory-bound work you should certainly be
>> using a 64-bit machine and OS so you can utilize the larger memory
>> space.
>
> They only have 3GB of RAM, which 32-bit OSes can address.  The benefits
> really come with more than that.
>
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From racinej at mcmaster.ca  Tue Dec 18 21:30:03 2007
From: racinej at mcmaster.ca (Jeffrey S. Racine)
Date: Tue, 18 Dec 2007 15:30:03 -0500
Subject: [R] [R-pkgs] Update of the np package (version 0.14-1)
Message-ID: <1198009803.37146.29.camel@pc-racine1.mcmaster.ca>

Dear R users,

An updated version of the np package has recently been uploaded to CRAN
(version 0.14-1). 

The package is briefly described in a recent issue of Rnews (October,
2007, http://cran.r-project.org/doc/Rnews/Rnews_2007-2.pdf) for those
who might be interested.

A somewhat more detailed paper that describes the np package is
forthcoming in the Journal of Statistical Software
(http://www.jstatsoft.org) for those might be interested.

A much more thorough treatment of the subject matter can be found in Li,
Q. and J. S. Racine (2007), Nonparametric Econometrics: Theory and
Practice, Princeton University Press, ISBN: 0691121613 (768 Pages) for
those who might be interested
(http://press.princeton.edu/titles/8355.html)

Information on the np package:

This package provides a variety of nonparametric (and semiparametric)
kernel methods that seamlessly handle a mix of continuous, unordered,
and ordered factor datatypes. We would like to gratefully acknowledge
support from  the Natural Sciences and Engineering Research Council of
Canada (NSERC:www.nserc.ca), the Social Sciences and Humanities Research
Council of Canada (SSHRC:www.sshrc.ca), and the Shared Hierarchical
Academic Research Computing Network (SHARCNET:www.sharcnet.ca).

Changes from version 0.13-1 to 0.14-1:

* now use optim rather than nlm for minimisation in single index and
smooth coefficient models
* fixed bug in klein-spady objective function
* regression standard errors are now available in the case of no
  continuous variables
* summary should look prettier, print additional information
* tidied up lingering issues with out-of-sample data and conditional
modes
* fixed error when plotting asymptotic errors with conditional densities
* fixed a bug in npplot with partially linear regressions and 
plot.behavior='data' or 'plot-data'
* maximum default number of multistarts is now set to 5
* least-squares cross-validation of conditional densities uses a new,
 faster algorithm
* new, faster algorithm for least-squares cross-validation for both 
local-constant and local linear regressions.
   The estimator has changed somewhat: both cross-validation and
   the estimator use a method of shrinking towards the local constant
   estimator rather than the standard ridge approach that shrinks
   towards zero
* optimised smooth coefficient code, added ridging
* fixed bug in uniform CDF kernel
* fixed bug where npindexbw would ignore bandwidth.compute = FALSE and
   compute bandwidths when supplied with a preexisting bw object
* now can handle estimation out of discrete support.
* summary would misreport the values of discrete scale factors which
   were computed with bwscaling = TRUE

We are grateful to John Fox, Achim Zeilies, Roger Koenker, and numerous
users for their valuable feedback which resulted in an improved version
of the package.

-- Jeffrey Racine & Tristen Hayfield.

-- 
Professor J. S. Racine         Phone:  (905) 525 9140 x 23825
Department of Economics        FAX:    (905) 521-8232
McMaster University            e-mail: racinej at mcmaster.ca
1280 Main St. W.,Hamilton,     URL:
http://www.economics.mcmaster.ca/racine/
Ontario, Canada. L8S 4M4

`The generation of random numbers is too important to be left to chance'

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From psingleton at fs.fed.us  Tue Dec 18 23:06:36 2007
From: psingleton at fs.fed.us (Peter H Singleton)
Date: Tue, 18 Dec 2007 14:06:36 -0800
Subject: [R] How can I extract the AIC score from a mixed model object
 produced using lmer?
Message-ID: <OFEA565530.971E9869-ON882573B5.00784A2E-882573B5.00797442@fs.fed.us>


I am running a series of candidate mixed models using lmer (package lme4)
and I'd like to be able to compile a list of the AIC scores for those
models so that I can quickly summarize and rank the models by AIC. When I
do logistic regression, I can easily generate this kind of list by creating
the model objects using glm, and doing:

> md <- c("md1.lr", "md2.lr", "md3.lr")
> aic <- c(md1.lr$aic, md2.lr$aic, md3.lr$aic)
> aic2 <- cbind(md, aic)

but when I try to extract the AIC score from the model object produced by
lmer I get:

> md1.lme$aic
NULL
Warning message:
In md1.lme$aic : $ operator not defined for this S4 class, returning NULL

So... How do I query the AIC value out of a mixed model object created by
lmer?

<<->><<->><<->><<->><<->><<->><<->>
Peter Singleton
USFS Pacific Northwest Research Station
1133 N. Western Ave.
Wenatchee WA 98801
Phone: (509)664-1732
Fax: (509)665-8362
E-mail: psingleton at fs.fed.us


From david.barron at sbs.ox.ac.uk  Tue Dec 18 23:21:03 2007
From: david.barron at sbs.ox.ac.uk (David Barron)
Date: Tue, 18 Dec 2007 22:21:03 +0000
Subject: [R] How can I extract the AIC score from a mixed model object
	produced using lmer?
In-Reply-To: <OFEA565530.971E9869-ON882573B5.00784A2E-882573B5.00797442@fs.fed.us>
References: <OFEA565530.971E9869-ON882573B5.00784A2E-882573B5.00797442@fs.fed.us>
Message-ID: <f21f775b0712181421w36e5fe6ep5d1229ed550cc58f@mail.gmail.com>

You can calculate the AIC as follows:

(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
aic1 <- AIC(logLik(fm1))

Hope this helps.
Dave

On 12/18/07, Peter H Singleton <psingleton at fs.fed.us> wrote:
>
> I am running a series of candidate mixed models using lmer (package lme4)
> and I'd like to be able to compile a list of the AIC scores for those
> models so that I can quickly summarize and rank the models by AIC. When I
> do logistic regression, I can easily generate this kind of list by creating
> the model objects using glm, and doing:
>
> > md <- c("md1.lr", "md2.lr", "md3.lr")
> > aic <- c(md1.lr$aic, md2.lr$aic, md3.lr$aic)
> > aic2 <- cbind(md, aic)
>
> but when I try to extract the AIC score from the model object produced by
> lmer I get:
>
> > md1.lme$aic
> NULL
> Warning message:
> In md1.lme$aic : $ operator not defined for this S4 class, returning NULL
>
> So... How do I query the AIC value out of a mixed model object created by
> lmer?
>
> <<->><<->><<->><<->><<->><<->><<->>
> Peter Singleton
> USFS Pacific Northwest Research Station
> 1133 N. Western Ave.
> Wenatchee WA 98801
> Phone: (509)664-1732
> Fax: (509)665-8362
> E-mail: psingleton at fs.fed.us
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
David Barron
Said Business School                 Jesus College
Park End Street                          Oxford
Oxford OX1 1HP                          OX1 3DW
01865 288906                              01865 279684


From naiara at mail.utexas.edu  Tue Dec 18 23:27:40 2007
From: naiara at mail.utexas.edu (Naiara Pinto)
Date: Tue, 18 Dec 2007 16:27:40 -0600
Subject: [R] Random forests
Message-ID: <9d4084070712181427y620c12fan12216987613e3dc2@mail.gmail.com>

Dear all,

I would like to use a tree regression method to analyze my dataset. I
am interested in the fact that random forests creates in-bag and
out-of-bag datasets, but I also need an estimate of support for each
split. That seems hard to do in random forests since each tree is
grown using a subset of the predictor variables.

I was thinking of setting mtry = number of predictor variables,
growing several trees, and computing the support for each node as the
number of times that a certain predictor variable was chosen for that
node. Can this be implemented using random forests?

Thanks!

Naiara.

-- 
Naiara Pinto
PhD Candidate
Ecology, Evolution and Behavior
University of Texas Austin


From r.turner at auckland.ac.nz  Tue Dec 18 20:26:13 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 19 Dec 2007 08:26:13 +1300
Subject: [R] calculating the number of days from dates
In-Reply-To: <8d5a36350712180946j3e2f153bx250e9c44834ccfb1@mail.gmail.com>
References: <8d5a36350712180946j3e2f153bx250e9c44834ccfb1@mail.gmail.com>
Message-ID: <C4C59FB5-787F-4F2E-8DA0-3D22131ECF2F@auckland.ac.nz>


On 19/12/2007, at 6:46 AM, bogdan romocea wrote:

>> Sorry for using library instead package, but
>> library() is one command for using packages.
>
> ... which is why all efforts to make folks say "package" instead of >>
> "library" << are doomed to fail, IMHO.

	Yes, but it gives so much pleasure to those who appreciate
	the distinction to rail at those who don't! :-)

		cheers,

			Rolf Turner

######################################################################
Attention:\ This e-mail message is privileged and confid...{{dropped:9}}


From h.wickham at gmail.com  Wed Dec 19 00:11:05 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 18 Dec 2007 17:11:05 -0600
Subject: [R] plotting magnitude
In-Reply-To: <44513.134.88.252.70.1198008376.squirrel@email.umassd.edu>
References: <44513.134.88.252.70.1198008376.squirrel@email.umassd.edu>
Message-ID: <f8e6ff050712181511k4aa1bcbco7e5b81f1850621c8@mail.gmail.com>

On Dec 18, 2007 2:06 PM,  <dkowalske at umassd.edu> wrote:
> I am plotting fishing vessel positions and want these points to be
> relative in size to the catch at that point.  Is this possible? I am just
> begining to use R and my search of the help section didnt help in this
> area.  Heres what Im using so far
>
> xyplot(data$latdeg~data$londeg |vessek , groups=vessek,
> xlim=rev(range(69:77)),ylim=(range(35:42)), data=data,
>         main=list ("Mackerel catches", cex=1.0),
>                 ylab="latitude", notch=T, varwidth=T,
>                 xlab="longitude", cex.axis=0.5,)
> any info would be appreciated

This is pretty easy to do with the ggplot2 package:

library(ggplot2)
qplot(longdeg, latdeg, data = data, facets = . ~ vessek, size = catch)

or maybe
qplot(longdeg, latdeg, data = data, facets = . ~ vessek, size = catch)
+ scale_area()

if you want the area of the points proportional to the catch, rather
than their radius

Hadley

-- 
http://had.co.nz/


From Ionut.Florescu at stevens.edu  Wed Dec 19 00:05:35 2007
From: Ionut.Florescu at stevens.edu (Ionut Florescu)
Date: Tue, 18 Dec 2007 18:05:35 -0500
Subject: [R] Clustering Question (Support Vector Clustering)
Message-ID: <01a401c841ca$7f64a280$7e2de780$@edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/87e25665/attachment.pl 

From johannes_graumann at web.de  Wed Dec 19 02:17:28 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Wed, 19 Dec 2007 02:17:28 +0100
Subject: [R] Factor Madness
Message-ID: <fk9kgo$rr$1@ger.gmane.org>

Why is class(spectrum[["Ion"]]) after this "factor"?

spectrum <- cbind(spectrum,Ion=rep("",
nrow(spectrum)),Deviation.AMU=rep(0.0, nrow(spectrum)))

slowly going crazy ...

Joh


From GPetris at uark.edu  Wed Dec 19 00:21:42 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Tue, 18 Dec 2007 17:21:42 -0600 (CST)
Subject: [R] Multiple plots with single box
Message-ID: <200712182321.lBINLgYU027646@definetti.ddns.uark.edu>


Hello,

I am trying to display some harmonic functions in a plot. The kind of
display I have in mind is like the one that cn be obtained by a call
to plot.ts with plot.type = "multiple". The only difference is that I
want a single box containing all the plots instead of one box per
plot. I thought box(which = "outer") would have done the job, but it
didn't. 

Below is the code I have used so far. (R 2.5.1, I know, I know...)

Any help is greatly appreciated. 

Thank you in advance,
Giovanni

=================
### Plot harmonic functions
n <- 6 # even
omega <- 2 * pi / n

par(mfrow = c(n - 1, 1), mar = c(0, 5.1, 0, 5.1), oma = c(3, 1, 2, 1))
for (i in 1:(n/2 - 1)) {
    curve(cos(x * i * omega), 0, n, ylim = c(-1.1, 1.1), ylab = "", axes = FALSE)
    points(1:n, cos(i * omega * 1:n))
    axis(2); abline(h = 0, col = "lightgrey")
    curve(sin(x * i * omega), 0, n, ylim = c(-1.1, 1.1), ylab = "", axes = FALSE)
    points(1:n, sin(i * omega * 1:n))
    axis(4); abline(h = 0, col = "lightgrey")
}
curve(cos(x * (n/2) * omega), 0, n, ylim = c(-1.1, 1.1), ylab = "", axes = FALSE)
points(1:n, rep(c(-1,1), n/2))
axis(1); axis(2); abline(h = 0, col = "lightgrey")

-- 

Giovanni Petris  <GPetris at uark.edu>
Department of Mathematical Sciences
University of Arkansas - Fayetteville, AR 72701
Ph: (479) 575-6324, 575-8630 (fax)
http://definetti.uark.edu/~gpetris/


From tplate at acm.org  Wed Dec 19 00:29:15 2007
From: tplate at acm.org (Tony Plate)
Date: Tue, 18 Dec 2007 16:29:15 -0700
Subject: [R] Factor Madness
In-Reply-To: <fk9kgo$rr$1@ger.gmane.org>
References: <fk9kgo$rr$1@ger.gmane.org>
Message-ID: <476857CB.2030603@acm.org>

 From ?cbind:

Data frame methods
The cbind data frame method is just a wrapper for data.frame(..., 
check.names = FALSE). This means that it will split matrix columns in data 
frame arguments, and convert character columns to factors unless 
stringsAsFactors = TRUE is passed.

(I'm guessing 'spectrum' is a data.frame before the code fragment you've shown)

hope this helps,

Tony Plate

Johannes Graumann wrote:
> Why is class(spectrum[["Ion"]]) after this "factor"?
> 
> spectrum <- cbind(spectrum,Ion=rep("",
> nrow(spectrum)),Deviation.AMU=rep(0.0, nrow(spectrum)))
> 
> slowly going crazy ...
> 
> Joh
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.otasuke at gmail.com  Wed Dec 19 00:56:51 2007
From: r.otasuke at gmail.com (Kunio takezawa)
Date: Wed, 19 Dec 2007 08:56:51 +0900
Subject: [R] "gam()" in "gam" package
In-Reply-To: <s767a051.089@tedmail.lgc.co.uk>
References: <s767a051.089@tedmail.lgc.co.uk>
Message-ID: <4b2e15cd0712181556q4adbd9f0i5c5cec4a0081adf5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/2984fc48/attachment.pl 

From maura.monville at gmail.com  Wed Dec 19 01:23:37 2007
From: maura.monville at gmail.com (Maura E Monville)
Date: Tue, 18 Dec 2007 18:23:37 -0600
Subject: [R] leaps
In-Reply-To: <A021CCB74C25E5499726ECA1B81C175B0D1DEA59@ILS133.uopnet.plymouth.ac.uk>
References: <A021CCB74C25E5499726ECA1B81C175B0D1DEA59@ILS133.uopnet.plymouth.ac.uk>
Message-ID: <36d691950712181623mbf339acwfc7b95222be8d285@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/d05afaf9/attachment.pl 

From tplate at acm.org  Wed Dec 19 01:47:19 2007
From: tplate at acm.org (Tony Plate)
Date: Tue, 18 Dec 2007 17:47:19 -0700
Subject: [R] Factor Madness
In-Reply-To: <476857CB.2030603@acm.org>
References: <fk9kgo$rr$1@ger.gmane.org> <476857CB.2030603@acm.org>
Message-ID: <47686A17.20800@acm.org>

Whoops, it looks like there's a typo in ?cbind (R version 2.6.0 Patched 
(2007-10-11 r43143)), and I blindly copied it into my message.

That should read (emphasis added):

"and convert character columns to factors unless stringsAsFactors = 
***FALSE***"

Here's an example:

 > x <- data.frame(X=1:3)
 > sapply(cbind(x, letters[1:3]), class)
            X letters[1:3]
    "integer"     "factor"
 > sapply(cbind(x, letters[1:3], stringsAsFactors=FALSE), class)
            X letters[1:3]
    "integer"  "character"
 >

Thanks to Mark Leeds for pointing that out to me in a private message!

(I see this still in the source at 
https://svn.r-project.org/R/trunk/src/library/base/man/cbind.Rd -- is that 
the right place to look for the latest source to make sure it hasn't been 
fixed already?)

-- Tony Plate


Tony Plate wrote:
>  From ?cbind:
> 
> Data frame methods
> The cbind data frame method is just a wrapper for data.frame(..., 
> check.names = FALSE). This means that it will split matrix columns in data 
> frame arguments, and convert character columns to factors unless 
> stringsAsFactors = TRUE is passed.
> 
> (I'm guessing 'spectrum' is a data.frame before the code fragment you've shown)
> 
> hope this helps,
> 
> Tony Plate
> 
> Johannes Graumann wrote:
>> Why is class(spectrum[["Ion"]]) after this "factor"?
>>
>> spectrum <- cbind(spectrum,Ion=rep("",
>> nrow(spectrum)),Deviation.AMU=rep(0.0, nrow(spectrum)))
>>
>> slowly going crazy ...
>>
>> Joh
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From owen at stanford.edu  Wed Dec 19 01:58:08 2007
From: owen at stanford.edu (Art Owen)
Date: Tue, 18 Dec 2007 16:58:08 -0800
Subject: [R] strange timings in convolve(x,y,type="open")
Message-ID: <47686CA0.5090208@stanford.edu>

Dear R-ophiles,

I've found something very odd when I apply convolve
to ever larger vectors.  Here is an example below
with vectors ranging from 2^11 to 2^17.   There is
a funny bump up at 2^12.  Then it gets very slow at 2^16.


 >  for( i in 11:20 )print( system.time(convolve(1:2^i,1:2^i,type="o")))
   user  system elapsed
  0.002   0.000   0.002
   user  system elapsed
  0.373   0.002   0.375
   user  system elapsed
  0.014   0.001   0.016
   user  system elapsed
  0.031   0.002   0.034
   user  system elapsed
  0.126   0.004   0.130
   user  system elapsed
194.095   0.013 194.185
   user  system elapsed
  0.345   0.011   0.356

This example is run on a fedora machine with 64 bits.  I hit the same
wall at 2^16 on a Macbook (Intel processor I think).  The fedora machine
is running R 2.5.0.  That's a bit old (April 07) but I saw no mention of 
this speed
problem in some web searching, and it's not mentioned in the 2.6
what's new notes.

I've rerun it and found the same bump at 12 and wall at 16.
The timing at 2^16  can change appreciably.  In one
other case it was about 270 user, 271 elapsed.
The 2^18 case ran for hours without ever finishing.

At first I thought that this was a memory latency issue.  Maybe it
is.  But that makes it hard to explain why 2^17 works better than
2^16.  I've seen that three times now, so I'm almost ready to call it 
reproducible.
Also, one of the machines I'm using has lots of memory.  Maybe it's
a cache issue ... but that still does not explain why 2^12 is slower
than 2^13 or 2^16 is slower than 2^17.

I've checked by running convolve without type="o" and I don't
see the wall.  Similarly fft does not have that problem.

Here's an example without type="open"
 > for( k in 11:20)print(system.time( convolve( 1:2^k,1:2^k)))
   user  system elapsed
  0.001   0.000   0.000
   user  system elapsed
  0.001   0.000   0.001
   user  system elapsed
  0.002   0.000   0.002
   user  system elapsed
  0.004   0.000   0.004
   user  system elapsed
  0.009   0.001   0.010
   user  system elapsed
  0.017   0.001   0.018
   user  system elapsed
  0.138   0.005   0.143
   user  system elapsed
  0.368   0.012   0.389
   user  system elapsed
  1.010   0.032   1.051
   user  system elapsed
  1.945   0.069   2.015

This is more what I expected.  Something like N or N log(N) , with
the difference hard to discern in granularity and noise.

The convolve function is not very big (see below).  When type is
not specified, it defaults to "circular".  So my guess is that something
mysterious might be happening inside the first else clause below,
at least on some architectures.

-Art Owen



 > convolve
function (x, y, conj = TRUE, type = c("circular", "open", "filter"))
{
    type <- match.arg(type)
    n <- length(x)
    ny <- length(y)
    Real <- is.numeric(x) && is.numeric(y)
    if (type == "circular") {
        if (ny != n)
            stop("length mismatch in convolution")
    }
    else {
        n1 <- ny - 1
        x <- c(rep.int(0, n1), x)
        n <- length(y <- c(y, rep.int(0, n - 1)))
    }
    x <- fft(fft(x) * (if (conj)
        Conj(fft(y))
    else fft(y)), inv = TRUE)
    if (type == "filter")
        (if (Real)
            Re(x)
        else x)[-c(1:n1, (n - n1 + 1):n)]/n
    else (if (Real)
        Re(x)
    else x)/n
}


From r.otasuke at gmail.com  Wed Dec 19 02:10:44 2007
From: r.otasuke at gmail.com (Kunio takezawa)
Date: Wed, 19 Dec 2007 10:10:44 +0900
Subject: [R] "gam()" in "gam" package
In-Reply-To: <s767a051.089@tedmail.lgc.co.uk>
References: <s767a051.089@tedmail.lgc.co.uk>
Message-ID: <4b2e15cd0712181710k68e930bdi938587229f6b6438@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/2151f79c/attachment.pl 

From m_olshansky at yahoo.com  Wed Dec 19 02:28:49 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Tue, 18 Dec 2007 17:28:49 -0800 (PST)
Subject: [R] accessing dimension names
In-Reply-To: <da3995450712180301s4ffd0a33u635bb6f85f0f84db@mail.gmail.com>
Message-ID: <22596.76035.qm@web32203.mail.mud.yahoo.com>

To access your dimension idx you could do either
assign("a",paste("dimnames(y)$x",idx,sep=""))
or
eval(parse(text=paste("a<-dimnames(y)$x",idx,sep="")))

--- born.to.b.wyld at gmail.com wrote:

> I have a matrix y:
> 
> > dimnames(y)
> $x93
> [1] "1" "2"
> 
> $x94
> [1] "0" "1" "2"
> .................. so on  (there are other
> dimensions as well)
> 
> 
> 
> I need to access a particular dimension, but a
> random mechanism tells me
> which dimension it would. So, sometimes I might need
> to access
> dimnames(y)$x93, some other time it would be
> dimnames(y)$x94.. and so on.
> Now let that random dimension be idx, then
> dimnames(y)$paste('x',idx,sep='')
> doesn't work.
> 
> Can anyone help?
> 
> Thanks!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From elw at stderr.org  Wed Dec 19 02:33:57 2007
From: elw at stderr.org (elw at stderr.org)
Date: Tue, 18 Dec 2007 19:33:57 -0600 (CST)
Subject: [R] creating a database
In-Reply-To: <14375875.post@talk.nabble.com>
References: <14375875.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0712181927090.22020@illuminati.stderr.org>



> Can a simple matrix be used for this or would it be better and more 
> efficient to create an external database to hold the data.  If so, 
> should the database be created using C and how would I do this (seeing 
> as that I have never programmed in C)?

You don't want to be down at the C level, most likely:  it would be much 
more straightforward and programmer-efficient to use one of the available 
bindings to one of the available open-source databases.

R has useful / usable bindings to postgresql, sqlite, and mysql, among 
many others.

These are, however, more generally useful when you reach the point that 
you simply can't manage the volume of your data in R objects or in data 
frames. [And, well, you can go a LONG way with intelligently named R 
objects.  :-)]

--elijah


From MP.Sylvestre at epimgh.mcgill.ca  Wed Dec 19 03:24:32 2007
From: MP.Sylvestre at epimgh.mcgill.ca (Marie Pierre Sylvestre)
Date: Tue, 18 Dec 2007 21:24:32 -0500
Subject: [R] assigning and saving datasets in a loop,
	with names changing with	"i"
Message-ID: <WorldClient-F200712182124.AA24320054@epimgh.mcgill.ca>

Dear R users,

I am analysing a very large data set and I need to perform several data
manipulations. The dataset is so big that the only way I can play with it
without having memory problems (E.g. "cannot allocate vectors of size...")
is to write a batch script to:

1. cut the data into pieces 
2. save the pieces in seperate .RData files
3. Remove everything from the environment
4. load one of the piece
5. perform the manipulations on it
6. save it and remove it from the environment
7. Redo 4-6 for every piece
8. Merge everything together at the end

It works if coded line by line but since I'll have to perform these tasks
on other data sets, I am trying to automate this as much as I can. 

I am using a loop in which I used 'assign' and 'get' (pseudo code below).
My problem is when I use 'get', it prints the whole object on the screen.
I am wondering whether there is a more efficient way to do what I need to
do. Any help would be appreciated. Please keep in mind that the whole
process is quite computer-intensive, so I can't keep everything in the
environment while R performs calculations.

Say I have 1 big dataframe called data. I use 'split' to divide it into a
list of 12 dataframes (call this list my.list)

my.fun is a function that takes a dataframe, performs several
manipulations on it and returns a dataframe.


for (i in 1:12){
  assign( paste( "data", i, sep=""),  my.fun(my.list[i]))   # this works
  # now I need to save this new object as a RData. 

  # The following line does not work
  save(paste("data", i, sep = ""),  file = paste(  paste("data", i, sep =
""), "RData", sep="."))
}

  # This works but it is a bit convoluted!!!
  temp <- get(paste("data", i, sep = ""))
  save(temp,  file = "lala.RData")
}


I am *sure* there is something more clever to do but I can't find it. Any
help would be appreciated.

best regards,

MP


From bcarvalh at jhsph.edu  Wed Dec 19 03:33:31 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 18 Dec 2007 21:33:31 -0500
Subject: [R] assigning and saving datasets in a loop,
	with names changing with	"i"
In-Reply-To: <WorldClient-F200712182124.AA24320054@epimgh.mcgill.ca>
References: <WorldClient-F200712182124.AA24320054@epimgh.mcgill.ca>
Message-ID: <D0BC3168-54FA-4DDC-882A-BABE2BC3D3AD@jhsph.edu>

you want to use:

save(list=paste("data", i, sep=""), file=paste("data", i, ".Rdata",  
sep=""))

b

On Dec 18, 2007, at 9:24 PM, Marie Pierre Sylvestre wrote:

> Dear R users,
>
> I am analysing a very large data set and I need to perform several  
> data
> manipulations. The dataset is so big that the only way I can play  
> with it
> without having memory problems (E.g. "cannot allocate vectors of  
> size...")
> is to write a batch script to:
>
> 1. cut the data into pieces
> 2. save the pieces in seperate .RData files
> 3. Remove everything from the environment
> 4. load one of the piece
> 5. perform the manipulations on it
> 6. save it and remove it from the environment
> 7. Redo 4-6 for every piece
> 8. Merge everything together at the end
>
> It works if coded line by line but since I'll have to perform these  
> tasks
> on other data sets, I am trying to automate this as much as I can.
>
> I am using a loop in which I used 'assign' and 'get' (pseudo code  
> below).
> My problem is when I use 'get', it prints the whole object on the  
> screen.
> I am wondering whether there is a more efficient way to do what I  
> need to
> do. Any help would be appreciated. Please keep in mind that the whole
> process is quite computer-intensive, so I can't keep everything in the
> environment while R performs calculations.
>
> Say I have 1 big dataframe called data. I use 'split' to divide it  
> into a
> list of 12 dataframes (call this list my.list)
>
> my.fun is a function that takes a dataframe, performs several
> manipulations on it and returns a dataframe.
>
>
> for (i in 1:12){
>  assign( paste( "data", i, sep=""),  my.fun(my.list[i]))   # this  
> works
>  # now I need to save this new object as a RData.
>
>  # The following line does not work
>  save(paste("data", i, sep = ""),  file = paste(  paste("data", i,  
> sep =
> ""), "RData", sep="."))
> }
>
>  # This works but it is a bit convoluted!!!
>  temp <- get(paste("data", i, sep = ""))
>  save(temp,  file = "lala.RData")
> }
>
>
> I am *sure* there is something more clever to do but I can't find  
> it. Any
> help would be appreciated.
>
> best regards,
>
> MP
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m_olshansky at yahoo.com  Wed Dec 19 05:45:50 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Tue, 18 Dec 2007 20:45:50 -0800 (PST)
Subject: [R] read.table() and precision?
In-Reply-To: <Pine.LNX.4.64.0712181002150.8324@gannet.stats.ox.ac.uk>
Message-ID: <325252.62059.qm@web32203.mail.mud.yahoo.com>

Thank you for your response!

'write.table' writes up to 15 decimal digits which is
not the machine (double) precision but not far from
that - sorry for the misleading comments!

After all I found a way to do what I needed without
using disk or much memory and doing only twice as much
work as I could with unlimited memory, so I will stick
to this approach.

--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Mon, 17 Dec 2007, Moshe Olshansky wrote:
> 
> > Dear List,
> >
> > Following the below question I have a question of
> my
> > own:
> > Suppose that I have large matrices which are
> produced
> > sequentially and must be used sequentially in the
> > reverse order. I do not have enough memory to
> store
> > them and so I would like to write them to disk and
> > then read them. This raises two questions:
> > 1) what is the fastest (and the most economic
> > space-wise) way to do this?
> 
> Using save/load is the simplest.  Don't worry about
> finding better 
> solutions until you know those are not good enough. 
> (serialize / 
> unserialize is another interface to the same
> underlying idea.)
> 
> > 2) functions like write, write.table, etc. write
> the
> > data the way it is printed and this may result in
> a
> > loss of accuracy. Is there any way to prevent
> this,
> > except for setting the "digits" option to a higher
> > value or using format prior to writing the data?
> 
> Do please read the help before making false claims.
> ?write.table says
> 
>       Real and complex numbers are written to the
> maximal possible
>       precision.
> 
> OTOH, ?write says it is a wrapper for cat, whose
> help says
> 
>       'cat' converts numeric/complex elements in the
> same way as 'print'
>       (and not in the same way as 'as.character'
> which is used by the S
>       equivalent), so 'options' '"digits"' and
> '"scipen"' are relevant.
>       However, it uses the minimum field width
> necessary for each
>       element, rather than the same field width for
> all elements.
> 
> so this hints as.character() might be a useful
> preprocessor.
> 
> > Is it possible to write binary files (similar to
> Fortran)?
> 
> See ?writeBin.  save/load by default write binary
> files, but use of 
> writeBin can be faster (and less flexible).
> 
> > Any suggestion will be greatly appreciated.
> 
> Somehow you have missed a great deal of information
> about R I/O.
> Try help.start() and reading the sections the search
> engine shows you 
> that look relevant.
> 
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
>


From cberry at tajo.ucsd.edu  Wed Dec 19 05:50:12 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 18 Dec 2007 20:50:12 -0800
Subject: [R] strange timings in convolve(x,y,type="open")
In-Reply-To: <47686CA0.5090208@stanford.edu>
References: <47686CA0.5090208@stanford.edu>
Message-ID: <Pine.LNX.4.64.0712182037280.10486@tajo.ucsd.edu>

On Tue, 18 Dec 2007, Art Owen wrote:

> Dear R-ophiles,
>
> I've found something very odd when I apply convolve
> to ever larger vectors.  Here is an example below
> with vectors ranging from 2^11 to 2^17.   There is
> a funny bump up at 2^12.  Then it gets very slow at 2^16.

The time is consumed by fft, which is called with vectors of length 
2*2^i-1 when type = 'o'

> system.time( fft(seq_len(2^13-1)) )
    user  system elapsed
    0.31    0.00    0.32
> system.time( fft(seq_len(2^14-1)) )
    user  system elapsed
       0       0       0
>


There are no factors of 2^13-1 or 2^17-1 or 2^18-1

> for (i in 11:20 ) print( c(index=i, nfact=length(which( 0 == 
(2^(i+1)-1)%%(2:trunc(sqrt(2^(i+1)-1)) )))))
index nfact
    11    11
index nfact
    12     0
index nfact
    13     3
index nfact
    14     3
index nfact
    15     7
index nfact
    16     0
index nfact
    17    15
index nfact
    18     0
index nfact
    19    23
index nfact
    20     5
>

It looks like the code in fft.c tries to find factors of the series length 
and works from there.

So, the size of the problem evidently depends on its factors.

HTH,

Chuck


>
>
> >  for( i in 11:20 )print( system.time(convolve(1:2^i,1:2^i,type="o")))
>   user  system elapsed
>  0.002   0.000   0.002
>   user  system elapsed
>  0.373   0.002   0.375
>   user  system elapsed
>  0.014   0.001   0.016
>   user  system elapsed
>  0.031   0.002   0.034
>   user  system elapsed
>  0.126   0.004   0.130
>   user  system elapsed
> 194.095   0.013 194.185
>   user  system elapsed
>  0.345   0.011   0.356
>
> This example is run on a fedora machine with 64 bits.  I hit the same
> wall at 2^16 on a Macbook (Intel processor I think).  The fedora machine
> is running R 2.5.0.  That's a bit old (April 07) but I saw no mention of
> this speed
> problem in some web searching, and it's not mentioned in the 2.6
> what's new notes.
>
> I've rerun it and found the same bump at 12 and wall at 16.
> The timing at 2^16  can change appreciably.  In one
> other case it was about 270 user, 271 elapsed.
> The 2^18 case ran for hours without ever finishing.
>
> At first I thought that this was a memory latency issue.  Maybe it
> is.  But that makes it hard to explain why 2^17 works better than
> 2^16.  I've seen that three times now, so I'm almost ready to call it
> reproducible.
> Also, one of the machines I'm using has lots of memory.  Maybe it's
> a cache issue ... but that still does not explain why 2^12 is slower
> than 2^13 or 2^16 is slower than 2^17.
>
> I've checked by running convolve without type="o" and I don't
> see the wall.  Similarly fft does not have that problem.
>
> Here's an example without type="open"
> > for( k in 11:20)print(system.time( convolve( 1:2^k,1:2^k)))
>   user  system elapsed
>  0.001   0.000   0.000
>   user  system elapsed
>  0.001   0.000   0.001
>   user  system elapsed
>  0.002   0.000   0.002
>   user  system elapsed
>  0.004   0.000   0.004
>   user  system elapsed
>  0.009   0.001   0.010
>   user  system elapsed
>  0.017   0.001   0.018
>   user  system elapsed
>  0.138   0.005   0.143
>   user  system elapsed
>  0.368   0.012   0.389
>   user  system elapsed
>  1.010   0.032   1.051
>   user  system elapsed
>  1.945   0.069   2.015
>
> This is more what I expected.  Something like N or N log(N) , with
> the difference hard to discern in granularity and noise.
>
> The convolve function is not very big (see below).  When type is
> not specified, it defaults to "circular".  So my guess is that something
> mysterious might be happening inside the first else clause below,
> at least on some architectures.
>
> -Art Owen
>
>
>
> > convolve
> function (x, y, conj = TRUE, type = c("circular", "open", "filter"))
> {
>    type <- match.arg(type)
>    n <- length(x)
>    ny <- length(y)
>    Real <- is.numeric(x) && is.numeric(y)
>    if (type == "circular") {
>        if (ny != n)
>            stop("length mismatch in convolution")
>    }
>    else {
>        n1 <- ny - 1
>        x <- c(rep.int(0, n1), x)
>        n <- length(y <- c(y, rep.int(0, n - 1)))
>    }
>    x <- fft(fft(x) * (if (conj)
>        Conj(fft(y))
>    else fft(y)), inv = TRUE)
>    if (type == "filter")
>        (if (Real)
>            Re(x)
>        else x)[-c(1:n1, (n - n1 + 1):n)]/n
>    else (if (Real)
>        Re(x)
>    else x)/n
> }
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From gallon.li at gmail.com  Wed Dec 19 07:59:38 2007
From: gallon.li at gmail.com (gallon li)
Date: Wed, 19 Dec 2007 14:59:38 +0800
Subject: [R] plot cummulative sum from calendar time
Message-ID: <54f7e7c30712182259x2251a8ben8fb37c13ccf7824f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/c407e0bd/attachment.pl 

From owen at stanford.edu  Wed Dec 19 08:13:33 2007
From: owen at stanford.edu (owen at stanford.edu)
Date: Tue, 18 Dec 2007 23:13:33 -0800
Subject: [R] strange timings in convolve(x,y,type="open")
In-Reply-To: <Pine.LNX.4.64.0712182037280.10486@tajo.ucsd.edu>
References: <47686CA0.5090208@stanford.edu>
	<Pine.LNX.4.64.0712182037280.10486@tajo.ucsd.edu>
Message-ID: <20071218231333.hk4f0yupjkcc8c0s@webmail.stanford.edu>


Thanks Charles.  That must be it.  (Berwin also
noticed this.)

When convolve hit the wall, I switched over
to FFTW in C.  That is actually pretty nice
code which runs in n log(n) even for prime n
and takes special account of factors of n up
to about 19 or so.  So if the R team ever wants
to put in a new FFT, that looks like a good one.

But I think easier fix for me, or others in this boat,
would just be to write a new convolve(x,y) that
pads x and y with zeros out to length
2*max( length(x), length(y) ).  Then if x and y
have very composite lengths, especially powers of
2, the fft should go quickly.

-Art



Quoting "Charles C. Berry" <cberry at tajo.ucsd.edu>:

> On Tue, 18 Dec 2007, Art Owen wrote:
>
>> Dear R-ophiles,
>>
>> I've found something very odd when I apply convolve
>> to ever larger vectors.  Here is an example below
>> with vectors ranging from 2^11 to 2^17.   There is
>> a funny bump up at 2^12.  Then it gets very slow at 2^16.
>
> The time is consumed by fft, which is called with vectors of length
> 2*2^i-1 when type = 'o'
>
>> system.time( fft(seq_len(2^13-1)) )
>    user  system elapsed
>    0.31    0.00    0.32
>> system.time( fft(seq_len(2^14-1)) )
>    user  system elapsed
>       0       0       0
>>
>
>
> There are no factors of 2^13-1 or 2^17-1 or 2^18-1
>
>> for (i in 11:20 ) print( c(index=i, nfact=length(which( 0 ==
> (2^(i+1)-1)%%(2:trunc(sqrt(2^(i+1)-1)) )))))
> index nfact
>    11    11
> index nfact
>    12     0
> index nfact
>    13     3
> index nfact
>    14     3
> index nfact
>    15     7
> index nfact
>    16     0
> index nfact
>    17    15
> index nfact
>    18     0
> index nfact
>    19    23
> index nfact
>    20     5
>>
>
> It looks like the code in fft.c tries to find factors of the series
> length and works from there.
>
> So, the size of the problem evidently depends on its factors.
>
> HTH,
>
> Chuck
>
>
>>
>>
>>>  for( i in 11:20 )print( system.time(convolve(1:2^i,1:2^i,type="o")))
>>  user  system elapsed
>> 0.002   0.000   0.002
>>  user  system elapsed
>> 0.373   0.002   0.375
>>  user  system elapsed
>> 0.014   0.001   0.016
>>  user  system elapsed
>> 0.031   0.002   0.034
>>  user  system elapsed
>> 0.126   0.004   0.130
>>  user  system elapsed
>> 194.095   0.013 194.185
>>  user  system elapsed
>> 0.345   0.011   0.356
>>
>> This example is run on a fedora machine with 64 bits.  I hit the same
>> wall at 2^16 on a Macbook (Intel processor I think).  The fedora machine
>> is running R 2.5.0.  That's a bit old (April 07) but I saw no mention of
>> this speed
>> problem in some web searching, and it's not mentioned in the 2.6
>> what's new notes.
>>
>> I've rerun it and found the same bump at 12 and wall at 16.
>> The timing at 2^16  can change appreciably.  In one
>> other case it was about 270 user, 271 elapsed.
>> The 2^18 case ran for hours without ever finishing.
>>
>> At first I thought that this was a memory latency issue.  Maybe it
>> is.  But that makes it hard to explain why 2^17 works better than
>> 2^16.  I've seen that three times now, so I'm almost ready to call it
>> reproducible.
>> Also, one of the machines I'm using has lots of memory.  Maybe it's
>> a cache issue ... but that still does not explain why 2^12 is slower
>> than 2^13 or 2^16 is slower than 2^17.
>>
>> I've checked by running convolve without type="o" and I don't
>> see the wall.  Similarly fft does not have that problem.
>>
>> Here's an example without type="open"
>>> for( k in 11:20)print(system.time( convolve( 1:2^k,1:2^k)))
>>  user  system elapsed
>> 0.001   0.000   0.000
>>  user  system elapsed
>> 0.001   0.000   0.001
>>  user  system elapsed
>> 0.002   0.000   0.002
>>  user  system elapsed
>> 0.004   0.000   0.004
>>  user  system elapsed
>> 0.009   0.001   0.010
>>  user  system elapsed
>> 0.017   0.001   0.018
>>  user  system elapsed
>> 0.138   0.005   0.143
>>  user  system elapsed
>> 0.368   0.012   0.389
>>  user  system elapsed
>> 1.010   0.032   1.051
>>  user  system elapsed
>> 1.945   0.069   2.015
>>
>> This is more what I expected.  Something like N or N log(N) , with
>> the difference hard to discern in granularity and noise.
>>
>> The convolve function is not very big (see below).  When type is
>> not specified, it defaults to "circular".  So my guess is that something
>> mysterious might be happening inside the first else clause below,
>> at least on some architectures.
>>
>> -Art Owen
>>
>>
>>
>>> convolve
>> function (x, y, conj = TRUE, type = c("circular", "open", "filter"))
>> {
>>   type <- match.arg(type)
>>   n <- length(x)
>>   ny <- length(y)
>>   Real <- is.numeric(x) && is.numeric(y)
>>   if (type == "circular") {
>>       if (ny != n)
>>           stop("length mismatch in convolution")
>>   }
>>   else {
>>       n1 <- ny - 1
>>       x <- c(rep.int(0, n1), x)
>>       n <- length(y <- c(y, rep.int(0, n - 1)))
>>   }
>>   x <- fft(fft(x) * (if (conj)
>>       Conj(fft(y))
>>   else fft(y)), inv = TRUE)
>>   if (type == "filter")
>>       (if (Real)
>>           Re(x)
>>       else x)[-c(1:n1, (n - n1 + 1):n)]/n
>>   else (if (Real)
>>       Re(x)
>>   else x)/n
>> }
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> Charles C. Berry                            (858) 534-2098
>                                             Dept of   
> Family/Preventive Medicine
> E mailto:cberry at tajo.ucsd.edu	            UC San Diego
> http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From eugen_pircalabelu at yahoo.com  Wed Dec 19 08:39:55 2007
From: eugen_pircalabelu at yahoo.com (eugen pircalabelu)
Date: Tue, 18 Dec 2007 23:39:55 -0800 (PST)
Subject: [R] 4 questions regarding hypothesis testing, survey package,
	ts on samples, plotting
Message-ID: <187205.27181.qm@web38605.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/91c3db02/attachment.pl 

From johannes_graumann at web.de  Wed Dec 19 11:48:30 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Wed, 19 Dec 2007 11:48:30 +0100
Subject: [R] Factor Madness
References: <fk9kgo$rr$1@ger.gmane.org> <476857CB.2030603@acm.org>
	<47686A17.20800@acm.org>
Message-ID: <fkalvf$f6b$1@ger.gmane.org>

Yep, but I figured that out quite fast ;0)
Thanks for giving me a hand ... you want believe a many times I skimmed the
cbind help without actually seeing this ... well, it was 0:30 ...

Thanks again, Joh

Tony Plate wrote:

> Whoops, it looks like there's a typo in ?cbind (R version 2.6.0 Patched
> (2007-10-11 r43143)), and I blindly copied it into my message.
> 
> That should read (emphasis added):
> 
> "and convert character columns to factors unless stringsAsFactors =
> ***FALSE***"
> 
> Here's an example:
> 
>  > x <- data.frame(X=1:3)
>  > sapply(cbind(x, letters[1:3]), class)
>             X letters[1:3]
>     "integer"     "factor"
>  > sapply(cbind(x, letters[1:3], stringsAsFactors=FALSE), class)
>             X letters[1:3]
>     "integer"  "character"
>  >
> 
> Thanks to Mark Leeds for pointing that out to me in a private message!
> 
> (I see this still in the source at
> https://svn.r-project.org/R/trunk/src/library/base/man/cbind.Rd -- is that
> the right place to look for the latest source to make sure it hasn't been
> fixed already?)
> 
> -- Tony Plate
> 
> 
> Tony Plate wrote:
>>  From ?cbind:
>> 
>> Data frame methods
>> The cbind data frame method is just a wrapper for data.frame(...,
>> check.names = FALSE). This means that it will split matrix columns in
>> data frame arguments, and convert character columns to factors unless
>> stringsAsFactors = TRUE is passed.
>> 
>> (I'm guessing 'spectrum' is a data.frame before the code fragment you've
>> shown)
>> 
>> hope this helps,
>> 
>> Tony Plate
>> 
>> Johannes Graumann wrote:
>>> Why is class(spectrum[["Ion"]]) after this "factor"?
>>>
>>> spectrum <- cbind(spectrum,Ion=rep("",
>>> nrow(spectrum)),Deviation.AMU=rep(0.0, nrow(spectrum)))
>>>
>>> slowly going crazy ...
>>>
>>> Joh
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html and provide commented,
>>> minimal, self-contained, reproducible code.
>>>
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From martin.tomko at geo.uzh.ch  Wed Dec 19 10:17:15 2007
From: martin.tomko at geo.uzh.ch (Martin Tomko)
Date: Wed, 19 Dec 2007 10:17:15 +0100
Subject: [R] median of binned values
Message-ID: <4768E19B.8090204@geo.uzh.ch>

Dear list,
I have a vector (array, table row, whatever is best) of frequency values 
for categories (or bins), and I need to find the median category. 
Trivial to do by hand, but I was wondering if there is a means to  do it 
in R in an elegant way.

The obvious medioan(vector) returns the median frequency for the binns, 
and that is not what I want. i.e,:
             freq
cat1    1
cat2   10  
cat3   100  
cat4   1000
cat5   10000

I want it to return cat5, instead of cat3.

Thanks a lot
Martin


From agoralczyk at gmail.com  Wed Dec 19 10:22:38 2007
From: agoralczyk at gmail.com (Armin Goralczyk)
Date: Wed, 19 Dec 2007 10:22:38 +0100
Subject: [R] Analyzing Publications from Pubmed via XML
In-Reply-To: <Xns9A0A9F55BFB52dNOTwinscomcast@80.91.229.13>
References: <bd93cdad0712131803t22eed96bk9fd2010b3aa745a9@mail.gmail.com>
	<971536df0712131842j604d59f5xe29541fde4626822@mail.gmail.com>
	<4762080F.8070606@fhcrc.org>
	<bd93cdad0712141216s23071d27n17d87a487ad06950@mail.gmail.com>
	<Xns9A077F740B4A0dNOTwinscomcast@80.91.229.13>
	<a695fbee0712171238g4995040x579e58f52f83376e@mail.gmail.com>
	<Xns9A09CA51DB1E4dNOTwinscomcast@80.91.229.13>
	<Xns9A09F06AFFDDDdNOTwinscomcast@80.91.229.13>
	<a695fbee0712180702k1a351b5cxca54d45b81096166@mail.gmail.com>
	<Xns9A0A9F55BFB52dNOTwinscomcast@80.91.229.13>
Message-ID: <a695fbee0712190122y1263c6b8p73962d8e86e2cf58@mail.gmail.com>

On Dec 18, 2007 9:39 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> "Armin Goralczyk" <agoralczyk at gmail.com> wrote in
> news:a695fbee0712180702k1a351b5cxca54d45b81096166 at mail.gmail.com:
>
> > It's not the spaces, the problem is the tag (sorry that I didn't
> > specify this), or maybe the string []. I am working on a Mac OS X 10.4
> > with R version 2.6. Is it maybe a string conversion problem? In the
> > following warning strings in the html adress seem to be different:
> > Fehler in .Call("RS_XML_ParseTree", as.character(file), handlers,
> > as.logical(ignoreBlanks),  :
> >  error in creating parser for
> > http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&ter
> > m=laryngeal neoplasms[mh]
> > I/O warning : failed to load external entity
> > "http%3A//eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi%3Fdb=pubme
> > d&term=laryngeal%20neoplasms%5Bmh%5D"
>
> I do not have an up-to-date version of R on my Mac, since I have not yet
> upgraded to OSX10.4. I can try with my older version of R, but failure
> (or even success) with versions OSX-10.2/R-2.0 is not likely to be very
> informative. If you will post an example of the input that is resulting
> in the error, I can try it on my WinXP machine. If we cannot reproduce it
> there, then it may be more appropriate to take further questions to the
> Mac-R mailing list. The error message suggests to me that the fault lies
> in the connection phase of the task.
>


Example:

> esearch <- function (term){
+ 	srch.stem <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"
+ 	srch.mode <- "db=pubmed&retmax=10000&retmode=xml&term="
+ 	doc <-xmlTreeParse(paste(srch.stem,srch.mode,term,sep=""),isURL = TRUE,
+ 		useInternalNodes = TRUE)
+ 	sapply(c("//Id"), xpathApply, doc = doc, fun = xmlValue)
+ 	}
>
> term <- 'meyer'
> pmid <- esearch(term) #search successfull
>
> term <- 'meyer[au]'
> pmid <- esearch(term)
Fehler in .Call("RS_XML_ParseTree", as.character(file), handlers,
as.logical(ignoreBlanks),  :
  error in creating parser for
http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmax=10000&retmode=xml&term=meyer[au]
>
I/O warning : failed to load external entity
"http%3A//eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi%3Fdb=pubmed&retmax=10000&retmode=xml&term=meyer%5Bau%5D"
>

Seems to be a Mac problem. I will post on the Mac-R mailing list.
Thanks
-- 
Armin


From Richard.Cotton at hsl.gov.uk  Wed Dec 19 10:23:04 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Wed, 19 Dec 2007 09:23:04 +0000
Subject: [R] Different labels by panel in barchart
Message-ID: <OF4C1A6466.345BD6D1-ON802573B5.00660E3E-802573B6.00338775@hsl.gov.uk>

Dear all,

I'm analysing a survey, and creating a barchart of the different responses 
for each question.  The questions are grouped according to a number of 
categories, so I'm using lattice to create a plot with each question in a 
category on it.  The problem is that the response set for different 
questions within the same category varies.  I want to be able to draw only 
the relevant bars, without spaces for nonrelevant reposnes.  An example 
may make it clearer:

testdf = data.frame(y=c(23,34,45, 56), x=factor(c("a", "a", "b", "c")), 
g=factor(c(1,2,1,2)))
barchart(y~x|g, data=testdf)

#In the first panel I would like two columns "a" and "b", positioned at 1 
and 2 respectively; in the second I would again like two columns at 1 and 
2, this time labelled "a" and "c".

barchart(y~x|g, data=testdf, scales=list(x=list(relation="free")), 
drop.unused.levels=TRUE)

#This is closer, but puts the columns in the wrong place in the second 
panel.

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From colinr23 at gmail.com  Wed Dec 19 10:24:24 2007
From: colinr23 at gmail.com (Colin Robertson)
Date: Wed, 19 Dec 2007 01:24:24 -0800
Subject: [R] using rcorr.cens for Goodman Kruskal gamma
Message-ID: <4768e34a.26ba720a.474e.ffffb66d@mx.google.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/db700271/attachment.pl 

From r.hankin at noc.soton.ac.uk  Wed Dec 19 10:26:44 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 19 Dec 2007 09:26:44 +0000
Subject: [R] array addition
Message-ID: <3972DD82-222B-4640-B299-62D9BBF0D9F4@noc.soton.ac.uk>

Hi

suppose I have two arrays x1,x2 of dimensions a1,b1,c1 and
a2,b2,c2 respectively.

I want  x = x1   "+"   x2 with dimensions c(max(a1,a2), max(b1,b2),max 
(c1,c2))

with

x[a,b,c] = x1[a1,b1,c1] + x2[a2,b2,c2] if    a <=min(a1,a2) , b<=min 
(b1,b2), c<=min(c1,c2)

and the other bits either x1 or x2 or zero according to whether the  
coordinates
are "in range" for x1 or x2 or neither.

The answer has to work for arbitrary-dimensioned arrays.

toy example follows (matrices):


 > x1
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    3    5    7    9
[2,]    2    4    6    8   10
 > x2
      [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6    9
 > x
      [,1] [,2] [,3] [,4] [,5]
[1,]    2    7   12    7    9
[2,]    4    9   14    8   10
[3,]    3    6    9    0    0
 >


Note the zeros at lower-right.


Is there a ready-made solution to this?





--
Robin Hankin
Uncertainty Analyst and Neutral Theorist,
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From jim at bitwrit.com.au  Wed Dec 19 10:36:50 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 19 Dec 2007 20:36:50 +1100
Subject: [R] bar plot colors
In-Reply-To: <794058.27400.qm@web32815.mail.mud.yahoo.com>
References: <794058.27400.qm@web32815.mail.mud.yahoo.com>
Message-ID: <4768E632.2060609@bitwrit.com.au>

John Kane wrote:
> I think you're going to find that barchart with that
> many values in a bar is going to be pretty well
> uninterpretable.  
> 
> Jim Lemon gives the desired barchart but it is very
> difficult to read.  
> 
> Stealing his code to create the same matrix I'd
> suggest may be looking at a dotchart.  I'm not sure if
> this is even close to an optimal solution but I do
> think it's a bit better than a barchart approach
> ======================================================
> heights<-matrix(sample(10:70,54),ncol=3)
> bar.colors<-rep(rep(2:7,each=3),3)
> cost.types <- c("Direct", "Indirec", "Induced")
> colnames(heights) <-  c("A", "B", "C")
> rownames(heights) <- c(rep(cost.types, 6))
> 
> dotchart(heights, col=bar.colors, pch=16, cex=.6)
> 
> =======================================================
Very nice John, but it does look like someone dropped the M&Ms.
Jim


From ccleland at optonline.net  Wed Dec 19 10:33:11 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 19 Dec 2007 04:33:11 -0500
Subject: [R] median of binned values
In-Reply-To: <4768E19B.8090204@geo.uzh.ch>
References: <4768E19B.8090204@geo.uzh.ch>
Message-ID: <4768E557.9000404@optonline.net>

Martin Tomko wrote:
> Dear list,
> I have a vector (array, table row, whatever is best) of frequency values 
> for categories (or bins), and I need to find the median category. 
> Trivial to do by hand, but I was wondering if there is a means to  do it 
> in R in an elegant way.
> 
> The obvious medioan(vector) returns the median frequency for the binns, 
> and that is not what I want. i.e,:
>              freq
> cat1    1
> cat2   10  
> cat3   100  
> cat4   1000
> cat5   10000
> 
> I want it to return cat5, instead of cat3.

df <- data.frame(binname = as.factor(paste("cat", 1:5, sep="")),
                 freq = c(1,10,100,1000,10000))

df
  binname  freq
1    cat1     1
2    cat2    10
3    cat3   100
4    cat4  1000
5    cat5 10000

with(df, levels(binname)[median(rep(as.numeric(binname), freq))])
[1] "cat5"

> Thanks a lot
> Martin
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From gavin.simpson at ucl.ac.uk  Wed Dec 19 10:39:17 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 19 Dec 2007 09:39:17 +0000
Subject: [R] Random forests
In-Reply-To: <9d4084070712181427y620c12fan12216987613e3dc2@mail.gmail.com>
References: <9d4084070712181427y620c12fan12216987613e3dc2@mail.gmail.com>
Message-ID: <1198057157.3095.12.camel@graptoleberis.geog.ucl.ac.uk>

On Tue, 2007-12-18 at 16:27 -0600, Naiara Pinto wrote:
> Dear all,
> 
> I would like to use a tree regression method to analyze my dataset. I
> am interested in the fact that random forests creates in-bag and
> out-of-bag datasets, but I also need an estimate of support for each
> split. That seems hard to do in random forests since each tree is
> grown using a subset of the predictor variables.
> 
> I was thinking of setting mtry = number of predictor variables,
> growing several trees, and computing the support for each node as the
> number of times that a certain predictor variable was chosen for that
> node. Can this be implemented using random forests?

Hi Naiara,

I'm so not an expert here, but what you propose with mty = number of
predictors will give you a procedure known as bagging.

You talk about support for the split and then for the node. Is this just
a typo or are you interested in the two different things?

I'm not aware of how you do the latter in bagging or random forests as
the whole point is to grow large trees not pruned ones. As to the
former, trees are unstable, change the data used to train them just a
little and you can get a very different fitted tree.

Bagging and random forests exploit this to produce a better prediction
machine / classifier by using n poor trees rather than one best tree.
They do this by adding randomness to the procedure by bootstrap sampling
the training data, and in the case of random forest, randomly sampling a
small number, mtry, of available predictors to grow each tree. As such
there is no correspondence between the splits of one tree and the splits
of another, so trying to compare how many times a certain split in one
or more trees is formed by the same predictor. So it doesn't make sense
(to me it may to others) to focus on individual splits in the n trees.

I don't know what you mean exactly by "support", but if you are trying
to get a measure of how important each of your predictors is in
explaining variance in your response, then take a look at the
importance() function in the randomForest package. This produces a
couple of measures that allow you to determine which predictors
contribute most to reducing node impurity or MSE.

HTH

G

> 
> Thanks!
> 
> Naiara.
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From rh at family-krueger.com  Wed Dec 19 10:44:00 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Wed, 19 Dec 2007 10:44:00 +0100
Subject: [R] GLM and factor in forular
In-Reply-To: <000301c8417f$b28b3a20$2f01a8c0@coresystem>
References: <000301c8417f$b28b3a20$2f01a8c0@coresystem>
Message-ID: <4768E7E0.70701@family-krueger.com>

Mark Leeds schrieb:
> That's because the first factor is being used as the "baseline". This is
> explained in any intro to GLMs text.
>
>
>   
Thank you Mark, I didn't found that hint.

Knut


From paul.bennett at vudu.com  Tue Dec 18 18:32:07 2007
From: paul.bennett at vudu.com (Paul Bennett)
Date: Tue, 18 Dec 2007 09:32:07 -0800
Subject: [R] R on EEE PC
In-Reply-To: <mailman.19.1197975603.7850.r-help@r-project.org>
References: <mailman.19.1197975603.7850.r-help@r-project.org>
Message-ID: <47680417.2020601@vudu.com>

Someone asked about R on the EEE PC. That was the second thing
I installed when I received mine (the first being the full KDE desktop).
I don't recall any issues with the installation. I used the standard debian
package (the native linux on the EEE is Xandros, a derivation of debian).
The caveat being that since Xandros isn't *exactly* debian, there might
be some issues, but I haven't run into any yet. Note that I haven't tried
anything particularly taxing, so take this with a grain of salt.

R seems to run fine, the only problem I've encountered so far has been
that since the screen size is 800x480, plots are longer than the window, so
you only see part of them. I haven't tried changing the default plot size
(I assume that can be done).

It does get warm to the touch quickly, but nothing I found unbearable.
I can't report on battery life, since I haven't really put it through 
its paces
yet -- maybe I'll report again after Christmas at the inlaws ... ;)


- Paul Bennett
Software Engineer
Vudu, Inc
paul.bennett at vudu.com


From vincent.nijs at gmail.com  Wed Dec 19 04:33:53 2007
From: vincent.nijs at gmail.com (Vincent Nijs)
Date: Tue, 18 Dec 2007 21:33:53 -0600
Subject: [R] Execute R-code from vim on Mac OS X
In-Reply-To: <89dc9acd-ee09-4d7f-a297-aaf5e1c71696@a35g2000prf.googlegroups.com>
References: <89dc9acd-ee09-4d7f-a297-aaf5e1c71696@a35g2000prf.googlegroups.com>
Message-ID: <7b6b9e400712181933n1f7ffb08m9c64e4ab73068f06@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071218/a91b138c/attachment.pl 

From hello123456world at gmail.com  Wed Dec 19 08:27:45 2007
From: hello123456world at gmail.com (R R)
Date: Wed, 19 Dec 2007 15:27:45 +0800
Subject: [R] prop.trend.test() and Cochran-Armitage Trend test
Message-ID: <ba4f00ae0712182327n6a0b21aex1d7539f050e2c8b4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/568361b9/attachment.pl 

From reilly at stat.auckland.ac.nz  Wed Dec 19 10:50:00 2007
From: reilly at stat.auckland.ac.nz (James Reilly)
Date: Wed, 19 Dec 2007 22:50:00 +1300
Subject: [R] 4 questions regarding hypothesis testing, survey package,
 ts on samples, plotting
In-Reply-To: <187205.27181.qm@web38605.mail.mud.yahoo.com>
References: <187205.27181.qm@web38605.mail.mud.yahoo.com>
Message-ID: <4768E948.8060601@stat.auckland.ac.nz>

On 19/12/07 8:39 PM, eugen pircalabelu wrote:
> 2. How can i find the median of a variable in survey package?
> a<-c(1:10)
> b<-sample(1:20, 10, replace=T)
> b1<-sample(0:1, 10, replace=T)
> c<-data.frame(a,b, b1)
> library(survey)
> design<-svydesign(id=~1, data=c)
> svymean(~b, design)
> 
> svymean gives me the mean, but what function gives me the svymedian, and what function gives me the svyproportion  for b1? 
> I want to know the median for a metric variable and the proportion for a binomial variable ?

The svyquantile function will give you the median, e.g.
svyquantile(~b, design, 0.5, interval.type="score")

Since a proportion is just the mean of a 0/1 variable, you can use 
svymean for that, e.g.
svymean(~b1, design)

HTH,
James
-- 
James Reilly
Department of Statistics, University of Auckland
Private Bag 92019, Auckland, New Zealand


From gavin.simpson at ucl.ac.uk  Wed Dec 19 11:01:52 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 19 Dec 2007 10:01:52 +0000
Subject: [R] plot cummulative sum from calendar time
In-Reply-To: <54f7e7c30712182259x2251a8ben8fb37c13ccf7824f@mail.gmail.com>
References: <54f7e7c30712182259x2251a8ben8fb37c13ccf7824f@mail.gmail.com>
Message-ID: <1198058512.3075.2.camel@graptoleberis.geog.ucl.ac.uk>

On Wed, 2007-12-19 at 14:59 +0800, gallon li wrote:
> I have the following list of observations of calendar time:
> 
>  [1] 03-Nov-1997 09-Oct-1991 27-Aug-1992 01-Jul-1994 19-Jan-1990 12-Nov-1993
>  [7] 08-Oct-1993 10-Nov-1982 08-Dec-1986 23-Dec-1987 02-Aug-1995 20-Oct-1998
> [13] 29-Apr-1991 16-Mar-1994 20-May-1991 28-Dec-1987 14-Jul-1999 27-Nov-1998
> [19] 09-Sep-1999 26-Aug-1999 20-Jun-1997 05-May-1995 26-Mar-1998 15-Aug-1994
> [25] 24-Jun-1996 02-Oct-1996 12-Aug-1985 24-Jun-1992 09-Jan-1991 04-Mar-1988
> 3230 Levels: 01-Apr-1987 01-Apr-1990 01-Apr-1991 01-Apr-1996 ... 31-Oct-1998
> 
> I want to make a plot where x-axis is the calendar time and y-axix is the
> cumulative sum of observations observed on or before that dates.

Is this what you want?

## some example data
dat <- cumsum(rnorm(100))
start.end <- as.Date(c("01/01/2007", "31/12/2007"), format = "%d/%m/%Y")
dates <- seq(start.end[1], start.end[2], length = 100)

## plot the data
plot(dat ~ dates, type = "l")

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From martin.tomko at geo.uzh.ch  Wed Dec 19 11:17:48 2007
From: martin.tomko at geo.uzh.ch (Martin Tomko)
Date: Wed, 19 Dec 2007 11:17:48 +0100
Subject: [R] median of binned values
In-Reply-To: <4768E557.9000404@optonline.net>
References: <4768E19B.8090204@geo.uzh.ch> <4768E557.9000404@optonline.net>
Message-ID: <4768EFCC.1010606@geo.uzh.ch>

Thank you, Chuck,
would you mind commenting a bit on the code, it is not all clear... HOw 
would you go to retrieve only the numeric value (not the category name)?
I am just starting with R, and the functionality of replicate and levels 
is not quite clear. I tried the documentation, but am not any wiser. 
What if I had a vector v <- vector(c(1,10,100,1000,10000)) and wanted to 
perform it on that?

Thanks a lot
Martin


Chuck Cleland wrote:
> Martin Tomko wrote:
>> Dear list,
>> I have a vector (array, table row, whatever is best) of frequency values 
>> for categories (or bins), and I need to find the median category. 
>> Trivial to do by hand, but I was wondering if there is a means to  do it 
>> in R in an elegant way.
>>
>> The obvious medioan(vector) returns the median frequency for the binns, 
>> and that is not what I want. i.e,:
>>              freq
>> cat1    1
>> cat2   10  
>> cat3   100  
>> cat4   1000
>> cat5   10000
>>
>> I want it to return cat5, instead of cat3.
> 
> df <- data.frame(binname = as.factor(paste("cat", 1:5, sep="")),
>                  freq = c(1,10,100,1000,10000))
> 
> df
>   binname  freq
> 1    cat1     1
> 2    cat2    10
> 3    cat3   100
> 4    cat4  1000
> 5    cat5 10000
> 
> with(df, levels(binname)[median(rep(as.numeric(binname), freq))])
> [1] "cat5"
> 
>> Thanks a lot
>> Martin
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code. 
> 

-- 
Martin Tomko
Postdoctoral Research Assistant

Geographic Information Systems Division
Department of Geography
University of Zurich - Irchel
Winterthurerstr. 190
CH-8057 Zurich, Switzerland

email: 	martin.tomko at geo.uzh.ch
site:	http://www.geo.uzh.ch/~mtomko
mob: 	+41-788 629 558
tel: 	+41-44-6355256
fax: 	+41-44-6356848


From tutanga at gmail.com  Wed Dec 19 11:24:25 2007
From: tutanga at gmail.com (lubaroz)
Date: Wed, 19 Dec 2007 02:24:25 -0800 (PST)
Subject: [R]  scale estimation for Gamma distribution
Message-ID: <14414551.post@talk.nabble.com>


Hey,
I make a regression for Gamma distribution with log link, in R and in SAS.
In R, the dispersion is estimated by 
\phi=Deviance/(#_of_observations),
In SAS, there are two options: 
\phi=Deviance/(#_of_observations-#_of_params) or
\phi=Pearson/(#_of_observations-#_of_params).

I understand that SAS formulae are correct, however, the coefficients,
obtained by R and by the first version of SAS, are equal, and the difference
is only in standard errors of betas.
Could anyone explain me the phenomenon? Maybe, there is smth. about the
Gamma distribution?
-- 
View this message in context: http://www.nabble.com/scale-estimation-for-Gamma-distribution-tp14414551p14414551.html
Sent from the R help mailing list archive at Nabble.com.


From n.lapidus at gmail.com  Wed Dec 19 11:29:16 2007
From: n.lapidus at gmail.com (N. Lapidus)
Date: Wed, 19 Dec 2007 11:29:16 +0100
Subject: [R] array addition
In-Reply-To: <3972DD82-222B-4640-B299-62D9BBF0D9F4@noc.soton.ac.uk>
References: <3972DD82-222B-4640-B299-62D9BBF0D9F4@noc.soton.ac.uk>
Message-ID: <70a3a4850712190229t4e605ffegd69ae09b71a5504f@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20071219/a02f77af/attachment.pl 

From ericlecoutre at gmail.com  Wed Dec 19 11:36:57 2007
From: ericlecoutre at gmail.com (Eric Lecoutre)
Date: Wed, 19 Dec 2007 11:36:57 +0100
Subject: [R] Obtaining replicates numbers of a vector
Message-ID: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/bc123728/attachment.pl 

From jim at bitwrit.com.au  Wed Dec 19 12:05:20 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 19 Dec 2007 22:05:20 +1100
Subject: [R] plotting magnitude
In-Reply-To: <44513.134.88.252.70.1198008376.squirrel@email.umassd.edu>
References: <44513.134.88.252.70.1198008376.squirrel@email.umassd.edu>
Message-ID: <4768FAF0.8050305@bitwrit.com.au>

dkowalske at umassd.edu wrote:
> I am plotting fishing vessel positions and want these points to be
> relative in size to the catch at that point.  Is this possible? I am just
> begining to use R and my search of the help section didnt help in this
> area.  Heres what Im using so far
> 
> xyplot(data$latdeg~data$londeg |vessek , groups=vessek,
> xlim=rev(range(69:77)),ylim=(range(35:42)), data=data,
> 	main=list ("Mackerel catches", cex=1.0),
> 		ylab="latitude", notch=T, varwidth=T,
> 		xlab="longitude", cex.axis=0.5,)
> any info would be appreciated
> 
Hi,

What you might want is something like this:

library(maps)
data(world2MapEnv)
map("world2",xlim=c(110,160),ylim=c(-45,-10))
axis(1)
axis(2)
library(plotrix)
draw.circle(155,-30,1,col="red")
draw.circle(155,-32,1.2,col="blue")
draw.circle(153,-34,1.5,col="green")

where 1, 1.2, 1.5 are the relative sizes of catch.
Oh, and you would probably want the North Atlantic part of the map...

Jim


From yn19832 at msn.com  Wed Dec 19 12:04:53 2007
From: yn19832 at msn.com (livia)
Date: Wed, 19 Dec 2007 03:04:53 -0800 (PST)
Subject: [R]  Calculate remainer
Message-ID: <14414906.post@talk.nabble.com>


Hello everyone,

I have got a question about a simple calculation. If I would like to
calculate 50/12 and return the result as 4 and the remainer 2. Is there a
function of doing this?

Many thanks.
-- 
View this message in context: http://www.nabble.com/Calculate-remainer-tp14414906p14414906.html
Sent from the R help mailing list archive at Nabble.com.


From ccleland at optonline.net  Wed Dec 19 12:21:15 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 19 Dec 2007 06:21:15 -0500
Subject: [R] Calculate remainer
In-Reply-To: <14414906.post@talk.nabble.com>
References: <14414906.post@talk.nabble.com>
Message-ID: <4768FEAB.8080509@optonline.net>

livia wrote:
> Hello everyone,
> 
> I have got a question about a simple calculation. If I would like to
> calculate 50/12 and return the result as 4 and the remainer 2. Is there a
> function of doing this?
> 
> Many thanks.

?"%%" to see how to get the remainder.  You might put the "result" and
remainder together in a function like this:

myfunc <- function(x,y){list(result = floor(x / y), remainder = x %% y)}

myfunc(x=50, y=12)
$result
[1] 4

$remainder
[1] 2

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From nitish at imtech.res.in  Wed Dec 19 12:16:56 2007
From: nitish at imtech.res.in (Nitish Kumar Mishra)
Date: Wed, 19 Dec 2007 16:46:56 +0530 (IST)
Subject: [R] Help me
Message-ID: <52000.59.160.112.38.1198063016.squirrel@webmail.imtech.res.in>

Hello R Users,

    I am interested in using R to generate quantitative structure-activity
relationships (QSARs) for small molecules given a set of molecular
descriptors and biological data tab-delimited or excel file. In which fist
value colum of each row is biological value and rest all are its
descriptors.

    Has anyone done this using R ? Would you be willing to share your R
scripts (or ideas) for doing this with me ?
I am particularly interested in R codes for optimizing variable selection as
well as validation of the generated QSAR equations, since
I have ~ 1500 molecular descriptors per molecule. How I can use R to find
the optimal (smallest) set of descriptors to fit the data ?
If you have any idea about this using other software the please also
suggest me.
I am a new user of R. I am trying to use R for this. Please reply me at
this mail ID because I have not joined R help group.
Thanking you
-- 
Nitish Kumar Mishra
SRF, BIC, IMTECH,
Chandigarh, India
E-Mail Address:
nitish_km at yahoo.com
nitish at imtech.res.in


From Richard.Cotton at hsl.gov.uk  Wed Dec 19 12:24:22 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Wed, 19 Dec 2007 11:24:22 +0000
Subject: [R] array addition
In-Reply-To: <3972DD82-222B-4640-B299-62D9BBF0D9F4@hsl.gov.uk>
Message-ID: <OF26D7C90C.C0898BAC-ON802573B6.003E42F9-802573B6.003EA279@hsl.gov.uk>

Try this:

"%add%" <- function(x1, x2)
  {
    dim1 <- dim(x1)
    dim2 <- dim(x2)

    seq1 <- list();for(i in 1:length(dim1)) seq1[[i]]=seq(dim1[i])
    filter1 <- paste(seq1, collapse=",") 
    cmd1 <- paste("out[", filter1, "]", sep="")
    seq2 <- list();for(i in 1:length(dim2)) seq2[[i]]=seq(dim2[i])
    filter2 <- paste(seq2, collapse=",")
    cmd2 <- paste("out[", filter2, "]", sep="")

    out <- array(0, dim=pmax(dim1,dim2)) 
    eval(parse(text=paste(cmd1, "<-", cmd1, "+ x1")))
    eval(parse(text=paste(cmd2, "<-", cmd2, "+ x2")))
    out
  }

x1 <- array(round(5*runif(30)), dim=c(2,3,5))
x2 <- array(round(5*runif(24)), dim=c(3,4,2))
x <- x1 %add% x2

You probably want to add a check to make the dimensions the same, and the 
code needs tidying, but you get the idea.

Regards,
Richie.

Mathematical Sciences Unit
HSL


r-help-bounces at r-project.org wrote on 19/12/2007 09:26:44:

> Hi
> 
> suppose I have two arrays x1,x2 of dimensions a1,b1,c1 and
> a2,b2,c2 respectively.
> 
> I want  x = x1   "+"   x2 with dimensions c(max(a1,a2), max(b1,b2),max 
> (c1,c2))
> 
> with
> 
> x[a,b,c] = x1[a1,b1,c1] + x2[a2,b2,c2] if    a <=min(a1,a2) , b<=min 
> (b1,b2), c<=min(c1,c2)
> 
> and the other bits either x1 or x2 or zero according to whether the 
> coordinates
> are "in range" for x1 or x2 or neither.
> 
> The answer has to work for arbitrary-dimensioned arrays.
> 
> toy example follows (matrices):
> 
> 
>  > x1
>       [,1] [,2] [,3] [,4] [,5]
> [1,]    1    3    5    7    9
> [2,]    2    4    6    8   10
>  > x2
>       [,1] [,2] [,3]
> [1,]    1    4    7
> [2,]    2    5    8
> [3,]    3    6    9
>  > x
>       [,1] [,2] [,3] [,4] [,5]
> [1,]    2    7   12    7    9
> [2,]    4    9   14    8   10
> [3,]    3    6    9    0    0
>  >
> 
> 
> Note the zeros at lower-right.
> 
> 
> Is there a ready-made solution to this?
> 
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst and Neutral Theorist,
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From wwwhsd at gmail.com  Wed Dec 19 12:25:39 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 19 Dec 2007 09:25:39 -0200
Subject: [R] Obtaining replicates numbers of a vector
In-Reply-To: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>
References: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>
Message-ID: <da79af330712190325r799d3607g6ff6a8086f2c36bf@mail.gmail.com>

Try this:

replicate <- vector("numeric", len=length(x))
replicate[order(x)] <- unlist(sapply(rle(sort(x))$lengths, seq_len))

On 19/12/2007, Eric Lecoutre <ericlecoutre at gmail.com> wrote:
> Dear R-help,
>
> I am trying to have a generic way to assess the replicates in a character
> vector.
> Say that I have the following vector:
>
> x <- c('A','B','A','C','C','B')
>
> I would like to obtain:
>
> replicates <- c(1,1,2,1,2,2)
>
> each number beeing the time we see the corresponding value in x.
>
> Any clever and generic way to obtain that?
>
> Eric
>
>
>
>
> --
> Eric Lecoutre
> Consultant - Business & Decision
> Business Intelligence & Customer Intelligence
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From wwwhsd at gmail.com  Wed Dec 19 12:25:39 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 19 Dec 2007 09:25:39 -0200
Subject: [R] Obtaining replicates numbers of a vector
In-Reply-To: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>
References: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>
Message-ID: <da79af330712190325r799d3607g6ff6a8086f2c36bf@mail.gmail.com>

Try this:

replicate <- vector("numeric", len=length(x))
replicate[order(x)] <- unlist(sapply(rle(sort(x))$lengths, seq_len))

On 19/12/2007, Eric Lecoutre <ericlecoutre at gmail.com> wrote:
> Dear R-help,
>
> I am trying to have a generic way to assess the replicates in a character
> vector.
> Say that I have the following vector:
>
> x <- c('A','B','A','C','C','B')
>
> I would like to obtain:
>
> replicates <- c(1,1,2,1,2,2)
>
> each number beeing the time we see the corresponding value in x.
>
> Any clever and generic way to obtain that?
>
> Eric
>
>
>
>
> --
> Eric Lecoutre
> Consultant - Business & Decision
> Business Intelligence & Customer Intelligence
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From wwwhsd at gmail.com  Wed Dec 19 12:25:39 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 19 Dec 2007 09:25:39 -0200
Subject: [R] Obtaining replicates numbers of a vector
In-Reply-To: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>
References: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>
Message-ID: <da79af330712190325r799d3607g6ff6a8086f2c36bf@mail.gmail.com>

Try this:

replicate <- vector("numeric", len=length(x))
replicate[order(x)] <- unlist(sapply(rle(sort(x))$lengths, seq_len))

On 19/12/2007, Eric Lecoutre <ericlecoutre at gmail.com> wrote:
> Dear R-help,
>
> I am trying to have a generic way to assess the replicates in a character
> vector.
> Say that I have the following vector:
>
> x <- c('A','B','A','C','C','B')
>
> I would like to obtain:
>
> replicates <- c(1,1,2,1,2,2)
>
> each number beeing the time we see the corresponding value in x.
>
> Any clever and generic way to obtain that?
>
> Eric
>
>
>
>
> --
> Eric Lecoutre
> Consultant - Business & Decision
> Business Intelligence & Customer Intelligence
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From j.knowles at manchester.ac.uk  Wed Dec 19 12:43:59 2007
From: j.knowles at manchester.ac.uk (Joshua Knowles)
Date: Wed, 19 Dec 2007 11:43:59 +0000
Subject: [R] randomForest() for regression produces offset predictions
Message-ID: <200712191143.59625.j.knowles@manchester.ac.uk>

Hi all,
 
I have observed that when using the randomForest package to do regression, the 
predicted values of the dependent variable given by a trained forest are not 
centred and have the wrong slope when plotted against the true values.
 
This means that the R^2 value obtained by squaring the Pearson correlation are 
better than those obtained by computing the coefficient of determination 
directly. The R^2 value obtained by squaring the Pearson can, however, be 
exactly reproduced by the coeff. of det. if the predicted values are first 
linearly transformed (using lm() to find the required intercept and slope).
 
Does anyone know why the randomForest behaves in this way - producing offset 
predictions? Does anyone know a fix for the problem?
 
(By the way, the feature is there even if the original dependent variable 
values are initially transformed to have zero mean and unit variance.)
 
As an example, here is some simple R code that uses the available swiss 
dataset to show the effect I am observing.

Thanks for any help.
 
--
#### EXAMPLE OF RANDOM FOREST REGRESSION
 
library(randomForest)
data(swiss)
swiss
 
#Build the random forest to predict Infant Mortality
rf.rf<-randomForest(Infant.Mortality ~ ., data=swiss)
 
#And predict the training set again
pred<-c(predict(rf.rf,swiss))
actual<-swiss$Infant.Mortality
 
#Plotting predicted against actual values shows the effect (uncomment to see
this)
#plot(pred,actual)
#abline(0,1)
 
# calculate R^2 as pearson coefficient squared
R2one<-cor(pred,actual)^2
 
# calculate R^2 value as fraction of variance explained
residOpt<-(actual-pred)
residnone<-(actual-mean(actual))
R2two<-1-var(residOpt,na.rm=TRUE)/var(residnone, na.rm=TRUE)
 
# now fit a line through the predicted and true values and
# use this to normalize the data before calculating R^2
 
fit<-lm(actual ~ pred)
coef(fit)
pred2<-pred*coef(fit)[2]+coef(fit)[1]
residOpt<-(actual-pred2)
R2three<-1-var(residOpt,na.rm=TRUE)/var(residnone, na.rm=TRUE)
 
cat("Pearson squared = ",R2one,"\n")
cat("Coeff of determination = ", R2two, "\n")
cat("Coeff of determination after linear fitting = ", R2three, "\n")
 
## END
 

-- 
Joshua Knowles .. j.knowles at manchester.ac.uk
BBSRC David Phillips Fellow
School of Computer Science
The University of Manchester
http://dbkgroup.org/knowles/


From ccleland at optonline.net  Wed Dec 19 12:47:59 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 19 Dec 2007 06:47:59 -0500
Subject: [R] median of binned values
In-Reply-To: <4768EFCC.1010606@geo.uzh.ch>
References: <4768E19B.8090204@geo.uzh.ch> <4768E557.9000404@optonline.net>
	<4768EFCC.1010606@geo.uzh.ch>
Message-ID: <476904EF.8070506@optonline.net>

Martin Tomko wrote:
> Thank you, Chuck,
> would you mind commenting a bit on the code, it is not all clear... HOw 
> would you go to retrieve only the numeric value (not the category name)?
> I am just starting with R, and the functionality of replicate and levels 
> is not quite clear. I tried the documentation, but am not any wiser. 
> What if I had a vector v <- vector(c(1,10,100,1000,10000)) and wanted to 
> perform it on that?
> 
> Thanks a lot
> Martin

  Retrieve the numeric value rather than the category name as follows:

with(df, freq[median(rep(as.numeric(binname), freq))])
[1] 10000

  To do essentially the same thing with a vector:

myvec <- c(1,10,100,1000,10000)

myvec[median(rep(1:length(myvec), myvec))]
[1] 10000

  I'm sure I cannot explain levels() and rep() any better than the help
pages for those functions.

> Chuck Cleland wrote:
>> Martin Tomko wrote:
>>> Dear list,
>>> I have a vector (array, table row, whatever is best) of frequency values 
>>> for categories (or bins), and I need to find the median category. 
>>> Trivial to do by hand, but I was wondering if there is a means to  do it 
>>> in R in an elegant way.
>>>
>>> The obvious medioan(vector) returns the median frequency for the binns, 
>>> and that is not what I want. i.e,:
>>>              freq
>>> cat1    1
>>> cat2   10  
>>> cat3   100  
>>> cat4   1000
>>> cat5   10000
>>>
>>> I want it to return cat5, instead of cat3.
>> df <- data.frame(binname = as.factor(paste("cat", 1:5, sep="")),
>>                  freq = c(1,10,100,1000,10000))
>>
>> df
>>   binname  freq
>> 1    cat1     1
>> 2    cat2    10
>> 3    cat3   100
>> 4    cat4  1000
>> 5    cat5 10000
>>
>> with(df, levels(binname)[median(rep(as.numeric(binname), freq))])
>> [1] "cat5"
>>
>>> Thanks a lot
>>> Martin
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.  

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From dgvirtual at akl.lt  Wed Dec 19 12:51:22 2007
From: dgvirtual at akl.lt (Donatas G.)
Date: Wed, 19 Dec 2007 13:51:22 +0200
Subject: [R] recode based on filter
Message-ID: <200712191351.24056.dgvirtual@akl.lt>

Hi, I have a data frame DATA, which (simplified of course) looks like this:

know1 = c("Y","N","N","Y","N","N","Y","Y","N")
par1=c(1,4,5,3,3,2,3,3,5)
know2 = c("Y","Y","N","Y","N","N","N","Y","Y")
par2=c(3,4,4,3,5,2,4,3,2)
DATA=data.frame(know1,par1,know2,par2)

it represents answers in a questionnaire, where respondents evaluate two 
things (par1 and par2) but they also indicate, whether they have detailed 
knowledge about those two things (know1 and know2).

I need to test correlations between parameters par1 and par2, but need to do 
that separately for cases where the respondents know about both things and 
where they do not know about both things. If, testing correlations of those 
who do know, a respondent knows only about item1 and not item2 (if the 
responces do not match), the values of par1 and par2 should be changed to 
NA's. The same goes if the respondent says he does not know only about one 
thing.

SO: Before doing analysis, I need to transform the DATA dataframe in two ways:

Testing correlations of those evaluating with knowledge:

> DATA
  know1 par1 know2 par2
1     Y    1     Y    3
2     N    NA    Y    NA
3     N    NA    N    NA
4     Y    3     Y    3
5     N    NA    N    NA
6     N    NA    N    NA
7     Y    NA    N    NA
8     Y    3     Y    3
9     N    NA    Y    NA

And the case of those evaluating without knowledge:

> DATA
  know1 par1 know2 par2
1     Y    NA    Y    NA
2     N    NA    Y    NA
3     N    5     N    4
4     Y    NA    Y    NA
5     N    3     N    5
6     N    2     N    2
7     Y    NA    N    NA
8     Y    NA    Y    NA
9     N    NA    Y    NA

Yes, yes, this example is stupid, but...

I know how to filter records, I know how to recode, but I need some 
combination of the two here and I am lost.. Any ideas?

-- 
Donatas Glodenis
http://dg.lapas.info


From ckc at sanger.ac.uk  Wed Dec 19 13:06:42 2007
From: ckc at sanger.ac.uk (Celine Carret)
Date: Wed, 19 Dec 2007 12:06:42 -0000
Subject: [R] FW: cgh package
Message-ID: <0526C4B4E593154B86E6A8651913C28D019BE4E9@exchsrv2.internal.sanger.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/6ad75b13/attachment.pl 

From wwwhsd at gmail.com  Wed Dec 19 13:12:16 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 19 Dec 2007 10:12:16 -0200
Subject: [R] recode based on filter
In-Reply-To: <200712191351.24056.dgvirtual@akl.lt>
References: <200712191351.24056.dgvirtual@akl.lt>
Message-ID: <da79af330712190412x7508c3e5q15f714c1d1a9ca4d@mail.gmail.com>

Perhaps you can try subset the data:

sapply(levels(DATA$know1), function(x)subset(DATA, (know1==x &
know2==x)), simplify=F)

On 19/12/2007, Donatas G. <dgvirtual at akl.lt> wrote:
> Hi, I have a data frame DATA, which (simplified of course) looks like this:
>
> know1 = c("Y","N","N","Y","N","N","Y","Y","N")
> par1=c(1,4,5,3,3,2,3,3,5)
> know2 = c("Y","Y","N","Y","N","N","N","Y","Y")
> par2=c(3,4,4,3,5,2,4,3,2)
> DATA=data.frame(know1,par1,know2,par2)
>
> it represents answers in a questionnaire, where respondents evaluate two
> things (par1 and par2) but they also indicate, whether they have detailed
> knowledge about those two things (know1 and know2).
>
> I need to test correlations between parameters par1 and par2, but need to do
> that separately for cases where the respondents know about both things and
> where they do not know about both things. If, testing correlations of those
> who do know, a respondent knows only about item1 and not item2 (if the
> responces do not match), the values of par1 and par2 should be changed to
> NA's. The same goes if the respondent says he does not know only about one
> thing.
>
> SO: Before doing analysis, I need to transform the DATA dataframe in two ways:
>
> Testing correlations of those evaluating with knowledge:
>
> > DATA
>  know1 par1 know2 par2
> 1     Y    1     Y    3
> 2     N    NA    Y    NA
> 3     N    NA    N    NA
> 4     Y    3     Y    3
> 5     N    NA    N    NA
> 6     N    NA    N    NA
> 7     Y    NA    N    NA
> 8     Y    3     Y    3
> 9     N    NA    Y    NA
>
> And the case of those evaluating without knowledge:
>
> > DATA
>  know1 par1 know2 par2
> 1     Y    NA    Y    NA
> 2     N    NA    Y    NA
> 3     N    5     N    4
> 4     Y    NA    Y    NA
> 5     N    3     N    5
> 6     N    2     N    2
> 7     Y    NA    N    NA
> 8     Y    NA    Y    NA
> 9     N    NA    Y    NA
>
> Yes, yes, this example is stupid, but...
>
> I know how to filter records, I know how to recode, but I need some
> combination of the two here and I am lost.. Any ideas?
>
> --
> Donatas Glodenis
> http://dg.lapas.info
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From ericlecoutre at gmail.com  Wed Dec 19 13:16:21 2007
From: ericlecoutre at gmail.com (Eric Lecoutre)
Date: Wed, 19 Dec 2007 13:16:21 +0100
Subject: [R] Obtaining replicates numbers of a vector
In-Reply-To: <da79af330712190325r799d3607g6ff6a8086f2c36bf@mail.gmail.com>
References: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>
	<da79af330712190325r799d3607g6ff6a8086f2c36bf@mail.gmail.com>
Message-ID: <5d897a2f0712190416x766f165ei5ea27b9bef19875c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/ff03e8d8/attachment.pl 

From gallon.li at gmail.com  Wed Dec 19 13:26:02 2007
From: gallon.li at gmail.com (gallon li)
Date: Wed, 19 Dec 2007 20:26:02 +0800
Subject: [R] want to make a plot similar to ecdf
Message-ID: <54f7e7c30712190426w3694291ai22ec0837390ad3cc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/0e86d462/attachment.pl 

From jholtman at gmail.com  Wed Dec 19 13:49:38 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 19 Dec 2007 07:49:38 -0500
Subject: [R] plot cummulative sum from calendar time
In-Reply-To: <54f7e7c30712182259x2251a8ben8fb37c13ccf7824f@mail.gmail.com>
References: <54f7e7c30712182259x2251a8ben8fb37c13ccf7824f@mail.gmail.com>
Message-ID: <644e1f320712190449w5d20b884j2dae313ef31c50bc@mail.gmail.com>

Here is an example of plotting the number of observations per month.
You can change this to any period you want.

# create some test data
x.d <- as.Date("2000-1-1") + runif(1000, 1, 500)
# create buckets of one month (or whatever period you want)
x.cut <- cut(x.d, breaks=seq(as.Date('2000-1-1'), max(x.d)+7, by="1 month"))
# plot the data
barplot(table(x.cut), main="Observations Per Month")




On Dec 19, 2007 1:59 AM, gallon li <gallon.li at gmail.com> wrote:
> I have the following list of observations of calendar time:
>
>  [1] 03-Nov-1997 09-Oct-1991 27-Aug-1992 01-Jul-1994 19-Jan-1990 12-Nov-1993
>  [7] 08-Oct-1993 10-Nov-1982 08-Dec-1986 23-Dec-1987 02-Aug-1995 20-Oct-1998
> [13] 29-Apr-1991 16-Mar-1994 20-May-1991 28-Dec-1987 14-Jul-1999 27-Nov-1998
> [19] 09-Sep-1999 26-Aug-1999 20-Jun-1997 05-May-1995 26-Mar-1998 15-Aug-1994
> [25] 24-Jun-1996 02-Oct-1996 12-Aug-1985 24-Jun-1992 09-Jan-1991 04-Mar-1988
> 3230 Levels: 01-Apr-1987 01-Apr-1990 01-Apr-1991 01-Apr-1996 ... 31-Oct-1998
>
> I want to make a plot where x-axis is the calendar time and y-axix is the
> cumulative sum of observations observed on or before that dates.
>
> Can anyone give some suggestions?
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From quesada at gmail.com  Wed Dec 19 13:54:08 2007
From: quesada at gmail.com (Jose Quesada )
Date: Wed, 19 Dec 2007 13:54:08 +0100
Subject: [R] Correlation when one variable has zero variance (polychoric?)
Message-ID: <op.t3k14iql4hcap5@dollar>

Hi,

I'm running this for a simulation study, so many combinations of parameter  
produce many predictions that I need to correlate with data.

The problem
----------------
I'm using rating data with 3 to 5 categories (e.g., too low, correct, too  
high). The underlying continuous scales should be normal, so I chose the  
polychoric correlation. I'm using library(polychor) in its latest version  
0.7.4

The problem is that sometimes the models predict always the same value  
(i.e., the median). Example frequency table:

> table(med$ADRI_LAN, rate$ADRI_LAN)
       2   3   4   5
   3  28 179 141  50

That is, there is no variability in one of the variables (the only value  
is 3, the median).

Pearson Product Moment Correlation consists of the covariation divided by  
the square root of the product of the standard deviations of the two  
variables. If the standard deviation of one of the variables is zero, then  
the denominator is zero and the correlation cannot be computed. R returns  
NA and a warning.

If I add jitter to the variable with no variability, then I get a  
virtually zero, but calculable, Pearson correlation.

However, when I use the polychoric correlation (using the default  
settings), I get just the opposite: a very high correlation!

> polychoric    = polychor( med$ADRI_LAN, rate$ADRI_LAN ) #, ML=T,  
> std.err=T
> polychoric
[1] 0.999959

This is very counterintuitive. I also ran the same analysis in 2005 (what  
has changed in the package polycor since then, I don't know) and the  
results were different. I think back then I contrasted them with SAS and  
they were the same. Maybe the approximation fails in extreme cases where  
most of the cells are zero? Maybe the approximation was not used in the  
first releases of the package? But it seems that the ML estimator doesn't  
work at all (at least in the current version of the package) with those  
tables when most cells are zero due to no variability on one variable):

> polychor(med$ADRI_LAN, rate$ADRI_LAN, ML=T)
Error in tab * log(P) : non-conformable arrays

I've seen some posts where sparse tables were trouble, eg:  
http://www.nabble.com/polychor-error-td5954345.html#a5954345
  "You're expecting a lot out of ML to get estimates of the first couple of  
thresholds for rows and the first for columns. [which were mostly zeroes]"

Are the polychoric estimates using the approximation completely wrong? Is  
there any way to compute a polychoric correlation with such a dataset?  
What should I conclude from data like these?
Maybe using correlation is not the right thing to do.

Thanks,
-Jose

-- 
Jose Quesada, PhD.
http://www.andrew.cmu.edu/~jquesada


From f.harrell at vanderbilt.edu  Wed Dec 19 13:58:27 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 19 Dec 2007 06:58:27 -0600
Subject: [R] using rcorr.cens for Goodman Kruskal gamma
In-Reply-To: <4768e34a.26ba720a.474e.ffffb66d@mx.google.com>
References: <4768e34a.26ba720a.474e.ffffb66d@mx.google.com>
Message-ID: <47691573.5040907@vanderbilt.edu>

Colin Robertson wrote:
> Dear List,
> 
>  
> 
> I would like to calculate the Goodman-Kruskal gamma for the predicted
> classes obtained from an ordinal regression model using lrm in the Design
> package. I couldn't find a way to get gamma for predicted values in Design
> so have found previous positings suggesting to use :
> 
>  
> 
> Rcorr.cens(x, S outx = TRUE)  in the Hmisc package
> 
>  
> 
> My question is, will this work for predicted vs observed factors?  I.e. x =
> predicted class and S = observed class? Or is there a better way to obtain
> this? I used the maximum individual probability for each observation to
> determine the predicted class. 

Rank correlation measures are for correlating a continuous or ordinal 
prediction with a response (continuous, ordinal, or binary).  So you 
should be able to do something like rcorr.cens(predict(fit), 
as.numeric(Y), outx=TRUE).  Note that rcorr is all lower case.  This 
assumes that the levels of Y are in order, as does lrm.

Note that the new version of lrm has a method for getting predicted mean 
scores from an ordinal lrm.

Frank

> 
>  
> 
> Any help appreciated, 
> 
>  
> 
> Thanks
> 
>  
> 
> Colin
> 
>  
> 
>  
> 
>  
> 
> Colin Robertson
> 
> Dept of Geography
> 
> University of Victoria


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From jholtman at gmail.com  Wed Dec 19 14:06:06 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 19 Dec 2007 08:06:06 -0500
Subject: [R] Obtaining replicates numbers of a vector
In-Reply-To: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>
References: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>
Message-ID: <644e1f320712190506m76794999n7ba0970f46a47a09@mail.gmail.com>

Here is another way of doing it:

> x <- c('A','B','A','C','C','B')
> ave(rep(1, length(x)), x, FUN=cumsum)
[1] 1 1 2 1 2 2
>


On Dec 19, 2007 5:36 AM, Eric Lecoutre <ericlecoutre at gmail.com> wrote:
> Dear R-help,
>
> I am trying to have a generic way to assess the replicates in a character
> vector.
> Say that I have the following vector:
>
> x <- c('A','B','A','C','C','B')
>
> I would like to obtain:
>
> replicates <- c(1,1,2,1,2,2)
>
> each number beeing the time we see the corresponding value in x.
>
> Any clever and generic way to obtain that?
>
> Eric
>
>
>
>
> --
> Eric Lecoutre
> Consultant - Business & Decision
> Business Intelligence & Customer Intelligence
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Wed Dec 19 14:11:09 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 19 Dec 2007 08:11:09 -0500
Subject: [R] want to make a plot similar to ecdf
In-Reply-To: <54f7e7c30712190426w3694291ai22ec0837390ad3cc@mail.gmail.com>
References: <54f7e7c30712190426w3694291ai22ec0837390ad3cc@mail.gmail.com>
Message-ID: <644e1f320712190511s5ac37e34s7dd0445dde697149@mail.gmail.com>

This should give you what you want:

 x <- scan(textConnection("0.00000000   2.39722222   4.35000000
-4.19722222   0.63611111
 1.08055556   5.90555556  -1.87222222   2.13333333  -1.18055556
  3.61666667   0.87777778   8.33888889   3.84166667   1.11111111
 -3.76111111 -11.67777778  -2.03055556   6.94444444 -11.76666667
  4.81111111  -7.25833333   1.42222222   5.37222222   4.68055556
  0.69166667  -5.36944444   5.35555556   4.26944444   6.14722222
  0.42500000   2.90555556  11.74166667   5.99444444   3.60555556
 -2.18333333   2.07777778  -9.79722222   7.26111111   4.50277778
  0.84722222   0.42222222   1.01388889  -0.04722222   5.03611111
  0.26666667   0.10555556   1.01666667   5.65833333   4.11111111
  0.23055556   8.53611111   4.42222222   4.93055556  13.41111111
  0.00000000   0.00000000  13.37500000   1.52500000   6.35833333"), what=0)
x <- sort(x)
plot(x, cumsum(rep(1, length(x))), type='s')



On Dec 19, 2007 7:26 AM, gallon li <gallon.li at gmail.com> wrote:
> I have a sample of observations:
>
> > yy
>  [1]   0.00000000   2.39722222   4.35000000  -4.19722222   0.63611111
>  [6]   1.08055556   5.90555556  -1.87222222   2.13333333  -1.18055556
>  [11]   3.61666667   0.87777778   8.33888889   3.84166667   1.11111111
>  [16]  -3.76111111 -11.67777778  -2.03055556   6.94444444 -11.76666667
>  [21]   4.81111111  -7.25833333   1.42222222   5.37222222   4.68055556
>  [26]   0.69166667  -5.36944444   5.35555556   4.26944444   6.14722222
>  [31]   0.42500000   2.90555556  11.74166667   5.99444444   3.60555556
>  [36]  -2.18333333   2.07777778  -9.79722222   7.26111111   4.50277778
>  [41]   0.84722222   0.42222222   1.01388889  -0.04722222   5.03611111
>  [46]   0.26666667   0.10555556   1.01666667   5.65833333   4.11111111
>  [51]   0.23055556   8.53611111   4.42222222   4.93055556  13.41111111
>  [56]   0.00000000   0.00000000  13.37500000   1.52500000   6.35833333
>
> I want to make an empirical distribution plot. But, I wish that the y-axis
> is the count instead of probability.
>
> plot(ecdf(yy))
>
> can only give the cumulative distribution on the y-axix.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From dgvirtual at akl.lt  Wed Dec 19 14:32:55 2007
From: dgvirtual at akl.lt (Donatas G.)
Date: Wed, 19 Dec 2007 15:32:55 +0200
Subject: [R] recode based on filter
In-Reply-To: <da79af330712190412x7508c3e5q15f714c1d1a9ca4d@mail.gmail.com>
References: <200712191351.24056.dgvirtual@akl.lt>
	<da79af330712190412x7508c3e5q15f714c1d1a9ca4d@mail.gmail.com>
Message-ID: <200712191532.55558.dgvirtual@akl.lt>

On Wednesday 19 December 2007 14:12:16 ra??te:
> sapply(levels(DATA$know1), function(x)subset(DATA, (know1==x &
> know2==x)), simplify=F)

Hey, thanks, that seems to work!

-- 
Donatas Glodenis
http://dg.lapas.info


From jfox at mcmaster.ca  Wed Dec 19 15:29:36 2007
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 19 Dec 2007 09:29:36 -0500
Subject: [R] Correlation when one variable has zero variance
	(polychoric?)
In-Reply-To: <op.t3k14iql4hcap5@dollar>
References: <op.t3k14iql4hcap5@dollar>
Message-ID: <000901c8424b$96d87e10$c4897a30$@ca>

Dear Jose,

> -----Original Message-----
> From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-
> project.org] On Behalf Of Jose Quesada
> Sent: December-19-07 7:54 AM
> To: r-help at lists.r-project.org
> Subject: [R] Correlation when one variable has zero variance
> (polychoric?)
> 
> Hi,
> 
> I'm running this for a simulation study, so many combinations of
> parameter
> produce many predictions that I need to correlate with data.
> 
> The problem
> ----------------
> I'm using rating data with 3 to 5 categories (e.g., too low, correct,
> too
> high). The underlying continuous scales should be normal, so I chose
> the
> polychoric correlation. I'm using library(polychor) in its latest
> version
> 0.7.4
> 
> The problem is that sometimes the models predict always the same value
> (i.e., the median). Example frequency table:
> 
> > table(med$ADRI_LAN, rate$ADRI_LAN)
>        2   3   4   5
>    3  28 179 141  50
> 
> That is, there is no variability in one of the variables (the only
> value
> is 3, the median).
> 
> Pearson Product Moment Correlation consists of the covariation divided
> by
> the square root of the product of the standard deviations of the two
> variables. If the standard deviation of one of the variables is zero,
> then
> the denominator is zero and the correlation cannot be computed. R
> returns
> NA and a warning.
> 
> If I add jitter to the variable with no variability, then I get a
> virtually zero, but calculable, Pearson correlation.
> 
> However, when I use the polychoric correlation (using the default
> settings), I get just the opposite: a very high correlation!
> 
> > polychoric    = polychor( med$ADRI_LAN, rate$ADRI_LAN ) #, ML=T,
> > std.err=T
> > polychoric
> [1] 0.999959
> 
> This is very counterintuitive. 

This is simply a bug in polychor(), which currently does the following test:

  if (r < 1) stop("the table has fewer than 2 rows")
  if (c < 2) stop("the table has fewer than 2 columns")

That is, my intention was to check (r < 2) and report an error. Actually, it
would probably be better to return NA and report a warning.

> I also ran the same analysis in 2005
> (what
> has changed in the package polycor since then, I don't know) and the
> results were different. I think back then I contrasted them with SAS
> and
> they were the same.

I don't entirely follow this. Are you referring to the table above with one
row, more generally to table with zero marginals, or to tables in which
there are interior zeroes?

> Maybe the approximation fails in extreme cases
> where
> most of the cells are zero? Maybe the approximation was not used in the
> first releases of the package? But it seems that the ML estimator
> doesn't
> work at all (at least in the current version of the package) with those
> tables when most cells are zero due to no variability on one variable):
> 
> > polychor(med$ADRI_LAN, rate$ADRI_LAN, ML=T)
> Error in tab * log(P) : non-conformable arrays

When there are zero marginals the ML estimate cannot be unique since there
is zero information about one or more of the thresholds.

> 
> I've seen some posts where sparse tables were trouble, eg:
> http://www.nabble.com/polychor-error-td5954345.html#a5954345
>   "You're expecting a lot out of ML to get estimates of the first
> couple of
> thresholds for rows and the first for columns. [which were mostly
> zeroes]"
> 
> Are the polychoric estimates using the approximation completely wrong?

Yes. If there is a zero marginal, then it shouldn't have been computed in
the first place (and was due to the error that I mentioned).

> Is
> there any way to compute a polychoric correlation with such a dataset?

I'd say no. There is no information in the data about the correlation.

> What should I conclude from data like these?

That the data aren't informative about the parameters of interest.

> Maybe using correlation is not the right thing to do.

Presumably the normally distributed latent variables that underlie the table
have some correlation, but you can't estimate it from the data.

I'll fix polycor() (and put it a test for 0 marginals as well as single-row
or -column tables) -- thanks for the bug report.

Regards,
John

> 
> Thanks,
> -Jose
> 
> --
> Jose Quesada, PhD.
> http://www.andrew.cmu.edu/~jquesada
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j.clayden at ucl.ac.uk  Wed Dec 19 15:32:28 2007
From: j.clayden at ucl.ac.uk (Jon Clayden)
Date: Wed, 19 Dec 2007 14:32:28 +0000
Subject: [R] Standard input and R
Message-ID: <7229BBA9-0901-4D74-A6B7-E9BB0479E4C2@ucl.ac.uk>

Dear all,

I am trying to wrap a *nix shell script around R for a particular  
purpose, for which I need to get R to execute predetermined commands  
but retain interactivity and allow user input during their execution.  
A straight redirection of standard input is therefore not appropriate,  
and I don't think "littler" is the solution because I don't want to  
write an independent R script.

What I want is effectively something like what ruby does with the '-e'  
option:

$ ruby -e 'gets'
<waits for input from terminal>

R seems to accept an '-e' option too, even though I can't see it  
mentioned in the output from 'R -h', but it doesn't work the same:

$ R --slave --vanilla -e 'scan(what=character(0),nlines=1,quiet=T)'
character(0)
$

The only way I can get this to work is by creating an .Rprofile file  
containing the commands I want to run, but this is suboptimal for a  
number of reasons: I have difficulties if an .Rprofile already exists  
in the working directory, the default packages are not loaded, and the  
process doesn't quit if stop() is called.

I have a vague inkling that this may be achievable with a  
pseudoterminal (pty) or named pipe, but if anyone knows a simpler  
route I'd be glad to avoid getting involved in that kind of thing...

Many thanks in advance.
Jon


From dusa.adrian at gmail.com  Wed Dec 19 15:34:26 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Wed, 19 Dec 2007 16:34:26 +0200
Subject: [R] connecting [logging] RMySQL to an external server - SOLVED
In-Reply-To: <971536df0712150854n55d5c15eoaf300527cd8ce844@mail.gmail.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<200712151833.04227.dusa.adrian@gmail.com>
	<971536df0712150854n55d5c15eoaf300527cd8ce844@mail.gmail.com>
Message-ID: <200712191634.26224.dusa.adrian@gmail.com>

On Saturday 15 December 2007, Gabor Grothendieck wrote:
> Use ssh forwarding to forward local port 3307 to remote port 3306
> specifying the remote account and password.  Then if you use local port
> 3306 you can access your local version of MySQL and if you
> use port 3307 you can access the remote version.   There is some
> info on the MySQL site.  First test it out by running the mysql command
> line program accessing the remote data base via port 3307 and once
> that works you know its ok and you can try RMySQL or RODBC packages.

For those interested in using R with a remote MySQL database, here's the most 
simple and straightforward solution (thanks to Gabor):

On the console, forward local port 3307 to remote port 3306
ssh -L 3307:xxx.xxx.xxx.xxx:3306 myuser at xxx.xxx.xxx.xxx

(where xxx.xxx.xxx.xxx is the IP of the remote server and myuser is the login 
name). After entering the password, keep that console open.

In R:

library(RMySQL)
drv <- dbDriver("MySQL")
dbConnect(drv, user="mysqluser", password="mysqlpassword", 
dbname="anydatabase", host="127.0.0.1", port="3307")

(where mysqluser and mysqlpassword are the remote machine's MySQL username and 
password).

Another possible solution is suggested by Prof. Ripley, using stunnel to 
create a secure tunnel between the local and the remote machines, but I 
haven't explored that in detail.


Best wishes,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From jrkrideau at yahoo.ca  Wed Dec 19 15:52:40 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 19 Dec 2007 09:52:40 -0500 (EST)
Subject: [R] bar plot colors
In-Reply-To: <4768E632.2060609@bitwrit.com.au>
Message-ID: <867142.50654.qm@web32812.mail.mud.yahoo.com>


--- Jim Lemon <jim at bitwrit.com.au> wrote:

> John Kane wrote:
> > I think you're going to find that barchart with
> that
> > many values in a bar is going to be pretty well
> > uninterpretable.  
> > 
> > Jim Lemon gives the desired barchart but it is
> very
> > difficult to read.  
> > 
> > Stealing his code to create the same matrix I'd
> > suggest may be looking at a dotchart.  I'm not
> sure if
> > this is even close to an optimal solution but I do
> > think it's a bit better than a barchart approach
> >
>
======================================================
> > heights<-matrix(sample(10:70,54),ncol=3)
> > bar.colors<-rep(rep(2:7,each=3),3)
> > cost.types <- c("Direct", "Indirec", "Induced")
> > colnames(heights) <-  c("A", "B", "C")
> > rownames(heights) <- c(rep(cost.types, 6))
> > 
> > dotchart(heights, col=bar.colors, pch=16, cex=.6)
> > 
> >
>
=======================================================
> Very nice John, but it does look like someone
> dropped the M&Ms.
> Jim

Please I'm Canadian, we have Smarties. Much nicer than
M&Ms :)  

It is a bit garish but I didn't see a fast way to turn
off the colours on the axis though from the Help it
looks possible.



[[replacing trailing spam]]


From jrkrideau at yahoo.ca  Wed Dec 19 16:25:20 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 19 Dec 2007 10:25:20 -0500 (EST)
Subject: [R] plot cummulative sum from calendar time
In-Reply-To: <54f7e7c30712182259x2251a8ben8fb37c13ccf7824f@mail.gmail.com>
Message-ID: <365279.56368.qm@web32801.mail.mud.yahoo.com>

I am not sure I really understand what you want but
will this work?

tt<-c("03-Nov-1997","09-Oct-1991","27-Aug-1992","01-Jul-1994","19-Jan-1990",
"12-Nov-1993","08-Oct-1993","10-Nov-1982","08-Dec-1986","23-Dec-1987","02-Aug-1995",
"20-Oct-1998","29-Apr-1991","16-Mar-1994","20-May-1991","28-Dec-1987","14-Jul-1999",
"27-Nov-1998","09-Sep-1999","26-Aug-1999","20-Jun-1997","05-May-1995","26-Mar-1998",
"15-Aug-1994","24-Jun-1996","02-Oct-1996","12-Aug-1985","24-Jun-1992","09-Jan-1991",
"04-Mar-1988")
tt1 <-  as.Date(tt, "%d-%b-%Y"))
aa <- cumsum(1:length(tt))
plot(tt1,aa)


--- gallon li <gallon.li at gmail.com> wrote:

> I have the following list of observations of
> calendar time:
> 
>  [1] 03-Nov-1997 09-Oct-1991 27-Aug-1992 01-Jul-1994
> 19-Jan-1990 12-Nov-1993
>  [7] 08-Oct-1993 10-Nov-1982 08-Dec-1986 23-Dec-1987
> 02-Aug-1995 20-Oct-1998
> [13] 29-Apr-1991 16-Mar-1994 20-May-1991 28-Dec-1987
> 14-Jul-1999 27-Nov-1998
> [19] 09-Sep-1999 26-Aug-1999 20-Jun-1997 05-May-1995
> 26-Mar-1998 15-Aug-1994
> [25] 24-Jun-1996 02-Oct-1996 12-Aug-1985 24-Jun-1992
> 09-Jan-1991 04-Mar-1988
> 3230 Levels: 01-Apr-1987 01-Apr-1990 01-Apr-1991
> 01-Apr-1996 ... 31-Oct-1998
> 
> I want to make a plot where x-axis is the calendar
> time and y-axix is the
> cumulative sum of observations observed on or before
> that dates.
> 
> Can anyone give some suggestions?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From jrkrideau at yahoo.ca  Wed Dec 19 16:29:27 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 19 Dec 2007 10:29:27 -0500 (EST)
Subject: [R] Factor Madness
In-Reply-To: <fk9kgo$rr$1@ger.gmane.org>
Message-ID: <421236.96648.qm@web32804.mail.mud.yahoo.com>

What was spectrum orginally?


--- Johannes Graumann <johannes_graumann at web.de>
wrote:

> Why is class(spectrum[["Ion"]]) after this "factor"?
> 
> spectrum <- cbind(spectrum,Ion=rep("",
> nrow(spectrum)),Deviation.AMU=rep(0.0,
> nrow(spectrum)))
> 
> slowly going crazy ...
> 
> Joh
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From dhewitt at vims.edu  Wed Dec 19 16:42:40 2007
From: dhewitt at vims.edu (David Hewitt)
Date: Wed, 19 Dec 2007 07:42:40 -0800 (PST)
Subject: [R] How can I extract the AIC score from a mixed model object
 produced using lmer?
In-Reply-To: <f21f775b0712181421w36e5fe6ep5d1229ed550cc58f@mail.gmail.com>
References: <OFEA565530.971E9869-ON882573B5.00784A2E-882573B5.00797442@fs.fed.us>
	<f21f775b0712181421w36e5fe6ep5d1229ed550cc58f@mail.gmail.com>
Message-ID: <14419438.post@talk.nabble.com>



David Barron-3 wrote:
> 
> You can calculate the AIC as follows:
> 
> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> aic1 <- AIC(logLik(fm1))
> 
> 

Is AIC() [extractAIC()] "valid" for models with random effects? I noticed
that the help page for extractAIC() does not list models with random
effects. I think this boils down to the difference between the likelihoods
for models with and without random effects, and I don't know. Just
curious...



> On 12/18/07, Peter H Singleton <psingleton at fs.fed.us> wrote:
>>
>> I am running a series of candidate mixed models using lmer (package lme4)
>> and I'd like to be able to compile a list of the AIC scores for those
>> models so that I can quickly summarize and rank the models by AIC. When I
>> do logistic regression, I can easily generate this kind of list by
>> creating
>> the model objects using glm, and doing:
>>
>> > md <- c("md1.lr", "md2.lr", "md3.lr")
>> > aic <- c(md1.lr$aic, md2.lr$aic, md3.lr$aic)
>> > aic2 <- cbind(md, aic)
>>
>> but when I try to extract the AIC score from the model object produced by
>> lmer I get:
>>
>> > md1.lme$aic
>> NULL
>> Warning message:
>> In md1.lme$aic : $ operator not defined for this S4 class, returning NULL
>>
>> So... How do I query the AIC value out of a mixed model object created by
>> lmer?
>>
______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

-----
David Hewitt
Virginia Institute of Marine Science
http://www.vims.edu/fish/students/dhewitt/
-- 
View this message in context: http://www.nabble.com/How-can-I-extract-the-AIC-score-from-a-mixed-model-object-produced-using-lmer--tp14406832p14419438.html
Sent from the R help mailing list archive at Nabble.com.


From arjun_kondamani at ml.com  Wed Dec 19 16:52:16 2007
From: arjun_kondamani at ml.com (Kondamani, Arjun (GMI - NY Corporate Bonds))
Date: Wed, 19 Dec 2007 10:52:16 -0500
Subject: [R] Aggregating by a grouping
Message-ID: <2893E11BC50DB445BC9E97835D9C219D0279F269@MLNYC727MB.amrs.win.ml.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/c88d0a04/attachment.pl 

From ajayshah at mayin.org  Wed Dec 19 16:58:34 2007
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed, 19 Dec 2007 21:28:34 +0530
Subject: [R] Code for articles in R news?
Message-ID: <20071219155834.GF302@lubyanka.local>

I went to the article on np in R news 7/2 (October 2007). What's the
general technique to get the source code associated with the article
as a .R file that I can play with?

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From edd at debian.org  Wed Dec 19 17:04:59 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 19 Dec 2007 10:04:59 -0600
Subject: [R] connecting [logging] RMySQL to an external server - SOLVED
In-Reply-To: <200712191634.26224.dusa.adrian@gmail.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<200712151833.04227.dusa.adrian@gmail.com>
	<971536df0712150854n55d5c15eoaf300527cd8ce844@mail.gmail.com>
	<200712191634.26224.dusa.adrian@gmail.com>
Message-ID: <20071219160459.GA30334@eddelbuettel.com>

On Wed, Dec 19, 2007 at 04:34:26PM +0200, Adrian Dusa wrote:
> On Saturday 15 December 2007, Gabor Grothendieck wrote:
> > Use ssh forwarding to forward local port 3307 to remote port 3306
> > specifying the remote account and password.  Then if you use local port
> > 3306 you can access your local version of MySQL and if you
> > use port 3307 you can access the remote version.   There is some
> > info on the MySQL site.  First test it out by running the mysql command
> > line program accessing the remote data base via port 3307 and once
> > that works you know its ok and you can try RMySQL or RODBC packages.
> 
> For those interested in using R with a remote MySQL database, here's the most 
> simple and straightforward solution (thanks to Gabor):
> 
> On the console, forward local port 3307 to remote port 3306
> ssh -L 3307:xxx.xxx.xxx.xxx:3306 myuser at xxx.xxx.xxx.xxx
> 
> (where xxx.xxx.xxx.xxx is the IP of the remote server and myuser is the login 
> name). After entering the password, keep that console open.
> 
> In R:
> 
> library(RMySQL)
> drv <- dbDriver("MySQL")
> dbConnect(drv, user="mysqluser", password="mysqlpassword", 
> dbname="anydatabase", host="127.0.0.1", port="3307")

Err, I am late to this but

 dbConnect(drv, user="mysqluser", password="mysqlpassword", dbname="anydatabase", host="xxx.xxx.xxx")

works fine for me (eg on Ubuntu with a remote MySQL on some other
box).  What's the problem you were seeing?

Dirk

-- 
Three out of two people have difficulties with fractions.


From Ingo.Holz at uni-hohenheim.de  Wed Dec 19 17:14:45 2007
From: Ingo.Holz at uni-hohenheim.de (Ingo Holz)
Date: Wed, 19 Dec 2007 17:14:45 +0100
Subject: [R] library(rpart) or library(tree)
Message-ID: <47695185.25095.209620F@ingoholz.uni-hohenheim.de>

Hi,

 I have a problem with library (rpart) (and/or library(tree)).

 I use a data.frame with variables
"pnV22" (observation: 1, 0 or yes, no)
"JTemp" (mean temperature)
"SNied"  (summer rain)

 I used function "rpart" to build a model:

	library(rpart)
	attach(data.frame)
	result <- rpart(pnV22 ~ JTemp + SNied)

 I got the following tree:

  n=55518 (50 observations deleted due to missingness)

node), split, n, deviance, yval
      * denotes terminal node

 1) root 55518 668.744500 0.0121942400  
   2) punkte[["JTemp"]]< 10.35 51251  18.992960 0.0003707245 *
   3) punkte[["JTemp"]]>=10.35 4267 556.532000 0.1542067000  
     6) punkte[["SNied"]]>=450 3136 291.318600 0.1036352000 *
     7) punkte[["SNied"]]< 450 1131 234.954900 0.2944297000  
      14) punkte[["JTemp"]]>=10.55 723 113.502100 0.1950207000 *
      15) punkte[["JTemp"]]< 10.55 408 101.647100 0.4705882000  
        30) punkte[["JTemp"]]< 10.45 48   4.479167 0.1041667000 *
        31) punkte[["JTemp"]]>=10.45 360  89.863890 0.5194444000 *

 I constructed a simple new.data.frame:

     new.data.fame <- data.frame
     new.data.frame[,"JTemp"] <- 10.5
     new.data.frame[,"SNied"] <- 430

Than I used predict() to predict values for "pnV22" in the following way:

    pred <- predict(result, data.frame)
    pred2 <- predict(result, new.data.frame)

The results are the same, which I checked by ploting the values of pred and pred2 and by

   table(pred ==pred2)  which is true for all values.

Looking at the tree I would expect that pred2 has the same high value for all elements of the 
vector. Did I make a mistake?

Thanks, Ingo


From Richard.Cotton at hsl.gov.uk  Wed Dec 19 17:36:40 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Wed, 19 Dec 2007 16:36:40 +0000
Subject: [R] Aggregating by a grouping
In-Reply-To: <2893E11BC50DB445BC9E97835D9C219D0279F269@hsl.gov.uk>
Message-ID: <OFD7228DF0.2E974BC3-ON802573B6.005AF53D-802573B6.005B3A2C@hsl.gov.uk>

> Suppose I have:
> 
> Book   Value 
> A   10 
> B   11 
> C   9 
> D   8 
> A   12 
> C   4 
> D   5 
> B   7 
> 
> I want to summarize above not by Book but by groupings of Books as in
> (below)
> 
> I have a list ... basic_map <- list(c("A",B"),c("C,D"))
> Big_names <- c("A1", "A2")
> Names(basic_map) <- big_names

Try this:

testdf <- data.frame(book=factor(c("A", "B", "C", "D", "A", "C", "D", 
"B")),  value=c(10,11,9,8,12,4,5,7))
bookgroup <- rep("A1", nrow(testdf))
bookgroup[testdf$book=="C" | testdf$book=="D"] <- "A2"
tapply(testdf$value, bookgroup, sum)

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From wwwhsd at gmail.com  Wed Dec 19 17:44:38 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 19 Dec 2007 14:44:38 -0200
Subject: [R] Aggregating by a grouping
In-Reply-To: <2893E11BC50DB445BC9E97835D9C219D0279F269@MLNYC727MB.amrs.win.ml.com>
References: <2893E11BC50DB445BC9E97835D9C219D0279F269@MLNYC727MB.amrs.win.ml.com>
Message-ID: <da79af330712190844r4090faeaxb965a6fd00db569e@mail.gmail.com>

try this:

apply(sapply(basic_map, function(x)tapply(df$Value, df$Book, sum)[x]), 2, sum)

On 19/12/2007, Kondamani, Arjun (GMI - NY Corporate Bonds)
<arjun_kondamani at ml.com> wrote:
> Suppose I have:
>
> Book    Value
> A       10
> B       11
> C       9
> D       8
> A       12
> C       4
> D       5
> B       7
>
> I want to summarize above not by Book but by groupings of Books as in
> (below)
>
> I have a list ... basic_map <- list(c("A",B"),c("C,D"))
> Big_names <- c("A1", "A2")
> Names(basic_map) <- big_names
>
> So I want to get :
>
> A1 40
> A2 26
>
> How do I use tapply AND the list to get my custom groupings?
>
> thx
> --------------------------------------------------------
>
> This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
> --------------------------------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From deepayan.sarkar at gmail.com  Wed Dec 19 17:45:48 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 19 Dec 2007 08:45:48 -0800
Subject: [R] Different labels by panel in barchart
In-Reply-To: <OF4C1A6466.345BD6D1-ON802573B5.00660E3E-802573B6.00338775@hsl.gov.uk>
References: <OF4C1A6466.345BD6D1-ON802573B5.00660E3E-802573B6.00338775@hsl.gov.uk>
Message-ID: <eb555e660712190845h1585923ap72e0123e4f25be78@mail.gmail.com>

On 12/19/07, Richard.Cotton at hsl.gov.uk <Richard.Cotton at hsl.gov.uk> wrote:
> Dear all,
>
> I'm analysing a survey, and creating a barchart of the different responses
> for each question.  The questions are grouped according to a number of
> categories, so I'm using lattice to create a plot with each question in a
> category on it.  The problem is that the response set for different
> questions within the same category varies.  I want to be able to draw only
> the relevant bars, without spaces for nonrelevant reposnes.  An example
> may make it clearer:
>
> testdf = data.frame(y=c(23,34,45, 56), x=factor(c("a", "a", "b", "c")),
> g=factor(c(1,2,1,2)))
> barchart(y~x|g, data=testdf)
>
> #In the first panel I would like two columns "a" and "b", positioned at 1
> and 2 respectively; in the second I would again like two columns at 1 and
> 2, this time labelled "a" and "c".
>
> barchart(y~x|g, data=testdf, scales=list(x=list(relation="free")),
> drop.unused.levels=TRUE)
>
> #This is closer, but puts the columns in the wrong place in the second
> panel.

You need to do the dropping of the levels separately for each panel:

barchart(y~x|g, data = testdf, scales=list(x=list(relation="free")),
         prepanel = function(x, y, ...) {
             x <- x[, drop = TRUE]
             prepanel.default.bwplot(x, y, ...)
         },
         panel = function(x, y, ...) {
             x <- x[, drop = TRUE]
             panel.barchart(x, y, ...)
         })

-Deepayan


From zuoj at un.org  Wed Dec 19 17:47:40 2007
From: zuoj at un.org (Jiaming Zuo)
Date: Wed, 19 Dec 2007 11:47:40 -0500
Subject: [R] (no subject)
Message-ID: <OF69166A53.AC650197-ON852573B6.005C1728-852573B6.005C4189@un.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/035e1811/attachment.pl 

From jrkrideau at yahoo.ca  Wed Dec 19 17:56:55 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 19 Dec 2007 11:56:55 -0500 (EST)
Subject: [R] Reshape Dataframe
In-Reply-To: <20071218153027.B665C67DE1@agave.telenet-ops.be>
Message-ID: <752525.13047.qm@web32815.mail.mud.yahoo.com>

I think if you use Gabor's suggested addtion of
add.missing to the original cast command you get what
you want.

cast(dfm, ... ~ Var3, add.missing=TRUE)

--- Bert Jacobs <b.jacobs at pandora.be> wrote:

> 
> Thx Hadley,
> It works, but I need some finetuning.
> 
> If I use the following expression:
> Newdf <-reshape(df, timevar="Var3",
> idvar=c("Var1","Var2"),direction="wide")
> 
> Newdf
> Var1	Var2	Var3.W1	Var3.W2	Var3.W3	var3.W4
> A	Fa	1	  	3		
> A	Si	2				4
> B	Si	5		
> C	La			6
> C	Do							7
> 
> Is there an option so that for each Var1 all
> possible combinations of Var2
> are listed (i.e. creation of blanco lines).
> Is it possible to name the columns with the values
> of the original Var3
> variable, so that the name Var3.W1 changes to W1? 
> 
> Var1	Var2	W1	W2	W3	W4
> A	Fa	1	3		
> A	Si	2		4
> A	La
> A	Do			
> B	Fa				
> B	Si	5		
> B	La
> B	Do
> C	Fa				
> C	Si			
> C	La		6
> C	Do				7
> 
> 
> Thx,
> Bert
> 
> -----Original Message-----
> From: hadley wickham [mailto:h.wickham at gmail.com] 
> Sent: 18 December 2007 15:16
> To: Bert Jacobs
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Reshape Dataframe
> 
> On 12/18/07, Bert Jacobs <b.jacobs at pandora.be>
> wrote:
> >
> > Hi,
> >
> > I'm having a bit of problems in creating a new
> dataframe.
> > Below you'll find a description of the current
> dataframe and of the
> > dataframe that needs to be created.
> > Can someone help me out on this one?
> 
> library(reshape)
> dfm <- melt(df, id = 1:3)
> cast(dfm, ... ~ Var3)
> 
> You can find out more about the reshape package at
> http://had.co.nz/reshape
> 
> Hadley
> 
> -- 
> http://had.co.nz/
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From rolf.wester at ilt.fraunhofer.de  Wed Dec 19 18:14:06 2007
From: rolf.wester at ilt.fraunhofer.de (Rolf Wester)
Date: Wed, 19 Dec 2007 18:14:06 +0100
Subject: [R] Genetic algorithm for feature selection
Message-ID: <4769515E.5040106@ilt.fraunhofer.de>

Hi,

I'm looking for a R-package that does feature selection for PLS using a 
genetic optimization algorithm.
I couldn't find one on CRAN and I wonder whether there is a free one. I 
would be very appreciative for any help.

Regards

Rolf


From DOCGArmelini at iese.edu  Wed Dec 19 18:20:32 2007
From: DOCGArmelini at iese.edu (Armelini, Guillermo)
Date: Wed, 19 Dec 2007 18:20:32 +0100
Subject: [R] question
Message-ID: <2DA68F10815D6E49956E3DC2CE18603E0113D7C9@SRVSTAFF.iese.org>

Hello everyone!
 
Is anybody can help me to solve this silly question that unfortunately I haven't found the right way to address it.
 
Supose I have a matrix X[n,n] dimension
 
I would like to calculate the product of the vectors
 
Y=X[1,n]*X[n,1]
 
Then I would like to run the following operation
 
H=if(Y>0) {H[,1]=1}
 
The problem is that the condition "if" works only for the first element of the matrix.
 
Does anybody knows how to apply the conditional "if" to a entire vector?
 
cheers
 
Guillermo
 


This message has been scanned for viruses by TRENDMICRO,
an IESE technology affiliate company  and global leader in antivirus and content security software.


From dusa.adrian at gmail.com  Wed Dec 19 18:30:57 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Wed, 19 Dec 2007 19:30:57 +0200
Subject: [R] connecting [logging] RMySQL to an external server - SOLVED
In-Reply-To: <20071219160459.GA30334@eddelbuettel.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<200712191634.26224.dusa.adrian@gmail.com>
	<20071219160459.GA30334@eddelbuettel.com>
Message-ID: <200712191930.58162.dusa.adrian@gmail.com>

On Wednesday 19 December 2007, Dirk Eddelbuettel wrote:
> [...]
>
> Err, I am late to this but
>
>  dbConnect(drv, user="mysqluser", password="mysqlpassword",
> dbname="anydatabase", host="xxx.xxx.xxx")
>
> works fine for me (eg on Ubuntu with a remote MySQL on some other
> box).  What's the problem you were seeing?

The remote host expects a login username and password (different from the 
MySQL username and password), which I cannot provide in the dbConnect 
function.
The solution is to first login to the external machine (a secure login using 
ssh), forwarding the MySQL port 3306 to the local port 3307 (not to interfere 
with the local MySQL server), and connecting R to the remote MySQL server 
using the ssh tunneling binded to local port 3307.

Best,
Adi


-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From daj025 at gmail.com  Wed Dec 19 18:38:25 2007
From: daj025 at gmail.com (David James)
Date: Wed, 19 Dec 2007 12:38:25 -0500
Subject: [R] connecting [logging] RMySQL to an external server - SOLVED
In-Reply-To: <20071219160459.GA30334@eddelbuettel.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<200712151833.04227.dusa.adrian@gmail.com>
	<971536df0712150854n55d5c15eoaf300527cd8ce844@mail.gmail.com>
	<200712191634.26224.dusa.adrian@gmail.com>
	<20071219160459.GA30334@eddelbuettel.com>
Message-ID: <74c69e370712190938j5ff9626bh84d36cc1d5f2024d@mail.gmail.com>

Hi,

I'm sorry I'm also coming late to this discussion, but  like Dirk, I
fail to understand what's wrong with using dbConnect() the way
the documentation, (see ?MySQL) suggests.

RMySQL was developed in a fully distributed client/server
environment, and it uses the MySQL-provided client API.  The
options that the MySQL client library allows (user, password, host,
port or socket file, compression, etc.) should work from RMySQL
(RMySQL just passes those directly to the MySQL client library).
If you have a specific (e.g., security-related) need that the MySQL client
API does not address, then the use of ssh may be the proper way
to go.  But for simple, vanilla style of client-server communications
the underlying MySQL client should be sufficient.

Regards,

--
David


On Dec 19, 2007 11:04 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
> On Wed, Dec 19, 2007 at 04:34:26PM +0200, Adrian Dusa wrote:
> > On Saturday 15 December 2007, Gabor Grothendieck wrote:
> > > Use ssh forwarding to forward local port 3307 to remote port 3306
> > > specifying the remote account and password.  Then if you use local port
> > > 3306 you can access your local version of MySQL and if you
> > > use port 3307 you can access the remote version.   There is some
> > > info on the MySQL site.  First test it out by running the mysql command
> > > line program accessing the remote data base via port 3307 and once
> > > that works you know its ok and you can try RMySQL or RODBC packages.
> >
> > For those interested in using R with a remote MySQL database, here's the most
> > simple and straightforward solution (thanks to Gabor):
> >
> > On the console, forward local port 3307 to remote port 3306
> > ssh -L 3307:xxx.xxx.xxx.xxx:3306 myuser at xxx.xxx.xxx.xxx
> >
> > (where xxx.xxx.xxx.xxx is the IP of the remote server and myuser is the login
> > name). After entering the password, keep that console open.
> >
> > In R:
> >
> > library(RMySQL)
> > drv <- dbDriver("MySQL")
> > dbConnect(drv, user="mysqluser", password="mysqlpassword",
> > dbname="anydatabase", host="127.0.0.1", port="3307")
>
> Err, I am late to this but
>
>  dbConnect(drv, user="mysqluser", password="mysqlpassword", dbname="anydatabase", host="xxx.xxx.xxx")
>
> works fine for me (eg on Ubuntu with a remote MySQL on some other
> box).  What's the problem you were seeing?
>
> Dirk
>
> --
> Three out of two people have difficulties with fractions.
>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dusa.adrian at gmail.com  Wed Dec 19 18:51:47 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Wed, 19 Dec 2007 19:51:47 +0200
Subject: [R] connecting [logging] RMySQL to an external server - SOLVED
In-Reply-To: <74c69e370712190938j5ff9626bh84d36cc1d5f2024d@mail.gmail.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<20071219160459.GA30334@eddelbuettel.com>
	<74c69e370712190938j5ff9626bh84d36cc1d5f2024d@mail.gmail.com>
Message-ID: <200712191951.47805.dusa.adrian@gmail.com>

Hi James,

On Wednesday 19 December 2007, David James wrote:
> Hi,
>
> I'm sorry I'm also coming late to this discussion, but  like Dirk, I
> fail to understand what's wrong with using dbConnect() the way
> the documentation, (see ?MySQL) suggests.
>
> RMySQL was developed in a fully distributed client/server
> environment, and it uses the MySQL-provided client API.  The
> options that the MySQL client library allows (user, password, host,
> port or socket file, compression, etc.) should work from RMySQL
> (RMySQL just passes those directly to the MySQL client library).
> If you have a specific (e.g., security-related) need that the MySQL client
> API does not address, then the use of ssh may be the proper way
> to go.  But for simple, vanilla style of client-server communications
> the underlying MySQL client should be sufficient.

Normally yes, you are right, but this particular MySQL server does not allow 
remote connections (it does not directly listens to port 3306) but only local 
connections using a socket (this terminology is unfamiliar to me, I may talk 
stupid).
So I have to first create a secure login and only after that connect to the 
remote MySQL server.

I hope my situation is more clear now,
Adrian



-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From edd at debian.org  Wed Dec 19 19:12:49 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 19 Dec 2007 12:12:49 -0600
Subject: [R] connecting [logging] RMySQL to an external server - SOLVED
In-Reply-To: <200712191951.47805.dusa.adrian@gmail.com>
References: <200712142055.15148.dusa.adrian@gmail.com>
	<20071219160459.GA30334@eddelbuettel.com>
	<74c69e370712190938j5ff9626bh84d36cc1d5f2024d@mail.gmail.com>
	<200712191951.47805.dusa.adrian@gmail.com>
Message-ID: <20071219181249.GB31399@eddelbuettel.com>

On Wed, Dec 19, 2007 at 07:51:47PM +0200, Adrian Dusa wrote:
> Hi James,
> 
> On Wednesday 19 December 2007, David James wrote:
> > Hi,
> >
> > I'm sorry I'm also coming late to this discussion, but  like Dirk, I
> > fail to understand what's wrong with using dbConnect() the way
> > the documentation, (see ?MySQL) suggests.
> >
> > RMySQL was developed in a fully distributed client/server
> > environment, and it uses the MySQL-provided client API.  The
> > options that the MySQL client library allows (user, password, host,
> > port or socket file, compression, etc.) should work from RMySQL
> > (RMySQL just passes those directly to the MySQL client library).
> > If you have a specific (e.g., security-related) need that the MySQL client
> > API does not address, then the use of ssh may be the proper way
> > to go.  But for simple, vanilla style of client-server communications
> > the underlying MySQL client should be sufficient.
> 
> Normally yes, you are right, but this particular MySQL server does not allow 
> remote connections (it does not directly listens to port 3306) but only local 
> connections using a socket (this terminology is unfamiliar to me, I may talk 
> stupid).
> So I have to first create a secure login and only after that connect to the 
> remote MySQL server.

Yes, that case is sometimes used on internet-facing or otherwise
widely visible machines where the number of open ports is to be
minimised.

In fact, this may be worth adding as a hint in the DBI or RMySQL
docs... 

Dirk

-- 
Three out of two people have difficulties with fractions.


From matthewsuderman at yahoo.com  Wed Dec 19 19:43:45 2007
From: matthewsuderman at yahoo.com (Matthew Suderman)
Date: Wed, 19 Dec 2007 10:43:45 -0800 (PST)
Subject: [R] unexpected behavior from gzfile and unz
Message-ID: <112228.85376.qm@web52507.mail.re2.yahoo.com>

I get unexpected behavior from "readLines()" and
"scan()" depending on how the file is opened with
"gzfile" or "unz".  More specifically:

> file <- gzfile("file.gz")
> readLines(file,1)
[1] "a\tb\tc"
> readLines(file,1)
[1] "a\tb\tc"
> close(file)

It seems that the stream is rewound between calls to
readLines.  The same is true if I replace readLines
with scan.

However, if I set argument 'open="r"', then rewinding
does not occur:

> file <- gzfile("file.gz",open="r")
> readLines(file,1)
[1] "a\tb\tc"
> readLines(file,1)
[1] "1\t2\t3"
> close(file)

Once again, I get the same behavior for scan.  The
rewinding behavior just described also appears if I
open a zip file with "unz".

> file <- unz("file.zip", "file.txt")
> readLines(file,1)
[1] "a\tb\tc"
> readLines(file,1)
[1] "a\tb\tc"
> close(file)

If I add the 'open="r"' argument to the call then I
get an error from readLines:

> file <- unz("file.zip", "file.txt", open="r")
> readLines(file,1)
Error in readLines(file, 1) : seek not enabled for
this connection
> close(file)


> file <- unz("file.zip", "file.txt", open="rb")
> readLines(file,1)
Error in readLines(file, 1) : seek not enabled for
this connection
> close(file)

However, if I instead use "scan" to read the file,
then there are no errors and I get the rewind/no
rewind behavior described above.

> file <- unz("file.zip", "file.txt")
> scan(file,nlines=1,sep="\t",what=character(0))
Read 3 items
[1] "a" "b" "c"
> scan(file,nlines=1,sep="\t",what=character(0))
Read 3 items
[1] "a" "b" "c"
> close(file)


> file <- unz("file.zip", "file.txt", open="r")
> scan(file,nlines=1,sep="\t",what=character(0))
Read 3 items
[1] "a" "b" "c"
> scan(file,nlines=1,sep="\t",what=character(0))
Read 3 items
[1] "1" "2" "3"
> close(file)

Is this a bug?

Matt



      ____________________________________________________________________________________
Be a better friend, newshound, and


From zuoj at un.org  Wed Dec 19 19:46:22 2007
From: zuoj at un.org (Jiaming Zuo)
Date: Wed, 19 Dec 2007 13:46:22 -0500
Subject: [R] (no subject)
Message-ID: <OFBE2AD317.965FAC01-ON852573B6.0067196D-852573B6.00671F96@un.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/a9072243/attachment.pl 

From Jacqueline.Spilak at EC.gc.ca  Wed Dec 19 19:50:29 2007
From: Jacqueline.Spilak at EC.gc.ca (Spilak,Jacqueline [Edm])
Date: Wed, 19 Dec 2007 11:50:29 -0700
Subject: [R] adding lines to a barchart
Message-ID: <4A6AB38B55B49C44A22E021A83CBEDDB021CA524@sr-pnr-exch3.prairie.int.ec.gc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/9222e10e/attachment.pl 

From johannes_graumann at web.de  Wed Dec 19 18:34:02 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Wed, 19 Dec 2007 18:34:02 +0100
Subject: [R] Factor Madness
References: <fk9kgo$rr$1@ger.gmane.org>
	<421236.96648.qm@web32804.mail.mud.yahoo.com>
Message-ID: <fkbdns$vh3$1@ger.gmane.org>

As Tony assumed: a data frame.

Joh

John Kane wrote:

> What was spectrum orginally?
> 
> 
> --- Johannes Graumann <johannes_graumann at web.de>
> wrote:
> 
>> Why is class(spectrum[["Ion"]]) after this "factor"?
>> 
>> spectrum <- cbind(spectrum,Ion=rep("",
>> nrow(spectrum)),Deviation.AMU=rep(0.0,
>> nrow(spectrum)))
>> 
>> slowly going crazy ...
>> 
>> Joh
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained,
>> reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From matthewsuderman at yahoo.com  Wed Dec 19 20:10:18 2007
From: matthewsuderman at yahoo.com (Matthew Suderman)
Date: Wed, 19 Dec 2007 11:10:18 -0800 (PST)
Subject: [R] unexpected behavior from gzfile and unz
Message-ID: <936154.69530.qm@web52502.mail.re2.yahoo.com>

I figured out the problem: functions
gzfile/unz/file/... create a connection to a file but
do not *open* the connection unless the 'open'
argument  is specified.  

... a little R gotcha for people who use other
programming languages and expect similar
concepts/behavior.

> I get unexpected behavior from "readLines()" and
> "scan()" depending on how the file is opened with
> "gzfile" or "unz".  More specifically:
> 
> > file <- gzfile("file.gz")
> > readLines(file,1)
> [1] "a\tb\tc"
> > readLines(file,1)
> [1] "a\tb\tc"
> > close(file)
> 
> It seems that the stream is rewound between calls to
> readLines.  The same is true if I replace readLines
> with scan.
> 
> However, if I set argument 'open="r"', then
> rewinding
> does not occur:
> 
> > file <- gzfile("file.gz",open="r")
> > readLines(file,1)
> [1] "a\tb\tc"
> > readLines(file,1)
> [1] "1\t2\t3"
> > close(file)
> 
> Once again, I get the same behavior for scan.  The
> rewinding behavior just described also appears if I
> open a zip file with "unz".
> 
> > file <- unz("file.zip", "file.txt")
> > readLines(file,1)
> [1] "a\tb\tc"
> > readLines(file,1)
> [1] "a\tb\tc"
> > close(file)
> 
> If I add the 'open="r"' argument to the call then I
> get an error from readLines:
> 
> > file <- unz("file.zip", "file.txt", open="r")
> > readLines(file,1)
> Error in readLines(file, 1) : seek not enabled for
> this connection
> > close(file)
> 
> 
> > file <- unz("file.zip", "file.txt", open="rb")
> > readLines(file,1)
> Error in readLines(file, 1) : seek not enabled for
> this connection
> > close(file)
> 
> However, if I instead use "scan" to read the file,
> then there are no errors and I get the rewind/no
> rewind behavior described above.
> 
> > file <- unz("file.zip", "file.txt")
> > scan(file,nlines=1,sep="\t",what=character(0))
> Read 3 items
> [1] "a" "b" "c"
> > scan(file,nlines=1,sep="\t",what=character(0))
> Read 3 items
> [1] "a" "b" "c"
> > close(file)
> 
> 
> > file <- unz("file.zip", "file.txt", open="r")
> > scan(file,nlines=1,sep="\t",what=character(0))
> Read 3 items
> [1] "a" "b" "c"
> > scan(file,nlines=1,sep="\t",what=character(0))
> Read 3 items
> [1] "1" "2" "3"
> > close(file)
> 
> Is this a bug?
> 
> Matt
> 
> 
> 



      ____________________________________________________________________________________
Be a better friend, newshound, and


From jrkrideau at yahoo.ca  Wed Dec 19 20:42:59 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 19 Dec 2007 14:42:59 -0500 (EST)
Subject: [R] question
In-Reply-To: <2DA68F10815D6E49956E3DC2CE18603E0113D7C9@SRVSTAFF.iese.org>
Message-ID: <981418.98479.qm@web32804.mail.mud.yahoo.com>

I think this is what you want.

Y[Y>0] <- 1

--- "Armelini, Guillermo" <DOCGArmelini at iese.edu>
wrote:

> Hello everyone!
>  
> Is anybody can help me to solve this silly question
> that unfortunately I haven't found the right way to
> address it.
>  
> Supose I have a matrix X[n,n] dimension
>  
> I would like to calculate the product of the vectors
>  
> Y=X[1,n]*X[n,1]
>  
> Then I would like to run the following operation
>  
> H=if(Y>0) {H[,1]=1}
>  
> The problem is that the condition "if" works only
> for the first element of the matrix.
>  
> Does anybody knows how to apply the conditional "if"
> to a entire vector?
>  
> cheers
>  
> Guillermo
>  
> 
> 
> This message has been scanned for viruses by
> TRENDMICRO,
> an IESE technology affiliate company  and global
> leader in antivirus and content security software.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
> 



      Looking for the perfect gift? Give the gift of Flickr!


From jturpin at med.wayne.edu  Wed Dec 19 20:50:27 2007
From: jturpin at med.wayne.edu (Turpin, Jennifer)
Date: Wed, 19 Dec 2007 14:50:27 -0500
Subject: [R] Biostatistician & Epidemiologist posting
Message-ID: <682B2B8F328A234CAEF60C4CB7D58AEA03EE6027@MED-CORE03-MS3.med.wayne.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/8c4588a9/attachment.pl 

From deepayan.sarkar at gmail.com  Wed Dec 19 21:31:59 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 19 Dec 2007 12:31:59 -0800
Subject: [R] adding lines to a barchart
In-Reply-To: <4A6AB38B55B49C44A22E021A83CBEDDB021CA524@sr-pnr-exch3.prairie.int.ec.gc.ca>
References: <4A6AB38B55B49C44A22E021A83CBEDDB021CA524@sr-pnr-exch3.prairie.int.ec.gc.ca>
Message-ID: <eb555e660712191231q74bb7992kc5e3dc63e75820e5@mail.gmail.com>

On 12/19/07, Spilak,Jacqueline [Edm] <Jacqueline.Spilak at ec.gc.ca> wrote:
> Hi all
> I can't find what I am looking for so I am asking here.  I have a
> dataset that looks something like this.
>
> Year   season  percent_below
> 2000 Winter     6.9179870
> 2000 Spring     1.6829436
> 2000 Summer     1.8463501
> 2000 Autumn     3.8184993
> 2001 Winter     2.8832806
> 2001 Spring     2.5870511
> 2001 Summer     0.0000000
> 2001 Autumn     4.7248240
> 2002 Winter     4.4532238
> 2002 Spring     3.7468846
> 2002 Summer     1.0784311
> 2002 Autumn     3.7061533
>
> I have plotted this nicely using
> barchart(percent_below ~ factor(Season, levels=Season), data= dataset1,
> group=Year)
> This gives me a barplot by season with each year shown in that season.
> Now the tricky part.  My data is from 2000 to 2007 and 2007 is an
> important year.  I have an upper and lower limit for percent_below for
> 2007 for each season.  I would like to add the upper and lower limit of
> 2007 as a black line to each season.  The upper and lower limit will be
> different for each season.  This is being done so for a comparison so I
> need it done this way.

I would suggest a slightly different format. Here's an example with
one set of limits; you can add another set similarly. If you really
want a single panel, you could use panel.segments().

barchart(percent_below ~ factor(Year) | factor(season, levels=unique(season)),
         data= dataset1, origin = 0, layout = c(4, 1),
         upper_2007 = c(6, 4, 5, 3),
         panel = function(..., upper_2007) {
             panel.abline(h = upper_2007[packet.number()])
             panel.barchart(...)
         })

-Deepayan


From topkatz at msn.com  Wed Dec 19 21:35:32 2007
From: topkatz at msn.com (Talbot Katz)
Date: Wed, 19 Dec 2007 15:35:32 -0500
Subject: [R] Function reference
Message-ID: <BAY108-W24399E16AF1D4423E67DF8AA5C0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/a37285f7/attachment.pl 

From john.bullock at aya.yale.edu  Wed Dec 19 21:55:26 2007
From: john.bullock at aya.yale.edu (John G. Bullock)
Date: Wed, 19 Dec 2007 12:55:26 -0800
Subject: [R] lattice: axes drawn when relation='free' or relation='sliced'
	but not when relation='same'
Message-ID: <fkc0gq$5s0$1@ger.gmane.org>


I'm using lattice to draw a multi-panel figure: 5 rows, 4 columns.  The y-axis for each panel is determined by

  yaxs            <- list(draw=T
                          , labels=c(0, '.5', '1', '1.5')
                          , at=c(0, .5, 1, 1.5)
                          , tck=c(.4, 0)
                          , cex=.7
                          , alternating=2
                          , relation='same'
                          , rot=90)

fig1 <- xyplot( [...], scales=list(x=xaxs, y=yaxs))

I want this to draw ticks and labels on the left-hand side of each panel.  Instead, I get ticks on the left-hand side of only the
panels in the leftmost column, and labels only on the right-hand side of the rightmost column.  What can account for that?

What seems especially peculiar is that when I change relation to 'sliced' or 'free', the axes are drawn as I want them to be drawn.

The data that I'm plotting are numeric and range from -.3 to 1.9.  I'm using R 2.6.1 and lattice 0.17-2 on Windows XP.

Thank you,
--John


From deepayan.sarkar at gmail.com  Wed Dec 19 22:12:32 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 19 Dec 2007 13:12:32 -0800
Subject: [R] lattice: axes drawn when relation='free' or
	relation='sliced' but not when relation='same'
In-Reply-To: <fkc0gq$5s0$1@ger.gmane.org>
References: <fkc0gq$5s0$1@ger.gmane.org>
Message-ID: <eb555e660712191312v736ba860i40d5e3275de3ac@mail.gmail.com>

On 12/19/07, John G. Bullock <john.bullock at aya.yale.edu> wrote:
>
> I'm using lattice to draw a multi-panel figure: 5 rows, 4 columns.  The y-axis for
> each panel is determined by
>
>   yaxs            <- list(draw=T
>                           , labels=c(0, '.5', '1', '1.5')
>                           , at=c(0, .5, 1, 1.5)
>                           , tck=c(.4, 0)
>                           , cex=.7
>                           , alternating=2
>                           , relation='same'
>                           , rot=90)
>
> fig1 <- xyplot( [...], scales=list(x=xaxs, y=yaxs))
>
> I want this to draw ticks and labels on the left-hand side of each panel.  Instead,
> I get ticks on the left-hand side of only the
> panels in the leftmost column, and labels only on the right-hand side of the
> rightmost column.  What can account for that?

Account for what? What you have described is a fundamental aspect of
the design of Trellis graphics (which clearly happens to be different
from how you would have designed it, but so what?).

> What seems especially peculiar is that when I change relation to 'sliced' or 'free',
> the axes are drawn as I want them to be drawn.

Well, doesn't seem very peculiar to me.

> The data that I'm plotting are numeric and range from -.3 to 1.9.  I'm using R 2.6.1
> and lattice 0.17-2 on Windows XP.

If you want the same limits in every panel even when relation="free",
put in an explicit ylim=c(-0.2, 2) or something similar.

-Deepayan


From tplate at acm.org  Wed Dec 19 22:15:03 2007
From: tplate at acm.org (Tony Plate)
Date: Wed, 19 Dec 2007 14:15:03 -0700
Subject: [R] Function reference
In-Reply-To: <BAY108-W24399E16AF1D4423E67DF8AA5C0@phx.gbl>
References: <BAY108-W24399E16AF1D4423E67DF8AA5C0@phx.gbl>
Message-ID: <476989D7.9050008@acm.org>

R does this sort of thing easily without any parse/eval acrobatics needed. 
  E.g., you can do:

 > stu <- function(x) {return( 1 + (2*x*x) - (3*x) )}
 > (x <- 0:3)
[1] 0 1 2 3
 > stu(x)
[1]  1  0  3 10
 > metafun <- function(FUN, data) FUN(data)
 > metafun(stu, x)
[1]  1  0  3 10
 > # if you want to be able use a character-data name for the function:
 > metafun2 <- function(FUN, data) {if (is.character(FUN)) FUN <- 
getFunction(FUN); FUN(data)}
 > metafun2("stu", x)
[1]  1  0  3 10
 > metafun2(stu, x)
[1]  1  0  3 10
 >

What went wrong with your code was that your parse() constructed a list of 
4 expressions, and evaluating that returned the value of the last one:

 > (fun <- "stu")
[1] "stu"
 > paste( fun, "(", x, ")", sep = "" )
[1] "stu(0)" "stu(1)" "stu(2)" "stu(3)"
 > parse( text = paste( fun, "(", x, ")", sep = "" ) )
expression(stu(0), stu(1), stu(2), stu(3))
attr(,"srcfile")
<text>
 >

(Others have observed that in a very large proportion of the situations 
where people reach for parse/eval, there's a neater, cleaner & more direct 
way of doing the job.)

-- Tony Plate

Talbot Katz wrote:
> Hi.
>  
> I'm looking for an R equivalent to something like function pointers in C/C++.  I have a search procedure that evaluates the fitness of each point it reaches as it moves along, and decides where to move next based on its fitness evaluation.  I want to be able to pass different fitness functions to this procedure.  I am trying to find a good way to do this.  I was thinking of passing in the name of the function and then using eval.  However, I haven't gotten this to work the way I'd like it to.  Consider the following example:
>  
>  
>> stu <- function(x) {return( 1 + (2*x*x) - (3*x) )}> (x=0:3)[1] 0 1 2 3> stu(x)[1]  1  0  3 10> (fun="stu")[1] "stu"> eval( parse( text = paste( fun, "(", x, ")", sep = "" ) ) )[1] 10>
>  
>  
> Notice that the function I defined called "stu" will operate on a vector x and return a vector y = stu(x) such that y[i] equals stu(x[i]).  When I tried to pass stu and x to a procedure that would evaluate stu(x) I only get stu(x[N]), when N is the last element of x.  What am I doing wrong?  Is there a better way to pass function references?
>  
> I can get the following to work, but it seems awfully clunky:
>  
>> sapply( 1:length(x), function(i){ return( eval( parse( text = paste( fun, "(", x[i], ")", sep = "" ) ) ) ) } )[1]  1  0  3 10>
>  
> Thanks!
>  
> --  TMK  --212-460-5430 home917-656-5351 cell
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Wed Dec 19 23:09:45 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Dec 2007 22:09:45 +0000 (GMT)
Subject: [R] library(rpart) or library(tree)
In-Reply-To: <47695185.25095.209620F@ingoholz.uni-hohenheim.de>
References: <47695185.25095.209620F@ingoholz.uni-hohenheim.de>
Message-ID: <Pine.LNX.4.64.0712191628040.2983@gannet.stats.ox.ac.uk>

You appear to have fitted a regression tree, which does not seem to be 
what your interpretation of 'pnV22' requires.

I have little idea what you actually did, but am confident that it is not 
what you claim you did.

Also, note fortune("dog"):

Firstly, don't call your matrix 'matrix'. Would you call your dog 'dog'?
Anyway, it might clash with the function 'matrix'.
    -- Barry Rowlingson
       R-help (October 2004)

On Wed, 19 Dec 2007, Ingo Holz wrote:

> Hi,
>
> I have a problem with library (rpart) (and/or library(tree)).
>
> I use a data.frame with variables
> "pnV22" (observation: 1, 0 or yes, no)
> "JTemp" (mean temperature)
> "SNied"  (summer rain)
>
> I used function "rpart" to build a model:
>
> 	library(rpart)
> 	attach(data.frame)
> 	result <- rpart(pnV22 ~ JTemp + SNied)
>
> I got the following tree:

I don't believe that: how could rpart know about 'punkte'?

>  n=55518 (50 observations deleted due to missingness)
>
> node), split, n, deviance, yval
>      * denotes terminal node
>
> 1) root 55518 668.744500 0.0121942400
>   2) punkte[["JTemp"]]< 10.35 51251  18.992960 0.0003707245 *
>   3) punkte[["JTemp"]]>=10.35 4267 556.532000 0.1542067000
>     6) punkte[["SNied"]]>=450 3136 291.318600 0.1036352000 *
>     7) punkte[["SNied"]]< 450 1131 234.954900 0.2944297000
>      14) punkte[["JTemp"]]>=10.55 723 113.502100 0.1950207000 *
>      15) punkte[["JTemp"]]< 10.55 408 101.647100 0.4705882000
>        30) punkte[["JTemp"]]< 10.45 48   4.479167 0.1041667000 *
>        31) punkte[["JTemp"]]>=10.45 360  89.863890 0.5194444000 *
>
> I constructed a simple new.data.frame:
>
>     new.data.fame <- data.frame
>     new.data.frame[,"JTemp"] <- 10.5
>     new.data.frame[,"SNied"] <- 430
>
> Than I used predict() to predict values for "pnV22" in the following way:
>
>    pred <- predict(result, data.frame)
>    pred2 <- predict(result, new.data.frame)

It is not finding the new values from the new data frame: they do not have 
names like 'punkte[["JTemp"]]'.

> The results are the same, which I checked by ploting the values of pred and pred2 and by
>
>   table(pred ==pred2)  which is true for all values.
>
> Looking at the tree I would expect that pred2 has the same high value for all elements of the
> vector. Did I make a mistake?
>
> Thanks, Ingo
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From m_olshansky at yahoo.com  Wed Dec 19 23:11:25 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 19 Dec 2007 14:11:25 -0800 (PST)
Subject: [R] creating a database
In-Reply-To: <14375875.post@talk.nabble.com>
Message-ID: <242682.90725.qm@web32210.mail.mud.yahoo.com>

If all your entries are double precision then you are
using 8 bytes per entry, so 20,000*n entries are just
160,000*n bytes, i.e. less than 160*n Kb. If your n is
100 you get 16 Mb which is not that much (especially
if you pre-allocate it only once). So just use the
matrix and don't worry!

--- dxc13 <dxc13 at health.state.ny.us> wrote:

> 
> useR's,
> 
> I am writing a program in which the input can be
> multidimensional.  As of
> now, to hold the input, I have created an n by m
> matrix where n is the
> number of observations and m is the number of
> variables.  The data that I
> could potentially use can contain well over 20,000
> observations.  
> 
> Can a simple matrix be used for this or would it be
> better and more
> efficient to create an external database to hold the
> data.  If so, should
> the database be created using C and how would I do
> this (seeing as that I
> have never programmed in C)?  
> 
> Any help would be greatly appreciated.  Thank you
> 
> Derek
> -- 
> View this message in context:
>
http://www.nabble.com/creating-a-database-tp14375875p14375875.html
> Sent from the R help mailing list archive at
> Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From jasonshi510 at hotmail.com  Wed Dec 19 23:12:29 2007
From: jasonshi510 at hotmail.com (Xin)
Date: Wed, 19 Dec 2007 22:12:29 -0000
Subject: [R] can optimize solve paired euqations?
Message-ID: <BAY141-DAV5B7B4AAB5E543044966C3F05C0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071219/5f5fc327/attachment.pl 

From m_olshansky at yahoo.com  Wed Dec 19 23:22:19 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 19 Dec 2007 14:22:19 -0800 (PST)
Subject: [R] median of binned values
In-Reply-To: <4768E557.9000404@optonline.net>
Message-ID: <320326.37982.qm@web32213.mail.mud.yahoo.com>

Alternatively
levels(df$binname)[which(df$freq >=
0.5*cumsum(df$freq)[nrow(df)])[1]]

--- Chuck Cleland <ccleland at optonline.net> wrote:

> Martin Tomko wrote:
> > Dear list,
> > I have a vector (array, table row, whatever is
> best) of frequency values 
> > for categories (or bins), and I need to find the
> median category. 
> > Trivial to do by hand, but I was wondering if
> there is a means to  do it 
> > in R in an elegant way.
> > 
> > The obvious medioan(vector) returns the median
> frequency for the binns, 
> > and that is not what I want. i.e,:
> >              freq
> > cat1    1
> > cat2   10  
> > cat3   100  
> > cat4   1000
> > cat5   10000
> > 
> > I want it to return cat5, instead of cat3.
> 
> df <- data.frame(binname = as.factor(paste("cat",
> 1:5, sep="")),
>                  freq = c(1,10,100,1000,10000))
> 
> df
>   binname  freq
> 1    cat1     1
> 2    cat2    10
> 3    cat3   100
> 4    cat4  1000
> 5    cat5 10000
> 
> with(df,
> levels(binname)[median(rep(as.numeric(binname),
> freq))])
> [1] "cat5"
> 
> > Thanks a lot
> > Martin
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code. 
> 
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From Sam.McClatchie at noaa.gov  Wed Dec 19 23:48:15 2007
From: Sam.McClatchie at noaa.gov (Sam McClatchie)
Date: Wed, 19 Dec 2007 14:48:15 -0800
Subject: [R] clim.pact package ncdf dependency/ netcdf.h
In-Reply-To: <4717DF25.5070506@noaa.gov>
References: <4717DF25.5070506@noaa.gov>
Message-ID: <47699FAF.1050901@NOAA.Gov>

System:
Linux kernel 2.6.22-14
Ubuntu 7.10 gutsy
ESS 5.3.0 on Emacs 22.1.1
R version 2.6.0

Colleagues

I would like to use the user contributed package "clim.pact". I'm having
trouble with the dependency of "clim.pact" on "ncdf". Like others I am
finding that R CMD INSTALL does not find the netcdf.h file, even when
the directory (/usr/local/include) holding the netcdf.h file is passed to
R CMD.

Unless I misunderstood one of the postings there was an error
related to this problem on the OSX
build of the ncdf package 
<http://osdir.com/ml/lang.r.mac/2006-09/msg00012.html>.

I've install the obvious netcdf packages and
dependencies on ubuntu and the ncdump works fine.

Has anyone solved this problem on linux?

Best fishes

Sam
-- 
Sam McClatchie,
Fisheries oceanographer
Southwest Fisheries Science Center, NOAA,
8604 La Jolla Shores Drive
La Jolla, CA 92037-1508, U.S.A.
email <Sam.McClatchie at noaa.gov>
work phone: 858 546 7083
Cellular:  858 752 8495
Research home page <http://www.fishocean.info>

                    /\
       ...>><xX(?>
                 //// \\\\
                    <?)Xx><<
               /////  \\\\\\
                         ><(((?>
   >><(((?>   ...>><xX(?>O<?)Xx><<


From jholtman at gmail.com  Wed Dec 19 23:48:28 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 19 Dec 2007 17:48:28 -0500
Subject: [R] can optimize solve paired euqations?
In-Reply-To: <BAY141-DAV5B7B4AAB5E543044966C3F05C0@phx.gbl>
References: <BAY141-DAV5B7B4AAB5E543044966C3F05C0@phx.gbl>
Message-ID: <644e1f320712191448r621b6a57if484648d1be8c2d8@mail.gmail.com>

In your first case you seem to be missing a comma after the "^2"; also
missing value for 'y'.  Probably should be something like:

optimize(function(x,y)
((327.727-(1-0.114^10)*y*(1-x)/x/(1-x^y))+(9517.336-327.727
*(1+(1-x)*(1+y)/x-327.727)))^2,
interval=c(0,1), y=1)


On Dec 19, 2007 5:12 PM, Xin <jasonshi510 at hotmail.com> wrote:
> I used the command below, but R gives me the error message--syntax error.
> can anyone see the mistakes I made?
>
>
> optimize(function(x,y)
> + ((327.727-(1-0.114^10)*y*(1-x)/x/(1-x^y))+(9517.336-327.727 *(1+(1-x)*(1+y)/x-327.727)))^2
> + interval=c(0,1))
>
>
> At the same time, I use nlm() but R gives me the code
>
> $code
> [1] 3
>
> function(vals) {
>
>        x <- vals[1]
>
>        y <- vals[2]
>
>        sum(c((199.458913633542-(1-0.114^10)*y*(1-x)/x/(1-x^y)),(5234.11964684527-199.458913633542*(1+(1-x)*(1+y)/x-199.458913633542)))^2)
>
> }
>
>
>
> nlm(f, c(2,2))
>
>
>
>
>
>
>
> Any one know how to deal with this case?
>
> Thanks
>
> Xin
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From mnevill at exitcheck.net  Wed Dec 19 23:43:35 2007
From: mnevill at exitcheck.net (Max)
Date: Wed, 19 Dec 2007 14:43:35 -0800
Subject: [R] Question about which kind of plot to use
Message-ID: <mn.9b737d7cee7588f6.83239@exitcheck.net>

Hi Everyone,

I've got a question about data representation. I have some psychometric 
data with 5 scores for 15 different groups. I've been asked to show 
some kind of mean plots. The data below is the mean and SD for a given 
group, unfortunately my employer doesn't want me posting full datasets. 
:(

 The groups V,W,X,Y,Z are divided into Bottom, (B), Middle (M) and Top 
(T). An example of my data is shown below.

Score 1
Mean				             SD
	B	    M	    T		    B	  M	    T
V	86.9	13.0	88.8		16.9  2.0	10.5
W	16.1	96.1	17.7		2.2	  4.6	1.7
X	50.7	61.1	74.7		4.7	  3.7	7.6
Y	68.5	99.7	37.6		6.0	  8.0	2.3
Z	92.7	22.3	69.4		6.5	  1.2	2.2

What I did before was a standard mean plot:

plotMeans(w$score1, w$Factor, 
error.bars="sd",xlab="Factor",ylab="Score",main="Group W Score 1 Plot")

 However, with 15 groups and 5 scores this turns into 75 individual 
graphs. Is there a way to layer mean plots? Or show several mean plots 
in the Same graph? Any ideas or suggestions would be great.

thanks,

-Max


From dylan.beaudette at gmail.com  Thu Dec 20 01:25:46 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Wed, 19 Dec 2007 16:25:46 -0800
Subject: [R] Question about which kind of plot to use
In-Reply-To: <mn.9b737d7cee7588f6.83239@exitcheck.net>
References: <mn.9b737d7cee7588f6.83239@exitcheck.net>
Message-ID: <200712191625.46587.dylan.beaudette@gmail.com>

On Wednesday 19 December 2007, Max wrote:
> Hi Everyone,
>
> I've got a question about data representation. I have some psychometric
> data with 5 scores for 15 different groups. I've been asked to show
> some kind of mean plots. The data below is the mean and SD for a given
> group, unfortunately my employer doesn't want me posting full datasets.
>
> :(
>
>  The groups V,W,X,Y,Z are divided into Bottom, (B), Middle (M) and Top
> (T). An example of my data is shown below.
>
> Score 1
> Mean				             SD
> 	B	    M	    T		    B	  M	    T
> V	86.9	13.0	88.8		16.9  2.0	10.5
> W	16.1	96.1	17.7		2.2	  4.6	1.7
> X	50.7	61.1	74.7		4.7	  3.7	7.6
> Y	68.5	99.7	37.6		6.0	  8.0	2.3
> Z	92.7	22.3	69.4		6.5	  1.2	2.2
>
> What I did before was a standard mean plot:
>
> plotMeans(w$score1, w$Factor,
> error.bars="sd",xlab="Factor",ylab="Score",main="Group W Score 1 Plot")
>
>  However, with 15 groups and 5 scores this turns into 75 individual
> graphs. Is there a way to layer mean plots? Or show several mean plots
> in the Same graph? Any ideas or suggestions would be great.
>
> thanks,
>
> -Max
>

How about a lattice plot using panels ? plot the distribution of each score 
(box and whisker style), using a panel for each group?

a <- rnorm(100)
b <- rnorm(100)
c <- rnorm(100)
 d <- rnorm(100)

library(lattice)
new <- make.groups(a,b,c,d

new$grp <- rep(gl(5,20, labels=c('A','B','C','D','E')), 4)

bwplot(data ~ which | grp, data=new)

Not quite means, but close!

Dylan



-- 
Dylan Beaudette
Soil Resource Laboratory
http://casoilresource.lawr.ucdavis.edu/
University of California at Davis
530.754.7341


From r.otasuke at gmail.com  Thu Dec 20 01:22:34 2007
From: r.otasuke at gmail.com (Kunio takezawa)
Date: Thu, 20 Dec 2007 09:22:34 +0900
Subject: [R] "gam()" in "gam" package
In-Reply-To: <4b2e15cd0712181710k68e930bdi938587229f6b6438@mail.gmail.com>
References: <s767a051.089@tedmail.lgc.co.uk>
	<4b2e15cd0712181710k68e930bdi938587229f6b6438@mail.gmail.com>
Message-ID: <4b2e15cd0712191622l38a5f904n70ba3e6539fe6808@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071220/1d0a6c13/attachment.pl 

From deepayan.sarkar at gmail.com  Thu Dec 20 01:40:37 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 19 Dec 2007 16:40:37 -0800
Subject: [R] Question about which kind of plot to use
In-Reply-To: <200712191625.46587.dylan.beaudette@gmail.com>
References: <mn.9b737d7cee7588f6.83239@exitcheck.net>
	<200712191625.46587.dylan.beaudette@gmail.com>
Message-ID: <eb555e660712191640g49709ac5hb84c0d0278d01fdb@mail.gmail.com>

On 12/19/07, Dylan Beaudette <dylan.beaudette at gmail.com> wrote:
> On Wednesday 19 December 2007, Max wrote:
> > Hi Everyone,
> >
> > I've got a question about data representation. I have some psychometric
> > data with 5 scores for 15 different groups. I've been asked to show
> > some kind of mean plots. The data below is the mean and SD for a given
> > group, unfortunately my employer doesn't want me posting full datasets.
> >
> > :(
> >
> >  The groups V,W,X,Y,Z are divided into Bottom, (B), Middle (M) and Top
> > (T). An example of my data is shown below.
> >
> > Score 1
> > Mean                                       SD
> >       B           M       T               B     M         T
> > V     86.9    13.0    88.8            16.9  2.0       10.5
> > W     16.1    96.1    17.7            2.2       4.6   1.7
> > X     50.7    61.1    74.7            4.7       3.7   7.6
> > Y     68.5    99.7    37.6            6.0       8.0   2.3
> > Z     92.7    22.3    69.4            6.5       1.2   2.2
> >
> > What I did before was a standard mean plot:
> >
> > plotMeans(w$score1, w$Factor,
> > error.bars="sd",xlab="Factor",ylab="Score",main="Group W Score 1 Plot")
> >
> >  However, with 15 groups and 5 scores this turns into 75 individual
> > graphs. Is there a way to layer mean plots? Or show several mean plots
> > in the Same graph? Any ideas or suggestions would be great.
> >
> > thanks,
> >
> > -Max
> >
>
> How about a lattice plot using panels ? plot the distribution of each score
> (box and whisker style), using a panel for each group?
>
> a <- rnorm(100)
> b <- rnorm(100)
> c <- rnorm(100)
>  d <- rnorm(100)
>
> library(lattice)
> new <- make.groups(a,b,c,d
>
> new$grp <- rep(gl(5,20, labels=c('A','B','C','D','E')), 4)
>
> bwplot(data ~ which | grp, data=new)
>
> Not quite means, but close!

And

demo("intervals", package = "lattice")

shows you how to incorporate confidence intervals.

-Deepayan


From m_olshansky at yahoo.com  Thu Dec 20 02:39:42 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 19 Dec 2007 17:39:42 -0800 (PST)
Subject: [R] Calculate remainer
In-Reply-To: <14414906.post@talk.nabble.com>
Message-ID: <35273.75725.qm@web32211.mail.mud.yahoo.com>

> 50 %% 12
[1] 2
> 50 %/% 12
[1] 4

> ?Arithmetic


--- livia <yn19832 at msn.com> wrote:

> 
> Hello everyone,
> 
> I have got a question about a simple calculation. If
> I would like to
> calculate 50/12 and return the result as 4 and the
> remainer 2. Is there a
> function of doing this?
> 
> Many thanks.
> -- 
> View this message in context:
>
http://www.nabble.com/Calculate-remainer-tp14414906p14414906.html
> Sent from the R help mailing list archive at
> Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From Antonio.Gasparrini at lshtm.ac.uk  Thu Dec 20 02:47:42 2007
From: Antonio.Gasparrini at lshtm.ac.uk (Antonio Gasparrini)
Date: Thu, 20 Dec 2007 01:47:42 +0000
Subject: [R] mle
Message-ID: <4769C9B3020000B200035DF1@smtp-a.lshtm.ac.uk>

Dear all,

I'm trying to estimate the parameters of a special case of a poisson
model, where the specified equation has an integral and several fixed
parameters.
I think that the MLE command in STATS4 package could be a good choice,
but it's a little complicated. I've got some problems with the offset
and I don't understand some of the functions. Do you know where can I
find some explicit tutorial about a poisson regression with MLE or,
maybe better, about maximum likelyhood estimation?

Best wishes

Antonio Gasparrini
Public and Environmental Health Research Unit (PEHRU)
London School of Hygiene & Tropical Medicine
Keppel Street, London WC1E 7HT, UK
Office: 0044 (0)20 79272406
Mobile: 0044 (0)79 64925523
www.lshtm.ac.uk/pehru/
antonio.gasparrini at lshtm.ac.uk


From rmailbox at justemail.net  Thu Dec 20 03:14:07 2007
From: rmailbox at justemail.net (Eric)
Date: Wed, 19 Dec 2007 18:14:07 -0800
Subject: [R] Question about which kind of plot to use
In-Reply-To: <eb555e660712191640g49709ac5hb84c0d0278d01fdb@mail.gmail.com>
References: <mn.9b737d7cee7588f6.83239@exitcheck.net>	<200712191625.46587.dylan.beaudette@gmail.com>
	<eb555e660712191640g49709ac5hb84c0d0278d01fdb@mail.gmail.com>
Message-ID: <4769CFEF.5090103@justemail.net>

Deepayan Sarkar wrote:
> On 12/19/07, Dylan Beaudette <dylan.beaudette at gmail.com> wrote:
>   
>> On Wednesday 19 December 2007, Max wrote:
>>     
>>> Hi Everyone,
>>>
>>> I've got a question about data representation. I have some psychometric
>>> data with 5 scores for 15 different groups. I've been asked to show
>>> some kind of mean plots. The data below is the mean and SD for a given
>>> group, unfortunately my employer doesn't want me posting full datasets.
>>>
>>> :(
>>>
>>>  The groups V,W,X,Y,Z are divided into Bottom, (B), Middle (M) and Top
>>> (T). An example of my data is shown below.
>>>
>>> Score 1
>>> Mean                                       SD
>>>       B           M       T               B     M         T
>>> V     86.9    13.0    88.8            16.9  2.0       10.5
>>> W     16.1    96.1    17.7            2.2       4.6   1.7
>>> X     50.7    61.1    74.7            4.7       3.7   7.6
>>> Y     68.5    99.7    37.6            6.0       8.0   2.3
>>> Z     92.7    22.3    69.4            6.5       1.2   2.2
>>>
>>> What I did before was a standard mean plot:
>>>
>>> plotMeans(w$score1, w$Factor,
>>> error.bars="sd",xlab="Factor",ylab="Score",main="Group W Score 1 Plot")
>>>
>>>  However, with 15 groups and 5 scores this turns into 75 individual
>>> graphs. Is there a way to layer mean plots? Or show several mean plots
>>> in the Same graph? Any ideas or suggestions would be great.
>>>
>>> thanks,
>>>
>>> -Max
>>>
>>>       
>> How about a lattice plot using panels ? plot the distribution of each score
>> (box and whisker style), using a panel for each group?
>>
>> a <- rnorm(100)
>> b <- rnorm(100)
>> c <- rnorm(100)
>>  d <- rnorm(100)
>>
>> library(lattice)
>> new <- make.groups(a,b,c,d
>>
>> new$grp <- rep(gl(5,20, labels=c('A','B','C','D','E')), 4)
>>
>> bwplot(data ~ which | grp, data=new)
>>
>> Not quite means, but close!
>>     
>
> And
>
> demo("intervals", package = "lattice")
>
> shows you how to incorporate confidence intervals.
>
> -Deepayan
>
>   

Perhaps as long as you're learning a new plotting system, you might also 
check out whether ggplot2 might be an option.

I did a quick and dirty version (which I'm sure Hadley can improve and 
also remind me how to get rid of the legend that shows the "3" that I 
set the size to).

Assuming your data is re-shaped, so it comes out something like mine in 
the artificial example below, then it's a two-liner in ggplot:


maxdat.df <- data.frame (
    score1 =  rnorm(9, mean = rep(c(10,20,30), each = 3), sd = 1 ) ,
    SD = runif(9) * 2 + .5,
    Group = factor ( rep ( c("V", "W", "X"), each = 3 ) ),
    subGroup = rep( c("B","M","T"), 3) )
   
maxdat.df

library(ggplot2)
ggp <- ggplot ( maxdat.df, aes (y = score1, x = interaction(Group , 
subGroup), min = score1 - SD, max = score1 + SD, size = 3) )
ggp + geom_pointrange() + coord_flip()


Eric


From m_olshansky at yahoo.com  Thu Dec 20 07:07:40 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 19 Dec 2007 22:07:40 -0800 (PST)
Subject: [R] assigning and saving datasets in a loop,
	with names changing with "i"
In-Reply-To: <WorldClient-F200712182124.AA24320054@epimgh.mcgill.ca>
Message-ID: <171801.33635.qm@web32207.mail.mud.yahoo.com>

Won't it be simpler to do:

for (i in 1:12){
data <- my.fun(my.list[i]))   
save(data,file = paste("data",i,".RData", sep="")) }


--- Marie Pierre Sylvestre
<MP.Sylvestre at epimgh.mcgill.ca> wrote:

> Dear R users,
> 
> I am analysing a very large data set and I need to
> perform several data
> manipulations. The dataset is so big that the only
> way I can play with it
> without having memory problems (E.g. "cannot
> allocate vectors of size...")
> is to write a batch script to:
> 
> 1. cut the data into pieces 
> 2. save the pieces in seperate .RData files
> 3. Remove everything from the environment
> 4. load one of the piece
> 5. perform the manipulations on it
> 6. save it and remove it from the environment
> 7. Redo 4-6 for every piece
> 8. Merge everything together at the end
> 
> It works if coded line by line but since I'll have
> to perform these tasks
> on other data sets, I am trying to automate this as
> much as I can. 
> 
> I am using a loop in which I used 'assign' and 'get'
> (pseudo code below).
> My problem is when I use 'get', it prints the whole
> object on the screen.
> I am wondering whether there is a more efficient way
> to do what I need to
> do. Any help would be appreciated. Please keep in
> mind that the whole
> process is quite computer-intensive, so I can't keep
> everything in the
> environment while R performs calculations.
> 
> Say I have 1 big dataframe called data. I use
> 'split' to divide it into a
> list of 12 dataframes (call this list my.list)
> 
> my.fun is a function that takes a dataframe,
> performs several
> manipulations on it and returns a dataframe.
> 
> 
> for (i in 1:12){
>   assign( paste( "data", i, sep=""), 
> my.fun(my.list[i]))   # this works
>   # now I need to save this new object as a RData. 
> 
>   # The following line does not work
>   save(paste("data", i, sep = ""),  file = paste( 
> paste("data", i, sep =
> ""), "RData", sep="."))
> }
> 
>   # This works but it is a bit convoluted!!!
>   temp <- get(paste("data", i, sep = ""))
>   save(temp,  file = "lala.RData")
> }
> 
> 
> I am *sure* there is something more clever to do but
> I can't find it. Any
> help would be appreciated.
> 
> best regards,
> 
> MP
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From jim at bitwrit.com.au  Thu Dec 20 07:58:56 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 20 Dec 2007 17:58:56 +1100
Subject: [R] Question about which kind of plot to use
In-Reply-To: <mn.9b737d7cee7588f6.83239@exitcheck.net>
References: <mn.9b737d7cee7588f6.83239@exitcheck.net>
Message-ID: <476A12B0.2080404@bitwrit.com.au>

Max wrote:
> Hi Everyone,
> 
> I've got a question about data representation. I have some psychometric 
> data with 5 scores for 15 different groups. I've been asked to show 
> some kind of mean plots. The data below is the mean and SD for a given 
> group, unfortunately my employer doesn't want me posting full datasets. 
> :(
> 
>  The groups V,W,X,Y,Z are divided into Bottom, (B), Middle (M) and Top 
> (T). An example of my data is shown below.
> 
> Score 1
> Mean				             SD
> 	B	    M	    T		    B	  M	    T
> V	86.9	13.0	88.8		16.9  2.0	10.5
> W	16.1	96.1	17.7		2.2	  4.6	1.7
> X	50.7	61.1	74.7		4.7	  3.7	7.6
> Y	68.5	99.7	37.6		6.0	  8.0	2.3
> Z	92.7	22.3	69.4		6.5	  1.2	2.2
> 
> What I did before was a standard mean plot:
> 
> plotMeans(w$score1, w$Factor, 
> error.bars="sd",xlab="Factor",ylab="Score",main="Group W Score 1 Plot")
> 
>  However, with 15 groups and 5 scores this turns into 75 individual 
> graphs. Is there a way to layer mean plots? Or show several mean plots 
> in the Same graph? Any ideas or suggestions would be great.
> 
Hi Max,
This may get a bit crowded, but brkdn.plot (plotrix) will do something 
like this. I think you would want your group variable as the "groups" 
argument (!) and the variable that specifies Bottom/Middle/Top as the 
"obs" argument. Pass "sd" as the "md" argument to get standard deviation 
bars.

Jim


From cmarcum at uci.edu  Thu Dec 20 08:33:15 2007
From: cmarcum at uci.edu (Christopher Marcum)
Date: Wed, 19 Dec 2007 23:33:15 -0800 (PST)
Subject: [R] factor manipulation: edgelist to a matrix?
Message-ID: <59796.75.128.40.165.1198135995.squirrel@webmail.uci.edu>

Hello All,

I have had considerable bad luck with attempting the following with for
loops. Here is the problem:


#	Suppose we have a data.frame with the following data, which can be
considered a type of edgelist (for those with networks backgrounds):
#
#       V1	V2
#	1	A
#	1	A
#	1	B
#	2	A
#	3	C
#	3	A
#	3	C
#	3	B
#
#	I want the output of the function to produce a matrix, such that #each
factor of V1 is a row, and each corresponding value at position k of V2 is
the i,j^th element of the new matrix, with missing values otherwise. The
desired output should be:
#	[,1]	[,2]	[,3]	[,4]
# [1,]	A	A	B	NA
# [2,]	A	NA	NA	NA
# [3,]	C	A	C	B

I have explored the reshape package as well as the network package in this
pursuit, with no luck.

Thanks,
Chris Marcum
UCI Sociology


From jacques.veslot at cemagref.fr  Thu Dec 20 08:53:41 2007
From: jacques.veslot at cemagref.fr (Veslot Jacques)
Date: Thu, 20 Dec 2007 08:53:41 +0100
Subject: [R] factor manipulation: edgelist to a matrix?
In-Reply-To: <59796.75.128.40.165.1198135995.squirrel@webmail.uci.edu>
References: <59796.75.128.40.165.1198135995.squirrel@webmail.uci.edu>
Message-ID: <13E5504DD4AE454AAD151A4D239FBE5E87D200@parpaioun.aix.cemagref.fr>

do.call(rbind,lapply(split(as.character(z[,2]),z[,1]), 
function(x) c(x, rep(NA, max(table(z[,1]))-length(x)))))

Jacques VESLOT

CEMAGREF - UR Hydrobiologie

Route de C?zanne - CS 40061      
13182 AIX-EN-PROVENCE Cedex 5, France

T?l      + 0033   04 42 66 99 76
email   jacques.veslot at cemagref.fr  

>-----Message d'origine-----
>De?: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] De
>la part de Christopher Marcum
>Envoy??: jeudi 20 d?cembre 2007 08:33
>??: r-help at stat.math.ethz.ch
>Objet?: [R] factor manipulation: edgelist to a matrix?
>
>Hello All,
>
>I have had considerable bad luck with attempting the following with for
>loops. Here is the problem:
>
>
>#	Suppose we have a data.frame with the following data, which can be
>considered a type of edgelist (for those with networks backgrounds):
>#
>#       V1	V2
>#	1	A
>#	1	A
>#	1	B
>#	2	A
>#	3	C
>#	3	A
>#	3	C
>#	3	B
>#
>#	I want the output of the function to produce a matrix, such that #each
>factor of V1 is a row, and each corresponding value at position k of V2 is
>the i,j^th element of the new matrix, with missing values otherwise. The
>desired output should be:
>#	[,1]	[,2]	[,3]	[,4]
># [1,]	A	A	B	NA
># [2,]	A	NA	NA	NA
># [3,]	C	A	C	B
>
>I have explored the reshape package as well as the network package in this
>pursuit, with no luck.
>
>Thanks,
>Chris Marcum
>UCI Sociology
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-
>guide.html
>and provide commented, minimal, self-contained, reproducible code.


From quesada at gmail.com  Wed Dec 19 17:26:40 2007
From: quesada at gmail.com (Jose)
Date: Wed, 19 Dec 2007 16:26:40 +0000 (UTC)
Subject: [R]
	=?utf-8?q?Correlation_when_one_variable_has_zero_variance=09?=
	=?utf-8?q?=28polychoric=3F=29?=
References: <op.t3k14iql4hcap5@dollar> <000901c8424b$96d87e10$c4897a30$@ca>
Message-ID: <loom.20071219T155924-73@post.gmane.org>

Dear John,

> I also ran the same analysis in 2005
> (what has changed in the package polycor since then, I don't know) and the
> results were different. I think back then I contrasted them with SAS
> and  they were the same.

John> I don't entirely follow this. Are you referring to the table above with 
one
John> row, more generally to table with zero marginals, or to tables in which
John> there are interior zeroes?> 

I have plenty of those tables, but I think quite a few of them have zero 
marginals (the case I posted might be a bit extreme). I have 400 observations, 
so no matter how centered the distributions are, some observations will be out 
of the center.

The results I got in 2005 cannot be reproduced now in 2007 with the same code; 
I guess this could be due to this bug you describe (maybe it was introduced 
later?). In 2007, I got many correlations has high as the one I described and I 
was wondering what the problem was. I don't have SAS available anymore so I 
cannot run the code I wrote in SAS to compare.

Where can I get the new code for polychor?

I'm in a predicament here; the data I'm analyzing are from a flight simulation 
and are extremely expensive to get, so running more experiments is out of 
question.

Any pointers as to how I could analyze this dataset? (i.e. one where there 
might be zero marginals?)

Thanks

-Jose


From Torsten.Hothorn at stat.uni-muenchen.de  Wed Dec 19 09:07:51 2007
From: Torsten.Hothorn at stat.uni-muenchen.de (Torsten Hothorn)
Date: Wed, 19 Dec 2007 09:07:51 +0100 (CET)
Subject: [R] R News, volume 7, issue 3 is now available
Message-ID: <Pine.LNX.4.64.0712190906510.5625@localhost.localdomain>


Dear useRs,

The December 2007 issue of `R News' is now available on CRAN under the
Documentation/Newsletter link.

Torsten
(on behalf of the R News Editorial Board)

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From milton_ruser at yahoo.com.br  Thu Dec 20 09:24:40 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Thu, 20 Dec 2007 00:24:40 -0800 (PST)
Subject: [R] continue a for() when occurs errors
Message-ID: <974143.33371.qm@web56005.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071220/1027fdd6/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Thu Dec 20 09:25:50 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 20 Dec 2007 09:25:50 +0100
Subject: [R] factor manipulation: edgelist to a matrix?
References: <59796.75.128.40.165.1198135995.squirrel@webmail.uci.edu>
Message-ID: <003a01c842e1$ee15a590$0540210a@www.domain>

one way is the following:

V1 <- c(1,1,1,2,3,3,3,3)
V2 <- LETTERS[c(1,1,2,1,3,1,3,2)]

tab <- table(V1, ave(V1, V1, FUN = seq_along))
vals <- as.vector(t(tab))
vals[vals != 0] <- unlist(split(V2, V1))
vals[vals == 0] <- NA
matrix(vals, nrow(tab), ncol(tab), TRUE)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christopher Marcum" <cmarcum at uci.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, December 20, 2007 8:33 AM
Subject: [R] factor manipulation: edgelist to a matrix?


> Hello All,
>
> I have had considerable bad luck with attempting the following with 
> for
> loops. Here is the problem:
>
>
> # Suppose we have a data.frame with the following data, which can be
> considered a type of edgelist (for those with networks backgrounds):
> #
> #       V1 V2
> # 1 A
> # 1 A
> # 1 B
> # 2 A
> # 3 C
> # 3 A
> # 3 C
> # 3 B
> #
> # I want the output of the function to produce a matrix, such that 
> #each
> factor of V1 is a row, and each corresponding value at position k of 
> V2 is
> the i,j^th element of the new matrix, with missing values otherwise. 
> The
> desired output should be:
> # [,1] [,2] [,3] [,4]
> # [1,] A A B NA
> # [2,] A NA NA NA
> # [3,] C A C B
>
> I have explored the reshape package as well as the network package 
> in this
> pursuit, with no luck.
>
> Thanks,
> Chris Marcum
> UCI Sociology
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From hb at stat.berkeley.edu  Thu Dec 20 10:26:57 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Thu, 20 Dec 2007 01:26:57 -0800
Subject: [R] assigning and saving datasets in a loop,
	with names changing with "i"
In-Reply-To: <WorldClient-F200712182124.AA24320054@epimgh.mcgill.ca>
References: <WorldClient-F200712182124.AA24320054@epimgh.mcgill.ca>
Message-ID: <59d7961d0712200126w3852b45evdab362b32fa3c71a@mail.gmail.com>

library(R.utils);

for (ii in 1:12) {
  value <- my.fun(my.list[ii]);
  saveObject(value, file=sprintf("data%02d.RData", ii));
  rm(value); gc();
}

for (ii in 1:12) {
  value <- loadObject(sprintf("data%02d.RData", ii));
}


On 18/12/2007, Marie Pierre Sylvestre <MP.Sylvestre at epimgh.mcgill.ca> wrote:
> Dear R users,
>
> I am analysing a very large data set and I need to perform several data
> manipulations. The dataset is so big that the only way I can play with it
> without having memory problems (E.g. "cannot allocate vectors of size...")
> is to write a batch script to:
>
> 1. cut the data into pieces
> 2. save the pieces in seperate .RData files
> 3. Remove everything from the environment
> 4. load one of the piece
> 5. perform the manipulations on it
> 6. save it and remove it from the environment
> 7. Redo 4-6 for every piece
> 8. Merge everything together at the end
>
> It works if coded line by line but since I'll have to perform these tasks
> on other data sets, I am trying to automate this as much as I can.
>
> I am using a loop in which I used 'assign' and 'get' (pseudo code below).
> My problem is when I use 'get', it prints the whole object on the screen.
> I am wondering whether there is a more efficient way to do what I need to
> do. Any help would be appreciated. Please keep in mind that the whole
> process is quite computer-intensive, so I can't keep everything in the
> environment while R performs calculations.
>
> Say I have 1 big dataframe called data. I use 'split' to divide it into a
> list of 12 dataframes (call this list my.list)
>
> my.fun is a function that takes a dataframe, performs several
> manipulations on it and returns a dataframe.
>
>
> for (i in 1:12){
>   assign( paste( "data", i, sep=""),  my.fun(my.list[i]))   # this works
>   # now I need to save this new object as a RData.
>
>   # The following line does not work
>   save(paste("data", i, sep = ""),  file = paste(  paste("data", i, sep =
> ""), "RData", sep="."))
> }
>
>   # This works but it is a bit convoluted!!!
>   temp <- get(paste("data", i, sep = ""))
>   save(temp,  file = "lala.RData")
> }
>
>
> I am *sure* there is something more clever to do but I can't find it. Any
> help would be appreciated.
>
> best regards,
>
> MP
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Ted.Harding at manchester.ac.uk  Thu Dec 20 11:00:34 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Thu, 20 Dec 2007 10:00:34 -0000 (GMT)
Subject: [R] continue a for() when occurs errors
In-Reply-To: <974143.33371.qm@web56005.mail.re3.yahoo.com>
Message-ID: <XFMail.071220100034.Ted.Harding@manchester.ac.uk>

On 20-Dec-07 08:24:40, Milton Cezar Ribeiro wrote:
> Dear all,
> 
> I am simulating some regressions in a for() looping and sometimes
> occours some error and the R stop my batch processing. I would like do
> save the step where the error happned and continue my for() looping.
> Is there a way to do that?
> 
> Thanks In Advance.
> Miltinho

try() is the simple way: see ?try for some basic details.
See also ?tryCatch

Example:

  if(class(temp<-try(log("a")))=="try-error"){
    print("Goodbye")
  } else {print("Hello:");print(temp)}
##Error in log(x) : Non-numeric argument to mathematical function
##[1] "Goodbye"

  if(class(temp<-try(log(10)))=="try-error"){
    print("Goodbye")
  } else {print("Hello:");print(temp)}
##[1] "Hello:"
##[1] 2.302585

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Dec-07                                       Time: 10:00:30
------------------------------ XFMail ------------------------------


From jegelka at cbs.mpg.de  Thu Dec 20 11:29:17 2007
From: jegelka at cbs.mpg.de (Daniel Jegelka)
Date: Thu, 20 Dec 2007 11:29:17 +0100
Subject: [R] auto named savings (pngs & data-frames)
Message-ID: <476A43FD.8090801@cbs.mpg.de>

Hello, i only got a small problem.

i try to create automatic new dataframes, or png?s. the main problem i
got is:

how can i create automatic a new name for a file (read out by simply
"for") -
i tried to use "(paste...) but theres an errormessage, about a wrong
declination. R told it is as.character, but need as.Real.

Should i use another method than "paste"?

i tried as this:
png("User-", paste(subject, ".png", sep = " "))

^^ as png file, example: User-DA5T.png  <- but if i create one, they are
empty.

Would anybody help me please?

thx Daniel




Prog:
files <- list.files(
		path = "/SCR/STATISTIK",
		pattern = ".*t_simple\.txt",
		all.files = TRUE,
        	full.names = TRUE,
		recursive = TRUE		)
list(files)
anz <- nrow(files)
print(anz)

for (file in files) {

    	lines <- readLines(file)
	name <- lines[1]
	print(name)
	#png("/SCR/STATISTIK/all-user-names.png")
	#name <- as.character(name)
	#barplot(name)
	#dev.off()
	for (line in lines[-1]) {
        	liste <- unlist(strsplit((line), " "))
		subject <- as.character(liste[1])
	        if (length(liste) < 2 ) {
        	    	data <- 0
        	}
		else {
			data  <- as.numeric(liste[-1])
		}
		#png(paste[subject],".png")
		dev.set()
		boxplot(data[subject], ylim=c(400,1500))
		dev.off()
		datalist <- data.frame(data)
		names(datalist) <- subject
		print(datalist)
		png("/SCR/STATISTIK/datalist.png", width=1024, height=768)
		
		plot(datalist)
		dev.off()}}
			#png("/SCR/all-show.png")
			#boxplot(datalist, main="Proband - ", col= "blue", ylab=
"reactiontime in milliseconds", xlab= "name")
			#dev.off()
			#dev.set()
			#Datenmatrix <- cbind(datalist)
			#print(Datenmatrix)
			#dev.off()
			#plot(datalist)
			#dev.off()

#einlesen <- datalist
#print(einlesen)
#Monate,storage, type="l", ylab="Datenvolumen [B]", xlab="Alter [M]",
main="Altersverteilung", col="blue", ylog=TRUE, xpd=TRUE
#print(x)

#
# alle kritischen Befehle wurden wohl von Felix "behoben"  :(  - aber
danke  :)
#
	(rbind(User)) -> ueber
	print(ueber)
	list(ueber)
		x <- ncol(ueber)
		print(x)
		y <- nrow(ueber)
		print(y)
#{#			for (i in 1:x)
#				{fileseq <- (unlist(strsplit((a[i]), " "))[-1])
#				print(zeilen <- list(fileseq))
#				zeilen[i]<- zeilen
#				print(fileseq)}	
#					for (j in 1:17)	
#					{fileline <- as.logical(unlist((zeilen), " "))
#					fileline[j] <- list(fileline)
#					print(fileline[j])}
#}

#
#

#

#list(ueber)
#fix(ueber)
#dim(ueber)
#names(ueber)
#ausgabe der userdaten in R
#ueber$Sephal.Length
#ueber$Sepal.Width
#ausgabe der userdaten in einem ?bersichtpopup

#anzahl der Zeilen, die "files" besitzt

#
#daten der einzelnen datein in den Speicher schreiben
#file <- unlist(files), NA = TRUE
#print(file)
#meinetabelle <- read.table(file, header=FALS)
#a <- read.table(file=stdin(User),header=FALSE)
#meinetabelle <- read.table(, sep=?\t?, header=FALSE)
#simul<-data.frame(times=c(0, anz),simul=c(3,4,5,6))
#mget(files, envir =  , mode = "any",
  #  inherits = TRUE)


#
#read.table(files)
#
#


#date(inbox)
#attach(files, pos == anz, name = deparse(substitute(files)),
#warn.conflicts = TRUE)

#step(anz, scope, scale = 1,
#     direction = c("forward"),
 #    trace = 1, keep = NULL, steps = anz, k = 1)
        #test <- as.numeric(unlist(strsplit((a[i]), " "))[-1])
	#print(nrow(test))

From wwwhsd at gmail.com  Thu Dec 20 11:37:14 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 20 Dec 2007 08:37:14 -0200
Subject: [R] auto named savings (pngs & data-frames)
In-Reply-To: <476A43FD.8090801@cbs.mpg.de>
References: <476A43FD.8090801@cbs.mpg.de>
Message-ID: <da79af330712200237n1b690424g889c33176e251cb7@mail.gmail.com>

Try this:

jpeg("UserDA%02dT.jpg")
sapply(1:10, function(x)plot(rnorm(100)))
dev.off()

On 20/12/2007, Daniel Jegelka <jegelka at cbs.mpg.de> wrote:
> Hello, i only got a small problem.
>
> i try to create automatic new dataframes, or png?s. the main problem i
> got is:
>
> how can i create automatic a new name for a file (read out by simply
> "for") -
> i tried to use "(paste...) but theres an errormessage, about a wrong
> declination. R told it is as.character, but need as.Real.
>
> Should i use another method than "paste"?
>
> i tried as this:
> png("User-", paste(subject, ".png", sep = " "))
>
> ^^ as png file, example: User-DA5T.png  <- but if i create one, they are
> empty.
>
> Would anybody help me please?
>
> thx Daniel
>
>
>
>
> Prog:
> files <- list.files(
>                path = "/SCR/STATISTIK",
>                pattern = ".*t_simple\.txt",
>                all.files = TRUE,
>                full.names = TRUE,
>                recursive = TRUE                )
> list(files)
> anz <- nrow(files)
> print(anz)
>
> for (file in files) {
>
>        lines <- readLines(file)
>        name <- lines[1]
>        print(name)
>        #png("/SCR/STATISTIK/all-user-names.png")
>        #name <- as.character(name)
>        #barplot(name)
>        #dev.off()
>        for (line in lines[-1]) {
>                liste <- unlist(strsplit((line), " "))
>                subject <- as.character(liste[1])
>                if (length(liste) < 2 ) {
>                        data <- 0
>                }
>                else {
>                        data  <- as.numeric(liste[-1])
>                }
>                #png(paste[subject],".png")
>                dev.set()
>                boxplot(data[subject], ylim=c(400,1500))
>                dev.off()
>                datalist <- data.frame(data)
>                names(datalist) <- subject
>                print(datalist)
>                png("/SCR/STATISTIK/datalist.png", width=1024, height=768)
>
>                plot(datalist)
>                dev.off()}}
>                        #png("/SCR/all-show.png")
>                        #boxplot(datalist, main="Proband - ", col= "blue", ylab=
> "reactiontime in milliseconds", xlab= "name")
>                        #dev.off()
>                        #dev.set()
>                        #Datenmatrix <- cbind(datalist)
>                        #print(Datenmatrix)
>                        #dev.off()
>                        #plot(datalist)
>                        #dev.off()
>
> #einlesen <- datalist
> #print(einlesen)
> #Monate,storage, type="l", ylab="Datenvolumen [B]", xlab="Alter [M]",
> main="Altersverteilung", col="blue", ylog=TRUE, xpd=TRUE
> #print(x)
>
> #
> # alle kritischen Befehle wurden wohl von Felix "behoben"  :(  - aber
> danke  :)
> #
>        (rbind(User)) -> ueber
>        print(ueber)
>        list(ueber)
>                x <- ncol(ueber)
>                print(x)
>                y <- nrow(ueber)
>                print(y)
> #{#                     for (i in 1:x)
> #                               {fileseq <- (unlist(strsplit((a[i]), " "))[-1])
> #                               print(zeilen <- list(fileseq))
> #                               zeilen[i]<- zeilen
> #                               print(fileseq)}
> #                                       for (j in 1:17)
> #                                       {fileline <- as.logical(unlist((zeilen), " "))
> #                                       fileline[j] <- list(fileline)
> #                                       print(fileline[j])}
> #}
>
> #
> #
>
> #
>
> #list(ueber)
> #fix(ueber)
> #dim(ueber)
> #names(ueber)
> #ausgabe der userdaten in R
> #ueber$Sephal.Length
> #ueber$Sepal.Width
> #ausgabe der userdaten in einem ?bersichtpopup
>
> #anzahl der Zeilen, die "files" besitzt
>
> #
> #daten der einzelnen datein in den Speicher schreiben
> #file <- unlist(files), NA = TRUE
> #print(file)
> #meinetabelle <- read.table(file, header=FALS)
> #a <- read.table(file=stdin(User),header=FALSE)
> #meinetabelle <- read.table(, sep=?\t?, header=FALSE)
> #simul<-data.frame(times=c(0, anz),simul=c(3,4,5,6))
> #mget(files, envir =  , mode = "any",
>  #  inherits = TRUE)
>
>
> #
> #read.table(files)
> #
> #
>
>
> #date(inbox)
> #attach(files, pos == anz, name = deparse(substitute(files)),
> #warn.conflicts = TRUE)
>
> #step(anz, scope, scale = 1,
> #     direction = c("forward"),
>  #    trace = 1, keep = NULL, steps = anz, k = 1)
>        #test <- as.numeric(unlist(strsplit((a[i]), " "))[-1])
>        #print(nrow(test))
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From jonas.malmros at gmail.com  Thu Dec 20 12:09:25 2007
From: jonas.malmros at gmail.com (Jonas Malmros)
Date: Thu, 20 Dec 2007 12:09:25 +0100
Subject: [R] Computing normal conf.intervals
Message-ID: <fd3c7adf0712200309w69c1ed29p26e695ca8f81e1a6@mail.gmail.com>

Hi everybody,

I wonder if there is a built-in function similar to Matlab's "normfit"
which computes 95% CI based on the normality assumption.
So, I have a vector of values and I want to calculate 95% normal CI.
Of course, I could write my own function, no problem, but I still
wonder if built-in functionality exists. (I wish quantile() had this
functionality included).
Anyone knows?

Also, I wonder if there is a function similar to Matlab's "flipud".
Obviously there is package "matlab" which has this function, but I
wonder if I can turn a matrix upside-down without loading matlab
package.

Thanks for your help in advance!

Best,
JM


-- 
Jonas Malmros
Stockholm University
Stockholm, Sweden


From rolf.wester at ilt.fraunhofer.de  Thu Dec 20 12:26:24 2007
From: rolf.wester at ilt.fraunhofer.de (Rolf Wester)
Date: Thu, 20 Dec 2007 12:26:24 +0100
Subject: [R] Genetic algorithm for feature selection
In-Reply-To: <476A3999.3060501@pburns.seanet.com>
References: <4769515E.5040106@ilt.fraunhofer.de>
	<476A3999.3060501@pburns.seanet.com>
Message-ID: <476A5160.6030709@ilt.fraunhofer.de>

Thank you very much for your reply. I found the information on your home 
page very useful.

What I want to do is a PLS regression of a data set with 60 features for 
calibration purposes. 
In order to optimize the performance of the calibration I have to find 
out what features to use
in the PLS regression. Although this is what PLS-regression does but I 
found some articles
(R. Leardi, "Genetic algorithms applied to feature selection in PLS 
regression: how and when
to use them", Chemometrics and Intelligent Laboratory Systems 41 (1998) 
195-207) that claim
that a genetic optimization algorithm applied to feature selection 
could  further improve the calibration.
I found Python code that does exactly this but although I'm much more 
experienced in programming
in Python I think it's probably better to use R for this task. So I'm 
going to implement the genetic
algorithm described in the papers by Leardi in R.

Regards

Rolf

Patrick Burns wrote:
> I don't know what you are trying to do exactly, but
> in case you don't find anything pre-built there is the
> 'Rgenoud' package for genetic optimization and also
> the 'genopt' function that you can get from S Poetry.
>
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Rolf Wester wrote:
>
>> Hi,
>>
>> I'm looking for a R-package that does feature selection for PLS using 
>> a genetic optimization algorithm.
>> I couldn't find one on CRAN and I wonder whether there is a free one. 
>> I would be very appreciative for any help.
>>
>> Regards
>>
>> Rolf
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>  
>>
>
>


-- 
------------------------------------
# Dr. Rolf Wester
# Fraunhofer Institut f. Lasertechnik
# Steinbachstrasse 15, D-52074 Aachen, Germany.
# Tel: + 49 (0) 241 8906 401, Fax: +49 (0) 241 8906 121
# EMail: rolf.wester at ilt.fraunhofer.de
# WWW:   http://www.ilt.fraunhofer.de


From P.Dalgaard at biostat.ku.dk  Thu Dec 20 12:34:44 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 20 Dec 2007 12:34:44 +0100
Subject: [R] Computing normal conf.intervals
In-Reply-To: <fd3c7adf0712200309w69c1ed29p26e695ca8f81e1a6@mail.gmail.com>
References: <fd3c7adf0712200309w69c1ed29p26e695ca8f81e1a6@mail.gmail.com>
Message-ID: <476A5354.9040903@biostat.ku.dk>

Jonas Malmros wrote:
> Hi everybody,
>
> I wonder if there is a built-in function similar to Matlab's "normfit"
> which computes 95% CI based on the normality assumption.
> So, I have a vector of values and I want to calculate 95% normal CI.
> Of course, I could write my own function, no problem, but I still
> wonder if built-in functionality exists. (I wish quantile() had this
> functionality included).
> Anyone knows?
>
>   
First, be more clear about what the intention is. Prediction intervals,
or confidence intervals for the mean? If the former, do you want the
crude version (plus/minus 1.96s) or the version that takes the
estimation variance into account

> x <- rnorm(10)
> qnorm(c(.025,.975), mean=mean(x), sd=sd(x))
[1] -1.763791  1.465144
> predict(lm(x~1), newdata=data.frame(1), interval="p")
            fit       lwr      upr
[1,] -0.1493235 -2.103664 1.805017
> confint(lm(x~1))
                 2.5 %    97.5 %
(Intercept) -0.7385793 0.4399324

> Also, I wonder if there is a function similar to Matlab's "flipud".
> Obviously there is package "matlab" which has this function, but I
> wonder if I can turn a matrix upside-down without loading matlab
> package.
>
>   
M[nrow(M):1,]

or (safer if nrow==0)

M[rev(seq_len(nrow(M))),]

> Thanks for your help in advance!
>
> Best,
> JM
>
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jfox at mcmaster.ca  Thu Dec 20 12:56:32 2007
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 20 Dec 2007 06:56:32 -0500
Subject: [R] Correlation when one variable has zero
	variance	(polychoric?)
In-Reply-To: <loom.20071219T155924-73@post.gmane.org>
References: <op.t3k14iql4hcap5@dollar> <000901c8424b$96d87e10$c4897a30$@ca>
	<loom.20071219T155924-73@post.gmane.org>
Message-ID: <000601c842ff$5cda6750$168f35f0$@ca>

Dear Jose,

> -----Original Message-----
> From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-
> project.org] On Behalf Of Jose
> Sent: December-19-07 11:27 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Correlation when one variable has zero variance
> (polychoric?)
> 
> Dear John,
> 
> > I also ran the same analysis in 2005
> > (what has changed in the package polycor since then, I don't know)
> and the
> > results were different. I think back then I contrasted them with SAS
> > and  they were the same.
> 
> John> I don't entirely follow this. Are you referring to the table
> above with
> one
> John> row, more generally to table with zero marginals, or to tables in
> which
> John> there are interior zeroes?>
> 
> I have plenty of those tables, but I think quite a few of them have
> zero
> marginals (the case I posted might be a bit extreme). I have 400
> observations,
> so no matter how centered the distributions are, some observations will
> be out
> of the center.

As I said, there's no basis for estimating polychoric correlations and all
thresholds when there are zero marginals. If there is more than one row and
column remaining with nonzero marginals, then you could simply eliminate the
rows/columns with zero marginals, but tables with only one nonzero row or
column have no information about the correlation. I'll think about doing
this -- i.e., removing zero rows and columns -- automatically and issuing a
warning.

> 
> The results I got in 2005 cannot be reproduced now in 2007 with the
> same code;
> I guess this could be due to this bug you describe (maybe it was
> introduced
> later?). In 2007, I got many correlations has high as the one I
> described and I
> was wondering what the problem was. I don't have SAS available anymore
> so I
> cannot run the code I wrote in SAS to compare.

No program, not even SAS, can magically estimate a correlation from a table
with one row or column. If polychor() did that in 2005, the answer it
provided was erroneous.

> 
> Where can I get the new code for polychor?

I plan to upload a new version of the polycor package to CRAN as soon as I
have a chance -- probably sometime this week. But you already have the code
for polychor() and can modify it yourself: Just fix the test so that it
checks for < 2 rather than < 1 row, and return NA (and issue a warning) in
this case.

> 
> I'm in a predicament here; the data I'm analyzing are from a flight
> simulation
> and are extremely expensive to get, so running more experiments is out
> of
> question.
> 
> Any pointers as to how I could analyze this dataset? (i.e. one where
> there
> might be zero marginals?)

I'm sorry, but as I said there's no magic solution here. The data, however
expensive, don't have information relevant to estimating the correlation.

Regards,
 John

> 
> Thanks
> 
> -Jose
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jfox at mcmaster.ca  Thu Dec 20 13:05:14 2007
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 20 Dec 2007 07:05:14 -0500
Subject: [R] Code for articles in R news?
In-Reply-To: <20071219155834.GF302@lubyanka.local>
References: <20071219155834.GF302@lubyanka.local>
Message-ID: <000701c84300$94616150$bd2423f0$@ca>

Dear Ajay,

At present, unfortunately, we have so mechanism for distributing code
associated with R News articles. We're considering changes to the
infrastructure that supports R News, including providing the kind of
facility that you suggest. I'm afraid that the best that I can suggest for
now is copying, pasting, and editing the code directly from the PDF file.

Sorry,
 John

--------------------------------
John Fox, Professor
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox


> -----Original Message-----
> From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-
> project.org] On Behalf Of Ajay Shah
> Sent: December-19-07 10:59 AM
> To: 'R-help'
> Subject: [R] Code for articles in R news?
> 
> I went to the article on np in R news 7/2 (October 2007). What's the
> general technique to get the source code associated with the article
> as a .R file that I can play with?
> 
> --
> Ajay Shah
> http://www.mayin.org/ajayshah
> ajayshah at mayin.org
> http://ajayshahblog.blogspot.com
> <*(:-? - wizard who doesn't know the answer.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m_olshansky at yahoo.com  Thu Dec 20 13:29:42 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Thu, 20 Dec 2007 04:29:42 -0800 (PST)
Subject: [R] Aggregating by a grouping
In-Reply-To: <2893E11BC50DB445BC9E97835D9C219D0279F269@MLNYC727MB.amrs.win.ml.com>
Message-ID: <702286.84479.qm@web32203.mail.mud.yahoo.com>

One possibility is:

> x
[1] "A" "B" "C" "D" "A" "C" "D" "B"
> y
[1] 10 11  9  8 12  4  5  7
> basic_map
[[1]]
[1] "A" "B"

[[2]]
[1] "C" "D"

> a <- which(sapply(basic_map,function(u) x %in%
u),arr.ind=TRUE)
> aggregate(y,list(a[order(a[,1]),2]),sum)
  Group.1  x
1       1 40
2       2 26
> 

--- "Kondamani, Arjun (GMI - NY Corporate Bonds)"
<arjun_kondamani at ml.com> wrote:

> Suppose I have:
> 
> Book	Value	
> A	10	
> B	11	
> C	9	
> D	8	
> A	12	
> C	4	
> D	5	
> B	7	
> 
> I want to summarize above not by Book but by
> groupings of Books as in
> (below)
> 
> I have a list ... basic_map <-
> list(c("A",B"),c("C,D"))
> Big_names <- c("A1", "A2")
> Names(basic_map) <- big_names
> 
> So I want to get :
> 
> A1 40
> A2 26
> 
> How do I use tapply AND the list to get my custom
> groupings?
> 
> thx
>
--------------------------------------------------------
> 
> This message w/attachments (message) may be
> privileged, confidential or proprietary, and if you
> are not an intended recipient, please notify the
> sender, do not use or share it and delete it. Unless
> specifically indicated, this message is not an offer
> to sell or a solicitation of any investment products
> or other financial product or service, an official
> confirmation of any transaction, or an official
> statement of Merrill Lynch. Subject to applicable
> law, Merrill Lynch may monitor, review and retain
> e-communications (EC) traveling through its
> networks/systems. The laws of the country of each
> sender/recipient may impact the handling of EC, and
> EC may be archived, supervised and produced in
> countries other than the country in which you are
> located. This message cannot be guaranteed to be
> secure or error-free. This message is subject to
> terms available at the following link:
> http://www.ml.com/e-communications_terms/. By
> messaging with Merrill Lynch you consent to the
> foregoing.
>
--------------------------------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From m_olshansky at yahoo.com  Thu Dec 20 13:58:52 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Thu, 20 Dec 2007 04:58:52 -0800 (PST)
Subject: [R] Obtaining replicates numbers of a vector
In-Reply-To: <5d897a2f0712190236m7a592ad4jf7dce064fbf0dd1a@mail.gmail.com>
Message-ID: <397209.41020.qm@web32208.mail.mud.yahoo.com>

You could do:

> x <- c('A','B','A','C','C','B')
> x
[1] "A" "B" "A" "C" "C" "B"
> mapply(function(i) sum(x[1:i] == x[i]),1:length(x))
[1] 1 1 2 1 2 2
> 

--- Eric Lecoutre <ericlecoutre at gmail.com> wrote:

> Dear R-help,
> 
> I am trying to have a generic way to assess the
> replicates in a character
> vector.
> Say that I have the following vector:
> 
> x <- c('A','B','A','C','C','B')
> 
> I would like to obtain:
> 
> replicates <- c(1,1,2,1,2,2)
> 
> each number beeing the time we see the corresponding
> value in x.
> 
> Any clever and generic way to obtain that?
> 
> Eric
> 
> 
> 
> 
> -- 
> Eric Lecoutre
> Consultant - Business & Decision
> Business Intelligence & Customer Intelligence
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From the13thday at yahoo.com  Thu Dec 20 14:12:10 2007
From: the13thday at yahoo.com (Brad B)
Date: Thu, 20 Dec 2007 05:12:10 -0800 (PST)
Subject: [R] plot3d, wireframe, persp help
Message-ID: <247660.34502.qm@web32907.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071220/279fb503/attachment.pl 

From rayorfan01101 at yahoo.co.uk  Thu Dec 20 14:13:40 2007
From: rayorfan01101 at yahoo.co.uk (Mr. Ray Orfan)
Date: Thu, 20 Dec 2007 20:13:40 +0700
Subject: [R] Greetings to you.
Message-ID: <E1J5LDY-0006Py-6D@ns1.naxzasoft.com>

Greetings to you. 

My name is Mr. Ray Orfan the Personal Assistant (PA) to the Head of an important International Organization. I have been directed to seek foriegn partnership with an interested foreigner on an issue relating to money. 

The issue invloved will be beneficial to all concerned and all legal issues related to it will be addressed legally. Expecting your respons through my this private e_mail. 

Best regards, 
Mr. Ray Orfan


From wwwhsd at gmail.com  Thu Dec 20 14:35:35 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 20 Dec 2007 11:35:35 -0200
Subject: [R] plot3d, wireframe, persp help
In-Reply-To: <247660.34502.qm@web32907.mail.mud.yahoo.com>
References: <247660.34502.qm@web32907.mail.mud.yahoo.com>
Message-ID: <da79af330712200535r10b482deqb6db8a73731a37ed@mail.gmail.com>

See 'akima' package.

On 20/12/2007, Brad B <the13thday at yahoo.com> wrote:
> Hello,
>  I am trying to get a surface plot of a data set that looks like the following,
>  1  2  5.6
>  5  9  2.4
>  9  8  9.8
>  ... to (60,000 rows down)
>
>  From my homework, the persp function only works with evenly spaced data points with the z data beeing in a matrix.  my data is not in that format.
>  the wireframe fxn gives me an error,
>  no applicable method for "wireframe"
> the plot3d fxn I got working thanks to Scionforbai's help.  However it does not represent the data like wireframe or persp would.
>  Any help would be appreciated.
>  thanks
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From samu.mantyniemi at helsinki.fi  Thu Dec 20 14:54:47 2007
From: samu.mantyniemi at helsinki.fi (=?ISO-8859-1?Q?Samu_M=E4ntyniemi?=)
Date: Thu, 20 Dec 2007 15:54:47 +0200
Subject: [R] Multicore computation in Windows network: How to set up Rmpi
Message-ID: <476A7427.8060702@helsinki.fi>

R-users,

My question is related to earlier posts about benefits of quadcore over
dualcore computers; I am trying to setup a cluster of windows xp
computers so that eventually I could make use of 10-20 cpu:s, but for
learning how to do this, I am playing around with two laptops.

I thought that the package snow would come handy in this situation, but
to use snow, I would probably need to install package Rmpi first. I
might also like to use Rmpi directly.

In order to use Rmpi in windows, I need to install MPI middle-ware, for
which I have found two options:

MPICH2: http://www.mcs.anl.gov/research/projects/mpich2/

DeinoMPI: http://mpi.deino.net/

First I tried MPICH2 1.06 + R-2.6.0 + Rmpi 0.5-5. (I downloaded windows
binaries of Rmpi from Rmpi website
:http://www.stats.uwo.ca/faculty/yu/Rmpi/)

With MPICH2 I managed to connect my computers so that I was able to
remotely launch Rgui on both machines but R hanged when calling
"library(Rmpi)". If only one Rgui was launched on the localhost,
"library(Rmpi)" worked without errors, but trying to use
"mpi.spawn.Rslaves()" resulted in an error message, and so did
"mpi.universe.size()". (In my current setup I can not reproduce this 
error message, but I can go back to this setup if this seems to be an 
important piece of information)

After that I removed MPICH2 from the system and installed DeinoMPI
instead, thus my setup was DeinoMPI 1.1.0 + R-2.6.0 + Rmpi 0.5-6.
Using this setup on a single machine seems to work,
"mpi.universe.size()" returns the correct number of cpu cores and
"mpi.spawn.Rslaves()" creates one master and one slave on the local 
unicore computer. However, trying to use two computers results in 
similar behavior as with MPICH2: Rgui gets started on both, but R hangs 
when trying "library(Rmpi)".

Following the advice given at the Rmpi website, this behavior could be
due to firewall settings. However, the result is the same if I take down
all my firewalls. Trying to debug this, I tried to run the example MPI
programs provided with the DeinoMPI installation. The example programs
work as expected: both machines participate to parallel computation and
the result is shown on the master node. This makes me believe that the
problem is likely related to R and Rmpi configuration, or the settings
I used when launching Rgui using mpiexec:

Settings on Mpiexec- tab:
Application: "C:\Program Files\R\R-2.6.0\bin\Rgui.exe"
Number of processes: 2
hosts: "akva26 samu" These are the computers with DeinoMPI installed.
DeinoMPI cluster-tab shows that they are ready to accept MPI jobs.)
localroot: checked

Other options were just empty.

I am sure that someone has tried these before, and I was hoping to find 
such users from this mailing list. Could you kindly share your 
experiences about this issue?

For example, does anyone have a working setup with DeinoMPI? According 
to Rmpi website DeinoMPI is the easiest way to set up MPI for a single 
windows machine, but I am not sure whether this is also intended to mean 
that one can not expect it to work with multiple computers.

Regards,

Samu M?ntyniemi


------------------------------------------
Samu M?ntyniemi
Researcher
Fisheries and Environmental Management Group (FEM)
Department of Biological and Environmental Sciences
Biocenter 3, room 4414
Viikinkaari 1
P.O. Box 65
FIN-00014 University of Helsinki

Phone: +358 9 191 58710
Fax: +358 9 191 58257

email: samu.mantyniemi  helsinki.fi
personal webpage: http://www.helsinki.fi/people/samu.mantyniemi/
FEM webpage: http://www.helsinki.fi/science/fem/


From ggrothendieck at gmail.com  Thu Dec 20 15:12:52 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 20 Dec 2007 09:12:52 -0500
Subject: [R] continue a for() when occurs errors
In-Reply-To: <974143.33371.qm@web56005.mail.re3.yahoo.com>
References: <974143.33371.qm@web56005.mail.re3.yahoo.com>
Message-ID: <971536df0712200612h7867ea3ey2b0062b27b8bc79@mail.gmail.com>

Its a FAQ:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-can-I-capture-or-ignore-errors-in-a-long-simulation_003f

On Dec 20, 2007 3:24 AM, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
> Dear all,
>
> I am simulating some regressions in a for() looping and sometimes occours some error and the R stop my batch processing. I would like do save the step where the error happned and continue my for() looping.
> Is there a way to do that?
>
> Thanks In Advance.
> Miltinho
>
>
>
>  para armazenamento!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ericlecoutre at gmail.com  Thu Dec 20 15:15:39 2007
From: ericlecoutre at gmail.com (Eric Lecoutre)
Date: Thu, 20 Dec 2007 15:15:39 +0100
Subject: [R] custom subset method / handling columns selection as logic in
	'...' parameter
Message-ID: <5d897a2f0712200615o6be7a190kc9946b93f306a8ed@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071220/22dfae07/attachment.pl 

From dxc13 at health.state.ny.us  Thu Dec 20 15:17:19 2007
From: dxc13 at health.state.ny.us (dxc13)
Date: Thu, 20 Dec 2007 06:17:19 -0800 (PST)
Subject: [R] plot3d, wireframe, persp help
In-Reply-To: <247660.34502.qm@web32907.mail.mud.yahoo.com>
References: <247660.34502.qm@web32907.mail.mud.yahoo.com>
Message-ID: <14437124.post@talk.nabble.com>


When using the wireframe function you need to create a 2D grid for your two X
variables.  Try the "expand.grid" function on your data, then run the
wireframe on that result




Brad B-2 wrote:
> 
> Hello,
>   I am trying to get a surface plot of a data set that looks like the
> following,
>   1  2  5.6
>   5  9  2.4
>   9  8  9.8
>   ... to (60,000 rows down)
>    
>   From my homework, the persp function only works with evenly spaced data
> points with the z data beeing in a matrix.  my data is not in that format.
>   the wireframe fxn gives me an error,
>   no applicable method for "wireframe"
> the plot3d fxn I got working thanks to Scionforbai's help.  However it
> does not represent the data like wireframe or persp would.
>   Any help would be appreciated.
>   thanks
>    
> 
>        
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/plot3d%2C-wireframe%2C-persp-help-tp14436239p14437124.html
Sent from the R help mailing list archive at Nabble.com.


From tomfool at as220.org  Thu Dec 20 15:06:34 2007
From: tomfool at as220.org (Tom Sgouros)
Date: Thu, 20 Dec 2007 09:06:34 -0500
Subject: [R] data shape
Message-ID: <20071220140634.F1689FACA21@as220.org>


Hello:

I have been give a spreadsheet to work with formed as one big table.
What it consists of is a 10-row-by-40-column table for each of about 70
different locations.  In other words, the table row names are repeated
70 times, once for each of the locations (whose names also appear in the
same column, where it's talking about the totals for that location), e.g.:
            A    B    C
 Location1 15   73  123   <-  this row is the sum of the following 3
 Under 10   6   42   23
 10 - 25    4   15   23
 Over 25    5   16   77
 Location2 18   75  113   <- same here
 Under 10   7   45   13
 10 - 25    5   18   44
 Over 25    6   12   56

I want to get this into R as a collection of data frames, one for each
of my locations.  My questions:

  1. There is a way to handle a collection of data frames, isn't there?
     No doubt there are plenty, but what's the easiest way, so that I
     can address them collectively, allowing me to ask such questions as
     what's the max of the over 25's in column C?

  2. What's the easiest way to read such a data array from a text file?
     I can do some editing of a csv file produced from the spreadsheet,
     but don't really know what to aim for.

  3. Is there some shortcut that would allow me to read this directly
     from a spreadsheet?

Many thanks,

 -tom


-- 
 ------------------------
 tomfool at as220 dot org
 http://sgouros.com  
 http://whatcheer.net


From dwinsemius at comcast.net  Thu Dec 20 15:17:05 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 20 Dec 2007 14:17:05 +0000 (UTC)
Subject: [R] 4 questions regarding hypothesis testing, survey package,
	ts on samples, plotting
References: <187205.27181.qm@web38605.mail.mud.yahoo.com>
Message-ID: <Xns9A0C5E74B531CdNOTwinscomcast@80.91.229.13>

eugen pircalabelu <eugen_pircalabelu at yahoo.com> wrote in
news:187205.27181.qm at web38605.mail.mud.yahoo.com: > 

> I have 4 questions which trouble me:
> 
snip 1-3
> 
> 4. I want to modify the scale of my axes within a plot but i really
> could not find  this option. I think  there is such an option, but i
> can not find it.  
>  x<-rnorm(100, 100, 3)
>  x<-ts(x, frequency=12)
>  acf(x)
>  plot(x)
> On the above example, i want to a scale like this
> 95,96,97,98,....104,105  (on y) and 1,2,3,4,...7,8 (on x). 

Instead of looking at ?plot, try:
?plot.default # see the xlim and ylim arguments.

-- 
David Winsemius


From b.jacobs at pandora.be  Thu Dec 20 15:27:21 2007
From: b.jacobs at pandora.be (Bert Jacobs)
Date: Thu, 20 Dec 2007 15:27:21 +0100
Subject: [R] Reshape Dataframe
In-Reply-To: <752525.13047.qm@web32815.mail.mud.yahoo.com>
Message-ID: <20071220142559.C894C300D1@harold.telenet-ops.be>


In which package do I find the "cast" function. At the moment it's not
recognized.
Thx,
Bert

-----Original Message-----
From: John Kane [mailto:jrkrideau at yahoo.ca] 
Sent: 19 December 2007 17:57
To: Bert Jacobs; 'hadley wickham'
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Reshape Dataframe

I think if you use Gabor's suggested addtion of
add.missing to the original cast command you get what
you want.

cast(dfm, ... ~ Var3, add.missing=TRUE)

--- Bert Jacobs <b.jacobs at pandora.be> wrote:

> 
> Thx Hadley,
> It works, but I need some finetuning.
> 
> If I use the following expression:
> Newdf <-reshape(df, timevar="Var3",
> idvar=c("Var1","Var2"),direction="wide")
> 
> Newdf
> Var1	Var2	Var3.W1	Var3.W2	Var3.W3	var3.W4
> A	Fa	1	  	3		
> A	Si	2				4
> B	Si	5		
> C	La			6
> C	Do							7
> 
> Is there an option so that for each Var1 all
> possible combinations of Var2
> are listed (i.e. creation of blanco lines).
> Is it possible to name the columns with the values
> of the original Var3
> variable, so that the name Var3.W1 changes to W1? 
> 
> Var1	Var2	W1	W2	W3	W4
> A	Fa	1	3		
> A	Si	2		4
> A	La
> A	Do			
> B	Fa				
> B	Si	5		
> B	La
> B	Do
> C	Fa				
> C	Si			
> C	La		6
> C	Do				7
> 
> 
> Thx,
> Bert
> 
> -----Original Message-----
> From: hadley wickham [mailto:h.wickham at gmail.com] 
> Sent: 18 December 2007 15:16
> To: Bert Jacobs
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Reshape Dataframe
> 
> On 12/18/07, Bert Jacobs <b.jacobs at pandora.be>
> wrote:
> >
> > Hi,
> >
> > I'm having a bit of problems in creating a new
> dataframe.
> > Below you'll find a description of the current
> dataframe and of the
> > dataframe that needs to be created.
> > Can someone help me out on this one?
> 
> library(reshape)
> dfm <- melt(df, id = 1:3)
> cast(dfm, ... ~ Var3)
> 
> You can find out more about the reshape package at
> http://had.co.nz/reshape
> 
> Hadley
> 
> -- 
> http://had.co.nz/
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
> 



      Looking for a X-Mas gift?  Everybody needs a Flickr Pro Account.

 

http://www.flickr.com/gift/


From mtmorgan at fhcrc.org  Thu Dec 20 15:46:58 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Thu, 20 Dec 2007 06:46:58 -0800
Subject: [R] custom subset method / handling columns selection as logic
 in '...' parameter
In-Reply-To: <5d897a2f0712200615o6be7a190kc9946b93f306a8ed@mail.gmail.com>
	(Eric Lecoutre's message of "Thu, 20 Dec 2007 15:15:39 +0100")
References: <5d897a2f0712200615o6be7a190kc9946b93f306a8ed@mail.gmail.com>
Message-ID: <6phtzmdtod9.fsf@gopher4.fhcrc.org>

Eric --

Please don't cross post

Please simplify your example so that others do not have to work hard
to understand what you are asking

See additional response on the Bioconductor mailing list.

Martin

"Eric Lecoutre" <ericlecoutre at gmail.com> writes:

> Dear R-helpers & bioconductor
>
>
> Sorry for cross-posting, this concerns R-programming stuff applied on
> Bioconductor context.
> Also sorry for this long message, I try to be complete in my request.
>
> I am trying to write a subset method for a specific class (ExpressionSet
> from Bioconductor) allowing selection more flexible than "[" method .
>
> The schema I am thinking for is the following:
>
> subset.ExpressionSet <- function(x,subset,...){
>
> }
>
> I will use the subset argument for rows (genes), as in default method.
>
> Now I would like to allow to select different columns (features) based on
> phenotypic data.
> phenotypic data provides detailed information about the columns.
>
> Basically, first function I have written allows the following:
>
>> sub1 <- subset(ExpressionSetObject, subset=NULL, V1=value1, v2=value2)
> # subset=NULL takes all rows
>
> See: there are two conditions on two variables belonging to the associated
> data.frame encapsulated in the ExpressionSetObject (to be complete, the
> conditions will be applied on more of 2 columns, as they are used on the
> phylogenic data.frame that concerns all variables)
> To simplify a little bit, this would nearly return:
> ExpressionSetObject[,V1==value & V2==value]
>
> This is nice as I can already handle any number of conditions on variables
> values thanks to '...'. First step is
> conditions <- list(...) and are then handled later in code
>
> Nevertheless, those conditions are basic (one value).
>
> I would like to handle arbitrary conditions, such as: V1 %in% c(value1,
> value2)
> More simple expression would be passed with V2==value instead of V2=value2
>
> My very problem is that I don't know how to turn '...' into an object
> containing those conditions that could be used later.
>
> My attempt which seems the nearest is:
>
>> foo <- function(...){
>> as.expression(substitute(list(...)))
>> }
>>foo(x==1,y%in%1:2)
> expression(list(x == 1, y %in% 1:2))
>
> where as I would like to have something like
> list(expression(x==1), expression(y %in% 1:2))
> those expressions beeing evaluated later on in the context of my specific
> object.
>
>
> Are there any existing function where '...' are already handled the way I
> want so that I can mimic?
>
> Thanks for any insight.
>
>
> Eric
>
> ---
>
> For those who have Biobase available, here is my current subset function and
> a demo-case that explains a little bit.
>
>
> library(Biobase)
> example(ExpressionSet) # create sample object
> print(expressionSet)
>
> # now my subset function as it is
>
> subset.ExpressionSet <- function(x,subset=NULL,verbose=TRUE,...){
>   # subset is used to subset on rows
>   # ... is used to make multiple conditions on columns based on pData
>   # list of conditions is handled in ...
>     stopifnot(is(x,"ExpressionSet"))
>     phenoData <- pData(x)
>     listCriteria <- list(...)
>     if (is.null(subset)) subset <- rep(TRUE,nrow(exprs(x)))
>     subset <- subset & !is.na(subset)
>     retainedCriteria <- list()
>     tmp <- sapply(names(listCriteria), function(critname) {
>       if(!critname %in% colnames(phenoData)){
>         if (verbose) cat("\n*** subsetCompounds: Dropped
> criteria:",critname, "not in phenoData of object\n")
>       }else{
>         if(is.null(listCriteria[critname])) listCriteria[[critname]]<-
> unique(phenoData[,critname])
>         retainedCriteria[[critname]] <<-  phenoData[,critname] %in%
> listCriteria[critname]
>       }
>       })
>       criteriaValues <- do.call("cbind",retainedCriteria)
>
>      selectedColumns <- rownames(phenoData)[apply(criteriaValues,1,logic)]
>       ## cbind(phenoData,criteriaValues)
>       out <- x[subset,selectedColumns]
>     if (verbose)  cat('\n',length(selectedColumns),' columns selected
> (',paste(selectedColumns,collapse=' '),
>       ')\n',sep='')
>      invisible(return(out))
>   }
>
> # looking at phenotypic data associated with the sample expressionSet
>> pData(expressionSet)
>      sex    type score
> A Female Control  0.75
> B   Male    Case  0.40
> C   Male Control  0.73
> D   Male    Case  0.42
> E Female    Case  0.93
> F   Male Control  0.22
> G   Male    Case  0.96
> H   Male    Case  0.79
> I Female    Case  0.37
> J   Male Control  0.63
> K   Male    Case  0.26
> L Female Control  0.36
> M   Male    Case  0.41
> N   Male    Case  0.80
> O Female    Case  0.10
> P Female Control  0.41
> Q Female    Case  0.16
> R   Male Control  0.72
> S   Male    Case  0.17
> T Female    Case  0.74
> U   Male Control  0.35
> V Female Control  0.77
> W   Male Control  0.27
> X   Male Control  0.98
> Y Female    Case  0.94
> Z Female    Case  0.32
>
>
> # now the sample use
>> (subset1 =subset(expressionSet,sex="Male",type="Control"))
> 7 columns selected (C F J R U W X)
> ExpressionSet (storageMode: lockedEnvironment)
> assayData: 500 features, 7 samples
>   element names: exprs, se.exprs
> phenoData
>   sampleNames: C, F, ..., X  (7 total)
>   varLabels and varMetadata description:
>     sex: Female/Male
>     type: Case/Control
>     score: Testing Score
> featureData
>   featureNames: AFFX-MurIL2_at, AFFX-MurIL10_at, ..., 31739_at  (500 total)
>   fvarLabels and fvarMetadata description: none
> experimentData: use 'experimentData(object)'
> Annotation: hgu95av2
>
>
> # what I would like to allow in use:
> (subset2 = subset(expressionSet, sex=="Male", score > 0.75) # note the ==
> instead of =
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Computational Biology / Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M2 B169
Phone: (206) 667-2793


From bates at stat.wisc.edu  Thu Dec 20 15:31:39 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 20 Dec 2007 08:31:39 -0600
Subject: [R] How can I extract the AIC score from a mixed model object
	produced using lmer?
In-Reply-To: <14419438.post@talk.nabble.com>
References: <OFEA565530.971E9869-ON882573B5.00784A2E-882573B5.00797442@fs.fed.us>
	<f21f775b0712181421w36e5fe6ep5d1229ed550cc58f@mail.gmail.com>
	<14419438.post@talk.nabble.com>
Message-ID: <40e66e0b0712200631i684de05ardd564c71e6b9ecb1@mail.gmail.com>

On Dec 19, 2007 9:42 AM, David Hewitt <dhewitt at vims.edu> wrote:
>
>
> David Barron-3 wrote:
> >
> > You can calculate the AIC as follows:
> >
> > (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> > aic1 <- AIC(logLik(fm1))
> >
> >

> Is AIC() [extractAIC()] "valid" for models with random effects? I noticed
> that the help page for extractAIC() does not list models with random
> effects. I think this boils down to the difference between the likelihoods
> for models with and without random effects, and I don't know. Just
> curious...

The log-likelihood for a linear mixed model is well-defined.  Whether
this makes AIC valid or not depends on how comfortable you are with
the idea of AIC in the first place.  My impression is that the
justification for AIC is not entirely rigorous but I must admit that I
haven't gone back to look at the original literature on it.

To the best of my knowledge and ability the log-likelihood from a
model fit by lmer with method = "ML" is properly defined and
accurately evaluated.  (The default estimation criterion in lmer is
REML and models fit by REML provide a close approximation to the
log-likelihood but not an exact result.  If you really want a
log-likelihood and AIC value you should refit with method = "ML".)
What is later done to the log-likelihood to obtain the AIC value is
more problematic.  In particular, one needs to provide a value for the
number of parameters in the model and that can be tricky.  Recently I
was working with models for data on test scores by students over time.
 There were over 200,000 students.  Under one way of counting
parameters, if I incorporate a random effect for the student this
costs me only 1 parameter, corresponding to the variance component for
that random effect.  However, I am incorporating over 200,000 random
effects to help model the observed responses.  So is the number of
parameters 1 or over 200,000?  I don't know.

Regarding the fact the the extractAIC help page doesn't mention models
with random effects, it can't list all the possible methods because
any package can add methods to a generic function.

>
>
> > On 12/18/07, Peter H Singleton <psingleton at fs.fed.us> wrote:
> >>
> >> I am running a series of candidate mixed models using lmer (package lme4)
> >> and I'd like to be able to compile a list of the AIC scores for those
> >> models so that I can quickly summarize and rank the models by AIC. When I
> >> do logistic regression, I can easily generate this kind of list by
> >> creating
> >> the model objects using glm, and doing:
> >>
> >> > md <- c("md1.lr", "md2.lr", "md3.lr")
> >> > aic <- c(md1.lr$aic, md2.lr$aic, md3.lr$aic)
> >> > aic2 <- cbind(md, aic)
> >>
> >> but when I try to extract the AIC score from the model object produced by
> >> lmer I get:
> >>
> >> > md1.lme$aic
> >> NULL
> >> Warning message:
> >> In md1.lme$aic : $ operator not defined for this S4 class, returning NULL
> >>
> >> So... How do I query the AIC value out of a mixed model object created by
> >> lmer?
> >>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> -----
> David Hewitt
> Virginia Institute of Marine Science
> http://www.vims.edu/fish/students/dhewitt/
> --
> View this message in context: http://www.nabble.com/How-can-I-extract-the-AIC-score-from-a-mixed-model-object-produced-using-lmer--tp14406832p14419438.html
> Sent from the R help mailing list archive at Nabble.com.
>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Thu Dec 20 16:11:11 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 20 Dec 2007 10:11:11 -0500 (EST)
Subject: [R] Reshape Dataframe
In-Reply-To: <20071220142559.C894C300D1@harold.telenet-ops.be>
Message-ID: <820907.5444.qm@web32812.mail.mud.yahoo.com>

reshape package.


--- Bert Jacobs <b.jacobs at pandora.be> wrote:

> 
> In which package do I find the "cast" function. At
> the moment it's not
> recognized.
> Thx,
> Bert
> 
> -----Original Message-----
> From: John Kane [mailto:jrkrideau at yahoo.ca] 
> Sent: 19 December 2007 17:57
> To: Bert Jacobs; 'hadley wickham'
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Reshape Dataframe
> 
> I think if you use Gabor's suggested addtion of
> add.missing to the original cast command you get
> what
> you want.
> 
> cast(dfm, ... ~ Var3, add.missing=TRUE)
> 
> --- Bert Jacobs <b.jacobs at pandora.be> wrote:
> 
> > 
> > Thx Hadley,
> > It works, but I need some finetuning.
> > 
> > If I use the following expression:
> > Newdf <-reshape(df, timevar="Var3",
> > idvar=c("Var1","Var2"),direction="wide")
> > 
> > Newdf
> > Var1	Var2	Var3.W1	Var3.W2	Var3.W3	var3.W4
> > A	Fa	1	  	3		
> > A	Si	2				4
> > B	Si	5		
> > C	La			6
> > C	Do							7
> > 
> > Is there an option so that for each Var1 all
> > possible combinations of Var2
> > are listed (i.e. creation of blanco lines).
> > Is it possible to name the columns with the values
> > of the original Var3
> > variable, so that the name Var3.W1 changes to W1? 
> > 
> > Var1	Var2	W1	W2	W3	W4
> > A	Fa	1	3		
> > A	Si	2		4
> > A	La
> > A	Do			
> > B	Fa				
> > B	Si	5		
> > B	La
> > B	Do
> > C	Fa				
> > C	Si			
> > C	La		6
> > C	Do				7
> > 
> > 
> > Thx,
> > Bert
> > 
> > -----Original Message-----
> > From: hadley wickham [mailto:h.wickham at gmail.com] 
> > Sent: 18 December 2007 15:16
> > To: Bert Jacobs
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Reshape Dataframe
> > 
> > On 12/18/07, Bert Jacobs <b.jacobs at pandora.be>
> > wrote:
> > >
> > > Hi,
> > >
> > > I'm having a bit of problems in creating a new
> > dataframe.
> > > Below you'll find a description of the current
> > dataframe and of the
> > > dataframe that needs to be created.
> > > Can someone help me out on this one?
> > 
> > library(reshape)
> > dfm <- melt(df, id = 1:3)
> > cast(dfm, ... ~ Var3)
> > 
> > You can find out more about the reshape package at
> > http://had.co.nz/reshape
> > 
> > Hadley
> > 
> > -- 
> > http://had.co.nz/
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> > reproducible code.
> > 
> 
> 
> 
>       Looking for a X-Mas gift?  Everybody needs a
> Flickr Pro Account.
> 
>  
> 
> http://www.flickr.com/gift/
> 
> 
>


From jrkrideau at yahoo.ca  Thu Dec 20 16:31:47 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 20 Dec 2007 10:31:47 -0500 (EST)
Subject: [R] Factor Madness
In-Reply-To: <fkbdns$vh3$1@ger.gmane.org>
Message-ID: <530210.21932.qm@web32801.mail.mud.yahoo.com>

You may be haning Ion coerced into a factor.  Have a
look at
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/98260.html
for some discussion of this. 

I find that I usually set 
options(stringsAsFactors = FALSE)  just because of
this but as Gabor points out it may have its own
disadvantages in sharing code.  I spent 2 hours once
trying to get an example to work only to find that the
code assume options(stringsAsFactors = TRUE).

--- Johannes Graumann <johannes_graumann at web.de>
wrote:

> As Tony assumed: a data frame.
> 
> Joh
> 
> John Kane wrote:
> 
> > What was spectrum orginally?
> > 
> > 
> > --- Johannes Graumann <johannes_graumann at web.de>
> > wrote:
> > 
> >> Why is class(spectrum[["Ion"]]) after this
> "factor"?
> >> 
> >> spectrum <- cbind(spectrum,Ion=rep("",
> >> nrow(spectrum)),Deviation.AMU=rep(0.0,
> >> nrow(spectrum)))
> >> 
> >> slowly going crazy ...
> >> 
> >> Joh
> >> 
> >> ______________________________________________
> >> R-help at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained,
> >> reproducible code.
> >>
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and
> provide commented,
> > minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From mmiller3 at iupui.edu  Thu Dec 20 16:33:05 2007
From: mmiller3 at iupui.edu (Michael A. Miller)
Date: Thu, 20 Dec 2007 10:33:05 -0500
Subject: [R] creating a factor from dates by subject?
Message-ID: <877ij9tm8e.fsf@lumen.indyrad.iupui.edu>

Dear R-help,

I have a data set consisting of measurements made on multiple
subjects.  Measurement sessions are repeated for each subject on
multiple dates.  Not all subjects have the same number of
sessions.  To create a factor that represents the session, I do
the following:

data <- read.csv('test-data.csv') # data appended below
data$date <- as.Date(data$date, format='%m/%d/%Y')
data$session <- rep(NA,nrow(data))
for (i in unique(data$ID)) {
  data$session[data$ID==i] <- as.numeric(factor(data$date[data$ID==i]))
}
data$session <- factor(data$session)

This results in a session column in the data frame that runs from
1 to the number of sessions for each subject ID. 

What do you R gurus think of this?  Is there a better more R-ish
way to do this with without creating the session variable in the
data frame and then looping?  I find myself doing this sort of
thing all the time and it feels crufty to me.  

Thanks, Mike

-- 
Michael A. Miller                               mmiller3 at iupui.edu
  Imaging Sciences, Department of Radiology, IU School of Medicine


"ID","date","session"
1,05/24/2006,1
1,02/01/2007,2
1,05/23/2007,3
1,07/06/2007,4
2,07/28/2006,1
2,09/24/2006,2
2,01/18/2007,3
3,07/24/2006,1
3,01/17/2007,2
3,03/22/2007,3
4,05/08/2006,1
4,07/24/2006,2
4,09/26/2006,3
4,03/16/2007,4
5,07/19/2006,1
5,01/11/2007,2
5,05/04/2007,3
6,06/27/2006,1
6,08/15/2006,2
6,10/31/2006,3
6,02/27/2007,4
7,08/01/2006,1
7,10/06/2006,2
7,03/16/2007,3
8,06/06/2006,1
8,11/16/2006,2
8,04/24/2007,3
9,03/13/2007,1
9,04/27/2007,2
9,05/13/2007,3
10,08/03/2006,1
10,01/03/2007,2
10,04/25/2007,3
10,06/12/2007,4
11,05/24/2005,1
11,08/31/2006,2
11,04/10/2007,3
12,01/25/2007,1
12,04/30/2007,2
12,06/11/2007,3
1,05/24/2006,1
1,02/01/2007,2
1,05/23/2007,3
1,07/06/2007,4
2,07/28/2006,1
2,09/24/2006,2
2,01/18/2007,3
3,07/24/2006,1
3,01/17/2007,2
3,03/22/2007,3
4,05/08/2006,1
4,07/24/2006,2
4,09/26/2006,3
4,03/16/2007,4
5,07/19/2006,1
5,01/11/2007,2
5,05/04/2007,3
6,06/27/2006,1
6,08/15/2006,2
6,10/31/2006,3
6,02/27/2007,4
7,08/01/2006,1
7,10/06/2006,2
7,03/16/2007,3
8,06/06/2006,1
8,11/16/2006,2
8,04/24/2007,3
9,03/13/2007,1
9,04/27/2007,2
9,05/13/2007,3
10,08/03/2006,1
10,01/03/2007,2
10,04/25/2007,3
10,06/12/2007,4
11,05/24/2005,1
11,08/31/2006,2
11,04/10/2007,3
12,01/25/2007,1
12,04/30/2007,2
12,06/11/2007,3
1,05/24/2006,1
1,02/01/2007,2
1,05/23/2007,3
1,07/06/2007,4
2,07/28/2006,1
2,09/24/2006,2
2,01/18/2007,3
3,07/24/2006,1
3,01/17/2007,2
3,03/22/2007,3
4,05/08/2006,1
4,07/24/2006,2
4,09/26/2006,3
4,03/16/2007,4
5,07/19/2006,1
5,01/11/2007,2
5,05/04/2007,3
6,06/27/2006,1
6,08/15/2006,2
6,10/31/2006,3
6,02/27/2007,4
7,08/01/2006,1
7,10/06/2006,2
7,03/16/2007,3
8,06/06/2006,1
8,11/16/2006,2
8,04/24/2007,3
9,03/13/2007,1
9,04/27/2007,2
9,05/13/2007,3
10,08/03/2006,1
10,01/03/2007,2
10,04/25/2007,3
10,06/12/2007,4
11,05/24/2005,1
11,08/31/2006,2
11,04/10/2007,3
12,01/25/2007,1
12,04/30/2007,2
12,06/11/2007,3
1,05/24/2006,1
1,02/01/2007,2
1,05/23/2007,3
1,07/06/2007,4
2,07/28/2006,1
2,09/24/2006,2
2,01/18/2007,3
3,07/24/2006,1
3,01/17/2007,2
3,03/22/2007,3
4,05/08/2006,1
4,07/24/2006,2
4,09/26/2006,3
4,03/16/2007,4
5,07/19/2006,1
5,01/11/2007,2
5,05/04/2007,3
6,06/27/2006,1
6,08/15/2006,2
6,10/31/2006,3
6,02/27/2007,4
7,08/01/2006,1
7,10/06/2006,2
7,03/16/2007,3
8,06/06/2006,1
8,11/16/2006,2
8,04/24/2007,3
9,03/13/2007,1
9,04/27/2007,2
9,05/13/2007,3
10,08/03/2006,1
10,01/03/2007,2
10,04/25/2007,3
10,06/12/2007,4
11,05/24/2005,1
11,08/31/2006,2
11,04/10/2007,3
12,01/25/2007,1
12,04/30/2007,2
12,06/11/2007,3


From dxc13 at health.state.ny.us  Thu Dec 20 16:35:57 2007
From: dxc13 at health.state.ny.us (dxc13)
Date: Thu, 20 Dec 2007 07:35:57 -0800 (PST)
Subject: [R]  alternate storage options
Message-ID: <14438736.post@talk.nabble.com>


useR's.

I am working with an algorithm in which I will need to create combinations
of all the points I have in a matrix.  When I have 2 variables, I simply use
expand.grid() to do this.  Sometimes I am faced with 3 or more variables and
if I run expand.grid(), R cannot process it due to the huge size.  Is there
any efficient way to workaround this? 

Thanks,
Derek
-- 
View this message in context: http://www.nabble.com/alternate-storage-options-tp14438736p14438736.html
Sent from the R help mailing list archive at Nabble.com.


From bcarvalh at jhsph.edu  Thu Dec 20 16:48:02 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Thu, 20 Dec 2007 10:48:02 -0500
Subject: [R] continue a for() when occurs errors
In-Reply-To: <974143.33371.qm@web56005.mail.re3.yahoo.com>
References: <974143.33371.qm@web56005.mail.re3.yahoo.com>
Message-ID: <49E00593-8750-4876-BBCD-134250D40A37@jhsph.edu>

your description is pretty vague, at least for me...

it would be very helpful to have the "commented, minimal, self- 
contained, reproducible code"...

but, anyways, you might want to take a look at the try() command.

best
b

On Dec 20, 2007, at 3:24 AM, Milton Cezar Ribeiro wrote:

> Dear all,
>
> I am simulating some regressions in a for() looping and sometimes  
> occours some error and the R stop my batch processing. I would like  
> do save the step where the error happned and continue my for()  
> looping.
> Is there a way to do that?
>
> Thanks In Advance.
> Miltinho
>
>
>
> para armazenamento!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marie-agnes.coutellec at rennes.inra.fr  Thu Dec 20 16:52:19 2007
From: marie-agnes.coutellec at rennes.inra.fr (Marie-Agnes Coutellec)
Date: Thu, 20 Dec 2007 16:52:19 +0100
Subject: [R] test for factor effect with nested glm
Message-ID: <476A8FB3.6050206@rennes.inra.fr>

Dear all,

I use a nested design with lm and glm, with factor2 nested within 
factor1. In order to test for the significance of both factors, I use 
anova tables on the obtained models such as follows:

/> mod1<-lm(A~factor1/factor2)
 > amod1<-anova(mod1, test="F")
Analysis of Variance Table
Response: A
                   Df Sum Sq Mean Sq F value    Pr(>F)   
factor1            15  85.99    5.73  3.1332 8.702e-05 ***
factor1:factor2    42 131.75    3.14  1.7145  0.005654 **
Residuals         308 563.52    1.83                      /

and recalculate the Fvalue for factor1 as follows:

/> ffactor1<-(85.99/15)/(131.75/42)
 > fpop
[1] 1.827491
 > 1-pf(ffactor1,15,42)
[1] 0.06285258/

My question is: am I allowed to do the same with an Analysis of Deviance 
table, calculated on a glm model, as done hereafter:

/> mod2<-glm(M~factor1/factor2, family=Gamma(link=identity))
 > amod2<-anova(mod2,test="F")
 > amod2
Analysis of Deviance Table
Model: Gamma, link: identity
Response: M
Terms added sequentially (first to last)

                 Df Deviance Resid. Df Resid. Dev      F    Pr(>F)   
NULL                               365    164.486                    
factor1          15   18.699       350    145.787 3.4365 1.997e-05 ***
factor1:factor2  42   38.973       308    106.814 2.5579 2.319e-06 ***
---

 > ffactor1<-(18.699/15)/(38.973/42)
[1] 1.343422
 > 1-pf(ffactor1,15,42)/
[1] 0.2206373


Thanks in advance for your answer, and sorry if my question doesn't fit 
with the R-help list.

Agn?s Coutellec

-- 

Marie-Agn?s Coutellec
UMR INRA-Agrocampus 985 EQHC
Equipe Ecotoxicologie et Qualit? des Milieux Aquatiques
65 rue de Saint-Brieuc - CS 84215
35042 Rennes cedex - FRANCE

t?l.: +33(0)2 23 48 52 48
fax:  +33(0)2 23 48 54 40


From cskiadas at gmail.com  Thu Dec 20 17:04:18 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Thu, 20 Dec 2007 11:04:18 -0500
Subject: [R] data shape
In-Reply-To: <20071220140634.F1689FACA21@as220.org>
References: <20071220140634.F1689FACA21@as220.org>
Message-ID: <799216B1-BA7C-4698-8332-2A516027A02B@gmail.com>

HI Tom,
On Dec 20, 2007, at 9:06 AM, Tom Sgouros wrote:

>
> Hello:
>
> I have been give a spreadsheet to work with formed as one big table.
> What it consists of is a 10-row-by-40-column table for each of  
> about 70
> different locations.  In other words, the table row names are repeated
> 70 times, once for each of the locations (whose names also appear  
> in the
> same column, where it's talking about the totals for that  
> location), e.g.:
>             A    B    C
>  Location1 15   73  123   <-  this row is the sum of the following 3
>  Under 10   6   42   23
>  10 - 25    4   15   23
>  Over 25    5   16   77
>  Location2 18   75  113   <- same here
>  Under 10   7   45   13
>  10 - 25    5   18   44
>  Over 25    6   12   56
>
> I want to get this into R as a collection of data frames, one for each
> of my locations.  My questions:
>
>   1. There is a way to handle a collection of data frames, isn't  
> there?
>      No doubt there are plenty, but what's the easiest way, so that I
>      can address them collectively, allowing me to ask such  
> questions as
>      what's the max of the over 25's in column C?

A list is the best way for that. Then you can use things like lapply  
and sapply, as I do towards the end of the script that follows.

>   2. What's the easiest way to read such a data array from a text  
> file?
>      I can do some editing of a csv file produced from the  
> spreadsheet,
>      but don't really know what to aim for.

Here is the code I used to read your example, which I saved as a  
comma-separated file, with the only addition that I added the name  
"Names" to the first column. You will probably need to adjust  
filename, nlocations and rows.per.location.

filename <- "~/Desktop/rows.txt"
nlocations <- 2
rows.per.location <- 3
data <- read.csv(filename)
data$Names <- gsub("\\s","", data$Names, perl=TRUE)  # Trim off  
whitespace from first column
totals <- data[4*seq_len(nlocations)-3,]             # Pick up the  
rows with the totals
actual.data <- data[-(4*seq_len(nlocations)-3),]     # Pick up the rest
location.names <-  totals[,1]                        # The location  
names are now the first column of totals
data.by.location <- split(actual.data, rep(location.names,  
each=rows.per.location)) # this is the "workhorse"
data.by.location <- lapply(data.by.location, function(x) {
   data.frame(x[,-1], row.names=x[,1])                   # Converting  
each list item to a better form
})
totals2 <- sapply(data.by.location, function(x) sapply(x,sum))
all(totals2 == t(totals[,-1]))   # Should return true if the totals  
add up


>   3. Is there some shortcut that would allow me to read this directly
>      from a spreadsheet?

Have a look at the xlsReadWrite package.

> Many thanks,
>
>  -tom
>

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From tomfool at as220.org  Thu Dec 20 17:36:39 2007
From: tomfool at as220.org (tom sgouros)
Date: Thu, 20 Dec 2007 11:36:39 -0500
Subject: [R] data shape
In-Reply-To: <799216B1-BA7C-4698-8332-2A516027A02B@gmail.com> 
References: <20071220140634.F1689FACA21@as220.org>
	<799216B1-BA7C-4698-8332-2A516027A02B@gmail.com>
Message-ID: <20071220163639.93311FACA3F@as220.org>


Charilaos Skiadas <cskiadas at gmail.com> wrote:

> >   2. What's the easiest way to read such a data array from a text
> > file?
> >      I can do some editing of a csv file produced from the
> > spreadsheet,
> >      but don't really know what to aim for.
> 
> Here is the code I used to read your example, which I saved as a
> comma-separated file, with the only addition that I added the name
> "Names" to the first column. You will probably need to adjust
> filename, nlocations and rows.per.location.

Thank you very much.  This is more than I'd hoped for.

 -tom

-- 
 ------------------------
 tomfool at as220 dot org
 http://sgouros.com  
 http://whatcheer.net


From b.jacobs at pandora.be  Thu Dec 20 17:56:23 2007
From: b.jacobs at pandora.be (Bert Jacobs)
Date: Thu, 20 Dec 2007 17:56:23 +0100
Subject: [R] Reshape Dataframe
In-Reply-To: <752525.13047.qm@web32815.mail.mud.yahoo.com>
Message-ID: <20071220165501.7ABAD300D5@harold.telenet-ops.be>

Hi,

The problem is probably that my Var4, does not contain number but factor
information, and therefore I think Gabor's suggestion does not work.
The same holds for Hadley's solution with the functions melt/cast, where the
resulting dataframe looks OK, but the dataframe is filled with the number of
times Var3 occurred for a certain Var1 and a certain Var2. It looks more
like:

Var1	Var2	W1	W2	W3	W4
A	Fa	1	1	
A	Si	1		1
B	Si	1		
C	La		1
C	Do				2

While it should be:

Var1	Var2	W1	W2	W3	W4
A	Fa	F1	F3	
A	Si	F2		F4
B	Si	F5		
C	La		F6
C	Do				F7
C	Do				F8

Or even better:

Var1	Var2	W1	W2	W3	W4
A	Fa	F1	F3	
A	Si	F2		F4
B	Si	F5		
C	La		F6
C	Do				F7/F8

If my original df is like
Var1	Var2	Var3	Var4
A	Fa	W1	F1
A	Si	W1	F2
A	Fa	W2	F3
A	Si	W3	F4
B	Si	W1	F5
C	La	W2	F6
C	Do	W4	F7
C	Do	W4	F8

-----Original Message-----
From: John Kane [mailto:jrkrideau at yahoo.ca] 
Sent: 19 December 2007 17:57
To: Bert Jacobs; 'hadley wickham'
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Reshape Dataframe

I think if you use Gabor's suggested addtion of
add.missing to the original cast command you get what
you want.

cast(dfm, ... ~ Var3, add.missing=TRUE)

--- Bert Jacobs <b.jacobs at pandora.be> wrote:

> 
> Thx Hadley,
> It works, but I need some finetuning.
> 
> If I use the following expression:
> Newdf <-reshape(df, timevar="Var3",
> idvar=c("Var1","Var2"),direction="wide")
> 
> Newdf
> Var1	Var2	Var3.W1	Var3.W2	Var3.W3	var3.W4
> A	Fa	1	  	3		
> A	Si	2				4
> B	Si	5		
> C	La			6
> C	Do							7
> 
> Is there an option so that for each Var1 all
> possible combinations of Var2
> are listed (i.e. creation of blanco lines).
> Is it possible to name the columns with the values
> of the original Var3
> variable, so that the name Var3.W1 changes to W1? 
> 
> Var1	Var2	W1	W2	W3	W4
> A	Fa	1	3		
> A	Si	2		4
> A	La
> A	Do			
> B	Fa				
> B	Si	5		
> B	La
> B	Do
> C	Fa				
> C	Si			
> C	La		6
> C	Do				7
> 
> 
> Thx,
> Bert
> 
> -----Original Message-----
> From: hadley wickham [mailto:h.wickham at gmail.com] 
> Sent: 18 December 2007 15:16
> To: Bert Jacobs
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Reshape Dataframe
> 
> On 12/18/07, Bert Jacobs <b.jacobs at pandora.be>
> wrote:
> >
> > Hi,
> >
> > I'm having a bit of problems in creating a new
> dataframe.
> > Below you'll find a description of the current
> dataframe and of the
> > dataframe that needs to be created.
> > Can someone help me out on this one?
> 
> library(reshape)
> dfm <- melt(df, id = 1:3)
> cast(dfm, ... ~ Var3)
> 
> You can find out more about the reshape package at
> http://had.co.nz/reshape
> 
> Hadley
> 
> -- 
> http://had.co.nz/
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
> 



      Looking for a X-Mas gift?  Everybody needs a Flickr Pro Account.

 

http://www.flickr.com/gift/


From h.wickham at gmail.com  Thu Dec 20 18:36:13 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 20 Dec 2007 17:36:13 +0000
Subject: [R] Reshape Dataframe
In-Reply-To: <20071220165501.7ABAD300D5@harold.telenet-ops.be>
References: <752525.13047.qm@web32815.mail.mud.yahoo.com>
	<20071220165501.7ABAD300D5@harold.telenet-ops.be>
Message-ID: <f8e6ff050712200936l7de42d9fwb51c7f1fc7da0355@mail.gmail.com>

On Dec 20, 2007 4:56 PM, Bert Jacobs <b.jacobs at pandora.be> wrote:
> Hi,
>
> The problem is probably that my Var4, does not contain number but factor
> information, and therefore I think Gabor's suggestion does not work.
> The same holds for Hadley's solution with the functions melt/cast, where the
> resulting dataframe looks OK, but the dataframe is filled with the number of
> times Var3 occurred for a certain Var1 and a certain Var2. It looks more
> like:
>
> Var1    Var2    W1      W2      W3      W4
> A       Fa      1       1
> A       Si      1               1
> B       Si      1
> C       La              1
> C       Do                              2
>
> While it should be:
>
> Var1    Var2    W1      W2      W3      W4
> A       Fa      F1      F3
> A       Si      F2              F4
> B       Si      F5
> C       La              F6
> C       Do                              F7
> C       Do                              F8
>
> Or even better:
>
> Var1    Var2    W1      W2      W3      W4
> A       Fa      F1      F3
> A       Si      F2              F4
> B       Si      F5
> C       La              F6
> C       Do                              F7/F8

In that case you'll need to specify an aggregation function.  You probably want:

cast(dfm, ... ~ Var3, function(x) paste(x, collapse="/"), add.missing=TRUE)

-- 
http://had.co.nz/


From h.wickham at gmail.com  Thu Dec 20 18:39:36 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 20 Dec 2007 17:39:36 +0000
Subject: [R] Question about which kind of plot to use
In-Reply-To: <4769CFEF.5090103@justemail.net>
References: <mn.9b737d7cee7588f6.83239@exitcheck.net>
	<200712191625.46587.dylan.beaudette@gmail.com>
	<eb555e660712191640g49709ac5hb84c0d0278d01fdb@mail.gmail.com>
	<4769CFEF.5090103@justemail.net>
Message-ID: <f8e6ff050712200939i3a26b1dcx789562ae0dce5c2e@mail.gmail.com>

> Perhaps as long as you're learning a new plotting system, you might also
> check out whether ggplot2 might be an option.
>
> I did a quick and dirty version (which I'm sure Hadley can improve and
> also remind me how to get rid of the legend that shows the "3" that I
> set the size to).
>
> Assuming your data is re-shaped, so it comes out something like mine in
> the artificial example below, then it's a two-liner in ggplot:
>
>
> maxdat.df <- data.frame (
>     score1 =  rnorm(9, mean = rep(c(10,20,30), each = 3), sd = 1 ) ,
>     SD = runif(9) * 2 + .5,
>     Group = factor ( rep ( c("V", "W", "X"), each = 3 ) ),
>     subGroup = rep( c("B","M","T"), 3) )
>
> maxdat.df
>
> library(ggplot2)
> ggp <- ggplot ( maxdat.df, aes (y = score1, x = interaction(Group ,
> subGroup), min = score1 - SD, max = score1 + SD, size = 3) )
> ggp + geom_pointrange() + coord_flip()

Take the size = 3 out of the aesthetic mappings, and put it directly
in geom_pointrange(size = 3) - this way you are setting the size to 3
(mm) rather than asking ggplot to map a variable containing only the
value 3 to the size of the points/lines.  It's a subtle but important
distinction, and I need to figure out how to explain it better.

Hadley

-- 
http://had.co.nz/


From Nicolas.Ris at sophia.inra.fr  Thu Dec 20 18:45:25 2007
From: Nicolas.Ris at sophia.inra.fr (Nicolas Ris)
Date: Thu, 20 Dec 2007 18:45:25 +0100
Subject: [R] hierarchical linear models, mixed models and lme
Message-ID: <476AAA35.4080000@sophia.inra.fr>

Dear R-users,

I am trying to analyse the data of the box 10.5 in the Biometry from 
Sokal and Rohlf (2001) using R. This is a three-level nested anova with 
equal sample size : 3 different treatments are compared ; 2 rats (coded 
1 or 2) / treatment are studied ; 3 preparations (coded 1, 2 or 3) / 
rats are available ; 2 readings of the glycogen content  / preparations 
are realised. Treatment is fixed whereas Rats (nested in Treatment)  and 
Prep (nested in Rats) are random effects.

According to a previous discussion found in the R-help archives (January 
2007), I have tried the following formula :
>  box105.lme<-lme(content~treatment, box105.gd, random=~1|rats/prep)
However,  the formula summary(box105.lme) gives wrong estimates for the 
variance components (rats within treatments, prep within rats, readings 
within preps) ! Moreover the numbers of rats and preps are also wrong, 
with respectively 2 and 6 instead of 3*2=6 and 6* 3=18 !

I have also tried the following formula :
>  box105bis.lme<-lme(content~treatment, box105.gd, 
random=~1|treatment/rats/prep
In this case, the variance components as well as the number of rats and 
preps are correct. Nevertheless, I have two new problems : (1) the 
treatment is first treated as a random effects although it is fixed ! 
(2) there is a serious problem of df when treatment is then treated as a 
fixed effect (18 df for the intercept and 0 for the two other treatments !)

What's wrong ? I didn't find such design and data in Pinheiro and Bates 
(2000)

Thanks for your help,

Nicolas


From dieter.menne at menne-biomed.de  Thu Dec 20 18:54:09 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 20 Dec 2007 17:54:09 +0000 (UTC)
Subject: [R] hierarchical linear models, mixed models and lme
References: <476AAA35.4080000@sophia.inra.fr>
Message-ID: <loom.20071220T174859-431@post.gmane.org>

Nicolas Ris <Nicolas.Ris <at> sophia.inra.fr> writes:

> I am trying to analyse the data of the box 10.5 in the Biometry from 
> Sokal and Rohlf (2001) using R. This is a three-level nested anova with 
> equal sample size : 3 different treatments are compared ; 2 rats (coded 
> 1 or 2) / treatment are studied ; 3 preparations (coded 1, 2 or 3) / 
> rats are available ; 2 readings of the glycogen content  / preparations 
> are realised. Treatment is fixed whereas Rats (nested in Treatment)  and 
> Prep (nested in Rats) are random effects.
> 
> According to a previous discussion found in the R-help archives (January 
> 2007), I have tried the following formula :
> >  box105.lme<-lme(content~treatment, box105.gd, random=~1|rats/prep)
> However,  the formula summary(box105.lme) gives wrong estimates for the 

Since your factors are codes as numbers, the first thing I would check if these
are really factors in your data frame. If these are numeric, you will get a
regression and degrees of freedom are totally off. Try

box105.gd$rats = as.factor(box105.gd$rats)
box105.gd$prep = as.factor(box105.gd$prep)

I am sure there will be some differences to Sokal/Rohlf afterwards, but you
should come closer. Please post a complete, self-running examples if you have
more questions.

Dieter


From jessen at econinfo.de  Thu Dec 20 19:29:28 2007
From: jessen at econinfo.de (Owe Jessen)
Date: Thu, 20 Dec 2007 19:29:28 +0100
Subject: [R] Recursive solution with for()
Message-ID: <476AB488.7010105@econinfo.de>

Hello,

i just ran into the following problem: I wanted to recursively solve 
equations of the type x_1[t]=x_1[t+1]+beta*x_2[t], and used a for-loop 
written

for(j in c(1:t-1, recursive=TRUE){
...
}

This didn't work, so i resolved to writing
for(j in c(10,9,...,1){

which worked, but is not terribly efficient. So, what did I do wrong?

Thanks in advance,
Owe

-- 
Owe Jessen
Diplom-Volkswirt
Hanssenstra?e 17
24106 Kiel

jessen at econinfo.de
http://www.econinfo.de


From christos at nuverabio.com  Thu Dec 20 19:58:19 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Thu, 20 Dec 2007 13:58:19 -0500
Subject: [R] Recursive solution with for()
In-Reply-To: <476AB488.7010105@econinfo.de>
References: <476AB488.7010105@econinfo.de>
Message-ID: <005401c8433a$49e18620$0e010a0a@headquarters.silicoinsights>

It not entirely clear, but I think that you are looking for

seq(t-1, 1)

-Christos 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Owe Jessen
> Sent: Thursday, December 20, 2007 1:29 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Recursive solution with for()
> 
> Hello,
> 
> i just ran into the following problem: I wanted to 
> recursively solve equations of the type 
> x_1[t]=x_1[t+1]+beta*x_2[t], and used a for-loop written
> 
> for(j in c(1:t-1, recursive=TRUE){
> ...
> }
> 
> This didn't work, so i resolved to writing for(j in c(10,9,...,1){
> 
> which worked, but is not terribly efficient. So, what did I do wrong?
> 
> Thanks in advance,
> Owe
> 
> --
> Owe Jessen
> Diplom-Volkswirt
> Hanssenstra?e 17
> 24106 Kiel
> 
> jessen at econinfo.de
> http://www.econinfo.de
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 


From David.John.Thompson at ontario.ca  Thu Dec 20 20:13:45 2007
From: David.John.Thompson at ontario.ca (Thompson, David (MNR))
Date: Thu, 20 Dec 2007 14:13:45 -0500
Subject: [R] Available environment variables
Message-ID: <ECF21B71808ECF4F8918C57EDBEE121D0189F10A@CTSPITDCEMMVX11.cihs.ad.gov.on.ca>

Hello,

I am trying to set my environment to streamline the downloading /
updating of packages. I have been through R-intro.html (10.8
Customizing-the-environment), R-FAQ (5.2 How can add-on packages be
installed?), rw-FAQ, and help pages for Sys.setenv {base},
download.packages {utils}, etc.,.

I am looking for something similar to the download.packages destdir
argument on a global environment level, e.g., destdir <- 'C:/Program
Files/_dave/stats/R/library/_download' in Rprofile.site. Any
suggestions?

Also, in a more general context, is there a way of listing all available
(or common, or default, or standard, . . .) environment variables? I
would like to know if there is an existing convention for any arbitrary
process that I may try to do without reinventing the wheel. This same
question could apply to options() other than those included with a
standard installation.

Thanks, DaveT.
*************************************
Silviculture Data Analyst
Ontario Forest Research Institute
Ontario Ministry of Natural Resources
david.john.thompson at ontario.ca
http://ofri.mnr.gov.on.ca


From rjohnson at ncifcrf.gov  Thu Dec 20 20:24:01 2007
From: rjohnson at ncifcrf.gov (Randall Johnson [Contr])
Date: Thu, 20 Dec 2007 14:24:01 -0500
Subject: [R] Data bug that read.csv doesn't like
Message-ID: <B5BB0295-C339-4A7E-AED1-FC253A6C0ABE@ncifcrf.gov>

Hello,
I have a bug in my data that read.csv doesn't like, but _only_ when  
specifying "na.strings = 'missing'". If I delete the offending Chinese  
characters the problem goes away as well. I'm satisfied that the  
problems with this data file are fixed, but is there anything I can to  
do avoid this in the future (other than avoiding Chinese characters).  
Any ideas as to what is going on here? I've attached the piece of the  
data file I used for the example below.

Best,
Randy

 > read.csv('../data/tmp.csv')
   Smoking_status Age_start_smoking         Pack_day
1              0
2              0
3        missing           missing          missing
4              1                18 \xc9\xd9\xc1\xbf
5              1                20                1
 > read.csv('../data/tmp.csv', na.strings = 'missing')
Error in type.convert(data[[i]], as.is = as.is[i], dec = dec,  
na.strings = character(0)) :
   invalid multibyte string
 > sessionInfo()
R version 2.6.1 (2007-11-26)
i386-apple-darwin9.1.0

locale:
en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base
 >

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Randall C Johnson
Bioinformatics Analyst
SAIC-Frederick, Inc (Contractor)
Laboratory of Genomic Diversity
NCI-Frederick, P.O. Box B
Bldg 560, Rm 11-85
Frederick, MD 21702
Phone: (301) 846-1304
Fax: (301) 846-1686
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


From waverley.paloalto at gmail.com  Thu Dec 20 20:29:04 2007
From: waverley.paloalto at gmail.com (Waverley)
Date: Thu, 20 Dec 2007 11:29:04 -0800
Subject: [R] Question how to get up triangle of a matrix
Message-ID: <8ee9d8f20712201129o126b6b83o327fe19c0cf8a9e2@mail.gmail.com>

Is there a simple way to get up triangle of a matrix and return as a vector?

Thanks much.

-- 
Waverley @ Palo Alto


From markleeds at verizon.net  Thu Dec 20 20:39:40 2007
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 20 Dec 2007 13:39:40 -0600 (CST)
Subject: [R] Question how to get up triangle of a matrix
Message-ID: <27306425.741151198179581055.JavaMail.root@vms246.mailsrvcs.net>

>From: Waverley <waverley.paloalto at gmail.com>
>Date: 2007/12/20 Thu PM 01:29:04 CST
>To: r-help at stat.math.ethz.ch
>Subject: [R] Question how to get up triangle of a matrix

?upper.tri

>Is there a simple way to get up triangle of a matrix and return as a vector?
>
>Thanks much.
>
>-- 
>Waverley @ Palo Alto
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Thu Dec 20 20:36:14 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 20 Dec 2007 14:36:14 -0500
Subject: [R] Available environment variables
In-Reply-To: <ECF21B71808ECF4F8918C57EDBEE121D0189F10A@CTSPITDCEMMVX11.cihs.ad.gov.on.ca>
References: <ECF21B71808ECF4F8918C57EDBEE121D0189F10A@CTSPITDCEMMVX11.cihs.ad.gov.on.ca>
Message-ID: <476AC42E.8070806@stats.uwo.ca>

On 20/12/2007 2:13 PM, Thompson, David (MNR) wrote:
> Hello,
> 
> I am trying to set my environment to streamline the downloading /
> updating of packages. I have been through R-intro.html (10.8
> Customizing-the-environment), R-FAQ (5.2 How can add-on packages be
> installed?), rw-FAQ, and help pages for Sys.setenv {base},
> download.packages {utils}, etc.,.
> 
> I am looking for something similar to the download.packages destdir
> argument on a global environment level, e.g., destdir <- 'C:/Program
> Files/_dave/stats/R/library/_download' in Rprofile.site. Any
> suggestions?

You could create your own function:

download.packages <- function(pkgs, destdir='C:/Program 
Files/_dave/stats/R/library/_download', ...)
   utils::download.packages(pkgs, destdir, ...)

> 
> Also, in a more general context, is there a way of listing all available
> (or common, or default, or standard, . . .) environment variables?

Sys.getenv() will return a list of the ones that are currently defined.
  I
> would like to know if there is an existing convention for any arbitrary
> process that I may try to do without reinventing the wheel. This same
> question could apply to options() other than those included with a
> standard installation.

I don't know what you mean about an "arbitrary process".

For options(), you could save the names that you consider standard (e.g. 
by starting an empty session and entering standardoptions <- 
names(options()), then at a later time do

newoptions <- names(options())
newoptions[ !(newoptions %in% standardoptions) ]

There's no easy way to know the options that would mean something in the 
current context but are not currently defined.

Duncan Murdoch


From pburns at pburns.seanet.com  Thu Dec 20 20:58:59 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 20 Dec 2007 19:58:59 +0000
Subject: [R] randomForest() for regression produces offset predictions
In-Reply-To: <200712191143.59625.j.knowles@manchester.ac.uk>
References: <200712191143.59625.j.knowles@manchester.ac.uk>
Message-ID: <476AC983.5020304@pburns.seanet.com>

What I see is the predictions being less extreme than the
actual values -- predictions for large actual values are smaller
than the actual, and predictions for small actual values are
larger than the actual.  That makes sense to me.  The object
is to maximize out-of-sample predictive power, not in-sample
predictive power.

Or am I missing something in what you are saying?


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


Joshua Knowles wrote:

>Hi all,
> 
>I have observed that when using the randomForest package to do regression, the 
>predicted values of the dependent variable given by a trained forest are not 
>centred and have the wrong slope when plotted against the true values.
> 
>This means that the R^2 value obtained by squaring the Pearson correlation are 
>better than those obtained by computing the coefficient of determination 
>directly. The R^2 value obtained by squaring the Pearson can, however, be 
>exactly reproduced by the coeff. of det. if the predicted values are first 
>linearly transformed (using lm() to find the required intercept and slope).
> 
>Does anyone know why the randomForest behaves in this way - producing offset 
>predictions? Does anyone know a fix for the problem?
> 
>(By the way, the feature is there even if the original dependent variable 
>values are initially transformed to have zero mean and unit variance.)
> 
>As an example, here is some simple R code that uses the available swiss 
>dataset to show the effect I am observing.
>
>Thanks for any help.
> 
>--
>#### EXAMPLE OF RANDOM FOREST REGRESSION
> 
>library(randomForest)
>data(swiss)
>swiss
> 
>#Build the random forest to predict Infant Mortality
>rf.rf<-randomForest(Infant.Mortality ~ ., data=swiss)
> 
>#And predict the training set again
>pred<-c(predict(rf.rf,swiss))
>actual<-swiss$Infant.Mortality
> 
>#Plotting predicted against actual values shows the effect (uncomment to see
>this)
>#plot(pred,actual)
>#abline(0,1)
> 
># calculate R^2 as pearson coefficient squared
>R2one<-cor(pred,actual)^2
> 
># calculate R^2 value as fraction of variance explained
>residOpt<-(actual-pred)
>residnone<-(actual-mean(actual))
>R2two<-1-var(residOpt,na.rm=TRUE)/var(residnone, na.rm=TRUE)
> 
># now fit a line through the predicted and true values and
># use this to normalize the data before calculating R^2
> 
>fit<-lm(actual ~ pred)
>coef(fit)
>pred2<-pred*coef(fit)[2]+coef(fit)[1]
>residOpt<-(actual-pred2)
>R2three<-1-var(residOpt,na.rm=TRUE)/var(residnone, na.rm=TRUE)
> 
>cat("Pearson squared = ",R2one,"\n")
>cat("Coeff of determination = ", R2two, "\n")
>cat("Coeff of determination after linear fitting = ", R2three, "\n")
> 
>## END
> 
>
>  
>


From adiamond at csidentity.com  Thu Dec 20 21:16:55 2007
From: adiamond at csidentity.com (adiamond)
Date: Thu, 20 Dec 2007 12:16:55 -0800 (PST)
Subject: [R]  Can't install RSPERL under windows
Message-ID: <14444277.post@talk.nabble.com>


I know that I'm a fool for trying to get this working under windows but I'm
obliged.  Note I have R 2.6.1 and the latest cygwin.  I'm running winxp sp2.

1) If I issue: R CMD INSTALL  RSPerl_0.92-1.tar.gz, it fails as such


---------- Making package RSPerl ------------

   **********************************************
   WARNING: this package has a configure script
         It probably needs manual configuration
   **********************************************

  adding build stamp to DESCRIPTION
Loading required package: graphics
Loading required package: stats
Loading required package: methods
Loading required package: DBI
  installing NAMESPACE file and metadata
Loading required package: graphics
Loading required package: stats
Loading required package: methods
Loading required package: DBI
  making DLL ...
Loading required package: graphics
Loading required package: stats
Loading required package: methods
Loading required package: DBI
make[3]: *** No rule to make target `version'.  Stop.
make[2]: *** [srcDynlib] Error 2
make[1]: *** [all] Error 2
make: *** [pkg-RSPerl] Error 2
*** Installation of RSPerl failed ***

2) If I try using the with-in-perl arg install complains that
--configure-args is not a valid arg.  My understanding is that it isn't
valid in windows R but I don't know what to do about it.

R CMD INSTALL  --configure-args='--with-in-perl' RSPerl_0.92-1.tar.gz
--configure-args='--with-in-perl' RSPerl_0.92-1.tar.gz
Unknown option: configure-args
Usage: R CMD INSTALL [options] pkgs
-- 
View this message in context: http://www.nabble.com/Can%27t-install-RSPERL-under-windows-tp14444277p14444277.html
Sent from the R help mailing list archive at Nabble.com.


From Greg.Snow at imail.org  Thu Dec 20 21:19:44 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Thu, 20 Dec 2007 13:19:44 -0700
Subject: [R] comparing poisson distributions
In-Reply-To: <BD808F22EEB72143BD185545452D0A787F23A2@FLMAIL1.fl.ad.scripps.edu>
References: <BD808F22EEB72143BD185545452D0A787F23A2@FLMAIL1.fl.ad.scripps.edu>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBDB6E40@LP-EXCHVS07.CO.IHC.COM>

There are a few different options that you can try depending on your
problem and your preferences:

1.  For large lambda the poisson can be approximated by a normal, for
large n (even for small lambda) the mean is approximately normal due to
the central limit theorem.  So if your lambda and n are large enough in
combination then you could just do a standard 2 sample t-test (t.test
function) and use the approximate p-value from there.

2.  Fit 2 models by maximum likelihood, one in which both lambdas are
equal and one in which they can differ (fitdistr from MASS may help, or
look at optim and friends), then do a likelihood ratio test on the
differences (-2 * likelihood diff is approx chisquared(1) under the
null).

3.  Do a permutation test:  find the difference in the
means/medians/(other stat of interest) between the 2 samples, then
permute the samples randomly (create 2 samples of the same sizes from
the original data values, but with random assignment as to which group a
value goes into) and find the same difference, repeate a bunch of times
(like 1998) and combine all the differences found into a vector, plot
the histogram of the values and look at where the original difference
fits into the distribution.  The number of values that are as or more
extreeme than the original value is your p-value.

4.  Create logical bins for values (e.g. 0-1, 2-3, 4-6, 7+) and count
how many from each group fall in each bin, use prop.test or chisq.test
to see if the proportions differ.

5.  Probably some others that don't come to mind right now.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Mark Gosink
> Sent: Tuesday, December 18, 2007 12:31 PM
> To: r-help at r-project.org
> Subject: [R] comparing poisson distributions
> 
> Hello all,
> 
>             I would like to compare two sets of count data 
> which form Poisson distributions. I'd like to generate some 
> sort of p-value of the likely-hood that the distributions are 
> the same. Thanks in advance for your advice.
> 
>  
> 
> Cheers,
> 
> Mark
> 
>  
> 
> Mark Gosink, Ph.D.
> 
> Head of Computational Biology
> Scripps Florida
> 5353 Parkside Drive - RFA
> Jupiter, FL  33458
> tel: 561-799-8921
> fax: 561-799-8952
> gosink at scripps.edu
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From p_connolly at slingshot.co.nz  Thu Dec 20 21:23:31 2007
From: p_connolly at slingshot.co.nz (p_connolly at slingshot.co.nz)
Date: Fri, 21 Dec 2007 09:23:31 +1300
Subject: [R] smoothScatter and geneplotter
Message-ID: <20071221092331.cgkkck0gk8gcg0gs@202.180.66.4>

On Tue, 18-Dec-2007 at 11:21AM -0500, James W. MacDonald wrote:

|> Duncan Murdoch wrote:
|> > Yes, I agree.  (As an aside, there's actually a capital S in
|> > smoothScatter(), and it's a bit of a pain to install, because
|> > geneplotter depends on something that depends on DBI, which is not so
|> > easily available these days.)
|>
|> Somehow I always forget the capital S and wonder if I have loaded the
|> correct package ;-D

Maybe you're misled from the output when the package is installed:


 >>> Building/Updating help pages for package 'geneplotter'
     Formats: text html latex example
  GetColor                          text    html    latex   example
  Makesense                         text    html    latex   example
  alongChrom                        text    html    latex   example
  amplicon.plot                     text    html    latex   example
  cColor                            text    html    latex   example

[...]

  multiecdf                         text    html    latex   example
  openHtmlPage                      text    html    latex   example
  panel.smoothScatter               text    html    latex   example
  plotChr                           text    html    latex   example
  plotExpressionGraph               text    html    latex   example
  savepng                           text    html    latex   example
  smoothscatter                     text    html    latex   example
** building package indices ...
* DONE (geneplotter)

Notice that the panel function has a capital S in its name but not the
basic function.  Does that point to a slight bug somewhere?  Could it
produce a more serious problem?


|>
|> As for installing the required dependencies, I believe this is actually
|> quite straightforward:
|>
|> source("http://www.bioconductor.org/biocLite.R")
|> biocLite("geneplotter")
|>
|> Should install geneplotter and all required dependencies.

That was about as easy as it would be possible to make it.  Thank you
for that.

best

--
*************************************************************
   ___      Patrick Connolly
 {~._.~}    HortResearch             Great minds discuss ideas;
 _( Y )_    Mt Albert                Average minds discuss events;
(:_~*~_:)   Auckland                 Small minds discuss people.
 (_)-(_)    New Zealand                                    .... Anon
            Ph: +64-9 815 4200 x 7188


From topkatz at msn.com  Thu Dec 20 21:43:05 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 20 Dec 2007 15:43:05 -0500
Subject: [R] Quote: An embedded and charset-unspecified text was scrubbed...
Message-ID: <BAY108-W41EF200D2555776D39D0C2AA5D0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071220/003279e3/attachment.pl 

From jmburgos at u.washington.edu  Thu Dec 20 21:47:50 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Thu, 20 Dec 2007 12:47:50 -0800
Subject: [R] Calculate remainer
In-Reply-To: <14414906.post@talk.nabble.com>
References: <14414906.post@talk.nabble.com>
Message-ID: <476AD4F6.9000606@u.washington.edu>

Hi Livia,

There are several ways to do this.  Try:

a=50/12

floor(a) will give you the entire portion, and

a-floor(a) will give you the remainder.

Julian

livia wrote:
> Hello everyone,
> 
> I have got a question about a simple calculation. If I would like to
> calculate 50/12 and return the result as 4 and the remainer 2. Is there a
> function of doing this?
> 
> Many thanks.


From Greg.Snow at imail.org  Thu Dec 20 21:57:56 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Thu, 20 Dec 2007 13:57:56 -0700
Subject: [R] assigning and saving datasets in a loop,
 with names changing with "i"
In-Reply-To: <WorldClient-F200712182124.AA24320054@epimgh.mcgill.ca>
References: <WorldClient-F200712182124.AA24320054@epimgh.mcgill.ca>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBDB6E5A@LP-EXCHVS07.CO.IHC.COM>

Have you looked at the SQLiteDF package?  It seems like it would do what
you want in a better way and much simpler.  Even if that does not work
then a database approach (look at the other db packages, probably RODBC
first) could be simpler, faster, and easier.  You may also want to look
at the g.data package for another approach.

Depending on what you are doing you may also want to look at the biglm
package (some similar functionality is in SQLiteDF).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Marie 
> Pierre Sylvestre
> Sent: Tuesday, December 18, 2007 7:25 PM
> To: r-help at r-project.org
> Subject: [R] assigning and saving datasets in a loop,with 
> names changing with "i"
> 
> Dear R users,
> 
> I am analysing a very large data set and I need to perform 
> several data manipulations. The dataset is so big that the 
> only way I can play with it without having memory problems 
> (E.g. "cannot allocate vectors of size...") is to write a 
> batch script to:
> 
> 1. cut the data into pieces
> 2. save the pieces in seperate .RData files 3. Remove 
> everything from the environment 4. load one of the piece 5. 
> perform the manipulations on it 6. save it and remove it from 
> the environment 7. Redo 4-6 for every piece 8. Merge 
> everything together at the end
> 
> It works if coded line by line but since I'll have to perform 
> these tasks on other data sets, I am trying to automate this 
> as much as I can. 
> 
> I am using a loop in which I used 'assign' and 'get' (pseudo 
> code below).
> My problem is when I use 'get', it prints the whole object on 
> the screen.
> I am wondering whether there is a more efficient way to do 
> what I need to do. Any help would be appreciated. Please keep 
> in mind that the whole process is quite computer-intensive, 
> so I can't keep everything in the environment while R 
> performs calculations.
> 
> Say I have 1 big dataframe called data. I use 'split' to 
> divide it into a list of 12 dataframes (call this list my.list)
> 
> my.fun is a function that takes a dataframe, performs several 
> manipulations on it and returns a dataframe.
> 
> 
> for (i in 1:12){
>   assign( paste( "data", i, sep=""),  my.fun(my.list[i]))   # 
> this works
>   # now I need to save this new object as a RData. 
> 
>   # The following line does not work
>   save(paste("data", i, sep = ""),  file = paste(  
> paste("data", i, sep = ""), "RData", sep=".")) }
> 
>   # This works but it is a bit convoluted!!!
>   temp <- get(paste("data", i, sep = ""))
>   save(temp,  file = "lala.RData")
> }
> 
> 
> I am *sure* there is something more clever to do but I can't 
> find it. Any help would be appreciated.
> 
> best regards,
> 
> MP
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From murdoch at stats.uwo.ca  Thu Dec 20 22:05:42 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 20 Dec 2007 16:05:42 -0500
Subject: [R] Quote: An embedded and charset-unspecified text was
	scrubbed...
In-Reply-To: <BAY108-W41EF200D2555776D39D0C2AA5D0@phx.gbl>
References: <BAY108-W41EF200D2555776D39D0C2AA5D0@phx.gbl>
Message-ID: <476AD926.10709@stats.uwo.ca>

On 20/12/2007 3:43 PM, Talbot Katz wrote:
> Occasionally when I click on a posting in the archives, I don't see the actual message, but instead, something like the following:
>  
> An embedded and charset-unspecified text was scrubbed...
> Name: not available
> Url: https://stat.ethz.ch/pipermail/r-help/attachments/200712XX/aXXXXXXX/attachment.pl
>  
>  
> In fact, this is what has happened with the past few messages I have posted.  Is this because of something contained in the emails I sent, or does it have to do with my browser?  Can the situation be "corrected" so that I can read the archived messages for which this occurs?  I'm using Internet Explorer 6.0.2900.2180.xpsp_sp2_gdr.070227-2254 on Windows XP Professional Version 5.1 (Build 2600.xpsp_sp2_gdr.070227-2254 : Service Pack 2).  I send my email from the current version of the Windows Live Hotmail site.  It appears that this scrubbing phenomenon is stable; I've never experienced a case where I could read a message one time but another time it was scrubbed.  I can read most messages, but if one has been scrubbed, then it appears that way each time I click on it.
>  
> Thanks!

I think it would help if you gave an actual example.  Could you post the 
URL you were clicking on?

Duncan Murdoch


From mpg33 at drexel.edu  Thu Dec 20 22:25:23 2007
From: mpg33 at drexel.edu (Michael Gormley)
Date: Thu, 20 Dec 2007 16:25:23 -0500
Subject: [R] running custom functions in Snow clusters
Message-ID: <003701c8434e$d470c710$0e161981@BIOINFORMATICS2>

When running a function in parallel using for example the clusterCall 
function in Snow is it possible to call other user-written functions from 
each cluster?  I get an error when I try to do this.  I tried to source the 
function on each cluster using the clusterEvalQ function but this doesn't 
seem to work.  This is more of an aesthetic problem than an actual one as 
obviously I can write a group of functions as one function and call this 
with clusterCall.

Thanks,
Mike Gormley
Drexel University


From Greg.Snow at imail.org  Thu Dec 20 22:29:59 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Thu, 20 Dec 2007 14:29:59 -0700
Subject: [R] Multiple plots with single box
In-Reply-To: <200712182321.lBINLgYU027646@definetti.ddns.uark.edu>
References: <200712182321.lBINLgYU027646@definetti.ddns.uark.edu>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBDB6E74@LP-EXCHVS07.CO.IHC.COM>


One possibility is to use the cnvrt.coords function from the
TeachingDemos package.  It shows an example of putting a rectangle
across multiple plots.  You would need to create the 1st (top) plot,
find the coordinate of the top and convert that to device coordinates,
then create the rest of your plots and after making the last (bottom)
one find the user coordinates for the left right and bottom, convert the
coordinate for the top from device to current user coordinates and then
plot the rectangle.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Giovanni Petris
> Sent: Tuesday, December 18, 2007 4:22 PM
> To: r-help at r-project.org
> Subject: [R] Multiple plots with single box
> 
> 
> Hello,
> 
> I am trying to display some harmonic functions in a plot. The 
> kind of display I have in mind is like the one that cn be 
> obtained by a call to plot.ts with plot.type = "multiple". 
> The only difference is that I want a single box containing 
> all the plots instead of one box per plot. I thought 
> box(which = "outer") would have done the job, but it didn't. 
> 
> Below is the code I have used so far. (R 2.5.1, I know, I know...)
> 
> Any help is greatly appreciated. 
> 
> Thank you in advance,
> Giovanni
> 
> =================
> ### Plot harmonic functions
> n <- 6 # even
> omega <- 2 * pi / n
> 
> par(mfrow = c(n - 1, 1), mar = c(0, 5.1, 0, 5.1), oma = c(3, 
> 1, 2, 1)) for (i in 1:(n/2 - 1)) {
>     curve(cos(x * i * omega), 0, n, ylim = c(-1.1, 1.1), ylab 
> = "", axes = FALSE)
>     points(1:n, cos(i * omega * 1:n))
>     axis(2); abline(h = 0, col = "lightgrey")
>     curve(sin(x * i * omega), 0, n, ylim = c(-1.1, 1.1), ylab 
> = "", axes = FALSE)
>     points(1:n, sin(i * omega * 1:n))
>     axis(4); abline(h = 0, col = "lightgrey") } curve(cos(x * 
> (n/2) * omega), 0, n, ylim = c(-1.1, 1.1), ylab = "", axes = 
> FALSE) points(1:n, rep(c(-1,1), n/2)) axis(1); axis(2); 
> abline(h = 0, col = "lightgrey")
> 
> -- 
> 
> Giovanni Petris  <GPetris at uark.edu>
> Department of Mathematical Sciences
> University of Arkansas - Fayetteville, AR 72701
> Ph: (479) 575-6324, 575-8630 (fax)
> http://definetti.uark.edu/~gpetris/
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From Greg.Snow at imail.org  Thu Dec 20 22:34:03 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Thu, 20 Dec 2007 14:34:03 -0700
Subject: [R] question
In-Reply-To: <2DA68F10815D6E49956E3DC2CE18603E0113D7C9@SRVSTAFF.iese.org>
References: <2DA68F10815D6E49956E3DC2CE18603E0113D7C9@SRVSTAFF.iese.org>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBDB6E78@LP-EXCHVS07.CO.IHC.COM>

?ifelse

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Armelini, Guillermo
> Sent: Wednesday, December 19, 2007 10:21 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] question
> 
> Hello everyone!
>  
> Is anybody can help me to solve this silly question that 
> unfortunately I haven't found the right way to address it.
>  
> Supose I have a matrix X[n,n] dimension
>  
> I would like to calculate the product of the vectors
>  
> Y=X[1,n]*X[n,1]
>  
> Then I would like to run the following operation
>  
> H=if(Y>0) {H[,1]=1}
>  
> The problem is that the condition "if" works only for the 
> first element of the matrix.
>  
> Does anybody knows how to apply the conditional "if" to a 
> entire vector?
>  
> cheers
>  
> Guillermo
>  
> 
> 
> This message has been scanned for viruses by TRENDMICRO, an 
> IESE technology affiliate company  and global leader in 
> antivirus and content security software.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From Greg.Snow at imail.org  Thu Dec 20 22:53:18 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Thu, 20 Dec 2007 14:53:18 -0700
Subject: [R] comparing poisson distributions
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBBDB6E40@LP-EXCHVS07.CO.IHC.COM>
References: <BD808F22EEB72143BD185545452D0A787F23A2@FLMAIL1.fl.ad.scripps.edu>
	<07E228A5BE53C24CAD490193A7381BBBDB6E40@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBDB6E8A@LP-EXCHVS07.CO.IHC.COM>

The other one I should have mentioned:

5.1:  Use the glm function with family = poisson.  The counts are the y
variable and the x variable is either 0/1 or a 2 level factor indicating
which group the values come from.  The p-value for the slope of x tests
for a difference in the 2 groups.

5.2 if this is just to make someone happy who always wants a p-value,
but doesn't understand it and will never actually use it, then use
runif.

5.3 ...

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Greg Snow
> Sent: Thursday, December 20, 2007 1:20 PM
> To: Mark Gosink; r-help at r-project.org
> Subject: Re: [R] comparing poisson distributions
> 
> There are a few different options that you can try depending 
> on your problem and your preferences:
> 
> 1.  For large lambda the poisson can be approximated by a 
> normal, for large n (even for small lambda) the mean is 
> approximately normal due to the central limit theorem.  So if 
> your lambda and n are large enough in combination then you 
> could just do a standard 2 sample t-test (t.test
> function) and use the approximate p-value from there.
> 
> 2.  Fit 2 models by maximum likelihood, one in which both 
> lambdas are equal and one in which they can differ (fitdistr 
> from MASS may help, or look at optim and friends), then do a 
> likelihood ratio test on the differences (-2 * likelihood 
> diff is approx chisquared(1) under the null).
> 
> 3.  Do a permutation test:  find the difference in the 
> means/medians/(other stat of interest) between the 2 samples, 
> then permute the samples randomly (create 2 samples of the 
> same sizes from the original data values, but with random 
> assignment as to which group a value goes into) and find the 
> same difference, repeate a bunch of times (like 1998) and 
> combine all the differences found into a vector, plot the 
> histogram of the values and look at where the original 
> difference fits into the distribution.  The number of values 
> that are as or more extreeme than the original value is your p-value.
> 
> 4.  Create logical bins for values (e.g. 0-1, 2-3, 4-6, 7+) 
> and count how many from each group fall in each bin, use 
> prop.test or chisq.test to see if the proportions differ.
> 
> 5.  Probably some others that don't come to mind right now.
> 
> Hope this helps,
> 
> --
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at imail.org
> (801) 408-8111
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at r-project.org
> > [mailto:r-help-bounces at r-project.org] On Behalf Of Mark Gosink
> > Sent: Tuesday, December 18, 2007 12:31 PM
> > To: r-help at r-project.org
> > Subject: [R] comparing poisson distributions
> > 
> > Hello all,
> > 
> >             I would like to compare two sets of count data 
> which form 
> > Poisson distributions. I'd like to generate some sort of p-value of 
> > the likely-hood that the distributions are the same. Thanks 
> in advance 
> > for your advice.
> > 
> >  
> > 
> > Cheers,
> > 
> > Mark
> > 
> >  
> > 
> > Mark Gosink, Ph.D.
> > 
> > Head of Computational Biology
> > Scripps Florida
> > 5353 Parkside Drive - RFA
> > Jupiter, FL  33458
> > tel: 561-799-8921
> > fax: 561-799-8952
> > gosink at scripps.edu
> > 
> >  
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From johannes_graumann at web.de  Thu Dec 20 22:43:05 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 20 Dec 2007 22:43:05 +0100
Subject: [R] Efficient way to find consecutive integers in vector?
Message-ID: <fkenl8$bt4$1@ger.gmane.org>

Hi all,

Does anybody have a magic trick handy to isolate directly consecutive
integers from something like this:
c(1,2,3,4,7,8,9,10,12,13)

The result should be, that groups 1-4, 7-10 and 12-13 are consecutive
integers ...

Thanks for any hints, Joh


From csardi at rmki.kfki.hu  Thu Dec 20 23:11:18 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Thu, 20 Dec 2007 23:11:18 +0100
Subject: [R] Efficient way to find consecutive integers in vector?
In-Reply-To: <fkenl8$bt4$1@ger.gmane.org>
References: <fkenl8$bt4$1@ger.gmane.org>
Message-ID: <20071220221118.GK5635@localdomain>

Joh,

x <- c(1,2,3,4,7,8,9,10,12,13)
which(diff(x) != 1)

gives the indices of the 'jumps'.

Gabor

On Thu, Dec 20, 2007 at 10:43:05PM +0100, Johannes Graumann wrote:
> Hi all,
> 
> Does anybody have a magic trick handy to isolate directly consecutive
> integers from something like this:
> c(1,2,3,4,7,8,9,10,12,13)
> 
> The result should be, that groups 1-4, 7-10 and 12-13 are consecutive
> integers ...
> 
> Thanks for any hints, Joh
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From milton_ruser at yahoo.com.br  Thu Dec 20 23:15:01 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Thu, 20 Dec 2007 14:15:01 -0800 (PST)
Subject: [R] collapsing a list in a var.
Message-ID: <726206.20810.qm@web56002.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071220/96e827c2/attachment.pl 

From PAlspach at hortresearch.co.nz  Thu Dec 20 23:23:38 2007
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Fri, 21 Dec 2007 11:23:38 +1300
Subject: [R] collapsing a list in a var.
In-Reply-To: <726206.20810.qm@web56002.mail.re3.yahoo.com>
References: <726206.20810.qm@web56002.mail.re3.yahoo.com>
Message-ID: <EC0F8FF776F3F74E9C63CE16641C96280276A535@AKLEXB02.hort.net.nz>

?sep

paste(my.obj, collapse=" ") 

HTH ......

Peter Alspach


> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Milton 
> Cezar Ribeiro
> Sent: Friday, 21 December 2007 11:15 a.m.
> To: R-help
> Subject: [R] collapsing a list in a var.
> 
> Dear all,
> 
> I have a object like my.obj<-c("a1","a2","a3") and I would 
> like my.new.obj="a1 a2 a3".
> I tryed to do my.new.obj<-paste(my.obj,sep=" ") but it return 
> three entries and I need only one entry as result.
> 
> Any idea?
> 
> Miltinho
> Brazil.
> 
> 
> 
>  para armazenamento!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

The contents of this e-mail are privileged and/or confidential to the named
 recipient and are not to be used by any other person and/or organisation.
 If you have received this e-mail in error, please notify the sender and delete
 all material pertaining to this e-mail.


From marc_schwartz at comcast.net  Thu Dec 20 23:33:54 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 20 Dec 2007 16:33:54 -0600
Subject: [R] Efficient way to find consecutive integers in vector?
In-Reply-To: <fkenl8$bt4$1@ger.gmane.org>
References: <fkenl8$bt4$1@ger.gmane.org>
Message-ID: <1198190034.4557.4.camel@Bellerophon.localdomain>


On Thu, 2007-12-20 at 22:43 +0100, Johannes Graumann wrote:
> Hi all,
> 
> Does anybody have a magic trick handy to isolate directly consecutive
> integers from something like this:
> c(1,2,3,4,7,8,9,10,12,13)
> 
> The result should be, that groups 1-4, 7-10 and 12-13 are consecutive
> integers ...
> 
> Thanks for any hints, Joh

Not fully tested, but here is one possible approach:

> Vec
 [1]  1  2  3  4  7  8  9 10 12 13

Breaks <- c(0, which(diff(Vec) != 1), length(Vec))

> Breaks
[1]  0  4  8 10

> sapply(seq(length(Breaks) - 1), 
         function(i) Vec[(Breaks[i] + 1):Breaks[i+1]])
[[1]]
[1] 1 2 3 4

[[2]]
[1]  7  8  9 10

[[3]]
[1] 12 13



For a quick test, I tried it on another vector:


set.seed(1)
Vec <- sort(sample(20, 15))

> Vec
 [1]  1  2  3  4  5  6  8  9 10 11 14 15 16 19 20

Breaks <- c(0, which(diff(Vec) != 1), length(Vec))

> Breaks
[1]  0  6 10 13 15

> sapply(seq(length(Breaks) - 1), 
         function(i) Vec[(Breaks[i] + 1):Breaks[i+1]])
[[1]]
[1] 1 2 3 4 5 6

[[2]]
[1]  8  9 10 11

[[3]]
[1] 14 15 16

[[4]]
[1] 19 20


HTH,

Marc Schwartz


From jholtman at gmail.com  Thu Dec 20 23:36:25 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 20 Dec 2007 17:36:25 -0500
Subject: [R] collapsing a list in a var.
In-Reply-To: <726206.20810.qm@web56002.mail.re3.yahoo.com>
References: <726206.20810.qm@web56002.mail.re3.yahoo.com>
Message-ID: <644e1f320712201436j51ab35fal23332d4eca663f@mail.gmail.com>

> my.obj<-c("a1","a2","a3")
> paste(my.obj, collapse=" ")
[1] "a1 a2 a3"


On Dec 20, 2007 5:15 PM, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
> Dear all,
>
> I have a object like my.obj<-c("a1","a2","a3") and I would like my.new.obj="a1 a2 a3".
> I tryed to do my.new.obj<-paste(my.obj,sep=" ") but it return three entries and I need only one entry as result.
>
> Any idea?
>
> Miltinho
> Brazil.
>
>
>
>  para armazenamento!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From rvaradhan at jhmi.edu  Thu Dec 20 23:38:56 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 20 Dec 2007 17:38:56 -0500
Subject: [R] can optimize solve paired euqations?
In-Reply-To: <BAY141-DAV5B7B4AAB5E543044966C3F05C0@phx.gbl>
References: <BAY141-DAV5B7B4AAB5E543044966C3F05C0@phx.gbl>
Message-ID: <005a01c84359$1aeab980$7c94100a@win.ad.jhu.edu>

Hi Xin,

You can use the nlsolve() function that I have written to solve a nonlinear
system of equations.  It converts a root finding problem into a minimization
problem, and uses optim() to find the minimizer.  

A well-known problem with this approach to root-finding is that a local
minimum of the squared residual function is not necessarily a root of the
nonlinear system.  A nice feature of nlsolve() is that it will try "really
hard" (by increasing the "nstarts" argument) to find the solution to the
system by identifying multiple local minima of the squared residual
function, if they exist.  

Here is a solution to your problem (I rearranged your equations a bit).

func <- function(x, A, B, pz) {
f <- rep(NA,length(x))
f[1] <- A/(1-pz^10) - x[2]*(1-x[1])/(x[1]*(1-x[1]^x[2]))
f[2] <- A + B/A - 1/x[1] - x[2]*(1-x[1])/x[1] 
f
}

> p0 <- c(0.2,10)
> set.seed(123)
> nlsolve(par=p0, fn=func, A=327.727, B=9517.336, pz=0.114^10, nstart=1000)
$par
[1] 0.03443476  11.68745035

$value
[1] 7.01806e-05

$counts
function gradient 
     148       15 

$convergence
[1] 0

$message
NULL


Best,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Xin
Sent: Wednesday, December 19, 2007 5:12 PM
To: R-Help
Subject: [R] can optimize solve paired euqations?

I used the command below, but R gives me the error message--syntax error.
can anyone see the mistakes I made?


optimize(function(x,y)
+ ((327.727-(1-0.114^10)*y*(1-x)/x/(1-x^y))+(9517.336-327.727
*(1+(1-x)*(1+y)/x-327.727)))^2
+ interval=c(0,1))


At the same time, I use nlm() but R gives me the code 

$code
[1] 3

function(vals) { 

        x <- vals[1] 

        y <- vals[2] 

 
sum(c((199.458913633542-(1-0.114^10)*y*(1-x)/x/(1-x^y)),(5234.11964684527-19
9.458913633542*(1+(1-x)*(1+y)/x-199.458913633542)))^2)

}

  

nlm(f, c(2,2))







Any one know how to deal with this case?

Thanks

Xin


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: nlsolve_R.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071220/90f06686/attachment.txt 

From topkatz at msn.com  Thu Dec 20 23:46:50 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 20 Dec 2007 17:46:50 -0500
Subject: [R] Quote: An embedded and charset-unspecified text was
 scrubbed...
In-Reply-To: <476AD926.10709@stats.uwo.ca>
References: <BAY108-W41EF200D2555776D39D0C2AA5D0@phx.gbl>
	<476AD926.10709@stats.uwo.ca>
Message-ID: <BAY108-W6FFE8243CACF572B2C27CAA5D0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071220/a7fb10ab/attachment.pl 

From r.turner at auckland.ac.nz  Thu Dec 20 23:47:27 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 21 Dec 2007 11:47:27 +1300
Subject: [R] comparing poisson distributions
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBBDB6E8A@LP-EXCHVS07.CO.IHC.COM>
References: <BD808F22EEB72143BD185545452D0A787F23A2@FLMAIL1.fl.ad.scripps.edu>
	<07E228A5BE53C24CAD490193A7381BBBDB6E40@LP-EXCHVS07.CO.IHC.COM>
	<07E228A5BE53C24CAD490193A7381BBBDB6E8A@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <5FB9B9B5-00B4-4F04-B3AB-705B7A67EE9C@auckland.ac.nz>


On 21/12/2007, at 10:53 AM, Greg Snow wrote:

>
> 5.2 if this is just to make someone happy who always wants a p-value,
> but doesn't understand it and will never actually use it, then use
> runif.

	A fortune?

		cheers,

			Rolf Turner

######################################################################
Attention:\ This e-mail message is privileged and confid...{{dropped:9}}


From murdoch at stats.uwo.ca  Thu Dec 20 23:59:31 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 20 Dec 2007 17:59:31 -0500
Subject: [R] Quote: An embedded and charset-unspecified text was
	scrubbed...
In-Reply-To: <BAY108-W6FFE8243CACF572B2C27CAA5D0@phx.gbl>
References: <BAY108-W41EF200D2555776D39D0C2AA5D0@phx.gbl>	<476AD926.10709@stats.uwo.ca>
	<BAY108-W6FFE8243CACF572B2C27CAA5D0@phx.gbl>
Message-ID: <476AF3D3.3070509@stats.uwo.ca>

On 20/12/2007 5:46 PM, Talbot Katz wrote:
> Hi Duncan. Thank you for responding.  Here is the URL in the R-help Archives for the original message I posted in this thread:
>  
> https://stat.ethz.ch/pipermail/r-help/2007-December/148923.html
>  
> When I go to that page, I see the scrub message.  I can read your response at:
>  
> https://stat.ethz.ch/pipermail/r-help/2007-December/148926.html
>  
> basically the same as when I opened your email.
>   --  TMK  --212-460-5430 home917-656-5351 cell > Date: Thu, 20 Dec 2007 16:05:42 -0500> From: murdoch at stats.uwo.ca> To: topkatz at msn.com> CC: r-help at stat.math.ethz.ch> Subject: Re: [R] Quote: An embedded and charset-unspecified text was scrubbed...> > On 20/12/2007 3:43 PM, Talbot Katz wrote:> > Occasionally when I click on a posting in the archives, I don't see the actual message, but instead, something like the following:> > > > An embedded and charset-unspecified text was scrubbed...> > Name: not available> > Url: https://stat.ethz.ch/pipermail/r-help/attachments/200712XX/aXXXXXXX/attachment.pl> > > > > > In fact, this is what has happened with the past few messages I have posted. Is this because of something contained in the emails I sent, or does it have to do with my browser? Can the situation be "corrected" so that I can read the archived messages for which this occurs? I'm using Internet Explorer 6.0.2900.2180.xpsp_sp2_gdr.070227-2254 on Windows XP Professional Ver
si!
>  on 5.1 (Build 2600.xpsp_sp2_gdr.070227-2254 : Service Pack 2). I send my email from the current version of the Windows Live Hotmail site. It appears that this scrubbing phenomenon is stable; I've never experienced a case where I could read a message one time but another time it was scrubbed. I can read most messages, but if one has been scrubbed, then it appears that way each time I click on it.> > > > Thanks!> > I think it would help if you gave an actual example. Could you post the > URL you were clicking on?> > Duncan Murdoch
> 	[[alternative HTML version deleted]]

I see the same message.  Perhaps it has something to do with the HTML in 
your message?  If you can get Hotmail to send plain ascii email, without 
HTML, your messages might make it into the archive.

Duncan Murdoch


From topkatz at msn.com  Fri Dec 21 00:17:10 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 20 Dec 2007 18:17:10 -0500
Subject: [R] Quote: An embedded and charset-unspecified text was
 scrubbed...
In-Reply-To: <476AF3D3.3070509@stats.uwo.ca>
References: <BAY108-W41EF200D2555776D39D0C2AA5D0@phx.gbl>
	<476AD926.10709@stats.uwo.ca>
	<BAY108-W6FFE8243CACF572B2C27CAA5D0@phx.gbl>
	<476AF3D3.3070509@stats.uwo.ca>
Message-ID: <BAY108-W2188F5D51F29332ADB2A15AA5D0@phx.gbl>


Duncan, thank you for the suggestion.  This should be a plain text message, perhaps it will post correctly to the R-help Archives.

--  TMK  --
212-460-5430	home
917-656-5351	cell


From m_olshansky at yahoo.com  Fri Dec 21 00:23:56 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Thu, 20 Dec 2007 15:23:56 -0800 (PST)
Subject: [R] Calculate remainer
In-Reply-To: <476AD4F6.9000606@u.washington.edu>
Message-ID: <556020.27146.qm@web32215.mail.mud.yahoo.com>

This is OK if the ratio is positive, but for -50
divided by 12 the floor is -5 and the remainder is 10
(and not -4 and -2 as one may want). By the way, using
%% and %/% leads to same result.
Using trunc will remedy the situation, i.e.

> x <- -50
> y <- 12
> a <- trunc(x/y)
> r <- x - a*y
> a
[1] -4
> r
[1] -2

--- Julian Burgos <jmburgos at u.washington.edu> wrote:

> Hi Livia,
> 
> There are several ways to do this.  Try:
> 
> a=50/12
> 
> floor(a) will give you the entire portion, and
> 
> a-floor(a) will give you the remainder.
> 
> Julian
> 
> livia wrote:
> > Hello everyone,
> > 
> > I have got a question about a simple calculation.
> If I would like to
> > calculate 50/12 and return the result as 4 and the
> remainer 2. Is there a
> > function of doing this?
> > 
> > Many thanks.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From johannes_graumann at web.de  Fri Dec 21 00:00:02 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Fri, 21 Dec 2007 00:00:02 +0100
Subject: [R] Efficient way to find consecutive integers in vector?
References: <fkenl8$bt4$1@ger.gmane.org>
Message-ID: <fkes5g$pnq$1@ger.gmane.org>

Thanks for your Inputs!

Joh

Johannes Graumann wrote:

> Hi all,
> 
> Does anybody have a magic trick handy to isolate directly consecutive
> integers from something like this:
> c(1,2,3,4,7,8,9,10,12,13)
> 
> The result should be, that groups 1-4, 7-10 and 12-13 are consecutive
> integers ...
> 
> Thanks for any hints, Joh
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From david at davidkatzconsulting.com  Fri Dec 21 00:37:52 2007
From: david at davidkatzconsulting.com (David Katz)
Date: Thu, 20 Dec 2007 15:37:52 -0800 (PST)
Subject: [R] randomForest() for regression produces offset predictions
In-Reply-To: <476AC983.5020304@pburns.seanet.com>
References: <200712191143.59625.j.knowles@manchester.ac.uk>
	<476AC983.5020304@pburns.seanet.com>
Message-ID: <14447468.post@talk.nabble.com>


I would expect this regression towards the mean behavior on a new or hold out
dataset, not on the training data. In RF terminology, this means that the
model prediction from predict is the in-bag estimate, but the out-of-bag
estimate is what you want for prediction. In Joshua's example,
rf.rf$predicted is an out-of-bag estimate, but since newdata is given, it
appears that the result is the in-bag estimate, which still needs an
adjustment like Joshua's  (and perhaps a more complex one might be needed in
some cases). This is a bit confusing since predict() usually matches what's
in model$fitted.values. I imagine that's why the author used "predicted" as
the component name instead of the standard "fitted.values".

The documentation for predict.randomForest explains:

"newdata - a data frame or matrix containing new data. (Note: If not given,
the out-of-bag prediction in object is returned. " 



Patrick Burns wrote:
> 
> What I see is the predictions being less extreme than the
> actual values -- predictions for large actual values are smaller
> than the actual, and predictions for small actual values are
> larger than the actual.  That makes sense to me.  The object
> is to maximize out-of-sample predictive power, not in-sample
> predictive power.
> 
> Or am I missing something in what you are saying?
> 
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> 
> Joshua Knowles wrote:
> 
>>Hi all,
>> 
>>I have observed that when using the randomForest package to do regression,
the 
>>predicted values of the dependent variable given by a trained forest are
not 
>>centred and have the wrong slope when plotted against the true values.
>> 
>>This means that the R^2 value obtained by squaring the Pearson correlation
are 
>>better than those obtained by computing the coefficient of determination 
>>directly. The R^2 value obtained by squaring the Pearson can, however, be 
>>exactly reproduced by the coeff. of det. if the predicted values are first 
>>linearly transformed (using lm() to find the required intercept and
slope).
>> 
>>Does anyone know why the randomForest behaves in this way - producing
offset 
>>predictions? Does anyone know a fix for the problem?
>> 
>>(By the way, the feature is there even if the original dependent variable 
>>values are initially transformed to have zero mean and unit variance.)
>> 
>>As an example, here is some simple R code that uses the available swiss 
>>dataset to show the effect I am observing.
>>
>>Thanks for any help.
>> 
>>--
>>#### EXAMPLE OF RANDOM FOREST REGRESSION
>> 
>>library(randomForest)
>>data(swiss)
>>swiss
>> 
>>#Build the random forest to predict Infant Mortality
>>rf.rf<-randomForest(Infant.Mortality ~ ., data=swiss)
>> 
>>#And predict the training set again
>>pred<-c(predict(rf.rf,swiss))
>>actual<-swiss$Infant.Mortality
>> 
>>#Plotting predicted against actual values shows the effect (uncomment to
see
>>this)
>>#plot(pred,actual)
>>#abline(0,1)
>> 
>># calculate R^2 as pearson coefficient squared
>>R2one<-cor(pred,actual)^2
>> 
>># calculate R^2 value as fraction of variance explained
>>residOpt<-(actual-pred)
>>residnone<-(actual-mean(actual))
>>R2two<-1-var(residOpt,na.rm=TRUE)/var(residnone, na.rm=TRUE)
>> 
>># now fit a line through the predicted and true values and
>># use this to normalize the data before calculating R^2
>> 
>>fit<-lm(actual ~ pred)
>>coef(fit)
>>pred2<-pred*coef(fit)[2]+coef(fit)[1]
>>residOpt<-(actual-pred2)
>>R2three<-1-var(residOpt,na.rm=TRUE)/var(residnone, na.rm=TRUE)
>> 
>>cat("Pearson squared = ",R2one,"\n")
>>cat("Coeff of determination = ", R2two, "\n")
>>cat("Coeff of determination after linear fitting = ", R2three, "\n")
>> 
>>## END
>> 
>>
>>  
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/randomForest%28%29-for-regression-produces-offset-predictions-tp14415517p14447468.html
Sent from the R help mailing list archive at Nabble.com.


From maura.monville at gmail.com  Fri Dec 21 00:39:29 2007
From: maura.monville at gmail.com (Maura E Monville)
Date: Thu, 20 Dec 2007 17:39:29 -0600
Subject: [R] package clim.pact
Message-ID: <36d691950712201539o7e006075k6c9d81441c718e10@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071220/2fdb53b9/attachment.pl 

From randy_walters at hotmail.com  Thu Dec 20 16:26:59 2007
From: randy_walters at hotmail.com (Randy Walters)
Date: Thu, 20 Dec 2007 07:26:59 -0800 (PST)
Subject: [R]  ifelse problem
Message-ID: <14438449.post@talk.nabble.com>


Could someone help me with the following code snippet.  The results are not
what I expect:

> Sheet1$Claims[1:10]
 [1] NA  1  2 NA NA NA NA NA NA NA

> Sheet1[1:10,"SubmissionStatus"]
 [1] Declined  Bound     Bound     Bound     Bound     Bound     Declined 
Dead      Declined 
[10] Not Taken
Levels: Bound Dead Declined Not Taken

> Sheet1$Claimsnum <- NA
> Sheet1$Claimsnum <- ifelse(Sheet1$SubmissionStatus != "Bound",99999,
+                     
ifelse(as.character(Sheet1$Claims)=="NA",0,Sheet1$Claims))
> 
> Sheet1$Claimsnum[1:10]
 [1] 99999     1     2    NA    NA    NA 99999 99999 99999 99999
> 

Here is Str(Sheet1)

 $ Claims                     : num  NA 1 2 NA NA NA NA NA NA NA ...
 $ SubmissionStatus           : Factor w/ 4 levels "Bound","Dead",..: 3 1 1
1 1 1 3 2 3 4 ...


I would expect Sheet1$Claimsnum[4] to be 0, since the true condition of the
2nd ifelse evaluations to 0.
Without the "as.character" the results are still not the way I want them:

> Sheet1$Claimsnum <- ifelse(Sheet1$SubmissionStatus != "Bound",99999,
+                      ifelse(Sheet1$Claims==NA,0,Sheet1$Claims))
> Sheet1$Claimsnum[1:10]
 [1] 99999    NA    NA    NA    NA    NA 99999 99999 99999 99999
> 

Much thanks!,

Randy



-- 
View this message in context: http://www.nabble.com/ifelse-problem-tp14438449p14438449.html
Sent from the R help mailing list archive at Nabble.com.


From m_olshansky at yahoo.com  Fri Dec 21 01:20:05 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Thu, 20 Dec 2007 16:20:05 -0800 (PST)
Subject: [R] alternate storage options
In-Reply-To: <14438736.post@talk.nabble.com>
Message-ID: <148604.8282.qm@web32211.mail.mud.yahoo.com>

You can use a loop...

If x,y and z are your vectors containing Nx,Ny and Nz
numbers respectively, then
for (Ix in 1:Nx) for (Iy in 1:Ny) for (Iz in 1:Nz) {
Point <- c(x[Ix],y[Iy],z[Iz])
do whatever you need with Point
}

A (probably better) compromise may be:

a <- matrix(0,nrow = Ny*Nz, ncol = 3)
for (i in 1:Nx) {
a[,2:3] <- expand.grid(y,z)
a[,1] <- x[i]
do whatever you want with a
}

--- dxc13 <dxc13 at health.state.ny.us> wrote:

> 
> useR's.
> 
> I am working with an algorithm in which I will need
> to create combinations
> of all the points I have in a matrix.  When I have 2
> variables, I simply use
> expand.grid() to do this.  Sometimes I am faced with
> 3 or more variables and
> if I run expand.grid(), R cannot process it due to
> the huge size.  Is there
> any efficient way to workaround this? 
> 
> Thanks,
> Derek
> -- 
> View this message in context:
>
http://www.nabble.com/alternate-storage-options-tp14438736p14438736.html
> Sent from the R help mailing list archive at
> Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From milton_ruser at yahoo.com.br  Fri Dec 21 01:27:20 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Thu, 20 Dec 2007 16:27:20 -0800 (PST)
Subject: [R] predicted mle() values.
Message-ID: <511820.58413.qm@web56011.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071220/609e0ceb/attachment.pl 

From louismartinbis at yahoo.fr  Fri Dec 21 01:37:02 2007
From: louismartinbis at yahoo.fr (Louis Martin)
Date: Fri, 21 Dec 2007 01:37:02 +0100 (CET)
Subject: [R] using apply to loop
Message-ID: <75879.1921.qm@web27402.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071221/bfcfa725/attachment.pl 

From cskiadas at gmail.com  Fri Dec 21 01:53:11 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Thu, 20 Dec 2007 19:53:11 -0500
Subject: [R] ifelse problem
In-Reply-To: <14438449.post@talk.nabble.com>
References: <14438449.post@talk.nabble.com>
Message-ID: <605FBA56-DCEE-4E65-B3C7-1F5B324A816A@gmail.com>

To check for NA, use is.na. For instance your second ifelse should read:

ifelse(is.na(Sheet1$Claims),0,Sheet1$Claims))

Converting Sheet1$Claims to character doesn't have the effect you  
think it does. NA is still NA, it does not become "NA". Try for  
instance:

as.character(NA)
as.character(NA) == "NA"

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College

On Dec 20, 2007, at 10:26 AM, Randy Walters wrote:

>
> Could someone help me with the following code snippet.  The results  
> are not
> what I expect:
>
>> Sheet1$Claims[1:10]
>  [1] NA  1  2 NA NA NA NA NA NA NA
>
>> Sheet1[1:10,"SubmissionStatus"]
>  [1] Declined  Bound     Bound     Bound     Bound     Bound      
> Declined
> Dead      Declined
> [10] Not Taken
> Levels: Bound Dead Declined Not Taken
>
>> Sheet1$Claimsnum <- NA
>> Sheet1$Claimsnum <- ifelse(Sheet1$SubmissionStatus != "Bound",99999,
> +
> ifelse(as.character(Sheet1$Claims)=="NA",0,Sheet1$Claims))
>>
>> Sheet1$Claimsnum[1:10]
>  [1] 99999     1     2    NA    NA    NA 99999 99999 99999 99999
>>
>
> Here is Str(Sheet1)
>
>  $ Claims                     : num  NA 1 2 NA NA NA NA NA NA NA ...
>  $ SubmissionStatus           : Factor w/ 4 levels  
> "Bound","Dead",..: 3 1 1
> 1 1 1 3 2 3 4 ...
>
>
> I would expect Sheet1$Claimsnum[4] to be 0, since the true  
> condition of the
> 2nd ifelse evaluations to 0.
> Without the "as.character" the results are still not the way I want  
> them:
>
>> Sheet1$Claimsnum <- ifelse(Sheet1$SubmissionStatus != "Bound",99999,
> +                      ifelse(Sheet1$Claims==NA,0,Sheet1$Claims))
>> Sheet1$Claimsnum[1:10]
>  [1] 99999    NA    NA    NA    NA    NA 99999 99999 99999 99999
>>
>
> Much thanks!,
>
> Randy
>
>
>
> -- 
> View this message in context: http://www.nabble.com/ifelse-problem- 
> tp14438449p14438449.html
> Sent from the R help mailing list archive at Nabble.com.
>


From Joe.Crombie at brs.gov.au  Fri Dec 21 04:47:13 2007
From: Joe.Crombie at brs.gov.au (Crombie, Joe)
Date: Fri, 21 Dec 2007 14:47:13 +1100
Subject: [R] using apply to loop [SEC=UNCLASSIFIED]
Message-ID: <61C2DEA055980B418D063F8646FCAEFC0476990A@ACT001CL03EX03.agdaff.gov.au>

Hi Louis,

You could try this:

# find the index of the maximum value in each row of _data_, #
disregarding the last column classified <-
apply(data[,-(nclass+1)],1,which.max)

## or, if the maximum may be repeated:
classified <- apply(data[,-(nclass+1)], 1, FUN = function(x) which(x ==
max(x)))

# the variable _truth_ is just the last column of _data_ ?
truth <- data[,nclass + 1]

?table
z <- table(classified, truth)


HTH  Joe

 
Joe Crombie
 
Biosecurity and Information Sciences
Bureau of Rural Science
Canberra  Australia
 
e: joe.crombie at brs.gov.au
 

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Louis Martin
Sent: Friday, 21 December 2007 11:37 AM
To: R-help at r-project.org
Subject: [R] using apply to loop

Hi,

I am running the following loop, but it takes hours to run as n is big.
Is there any way "apply" can be used? Thanks.
### Start
    nclass <- dim(data)[[2]] - 1
    z <- matrix(0, ncol = nclass, nrow = nclass)
    n <- dim(data)[[1]]
    x <- c(1:nclass)
# loop starts
for(loop in 1:n) {
        r <- data[loop, 1:nclass]
        classified <- x[r == max(r)]
       
        truth <- data[loop, nclass + 1]
        z[classified, truth] <- z[classified, truth] + 1
    }
# loop ends


             
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

------IMPORTANT - This message has been issued by The Department of Agriculture, Fisheries and Forestry (DAFF). The information transmitted is for the use of the intended recipient only and may contain confidential and/or legally privileged material. It is your responsibility to check any attachments for viruses and defects before opening or sending them on. 

Any reproduction, publication, communication, re-transmission, disclosure, dissemination or other use of the information contained in this e-mail by persons or entities other than the intended recipient is prohibited. The taking of any action in reliance upon this information by persons or entities other than the intended recipient is prohibited. If you have received this e-mail in error please notify the sender and delete all copies of this transmission together with any attachments. If you have received this e-mail as part of a valid mailing list and no longer want to receive a message such as this one advise the sender by return e-mail accordingly. Only e-mail correspondence which includes this footer, has been authorised by DAFF 
------


From tyler.smith at mail.mcgill.ca  Fri Dec 21 05:27:31 2007
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Fri, 21 Dec 2007 04:27:31 +0000 (UTC)
Subject: [R] using apply to loop
References: <75879.1921.qm@web27402.mail.ukl.yahoo.com>
Message-ID: <slrnfmmg5j.ie9.tyler.smith@blackbart.sedgenet>

On 2007-12-21, Louis Martin <louismartinbis at yahoo.fr> wrote:
> Hi,
>
> I am running the following loop, but it takes hours to run as n is big. Is there any way "apply" can be used? Thanks.
> ### Start
>     nclass <- dim(data)[[2]] - 1
>     z <- matrix(0, ncol = nclass, nrow = nclass)
>     n <- dim(data)[[1]]
>     x <- c(1:nclass)
> # loop starts
> for(loop in 1:n) { # looping over rows in data
>         r <- data[loop, 1:nclass] # vector of length(nclass)
>         classified <- x[r == max(r)] # index of rows == max(r)
>        
>         truth <- data[loop, nclass + 1] # next column, single value
>         z[classified, truth] <- z[classified, truth] + 1 # increment
>         the values of 
>     }
> # loop ends
>

Off the top, data is a bad choice for your dataframe, as it conflicts
with a standard function. Also, including some actual data would make
this easier to work with. I think you're using dim(data)[[1]] to get
the number of rows of data? That can be more clearly expressed as
nrow(data), and dim(data)[[2]] == ncol(data).

Anyways, this might be helpful:

add.mat <- apply(data[,1:nclass], MAR = 1, FUN = function(x) 
	         ifelse(x == max(x), 1, 0))

for(i in 1:n)
    z[ , data[i, ncol(data)]] <- z[ , data[i, ncol(data)]] + add.mat[,i]

There's still a loop, but it might not be needed depending on what
values 'truth' holds. Most of the calculations are shifted into the
apply() call, so the one line loop should run at least a little faster
than what you started with.

HTH,

Tyler


From rsela at stern.nyu.edu  Fri Dec 21 06:07:10 2007
From: rsela at stern.nyu.edu (Rebecca Sela)
Date: Fri, 21 Dec 2007 00:07:10 -0500 (EST)
Subject: [R] NaN as a parameter in NLMINB optimization
Message-ID: <354443.40411198213630658.JavaMail.root@calliope.stern.nyu.edu>

I am trying to optimize a likelihood function using NLMINB.  After running without a problem for quite a few iterations (enough that my intermediate output extends further than I can scroll back), it tries a vector of parameter values NaN.  This has happened with multiple Monte Carlo datasets, and a few different (but very similar) likelihood functions.  (They are complicated, but I can send them to someone if desired.)

Is this something that can happen with NLMINB, perhaps because of a 0/0 in the gradient calculations?  Or is it unique to my code?

Thanks in advance!

Rebecca

--
Rebecca Sela
Statistics Department/IOMS
Stern School of Business
New York University


From johannes_graumann at web.de  Fri Dec 21 10:56:30 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Fri, 21 Dec 2007 10:56:30 +0100
Subject: [R] Finding overlaps in vector
Message-ID: <fkfrm5$vj3$1@ger.gmane.org>

<posted & mailed>

Dear all,

I'm trying to solve the problem, of how to find clusters of values in a
vector that are closer than a given value. Illustrated this might look as
follows:

vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)

When using '0.5' as the proximity requirement, the following groups would
result:
0,0.45
3,3.25,3.33,3.75,4.1
6,6.45
7,7.1

Jim Holtman proposed a very elegant solution in
http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I have
modified and perused since he wrote it to me. The beauty of this approach
is that it will not only work for constant proximity requirements as above,
but also for overlap-windows defined in terms of ppm around each value.
Now I have an additional need and have found no way (short of iteratively
step through all the groups returned) to figure out how to do that with
Jim's approach: how to figure out that 6,6.45 and 7,7.1 are separate
clusters?

Thanks for any hints, Joh


From samu.mantyniemi at helsinki.fi  Fri Dec 21 09:07:47 2007
From: samu.mantyniemi at helsinki.fi (=?ISO-8859-1?Q?Samu_M=E4ntyniemi?=)
Date: Fri, 21 Dec 2007 10:07:47 +0200
Subject: [R] Multicore computation in Windows network: How to set up Rmpi
In-Reply-To: <476A7427.8060702@helsinki.fi>
References: <476A7427.8060702@helsinki.fi>
Message-ID: <476B7453.1010406@helsinki.fi>

Some progress in my problem:

Samu M?ntyniemi kirjoitti:

> With MPICH2 I managed to connect my computers so that I was able to
> remotely launch Rgui on both machines but R hanged when calling
> "library(Rmpi)". If only one Rgui was launched on the localhost,
> "library(Rmpi)" worked without errors, but trying to use
> "mpi.spawn.Rslaves()" resulted in an error message, and so did
> "mpi.universe.size()". (In my current setup I can not reproduce this 
> error message, but I can go back to this setup if this seems to be an 
> important piece of information)

I vent back to MPICH2 installation to see what the error was:
"ERROR in names(HOSTNAMES)<-base: attempt to set an attribute on NULL"

Trying to rethink what the problem was I realized that unlike in 
DeinoMPI, I need to write the host names manually on the "configurable 
settings" -window, and in order to have one cpu available on the local 
machine, I need to write "myhostname:2".

After these changes MPICH2 1.06 +R-2.6.0+Rmpi 0.5-5 work on the single 
machine in the same way as my DeinoMPI installation: Correct number of 
cpu:s is detected and I can "mpi.spawn.Rslaves()"

I will try to do this with two hosts next and see if there is more luck 
with MPICH2 than DeinoMPI.

Samu




------------------------------------------
Samu M?ntyniemi
Researcher
Fisheries and Environmental Management Group (FEM)
Department of Biological and Environmental Sciences
Biocenter 3, room 4414
Viikinkaari 1
P.O. Box 65
FIN-00014 University of Helsinki

Phone: +358 9 191 58710
Fax: +358 9 191 58257

email: samu.mantyniemi at helsinki.fi
personal webpage: http://www.helsinki.fi/people/samu.mantyniemi/
FEM webpage: http://www.helsinki.fi/science/fem/


From petr.pikal at precheza.cz  Fri Dec 21 09:45:51 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Fri, 21 Dec 2007 09:45:51 +0100
Subject: [R] Odp:  creating a factor from dates by subject?
In-Reply-To: <877ij9tm8e.fsf@lumen.indyrad.iupui.edu>
Message-ID: <OFD7F032FD.0FAEADA1-ONC12573B8.002FC020-C12573B8.003024EB@precheza.cz>

r-help-bounces at r-project.org napsal dne 20.12.2007 16:33:05:

> Dear R-help,
> 
> I have a data set consisting of measurements made on multiple
> subjects.  Measurement sessions are repeated for each subject on
> multiple dates.  Not all subjects have the same number of
> sessions.  To create a factor that represents the session, I do
> the following:
> 
> data <- read.csv('test-data.csv') # data appended below
> data$date <- as.Date(data$date, format='%m/%d/%Y')
> data$session <- rep(NA,nrow(data))
> for (i in unique(data$ID)) {
>   data$session[data$ID==i] <- as.numeric(factor(data$date[data$ID==i]))
> }
> data$session <- factor(data$session)

If you do not heve identical dates in one ID then

unlist(lapply(rle(test$V1)$lengths, seq))

shall give you session vector.

Regards
Petr


> 
> This results in a session column in the data frame that runs from
> 1 to the number of sessions for each subject ID. 
> 
> What do you R gurus think of this?  Is there a better more R-ish
> way to do this with without creating the session variable in the
> data frame and then looping?  I find myself doing this sort of
> thing all the time and it feels crufty to me. 
> 
> Thanks, Mike
> 
> -- 
> Michael A. Miller                               mmiller3 at iupui.edu
>   Imaging Sciences, Department of Radiology, IU School of Medicine
> 
> 
> "ID","date","session"
> 1,05/24/2006,1
> 1,02/01/2007,2
> 1,05/23/2007,3
> 1,07/06/2007,4
> 2,07/28/2006,1
> 2,09/24/2006,2
> 2,01/18/2007,3
> 3,07/24/2006,1
> 3,01/17/2007,2
> 3,03/22/2007,3
> 4,05/08/2006,1
> 4,07/24/2006,2
> 4,09/26/2006,3
> 4,03/16/2007,4
> 5,07/19/2006,1
> 5,01/11/2007,2
> 5,05/04/2007,3
> 6,06/27/2006,1
> 6,08/15/2006,2
> 6,10/31/2006,3
> 6,02/27/2007,4
> 7,08/01/2006,1
> 7,10/06/2006,2
> 7,03/16/2007,3
> 8,06/06/2006,1
> 8,11/16/2006,2
> 8,04/24/2007,3
> 9,03/13/2007,1
> 9,04/27/2007,2
> 9,05/13/2007,3
> 10,08/03/2006,1
> 10,01/03/2007,2
> 10,04/25/2007,3
> 10,06/12/2007,4
> 11,05/24/2005,1
> 11,08/31/2006,2
> 11,04/10/2007,3
> 12,01/25/2007,1
> 12,04/30/2007,2
> 12,06/11/2007,3
> 1,05/24/2006,1
> 1,02/01/2007,2
> 1,05/23/2007,3
> 1,07/06/2007,4
> 2,07/28/2006,1
> 2,09/24/2006,2
> 2,01/18/2007,3
> 3,07/24/2006,1
> 3,01/17/2007,2
> 3,03/22/2007,3
> 4,05/08/2006,1
> 4,07/24/2006,2
> 4,09/26/2006,3
> 4,03/16/2007,4
> 5,07/19/2006,1
> 5,01/11/2007,2
> 5,05/04/2007,3
> 6,06/27/2006,1
> 6,08/15/2006,2
> 6,10/31/2006,3
> 6,02/27/2007,4
> 7,08/01/2006,1
> 7,10/06/2006,2
> 7,03/16/2007,3
> 8,06/06/2006,1
> 8,11/16/2006,2
> 8,04/24/2007,3
> 9,03/13/2007,1
> 9,04/27/2007,2
> 9,05/13/2007,3
> 10,08/03/2006,1
> 10,01/03/2007,2
> 10,04/25/2007,3
> 10,06/12/2007,4
> 11,05/24/2005,1
> 11,08/31/2006,2
> 11,04/10/2007,3
> 12,01/25/2007,1
> 12,04/30/2007,2
> 12,06/11/2007,3
> 1,05/24/2006,1
> 1,02/01/2007,2
> 1,05/23/2007,3
> 1,07/06/2007,4
> 2,07/28/2006,1
> 2,09/24/2006,2
> 2,01/18/2007,3
> 3,07/24/2006,1
> 3,01/17/2007,2
> 3,03/22/2007,3
> 4,05/08/2006,1
> 4,07/24/2006,2
> 4,09/26/2006,3
> 4,03/16/2007,4
> 5,07/19/2006,1
> 5,01/11/2007,2
> 5,05/04/2007,3
> 6,06/27/2006,1
> 6,08/15/2006,2
> 6,10/31/2006,3
> 6,02/27/2007,4
> 7,08/01/2006,1
> 7,10/06/2006,2
> 7,03/16/2007,3
> 8,06/06/2006,1
> 8,11/16/2006,2
> 8,04/24/2007,3
> 9,03/13/2007,1
> 9,04/27/2007,2
> 9,05/13/2007,3
> 10,08/03/2006,1
> 10,01/03/2007,2
> 10,04/25/2007,3
> 10,06/12/2007,4
> 11,05/24/2005,1
> 11,08/31/2006,2
> 11,04/10/2007,3
> 12,01/25/2007,1
> 12,04/30/2007,2
> 12,06/11/2007,3
> 1,05/24/2006,1
> 1,02/01/2007,2
> 1,05/23/2007,3
> 1,07/06/2007,4
> 2,07/28/2006,1
> 2,09/24/2006,2
> 2,01/18/2007,3
> 3,07/24/2006,1
> 3,01/17/2007,2
> 3,03/22/2007,3
> 4,05/08/2006,1
> 4,07/24/2006,2
> 4,09/26/2006,3
> 4,03/16/2007,4
> 5,07/19/2006,1
> 5,01/11/2007,2
> 5,05/04/2007,3
> 6,06/27/2006,1
> 6,08/15/2006,2
> 6,10/31/2006,3
> 6,02/27/2007,4
> 7,08/01/2006,1
> 7,10/06/2006,2
> 7,03/16/2007,3
> 8,06/06/2006,1
> 8,11/16/2006,2
> 8,04/24/2007,3
> 9,03/13/2007,1
> 9,04/27/2007,2
> 9,05/13/2007,3
> 10,08/03/2006,1
> 10,01/03/2007,2
> 10,04/25/2007,3
> 10,06/12/2007,4
> 11,05/24/2005,1
> 11,08/31/2006,2
> 11,04/10/2007,3
> 12,01/25/2007,1
> 12,04/30/2007,2
> 12,06/11/2007,3
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.hankin at noc.soton.ac.uk  Fri Dec 21 09:55:08 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 21 Dec 2007 08:55:08 +0000
Subject: [R] array addition
In-Reply-To: <3972DD82-222B-4640-B299-62D9BBF0D9F4@noc.soton.ac.uk>
References: <3972DD82-222B-4640-B299-62D9BBF0D9F4@noc.soton.ac.uk>
Message-ID: <6FF5E117-06EB-416F-8C7D-31C0BBACDDF3@noc.soton.ac.uk>


[snip snip snip]

> suppose I have two arrays x1,x2 of dimensions a1,b1,c1 and
> a2,b2,c2 respectively.
>
> I want  x = x1   "+"   x2 with dimensions c(max(a1,a2), max(b1,b2),max
> (c1,c2))


[snip snip snip]

perhaps it wouldn't be too much to ask for you to
check the most recent version of the "magic" package?

[and we *really* don't want any whingeing about
magic_1.3-31 not being available.  If I were you, I'd
email the package maintainer and tell him to release
updates in a more timely manner. . . ]





 > library(magic)
 > aplus
function (...)
{
     args <- list(...)
     if (length(args) == 1) {
         return(args[[1]])
     }
     if (length(args) > 2) {
         jj <- do.call("Recall", c(args[-1]))
         return(do.call("Recall", c(list(args[[1]]), list(jj))))
     }
     a <- args[[1]]
     b <- args[[2]]
     dima <- dim(a)
     dimb <- dim(b)
     stopifnot(length(dima) == length(dimb))
     out <- array(0, pmax(dima, dimb))
     return(do.call("[<-", c(list(out), lapply(dima, seq_len),
         list(a))) + do.call("[<-", c(list(out), lapply(dimb,
         seq_len), list(b))))
}
 >



--
Robin Hankin
Uncertainty Analyst and Neutral Theorist,
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From rh at family-krueger.com  Fri Dec 21 10:58:40 2007
From: rh at family-krueger.com (Knut Krueger)
Date: Fri, 21 Dec 2007 10:58:40 +0100
Subject: [R] RMySQL installation problem - partially solved
In-Reply-To: <4766C2F7.3020302@web.de>
References: <BAY120-W88A17C859B8C463333777C7820@phx.gbl>	<Pine.LNX.4.64.0711152120050.7199@gannet.stats.ox.ac.uk>	<476384AE.5010500@family-krueger.com>	<4766365B.7010707@family-krueger.com>
	<4766505B.1060407@web.de> <4766593B.2060907@family-krueger.com>
	<4766C2F7.3020302@web.de>
Message-ID: <476B8E50.9050905@family-krueger.com>

First question: is this the right mailing list to discuss installation
problems?

if yes:
(Windows XP professional environment )
Installing a second  packege after the first causes an error:

-----------------------------------------------------------------------------------------------------------------
> utils:::menuInstallLocal()
package 'RMySQL' successfully unpacked and MD5 sums checked
updating HTML package descriptions
> utils:::menuInstallLocal()
package 'DBI' successfully unpacked and MD5 sums checked
Warning: unable to move temporary installation
'C:\Programme\R\R-2.6.1\library\file3d6c4ae1\DBI' to
'C:\Programme\R\R-2.6.1\library\DBI'
updating HTML package descriptions
-----------------------------------------------------------------------------------------------------------------

but installing DBI first causes the same error but than with RMySQL:

-----------------------------------------------------------------------------------------------------------------
> utils:::menuInstallLocal()
package 'DBI' successfully unpacked and MD5 sums checked
updating HTML package descriptions
> utils:::menuInstallLocal()
package 'RMySQL' successfully unpacked and MD5 sums checked
Warnung: unable to move temporary installation
'C:\Programme\R\R-2.6.1\library\file3d6c4ae1\RMySQL' to
'C:\Programme\R\R-2.6.1\library\RMySQL'
updating HTML package descriptions
-----------------------------------------------------------------------------------------------------------------
The directory C:\Programme\R\R-2.6.1\library\file3d6c4ae1\ is not
present ...

I tried a several times to restart the Rgui no without saving the
workspace and tried to install the package again no change.
The last time I saved the Workspace and got a success after installing
the package:
>
but the initial problem is still there

>  library(DBI)
>  library(RMySQL)
Error in dyn.load(file, ...) :
  cannot load shared library
'C:/Programme/R/R-2.6.1/library/RMySQL/libs/RMySQL.dll'
LoadLibrary failure:  module not found (Remark translated message)


Fehler: Laden von Paket/Namensraum f?r 'RMySQL' fehlgeschlagen
>


Regards Knut


From cgenolin at u-paris10.fr  Fri Dec 21 12:14:52 2007
From: cgenolin at u-paris10.fr (cgenolin at u-paris10.fr)
Date: Fri, 21 Dec 2007 12:14:52 +0100
Subject: [R] Within and Between matrix
Message-ID: <20071221121452.zzppsadwgggoo0s8@icare.u-paris10.fr>

Hi the list...

I am working on cluster analysis, more precisely on Calinski and 
Harabasz criterion
( C(g)=Trace(between)/Trace(Within)*(n-g)/(g-1)) )
Is there a package for calculating within and between matrix?
Morever, when we are using matrix with missing value, how do they 
handle it? On a more theorical point of view, how shall we do? 
Imputation will probably underestimate it, isn't it ?

Christophe



----------------------------------------------------------------
Ce message a ete envoye par IMP, grace a l'Universite Paris 10 Nanterre


From louismartinbis at yahoo.fr  Fri Dec 21 12:24:27 2007
From: louismartinbis at yahoo.fr (Louis Martin)
Date: Fri, 21 Dec 2007 12:24:27 +0100 (CET)
Subject: [R] RE : RE:  using apply to loop [SEC=UNCLASSIFIED]
In-Reply-To: <61C2DEA055980B418D063F8646FCAEFC0476990A@ACT001CL03EX03.agdaff.gov.au>
Message-ID: <338134.37805.qm@web27415.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071221/4c261705/attachment.pl 

From murdoch at stats.uwo.ca  Fri Dec 21 13:03:22 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 21 Dec 2007 07:03:22 -0500
Subject: [R] NaN as a parameter in NLMINB optimization
In-Reply-To: <354443.40411198213630658.JavaMail.root@calliope.stern.nyu.edu>
References: <354443.40411198213630658.JavaMail.root@calliope.stern.nyu.edu>
Message-ID: <476BAB8A.3090800@stats.uwo.ca>

On 21/12/2007 12:07 AM, Rebecca Sela wrote:
> I am trying to optimize a likelihood function using NLMINB.  After running without a problem for quite a few iterations (enough that my intermediate output extends further than I can scroll back), it tries a vector of parameter values NaN.  This has happened with multiple Monte Carlo datasets, and a few different (but very similar) likelihood functions.  (They are complicated, but I can send them to someone if desired.)
> 
> Is this something that can happen with NLMINB, perhaps because of a 0/0 in the gradient calculations?  Or is it unique to my code?

I would say it's a bug. I can't tell if it's a bug in your code or in 
the nlminb code, but even if it's a problem with your code, it would be 
nice if nlminb could recognize it and give a error report rather than 
passing garbage to your function.

Can you put together a small enough example to post here?  If not, you 
could send me the full example, but I may not get to it for a couple of 
weeks.  In either case, try to make sure the error occurs on every run 
by using set.seed() and/or a fixed dataset.  If the problem is 
intermittent even with that, please let us know.

Duncan Murdoch


From maechler at stat.math.ethz.ch  Fri Dec 21 15:54:22 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 21 Dec 2007 15:54:22 +0100
Subject: [R] Efficient way to find consecutive integers in vector?
In-Reply-To: <1198190034.4557.4.camel@Bellerophon.localdomain>
References: <fkenl8$bt4$1@ger.gmane.org>
	<1198190034.4557.4.camel@Bellerophon.localdomain>
Message-ID: <18283.54174.973381.750616@stat.math.ethz.ch>

>>>>> "MS" == Marc Schwartz <marc_schwartz at comcast.net>
>>>>>     on Thu, 20 Dec 2007 16:33:54 -0600 writes:

    MS> On Thu, 2007-12-20 at 22:43 +0100, Johannes Graumann wrote:
    >> Hi all,
    >> 
    >> Does anybody have a magic trick handy to isolate directly consecutive
    >> integers from something like this:
    >> c(1,2,3,4,7,8,9,10,12,13)
    >> 
    >> The result should be, that groups 1-4, 7-10 and 12-13 are consecutive
    >> integers ...
    >> 
    >> Thanks for any hints, Joh

    MS> Not fully tested, but here is one possible approach:

    >> Vec
    MS> [1]  1  2  3  4  7  8  9 10 12 13

    MS> Breaks <- c(0, which(diff(Vec) != 1), length(Vec))

    >> Breaks
    MS> [1]  0  4  8 10

    >> sapply(seq(length(Breaks) - 1), 
    MS> function(i) Vec[(Breaks[i] + 1):Breaks[i+1]])
    MS> [[1]]
    MS> [1] 1 2 3 4

    MS> [[2]]
    MS> [1]  7  8  9 10

    MS> [[3]]
    MS> [1] 12 13



    MS> For a quick test, I tried it on another vector:


    MS> set.seed(1)
    MS> Vec <- sort(sample(20, 15))

    >> Vec
    MS> [1]  1  2  3  4  5  6  8  9 10 11 14 15 16 19 20

    MS> Breaks <- c(0, which(diff(Vec) != 1), length(Vec))

    >> Breaks
    MS> [1]  0  6 10 13 15

    >> sapply(seq(length(Breaks) - 1), 
    MS> function(i) Vec[(Breaks[i] + 1):Breaks[i+1]])
    MS> [[1]]
    MS> [1] 1 2 3 4 5 6

    MS> [[2]]
    MS> [1]  8  9 10 11

    MS> [[3]]
    MS> [1] 14 15 16

    MS> [[4]]
    MS> [1] 19 20

Seems ok, but ``only works for increasing sequences''.
More than 12 years ago, I had encountered the same problem and
solved it like this:

In package 'sfsmisc', there has been the function  inv.seq(),
named for "inversion of seq()",
which does this too, currently returning an expression,
but returning a call in the development version of sfsmisc:

Its definition is currently

inv.seq <- function(i) {
  ## Purpose: 'Inverse seq': Return a short expression for the 'index'  `i'
  ## --------------------------------------------------------------------
  ## Arguments: i: vector of (usually increasing) integers.
  ## --------------------------------------------------------------------
  ## Author: Martin Maechler, Date:  3 Oct 95, 18:08
  ## --------------------------------------------------------------------
  ## EXAMPLES: cat(rr <- inv.seq(c(3:12, 20:24, 27, 30:33)),"\n"); eval(rr)
  ##           r2 <- inv.seq(c(20:13, 3:12, -1:-4, 27, 30:31)); eval(r2); r2
  li <- length(i <- as.integer(i))
  if(li == 0) return(expression(NULL))
  else if(li == 1) return(as.expression(i))
  ##-- now have: length(i) >= 2
  di1 <- abs(diff(i)) == 1	#-- those are just simple sequences  n1:n2 !
  s1 <- i[!c(FALSE,di1)] # beginnings
  s2 <- i[!c(di1,FALSE)] # endings

  ## using text & parse {cheap and dirty} :
  mkseq <- function(i,j) if(i == j) i else paste(i,":",j, sep="")
  parse(text =
        paste("c(", paste(mapply(mkseq, s1,s2), collapse = ","), ")", sep = ""),
        srcfile = NULL)[[1]]
}

with example code

 > v <- c(1:10,11,6,5,4,0,1)
 > (iv <- inv.seq(v))
 c(1:11, 6:4, 0:1)
 > stopifnot(identical(eval(iv), as.integer(v)))
 > iv[[2]]
 1:11
 > str(iv)
  language c(1:11, 6:4, 0:1)
 > str(iv[[2]])
  language 1:11
 > 


Now, given that this stems from  1995,  I should be excused for
using   parse(text = *)  [see  fortune(106) if you don't understand].

However, doing this differently by constructing the resulting
language object directly {using substitute(), as.symbol(),
	 		  as.expression() ... etc}
seems not quite trivial.

So here's the Friday afternoon /  Christmas break quizz:  

  What's the most elegant way
  to replace the last statements in  inv.seq()
  ------------------------------------------------------------------------
  ## using text & parse {cheap and dirty} :
  mkseq <- function(i,j) if(i == j) i else paste(i,":",j, sep="")
  parse(text =
        paste("c(", paste(mapply(mkseq, s1,s2), collapse = ","), ")", sep = ""),
	      srcfile = NULL)[[1]]
  ------------------------------------------------------------------------

  by code that does not use parse (or source() or similar) ???

I don't have an answer yet, at least not at all an elegant one.
And maybe, the solution to the quiz is that there is no elegant
solution.

Martin


    MS> HTH,

    MS> Marc Schwartz


From Kazumi.Maniwa at uni-konstanz.de  Fri Dec 21 14:30:43 2007
From: Kazumi.Maniwa at uni-konstanz.de (Kazumi Maniwa)
Date: Fri, 21 Dec 2007 14:30:43 +0100
Subject: [R] post hoc in repeated measures of anova
Message-ID: <20071221143043.xe5msd8ic8wo00cc@webmail.uni-konstanz.de>

Hallo, I have this dataset with repeated measures. There are two  
within-subject factors, "formant" (2 levels: 1 and 2) and "f2 Ref" (25  
levels: 670, 729, 788, 846, 905, 1080, 1100, 1120, 1140, 1170, 1480,  
1470, 1450, 1440, 1430, 1890, 1840, 1790, 1740, 1690, 2290, 2210,  
2120, 2040, 1950), and one between-subject factor, lang (2 levels:1  
and 2). The response variable is "thresh".

I ran a three-way ANOVA with repeated measures:

vThresh<-read.table("rRes.csv",header=T,sep=",")
attach(vThresh)
vThresh.aov<-aov(thresh~factor(lang)*factor(formant)*factor(f2Ref)+Error(factor(sub)))
summary(vThresh.aov)
detach(vThresh)

The table looks like this:

Error: factor(sub)
              Df Sum Sq Mean Sq F value  Pr(>F)
factor(lang)  1  442.6   442.6  4.8849 0.03112 *
Residuals    57 5164.2    90.6
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Error: Within
                                              Df Sum Sq Mean Sq  F value
factor(formant)                               1  424.4   424.4 179.3530
factor(f2Ref)                                24  658.3    27.4  11.5937
factor(lang):factor(formant)                  1   83.3    83.3  35.2225
factor(lang):factor(f2Ref)                   24   48.3     2.0   0.8510
factor(formant):factor(f2Ref)                24  952.7    39.7  16.7782
factor(lang):factor(formant):factor(f2Ref)   24   84.2     3.5   1.4825
Residuals                                  2793 6608.3     2.4
                                               Pr(>F)
factor(formant)                            < 2.2e-16 ***
factor(f2Ref)                              < 2.2e-16 ***
factor(lang):factor(formant)               3.304e-09 ***
factor(lang):factor(f2Ref)                   0.67204
factor(formant):factor(f2Ref)              < 2.2e-16 ***
factor(lang):factor(formant):factor(f2Ref)   0.06139 .
Residuals
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


Most of the main effects and interactions are significant, so I would  
like to do post hoc comparison for all main effects and interactins.
I have been trying hard to figure out how to do, but I am sorry, I am  
still novice and could not find the way to do.

It will be great if you could help me with codes for post hoc in  
repeated measures ANOVA.

Here is the dataset:

sub	thresh	lang	formant	f2Ref
1.00E+00	9.57E+00	1.00E+00	1.00E+00	6.70E+02
2.00E+00	1.04E+01	1.00E+00	1.00E+00	6.70E+02
3.00E+00	7.25E+00	1.00E+00	1.00E+00	6.70E+02
4.00E+00	5.59E+00	1.00E+00	1.00E+00	6.70E+02
5.00E+00	7.35E+00	1.00E+00	1.00E+00	6.70E+02
6.00E+00	6.02E+00	1.00E+00	1.00E+00	6.70E+02
7.00E+00	5.35E+00	1.00E+00	1.00E+00	6.70E+02
8.00E+00	6.95E+00	1.00E+00	1.00E+00	6.70E+02
9.00E+00	1.14E+01	1.00E+00	1.00E+00	6.70E+02
1.00E+01	1.02E+01	1.00E+00	1.00E+00	6.70E+02
1.10E+01	8.48E+00	1.00E+00	1.00E+00	6.70E+02
1.20E+01	6.70E+00	1.00E+00	1.00E+00	6.70E+02
1.30E+01	7.11E+00	1.00E+00	1.00E+00	6.70E+02
1.40E+01	4.90E+00	1.00E+00	1.00E+00	6.70E+02
1.50E+01	5.95E+00	1.00E+00	1.00E+00	6.70E+02
1.60E+01	6.77E+00	1.00E+00	1.00E+00	6.70E+02
1.70E+01	8.41E+00	1.00E+00	1.00E+00	6.70E+02
1.80E+01	8.52E+00	1.00E+00	1.00E+00	6.70E+02
1.90E+01	7.27E+00	1.00E+00	1.00E+00	6.70E+02
2.00E+01	8.87E+00	1.00E+00	1.00E+00	6.70E+02
2.10E+01	7.51E+00	1.00E+00	1.00E+00	6.70E+02
2.20E+01	6.57E+00	1.00E+00	1.00E+00	6.70E+02
2.30E+01	9.75E+00	1.00E+00	1.00E+00	6.70E+02
2.40E+01	7.80E+00	1.00E+00	1.00E+00	6.70E+02
2.50E+01	8.38E+00	1.00E+00	1.00E+00	6.70E+02
2.60E+01	8.68E+00	1.00E+00	1.00E+00	6.70E+02
2.70E+01	6.34E+00	1.00E+00	1.00E+00	6.70E+02
2.80E+01	6.60E+00	1.00E+00	1.00E+00	6.70E+02
2.90E+01	6.66E+00	1.00E+00	1.00E+00	6.70E+02
3.00E+01	7.68E+00	2.00E+00	1.00E+00	6.70E+02
3.10E+01	6.13E+00	2.00E+00	1.00E+00	6.70E+02
3.20E+01	6.34E+00	2.00E+00	1.00E+00	6.70E+02
3.30E+01	9.89E+00	2.00E+00	1.00E+00	6.70E+02
3.40E+01	6.84E+00	2.00E+00	1.00E+00	6.70E+02
3.50E+01	7.93E+00	2.00E+00	1.00E+00	6.70E+02
3.60E+01	5.87E+00	2.00E+00	1.00E+00	6.70E+02
3.70E+01	5.90E+00	2.00E+00	1.00E+00	6.70E+02
3.80E+01	5.63E+00	2.00E+00	1.00E+00	6.70E+02
3.90E+01	8.93E+00	2.00E+00	1.00E+00	6.70E+02
4.00E+01	7.68E+00	2.00E+00	1.00E+00	6.70E+02
4.10E+01	7.69E+00	2.00E+00	1.00E+00	6.70E+02
4.20E+01	8.21E+00	2.00E+00	1.00E+00	6.70E+02
4.30E+01	6.89E+00	2.00E+00	1.00E+00	6.70E+02
4.40E+01	7.77E+00	2.00E+00	1.00E+00	6.70E+02
4.50E+01	7.57E+00	2.00E+00	1.00E+00	6.70E+02
4.60E+01	1.04E+01	2.00E+00	1.00E+00	6.70E+02
4.70E+01	5.93E+00	2.00E+00	1.00E+00	6.70E+02
4.80E+01	7.35E+00	2.00E+00	1.00E+00	6.70E+02
4.90E+01	6.41E+00	2.00E+00	1.00E+00	6.70E+02
5.00E+01	4.42E+00	2.00E+00	1.00E+00	6.70E+02
5.10E+01	7.72E+00	2.00E+00	1.00E+00	6.70E+02
5.20E+01	7.72E+00	2.00E+00	1.00E+00	6.70E+02
5.30E+01	7.42E+00	2.00E+00	1.00E+00	6.70E+02
5.40E+01	1.00E+01	2.00E+00	1.00E+00	6.70E+02
5.50E+01	5.98E+00	2.00E+00	1.00E+00	6.70E+02
5.60E+01	6.33E+00	2.00E+00	1.00E+00	6.70E+02
5.70E+01	7.44E+00	2.00E+00	1.00E+00	6.70E+02
5.80E+01	9.19E+00	2.00E+00	1.00E+00	6.70E+02
5.90E+01	1.20E+01	2.00E+00	1.00E+00	6.70E+02
1.00E+00	8.04E+00	1.00E+00	2.00E+00	6.70E+02
2.00E+00	8.16E+00	1.00E+00	2.00E+00	6.70E+02
3.00E+00	5.32E+00	1.00E+00	2.00E+00	6.70E+02
4.00E+00	5.52E+00	1.00E+00	2.00E+00	6.70E+02
5.00E+00	9.31E+00	1.00E+00	2.00E+00	6.70E+02
6.00E+00	7.29E+00	1.00E+00	2.00E+00	6.70E+02
7.00E+00	6.78E+00	1.00E+00	2.00E+00	6.70E+02
8.00E+00	1.06E+01	1.00E+00	2.00E+00	6.70E+02
9.00E+00	9.86E+00	1.00E+00	2.00E+00	6.70E+02
1.00E+01	8.38E+00	1.00E+00	2.00E+00	6.70E+02
1.10E+01	6.56E+00	1.00E+00	2.00E+00	6.70E+02
1.20E+01	9.62E+00	1.00E+00	2.00E+00	6.70E+02
1.30E+01	8.82E+00	1.00E+00	2.00E+00	6.70E+02
1.40E+01	5.31E+00	1.00E+00	2.00E+00	6.70E+02
1.50E+01	8.87E+00	1.00E+00	2.00E+00	6.70E+02
1.60E+01	5.63E+00	1.00E+00	2.00E+00	6.70E+02
1.70E+01	5.24E+00	1.00E+00	2.00E+00	6.70E+02
1.80E+01	7.47E+00	1.00E+00	2.00E+00	6.70E+02
1.90E+01	8.18E+00	1.00E+00	2.00E+00	6.70E+02
2.00E+01	1.10E+01	1.00E+00	2.00E+00	6.70E+02
2.10E+01	8.62E+00	1.00E+00	2.00E+00	6.70E+02
2.20E+01	7.77E+00	1.00E+00	2.00E+00	6.70E+02
2.30E+01	8.69E+00	1.00E+00	2.00E+00	6.70E+02
2.40E+01	1.04E+01	1.00E+00	2.00E+00	6.70E+02
2.50E+01	1.00E+01	1.00E+00	2.00E+00	6.70E+02
2.60E+01	6.32E+00	1.00E+00	2.00E+00	6.70E+02
2.70E+01	1.19E+01	1.00E+00	2.00E+00	6.70E+02
2.80E+01	5.28E+00	1.00E+00	2.00E+00	6.70E+02
2.90E+01	5.74E+00	1.00E+00	2.00E+00	6.70E+02
3.00E+01	6.04E+00	2.00E+00	2.00E+00	6.70E+02
3.10E+01	3.65E+00	2.00E+00	2.00E+00	6.70E+02
3.20E+01	8.11E+00	2.00E+00	2.00E+00	6.70E+02
3.30E+01	7.66E+00	2.00E+00	2.00E+00	6.70E+02
3.40E+01	9.52E+00	2.00E+00	2.00E+00	6.70E+02
3.50E+01	6.51E+00	2.00E+00	2.00E+00	6.70E+02
3.60E+01	7.85E+00	2.00E+00	2.00E+00	6.70E+02
3.70E+01	4.52E+00	2.00E+00	2.00E+00	6.70E+02
3.80E+01	6.76E+00	2.00E+00	2.00E+00	6.70E+02
3.90E+01	9.42E+00	2.00E+00	2.00E+00	6.70E+02
4.00E+01	6.28E+00	2.00E+00	2.00E+00	6.70E+02
4.10E+01	8.33E+00	2.00E+00	2.00E+00	6.70E+02
4.20E+01	7.88E+00	2.00E+00	2.00E+00	6.70E+02
4.30E+01	8.49E+00	2.00E+00	2.00E+00	6.70E+02
4.40E+01	6.47E+00	2.00E+00	2.00E+00	6.70E+02
4.50E+01	7.78E+00	2.00E+00	2.00E+00	6.70E+02
4.60E+01	5.44E+00	2.00E+00	2.00E+00	6.70E+02
4.70E+01	4.96E+00	2.00E+00	2.00E+00	6.70E+02
4.80E+01	7.71E+00	2.00E+00	2.00E+00	6.70E+02
4.90E+01	6.47E+00	2.00E+00	2.00E+00	6.70E+02
5.00E+01	7.71E+00	2.00E+00	2.00E+00	6.70E+02
5.10E+01	7.99E+00	2.00E+00	2.00E+00	6.70E+02
5.20E+01	7.99E+00	2.00E+00	2.00E+00	6.70E+02
5.30E+01	6.48E+00	2.00E+00	2.00E+00	6.70E+02
5.40E+01	9.64E+00	2.00E+00	2.00E+00	6.70E+02
5.50E+01	6.79E+00	2.00E+00	2.00E+00	6.70E+02
5.60E+01	3.61E+00	2.00E+00	2.00E+00	6.70E+02
5.70E+01	3.11E+00	2.00E+00	2.00E+00	6.70E+02
5.80E+01	8.65E+00	2.00E+00	2.00E+00	6.70E+02
5.90E+01	8.35E+00	2.00E+00	2.00E+00	6.70E+02
1.00E+00	7.22E+00	1.00E+00	1.00E+00	7.29E+02
2.00E+00	5.53E+00	1.00E+00	1.00E+00	7.29E+02
3.00E+00	4.03E+00	1.00E+00	1.00E+00	7.29E+02
4.00E+00	7.91E+00	1.00E+00	1.00E+00	7.29E+02
5.00E+00	1.18E+01	1.00E+00	1.00E+00	7.29E+02
6.00E+00	8.68E+00	1.00E+00	1.00E+00	7.29E+02
7.00E+00	7.40E+00	1.00E+00	1.00E+00	7.29E+02
8.00E+00	7.10E+00	1.00E+00	1.00E+00	7.29E+02
9.00E+00	7.83E+00	1.00E+00	1.00E+00	7.29E+02
1.00E+01	5.22E+00	1.00E+00	1.00E+00	7.29E+02
1.10E+01	6.63E+00	1.00E+00	1.00E+00	7.29E+02
1.20E+01	6.37E+00	1.00E+00	1.00E+00	7.29E+02
1.30E+01	8.54E+00	1.00E+00	1.00E+00	7.29E+02
1.40E+01	6.00E+00	1.00E+00	1.00E+00	7.29E+02
1.50E+01	6.68E+00	1.00E+00	1.00E+00	7.29E+02
1.60E+01	8.76E+00	1.00E+00	1.00E+00	7.29E+02
1.70E+01	5.30E+00	1.00E+00	1.00E+00	7.29E+02
1.80E+01	1.09E+01	1.00E+00	1.00E+00	7.29E+02
1.90E+01	9.55E+00	1.00E+00	1.00E+00	7.29E+02
2.00E+01	8.00E+00	1.00E+00	1.00E+00	7.29E+02
2.10E+01	9.27E+00	1.00E+00	1.00E+00	7.29E+02
2.20E+01	5.53E+00	1.00E+00	1.00E+00	7.29E+02
2.30E+01	9.04E+00	1.00E+00	1.00E+00	7.29E+02
2.40E+01	6.99E+00	1.00E+00	1.00E+00	7.29E+02
2.50E+01	6.28E+00	1.00E+00	1.00E+00	7.29E+02
2.60E+01	9.16E+00	1.00E+00	1.00E+00	7.29E+02
2.70E+01	8.04E+00	1.00E+00	1.00E+00	7.29E+02
2.80E+01	5.94E+00	1.00E+00	1.00E+00	7.29E+02
2.90E+01	6.94E+00	1.00E+00	1.00E+00	7.29E+02
3.00E+01	6.25E+00	2.00E+00	1.00E+00	7.29E+02
3.10E+01	4.43E+00	2.00E+00	1.00E+00	7.29E+02
3.20E+01	5.72E+00	2.00E+00	1.00E+00	7.29E+02
3.30E+01	8.43E+00	2.00E+00	1.00E+00	7.29E+02
3.40E+01	8.34E+00	2.00E+00	1.00E+00	7.29E+02
3.50E+01	6.25E+00	2.00E+00	1.00E+00	7.29E+02
3.60E+01	6.96E+00	2.00E+00	1.00E+00	7.29E+02
3.70E+01	3.25E+00	2.00E+00	1.00E+00	7.29E+02
3.80E+01	6.21E+00	2.00E+00	1.00E+00	7.29E+02
3.90E+01	7.89E+00	2.00E+00	1.00E+00	7.29E+02
4.00E+01	6.44E+00	2.00E+00	1.00E+00	7.29E+02
4.10E+01	3.07E+00	2.00E+00	1.00E+00	7.29E+02
4.20E+01	8.60E+00	2.00E+00	1.00E+00	7.29E+02
4.30E+01	5.91E+00	2.00E+00	1.00E+00	7.29E+02
4.40E+01	6.88E+00	2.00E+00	1.00E+00	7.29E+02
4.50E+01	8.56E+00	2.00E+00	1.00E+00	7.29E+02
4.60E+01	6.95E+00	2.00E+00	1.00E+00	7.29E+02
4.70E+01	7.49E+00	2.00E+00	1.00E+00	7.29E+02
4.80E+01	6.69E+00	2.00E+00	1.00E+00	7.29E+02
4.90E+01	5.30E+00	2.00E+00	1.00E+00	7.29E+02
5.00E+01	9.29E+00	2.00E+00	1.00E+00	7.29E+02
5.10E+01	5.78E+00	2.00E+00	1.00E+00	7.29E+02
5.20E+01	5.78E+00	2.00E+00	1.00E+00	7.29E+02
5.30E+01	5.97E+00	2.00E+00	1.00E+00	7.29E+02
5.40E+01	7.76E+00	2.00E+00	1.00E+00	7.29E+02
5.50E+01	5.50E+00	2.00E+00	1.00E+00	7.29E+02
5.60E+01	6.97E+00	2.00E+00	1.00E+00	7.29E+02
5.70E+01	6.07E+00	2.00E+00	1.00E+00	7.29E+02
5.80E+01	4.87E+00	2.00E+00	1.00E+00	7.29E+02
5.90E+01	5.30E+00	2.00E+00	1.00E+00	7.29E+02
1.00E+00	6.57E+00	1.00E+00	2.00E+00	7.29E+02
2.00E+00	8.75E+00	1.00E+00	2.00E+00	7.29E+02
3.00E+00	6.04E+00	1.00E+00	2.00E+00	7.29E+02
4.00E+00	5.83E+00	1.00E+00	2.00E+00	7.29E+02
5.00E+00	7.86E+00	1.00E+00	2.00E+00	7.29E+02
6.00E+00	8.62E+00	1.00E+00	2.00E+00	7.29E+02
7.00E+00	7.79E+00	1.00E+00	2.00E+00	7.29E+02
8.00E+00	1.04E+01	1.00E+00	2.00E+00	7.29E+02
9.00E+00	8.89E+00	1.00E+00	2.00E+00	7.29E+02
1.00E+01	1.04E+01	1.00E+00	2.00E+00	7.29E+02
1.10E+01	7.19E+00	1.00E+00	2.00E+00	7.29E+02
1.20E+01	7.49E+00	1.00E+00	2.00E+00	7.29E+02
1.30E+01	7.92E+00	1.00E+00	2.00E+00	7.29E+02
1.40E+01	5.76E+00	1.00E+00	2.00E+00	7.29E+02
1.50E+01	5.68E+00	1.00E+00	2.00E+00	7.29E+02
1.60E+01	6.96E+00	1.00E+00	2.00E+00	7.29E+02
1.70E+01	6.08E+00	1.00E+00	2.00E+00	7.29E+02
1.80E+01	1.09E+01	1.00E+00	2.00E+00	7.29E+02
1.90E+01	1.02E+01	1.00E+00	2.00E+00	7.29E+02
2.00E+01	7.82E+00	1.00E+00	2.00E+00	7.29E+02
2.10E+01	8.71E+00	1.00E+00	2.00E+00	7.29E+02
2.20E+01	9.94E+00	1.00E+00	2.00E+00	7.29E+02
2.30E+01	8.41E+00	1.00E+00	2.00E+00	7.29E+02
2.40E+01	5.91E+00	1.00E+00	2.00E+00	7.29E+02
2.50E+01	9.16E+00	1.00E+00	2.00E+00	7.29E+02
2.60E+01	7.00E+00	1.00E+00	2.00E+00	7.29E+02
2.70E+01	9.53E+00	1.00E+00	2.00E+00	7.29E+02
2.80E+01	6.55E+00	1.00E+00	2.00E+00	7.29E+02
2.90E+01	7.26E+00	1.00E+00	2.00E+00	7.29E+02
3.00E+01	4.89E+00	2.00E+00	2.00E+00	7.29E+02
3.10E+01	4.42E+00	2.00E+00	2.00E+00	7.29E+02
3.20E+01	5.81E+00	2.00E+00	2.00E+00	7.29E+02
3.30E+01	5.84E+00	2.00E+00	2.00E+00	7.29E+02
3.40E+01	6.57E+00	2.00E+00	2.00E+00	7.29E+02
3.50E+01	7.84E+00	2.00E+00	2.00E+00	7.29E+02
3.60E+01	4.16E+00	2.00E+00	2.00E+00	7.29E+02
3.70E+01	4.14E+00	2.00E+00	2.00E+00	7.29E+02
3.80E+01	5.57E+00	2.00E+00	2.00E+00	7.29E+02
3.90E+01	6.61E+00	2.00E+00	2.00E+00	7.29E+02
4.00E+01	8.42E+00	2.00E+00	2.00E+00	7.29E+02
4.10E+01	5.92E+00	2.00E+00	2.00E+00	7.29E+02
4.20E+01	7.55E+00	2.00E+00	2.00E+00	7.29E+02
4.30E+01	9.84E+00	2.00E+00	2.00E+00	7.29E+02
4.40E+01	6.59E+00	2.00E+00	2.00E+00	7.29E+02
4.50E+01	6.91E+00	2.00E+00	2.00E+00	7.29E+02
4.60E+01	5.12E+00	2.00E+00	2.00E+00	7.29E+02
4.70E+01	5.88E+00	2.00E+00	2.00E+00	7.29E+02
4.80E+01	9.53E+00	2.00E+00	2.00E+00	7.29E+02
4.90E+01	4.43E+00	2.00E+00	2.00E+00	7.29E+02
5.00E+01	6.79E+00	2.00E+00	2.00E+00	7.29E+02
5.10E+01	4.98E+00	2.00E+00	2.00E+00	7.29E+02
5.20E+01	4.98E+00	2.00E+00	2.00E+00	7.29E+02
5.30E+01	6.95E+00	2.00E+00	2.00E+00	7.29E+02
5.40E+01	3.77E+00	2.00E+00	2.00E+00	7.29E+02
5.50E+01	9.35E+00	2.00E+00	2.00E+00	7.29E+02
5.60E+01	5.00E+00	2.00E+00	2.00E+00	7.29E+02
5.70E+01	4.05E+00	2.00E+00	2.00E+00	7.29E+02
5.80E+01	5.84E+00	2.00E+00	2.00E+00	7.29E+02
5.90E+01	8.31E+00	2.00E+00	2.00E+00	7.29E+02
1.00E+00	5.08E+00	1.00E+00	1.00E+00	7.88E+02
2.00E+00	8.64E+00	1.00E+00	1.00E+00	7.88E+02
3.00E+00	4.96E+00	1.00E+00	1.00E+00	7.88E+02
4.00E+00	3.89E+00	1.00E+00	1.00E+00	7.88E+02
5.00E+00	6.48E+00	1.00E+00	1.00E+00	7.88E+02
6.00E+00	5.32E+00	1.00E+00	1.00E+00	7.88E+02
7.00E+00	5.09E+00	1.00E+00	1.00E+00	7.88E+02
8.00E+00	7.23E+00	1.00E+00	1.00E+00	7.88E+02
9.00E+00	7.79E+00	1.00E+00	1.00E+00	7.88E+02
1.00E+01	6.46E+00	1.00E+00	1.00E+00	7.88E+02
1.10E+01	8.46E+00	1.00E+00	1.00E+00	7.88E+02
1.20E+01	4.06E+00	1.00E+00	1.00E+00	7.88E+02
1.30E+01	5.67E+00	1.00E+00	1.00E+00	7.88E+02
1.40E+01	3.34E+00	1.00E+00	1.00E+00	7.88E+02
1.50E+01	6.82E+00	1.00E+00	1.00E+00	7.88E+02
1.60E+01	1.06E+01	1.00E+00	1.00E+00	7.88E+02
1.70E+01	5.38E+00	1.00E+00	1.00E+00	7.88E+02
1.80E+01	1.13E+01	1.00E+00	1.00E+00	7.88E+02
1.90E+01	6.47E+00	1.00E+00	1.00E+00	7.88E+02
2.00E+01	7.72E+00	1.00E+00	1.00E+00	7.88E+02
2.10E+01	6.81E+00	1.00E+00	1.00E+00	7.88E+02
2.20E+01	6.59E+00	1.00E+00	1.00E+00	7.88E+02
2.30E+01	9.83E+00	1.00E+00	1.00E+00	7.88E+02
2.40E+01	5.60E+00	1.00E+00	1.00E+00	7.88E+02
2.50E+01	7.93E+00	1.00E+00	1.00E+00	7.88E+02
2.60E+01	5.14E+00	1.00E+00	1.00E+00	7.88E+02
2.70E+01	8.18E+00	1.00E+00	1.00E+00	7.88E+02
2.80E+01	4.96E+00	1.00E+00	1.00E+00	7.88E+02
2.90E+01	4.24E+00	1.00E+00	1.00E+00	7.88E+02
3.00E+01	4.74E+00	2.00E+00	1.00E+00	7.88E+02
3.10E+01	4.32E+00	2.00E+00	1.00E+00	7.88E+02
3.20E+01	6.85E+00	2.00E+00	1.00E+00	7.88E+02
3.30E+01	5.58E+00	2.00E+00	1.00E+00	7.88E+02
3.40E+01	7.83E+00	2.00E+00	1.00E+00	7.88E+02
3.50E+01	6.69E+00	2.00E+00	1.00E+00	7.88E+02
3.60E+01	7.07E+00	2.00E+00	1.00E+00	7.88E+02
3.70E+01	5.36E+00	2.00E+00	1.00E+00	7.88E+02
3.80E+01	5.68E+00	2.00E+00	1.00E+00	7.88E+02
3.90E+01	6.22E+00	2.00E+00	1.00E+00	7.88E+02
4.00E+01	7.20E+00	2.00E+00	1.00E+00	7.88E+02
4.10E+01	5.79E+00	2.00E+00	1.00E+00	7.88E+02
4.20E+01	6.05E+00	2.00E+00	1.00E+00	7.88E+02
4.30E+01	5.69E+00	2.00E+00	1.00E+00	7.88E+02
4.40E+01	6.28E+00	2.00E+00	1.00E+00	7.88E+02
4.50E+01	8.40E+00	2.00E+00	1.00E+00	7.88E+02
4.60E+01	6.97E+00	2.00E+00	1.00E+00	7.88E+02
4.70E+01	5.94E+00	2.00E+00	1.00E+00	7.88E+02
4.80E+01	7.95E+00	2.00E+00	1.00E+00	7.88E+02
4.90E+01	4.61E+00	2.00E+00	1.00E+00	7.88E+02
5.00E+01	6.18E+00	2.00E+00	1.00E+00	7.88E+02
5.10E+01	4.25E+00	2.00E+00	1.00E+00	7.88E+02
5.20E+01	4.25E+00	2.00E+00	1.00E+00	7.88E+02
5.30E+01	5.86E+00	2.00E+00	1.00E+00	7.88E+02
5.40E+01	8.72E+00	2.00E+00	1.00E+00	7.88E+02
5.50E+01	6.14E+00	2.00E+00	1.00E+00	7.88E+02
5.60E+01	6.53E+00	2.00E+00	1.00E+00	7.88E+02
5.70E+01	2.24E+00	2.00E+00	1.00E+00	7.88E+02
5.80E+01	6.66E+00	2.00E+00	1.00E+00	7.88E+02
5.90E+01	6.84E+00	2.00E+00	1.00E+00	7.88E+02
1.00E+00	5.41E+00	1.00E+00	2.00E+00	7.88E+02
2.00E+00	8.48E+00	1.00E+00	2.00E+00	7.88E+02
3.00E+00	6.05E+00	1.00E+00	2.00E+00	7.88E+02
4.00E+00	6.14E+00	1.00E+00	2.00E+00	7.88E+02
5.00E+00	1.04E+01	1.00E+00	2.00E+00	7.88E+02
6.00E+00	7.63E+00	1.00E+00	2.00E+00	7.88E+02
7.00E+00	9.46E+00	1.00E+00	2.00E+00	7.88E+02
8.00E+00	6.38E+00	1.00E+00	2.00E+00	7.88E+02
9.00E+00	7.79E+00	1.00E+00	2.00E+00	7.88E+02
1.00E+01	8.63E+00	1.00E+00	2.00E+00	7.88E+02
1.10E+01	7.28E+00	1.00E+00	2.00E+00	7.88E+02
1.20E+01	8.82E+00	1.00E+00	2.00E+00	7.88E+02
1.30E+01	8.05E+00	1.00E+00	2.00E+00	7.88E+02
1.40E+01	5.02E+00	1.00E+00	2.00E+00	7.88E+02
1.50E+01	7.85E+00	1.00E+00	2.00E+00	7.88E+02
1.60E+01	5.10E+00	1.00E+00	2.00E+00	7.88E+02
1.70E+01	7.75E+00	1.00E+00	2.00E+00	7.88E+02
1.80E+01	6.91E+00	1.00E+00	2.00E+00	7.88E+02
1.90E+01	7.67E+00	1.00E+00	2.00E+00	7.88E+02
2.00E+01	9.90E+00	1.00E+00	2.00E+00	7.88E+02
2.10E+01	8.24E+00	1.00E+00	2.00E+00	7.88E+02
2.20E+01	5.94E+00	1.00E+00	2.00E+00	7.88E+02
2.30E+01	9.36E+00	1.00E+00	2.00E+00	7.88E+02
2.40E+01	6.28E+00	1.00E+00	2.00E+00	7.88E+02
2.50E+01	8.34E+00	1.00E+00	2.00E+00	7.88E+02
2.60E+01	6.88E+00	1.00E+00	2.00E+00	7.88E+02
2.70E+01	9.92E+00	1.00E+00	2.00E+00	7.88E+02
2.80E+01	6.29E+00	1.00E+00	2.00E+00	7.88E+02
2.90E+01	6.33E+00	1.00E+00	2.00E+00	7.88E+02
3.00E+01	5.08E+00	2.00E+00	2.00E+00	7.88E+02
3.10E+01	4.54E+00	2.00E+00	2.00E+00	7.88E+02
3.20E+01	7.89E+00	2.00E+00	2.00E+00	7.88E+02
3.30E+01	6.86E+00	2.00E+00	2.00E+00	7.88E+02
3.40E+01	5.96E+00	2.00E+00	2.00E+00	7.88E+02
3.50E+01	9.23E+00	2.00E+00	2.00E+00	7.88E+02
3.60E+01	6.85E+00	2.00E+00	2.00E+00	7.88E+02
3.70E+01	4.16E+00	2.00E+00	2.00E+00	7.88E+02
3.80E+01	6.91E+00	2.00E+00	2.00E+00	7.88E+02
3.90E+01	8.09E+00	2.00E+00	2.00E+00	7.88E+02
4.00E+01	6.27E+00	2.00E+00	2.00E+00	7.88E+02
4.10E+01	8.66E+00	2.00E+00	2.00E+00	7.88E+02
4.20E+01	7.89E+00	2.00E+00	2.00E+00	7.88E+02
4.30E+01	8.16E+00	2.00E+00	2.00E+00	7.88E+02
4.40E+01	7.67E+00	2.00E+00	2.00E+00	7.88E+02
4.50E+01	9.87E+00	2.00E+00	2.00E+00	7.88E+02
4.60E+01	6.04E+00	2.00E+00	2.00E+00	7.88E+02
4.70E+01	6.81E+00	2.00E+00	2.00E+00	7.88E+02
4.80E+01	9.35E+00	2.00E+00	2.00E+00	7.88E+02
4.90E+01	4.99E+00	2.00E+00	2.00E+00	7.88E+02
5.00E+01	7.07E+00	2.00E+00	2.00E+00	7.88E+02
5.10E+01	6.77E+00	2.00E+00	2.00E+00	7.88E+02
5.20E+01	6.77E+00	2.00E+00	2.00E+00	7.88E+02
5.30E+01	7.73E+00	2.00E+00	2.00E+00	7.88E+02
5.40E+01	9.31E+00	2.00E+00	2.00E+00	7.88E+02
5.50E+01	7.10E+00	2.00E+00	2.00E+00	7.88E+02
5.60E+01	6.33E+00	2.00E+00	2.00E+00	7.88E+02
5.70E+01	6.29E+00	2.00E+00	2.00E+00	7.88E+02
5.80E+01	5.94E+00	2.00E+00	2.00E+00	7.88E+02
5.90E+01	8.94E+00	2.00E+00	2.00E+00	7.88E+02
1.00E+00	7.83E+00	1.00E+00	1.00E+00	8.46E+02
2.00E+00	6.73E+00	1.00E+00	1.00E+00	8.46E+02
3.00E+00	3.53E+00	1.00E+00	1.00E+00	8.46E+02
4.00E+00	4.69E+00	1.00E+00	1.00E+00	8.46E+02
5.00E+00	6.56E+00	1.00E+00	1.00E+00	8.46E+02
6.00E+00	6.81E+00	1.00E+00	1.00E+00	8.46E+02
7.00E+00	6.26E+00	1.00E+00	1.00E+00	8.46E+02
8.00E+00	5.89E+00	1.00E+00	1.00E+00	8.46E+02
9.00E+00	8.23E+00	1.00E+00	1.00E+00	8.46E+02
1.00E+01	5.09E+00	1.00E+00	1.00E+00	8.46E+02
1.10E+01	5.01E+00	1.00E+00	1.00E+00	8.46E+02
1.20E+01	5.13E+00	1.00E+00	1.00E+00	8.46E+02
1.30E+01	6.58E+00	1.00E+00	1.00E+00	8.46E+02
1.40E+01	2.87E+00	1.00E+00	1.00E+00	8.46E+02
1.50E+01	4.12E+00	1.00E+00	1.00E+00	8.46E+02
1.60E+01	7.49E+00	1.00E+00	1.00E+00	8.46E+02
1.70E+01	4.15E+00	1.00E+00	1.00E+00	8.46E+02
1.80E+01	1.27E+01	1.00E+00	1.00E+00	8.46E+02
1.90E+01	7.35E+00	1.00E+00	1.00E+00	8.46E+02
2.00E+01	7.87E+00	1.00E+00	1.00E+00	8.46E+02
2.10E+01	6.45E+00	1.00E+00	1.00E+00	8.46E+02
2.20E+01	6.24E+00	1.00E+00	1.00E+00	8.46E+02
2.30E+01	4.72E+00	1.00E+00	1.00E+00	8.46E+02
2.40E+01	3.89E+00	1.00E+00	1.00E+00	8.46E+02
2.50E+01	7.74E+00	1.00E+00	1.00E+00	8.46E+02
2.60E+01	5.74E+00	1.00E+00	1.00E+00	8.46E+02
2.70E+01	8.57E+00	1.00E+00	1.00E+00	8.46E+02
2.80E+01	4.20E+00	1.00E+00	1.00E+00	8.46E+02
2.90E+01	3.66E+00	1.00E+00	1.00E+00	8.46E+02
3.00E+01	5.48E+00	2.00E+00	1.00E+00	8.46E+02
3.10E+01	4.03E+00	2.00E+00	1.00E+00	8.46E+02
3.20E+01	3.65E+00	2.00E+00	1.00E+00	8.46E+02
3.30E+01	5.94E+00	2.00E+00	1.00E+00	8.46E+02
3.40E+01	6.42E+00	2.00E+00	1.00E+00	8.46E+02
3.50E+01	6.36E+00	2.00E+00	1.00E+00	8.46E+02
3.60E+01	4.18E+00	2.00E+00	1.00E+00	8.46E+02
3.70E+01	4.82E+00	2.00E+00	1.00E+00	8.46E+02
3.80E+01	4.50E+00	2.00E+00	1.00E+00	8.46E+02
3.90E+01	5.27E+00	2.00E+00	1.00E+00	8.46E+02
4.00E+01	6.58E+00	2.00E+00	1.00E+00	8.46E+02
4.10E+01	5.44E+00	2.00E+00	1.00E+00	8.46E+02
4.20E+01	6.10E+00	2.00E+00	1.00E+00	8.46E+02
4.30E+01	6.24E+00	2.00E+00	1.00E+00	8.46E+02
4.40E+01	5.57E+00	2.00E+00	1.00E+00	8.46E+02
4.50E+01	7.40E+00	2.00E+00	1.00E+00	8.46E+02
4.60E+01	4.66E+00	2.00E+00	1.00E+00	8.46E+02
4.70E+01	4.51E+00	2.00E+00	1.00E+00	8.46E+02
4.80E+01	7.21E+00	2.00E+00	1.00E+00	8.46E+02
4.90E+01	7.06E+00	2.00E+00	1.00E+00	8.46E+02
5.00E+01	6.31E+00	2.00E+00	1.00E+00	8.46E+02
5.10E+01	8.07E+00	2.00E+00	1.00E+00	8.46E+02
5.20E+01	8.07E+00	2.00E+00	1.00E+00	8.46E+02
5.30E+01	5.89E+00	2.00E+00	1.00E+00	8.46E+02
5.40E+01	6.48E+00	2.00E+00	1.00E+00	8.46E+02
5.50E+01	5.65E+00	2.00E+00	1.00E+00	8.46E+02
5.60E+01	4.15E+00	2.00E+00	1.00E+00	8.46E+02
5.70E+01	3.34E+00	2.00E+00	1.00E+00	8.46E+02
5.80E+01	5.92E+00	2.00E+00	1.00E+00	8.46E+02
5.90E+01	3.56E+00	2.00E+00	1.00E+00	8.46E+02
1.00E+00	6.41E+00	1.00E+00	2.00E+00	8.46E+02
2.00E+00	9.83E+00	1.00E+00	2.00E+00	8.46E+02
3.00E+00	6.42E+00	1.00E+00	2.00E+00	8.46E+02
4.00E+00	6.91E+00	1.00E+00	2.00E+00	8.46E+02
5.00E+00	8.08E+00	1.00E+00	2.00E+00	8.46E+02
6.00E+00	7.02E+00	1.00E+00	2.00E+00	8.46E+02
7.00E+00	9.19E+00	1.00E+00	2.00E+00	8.46E+02
8.00E+00	7.68E+00	1.00E+00	2.00E+00	8.46E+02
9.00E+00	8.38E+00	1.00E+00	2.00E+00	8.46E+02
1.00E+01	8.14E+00	1.00E+00	2.00E+00	8.46E+02
1.10E+01	8.24E+00	1.00E+00	2.00E+00	8.46E+02
1.20E+01	7.26E+00	1.00E+00	2.00E+00	8.46E+02
1.30E+01	5.97E+00	1.00E+00	2.00E+00	8.46E+02
1.40E+01	4.91E+00	1.00E+00	2.00E+00	8.46E+02
1.50E+01	7.97E+00	1.00E+00	2.00E+00	8.46E+02
1.60E+01	6.53E+00	1.00E+00	2.00E+00	8.46E+02
1.70E+01	7.75E+00	1.00E+00	2.00E+00	8.46E+02
1.80E+01	1.18E+01	1.00E+00	2.00E+00	8.46E+02
1.90E+01	1.08E+01	1.00E+00	2.00E+00	8.46E+02
2.00E+01	8.47E+00	1.00E+00	2.00E+00	8.46E+02
2.10E+01	9.75E+00	1.00E+00	2.00E+00	8.46E+02
2.20E+01	8.50E+00	1.00E+00	2.00E+00	8.46E+02
2.30E+01	8.10E+00	1.00E+00	2.00E+00	8.46E+02
2.40E+01	6.27E+00	1.00E+00	2.00E+00	8.46E+02
2.50E+01	1.08E+01	1.00E+00	2.00E+00	8.46E+02
2.60E+01	7.12E+00	1.00E+00	2.00E+00	8.46E+02
2.70E+01	7.94E+00	1.00E+00	2.00E+00	8.46E+02
2.80E+01	7.08E+00	1.00E+00	2.00E+00	8.46E+02
2.90E+01	7.30E+00	1.00E+00	2.00E+00	8.46E+02
3.00E+01	6.28E+00	2.00E+00	2.00E+00	8.46E+02
3.10E+01	3.49E+00	2.00E+00	2.00E+00	8.46E+02
3.20E+01	6.99E+00	2.00E+00	2.00E+00	8.46E+02
3.30E+01	9.40E+00	2.00E+00	2.00E+00	8.46E+02
3.40E+01	9.99E+00	2.00E+00	2.00E+00	8.46E+02
3.50E+01	8.09E+00	2.00E+00	2.00E+00	8.46E+02
3.60E+01	8.09E+00	2.00E+00	2.00E+00	8.46E+02
3.70E+01	6.20E+00	2.00E+00	2.00E+00	8.46E+02
3.80E+01	7.10E+00	2.00E+00	2.00E+00	8.46E+02
3.90E+01	7.57E+00	2.00E+00	2.00E+00	8.46E+02
4.00E+01	6.00E+00	2.00E+00	2.00E+00	8.46E+02
4.10E+01	6.22E+00	2.00E+00	2.00E+00	8.46E+02
4.20E+01	9.95E+00	2.00E+00	2.00E+00	8.46E+02
4.30E+01	1.06E+01	2.00E+00	2.00E+00	8.46E+02
4.40E+01	7.62E+00	2.00E+00	2.00E+00	8.46E+02
4.50E+01	7.75E+00	2.00E+00	2.00E+00	8.46E+02
4.60E+01	5.03E+00	2.00E+00	2.00E+00	8.46E+02
4.70E+01	4.51E+00	2.00E+00	2.00E+00	8.46E+02
4.80E+01	7.41E+00	2.00E+00	2.00E+00	8.46E+02
4.90E+01	5.26E+00	2.00E+00	2.00E+00	8.46E+02
5.00E+01	6.42E+00	2.00E+00	2.00E+00	8.46E+02
5.10E+01	8.56E+00	2.00E+00	2.00E+00	8.46E+02
5.20E+01	8.56E+00	2.00E+00	2.00E+00	8.46E+02
5.30E+01	1.01E+01	2.00E+00	2.00E+00	8.46E+02
5.40E+01	8.12E+00	2.00E+00	2.00E+00	8.46E+02
5.50E+01	8.74E+00	2.00E+00	2.00E+00	8.46E+02
5.60E+01	8.08E+00	2.00E+00	2.00E+00	8.46E+02
5.70E+01	6.91E+00	2.00E+00	2.00E+00	8.46E+02
5.80E+01	6.97E+00	2.00E+00	2.00E+00	8.46E+02
5.90E+01	5.35E+00	2.00E+00	2.00E+00	8.46E+02
1.00E+00	5.44E+00	1.00E+00	1.00E+00	9.05E+02
2.00E+00	6.32E+00	1.00E+00	1.00E+00	9.05E+02
3.00E+00	3.82E+00	1.00E+00	1.00E+00	9.05E+02
4.00E+00	4.46E+00	1.00E+00	1.00E+00	9.05E+02
5.00E+00	9.68E+00	1.00E+00	1.00E+00	9.05E+02
6.00E+00	4.98E+00	1.00E+00	1.00E+00	9.05E+02
7.00E+00	5.17E+00	1.00E+00	1.00E+00	9.05E+02
8.00E+00	5.03E+00	1.00E+00	1.00E+00	9.05E+02
9.00E+00	7.65E+00	1.00E+00	1.00E+00	9.05E+02
1.00E+01	4.45E+00	1.00E+00	1.00E+00	9.05E+02
1.10E+01	4.86E+00	1.00E+00	1.00E+00	9.05E+02
1.20E+01	4.15E+00	1.00E+00	1.00E+00	9.05E+02
1.30E+01	5.89E+00	1.00E+00	1.00E+00	9.05E+02
1.40E+01	5.38E+00	1.00E+00	1.00E+00	9.05E+02
1.50E+01	6.14E+00	1.00E+00	1.00E+00	9.05E+02
1.60E+01	7.86E+00	1.00E+00	1.00E+00	9.05E+02
1.70E+01	5.60E+00	1.00E+00	1.00E+00	9.05E+02
1.80E+01	1.35E+01	1.00E+00	1.00E+00	9.05E+02
1.90E+01	5.63E+00	1.00E+00	1.00E+00	9.05E+02
2.00E+01	4.80E+00	1.00E+00	1.00E+00	9.05E+02
2.10E+01	5.20E+00	1.00E+00	1.00E+00	9.05E+02
2.20E+01	6.44E+00	1.00E+00	1.00E+00	9.05E+02
2.30E+01	3.70E+00	1.00E+00	1.00E+00	9.05E+02
2.40E+01	4.02E+00	1.00E+00	1.00E+00	9.05E+02
2.50E+01	9.36E+00	1.00E+00	1.00E+00	9.05E+02
2.60E+01	4.38E+00	1.00E+00	1.00E+00	9.05E+02
2.70E+01	8.41E+00	1.00E+00	1.00E+00	9.05E+02
2.80E+01	3.95E+00	1.00E+00	1.00E+00	9.05E+02
2.90E+01	2.67E+00	1.00E+00	1.00E+00	9.05E+02
3.00E+01	3.65E+00	2.00E+00	1.00E+00	9.05E+02
3.10E+01	4.75E+00	2.00E+00	1.00E+00	9.05E+02
3.20E+01	6.77E+00	2.00E+00	1.00E+00	9.05E+02
3.30E+01	2.88E+00	2.00E+00	1.00E+00	9.05E+02
3.40E+01	6.34E+00	2.00E+00	1.00E+00	9.05E+02
3.50E+01	7.62E+00	2.00E+00	1.00E+00	9.05E+02
3.60E+01	4.64E+00	2.00E+00	1.00E+00	9.05E+02
3.70E+01	3.69E+00	2.00E+00	1.00E+00	9.05E+02
3.80E+01	4.84E+00	2.00E+00	1.00E+00	9.05E+02
3.90E+01	3.06E+00	2.00E+00	1.00E+00	9.05E+02
4.00E+01	5.83E+00	2.00E+00	1.00E+00	9.05E+02
4.10E+01	4.63E+00	2.00E+00	1.00E+00	9.05E+02
4.20E+01	5.29E+00	2.00E+00	1.00E+00	9.05E+02
4.30E+01	5.37E+00	2.00E+00	1.00E+00	9.05E+02
4.40E+01	2.23E+00	2.00E+00	1.00E+00	9.05E+02
4.50E+01	5.99E+00	2.00E+00	1.00E+00	9.05E+02
4.60E+01	3.51E+00	2.00E+00	1.00E+00	9.05E+02
4.70E+01	3.19E+00	2.00E+00	1.00E+00	9.05E+02
4.80E+01	6.66E+00	2.00E+00	1.00E+00	9.05E+02
4.90E+01	3.83E+00	2.00E+00	1.00E+00	9.05E+02
5.00E+01	5.10E+00	2.00E+00	1.00E+00	9.05E+02
5.10E+01	7.23E+00	2.00E+00	1.00E+00	9.05E+02
5.20E+01	7.23E+00	2.00E+00	1.00E+00	9.05E+02
5.30E+01	6.54E+00	2.00E+00	1.00E+00	9.05E+02
5.40E+01	9.15E+00	2.00E+00	1.00E+00	9.05E+02
5.50E+01	2.29E+00	2.00E+00	1.00E+00	9.05E+02
5.60E+01	2.47E+00	2.00E+00	1.00E+00	9.05E+02
5.70E+01	3.98E+00	2.00E+00	1.00E+00	9.05E+02
5.80E+01	5.10E+00	2.00E+00	1.00E+00	9.05E+02
5.90E+01	1.34E+01	2.00E+00	1.00E+00	9.05E+02
1.00E+00	6.18E+00	1.00E+00	2.00E+00	9.05E+02
2.00E+00	9.71E+00	1.00E+00	2.00E+00	9.05E+02
3.00E+00	5.50E+00	1.00E+00	2.00E+00	9.05E+02
4.00E+00	4.53E+00	1.00E+00	2.00E+00	9.05E+02
5.00E+00	5.32E+00	1.00E+00	2.00E+00	9.05E+02
6.00E+00	5.96E+00	1.00E+00	2.00E+00	9.05E+02
7.00E+00	7.29E+00	1.00E+00	2.00E+00	9.05E+02
8.00E+00	8.09E+00	1.00E+00	2.00E+00	9.05E+02
9.00E+00	7.56E+00	1.00E+00	2.00E+00	9.05E+02
1.00E+01	4.82E+00	1.00E+00	2.00E+00	9.05E+02
1.10E+01	8.05E+00	1.00E+00	2.00E+00	9.05E+02
1.20E+01	7.19E+00	1.00E+00	2.00E+00	9.05E+02
1.30E+01	4.90E+00	1.00E+00	2.00E+00	9.05E+02
1.40E+01	3.46E+00	1.00E+00	2.00E+00	9.05E+02
1.50E+01	8.75E+00	1.00E+00	2.00E+00	9.05E+02
1.60E+01	6.72E+00	1.00E+00	2.00E+00	9.05E+02
1.70E+01	7.65E+00	1.00E+00	2.00E+00	9.05E+02
1.80E+01	1.03E+01	1.00E+00	2.00E+00	9.05E+02
1.90E+01	9.48E+00	1.00E+00	2.00E+00	9.05E+02
2.00E+01	4.74E+00	1.00E+00	2.00E+00	9.05E+02
2.10E+01	7.47E+00	1.00E+00	2.00E+00	9.05E+02
2.20E+01	6.52E+00	1.00E+00	2.00E+00	9.05E+02
2.30E+01	4.89E+00	1.00E+00	2.00E+00	9.05E+02
2.40E+01	6.04E+00	1.00E+00	2.00E+00	9.05E+02
2.50E+01	4.94E+00	1.00E+00	2.00E+00	9.05E+02
2.60E+01	5.12E+00	1.00E+00	2.00E+00	9.05E+02
2.70E+01	1.19E+01	1.00E+00	2.00E+00	9.05E+02
2.80E+01	7.09E+00	1.00E+00	2.00E+00	9.05E+02
2.90E+01	5.61E+00	1.00E+00	2.00E+00	9.05E+02
3.00E+01	5.97E+00	2.00E+00	2.00E+00	9.05E+02
3.10E+01	5.03E+00	2.00E+00	2.00E+00	9.05E+02
3.20E+01	4.66E+00	2.00E+00	2.00E+00	9.05E+02
3.30E+01	8.91E+00	2.00E+00	2.00E+00	9.05E+02
3.40E+01	8.37E+00	2.00E+00	2.00E+00	9.05E+02
3.50E+01	7.72E+00	2.00E+00	2.00E+00	9.05E+02
3.60E+01	6.06E+00	2.00E+00	2.00E+00	9.05E+02
3.70E+01	5.11E+00	2.00E+00	2.00E+00	9.05E+02
3.80E+01	7.65E+00	2.00E+00	2.00E+00	9.05E+02
3.90E+01	6.99E+00	2.00E+00	2.00E+00	9.05E+02
4.00E+01	6.66E+00	2.00E+00	2.00E+00	9.05E+02
4.10E+01	7.02E+00	2.00E+00	2.00E+00	9.05E+02
4.20E+01	6.99E+00	2.00E+00	2.00E+00	9.05E+02
4.30E+01	7.07E+00	2.00E+00	2.00E+00	9.05E+02
4.40E+01	6.95E+00	2.00E+00	2.00E+00	9.05E+02
4.50E+01	5.81E+00	2.00E+00	2.00E+00	9.05E+02
4.60E+01	6.78E+00	2.00E+00	2.00E+00	9.05E+02
4.70E+01	5.77E+00	2.00E+00	2.00E+00	9.05E+02
4.80E+01	6.42E+00	2.00E+00	2.00E+00	9.05E+02
4.90E+01	6.82E+00	2.00E+00	2.00E+00	9.05E+02
5.00E+01	4.75E+00	2.00E+00	2.00E+00	9.05E+02
5.10E+01	4.96E+00	2.00E+00	2.00E+00	9.05E+02
5.20E+01	4.96E+00	2.00E+00	2.00E+00	9.05E+02
5.30E+01	5.59E+00	2.00E+00	2.00E+00	9.05E+02
5.40E+01	1.04E+01	2.00E+00	2.00E+00	9.05E+02
5.50E+01	6.15E+00	2.00E+00	2.00E+00	9.05E+02
5.60E+01	7.01E+00	2.00E+00	2.00E+00	9.05E+02
5.70E+01	4.94E+00	2.00E+00	2.00E+00	9.05E+02
5.80E+01	4.65E+00	2.00E+00	2.00E+00	9.05E+02
5.90E+01	6.81E+00	2.00E+00	2.00E+00	9.05E+02
1.00E+00	8.85E+00	1.00E+00	1.00E+00	1.08E+03
2.00E+00	8.86E+00	1.00E+00	1.00E+00	1.08E+03
3.00E+00	5.83E+00	1.00E+00	1.00E+00	1.08E+03
4.00E+00	7.44E+00	1.00E+00	1.00E+00	1.08E+03
5.00E+00	6.02E+00	1.00E+00	1.00E+00	1.08E+03
6.00E+00	6.38E+00	1.00E+00	1.00E+00	1.08E+03
7.00E+00	1.03E+01	1.00E+00	1.00E+00	1.08E+03
8.00E+00	8.50E+00	1.00E+00	1.00E+00	1.08E+03
9.00E+00	5.59E+00	1.00E+00	1.00E+00	1.08E+03
1.00E+01	9.36E+00	1.00E+00	1.00E+00	1.08E+03
1.10E+01	9.61E+00	1.00E+00	1.00E+00	1.08E+03
1.20E+01	9.53E+00	1.00E+00	1.00E+00	1.08E+03
1.30E+01	8.12E+00	1.00E+00	1.00E+00	1.08E+03
1.40E+01	7.71E+00	1.00E+00	1.00E+00	1.08E+03
1.50E+01	6.09E+00	1.00E+00	1.00E+00	1.08E+03
1.60E+01	1.03E+01	1.00E+00	1.00E+00	1.08E+03
1.70E+01	8.76E+00	1.00E+00	1.00E+00	1.08E+03
1.80E+01	1.08E+01	1.00E+00	1.00E+00	1.08E+03
1.90E+01	9.26E+00	1.00E+00	1.00E+00	1.08E+03
2.00E+01	8.45E+00	1.00E+00	1.00E+00	1.08E+03
2.10E+01	8.44E+00	1.00E+00	1.00E+00	1.08E+03
2.20E+01	6.34E+00	1.00E+00	1.00E+00	1.08E+03
2.30E+01	6.15E+00	1.00E+00	1.00E+00	1.08E+03
2.40E+01	8.56E+00	1.00E+00	1.00E+00	1.08E+03
2.50E+01	7.64E+00	1.00E+00	1.00E+00	1.08E+03
2.60E+01	9.66E+00	1.00E+00	1.00E+00	1.08E+03
2.70E+01	8.05E+00	1.00E+00	1.00E+00	1.08E+03
2.80E+01	8.12E+00	1.00E+00	1.00E+00	1.08E+03
2.90E+01	8.20E+00	1.00E+00	1.00E+00	1.08E+03
3.00E+01	6.98E+00	2.00E+00	1.00E+00	1.08E+03
3.10E+01	5.95E+00	2.00E+00	1.00E+00	1.08E+03
3.20E+01	8.32E+00	2.00E+00	1.00E+00	1.08E+03
3.30E+01	5.11E+00	2.00E+00	1.00E+00	1.08E+03
3.40E+01	8.59E+00	2.00E+00	1.00E+00	1.08E+03
3.50E+01	8.48E+00	2.00E+00	1.00E+00	1.08E+03
3.60E+01	7.10E+00	2.00E+00	1.00E+00	1.08E+03
3.70E+01	5.62E+00	2.00E+00	1.00E+00	1.08E+03
3.80E+01	7.00E+00	2.00E+00	1.00E+00	1.08E+03
3.90E+01	8.06E+00	2.00E+00	1.00E+00	1.08E+03
4.00E+01	9.13E+00	2.00E+00	1.00E+00	1.08E+03
4.10E+01	9.22E+00	2.00E+00	1.00E+00	1.08E+03
4.20E+01	8.98E+00	2.00E+00	1.00E+00	1.08E+03
4.30E+01	6.50E+00	2.00E+00	1.00E+00	1.08E+03
4.40E+01	5.11E+00	2.00E+00	1.00E+00	1.08E+03
4.50E+01	7.36E+00	2.00E+00	1.00E+00	1.08E+03
4.60E+01	7.18E+00	2.00E+00	1.00E+00	1.08E+03
4.70E+01	8.58E+00	2.00E+00	1.00E+00	1.08E+03
4.80E+01	6.41E+00	2.00E+00	1.00E+00	1.08E+03
4.90E+01	8.77E+00	2.00E+00	1.00E+00	1.08E+03
5.00E+01	8.62E+00	2.00E+00	1.00E+00	1.08E+03
5.10E+01	6.29E+00	2.00E+00	1.00E+00	1.08E+03
5.20E+01	6.29E+00	2.00E+00	1.00E+00	1.08E+03
5.30E+01	7.85E+00	2.00E+00	1.00E+00	1.08E+03
5.40E+01	1.09E+01	2.00E+00	1.00E+00	1.08E+03
5.50E+01	6.47E+00	2.00E+00	1.00E+00	1.08E+03
5.60E+01	6.34E+00	2.00E+00	1.00E+00	1.08E+03
5.70E+01	6.02E+00	2.00E+00	1.00E+00	1.08E+03
5.80E+01	8.03E+00	2.00E+00	1.00E+00	1.08E+03
5.90E+01	8.44E+00	2.00E+00	1.00E+00	1.08E+03
1.00E+00	6.55E+00	1.00E+00	2.00E+00	1.08E+03
2.00E+00	8.47E+00	1.00E+00	2.00E+00	1.08E+03
3.00E+00	3.24E+00	1.00E+00	2.00E+00	1.08E+03
4.00E+00	6.73E+00	1.00E+00	2.00E+00	1.08E+03
5.00E+00	8.33E+00	1.00E+00	2.00E+00	1.08E+03
6.00E+00	5.38E+00	1.00E+00	2.00E+00	1.08E+03
7.00E+00	9.93E+00	1.00E+00	2.00E+00	1.08E+03
8.00E+00	9.35E+00	1.00E+00	2.00E+00	1.08E+03
9.00E+00	9.87E+00	1.00E+00	2.00E+00	1.08E+03
1.00E+01	7.02E+00	1.00E+00	2.00E+00	1.08E+03
1.10E+01	9.27E+00	1.00E+00	2.00E+00	1.08E+03
1.20E+01	3.77E+00	1.00E+00	2.00E+00	1.08E+03
1.30E+01	6.95E+00	1.00E+00	2.00E+00	1.08E+03
1.40E+01	3.52E+00	1.00E+00	2.00E+00	1.08E+03
1.50E+01	6.11E+00	1.00E+00	2.00E+00	1.08E+03
1.60E+01	3.66E+00	1.00E+00	2.00E+00	1.08E+03
1.70E+01	3.54E+00	1.00E+00	2.00E+00	1.08E+03
1.80E+01	1.19E+01	1.00E+00	2.00E+00	1.08E+03
1.90E+01	7.22E+00	1.00E+00	2.00E+00	1.08E+03
2.00E+01	8.11E+00	1.00E+00	2.00E+00	1.08E+03
2.10E+01	9.11E+00	1.00E+00	2.00E+00	1.08E+03
2.20E+01	1.03E+01	1.00E+00	2.00E+00	1.08E+03
2.30E+01	7.29E+00	1.00E+00	2.00E+00	1.08E+03
2.40E+01	8.24E+00	1.00E+00	2.00E+00	1.08E+03
2.50E+01	9.29E+00	1.00E+00	2.00E+00	1.08E+03
2.60E+01	3.08E+00	1.00E+00	2.00E+00	1.08E+03
2.70E+01	8.15E+00	1.00E+00	2.00E+00	1.08E+03
2.80E+01	2.81E+00	1.00E+00	2.00E+00	1.08E+03
2.90E+01	3.83E+00	1.00E+00	2.00E+00	1.08E+03
3.00E+01	6.08E+00	2.00E+00	2.00E+00	1.08E+03
3.10E+01	3.54E+00	2.00E+00	2.00E+00	1.08E+03
3.20E+01	3.29E+00	2.00E+00	2.00E+00	1.08E+03
3.30E+01	3.59E+00	2.00E+00	2.00E+00	1.08E+03
3.40E+01	4.97E+00	2.00E+00	2.00E+00	1.08E+03
3.50E+01	8.13E+00	2.00E+00	2.00E+00	1.08E+03
3.60E+01	4.25E+00	2.00E+00	2.00E+00	1.08E+03
3.70E+01	2.49E+00	2.00E+00	2.00E+00	1.08E+03
3.80E+01	4.60E+00	2.00E+00	2.00E+00	1.08E+03
3.90E+01	7.05E+00	2.00E+00	2.00E+00	1.08E+03
4.00E+01	6.49E+00	2.00E+00	2.00E+00	1.08E+03
4.10E+01	8.05E+00	2.00E+00	2.00E+00	1.08E+03
4.20E+01	3.36E+00	2.00E+00	2.00E+00	1.08E+03
4.30E+01	8.54E+00	2.00E+00	2.00E+00	1.08E+03
4.40E+01	4.40E+00	2.00E+00	2.00E+00	1.08E+03
4.50E+01	8.39E+00	2.00E+00	2.00E+00	1.08E+03
4.60E+01	2.78E+00	2.00E+00	2.00E+00	1.08E+03
4.70E+01	5.00E+00	2.00E+00	2.00E+00	1.08E+03
4.80E+01	9.90E+00	2.00E+00	2.00E+00	1.08E+03
4.90E+01	5.50E+00	2.00E+00	2.00E+00	1.08E+03
5.00E+01	4.47E+00	2.00E+00	2.00E+00	1.08E+03
5.10E+01	6.47E+00	2.00E+00	2.00E+00	1.08E+03
5.20E+01	6.47E+00	2.00E+00	2.00E+00	1.08E+03
5.30E+01	5.83E+00	2.00E+00	2.00E+00	1.08E+03
5.40E+01	1.18E+01	2.00E+00	2.00E+00	1.08E+03
5.50E+01	6.00E+00	2.00E+00	2.00E+00	1.08E+03
5.60E+01	4.09E+00	2.00E+00	2.00E+00	1.08E+03
5.70E+01	3.17E+00	2.00E+00	2.00E+00	1.08E+03
5.80E+01	4.67E+00	2.00E+00	2.00E+00	1.08E+03
5.90E+01	6.79E+00	2.00E+00	2.00E+00	1.08E+03
1.00E+00	5.60E+00	1.00E+00	1.00E+00	1.10E+03
2.00E+00	7.73E+00	1.00E+00	1.00E+00	1.10E+03
3.00E+00	2.71E+00	1.00E+00	1.00E+00	1.10E+03
4.00E+00	7.98E+00	1.00E+00	1.00E+00	1.10E+03
5.00E+00	7.47E+00	1.00E+00	1.00E+00	1.10E+03
6.00E+00	9.71E+00	1.00E+00	1.00E+00	1.10E+03
7.00E+00	8.24E+00	1.00E+00	1.00E+00	1.10E+03
8.00E+00	8.20E+00	1.00E+00	1.00E+00	1.10E+03
9.00E+00	6.47E+00	1.00E+00	1.00E+00	1.10E+03
1.00E+01	9.23E+00	1.00E+00	1.00E+00	1.10E+03
1.10E+01	4.72E+00	1.00E+00	1.00E+00	1.10E+03
1.20E+01	8.27E+00	1.00E+00	1.00E+00	1.10E+03
1.30E+01	7.15E+00	1.00E+00	1.00E+00	1.10E+03
1.40E+01	4.26E+00	1.00E+00	1.00E+00	1.10E+03
1.50E+01	7.30E+00	1.00E+00	1.00E+00	1.10E+03
1.60E+01	7.10E+00	1.00E+00	1.00E+00	1.10E+03
1.70E+01	5.54E+00	1.00E+00	1.00E+00	1.10E+03
1.80E+01	1.30E+01	1.00E+00	1.00E+00	1.10E+03
1.90E+01	9.01E+00	1.00E+00	1.00E+00	1.10E+03
2.00E+01	8.79E+00	1.00E+00	1.00E+00	1.10E+03
2.10E+01	6.30E+00	1.00E+00	1.00E+00	1.10E+03
2.20E+01	8.50E+00	1.00E+00	1.00E+00	1.10E+03
2.30E+01	6.29E+00	1.00E+00	1.00E+00	1.10E+03
2.40E+01	8.51E+00	1.00E+00	1.00E+00	1.10E+03
2.50E+01	6.44E+00	1.00E+00	1.00E+00	1.10E+03
2.60E+01	6.99E+00	1.00E+00	1.00E+00	1.10E+03
2.70E+01	1.06E+01	1.00E+00	1.00E+00	1.10E+03
2.80E+01	5.98E+00	1.00E+00	1.00E+00	1.10E+03
2.90E+01	7.28E+00	1.00E+00	1.00E+00	1.10E+03
3.00E+01	8.18E+00	2.00E+00	1.00E+00	1.10E+03
3.10E+01	5.59E+00	2.00E+00	1.00E+00	1.10E+03
3.20E+01	5.12E+00	2.00E+00	1.00E+00	1.10E+03
3.30E+01	5.13E+00	2.00E+00	1.00E+00	1.10E+03
3.40E+01	9.93E+00	2.00E+00	1.00E+00	1.10E+03
3.50E+01	6.54E+00	2.00E+00	1.00E+00	1.10E+03
3.60E+01	4.93E+00	2.00E+00	1.00E+00	1.10E+03
3.70E+01	6.64E+00	2.00E+00	1.00E+00	1.10E+03
3.80E+01	6.86E+00	2.00E+00	1.00E+00	1.10E+03
3.90E+01	7.08E+00	2.00E+00	1.00E+00	1.10E+03
4.00E+01	9.45E+00	2.00E+00	1.00E+00	1.10E+03
4.10E+01	8.92E+00	2.00E+00	1.00E+00	1.10E+03
4.20E+01	9.81E+00	2.00E+00	1.00E+00	1.10E+03
4.30E+01	9.02E+00	2.00E+00	1.00E+00	1.10E+03
4.40E+01	6.72E+00	2.00E+00	1.00E+00	1.10E+03
4.50E+01	5.79E+00	2.00E+00	1.00E+00	1.10E+03
4.60E+01	7.98E+00	2.00E+00	1.00E+00	1.10E+03
4.70E+01	9.87E+00	2.00E+00	1.00E+00	1.10E+03
4.80E+01	6.64E+00	2.00E+00	1.00E+00	1.10E+03
4.90E+01	6.93E+00	2.00E+00	1.00E+00	1.10E+03
5.00E+01	6.00E+00	2.00E+00	1.00E+00	1.10E+03
5.10E+01	7.67E+00	2.00E+00	1.00E+00	1.10E+03
5.20E+01	7.67E+00	2.00E+00	1.00E+00	1.10E+03
5.30E+01	9.67E+00	2.00E+00	1.00E+00	1.10E+03
5.40E+01	7.50E+00	2.00E+00	1.00E+00	1.10E+03
5.50E+01	8.61E+00	2.00E+00	1.00E+00	1.10E+03
5.60E+01	8.02E+00	2.00E+00	1.00E+00	1.10E+03
5.70E+01	4.68E+00	2.00E+00	1.00E+00	1.10E+03
5.80E+01	7.26E+00	2.00E+00	1.00E+00	1.10E+03
5.90E+01	9.29E+00	2.00E+00	1.00E+00	1.10E+03
1.00E+00	5.92E+00	1.00E+00	2.00E+00	1.10E+03
2.00E+00	7.25E+00	1.00E+00	2.00E+00	1.10E+03
3.00E+00	5.40E+00	1.00E+00	2.00E+00	1.10E+03
4.00E+00	5.02E+00	1.00E+00	2.00E+00	1.10E+03
5.00E+00	9.10E+00	1.00E+00	2.00E+00	1.10E+03
6.00E+00	6.96E+00	1.00E+00	2.00E+00	1.10E+03
7.00E+00	5.10E+00	1.00E+00	2.00E+00	1.10E+03
8.00E+00	6.63E+00	1.00E+00	2.00E+00	1.10E+03
9.00E+00	7.96E+00	1.00E+00	2.00E+00	1.10E+03
1.00E+01	6.51E+00	1.00E+00	2.00E+00	1.10E+03
1.10E+01	7.35E+00	1.00E+00	2.00E+00	1.10E+03
1.20E+01	5.22E+00	1.00E+00	2.00E+00	1.10E+03
1.30E+01	6.01E+00	1.00E+00	2.00E+00	1.10E+03
1.40E+01	4.35E+00	1.00E+00	2.00E+00	1.10E+03
1.50E+01	8.15E+00	1.00E+00	2.00E+00	1.10E+03
1.60E+01	4.77E+00	1.00E+00	2.00E+00	1.10E+03
1.70E+01	4.60E+00	1.00E+00	2.00E+00	1.10E+03
1.80E+01	1.18E+01	1.00E+00	2.00E+00	1.10E+03
1.90E+01	8.96E+00	1.00E+00	2.00E+00	1.10E+03
2.00E+01	9.99E+00	1.00E+00	2.00E+00	1.10E+03
2.10E+01	8.42E+00	1.00E+00	2.00E+00	1.10E+03
2.20E+01	8.50E+00	1.00E+00	2.00E+00	1.10E+03
2.30E+01	7.11E+00	1.00E+00	2.00E+00	1.10E+03
2.40E+01	6.85E+00	1.00E+00	2.00E+00	1.10E+03
2.50E+01	1.06E+01	1.00E+00	2.00E+00	1.10E+03
2.60E+01	2.77E+00	1.00E+00	2.00E+00	1.10E+03
2.70E+01	6.11E+00	1.00E+00	2.00E+00	1.10E+03
2.80E+01	4.17E+00	1.00E+00	2.00E+00	1.10E+03
2.90E+01	6.79E+00	1.00E+00	2.00E+00	1.10E+03
3.00E+01	3.27E+00	2.00E+00	2.00E+00	1.10E+03
3.10E+01	2.11E+00	2.00E+00	2.00E+00	1.10E+03
3.20E+01	4.00E+00	2.00E+00	2.00E+00	1.10E+03
3.30E+01	5.39E+00	2.00E+00	2.00E+00	1.10E+03
3.40E+01	4.97E+00	2.00E+00	2.00E+00	1.10E+03
3.50E+01	6.85E+00	2.00E+00	2.00E+00	1.10E+03
3.60E+01	3.78E+00	2.00E+00	2.00E+00	1.10E+03
3.70E+01	4.81E+00	2.00E+00	2.00E+00	1.10E+03
3.80E+01	4.59E+00	2.00E+00	2.00E+00	1.10E+03
3.90E+01	4.50E+00	2.00E+00	2.00E+00	1.10E+03
4.00E+01	6.91E+00	2.00E+00	2.00E+00	1.10E+03
4.10E+01	6.69E+00	2.00E+00	2.00E+00	1.10E+03
4.20E+01	7.83E+00	2.00E+00	2.00E+00	1.10E+03
4.30E+01	5.20E+00	2.00E+00	2.00E+00	1.10E+03
4.40E+01	5.43E+00	2.00E+00	2.00E+00	1.10E+03
4.50E+01	6.37E+00	2.00E+00	2.00E+00	1.10E+03
4.60E+01	4.94E+00	2.00E+00	2.00E+00	1.10E+03
4.70E+01	4.01E+00	2.00E+00	2.00E+00	1.10E+03
4.80E+01	5.57E+00	2.00E+00	2.00E+00	1.10E+03
4.90E+01	5.84E+00	2.00E+00	2.00E+00	1.10E+03
5.00E+01	5.19E+00	2.00E+00	2.00E+00	1.10E+03
5.10E+01	6.52E+00	2.00E+00	2.00E+00	1.10E+03
5.20E+01	6.52E+00	2.00E+00	2.00E+00	1.10E+03
5.30E+01	5.61E+00	2.00E+00	2.00E+00	1.10E+03
5.40E+01	7.48E+00	2.00E+00	2.00E+00	1.10E+03
5.50E+01	9.16E+00	2.00E+00	2.00E+00	1.10E+03
5.60E+01	4.00E+00	2.00E+00	2.00E+00	1.10E+03
5.70E+01	1.72E+00	2.00E+00	2.00E+00	1.10E+03
5.80E+01	6.84E+00	2.00E+00	2.00E+00	1.10E+03
5.90E+01	7.59E+00	2.00E+00	2.00E+00	1.10E+03
1.00E+00	7.15E+00	1.00E+00	1.00E+00	1.12E+03
2.00E+00	8.13E+00	1.00E+00	1.00E+00	1.12E+03
3.00E+00	5.07E+00	1.00E+00	1.00E+00	1.12E+03
4.00E+00	5.40E+00	1.00E+00	1.00E+00	1.12E+03
5.00E+00	7.65E+00	1.00E+00	1.00E+00	1.12E+03
6.00E+00	5.67E+00	1.00E+00	1.00E+00	1.12E+03
7.00E+00	8.05E+00	1.00E+00	1.00E+00	1.12E+03
8.00E+00	7.93E+00	1.00E+00	1.00E+00	1.12E+03
9.00E+00	9.44E+00	1.00E+00	1.00E+00	1.12E+03
1.00E+01	7.10E+00	1.00E+00	1.00E+00	1.12E+03
1.10E+01	6.80E+00	1.00E+00	1.00E+00	1.12E+03
1.20E+01	4.73E+00	1.00E+00	1.00E+00	1.12E+03
1.30E+01	3.35E+00	1.00E+00	1.00E+00	1.12E+03
1.40E+01	6.03E+00	1.00E+00	1.00E+00	1.12E+03
1.50E+01	4.96E+00	1.00E+00	1.00E+00	1.12E+03
1.60E+01	7.17E+00	1.00E+00	1.00E+00	1.12E+03
1.70E+01	7.85E+00	1.00E+00	1.00E+00	1.12E+03
1.80E+01	8.07E+00	1.00E+00	1.00E+00	1.12E+03
1.90E+01	6.23E+00	1.00E+00	1.00E+00	1.12E+03
2.00E+01	9.61E+00	1.00E+00	1.00E+00	1.12E+03
2.10E+01	9.78E+00	1.00E+00	1.00E+00	1.12E+03
2.20E+01	9.16E+00	1.00E+00	1.00E+00	1.12E+03
2.30E+01	7.52E+00	1.00E+00	1.00E+00	1.12E+03
2.40E+01	6.83E+00	1.00E+00	1.00E+00	1.12E+03
2.50E+01	7.83E+00	1.00E+00	1.00E+00	1.12E+03
2.60E+01	5.44E+00	1.00E+00	1.00E+00	1.12E+03
2.70E+01	1.05E+01	1.00E+00	1.00E+00	1.12E+03
2.80E+01	6.05E+00	1.00E+00	1.00E+00	1.12E+03
2.90E+01	6.25E+00	1.00E+00	1.00E+00	1.12E+03
3.00E+01	7.33E+00	2.00E+00	1.00E+00	1.12E+03
3.10E+01	5.20E+00	2.00E+00	1.00E+00	1.12E+03
3.20E+01	5.63E+00	2.00E+00	1.00E+00	1.12E+03
3.30E+01	6.46E+00	2.00E+00	1.00E+00	1.12E+03
3.40E+01	9.74E+00	2.00E+00	1.00E+00	1.12E+03
3.50E+01	8.59E+00	2.00E+00	1.00E+00	1.12E+03
3.60E+01	5.80E+00	2.00E+00	1.00E+00	1.12E+03
3.70E+01	5.99E+00	2.00E+00	1.00E+00	1.12E+03
3.80E+01	5.34E+00	2.00E+00	1.00E+00	1.12E+03
3.90E+01	6.76E+00	2.00E+00	1.00E+00	1.12E+03
4.00E+01	5.38E+00	2.00E+00	1.00E+00	1.12E+03
4.10E+01	6.20E+00	2.00E+00	1.00E+00	1.12E+03
4.20E+01	6.67E+00	2.00E+00	1.00E+00	1.12E+03
4.30E+01	8.91E+00	2.00E+00	1.00E+00	1.12E+03
4.40E+01	7.03E+00	2.00E+00	1.00E+00	1.12E+03
4.50E+01	8.97E+00	2.00E+00	1.00E+00	1.12E+03
4.60E+01	4.37E+00	2.00E+00	1.00E+00	1.12E+03
4.70E+01	4.55E+00	2.00E+00	1.00E+00	1.12E+03
4.80E+01	6.10E+00	2.00E+00	1.00E+00	1.12E+03
4.90E+01	8.80E+00	2.00E+00	1.00E+00	1.12E+03
5.00E+01	5.95E+00	2.00E+00	1.00E+00	1.12E+03
5.10E+01	7.24E+00	2.00E+00	1.00E+00	1.12E+03
5.20E+01	7.24E+00	2.00E+00	1.00E+00	1.12E+03
5.30E+01	7.45E+00	2.00E+00	1.00E+00	1.12E+03
5.40E+01	8.01E+00	2.00E+00	1.00E+00	1.12E+03
5.50E+01	9.12E+00	2.00E+00	1.00E+00	1.12E+03
5.60E+01	6.12E+00	2.00E+00	1.00E+00	1.12E+03
5.70E+01	5.85E+00	2.00E+00	1.00E+00	1.12E+03
5.80E+01	5.59E+00	2.00E+00	1.00E+00	1.12E+03
5.90E+01	7.74E+00	2.00E+00	1.00E+00	1.12E+03
1.00E+00	7.54E+00	1.00E+00	2.00E+00	1.12E+03
2.00E+00	7.53E+00	1.00E+00	2.00E+00	1.12E+03
3.00E+00	5.20E+00	1.00E+00	2.00E+00	1.12E+03
4.00E+00	2.99E+00	1.00E+00	2.00E+00	1.12E+03
5.00E+00	7.20E+00	1.00E+00	2.00E+00	1.12E+03
6.00E+00	5.55E+00	1.00E+00	2.00E+00	1.12E+03
7.00E+00	6.48E+00	1.00E+00	2.00E+00	1.12E+03
8.00E+00	8.57E+00	1.00E+00	2.00E+00	1.12E+03
9.00E+00	8.81E+00	1.00E+00	2.00E+00	1.12E+03
1.00E+01	7.90E+00	1.00E+00	2.00E+00	1.12E+03
1.10E+01	7.64E+00	1.00E+00	2.00E+00	1.12E+03
1.20E+01	3.55E+00	1.00E+00	2.00E+00	1.12E+03
1.30E+01	5.61E+00	1.00E+00	2.00E+00	1.12E+03
1.40E+01	3.21E+00	1.00E+00	2.00E+00	1.12E+03
1.50E+01	7.83E+00	1.00E+00	2.00E+00	1.12E+03
1.60E+01	6.55E+00	1.00E+00	2.00E+00	1.12E+03
1.70E+01	2.66E+00	1.00E+00	2.00E+00	1.12E+03
1.80E+01	1.22E+01	1.00E+00	2.00E+00	1.12E+03
1.90E+01	8.15E+00	1.00E+00	2.00E+00	1.12E+03
2.00E+01	4.93E+00	1.00E+00	2.00E+00	1.12E+03
2.10E+01	1.03E+01	1.00E+00	2.00E+00	1.12E+03
2.20E+01	7.50E+00	1.00E+00	2.00E+00	1.12E+03
2.30E+01	6.20E+00	1.00E+00	2.00E+00	1.12E+03
2.40E+01	7.44E+00	1.00E+00	2.00E+00	1.12E+03
2.50E+01	6.99E+00	1.00E+00	2.00E+00	1.12E+03
2.60E+01	5.93E+00	1.00E+00	2.00E+00	1.12E+03
2.70E+01	6.31E+00	1.00E+00	2.00E+00	1.12E+03
2.80E+01	4.22E+00	1.00E+00	2.00E+00	1.12E+03
2.90E+01	7.33E+00	1.00E+00	2.00E+00	1.12E+03
3.00E+01	3.72E+00	2.00E+00	2.00E+00	1.12E+03
3.10E+01	1.73E+00	2.00E+00	2.00E+00	1.12E+03
3.20E+01	5.95E+00	2.00E+00	2.00E+00	1.12E+03
3.30E+01	4.16E+00	2.00E+00	2.00E+00	1.12E+03
3.40E+01	4.25E+00	2.00E+00	2.00E+00	1.12E+03
3.50E+01	6.43E+00	2.00E+00	2.00E+00	1.12E+03
3.60E+01	4.86E+00	2.00E+00	2.00E+00	1.12E+03
3.70E+01	3.17E+00	2.00E+00	2.00E+00	1.12E+03
3.80E+01	4.10E+00	2.00E+00	2.00E+00	1.12E+03
3.90E+01	5.84E+00	2.00E+00	2.00E+00	1.12E+03
4.00E+01	5.21E+00	2.00E+00	2.00E+00	1.12E+03
4.10E+01	8.84E+00	2.00E+00	2.00E+00	1.12E+03
4.20E+01	5.22E+00	2.00E+00	2.00E+00	1.12E+03
4.30E+01	9.79E+00	2.00E+00	2.00E+00	1.12E+03
4.40E+01	5.21E+00	2.00E+00	2.00E+00	1.12E+03
4.50E+01	7.92E+00	2.00E+00	2.00E+00	1.12E+03
4.60E+01	3.33E+00	2.00E+00	2.00E+00	1.12E+03
4.70E+01	6.94E+00	2.00E+00	2.00E+00	1.12E+03
4.80E+01	5.94E+00	2.00E+00	2.00E+00	1.12E+03
4.90E+01	3.88E+00	2.00E+00	2.00E+00	1.12E+03
5.00E+01	3.55E+00	2.00E+00	2.00E+00	1.12E+03
5.10E+01	7.86E+00	2.00E+00	2.00E+00	1.12E+03
5.20E+01	7.86E+00	2.00E+00	2.00E+00	1.12E+03
5.30E+01	6.02E+00	2.00E+00	2.00E+00	1.12E+03
5.40E+01	1.09E+01	2.00E+00	2.00E+00	1.12E+03
5.50E+01	4.95E+00	2.00E+00	2.00E+00	1.12E+03
5.60E+01	4.27E+00	2.00E+00	2.00E+00	1.12E+03
5.70E+01	2.75E+00	2.00E+00	2.00E+00	1.12E+03
5.80E+01	6.27E+00	2.00E+00	2.00E+00	1.12E+03
5.90E+01	6.99E+00	2.00E+00	2.00E+00	1.12E+03
1.00E+00	7.53E+00	1.00E+00	1.00E+00	1.14E+03
2.00E+00	9.71E+00	1.00E+00	1.00E+00	1.14E+03
3.00E+00	4.48E+00	1.00E+00	1.00E+00	1.14E+03
4.00E+00	4.82E+00	1.00E+00	1.00E+00	1.14E+03
5.00E+00	6.03E+00	1.00E+00	1.00E+00	1.14E+03
6.00E+00	4.24E+00	1.00E+00	1.00E+00	1.14E+03
7.00E+00	6.04E+00	1.00E+00	1.00E+00	1.14E+03
8.00E+00	7.87E+00	1.00E+00	1.00E+00	1.14E+03
9.00E+00	8.56E+00	1.00E+00	1.00E+00	1.14E+03
1.00E+01	5.55E+00	1.00E+00	1.00E+00	1.14E+03
1.10E+01	6.65E+00	1.00E+00	1.00E+00	1.14E+03
1.20E+01	4.07E+00	1.00E+00	1.00E+00	1.14E+03
1.30E+01	2.56E+00	1.00E+00	1.00E+00	1.14E+03
1.40E+01	3.70E+00	1.00E+00	1.00E+00	1.14E+03
1.50E+01	4.97E+00	1.00E+00	1.00E+00	1.14E+03
1.60E+01	7.97E+00	1.00E+00	1.00E+00	1.14E+03
1.70E+01	6.39E+00	1.00E+00	1.00E+00	1.14E+03
1.80E+01	1.31E+01	1.00E+00	1.00E+00	1.14E+03
1.90E+01	6.72E+00	1.00E+00	1.00E+00	1.14E+03
2.00E+01	7.45E+00	1.00E+00	1.00E+00	1.14E+03
2.10E+01	5.80E+00	1.00E+00	1.00E+00	1.14E+03
2.20E+01	8.62E+00	1.00E+00	1.00E+00	1.14E+03
2.30E+01	8.13E+00	1.00E+00	1.00E+00	1.14E+03
2.40E+01	5.93E+00	1.00E+00	1.00E+00	1.14E+03
2.50E+01	5.33E+00	1.00E+00	1.00E+00	1.14E+03
2.60E+01	3.83E+00	1.00E+00	1.00E+00	1.14E+03
2.70E+01	7.84E+00	1.00E+00	1.00E+00	1.14E+03
2.80E+01	4.14E+00	1.00E+00	1.00E+00	1.14E+03
2.90E+01	5.32E+00	1.00E+00	1.00E+00	1.14E+03
3.00E+01	2.82E+00	2.00E+00	1.00E+00	1.14E+03
3.10E+01	2.09E+00	2.00E+00	1.00E+00	1.14E+03
3.20E+01	4.85E+00	2.00E+00	1.00E+00	1.14E+03
3.30E+01	2.71E+00	2.00E+00	1.00E+00	1.14E+03
3.40E+01	7.73E+00	2.00E+00	1.00E+00	1.14E+03
3.50E+01	8.60E+00	2.00E+00	1.00E+00	1.14E+03
3.60E+01	4.27E+00	2.00E+00	1.00E+00	1.14E+03
3.70E+01	4.43E+00	2.00E+00	1.00E+00	1.14E+03
3.80E+01	4.99E+00	2.00E+00	1.00E+00	1.14E+03
3.90E+01	6.26E+00	2.00E+00	1.00E+00	1.14E+03
4.00E+01	5.92E+00	2.00E+00	1.00E+00	1.14E+03
4.10E+01	7.10E+00	2.00E+00	1.00E+00	1.14E+03
4.20E+01	7.30E+00	2.00E+00	1.00E+00	1.14E+03
4.30E+01	6.02E+00	2.00E+00	1.00E+00	1.14E+03
4.40E+01	6.85E+00	2.00E+00	1.00E+00	1.14E+03
4.50E+01	8.52E+00	2.00E+00	1.00E+00	1.14E+03
4.60E+01	6.32E+00	2.00E+00	1.00E+00	1.14E+03
4.70E+01	6.88E+00	2.00E+00	1.00E+00	1.14E+03
4.80E+01	5.70E+00	2.00E+00	1.00E+00	1.14E+03
4.90E+01	5.99E+00	2.00E+00	1.00E+00	1.14E+03
5.00E+01	4.24E+00	2.00E+00	1.00E+00	1.14E+03
5.10E+01	7.59E+00	2.00E+00	1.00E+00	1.14E+03
5.20E+01	7.59E+00	2.00E+00	1.00E+00	1.14E+03
5.30E+01	5.16E+00	2.00E+00	1.00E+00	1.14E+03
5.40E+01	9.51E+00	2.00E+00	1.00E+00	1.14E+03
5.50E+01	4.10E+00	2.00E+00	1.00E+00	1.14E+03
5.60E+01	3.07E+00	2.00E+00	1.00E+00	1.14E+03
5.70E+01	3.37E+00	2.00E+00	1.00E+00	1.14E+03
5.80E+01	6.06E+00	2.00E+00	1.00E+00	1.14E+03
5.90E+01	1.23E+01	2.00E+00	1.00E+00	1.14E+03
1.00E+00	4.42E+00	1.00E+00	2.00E+00	1.14E+03
2.00E+00	9.84E+00	1.00E+00	2.00E+00	1.14E+03
3.00E+00	5.23E+00	1.00E+00	2.00E+00	1.14E+03
4.00E+00	5.81E+00	1.00E+00	2.00E+00	1.14E+03
5.00E+00	7.22E+00	1.00E+00	2.00E+00	1.14E+03
6.00E+00	7.68E+00	1.00E+00	2.00E+00	1.14E+03
7.00E+00	4.98E+00	1.00E+00	2.00E+00	1.14E+03
8.00E+00	8.23E+00	1.00E+00	2.00E+00	1.14E+03
9.00E+00	7.51E+00	1.00E+00	2.00E+00	1.14E+03
1.00E+01	6.64E+00	1.00E+00	2.00E+00	1.14E+03
1.10E+01	7.03E+00	1.00E+00	2.00E+00	1.14E+03
1.20E+01	5.51E+00	1.00E+00	2.00E+00	1.14E+03
1.30E+01	5.33E+00	1.00E+00	2.00E+00	1.14E+03
1.40E+01	4.01E+00	1.00E+00	2.00E+00	1.14E+03
1.50E+01	5.54E+00	1.00E+00	2.00E+00	1.14E+03
1.60E+01	6.71E+00	1.00E+00	2.00E+00	1.14E+03
1.70E+01	4.78E+00	1.00E+00	2.00E+00	1.14E+03
1.80E+01	1.23E+01	1.00E+00	2.00E+00	1.14E+03
1.90E+01	6.02E+00	1.00E+00	2.00E+00	1.14E+03
2.00E+01	6.42E+00	1.00E+00	2.00E+00	1.14E+03
2.10E+01	6.56E+00	1.00E+00	2.00E+00	1.14E+03
2.20E+01	8.09E+00	1.00E+00	2.00E+00	1.14E+03
2.30E+01	3.08E+00	1.00E+00	2.00E+00	1.14E+03
2.40E+01	8.43E+00	1.00E+00	2.00E+00	1.14E+03
2.50E+01	7.39E+00	1.00E+00	2.00E+00	1.14E+03
2.60E+01	4.39E+00	1.00E+00	2.00E+00	1.14E+03
2.70E+01	9.84E+00	1.00E+00	2.00E+00	1.14E+03
2.80E+01	5.13E+00	1.00E+00	2.00E+00	1.14E+03
2.90E+01	6.61E+00	1.00E+00	2.00E+00	1.14E+03
3.00E+01	3.99E+00	2.00E+00	2.00E+00	1.14E+03
3.10E+01	1.54E+00	2.00E+00	2.00E+00	1.14E+03
3.20E+01	3.44E+00	2.00E+00	2.00E+00	1.14E+03
3.30E+01	4.89E+00	2.00E+00	2.00E+00	1.14E+03
3.40E+01	6.63E+00	2.00E+00	2.00E+00	1.14E+03
3.50E+01	6.66E+00	2.00E+00	2.00E+00	1.14E+03
3.60E+01	5.43E+00	2.00E+00	2.00E+00	1.14E+03
3.70E+01	2.21E+00	2.00E+00	2.00E+00	1.14E+03
3.80E+01	3.95E+00	2.00E+00	2.00E+00	1.14E+03
3.90E+01	4.14E+00	2.00E+00	2.00E+00	1.14E+03
4.00E+01	5.79E+00	2.00E+00	2.00E+00	1.14E+03
4.10E+01	8.00E+00	2.00E+00	2.00E+00	1.14E+03
4.20E+01	5.43E+00	2.00E+00	2.00E+00	1.14E+03
4.30E+01	5.76E+00	2.00E+00	2.00E+00	1.14E+03
4.40E+01	3.93E+00	2.00E+00	2.00E+00	1.14E+03
4.50E+01	6.83E+00	2.00E+00	2.00E+00	1.14E+03
4.60E+01	2.44E+00	2.00E+00	2.00E+00	1.14E+03
4.70E+01	4.26E+00	2.00E+00	2.00E+00	1.14E+03
4.80E+01	9.07E+00	2.00E+00	2.00E+00	1.14E+03
4.90E+01	5.88E+00	2.00E+00	2.00E+00	1.14E+03
5.00E+01	3.44E+00	2.00E+00	2.00E+00	1.14E+03
5.10E+01	8.77E+00	2.00E+00	2.00E+00	1.14E+03
5.20E+01	8.77E+00	2.00E+00	2.00E+00	1.14E+03
5.30E+01	5.34E+00	2.00E+00	2.00E+00	1.14E+03
5.40E+01	9.99E+00	2.00E+00	2.00E+00	1.14E+03
5.50E+01	3.31E+00	2.00E+00	2.00E+00	1.14E+03
5.60E+01	2.84E+00	2.00E+00	2.00E+00	1.14E+03
5.70E+01	1.64E+00	2.00E+00	2.00E+00	1.14E+03
5.80E+01	9.67E+00	2.00E+00	2.00E+00	1.14E+03
5.90E+01	1.14E+01	2.00E+00	2.00E+00	1.14E+03
1.00E+00	6.01E+00	1.00E+00	1.00E+00	1.17E+03
2.00E+00	1.06E+01	1.00E+00	1.00E+00	1.17E+03
3.00E+00	2.53E+00	1.00E+00	1.00E+00	1.17E+03
4.00E+00	2.53E+00	1.00E+00	1.00E+00	1.17E+03
5.00E+00	7.13E+00	1.00E+00	1.00E+00	1.17E+03
6.00E+00	4.35E+00	1.00E+00	1.00E+00	1.17E+03
7.00E+00	6.15E+00	1.00E+00	1.00E+00	1.17E+03
8.00E+00	5.92E+00	1.00E+00	1.00E+00	1.17E+03
9.00E+00	8.87E+00	1.00E+00	1.00E+00	1.17E+03
1.00E+01	4.85E+00	1.00E+00	1.00E+00	1.17E+03
1.10E+01	8.06E+00	1.00E+00	1.00E+00	1.17E+03
1.20E+01	5.17E+00	1.00E+00	1.00E+00	1.17E+03
1.30E+01	4.35E+00	1.00E+00	1.00E+00	1.17E+03
1.40E+01	2.22E+00	1.00E+00	1.00E+00	1.17E+03
1.50E+01	6.31E+00	1.00E+00	1.00E+00	1.17E+03
1.60E+01	6.69E+00	1.00E+00	1.00E+00	1.17E+03
1.70E+01	5.46E+00	1.00E+00	1.00E+00	1.17E+03
1.80E+01	1.31E+01	1.00E+00	1.00E+00	1.17E+03
1.90E+01	5.72E+00	1.00E+00	1.00E+00	1.17E+03
2.00E+01	5.86E+00	1.00E+00	1.00E+00	1.17E+03
2.10E+01	9.10E+00	1.00E+00	1.00E+00	1.17E+03
2.20E+01	7.08E+00	1.00E+00	1.00E+00	1.17E+03
2.30E+01	6.75E+00	1.00E+00	1.00E+00	1.17E+03
2.40E+01	3.26E+00	1.00E+00	1.00E+00	1.17E+03
2.50E+01	6.08E+00	1.00E+00	1.00E+00	1.17E+03
2.60E+01	3.06E+00	1.00E+00	1.00E+00	1.17E+03
2.70E+01	7.83E+00	1.00E+00	1.00E+00	1.17E+03
2.80E+01	4.48E+00	1.00E+00	1.00E+00	1.17E+03
2.90E+01	3.38E+00	1.00E+00	1.00E+00	1.17E+03
3.00E+01	4.00E+00	2.00E+00	1.00E+00	1.17E+03
3.10E+01	3.62E+00	2.00E+00	1.00E+00	1.17E+03
3.20E+01	4.31E+00	2.00E+00	1.00E+00	1.17E+03
3.30E+01	2.97E+00	2.00E+00	1.00E+00	1.17E+03
3.40E+01	5.57E+00	2.00E+00	1.00E+00	1.17E+03
3.50E+01	6.04E+00	2.00E+00	1.00E+00	1.17E+03
3.60E+01	4.41E+00	2.00E+00	1.00E+00	1.17E+03
3.70E+01	3.54E+00	2.00E+00	1.00E+00	1.17E+03
3.80E+01	3.91E+00	2.00E+00	1.00E+00	1.17E+03
3.90E+01	2.70E+00	2.00E+00	1.00E+00	1.17E+03
4.00E+01	4.83E+00	2.00E+00	1.00E+00	1.17E+03
4.10E+01	5.21E+00	2.00E+00	1.00E+00	1.17E+03
4.20E+01	3.50E+00	2.00E+00	1.00E+00	1.17E+03
4.30E+01	7.85E+00	2.00E+00	1.00E+00	1.17E+03
4.40E+01	5.16E+00	2.00E+00	1.00E+00	1.17E+03
4.50E+01	5.19E+00	2.00E+00	1.00E+00	1.17E+03
4.60E+01	3.54E+00	2.00E+00	1.00E+00	1.17E+03
4.70E+01	7.15E+00	2.00E+00	1.00E+00	1.17E+03
4.80E+01	8.38E+00	2.00E+00	1.00E+00	1.17E+03
4.90E+01	4.58E+00	2.00E+00	1.00E+00	1.17E+03
5.00E+01	4.55E+00	2.00E+00	1.00E+00	1.17E+03
5.10E+01	7.88E+00	2.00E+00	1.00E+00	1.17E+03
5.20E+01	7.88E+00	2.00E+00	1.00E+00	1.17E+03
5.30E+01	4.80E+00	2.00E+00	1.00E+00	1.17E+03
5.40E+01	8.58E+00	2.00E+00	1.00E+00	1.17E+03
5.50E+01	6.75E+00	2.00E+00	1.00E+00	1.17E+03
5.60E+01	4.17E+00	2.00E+00	1.00E+00	1.17E+03
5.70E+01	3.05E+00	2.00E+00	1.00E+00	1.17E+03
5.80E+01	5.85E+00	2.00E+00	1.00E+00	1.17E+03
5.90E+01	7.95E+00	2.00E+00	1.00E+00	1.17E+03
1.00E+00	5.37E+00	1.00E+00	2.00E+00	1.17E+03
2.00E+00	8.23E+00	1.00E+00	2.00E+00	1.17E+03
3.00E+00	4.66E+00	1.00E+00	2.00E+00	1.17E+03
4.00E+00	3.12E+00	1.00E+00	2.00E+00	1.17E+03
5.00E+00	9.35E+00	1.00E+00	2.00E+00	1.17E+03
6.00E+00	7.30E+00	1.00E+00	2.00E+00	1.17E+03
7.00E+00	7.31E+00	1.00E+00	2.00E+00	1.17E+03
8.00E+00	5.25E+00	1.00E+00	2.00E+00	1.17E+03
9.00E+00	6.73E+00	1.00E+00	2.00E+00	1.17E+03
1.00E+01	7.02E+00	1.00E+00	2.00E+00	1.17E+03
1.10E+01	6.92E+00	1.00E+00	2.00E+00	1.17E+03
1.20E+01	6.79E+00	1.00E+00	2.00E+00	1.17E+03
1.30E+01	3.99E+00	1.00E+00	2.00E+00	1.17E+03
1.40E+01	5.50E+00	1.00E+00	2.00E+00	1.17E+03
1.50E+01	7.33E+00	1.00E+00	2.00E+00	1.17E+03
1.60E+01	8.19E+00	1.00E+00	2.00E+00	1.17E+03
1.70E+01	5.20E+00	1.00E+00	2.00E+00	1.17E+03
1.80E+01	1.27E+01	1.00E+00	2.00E+00	1.17E+03
1.90E+01	7.47E+00	1.00E+00	2.00E+00	1.17E+03
2.00E+01	6.17E+00	1.00E+00	2.00E+00	1.17E+03
2.10E+01	5.10E+00	1.00E+00	2.00E+00	1.17E+03
2.20E+01	7.97E+00	1.00E+00	2.00E+00	1.17E+03
2.30E+01	5.98E+00	1.00E+00	2.00E+00	1.17E+03
2.40E+01	4.70E+00	1.00E+00	2.00E+00	1.17E+03
2.50E+01	7.44E+00	1.00E+00	2.00E+00	1.17E+03
2.60E+01	7.29E+00	1.00E+00	2.00E+00	1.17E+03
2.70E+01	1.09E+01	1.00E+00	2.00E+00	1.17E+03
2.80E+01	5.70E+00	1.00E+00	2.00E+00	1.17E+03
2.90E+01	4.04E+00	1.00E+00	2.00E+00	1.17E+03
3.00E+01	5.08E+00	2.00E+00	2.00E+00	1.17E+03
3.10E+01	1.48E+00	2.00E+00	2.00E+00	1.17E+03
3.20E+01	6.21E+00	2.00E+00	2.00E+00	1.17E+03
3.30E+01	4.53E+00	2.00E+00	2.00E+00	1.17E+03
3.40E+01	5.81E+00	2.00E+00	2.00E+00	1.17E+03
3.50E+01	8.47E+00	2.00E+00	2.00E+00	1.17E+03
3.60E+01	4.71E+00	2.00E+00	2.00E+00	1.17E+03
3.70E+01	2.86E+00	2.00E+00	2.00E+00	1.17E+03
3.80E+01	4.61E+00	2.00E+00	2.00E+00	1.17E+03
3.90E+01	5.24E+00	2.00E+00	2.00E+00	1.17E+03
4.00E+01	5.26E+00	2.00E+00	2.00E+00	1.17E+03
4.10E+01	9.23E+00	2.00E+00	2.00E+00	1.17E+03
4.20E+01	8.33E+00	2.00E+00	2.00E+00	1.17E+03
4.30E+01	8.54E+00	2.00E+00	2.00E+00	1.17E+03
4.40E+01	6.65E+00	2.00E+00	2.00E+00	1.17E+03
4.50E+01	5.09E+00	2.00E+00	2.00E+00	1.17E+03
4.60E+01	2.75E+00	2.00E+00	2.00E+00	1.17E+03
4.70E+01	3.99E+00	2.00E+00	2.00E+00	1.17E+03
4.80E+01	8.90E+00	2.00E+00	2.00E+00	1.17E+03
4.90E+01	5.30E+00	2.00E+00	2.00E+00	1.17E+03
5.00E+01	6.00E+00	2.00E+00	2.00E+00	1.17E+03
5.10E+01	5.75E+00	2.00E+00	2.00E+00	1.17E+03
5.20E+01	5.75E+00	2.00E+00	2.00E+00	1.17E+03
5.30E+01	5.34E+00	2.00E+00	2.00E+00	1.17E+03
5.40E+01	8.35E+00	2.00E+00	2.00E+00	1.17E+03
5.50E+01	4.54E+00	2.00E+00	2.00E+00	1.17E+03
5.60E+01	3.84E+00	2.00E+00	2.00E+00	1.17E+03
5.70E+01	1.82E+00	2.00E+00	2.00E+00	1.17E+03
5.80E+01	1.06E+01	2.00E+00	2.00E+00	1.17E+03
5.90E+01	6.74E+00	2.00E+00	2.00E+00	1.17E+03
1.00E+00	1.03E+01	1.00E+00	1.00E+00	1.48E+03
2.00E+00	9.98E+00	1.00E+00	1.00E+00	1.48E+03
3.00E+00	6.22E+00	1.00E+00	1.00E+00	1.48E+03
4.00E+00	7.33E+00	1.00E+00	1.00E+00	1.48E+03
5.00E+00	8.26E+00	1.00E+00	1.00E+00	1.48E+03
6.00E+00	9.26E+00	1.00E+00	1.00E+00	1.48E+03
7.00E+00	7.29E+00	1.00E+00	1.00E+00	1.48E+03
8.00E+00	8.17E+00	1.00E+00	1.00E+00	1.48E+03
9.00E+00	9.29E+00	1.00E+00	1.00E+00	1.48E+03
1.00E+01	9.78E+00	1.00E+00	1.00E+00	1.48E+03
1.10E+01	6.45E+00	1.00E+00	1.00E+00	1.48E+03
1.20E+01	7.81E+00	1.00E+00	1.00E+00	1.48E+03
1.30E+01	8.58E+00	1.00E+00	1.00E+00	1.48E+03
1.40E+01	9.65E+00	1.00E+00	1.00E+00	1.48E+03
1.50E+01	6.87E+00	1.00E+00	1.00E+00	1.48E+03
1.60E+01	7.58E+00	1.00E+00	1.00E+00	1.48E+03
1.70E+01	8.81E+00	1.00E+00	1.00E+00	1.48E+03
1.80E+01	7.41E+00	1.00E+00	1.00E+00	1.48E+03
1.90E+01	7.94E+00	1.00E+00	1.00E+00	1.48E+03
2.00E+01	7.61E+00	1.00E+00	1.00E+00	1.48E+03
2.10E+01	8.63E+00	1.00E+00	1.00E+00	1.48E+03
2.20E+01	9.69E+00	1.00E+00	1.00E+00	1.48E+03
2.30E+01	6.24E+00	1.00E+00	1.00E+00	1.48E+03
2.40E+01	1.00E+01	1.00E+00	1.00E+00	1.48E+03
2.50E+01	7.42E+00	1.00E+00	1.00E+00	1.48E+03
2.60E+01	7.77E+00	1.00E+00	1.00E+00	1.48E+03
2.70E+01	6.37E+00	1.00E+00	1.00E+00	1.48E+03
2.80E+01	4.12E+00	1.00E+00	1.00E+00	1.48E+03
2.90E+01	4.97E+00	1.00E+00	1.00E+00	1.48E+03
3.00E+01	6.78E+00	2.00E+00	1.00E+00	1.48E+03
3.10E+01	4.86E+00	2.00E+00	1.00E+00	1.48E+03
3.20E+01	7.80E+00	2.00E+00	1.00E+00	1.48E+03
3.30E+01	7.07E+00	2.00E+00	1.00E+00	1.48E+03
3.40E+01	8.92E+00	2.00E+00	1.00E+00	1.48E+03
3.50E+01	7.02E+00	2.00E+00	1.00E+00	1.48E+03
3.60E+01	6.15E+00	2.00E+00	1.00E+00	1.48E+03
3.70E+01	7.73E+00	2.00E+00	1.00E+00	1.48E+03
3.80E+01	5.39E+00	2.00E+00	1.00E+00	1.48E+03
3.90E+01	8.02E+00	2.00E+00	1.00E+00	1.48E+03
4.00E+01	8.11E+00	2.00E+00	1.00E+00	1.48E+03
4.10E+01	9.32E+00	2.00E+00	1.00E+00	1.48E+03
4.20E+01	7.02E+00	2.00E+00	1.00E+00	1.48E+03
4.30E+01	8.61E+00	2.00E+00	1.00E+00	1.48E+03
4.40E+01	6.61E+00	2.00E+00	1.00E+00	1.48E+03
4.50E+01	8.84E+00	2.00E+00	1.00E+00	1.48E+03
4.60E+01	6.61E+00	2.00E+00	1.00E+00	1.48E+03
4.70E+01	8.52E+00	2.00E+00	1.00E+00	1.48E+03
4.80E+01	7.41E+00	2.00E+00	1.00E+00	1.48E+03
4.90E+01	9.22E+00	2.00E+00	1.00E+00	1.48E+03
5.00E+01	7.81E+00	2.00E+00	1.00E+00	1.48E+03
5.10E+01	7.99E+00	2.00E+00	1.00E+00	1.48E+03
5.20E+01	7.99E+00	2.00E+00	1.00E+00	1.48E+03
5.30E+01	9.34E+00	2.00E+00	1.00E+00	1.48E+03
5.40E+01	9.13E+00	2.00E+00	1.00E+00	1.48E+03
5.50E+01	6.20E+00	2.00E+00	1.00E+00	1.48E+03
5.60E+01	8.31E+00	2.00E+00	1.00E+00	1.48E+03
5.70E+01	6.36E+00	2.00E+00	1.00E+00	1.48E+03
5.80E+01	7.65E+00	2.00E+00	1.00E+00	1.48E+03
5.90E+01	8.09E+00	2.00E+00	1.00E+00	1.48E+03
1.00E+00	6.01E+00	1.00E+00	2.00E+00	1.48E+03
2.00E+00	8.56E+00	1.00E+00	2.00E+00	1.48E+03
3.00E+00	4.74E+00	1.00E+00	2.00E+00	1.48E+03
4.00E+00	5.96E+00	1.00E+00	2.00E+00	1.48E+03
5.00E+00	7.89E+00	1.00E+00	2.00E+00	1.48E+03
6.00E+00	6.07E+00	1.00E+00	2.00E+00	1.48E+03
7.00E+00	1.03E+01	1.00E+00	2.00E+00	1.48E+03
8.00E+00	9.95E+00	1.00E+00	2.00E+00	1.48E+03
9.00E+00	6.09E+00	1.00E+00	2.00E+00	1.48E+03
1.00E+01	8.34E+00	1.00E+00	2.00E+00	1.48E+03
1.10E+01	6.95E+00	1.00E+00	2.00E+00	1.48E+03
1.20E+01	6.22E+00	1.00E+00	2.00E+00	1.48E+03
1.30E+01	5.89E+00	1.00E+00	2.00E+00	1.48E+03
1.40E+01	4.78E+00	1.00E+00	2.00E+00	1.48E+03
1.50E+01	7.54E+00	1.00E+00	2.00E+00	1.48E+03
1.60E+01	8.74E+00	1.00E+00	2.00E+00	1.48E+03
1.70E+01	2.14E+00	1.00E+00	2.00E+00	1.48E+03
1.80E+01	1.22E+01	1.00E+00	2.00E+00	1.48E+03
1.90E+01	6.91E+00	1.00E+00	2.00E+00	1.48E+03
2.00E+01	7.70E+00	1.00E+00	2.00E+00	1.48E+03
2.10E+01	6.55E+00	1.00E+00	2.00E+00	1.48E+03
2.20E+01	7.97E+00	1.00E+00	2.00E+00	1.48E+03
2.30E+01	8.78E+00	1.00E+00	2.00E+00	1.48E+03
2.40E+01	5.13E+00	1.00E+00	2.00E+00	1.48E+03
2.50E+01	7.84E+00	1.00E+00	2.00E+00	1.48E+03
2.60E+01	4.64E+00	1.00E+00	2.00E+00	1.48E+03
2.70E+01	1.09E+01	1.00E+00	2.00E+00	1.48E+03
2.80E+01	1.74E+00	1.00E+00	2.00E+00	1.48E+03
2.90E+01	3.37E+00	1.00E+00	2.00E+00	1.48E+03
3.00E+01	2.82E+00	2.00E+00	2.00E+00	1.48E+03
3.10E+01	4.44E+00	2.00E+00	2.00E+00	1.48E+03
3.20E+01	4.82E+00	2.00E+00	2.00E+00	1.48E+03
3.30E+01	7.90E+00	2.00E+00	2.00E+00	1.48E+03
3.40E+01	4.58E+00	2.00E+00	2.00E+00	1.48E+03
3.50E+01	6.63E+00	2.00E+00	2.00E+00	1.48E+03
3.60E+01	4.49E+00	2.00E+00	2.00E+00	1.48E+03
3.70E+01	4.13E+00	2.00E+00	2.00E+00	1.48E+03
3.80E+01	1.70E+00	2.00E+00	2.00E+00	1.48E+03
3.90E+01	2.80E+00	2.00E+00	2.00E+00	1.48E+03
4.00E+01	6.33E+00	2.00E+00	2.00E+00	1.48E+03
4.10E+01	8.36E+00	2.00E+00	2.00E+00	1.48E+03
4.20E+01	2.86E+00	2.00E+00	2.00E+00	1.48E+03
4.30E+01	7.61E+00	2.00E+00	2.00E+00	1.48E+03
4.40E+01	6.61E+00	2.00E+00	2.00E+00	1.48E+03
4.50E+01	9.95E+00	2.00E+00	2.00E+00	1.48E+03
4.60E+01	3.89E+00	2.00E+00	2.00E+00	1.48E+03
4.70E+01	4.62E+00	2.00E+00	2.00E+00	1.48E+03
4.80E+01	6.85E+00	2.00E+00	2.00E+00	1.48E+03
4.90E+01	5.60E+00	2.00E+00	2.00E+00	1.48E+03
5.00E+01	2.11E+00	2.00E+00	2.00E+00	1.48E+03
5.10E+01	8.06E+00	2.00E+00	2.00E+00	1.48E+03
5.20E+01	8.06E+00	2.00E+00	2.00E+00	1.48E+03
5.30E+01	4.04E+00	2.00E+00	2.00E+00	1.48E+03
5.40E+01	1.01E+01	2.00E+00	2.00E+00	1.48E+03
5.50E+01	4.20E+00	2.00E+00	2.00E+00	1.48E+03
5.60E+01	3.89E+00	2.00E+00	2.00E+00	1.48E+03
5.70E+01	1.05E+00	2.00E+00	2.00E+00	1.48E+03
5.80E+01	2.48E+00	2.00E+00	2.00E+00	1.48E+03
5.90E+01	6.28E+00	2.00E+00	2.00E+00	1.48E+03
1.00E+00	8.13E+00	1.00E+00	1.00E+00	1.47E+03
2.00E+00	8.04E+00	1.00E+00	1.00E+00	1.47E+03
3.00E+00	6.64E+00	1.00E+00	1.00E+00	1.47E+03
4.00E+00	7.39E+00	1.00E+00	1.00E+00	1.47E+03
5.00E+00	5.52E+00	1.00E+00	1.00E+00	1.47E+03
6.00E+00	6.69E+00	1.00E+00	1.00E+00	1.47E+03
7.00E+00	1.06E+01	1.00E+00	1.00E+00	1.47E+03
8.00E+00	9.16E+00	1.00E+00	1.00E+00	1.47E+03
9.00E+00	9.13E+00	1.00E+00	1.00E+00	1.47E+03
1.00E+01	9.51E+00	1.00E+00	1.00E+00	1.47E+03
1.10E+01	7.53E+00	1.00E+00	1.00E+00	1.47E+03
1.20E+01	8.15E+00	1.00E+00	1.00E+00	1.47E+03
1.30E+01	6.68E+00	1.00E+00	1.00E+00	1.47E+03
1.40E+01	5.30E+00	1.00E+00	1.00E+00	1.47E+03
1.50E+01	7.18E+00	1.00E+00	1.00E+00	1.47E+03
1.60E+01	7.35E+00	1.00E+00	1.00E+00	1.47E+03
1.70E+01	8.39E+00	1.00E+00	1.00E+00	1.47E+03
1.80E+01	7.55E+00	1.00E+00	1.00E+00	1.47E+03
1.90E+01	8.72E+00	1.00E+00	1.00E+00	1.47E+03
2.00E+01	6.80E+00	1.00E+00	1.00E+00	1.47E+03
2.10E+01	5.21E+00	1.00E+00	1.00E+00	1.47E+03
2.20E+01	8.10E+00	1.00E+00	1.00E+00	1.47E+03
2.30E+01	9.27E+00	1.00E+00	1.00E+00	1.47E+03
2.40E+01	8.02E+00	1.00E+00	1.00E+00	1.47E+03
2.50E+01	7.29E+00	1.00E+00	1.00E+00	1.47E+03
2.60E+01	9.57E+00	1.00E+00	1.00E+00	1.47E+03
2.70E+01	6.41E+00	1.00E+00	1.00E+00	1.47E+03
2.80E+01	6.90E+00	1.00E+00	1.00E+00	1.47E+03
2.90E+01	6.33E+00	1.00E+00	1.00E+00	1.47E+03
3.00E+01	6.59E+00	2.00E+00	1.00E+00	1.47E+03
3.10E+01	4.86E+00	2.00E+00	1.00E+00	1.47E+03
3.20E+01	7.06E+00	2.00E+00	1.00E+00	1.47E+03
3.30E+01	7.24E+00	2.00E+00	1.00E+00	1.47E+03
3.40E+01	9.99E+00	2.00E+00	1.00E+00	1.47E+03
3.50E+01	7.98E+00	2.00E+00	1.00E+00	1.47E+03
3.60E+01	7.15E+00	2.00E+00	1.00E+00	1.47E+03
3.70E+01	4.86E+00	2.00E+00	1.00E+00	1.47E+03
3.80E+01	6.62E+00	2.00E+00	1.00E+00	1.47E+03
3.90E+01	7.28E+00	2.00E+00	1.00E+00	1.47E+03
4.00E+01	7.33E+00	2.00E+00	1.00E+00	1.47E+03
4.10E+01	7.51E+00	2.00E+00	1.00E+00	1.47E+03
4.20E+01	6.44E+00	2.00E+00	1.00E+00	1.47E+03
4.30E+01	9.86E+00	2.00E+00	1.00E+00	1.47E+03
4.40E+01	6.61E+00	2.00E+00	1.00E+00	1.47E+03
4.50E+01	6.98E+00	2.00E+00	1.00E+00	1.47E+03
4.60E+01	6.45E+00	2.00E+00	1.00E+00	1.47E+03
4.70E+01	7.63E+00	2.00E+00	1.00E+00	1.47E+03
4.80E+01	8.08E+00	2.00E+00	1.00E+00	1.47E+03
4.90E+01	7.53E+00	2.00E+00	1.00E+00	1.47E+03
5.00E+01	7.61E+00	2.00E+00	1.00E+00	1.47E+03
5.10E+01	1.04E+01	2.00E+00	1.00E+00	1.47E+03
5.20E+01	1.04E+01	2.00E+00	1.00E+00	1.47E+03
5.30E+01	7.01E+00	2.00E+00	1.00E+00	1.47E+03
5.40E+01	9.29E+00	2.00E+00	1.00E+00	1.47E+03
5.50E+01	5.25E+00	2.00E+00	1.00E+00	1.47E+03
5.60E+01	7.41E+00	2.00E+00	1.00E+00	1.47E+03
5.70E+01	6.30E+00	2.00E+00	1.00E+00	1.47E+03
5.80E+01	6.52E+00	2.00E+00	1.00E+00	1.47E+03
5.90E+01	9.04E+00	2.00E+00	1.00E+00	1.47E+03
1.00E+00	4.59E+00	1.00E+00	2.00E+00	1.47E+03
2.00E+00	7.23E+00	1.00E+00	2.00E+00	1.47E+03
3.00E+00	2.50E+00	1.00E+00	2.00E+00	1.47E+03
4.00E+00	4.08E+00	1.00E+00	2.00E+00	1.47E+03
5.00E+00	6.87E+00	1.00E+00	2.00E+00	1.47E+03
6.00E+00	8.74E+00	1.00E+00	2.00E+00	1.47E+03
7.00E+00	7.69E+00	1.00E+00	2.00E+00	1.47E+03
8.00E+00	8.03E+00	1.00E+00	2.00E+00	1.47E+03
9.00E+00	1.00E+01	1.00E+00	2.00E+00	1.47E+03
1.00E+01	8.25E+00	1.00E+00	2.00E+00	1.47E+03
1.10E+01	5.97E+00	1.00E+00	2.00E+00	1.47E+03
1.20E+01	5.51E+00	1.00E+00	2.00E+00	1.47E+03
1.30E+01	5.22E+00	1.00E+00	2.00E+00	1.47E+03
1.40E+01	3.29E+00	1.00E+00	2.00E+00	1.47E+03
1.50E+01	1.03E+01	1.00E+00	2.00E+00	1.47E+03
1.60E+01	8.88E+00	1.00E+00	2.00E+00	1.47E+03
1.70E+01	1.64E+00	1.00E+00	2.00E+00	1.47E+03
1.80E+01	1.04E+01	1.00E+00	2.00E+00	1.47E+03
1.90E+01	9.86E+00	1.00E+00	2.00E+00	1.47E+03
2.00E+01	8.11E+00	1.00E+00	2.00E+00	1.47E+03
2.10E+01	9.97E+00	1.00E+00	2.00E+00	1.47E+03
2.20E+01	8.59E+00	1.00E+00	2.00E+00	1.47E+03
2.30E+01	5.53E+00	1.00E+00	2.00E+00	1.47E+03
2.40E+01	7.00E+00	1.00E+00	2.00E+00	1.47E+03
2.50E+01	7.84E+00	1.00E+00	2.00E+00	1.47E+03
2.60E+01	6.81E+00	1.00E+00	2.00E+00	1.47E+03
2.70E+01	8.69E+00	1.00E+00	2.00E+00	1.47E+03
2.80E+01	6.47E+00	1.00E+00	2.00E+00	1.47E+03
2.90E+01	3.46E+00	1.00E+00	2.00E+00	1.47E+03
3.00E+01	2.76E+00	2.00E+00	2.00E+00	1.47E+03
3.10E+01	1.91E+00	2.00E+00	2.00E+00	1.47E+03
3.20E+01	4.39E+00	2.00E+00	2.00E+00	1.47E+03
3.30E+01	6.49E+00	2.00E+00	2.00E+00	1.47E+03
3.40E+01	6.59E+00	2.00E+00	2.00E+00	1.47E+03
3.50E+01	8.27E+00	2.00E+00	2.00E+00	1.47E+03
3.60E+01	5.31E+00	2.00E+00	2.00E+00	1.47E+03
3.70E+01	2.10E+00	2.00E+00	2.00E+00	1.47E+03
3.80E+01	3.71E+00	2.00E+00	2.00E+00	1.47E+03
3.90E+01	4.21E+00	2.00E+00	2.00E+00	1.47E+03
4.00E+01	6.12E+00	2.00E+00	2.00E+00	1.47E+03
4.10E+01	4.99E+00	2.00E+00	2.00E+00	1.47E+03
4.20E+01	7.54E+00	2.00E+00	2.00E+00	1.47E+03
4.30E+01	7.48E+00	2.00E+00	2.00E+00	1.47E+03
4.40E+01	4.44E+00	2.00E+00	2.00E+00	1.47E+03
4.50E+01	6.44E+00	2.00E+00	2.00E+00	1.47E+03
4.60E+01	2.45E+00	2.00E+00	2.00E+00	1.47E+03
4.70E+01	1.99E+00	2.00E+00	2.00E+00	1.47E+03
4.80E+01	6.92E+00	2.00E+00	2.00E+00	1.47E+03
4.90E+01	5.99E+00	2.00E+00	2.00E+00	1.47E+03
5.00E+01	1.96E+00	2.00E+00	2.00E+00	1.47E+03
5.10E+01	6.40E+00	2.00E+00	2.00E+00	1.47E+03
5.20E+01	6.40E+00	2.00E+00	2.00E+00	1.47E+03
5.30E+01	5.85E+00	2.00E+00	2.00E+00	1.47E+03
5.40E+01	9.66E+00	2.00E+00	2.00E+00	1.47E+03
5.50E+01	4.89E+00	2.00E+00	2.00E+00	1.47E+03
5.60E+01	1.96E+00	2.00E+00	2.00E+00	1.47E+03
5.70E+01	1.11E+00	2.00E+00	2.00E+00	1.47E+03
5.80E+01	5.41E+00	2.00E+00	2.00E+00	1.47E+03
5.90E+01	1.24E+01	2.00E+00	2.00E+00	1.47E+03
1.00E+00	6.43E+00	1.00E+00	1.00E+00	1.45E+03
2.00E+00	9.51E+00	1.00E+00	1.00E+00	1.45E+03
3.00E+00	3.46E+00	1.00E+00	1.00E+00	1.45E+03
4.00E+00	4.03E+00	1.00E+00	1.00E+00	1.45E+03
5.00E+00	1.14E+01	1.00E+00	1.00E+00	1.45E+03
6.00E+00	7.28E+00	1.00E+00	1.00E+00	1.45E+03
7.00E+00	7.05E+00	1.00E+00	1.00E+00	1.45E+03
8.00E+00	1.43E+00	1.00E+00	1.00E+00	1.45E+03
9.00E+00	8.46E+00	1.00E+00	1.00E+00	1.45E+03
1.00E+01	5.72E+00	1.00E+00	1.00E+00	1.45E+03
1.10E+01	5.84E+00	1.00E+00	1.00E+00	1.45E+03
1.20E+01	7.49E+00	1.00E+00	1.00E+00	1.45E+03
1.30E+01	5.92E+00	1.00E+00	1.00E+00	1.45E+03
1.40E+01	6.73E+00	1.00E+00	1.00E+00	1.45E+03
1.50E+01	6.51E+00	1.00E+00	1.00E+00	1.45E+03
1.60E+01	6.95E+00	1.00E+00	1.00E+00	1.45E+03
1.70E+01	5.97E+00	1.00E+00	1.00E+00	1.45E+03
1.80E+01	8.19E+00	1.00E+00	1.00E+00	1.45E+03
1.90E+01	7.65E+00	1.00E+00	1.00E+00	1.45E+03
2.00E+01	7.56E+00	1.00E+00	1.00E+00	1.45E+03
2.10E+01	5.04E+00	1.00E+00	1.00E+00	1.45E+03
2.20E+01	8.37E+00	1.00E+00	1.00E+00	1.45E+03
2.30E+01	6.18E+00	1.00E+00	1.00E+00	1.45E+03
2.40E+01	9.23E+00	1.00E+00	1.00E+00	1.45E+03
2.50E+01	8.62E+00	1.00E+00	1.00E+00	1.45E+03
2.60E+01	7.86E+00	1.00E+00	1.00E+00	1.45E+03
2.70E+01	9.32E+00	1.00E+00	1.00E+00	1.45E+03
2.80E+01	6.49E+00	1.00E+00	1.00E+00	1.45E+03
2.90E+01	4.91E+00	1.00E+00	1.00E+00	1.45E+03
3.00E+01	3.74E+00	2.00E+00	1.00E+00	1.45E+03
3.10E+01	3.32E+00	2.00E+00	1.00E+00	1.45E+03
3.20E+01	4.39E+00	2.00E+00	1.00E+00	1.45E+03
3.30E+01	6.46E+00	2.00E+00	1.00E+00	1.45E+03
3.40E+01	6.67E+00	2.00E+00	1.00E+00	1.45E+03
3.50E+01	4.01E+00	2.00E+00	1.00E+00	1.45E+03
3.60E+01	6.17E+00	2.00E+00	1.00E+00	1.45E+03
3.70E+01	4.68E+00	2.00E+00	1.00E+00	1.45E+03
3.80E+01	5.96E+00	2.00E+00	1.00E+00	1.45E+03
3.90E+01	8.25E+00	2.00E+00	1.00E+00	1.45E+03
4.00E+01	5.74E+00	2.00E+00	1.00E+00	1.45E+03
4.10E+01	1.03E+01	2.00E+00	1.00E+00	1.45E+03
4.20E+01	8.29E+00	2.00E+00	1.00E+00	1.45E+03
4.30E+01	6.87E+00	2.00E+00	1.00E+00	1.45E+03
4.40E+01	7.85E+00	2.00E+00	1.00E+00	1.45E+03
4.50E+01	4.30E+00	2.00E+00	1.00E+00	1.45E+03
4.60E+01	4.55E+00	2.00E+00	1.00E+00	1.45E+03
4.70E+01	9.27E+00	2.00E+00	1.00E+00	1.45E+03
4.80E+01	5.08E+00	2.00E+00	1.00E+00	1.45E+03
4.90E+01	8.24E+00	2.00E+00	1.00E+00	1.45E+03
5.00E+01	7.06E+00	2.00E+00	1.00E+00	1.45E+03
5.10E+01	4.67E+00	2.00E+00	1.00E+00	1.45E+03
5.20E+01	4.67E+00	2.00E+00	1.00E+00	1.45E+03
5.30E+01	8.45E+00	2.00E+00	1.00E+00	1.45E+03
5.40E+01	7.36E+00	2.00E+00	1.00E+00	1.45E+03
5.50E+01	6.37E+00	2.00E+00	1.00E+00	1.45E+03
5.60E+01	6.74E+00	2.00E+00	1.00E+00	1.45E+03
5.70E+01	5.05E+00	2.00E+00	1.00E+00	1.45E+03
5.80E+01	7.86E+00	2.00E+00	1.00E+00	1.45E+03
5.90E+01	8.44E+00	2.00E+00	1.00E+00	1.45E+03
1.00E+00	5.54E+00	1.00E+00	2.00E+00	1.45E+03
2.00E+00	5.99E+00	1.00E+00	2.00E+00	1.45E+03
3.00E+00	3.32E+00	1.00E+00	2.00E+00	1.45E+03
4.00E+00	3.27E+00	1.00E+00	2.00E+00	1.45E+03
5.00E+00	8.33E+00	1.00E+00	2.00E+00	1.45E+03
6.00E+00	7.70E+00	1.00E+00	2.00E+00	1.45E+03
7.00E+00	5.95E+00	1.00E+00	2.00E+00	1.45E+03
8.00E+00	6.42E+00	1.00E+00	2.00E+00	1.45E+03
9.00E+00	8.54E+00	1.00E+00	2.00E+00	1.45E+03
1.00E+01	6.89E+00	1.00E+00	2.00E+00	1.45E+03
1.10E+01	9.49E+00	1.00E+00	2.00E+00	1.45E+03
1.20E+01	6.48E+00	1.00E+00	2.00E+00	1.45E+03
1.30E+01	4.89E+00	1.00E+00	2.00E+00	1.45E+03
1.40E+01	4.54E+00	1.00E+00	2.00E+00	1.45E+03
1.50E+01	8.68E+00	1.00E+00	2.00E+00	1.45E+03
1.60E+01	1.20E+01	1.00E+00	2.00E+00	1.45E+03
1.70E+01	2.82E+00	1.00E+00	2.00E+00	1.45E+03
1.80E+01	8.91E+00	1.00E+00	2.00E+00	1.45E+03
1.90E+01	5.29E+00	1.00E+00	2.00E+00	1.45E+03
2.00E+01	7.98E+00	1.00E+00	2.00E+00	1.45E+03
2.10E+01	8.16E+00	1.00E+00	2.00E+00	1.45E+03
2.20E+01	8.56E+00	1.00E+00	2.00E+00	1.45E+03
2.30E+01	6.29E+00	1.00E+00	2.00E+00	1.45E+03
2.40E+01	9.51E+00	1.00E+00	2.00E+00	1.45E+03
2.50E+01	1.06E+01	1.00E+00	2.00E+00	1.45E+03
2.60E+01	6.38E+00	1.00E+00	2.00E+00	1.45E+03
2.70E+01	9.73E+00	1.00E+00	2.00E+00	1.45E+03
2.80E+01	2.23E+00	1.00E+00	2.00E+00	1.45E+03
2.90E+01	5.19E+00	1.00E+00	2.00E+00	1.45E+03
3.00E+01	2.45E+00	2.00E+00	2.00E+00	1.45E+03
3.10E+01	2.00E+00	2.00E+00	2.00E+00	1.45E+03
3.20E+01	6.00E+00	2.00E+00	2.00E+00	1.45E+03
3.30E+01	8.24E+00	2.00E+00	2.00E+00	1.45E+03
3.40E+01	4.38E+00	2.00E+00	2.00E+00	1.45E+03
3.50E+01	3.93E+00	2.00E+00	2.00E+00	1.45E+03
3.60E+01	6.38E+00	2.00E+00	2.00E+00	1.45E+03
3.70E+01	2.78E+00	2.00E+00	2.00E+00	1.45E+03
3.80E+01	4.14E+00	2.00E+00	2.00E+00	1.45E+03
3.90E+01	4.27E+00	2.00E+00	2.00E+00	1.45E+03
4.00E+01	3.46E+00	2.00E+00	2.00E+00	1.45E+03
4.10E+01	7.32E+00	2.00E+00	2.00E+00	1.45E+03
4.20E+01	6.35E+00	2.00E+00	2.00E+00	1.45E+03
4.30E+01	7.16E+00	2.00E+00	2.00E+00	1.45E+03
4.40E+01	3.35E+00	2.00E+00	2.00E+00	1.45E+03
4.50E+01	5.43E+00	2.00E+00	2.00E+00	1.45E+03
4.60E+01	2.67E+00	2.00E+00	2.00E+00	1.45E+03
4.70E+01	5.38E+00	2.00E+00	2.00E+00	1.45E+03
4.80E+01	5.11E+00	2.00E+00	2.00E+00	1.45E+03
4.90E+01	4.48E+00	2.00E+00	2.00E+00	1.45E+03
5.00E+01	3.89E+00	2.00E+00	2.00E+00	1.45E+03
5.10E+01	6.13E+00	2.00E+00	2.00E+00	1.45E+03
5.20E+01	6.13E+00	2.00E+00	2.00E+00	1.45E+03
5.30E+01	5.89E+00	2.00E+00	2.00E+00	1.45E+03
5.40E+01	1.02E+01	2.00E+00	2.00E+00	1.45E+03
5.50E+01	6.26E+00	2.00E+00	2.00E+00	1.45E+03
5.60E+01	3.64E+00	2.00E+00	2.00E+00	1.45E+03
5.70E+01	1.09E+00	2.00E+00	2.00E+00	1.45E+03
5.80E+01	5.00E+00	2.00E+00	2.00E+00	1.45E+03
5.90E+01	6.19E+00	2.00E+00	2.00E+00	1.45E+03
1.00E+00	7.29E+00	1.00E+00	1.00E+00	1.44E+03
2.00E+00	6.63E+00	1.00E+00	1.00E+00	1.44E+03
3.00E+00	3.87E+00	1.00E+00	1.00E+00	1.44E+03
4.00E+00	4.69E+00	1.00E+00	1.00E+00	1.44E+03
5.00E+00	8.00E+00	1.00E+00	1.00E+00	1.44E+03
6.00E+00	7.12E+00	1.00E+00	1.00E+00	1.44E+03
7.00E+00	6.74E+00	1.00E+00	1.00E+00	1.44E+03
8.00E+00	4.90E+00	1.00E+00	1.00E+00	1.44E+03
9.00E+00	5.59E+00	1.00E+00	1.00E+00	1.44E+03
1.00E+01	6.47E+00	1.00E+00	1.00E+00	1.44E+03
1.10E+01	6.94E+00	1.00E+00	1.00E+00	1.44E+03
1.20E+01	6.82E+00	1.00E+00	1.00E+00	1.44E+03
1.30E+01	4.81E+00	1.00E+00	1.00E+00	1.44E+03
1.40E+01	3.45E+00	1.00E+00	1.00E+00	1.44E+03
1.50E+01	7.42E+00	1.00E+00	1.00E+00	1.44E+03
1.60E+01	9.01E+00	1.00E+00	1.00E+00	1.44E+03
1.70E+01	5.33E+00	1.00E+00	1.00E+00	1.44E+03
1.80E+01	1.04E+01	1.00E+00	1.00E+00	1.44E+03
1.90E+01	6.20E+00	1.00E+00	1.00E+00	1.44E+03
2.00E+01	8.92E+00	1.00E+00	1.00E+00	1.44E+03
2.10E+01	5.69E+00	1.00E+00	1.00E+00	1.44E+03
2.20E+01	7.42E+00	1.00E+00	1.00E+00	1.44E+03
2.30E+01	8.30E+00	1.00E+00	1.00E+00	1.44E+03
2.40E+01	6.73E+00	1.00E+00	1.00E+00	1.44E+03
2.50E+01	8.03E+00	1.00E+00	1.00E+00	1.44E+03
2.60E+01	3.73E+00	1.00E+00	1.00E+00	1.44E+03
2.70E+01	1.07E+01	1.00E+00	1.00E+00	1.44E+03
2.80E+01	4.74E+00	1.00E+00	1.00E+00	1.44E+03
2.90E+01	5.05E+00	1.00E+00	1.00E+00	1.44E+03
3.00E+01	2.78E+00	2.00E+00	1.00E+00	1.44E+03
3.10E+01	3.56E+00	2.00E+00	1.00E+00	1.44E+03
3.20E+01	6.71E+00	2.00E+00	1.00E+00	1.44E+03
3.30E+01	5.32E+00	2.00E+00	1.00E+00	1.44E+03
3.40E+01	6.77E+00	2.00E+00	1.00E+00	1.44E+03
3.50E+01	7.32E+00	2.00E+00	1.00E+00	1.44E+03
3.60E+01	4.03E+00	2.00E+00	1.00E+00	1.44E+03
3.70E+01	4.93E+00	2.00E+00	1.00E+00	1.44E+03
3.80E+01	4.33E+00	2.00E+00	1.00E+00	1.44E+03
3.90E+01	5.37E+00	2.00E+00	1.00E+00	1.44E+03
4.00E+01	6.57E+00	2.00E+00	1.00E+00	1.44E+03
4.10E+01	3.42E+00	2.00E+00	1.00E+00	1.44E+03
4.20E+01	7.61E+00	2.00E+00	1.00E+00	1.44E+03
4.30E+01	7.89E+00	2.00E+00	1.00E+00	1.44E+03
4.40E+01	7.61E+00	2.00E+00	1.00E+00	1.44E+03
4.50E+01	7.40E+00	2.00E+00	1.00E+00	1.44E+03
4.60E+01	6.69E+00	2.00E+00	1.00E+00	1.44E+03
4.70E+01	6.40E+00	2.00E+00	1.00E+00	1.44E+03
4.80E+01	7.73E+00	2.00E+00	1.00E+00	1.44E+03
4.90E+01	4.77E+00	2.00E+00	1.00E+00	1.44E+03
5.00E+01	8.69E+00	2.00E+00	1.00E+00	1.44E+03
5.10E+01	6.98E+00	2.00E+00	1.00E+00	1.44E+03
5.20E+01	6.98E+00	2.00E+00	1.00E+00	1.44E+03
5.30E+01	4.72E+00	2.00E+00	1.00E+00	1.44E+03
5.40E+01	1.02E+01	2.00E+00	1.00E+00	1.44E+03
5.50E+01	8.49E+00	2.00E+00	1.00E+00	1.44E+03
5.60E+01	7.02E+00	2.00E+00	1.00E+00	1.44E+03
5.70E+01	4.48E+00	2.00E+00	1.00E+00	1.44E+03
5.80E+01	5.71E+00	2.00E+00	1.00E+00	1.44E+03
5.90E+01	5.59E+00	2.00E+00	1.00E+00	1.44E+03
1.00E+00	7.15E+00	1.00E+00	2.00E+00	1.44E+03
2.00E+00	6.25E+00	1.00E+00	2.00E+00	1.44E+03
3.00E+00	2.86E+00	1.00E+00	2.00E+00	1.44E+03
4.00E+00	4.98E+00	1.00E+00	2.00E+00	1.44E+03
5.00E+00	9.55E+00	1.00E+00	2.00E+00	1.44E+03
6.00E+00	4.06E+00	1.00E+00	2.00E+00	1.44E+03
7.00E+00	5.61E+00	1.00E+00	2.00E+00	1.44E+03
8.00E+00	7.76E+00	1.00E+00	2.00E+00	1.44E+03
9.00E+00	9.37E+00	1.00E+00	2.00E+00	1.44E+03
1.00E+01	8.27E+00	1.00E+00	2.00E+00	1.44E+03
1.10E+01	6.62E+00	1.00E+00	2.00E+00	1.44E+03
1.20E+01	5.36E+00	1.00E+00	2.00E+00	1.44E+03
1.30E+01	4.43E+00	1.00E+00	2.00E+00	1.44E+03
1.40E+01	5.58E+00	1.00E+00	2.00E+00	1.44E+03
1.50E+01	5.35E+00	1.00E+00	2.00E+00	1.44E+03
1.60E+01	7.66E+00	1.00E+00	2.00E+00	1.44E+03
1.70E+01	3.58E+00	1.00E+00	2.00E+00	1.44E+03
1.80E+01	1.09E+01	1.00E+00	2.00E+00	1.44E+03
1.90E+01	8.29E+00	1.00E+00	2.00E+00	1.44E+03
2.00E+01	4.35E+00	1.00E+00	2.00E+00	1.44E+03
2.10E+01	6.17E+00	1.00E+00	2.00E+00	1.44E+03
2.20E+01	9.97E+00	1.00E+00	2.00E+00	1.44E+03
2.30E+01	5.66E+00	1.00E+00	2.00E+00	1.44E+03
2.40E+01	6.20E+00	1.00E+00	2.00E+00	1.44E+03
2.50E+01	6.37E+00	1.00E+00	2.00E+00	1.44E+03
2.60E+01	8.11E+00	1.00E+00	2.00E+00	1.44E+03
2.70E+01	9.81E+00	1.00E+00	2.00E+00	1.44E+03
2.80E+01	2.70E+00	1.00E+00	2.00E+00	1.44E+03
2.90E+01	2.46E+00	1.00E+00	2.00E+00	1.44E+03
3.00E+01	5.19E+00	2.00E+00	2.00E+00	1.44E+03
3.10E+01	9.99E-01	2.00E+00	2.00E+00	1.44E+03
3.20E+01	2.99E+00	2.00E+00	2.00E+00	1.44E+03
3.30E+01	4.80E+00	2.00E+00	2.00E+00	1.44E+03
3.40E+01	4.88E+00	2.00E+00	2.00E+00	1.44E+03
3.50E+01	7.05E+00	2.00E+00	2.00E+00	1.44E+03
3.60E+01	6.82E+00	2.00E+00	2.00E+00	1.44E+03
3.70E+01	2.75E+00	2.00E+00	2.00E+00	1.44E+03
3.80E+01	4.58E+00	2.00E+00	2.00E+00	1.44E+03
3.90E+01	3.23E+00	2.00E+00	2.00E+00	1.44E+03
4.00E+01	5.16E+00	2.00E+00	2.00E+00	1.44E+03
4.10E+01	7.66E+00	2.00E+00	2.00E+00	1.44E+03
4.20E+01	6.36E+00	2.00E+00	2.00E+00	1.44E+03
4.30E+01	8.12E+00	2.00E+00	2.00E+00	1.44E+03
4.40E+01	6.46E+00	2.00E+00	2.00E+00	1.44E+03
4.50E+01	6.68E+00	2.00E+00	2.00E+00	1.44E+03
4.60E+01	2.33E+00	2.00E+00	2.00E+00	1.44E+03
4.70E+01	2.21E+00	2.00E+00	2.00E+00	1.44E+03
4.80E+01	9.69E+00	2.00E+00	2.00E+00	1.44E+03
4.90E+01	6.14E+00	2.00E+00	2.00E+00	1.44E+03
5.00E+01	5.59E+00	2.00E+00	2.00E+00	1.44E+03
5.10E+01	6.58E+00	2.00E+00	2.00E+00	1.44E+03
5.20E+01	6.58E+00	2.00E+00	2.00E+00	1.44E+03
5.30E+01	5.62E+00	2.00E+00	2.00E+00	1.44E+03
5.40E+01	1.02E+01	2.00E+00	2.00E+00	1.44E+03
5.50E+01	8.13E+00	2.00E+00	2.00E+00	1.44E+03
5.60E+01	2.16E+00	2.00E+00	2.00E+00	1.44E+03
5.70E+01	2.87E+00	2.00E+00	2.00E+00	1.44E+03
5.80E+01	8.76E+00	2.00E+00	2.00E+00	1.44E+03
5.90E+01	5.23E+00	2.00E+00	2.00E+00	1.44E+03
1.00E+00	7.44E+00	1.00E+00	1.00E+00	1.43E+03
2.00E+00	7.96E+00	1.00E+00	1.00E+00	1.43E+03
3.00E+00	1.03E+00	1.00E+00	1.00E+00	1.43E+03
4.00E+00	3.14E+00	1.00E+00	1.00E+00	1.43E+03
5.00E+00	9.42E+00	1.00E+00	1.00E+00	1.43E+03
6.00E+00	3.42E+00	1.00E+00	1.00E+00	1.43E+03
7.00E+00	6.44E+00	1.00E+00	1.00E+00	1.43E+03
8.00E+00	7.63E+00	1.00E+00	1.00E+00	1.43E+03
9.00E+00	8.07E+00	1.00E+00	1.00E+00	1.43E+03
1.00E+01	5.92E+00	1.00E+00	1.00E+00	1.43E+03
1.10E+01	5.11E+00	1.00E+00	1.00E+00	1.43E+03
1.20E+01	4.91E+00	1.00E+00	1.00E+00	1.43E+03
1.30E+01	2.60E+00	1.00E+00	1.00E+00	1.43E+03
1.40E+01	3.46E+00	1.00E+00	1.00E+00	1.43E+03
1.50E+01	6.73E+00	1.00E+00	1.00E+00	1.43E+03
1.60E+01	9.67E+00	1.00E+00	1.00E+00	1.43E+03
1.70E+01	3.58E+00	1.00E+00	1.00E+00	1.43E+03
1.80E+01	1.13E+01	1.00E+00	1.00E+00	1.43E+03
1.90E+01	8.03E+00	1.00E+00	1.00E+00	1.43E+03
2.00E+01	8.09E+00	1.00E+00	1.00E+00	1.43E+03
2.10E+01	7.18E+00	1.00E+00	1.00E+00	1.43E+03
2.20E+01	8.37E+00	1.00E+00	1.00E+00	1.43E+03
2.30E+01	5.40E+00	1.00E+00	1.00E+00	1.43E+03
2.40E+01	3.75E+00	1.00E+00	1.00E+00	1.43E+03
2.50E+01	6.19E+00	1.00E+00	1.00E+00	1.43E+03
2.60E+01	5.28E+00	1.00E+00	1.00E+00	1.43E+03
2.70E+01	1.03E+01	1.00E+00	1.00E+00	1.43E+03
2.80E+01	6.51E+00	1.00E+00	1.00E+00	1.43E+03
2.90E+01	4.30E+00	1.00E+00	1.00E+00	1.43E+03
3.00E+01	3.39E+00	2.00E+00	1.00E+00	1.43E+03
3.10E+01	2.19E+00	2.00E+00	1.00E+00	1.43E+03
3.20E+01	4.81E+00	2.00E+00	1.00E+00	1.43E+03
3.30E+01	3.31E+00	2.00E+00	1.00E+00	1.43E+03
3.40E+01	6.81E+00	2.00E+00	1.00E+00	1.43E+03
3.50E+01	6.33E+00	2.00E+00	1.00E+00	1.43E+03
3.60E+01	6.07E+00	2.00E+00	1.00E+00	1.43E+03
3.70E+01	3.81E+00	2.00E+00	1.00E+00	1.43E+03
3.80E+01	4.84E+00	2.00E+00	1.00E+00	1.43E+03
3.90E+01	3.90E+00	2.00E+00	1.00E+00	1.43E+03
4.00E+01	5.04E+00	2.00E+00	1.00E+00	1.43E+03
4.10E+01	4.07E+00	2.00E+00	1.00E+00	1.43E+03
4.20E+01	5.68E+00	2.00E+00	1.00E+00	1.43E+03
4.30E+01	7.64E+00	2.00E+00	1.00E+00	1.43E+03
4.40E+01	5.94E+00	2.00E+00	1.00E+00	1.43E+03
4.50E+01	8.27E+00	2.00E+00	1.00E+00	1.43E+03
4.60E+01	3.70E+00	2.00E+00	1.00E+00	1.43E+03
4.70E+01	4.55E+00	2.00E+00	1.00E+00	1.43E+03
4.80E+01	7.71E+00	2.00E+00	1.00E+00	1.43E+03
4.90E+01	5.95E+00	2.00E+00	1.00E+00	1.43E+03
5.00E+01	6.10E+00	2.00E+00	1.00E+00	1.43E+03
5.10E+01	8.18E+00	2.00E+00	1.00E+00	1.43E+03
5.20E+01	8.18E+00	2.00E+00	1.00E+00	1.43E+03
5.30E+01	4.83E+00	2.00E+00	1.00E+00	1.43E+03
5.40E+01	8.52E+00	2.00E+00	1.00E+00	1.43E+03
5.50E+01	5.08E+00	2.00E+00	1.00E+00	1.43E+03
5.60E+01	2.09E+00	2.00E+00	1.00E+00	1.43E+03
5.70E+01	2.21E+00	2.00E+00	1.00E+00	1.43E+03
5.80E+01	2.57E+00	2.00E+00	1.00E+00	1.43E+03
5.90E+01	6.71E+00	2.00E+00	1.00E+00	1.43E+03
1.00E+00	4.30E+00	1.00E+00	2.00E+00	1.43E+03
2.00E+00	7.23E+00	1.00E+00	2.00E+00	1.43E+03
3.00E+00	1.91E+00	1.00E+00	2.00E+00	1.43E+03
4.00E+00	4.68E+00	1.00E+00	2.00E+00	1.43E+03
5.00E+00	8.33E+00	1.00E+00	2.00E+00	1.43E+03
6.00E+00	6.12E+00	1.00E+00	2.00E+00	1.43E+03
7.00E+00	3.29E+00	1.00E+00	2.00E+00	1.43E+03
8.00E+00	9.42E+00	1.00E+00	2.00E+00	1.43E+03
9.00E+00	7.57E+00	1.00E+00	2.00E+00	1.43E+03
1.00E+01	5.02E+00	1.00E+00	2.00E+00	1.43E+03
1.10E+01	6.99E+00	1.00E+00	2.00E+00	1.43E+03
1.20E+01	5.53E+00	1.00E+00	2.00E+00	1.43E+03
1.30E+01	5.88E+00	1.00E+00	2.00E+00	1.43E+03
1.40E+01	2.90E+00	1.00E+00	2.00E+00	1.43E+03
1.50E+01	7.05E+00	1.00E+00	2.00E+00	1.43E+03
1.60E+01	7.70E+00	1.00E+00	2.00E+00	1.43E+03
1.70E+01	2.77E+00	1.00E+00	2.00E+00	1.43E+03
1.80E+01	9.86E+00	1.00E+00	2.00E+00	1.43E+03
1.90E+01	7.31E+00	1.00E+00	2.00E+00	1.43E+03
2.00E+01	6.72E+00	1.00E+00	2.00E+00	1.43E+03
2.10E+01	6.33E+00	1.00E+00	2.00E+00	1.43E+03
2.20E+01	6.34E+00	1.00E+00	2.00E+00	1.43E+03
2.30E+01	3.68E+00	1.00E+00	2.00E+00	1.43E+03
2.40E+01	9.55E+00	1.00E+00	2.00E+00	1.43E+03
2.50E+01	7.18E+00	1.00E+00	2.00E+00	1.43E+03
2.60E+01	5.28E+00	1.00E+00	2.00E+00	1.43E+03
2.70E+01	7.78E+00	1.00E+00	2.00E+00	1.43E+03
2.80E+01	4.47E+00	1.00E+00	2.00E+00	1.43E+03
2.90E+01	3.50E+00	1.00E+00	2.00E+00	1.43E+03
3.00E+01	4.94E+00	2.00E+00	2.00E+00	1.43E+03
3.10E+01	2.70E+00	2.00E+00	2.00E+00	1.43E+03
3.20E+01	5.28E+00	2.00E+00	2.00E+00	1.43E+03
3.30E+01	4.10E+00	2.00E+00	2.00E+00	1.43E+03
3.40E+01	7.89E+00	2.00E+00	2.00E+00	1.43E+03
3.50E+01	9.19E+00	2.00E+00	2.00E+00	1.43E+03
3.60E+01	6.81E+00	2.00E+00	2.00E+00	1.43E+03
3.70E+01	4.93E+00	2.00E+00	2.00E+00	1.43E+03
3.80E+01	4.49E+00	2.00E+00	2.00E+00	1.43E+03
3.90E+01	3.16E+00	2.00E+00	2.00E+00	1.43E+03
4.00E+01	3.20E+00	2.00E+00	2.00E+00	1.43E+03
4.10E+01	8.51E+00	2.00E+00	2.00E+00	1.43E+03
4.20E+01	6.14E+00	2.00E+00	2.00E+00	1.43E+03
4.30E+01	7.07E+00	2.00E+00	2.00E+00	1.43E+03
4.40E+01	5.25E+00	2.00E+00	2.00E+00	1.43E+03
4.50E+01	5.88E+00	2.00E+00	2.00E+00	1.43E+03
4.60E+01	1.12E+00	2.00E+00	2.00E+00	1.43E+03
4.70E+01	2.30E+00	2.00E+00	2.00E+00	1.43E+03
4.80E+01	1.05E+01	2.00E+00	2.00E+00	1.43E+03
4.90E+01	6.65E+00	2.00E+00	2.00E+00	1.43E+03
5.00E+01	2.63E+00	2.00E+00	2.00E+00	1.43E+03
5.10E+01	5.94E+00	2.00E+00	2.00E+00	1.43E+03
5.20E+01	5.94E+00	2.00E+00	2.00E+00	1.43E+03
5.30E+01	6.58E+00	2.00E+00	2.00E+00	1.43E+03
5.40E+01	9.53E+00	2.00E+00	2.00E+00	1.43E+03
5.50E+01	2.01E+00	2.00E+00	2.00E+00	1.43E+03
5.60E+01	5.48E+00	2.00E+00	2.00E+00	1.43E+03
5.70E+01	2.26E+00	2.00E+00	2.00E+00	1.43E+03
5.80E+01	8.19E+00	2.00E+00	2.00E+00	1.43E+03
5.90E+01	6.09E+00	2.00E+00	2.00E+00	1.43E+03
1.00E+00	1.02E+01	1.00E+00	1.00E+00	1.89E+03
2.00E+00	7.85E+00	1.00E+00	1.00E+00	1.89E+03
3.00E+00	6.02E+00	1.00E+00	1.00E+00	1.89E+03
4.00E+00	7.57E+00	1.00E+00	1.00E+00	1.89E+03
5.00E+00	9.46E+00	1.00E+00	1.00E+00	1.89E+03
6.00E+00	6.93E+00	1.00E+00	1.00E+00	1.89E+03
7.00E+00	9.18E+00	1.00E+00	1.00E+00	1.89E+03
8.00E+00	1.14E+01	1.00E+00	1.00E+00	1.89E+03
9.00E+00	7.46E+00	1.00E+00	1.00E+00	1.89E+03
1.00E+01	9.17E+00	1.00E+00	1.00E+00	1.89E+03
1.10E+01	8.46E+00	1.00E+00	1.00E+00	1.89E+03
1.20E+01	6.57E+00	1.00E+00	1.00E+00	1.89E+03
1.30E+01	8.10E+00	1.00E+00	1.00E+00	1.89E+03
1.40E+01	7.88E+00	1.00E+00	1.00E+00	1.89E+03
1.50E+01	9.25E+00	1.00E+00	1.00E+00	1.89E+03
1.60E+01	9.36E+00	1.00E+00	1.00E+00	1.89E+03
1.70E+01	9.31E+00	1.00E+00	1.00E+00	1.89E+03
1.80E+01	8.94E+00	1.00E+00	1.00E+00	1.89E+03
1.90E+01	8.98E+00	1.00E+00	1.00E+00	1.89E+03
2.00E+01	8.91E+00	1.00E+00	1.00E+00	1.89E+03
2.10E+01	8.13E+00	1.00E+00	1.00E+00	1.89E+03
2.20E+01	8.54E+00	1.00E+00	1.00E+00	1.89E+03
2.30E+01	1.15E+01	1.00E+00	1.00E+00	1.89E+03
2.40E+01	8.99E+00	1.00E+00	1.00E+00	1.89E+03
2.50E+01	9.37E+00	1.00E+00	1.00E+00	1.89E+03
2.60E+01	8.67E+00	1.00E+00	1.00E+00	1.89E+03
2.70E+01	1.04E+01	1.00E+00	1.00E+00	1.89E+03
2.80E+01	6.67E+00	1.00E+00	1.00E+00	1.89E+03
2.90E+01	8.00E+00	1.00E+00	1.00E+00	1.89E+03
3.00E+01	5.77E+00	2.00E+00	1.00E+00	1.89E+03
3.10E+01	7.79E+00	2.00E+00	1.00E+00	1.89E+03
3.20E+01	8.14E+00	2.00E+00	1.00E+00	1.89E+03
3.30E+01	7.26E+00	2.00E+00	1.00E+00	1.89E+03
3.40E+01	6.87E+00	2.00E+00	1.00E+00	1.89E+03
3.50E+01	7.94E+00	2.00E+00	1.00E+00	1.89E+03
3.60E+01	9.18E+00	2.00E+00	1.00E+00	1.89E+03
3.70E+01	6.97E+00	2.00E+00	1.00E+00	1.89E+03
3.80E+01	6.61E+00	2.00E+00	1.00E+00	1.89E+03
3.90E+01	6.66E+00	2.00E+00	1.00E+00	1.89E+03
4.00E+01	8.85E+00	2.00E+00	1.00E+00	1.89E+03
4.10E+01	7.56E+00	2.00E+00	1.00E+00	1.89E+03
4.20E+01	8.54E+00	2.00E+00	1.00E+00	1.89E+03
4.30E+01	7.57E+00	2.00E+00	1.00E+00	1.89E+03
4.40E+01	7.58E+00	2.00E+00	1.00E+00	1.89E+03
4.50E+01	9.50E+00	2.00E+00	1.00E+00	1.89E+03
4.60E+01	8.62E+00	2.00E+00	1.00E+00	1.89E+03
4.70E+01	5.56E+00	2.00E+00	1.00E+00	1.89E+03
4.80E+01	8.86E+00	2.00E+00	1.00E+00	1.89E+03
4.90E+01	9.53E+00	2.00E+00	1.00E+00	1.89E+03
5.00E+01	9.76E+00	2.00E+00	1.00E+00	1.89E+03
5.10E+01	9.32E+00	2.00E+00	1.00E+00	1.89E+03
5.20E+01	9.32E+00	2.00E+00	1.00E+00	1.89E+03
5.30E+01	9.33E+00	2.00E+00	1.00E+00	1.89E+03
5.40E+01	9.44E+00	2.00E+00	1.00E+00	1.89E+03
5.50E+01	7.56E+00	2.00E+00	1.00E+00	1.89E+03
5.60E+01	4.87E+00	2.00E+00	1.00E+00	1.89E+03
5.70E+01	7.71E+00	2.00E+00	1.00E+00	1.89E+03
5.80E+01	9.07E+00	2.00E+00	1.00E+00	1.89E+03
5.90E+01	7.60E+00	2.00E+00	1.00E+00	1.89E+03
1.00E+00	5.60E+00	1.00E+00	2.00E+00	1.89E+03
2.00E+00	1.05E+01	1.00E+00	2.00E+00	1.89E+03
3.00E+00	2.73E+00	1.00E+00	2.00E+00	1.89E+03
4.00E+00	5.89E+00	1.00E+00	2.00E+00	1.89E+03
5.00E+00	8.57E+00	1.00E+00	2.00E+00	1.89E+03
6.00E+00	8.71E+00	1.00E+00	2.00E+00	1.89E+03
7.00E+00	7.04E+00	1.00E+00	2.00E+00	1.89E+03
8.00E+00	8.61E+00	1.00E+00	2.00E+00	1.89E+03
9.00E+00	7.32E+00	1.00E+00	2.00E+00	1.89E+03
1.00E+01	7.13E+00	1.00E+00	2.00E+00	1.89E+03
1.10E+01	7.54E+00	1.00E+00	2.00E+00	1.89E+03
1.20E+01	6.66E+00	1.00E+00	2.00E+00	1.89E+03
1.30E+01	3.79E+00	1.00E+00	2.00E+00	1.89E+03
1.40E+01	3.49E+00	1.00E+00	2.00E+00	1.89E+03
1.50E+01	7.87E+00	1.00E+00	2.00E+00	1.89E+03
1.60E+01	7.77E+00	1.00E+00	2.00E+00	1.89E+03
1.70E+01	4.61E+00	1.00E+00	2.00E+00	1.89E+03
1.80E+01	1.03E+01	1.00E+00	2.00E+00	1.89E+03
1.90E+01	8.78E+00	1.00E+00	2.00E+00	1.89E+03
2.00E+01	6.05E+00	1.00E+00	2.00E+00	1.89E+03
2.10E+01	7.93E+00	1.00E+00	2.00E+00	1.89E+03
2.20E+01	5.77E+00	1.00E+00	2.00E+00	1.89E+03
2.30E+01	7.91E+00	1.00E+00	2.00E+00	1.89E+03
2.40E+01	4.98E+00	1.00E+00	2.00E+00	1.89E+03
2.50E+01	6.99E+00	1.00E+00	2.00E+00	1.89E+03
2.60E+01	5.90E+00	1.00E+00	2.00E+00	1.89E+03
2.70E+01	9.27E+00	1.00E+00	2.00E+00	1.89E+03
2.80E+01	3.69E+00	1.00E+00	2.00E+00	1.89E+03
2.90E+01	2.82E+00	1.00E+00	2.00E+00	1.89E+03
3.00E+01	1.32E+00	2.00E+00	2.00E+00	1.89E+03
3.10E+01	1.47E+00	2.00E+00	2.00E+00	1.89E+03
3.20E+01	4.02E+00	2.00E+00	2.00E+00	1.89E+03
3.30E+01	5.68E+00	2.00E+00	2.00E+00	1.89E+03
3.40E+01	3.11E+00	2.00E+00	2.00E+00	1.89E+03
3.50E+01	5.25E+00	2.00E+00	2.00E+00	1.89E+03
3.60E+01	3.83E+00	2.00E+00	2.00E+00	1.89E+03
3.70E+01	4.36E+00	2.00E+00	2.00E+00	1.89E+03
3.80E+01	3.89E+00	2.00E+00	2.00E+00	1.89E+03
3.90E+01	2.07E+00	2.00E+00	2.00E+00	1.89E+03
4.00E+01	6.82E+00	2.00E+00	2.00E+00	1.89E+03
4.10E+01	1.13E+01	2.00E+00	2.00E+00	1.89E+03
4.20E+01	7.01E+00	2.00E+00	2.00E+00	1.89E+03
4.30E+01	4.73E+00	2.00E+00	2.00E+00	1.89E+03
4.40E+01	5.12E+00	2.00E+00	2.00E+00	1.89E+03
4.50E+01	9.29E+00	2.00E+00	2.00E+00	1.89E+03
4.60E+01	2.42E+00	2.00E+00	2.00E+00	1.89E+03
4.70E+01	3.38E+00	2.00E+00	2.00E+00	1.89E+03
4.80E+01	7.59E+00	2.00E+00	2.00E+00	1.89E+03
4.90E+01	6.17E+00	2.00E+00	2.00E+00	1.89E+03
5.00E+01	4.25E+00	2.00E+00	2.00E+00	1.89E+03
5.10E+01	8.03E+00	2.00E+00	2.00E+00	1.89E+03
5.20E+01	8.03E+00	2.00E+00	2.00E+00	1.89E+03
5.30E+01	6.96E+00	2.00E+00	2.00E+00	1.89E+03
5.40E+01	9.95E+00	2.00E+00	2.00E+00	1.89E+03
5.50E+01	2.82E+00	2.00E+00	2.00E+00	1.89E+03
5.60E+01	4.43E+00	2.00E+00	2.00E+00	1.89E+03
5.70E+01	2.33E+00	2.00E+00	2.00E+00	1.89E+03
5.80E+01	5.02E+00	2.00E+00	2.00E+00	1.89E+03
5.90E+01	9.82E+00	2.00E+00	2.00E+00	1.89E+03
1.00E+00	7.07E+00	1.00E+00	1.00E+00	1.84E+03
2.00E+00	7.97E+00	1.00E+00	1.00E+00	1.84E+03
3.00E+00	4.92E+00	1.00E+00	1.00E+00	1.84E+03
4.00E+00	7.30E+00	1.00E+00	1.00E+00	1.84E+03
5.00E+00	8.99E+00	1.00E+00	1.00E+00	1.84E+03
6.00E+00	9.25E+00	1.00E+00	1.00E+00	1.84E+03
7.00E+00	9.14E+00	1.00E+00	1.00E+00	1.84E+03
8.00E+00	5.43E+00	1.00E+00	1.00E+00	1.84E+03
9.00E+00	7.44E+00	1.00E+00	1.00E+00	1.84E+03
1.00E+01	5.34E+00	1.00E+00	1.00E+00	1.84E+03
1.10E+01	7.27E+00	1.00E+00	1.00E+00	1.84E+03
1.20E+01	8.98E+00	1.00E+00	1.00E+00	1.84E+03
1.30E+01	8.74E+00	1.00E+00	1.00E+00	1.84E+03
1.40E+01	6.93E+00	1.00E+00	1.00E+00	1.84E+03
1.50E+01	8.97E+00	1.00E+00	1.00E+00	1.84E+03
1.60E+01	9.92E+00	1.00E+00	1.00E+00	1.84E+03
1.70E+01	8.83E+00	1.00E+00	1.00E+00	1.84E+03
1.80E+01	2.77E+00	1.00E+00	1.00E+00	1.84E+03
1.90E+01	8.73E+00	1.00E+00	1.00E+00	1.84E+03
2.00E+01	9.90E+00	1.00E+00	1.00E+00	1.84E+03
2.10E+01	8.03E+00	1.00E+00	1.00E+00	1.84E+03
2.20E+01	9.04E+00	1.00E+00	1.00E+00	1.84E+03
2.30E+01	7.07E+00	1.00E+00	1.00E+00	1.84E+03
2.40E+01	9.78E+00	1.00E+00	1.00E+00	1.84E+03
2.50E+01	7.58E+00	1.00E+00	1.00E+00	1.84E+03
2.60E+01	8.31E+00	1.00E+00	1.00E+00	1.84E+03
2.70E+01	9.45E+00	1.00E+00	1.00E+00	1.84E+03
2.80E+01	8.71E+00	1.00E+00	1.00E+00	1.84E+03
2.90E+01	7.90E+00	1.00E+00	1.00E+00	1.84E+03
3.00E+01	6.36E+00	2.00E+00	1.00E+00	1.84E+03
3.10E+01	7.55E+00	2.00E+00	1.00E+00	1.84E+03
3.20E+01	7.66E+00	2.00E+00	1.00E+00	1.84E+03
3.30E+01	5.19E+00	2.00E+00	1.00E+00	1.84E+03
3.40E+01	8.08E+00	2.00E+00	1.00E+00	1.84E+03
3.50E+01	7.80E+00	2.00E+00	1.00E+00	1.84E+03
3.60E+01	4.57E+00	2.00E+00	1.00E+00	1.84E+03
3.70E+01	6.94E+00	2.00E+00	1.00E+00	1.84E+03
3.80E+01	7.25E+00	2.00E+00	1.00E+00	1.84E+03
3.90E+01	8.89E+00	2.00E+00	1.00E+00	1.84E+03
4.00E+01	3.71E+00	2.00E+00	1.00E+00	1.84E+03
4.10E+01	5.05E+00	2.00E+00	1.00E+00	1.84E+03
4.20E+01	7.26E+00	2.00E+00	1.00E+00	1.84E+03
4.30E+01	8.15E+00	2.00E+00	1.00E+00	1.84E+03
4.40E+01	6.35E+00	2.00E+00	1.00E+00	1.84E+03
4.50E+01	8.37E+00	2.00E+00	1.00E+00	1.84E+03
4.60E+01	6.95E+00	2.00E+00	1.00E+00	1.84E+03
4.70E+01	8.95E+00	2.00E+00	1.00E+00	1.84E+03
4.80E+01	9.54E+00	2.00E+00	1.00E+00	1.84E+03
4.90E+01	1.02E+01	2.00E+00	1.00E+00	1.84E+03
5.00E+01	9.31E+00	2.00E+00	1.00E+00	1.84E+03
5.10E+01	9.19E+00	2.00E+00	1.00E+00	1.84E+03
5.20E+01	9.19E+00	2.00E+00	1.00E+00	1.84E+03
5.30E+01	7.34E+00	2.00E+00	1.00E+00	1.84E+03
5.40E+01	7.80E+00	2.00E+00	1.00E+00	1.84E+03
5.50E+01	6.01E+00	2.00E+00	1.00E+00	1.84E+03
5.60E+01	7.58E+00	2.00E+00	1.00E+00	1.84E+03
5.70E+01	7.79E+00	2.00E+00	1.00E+00	1.84E+03
5.80E+01	9.79E+00	2.00E+00	1.00E+00	1.84E+03
5.90E+01	1.03E+01	2.00E+00	1.00E+00	1.84E+03
1.00E+00	4.89E+00	1.00E+00	2.00E+00	1.84E+03
2.00E+00	6.69E+00	1.00E+00	2.00E+00	1.84E+03
3.00E+00	4.02E+00	1.00E+00	2.00E+00	1.84E+03
4.00E+00	3.11E+00	1.00E+00	2.00E+00	1.84E+03
5.00E+00	9.13E+00	1.00E+00	2.00E+00	1.84E+03
6.00E+00	4.14E+00	1.00E+00	2.00E+00	1.84E+03
7.00E+00	7.36E+00	1.00E+00	2.00E+00	1.84E+03
8.00E+00	7.63E+00	1.00E+00	2.00E+00	1.84E+03
9.00E+00	8.62E+00	1.00E+00	2.00E+00	1.84E+03
1.00E+01	6.66E+00	1.00E+00	2.00E+00	1.84E+03
1.10E+01	7.79E+00	1.00E+00	2.00E+00	1.84E+03
1.20E+01	5.21E+00	1.00E+00	2.00E+00	1.84E+03
1.30E+01	2.01E+00	1.00E+00	2.00E+00	1.84E+03
1.40E+01	3.04E+00	1.00E+00	2.00E+00	1.84E+03
1.50E+01	8.68E+00	1.00E+00	2.00E+00	1.84E+03
1.60E+01	6.95E+00	1.00E+00	2.00E+00	1.84E+03
1.70E+01	3.82E+00	1.00E+00	2.00E+00	1.84E+03
1.80E+01	9.79E+00	1.00E+00	2.00E+00	1.84E+03
1.90E+01	9.87E+00	1.00E+00	2.00E+00	1.84E+03
2.00E+01	7.04E+00	1.00E+00	2.00E+00	1.84E+03
2.10E+01	1.07E+01	1.00E+00	2.00E+00	1.84E+03
2.20E+01	8.69E+00	1.00E+00	2.00E+00	1.84E+03
2.30E+01	4.96E+00	1.00E+00	2.00E+00	1.84E+03
2.40E+01	2.71E+00	1.00E+00	2.00E+00	1.84E+03
2.50E+01	6.89E+00	1.00E+00	2.00E+00	1.84E+03
2.60E+01	5.31E+00	1.00E+00	2.00E+00	1.84E+03
2.70E+01	8.42E+00	1.00E+00	2.00E+00	1.84E+03
2.80E+01	5.95E+00	1.00E+00	2.00E+00	1.84E+03
2.90E+01	2.93E+00	1.00E+00	2.00E+00	1.84E+03
3.00E+01	6.04E+00	2.00E+00	2.00E+00	1.84E+03
3.10E+01	1.16E+00	2.00E+00	2.00E+00	1.84E+03
3.20E+01	5.04E+00	2.00E+00	2.00E+00	1.84E+03
3.30E+01	3.23E+00	2.00E+00	2.00E+00	1.84E+03
3.40E+01	5.55E+00	2.00E+00	2.00E+00	1.84E+03
3.50E+01	7.00E+00	2.00E+00	2.00E+00	1.84E+03
3.60E+01	6.69E+00	2.00E+00	2.00E+00	1.84E+03
3.70E+01	2.18E+00	2.00E+00	2.00E+00	1.84E+03
3.80E+01	2.76E+00	2.00E+00	2.00E+00	1.84E+03
3.90E+01	2.89E+00	2.00E+00	2.00E+00	1.84E+03
4.00E+01	3.32E+00	2.00E+00	2.00E+00	1.84E+03
4.10E+01	9.83E+00	2.00E+00	2.00E+00	1.84E+03
4.20E+01	5.78E+00	2.00E+00	2.00E+00	1.84E+03
4.30E+01	9.75E+00	2.00E+00	2.00E+00	1.84E+03
4.40E+01	7.40E+00	2.00E+00	2.00E+00	1.84E+03
4.50E+01	8.87E+00	2.00E+00	2.00E+00	1.84E+03
4.60E+01	1.30E+00	2.00E+00	2.00E+00	1.84E+03
4.70E+01	3.13E+00	2.00E+00	2.00E+00	1.84E+03
4.80E+01	9.60E+00	2.00E+00	2.00E+00	1.84E+03
4.90E+01	4.21E+00	2.00E+00	2.00E+00	1.84E+03
5.00E+01	3.78E+00	2.00E+00	2.00E+00	1.84E+03
5.10E+01	4.98E+00	2.00E+00	2.00E+00	1.84E+03
5.20E+01	4.98E+00	2.00E+00	2.00E+00	1.84E+03
5.30E+01	4.39E+00	2.00E+00	2.00E+00	1.84E+03
5.40E+01	8.99E+00	2.00E+00	2.00E+00	1.84E+03
5.50E+01	1.05E+01	2.00E+00	2.00E+00	1.84E+03
5.60E+01	3.59E+00	2.00E+00	2.00E+00	1.84E+03
5.70E+01	1.59E+00	2.00E+00	2.00E+00	1.84E+03
5.80E+01	8.71E+00	2.00E+00	2.00E+00	1.84E+03
5.90E+01	8.73E+00	2.00E+00	2.00E+00	1.84E+03
1.00E+00	8.72E+00	1.00E+00	1.00E+00	1.79E+03
2.00E+00	8.34E+00	1.00E+00	1.00E+00	1.79E+03
3.00E+00	4.13E+00	1.00E+00	1.00E+00	1.79E+03
4.00E+00	7.32E+00	1.00E+00	1.00E+00	1.79E+03
5.00E+00	7.75E+00	1.00E+00	1.00E+00	1.79E+03
6.00E+00	7.61E+00	1.00E+00	1.00E+00	1.79E+03
7.00E+00	7.95E+00	1.00E+00	1.00E+00	1.79E+03
8.00E+00	7.51E+00	1.00E+00	1.00E+00	1.79E+03
9.00E+00	8.80E+00	1.00E+00	1.00E+00	1.79E+03
1.00E+01	8.71E+00	1.00E+00	1.00E+00	1.79E+03
1.10E+01	9.90E+00	1.00E+00	1.00E+00	1.79E+03
1.20E+01	7.39E+00	1.00E+00	1.00E+00	1.79E+03
1.30E+01	7.60E+00	1.00E+00	1.00E+00	1.79E+03
1.40E+01	5.77E+00	1.00E+00	1.00E+00	1.79E+03
1.50E+01	7.95E+00	1.00E+00	1.00E+00	1.79E+03
1.60E+01	6.18E+00	1.00E+00	1.00E+00	1.79E+03
1.70E+01	6.77E+00	1.00E+00	1.00E+00	1.79E+03
1.80E+01	9.96E+00	1.00E+00	1.00E+00	1.79E+03
1.90E+01	9.98E+00	1.00E+00	1.00E+00	1.79E+03
2.00E+01	8.02E+00	1.00E+00	1.00E+00	1.79E+03
2.10E+01	5.32E+00	1.00E+00	1.00E+00	1.79E+03
2.20E+01	9.72E+00	1.00E+00	1.00E+00	1.79E+03
2.30E+01	9.02E+00	1.00E+00	1.00E+00	1.79E+03
2.40E+01	6.80E+00	1.00E+00	1.00E+00	1.79E+03
2.50E+01	6.52E+00	1.00E+00	1.00E+00	1.79E+03
2.60E+01	6.81E+00	1.00E+00	1.00E+00	1.79E+03
2.70E+01	1.19E+01	1.00E+00	1.00E+00	1.79E+03
2.80E+01	7.57E+00	1.00E+00	1.00E+00	1.79E+03
2.90E+01	4.73E+00	1.00E+00	1.00E+00	1.79E+03
3.00E+01	4.84E+00	2.00E+00	1.00E+00	1.79E+03
3.10E+01	6.18E+00	2.00E+00	1.00E+00	1.79E+03
3.20E+01	6.32E+00	2.00E+00	1.00E+00	1.79E+03
3.30E+01	5.48E+00	2.00E+00	1.00E+00	1.79E+03
3.40E+01	7.17E+00	2.00E+00	1.00E+00	1.79E+03
3.50E+01	8.69E+00	2.00E+00	1.00E+00	1.79E+03
3.60E+01	6.62E+00	2.00E+00	1.00E+00	1.79E+03
3.70E+01	6.03E+00	2.00E+00	1.00E+00	1.79E+03
3.80E+01	7.85E+00	2.00E+00	1.00E+00	1.79E+03
3.90E+01	5.97E+00	2.00E+00	1.00E+00	1.79E+03
4.00E+01	6.73E+00	2.00E+00	1.00E+00	1.79E+03
4.10E+01	7.33E+00	2.00E+00	1.00E+00	1.79E+03
4.20E+01	7.13E+00	2.00E+00	1.00E+00	1.79E+03
4.30E+01	7.91E+00	2.00E+00	1.00E+00	1.79E+03
4.40E+01	8.38E+00	2.00E+00	1.00E+00	1.79E+03
4.50E+01	8.61E+00	2.00E+00	1.00E+00	1.79E+03
4.60E+01	7.75E+00	2.00E+00	1.00E+00	1.79E+03
4.70E+01	6.55E+00	2.00E+00	1.00E+00	1.79E+03
4.80E+01	6.68E+00	2.00E+00	1.00E+00	1.79E+03
4.90E+01	5.57E+00	2.00E+00	1.00E+00	1.79E+03
5.00E+01	6.77E+00	2.00E+00	1.00E+00	1.79E+03
5.10E+01	9.23E+00	2.00E+00	1.00E+00	1.79E+03
5.20E+01	9.23E+00	2.00E+00	1.00E+00	1.79E+03
5.30E+01	5.20E+00	2.00E+00	1.00E+00	1.79E+03
5.40E+01	7.45E+00	2.00E+00	1.00E+00	1.79E+03
5.50E+01	8.21E+00	2.00E+00	1.00E+00	1.79E+03
5.60E+01	6.68E+00	2.00E+00	1.00E+00	1.79E+03
5.70E+01	5.56E+00	2.00E+00	1.00E+00	1.79E+03
5.80E+01	7.97E+00	2.00E+00	1.00E+00	1.79E+03
5.90E+01	9.88E+00	2.00E+00	1.00E+00	1.79E+03
1.00E+00	6.86E+00	1.00E+00	2.00E+00	1.79E+03
2.00E+00	7.69E+00	1.00E+00	2.00E+00	1.79E+03
3.00E+00	4.05E+00	1.00E+00	2.00E+00	1.79E+03
4.00E+00	4.32E+00	1.00E+00	2.00E+00	1.79E+03
5.00E+00	6.09E+00	1.00E+00	2.00E+00	1.79E+03
6.00E+00	8.05E+00	1.00E+00	2.00E+00	1.79E+03
7.00E+00	4.18E+00	1.00E+00	2.00E+00	1.79E+03
8.00E+00	5.33E+00	1.00E+00	2.00E+00	1.79E+03
9.00E+00	1.02E+01	1.00E+00	2.00E+00	1.79E+03
1.00E+01	5.78E+00	1.00E+00	2.00E+00	1.79E+03
1.10E+01	8.01E+00	1.00E+00	2.00E+00	1.79E+03
1.20E+01	5.85E+00	1.00E+00	2.00E+00	1.79E+03
1.30E+01	3.04E+00	1.00E+00	2.00E+00	1.79E+03
1.40E+01	2.67E+00	1.00E+00	2.00E+00	1.79E+03
1.50E+01	6.39E+00	1.00E+00	2.00E+00	1.79E+03
1.60E+01	6.79E+00	1.00E+00	2.00E+00	1.79E+03
1.70E+01	2.38E+00	1.00E+00	2.00E+00	1.79E+03
1.80E+01	1.14E+01	1.00E+00	2.00E+00	1.79E+03
1.90E+01	9.77E+00	1.00E+00	2.00E+00	1.79E+03
2.00E+01	9.71E+00	1.00E+00	2.00E+00	1.79E+03
2.10E+01	6.94E+00	1.00E+00	2.00E+00	1.79E+03
2.20E+01	6.53E+00	1.00E+00	2.00E+00	1.79E+03
2.30E+01	6.12E+00	1.00E+00	2.00E+00	1.79E+03
2.40E+01	6.62E+00	1.00E+00	2.00E+00	1.79E+03
2.50E+01	7.45E+00	1.00E+00	2.00E+00	1.79E+03
2.60E+01	3.57E+00	1.00E+00	2.00E+00	1.79E+03
2.70E+01	6.94E+00	1.00E+00	2.00E+00	1.79E+03
2.80E+01	6.76E+00	1.00E+00	2.00E+00	1.79E+03
2.90E+01	4.13E+00	1.00E+00	2.00E+00	1.79E+03
3.00E+01	3.50E+00	2.00E+00	2.00E+00	1.79E+03
3.10E+01	9.02E-01	2.00E+00	2.00E+00	1.79E+03
3.20E+01	3.70E+00	2.00E+00	2.00E+00	1.79E+03
3.30E+01	4.04E+00	2.00E+00	2.00E+00	1.79E+03
3.40E+01	4.99E+00	2.00E+00	2.00E+00	1.79E+03
3.50E+01	7.48E+00	2.00E+00	2.00E+00	1.79E+03
3.60E+01	8.52E+00	2.00E+00	2.00E+00	1.79E+03
3.70E+01	3.65E+00	2.00E+00	2.00E+00	1.79E+03
3.80E+01	3.16E+00	2.00E+00	2.00E+00	1.79E+03
3.90E+01	4.84E+00	2.00E+00	2.00E+00	1.79E+03
4.00E+01	3.18E+00	2.00E+00	2.00E+00	1.79E+03
4.10E+01	8.39E+00	2.00E+00	2.00E+00	1.79E+03
4.20E+01	5.82E+00	2.00E+00	2.00E+00	1.79E+03
4.30E+01	6.47E+00	2.00E+00	2.00E+00	1.79E+03
4.40E+01	5.96E+00	2.00E+00	2.00E+00	1.79E+03
4.50E+01	5.78E+00	2.00E+00	2.00E+00	1.79E+03
4.60E+01	3.19E+00	2.00E+00	2.00E+00	1.79E+03
4.70E+01	1.24E+00	2.00E+00	2.00E+00	1.79E+03
4.80E+01	5.01E+00	2.00E+00	2.00E+00	1.79E+03
4.90E+01	3.21E+00	2.00E+00	2.00E+00	1.79E+03
5.00E+01	2.01E+00	2.00E+00	2.00E+00	1.79E+03
5.10E+01	7.54E+00	2.00E+00	2.00E+00	1.79E+03
5.20E+01	7.54E+00	2.00E+00	2.00E+00	1.79E+03
5.30E+01	8.82E+00	2.00E+00	2.00E+00	1.79E+03
5.40E+01	6.75E+00	2.00E+00	2.00E+00	1.79E+03
5.50E+01	5.13E+00	2.00E+00	2.00E+00	1.79E+03
5.60E+01	4.53E+00	2.00E+00	2.00E+00	1.79E+03
5.70E+01	1.50E+00	2.00E+00	2.00E+00	1.79E+03
5.80E+01	9.39E+00	2.00E+00	2.00E+00	1.79E+03
5.90E+01	6.04E+00	2.00E+00	2.00E+00	1.79E+03
1.00E+00	5.93E+00	1.00E+00	1.00E+00	1.74E+03
2.00E+00	7.22E+00	1.00E+00	1.00E+00	1.74E+03
3.00E+00	3.59E+00	1.00E+00	1.00E+00	1.74E+03
4.00E+00	5.28E+00	1.00E+00	1.00E+00	1.74E+03
5.00E+00	6.66E+00	1.00E+00	1.00E+00	1.74E+03
6.00E+00	4.97E+00	1.00E+00	1.00E+00	1.74E+03
7.00E+00	7.44E+00	1.00E+00	1.00E+00	1.74E+03
8.00E+00	6.48E+00	1.00E+00	1.00E+00	1.74E+03
9.00E+00	9.80E+00	1.00E+00	1.00E+00	1.74E+03
1.00E+01	7.59E+00	1.00E+00	1.00E+00	1.74E+03
1.10E+01	7.61E+00	1.00E+00	1.00E+00	1.74E+03
1.20E+01	5.59E+00	1.00E+00	1.00E+00	1.74E+03
1.30E+01	6.10E+00	1.00E+00	1.00E+00	1.74E+03
1.40E+01	5.49E+00	1.00E+00	1.00E+00	1.74E+03
1.50E+01	5.81E+00	1.00E+00	1.00E+00	1.74E+03
1.60E+01	8.36E+00	1.00E+00	1.00E+00	1.74E+03
1.70E+01	4.21E+00	1.00E+00	1.00E+00	1.74E+03
1.80E+01	1.03E+01	1.00E+00	1.00E+00	1.74E+03
1.90E+01	9.01E+00	1.00E+00	1.00E+00	1.74E+03
2.00E+01	7.71E+00	1.00E+00	1.00E+00	1.74E+03
2.10E+01	8.05E+00	1.00E+00	1.00E+00	1.74E+03
2.20E+01	7.22E+00	1.00E+00	1.00E+00	1.74E+03
2.30E+01	8.75E+00	1.00E+00	1.00E+00	1.74E+03
2.40E+01	5.32E+00	1.00E+00	1.00E+00	1.74E+03
2.50E+01	8.97E+00	1.00E+00	1.00E+00	1.74E+03
2.60E+01	7.32E+00	1.00E+00	1.00E+00	1.74E+03
2.70E+01	9.77E+00	1.00E+00	1.00E+00	1.74E+03
2.80E+01	5.68E+00	1.00E+00	1.00E+00	1.74E+03
2.90E+01	5.00E+00	1.00E+00	1.00E+00	1.74E+03
3.00E+01	3.53E+00	2.00E+00	1.00E+00	1.74E+03
3.10E+01	4.49E+00	2.00E+00	1.00E+00	1.74E+03
3.20E+01	4.81E+00	2.00E+00	1.00E+00	1.74E+03
3.30E+01	4.83E+00	2.00E+00	1.00E+00	1.74E+03
3.40E+01	6.51E+00	2.00E+00	1.00E+00	1.74E+03
3.50E+01	8.47E+00	2.00E+00	1.00E+00	1.74E+03
3.60E+01	5.49E+00	2.00E+00	1.00E+00	1.74E+03
3.70E+01	5.01E+00	2.00E+00	1.00E+00	1.74E+03
3.80E+01	6.03E+00	2.00E+00	1.00E+00	1.74E+03
3.90E+01	6.05E+00	2.00E+00	1.00E+00	1.74E+03
4.00E+01	4.30E+00	2.00E+00	1.00E+00	1.74E+03
4.10E+01	8.88E+00	2.00E+00	1.00E+00	1.74E+03
4.20E+01	5.06E+00	2.00E+00	1.00E+00	1.74E+03
4.30E+01	7.20E+00	2.00E+00	1.00E+00	1.74E+03
4.40E+01	5.30E+00	2.00E+00	1.00E+00	1.74E+03
4.50E+01	5.78E+00	2.00E+00	1.00E+00	1.74E+03
4.60E+01	6.88E+00	2.00E+00	1.00E+00	1.74E+03
4.70E+01	6.32E+00	2.00E+00	1.00E+00	1.74E+03
4.80E+01	4.75E+00	2.00E+00	1.00E+00	1.74E+03
4.90E+01	5.23E+00	2.00E+00	1.00E+00	1.74E+03
5.00E+01	3.78E+00	2.00E+00	1.00E+00	1.74E+03
5.10E+01	6.26E+00	2.00E+00	1.00E+00	1.74E+03
5.20E+01	6.26E+00	2.00E+00	1.00E+00	1.74E+03
5.30E+01	5.01E+00	2.00E+00	1.00E+00	1.74E+03
5.40E+01	9.50E+00	2.00E+00	1.00E+00	1.74E+03
5.50E+01	6.12E+00	2.00E+00	1.00E+00	1.74E+03
5.60E+01	5.38E+00	2.00E+00	1.00E+00	1.74E+03
5.70E+01	4.10E+00	2.00E+00	1.00E+00	1.74E+03
5.80E+01	6.62E+00	2.00E+00	1.00E+00	1.74E+03
5.90E+01	8.56E+00	2.00E+00	1.00E+00	1.74E+03
1.00E+00	5.88E+00	1.00E+00	2.00E+00	1.74E+03
2.00E+00	9.84E+00	1.00E+00	2.00E+00	1.74E+03
3.00E+00	1.07E+00	1.00E+00	2.00E+00	1.74E+03
4.00E+00	4.50E+00	1.00E+00	2.00E+00	1.74E+03
5.00E+00	7.88E+00	1.00E+00	2.00E+00	1.74E+03
6.00E+00	7.32E+00	1.00E+00	2.00E+00	1.74E+03
7.00E+00	4.25E+00	1.00E+00	2.00E+00	1.74E+03
8.00E+00	5.85E+00	1.00E+00	2.00E+00	1.74E+03
9.00E+00	6.55E+00	1.00E+00	2.00E+00	1.74E+03
1.00E+01	5.01E+00	1.00E+00	2.00E+00	1.74E+03
1.10E+01	6.58E+00	1.00E+00	2.00E+00	1.74E+03
1.20E+01	3.89E+00	1.00E+00	2.00E+00	1.74E+03
1.30E+01	3.31E+00	1.00E+00	2.00E+00	1.74E+03
1.40E+01	2.59E+00	1.00E+00	2.00E+00	1.74E+03
1.50E+01	7.37E+00	1.00E+00	2.00E+00	1.74E+03
1.60E+01	6.53E+00	1.00E+00	2.00E+00	1.74E+03
1.70E+01	5.82E-01	1.00E+00	2.00E+00	1.74E+03
1.80E+01	1.18E+01	1.00E+00	2.00E+00	1.74E+03
1.90E+01	9.08E+00	1.00E+00	2.00E+00	1.74E+03
2.00E+01	6.66E+00	1.00E+00	2.00E+00	1.74E+03
2.10E+01	7.66E+00	1.00E+00	2.00E+00	1.74E+03
2.20E+01	1.01E+01	1.00E+00	2.00E+00	1.74E+03
2.30E+01	6.78E+00	1.00E+00	2.00E+00	1.74E+03
2.40E+01	1.04E+01	1.00E+00	2.00E+00	1.74E+03
2.50E+01	7.31E+00	1.00E+00	2.00E+00	1.74E+03
2.60E+01	7.74E+00	1.00E+00	2.00E+00	1.74E+03
2.70E+01	9.17E+00	1.00E+00	2.00E+00	1.74E+03
2.80E+01	3.84E+00	1.00E+00	2.00E+00	1.74E+03
2.90E+01	4.49E+00	1.00E+00	2.00E+00	1.74E+03
3.00E+01	3.78E+00	2.00E+00	2.00E+00	1.74E+03
3.10E+01	4.31E-01	2.00E+00	2.00E+00	1.74E+03
3.20E+01	2.17E+00	2.00E+00	2.00E+00	1.74E+03
3.30E+01	4.92E+00	2.00E+00	2.00E+00	1.74E+03
3.40E+01	4.30E+00	2.00E+00	2.00E+00	1.74E+03
3.50E+01	7.54E+00	2.00E+00	2.00E+00	1.74E+03
3.60E+01	7.44E+00	2.00E+00	2.00E+00	1.74E+03
3.70E+01	1.78E+00	2.00E+00	2.00E+00	1.74E+03
3.80E+01	3.61E+00	2.00E+00	2.00E+00	1.74E+03
3.90E+01	4.22E+00	2.00E+00	2.00E+00	1.74E+03
4.00E+01	5.11E+00	2.00E+00	2.00E+00	1.74E+03
4.10E+01	1.09E+01	2.00E+00	2.00E+00	1.74E+03
4.20E+01	8.79E+00	2.00E+00	2.00E+00	1.74E+03
4.30E+01	1.06E+01	2.00E+00	2.00E+00	1.74E+03
4.40E+01	5.49E+00	2.00E+00	2.00E+00	1.74E+03
4.50E+01	9.47E+00	2.00E+00	2.00E+00	1.74E+03
4.60E+01	2.42E+00	2.00E+00	2.00E+00	1.74E+03
4.70E+01	1.60E+00	2.00E+00	2.00E+00	1.74E+03
4.80E+01	7.66E+00	2.00E+00	2.00E+00	1.74E+03
4.90E+01	3.33E+00	2.00E+00	2.00E+00	1.74E+03
5.00E+01	3.23E+00	2.00E+00	2.00E+00	1.74E+03
5.10E+01	6.09E+00	2.00E+00	2.00E+00	1.74E+03
5.20E+01	6.09E+00	2.00E+00	2.00E+00	1.74E+03
5.30E+01	8.58E+00	2.00E+00	2.00E+00	1.74E+03
5.40E+01	6.61E+00	2.00E+00	2.00E+00	1.74E+03
5.50E+01	8.74E+00	2.00E+00	2.00E+00	1.74E+03
5.60E+01	3.78E+00	2.00E+00	2.00E+00	1.74E+03
5.70E+01	1.65E+00	2.00E+00	2.00E+00	1.74E+03
5.80E+01	9.87E+00	2.00E+00	2.00E+00	1.74E+03
5.90E+01	8.70E+00	2.00E+00	2.00E+00	1.74E+03
1.00E+00	4.67E+00	1.00E+00	1.00E+00	1.69E+03
2.00E+00	7.05E+00	1.00E+00	1.00E+00	1.69E+03
3.00E+00	3.49E+00	1.00E+00	1.00E+00	1.69E+03
4.00E+00	3.64E+00	1.00E+00	1.00E+00	1.69E+03
5.00E+00	7.51E+00	1.00E+00	1.00E+00	1.69E+03
6.00E+00	6.28E+00	1.00E+00	1.00E+00	1.69E+03
7.00E+00	6.33E+00	1.00E+00	1.00E+00	1.69E+03
8.00E+00	6.18E+00	1.00E+00	1.00E+00	1.69E+03
9.00E+00	9.74E+00	1.00E+00	1.00E+00	1.69E+03
1.00E+01	5.73E+00	1.00E+00	1.00E+00	1.69E+03
1.10E+01	5.13E+00	1.00E+00	1.00E+00	1.69E+03
1.20E+01	6.33E+00	1.00E+00	1.00E+00	1.69E+03
1.30E+01	5.37E+00	1.00E+00	1.00E+00	1.69E+03
1.40E+01	4.26E+00	1.00E+00	1.00E+00	1.69E+03
1.50E+01	9.82E+00	1.00E+00	1.00E+00	1.69E+03
1.60E+01	8.99E+00	1.00E+00	1.00E+00	1.69E+03
1.70E+01	3.48E+00	1.00E+00	1.00E+00	1.69E+03
1.80E+01	1.22E+01	1.00E+00	1.00E+00	1.69E+03
1.90E+01	6.21E+00	1.00E+00	1.00E+00	1.69E+03
2.00E+01	8.36E+00	1.00E+00	1.00E+00	1.69E+03
2.10E+01	6.51E+00	1.00E+00	1.00E+00	1.69E+03
2.20E+01	9.91E+00	1.00E+00	1.00E+00	1.69E+03
2.30E+01	7.32E+00	1.00E+00	1.00E+00	1.69E+03
2.40E+01	6.46E+00	1.00E+00	1.00E+00	1.69E+03
2.50E+01	6.67E+00	1.00E+00	1.00E+00	1.69E+03
2.60E+01	5.73E+00	1.00E+00	1.00E+00	1.69E+03
2.70E+01	7.96E+00	1.00E+00	1.00E+00	1.69E+03
2.80E+01	4.97E+00	1.00E+00	1.00E+00	1.69E+03
2.90E+01	2.17E+00	1.00E+00	1.00E+00	1.69E+03
3.00E+01	1.70E+00	2.00E+00	1.00E+00	1.69E+03
3.10E+01	3.91E+00	2.00E+00	1.00E+00	1.69E+03
3.20E+01	3.95E+00	2.00E+00	1.00E+00	1.69E+03
3.30E+01	3.46E+00	2.00E+00	1.00E+00	1.69E+03
3.40E+01	8.94E+00	2.00E+00	1.00E+00	1.69E+03
3.50E+01	6.59E+00	2.00E+00	1.00E+00	1.69E+03
3.60E+01	5.16E+00	2.00E+00	1.00E+00	1.69E+03
3.70E+01	3.89E+00	2.00E+00	1.00E+00	1.69E+03
3.80E+01	3.59E+00	2.00E+00	1.00E+00	1.69E+03
3.90E+01	7.50E+00	2.00E+00	1.00E+00	1.69E+03
4.00E+01	5.89E+00	2.00E+00	1.00E+00	1.69E+03
4.10E+01	5.36E+00	2.00E+00	1.00E+00	1.69E+03
4.20E+01	4.83E+00	2.00E+00	1.00E+00	1.69E+03
4.30E+01	7.00E+00	2.00E+00	1.00E+00	1.69E+03
4.40E+01	3.52E+00	2.00E+00	1.00E+00	1.69E+03
4.50E+01	7.96E+00	2.00E+00	1.00E+00	1.69E+03
4.60E+01	3.42E+00	2.00E+00	1.00E+00	1.69E+03
4.70E+01	5.71E+00	2.00E+00	1.00E+00	1.69E+03
4.80E+01	6.72E+00	2.00E+00	1.00E+00	1.69E+03
4.90E+01	6.88E+00	2.00E+00	1.00E+00	1.69E+03
5.00E+01	6.10E+00	2.00E+00	1.00E+00	1.69E+03
5.10E+01	9.08E+00	2.00E+00	1.00E+00	1.69E+03
5.20E+01	9.08E+00	2.00E+00	1.00E+00	1.69E+03
5.30E+01	6.24E+00	2.00E+00	1.00E+00	1.69E+03
5.40E+01	9.96E+00	2.00E+00	1.00E+00	1.69E+03
5.50E+01	3.41E+00	2.00E+00	1.00E+00	1.69E+03
5.60E+01	3.87E+00	2.00E+00	1.00E+00	1.69E+03
5.70E+01	3.19E+00	2.00E+00	1.00E+00	1.69E+03
5.80E+01	5.18E+00	2.00E+00	1.00E+00	1.69E+03
5.90E+01	1.04E+01	2.00E+00	1.00E+00	1.69E+03
1.00E+00	4.48E+00	1.00E+00	2.00E+00	1.69E+03
2.00E+00	8.81E+00	1.00E+00	2.00E+00	1.69E+03
3.00E+00	2.61E+00	1.00E+00	2.00E+00	1.69E+03
4.00E+00	5.11E+00	1.00E+00	2.00E+00	1.69E+03
5.00E+00	5.26E+00	1.00E+00	2.00E+00	1.69E+03
6.00E+00	5.69E+00	1.00E+00	2.00E+00	1.69E+03
7.00E+00	4.11E+00	1.00E+00	2.00E+00	1.69E+03
8.00E+00	8.91E+00	1.00E+00	2.00E+00	1.69E+03
9.00E+00	1.09E+01	1.00E+00	2.00E+00	1.69E+03
1.00E+01	4.24E+00	1.00E+00	2.00E+00	1.69E+03
1.10E+01	8.59E+00	1.00E+00	2.00E+00	1.69E+03
1.20E+01	4.05E+00	1.00E+00	2.00E+00	1.69E+03
1.30E+01	4.30E+00	1.00E+00	2.00E+00	1.69E+03
1.40E+01	2.13E+00	1.00E+00	2.00E+00	1.69E+03
1.50E+01	6.20E+00	1.00E+00	2.00E+00	1.69E+03
1.60E+01	6.63E+00	1.00E+00	2.00E+00	1.69E+03
1.70E+01	3.10E+00	1.00E+00	2.00E+00	1.69E+03
1.80E+01	1.13E+01	1.00E+00	2.00E+00	1.69E+03
1.90E+01	7.06E+00	1.00E+00	2.00E+00	1.69E+03
2.00E+01	8.12E+00	1.00E+00	2.00E+00	1.69E+03
2.10E+01	5.80E+00	1.00E+00	2.00E+00	1.69E+03
2.20E+01	7.79E+00	1.00E+00	2.00E+00	1.69E+03
2.30E+01	4.73E+00	1.00E+00	2.00E+00	1.69E+03
2.40E+01	8.53E+00	1.00E+00	2.00E+00	1.69E+03
2.50E+01	7.83E+00	1.00E+00	2.00E+00	1.69E+03
2.60E+01	4.08E+00	1.00E+00	2.00E+00	1.69E+03
2.70E+01	9.72E+00	1.00E+00	2.00E+00	1.69E+03
2.80E+01	4.55E+00	1.00E+00	2.00E+00	1.69E+03
2.90E+01	3.47E+00	1.00E+00	2.00E+00	1.69E+03
3.00E+01	1.52E+00	2.00E+00	2.00E+00	1.69E+03
3.10E+01	5.07E-01	2.00E+00	2.00E+00	1.69E+03
3.20E+01	5.63E+00	2.00E+00	2.00E+00	1.69E+03
3.30E+01	4.36E+00	2.00E+00	2.00E+00	1.69E+03
3.40E+01	8.34E+00	2.00E+00	2.00E+00	1.69E+03
3.50E+01	7.22E+00	2.00E+00	2.00E+00	1.69E+03
3.60E+01	5.56E+00	2.00E+00	2.00E+00	1.69E+03
3.70E+01	2.69E+00	2.00E+00	2.00E+00	1.69E+03
3.80E+01	5.10E+00	2.00E+00	2.00E+00	1.69E+03
3.90E+01	2.53E+00	2.00E+00	2.00E+00	1.69E+03
4.00E+01	5.62E+00	2.00E+00	2.00E+00	1.69E+03
4.10E+01	5.59E+00	2.00E+00	2.00E+00	1.69E+03
4.20E+01	5.33E+00	2.00E+00	2.00E+00	1.69E+03
4.30E+01	6.24E+00	2.00E+00	2.00E+00	1.69E+03
4.40E+01	4.55E+00	2.00E+00	2.00E+00	1.69E+03
4.50E+01	7.84E+00	2.00E+00	2.00E+00	1.69E+03
4.60E+01	2.42E+00	2.00E+00	2.00E+00	1.69E+03
4.70E+01	2.40E+00	2.00E+00	2.00E+00	1.69E+03
4.80E+01	9.05E+00	2.00E+00	2.00E+00	1.69E+03
4.90E+01	4.96E+00	2.00E+00	2.00E+00	1.69E+03
5.00E+01	3.94E+00	2.00E+00	2.00E+00	1.69E+03
5.10E+01	7.07E+00	2.00E+00	2.00E+00	1.69E+03
5.20E+01	7.07E+00	2.00E+00	2.00E+00	1.69E+03
5.30E+01	6.47E+00	2.00E+00	2.00E+00	1.69E+03
5.40E+01	7.01E+00	2.00E+00	2.00E+00	1.69E+03
5.50E+01	5.87E+00	2.00E+00	2.00E+00	1.69E+03
5.60E+01	4.58E+00	2.00E+00	2.00E+00	1.69E+03
5.70E+01	1.26E+00	2.00E+00	2.00E+00	1.69E+03
5.80E+01	8.93E+00	2.00E+00	2.00E+00	1.69E+03
5.90E+01	1.00E+01	2.00E+00	2.00E+00	1.69E+03
1.00E+00	6.63E+00	1.00E+00	1.00E+00	2.29E+03
2.00E+00	9.26E+00	1.00E+00	1.00E+00	2.29E+03
3.00E+00	6.67E+00	1.00E+00	1.00E+00	2.29E+03
4.00E+00	7.76E+00	1.00E+00	1.00E+00	2.29E+03
5.00E+00	9.08E+00	1.00E+00	1.00E+00	2.29E+03
6.00E+00	7.58E+00	1.00E+00	1.00E+00	2.29E+03
7.00E+00	6.60E+00	1.00E+00	1.00E+00	2.29E+03
8.00E+00	7.49E+00	1.00E+00	1.00E+00	2.29E+03
9.00E+00	8.51E+00	1.00E+00	1.00E+00	2.29E+03
1.00E+01	7.25E+00	1.00E+00	1.00E+00	2.29E+03
1.10E+01	4.86E+00	1.00E+00	1.00E+00	2.29E+03
1.20E+01	9.67E+00	1.00E+00	1.00E+00	2.29E+03
1.30E+01	9.33E+00	1.00E+00	1.00E+00	2.29E+03
1.40E+01	9.07E+00	1.00E+00	1.00E+00	2.29E+03
1.50E+01	6.62E+00	1.00E+00	1.00E+00	2.29E+03
1.60E+01	8.77E+00	1.00E+00	1.00E+00	2.29E+03
1.70E+01	8.52E+00	1.00E+00	1.00E+00	2.29E+03
1.80E+01	1.01E+01	1.00E+00	1.00E+00	2.29E+03
1.90E+01	8.57E+00	1.00E+00	1.00E+00	2.29E+03
2.00E+01	7.83E+00	1.00E+00	1.00E+00	2.29E+03
2.10E+01	1.00E+01	1.00E+00	1.00E+00	2.29E+03
2.20E+01	6.24E+00	1.00E+00	1.00E+00	2.29E+03
2.30E+01	1.14E+01	1.00E+00	1.00E+00	2.29E+03
2.40E+01	9.27E+00	1.00E+00	1.00E+00	2.29E+03
2.50E+01	5.94E+00	1.00E+00	1.00E+00	2.29E+03
2.60E+01	7.60E+00	1.00E+00	1.00E+00	2.29E+03
2.70E+01	6.02E+00	1.00E+00	1.00E+00	2.29E+03
2.80E+01	6.89E+00	1.00E+00	1.00E+00	2.29E+03
2.90E+01	9.25E+00	1.00E+00	1.00E+00	2.29E+03
3.00E+01	9.46E+00	2.00E+00	1.00E+00	2.29E+03
3.10E+01	8.54E+00	2.00E+00	1.00E+00	2.29E+03
3.20E+01	4.93E+00	2.00E+00	1.00E+00	2.29E+03
3.30E+01	7.89E+00	2.00E+00	1.00E+00	2.29E+03
3.40E+01	9.44E+00	2.00E+00	1.00E+00	2.29E+03
3.50E+01	9.14E+00	2.00E+00	1.00E+00	2.29E+03
3.60E+01	1.05E+01	2.00E+00	1.00E+00	2.29E+03
3.70E+01	9.24E+00	2.00E+00	1.00E+00	2.29E+03
3.80E+01	9.06E+00	2.00E+00	1.00E+00	2.29E+03
3.90E+01	9.79E+00	2.00E+00	1.00E+00	2.29E+03
4.00E+01	1.06E+01	2.00E+00	1.00E+00	2.29E+03
4.10E+01	1.08E+01	2.00E+00	1.00E+00	2.29E+03
4.20E+01	9.50E+00	2.00E+00	1.00E+00	2.29E+03
4.30E+01	8.87E+00	2.00E+00	1.00E+00	2.29E+03
4.40E+01	8.72E+00	2.00E+00	1.00E+00	2.29E+03
4.50E+01	5.20E+00	2.00E+00	1.00E+00	2.29E+03
4.60E+01	5.39E+00	2.00E+00	1.00E+00	2.29E+03
4.70E+01	6.31E+00	2.00E+00	1.00E+00	2.29E+03
4.80E+01	6.97E+00	2.00E+00	1.00E+00	2.29E+03
4.90E+01	9.06E+00	2.00E+00	1.00E+00	2.29E+03
5.00E+01	8.90E+00	2.00E+00	1.00E+00	2.29E+03
5.10E+01	8.20E+00	2.00E+00	1.00E+00	2.29E+03
5.20E+01	8.20E+00	2.00E+00	1.00E+00	2.29E+03
5.30E+01	6.42E+00	2.00E+00	1.00E+00	2.29E+03
5.40E+01	9.90E+00	2.00E+00	1.00E+00	2.29E+03
5.50E+01	8.53E+00	2.00E+00	1.00E+00	2.29E+03
5.60E+01	8.44E+00	2.00E+00	1.00E+00	2.29E+03
5.70E+01	7.24E+00	2.00E+00	1.00E+00	2.29E+03
5.80E+01	7.74E+00	2.00E+00	1.00E+00	2.29E+03
5.90E+01	9.41E+00	2.00E+00	1.00E+00	2.29E+03
1.00E+00	6.24E+00	1.00E+00	2.00E+00	2.29E+03
2.00E+00	1.15E+01	1.00E+00	2.00E+00	2.29E+03
3.00E+00	4.42E+00	1.00E+00	2.00E+00	2.29E+03
4.00E+00	5.81E+00	1.00E+00	2.00E+00	2.29E+03
5.00E+00	8.30E+00	1.00E+00	2.00E+00	2.29E+03
6.00E+00	6.04E+00	1.00E+00	2.00E+00	2.29E+03
7.00E+00	6.55E+00	1.00E+00	2.00E+00	2.29E+03
8.00E+00	7.90E+00	1.00E+00	2.00E+00	2.29E+03
9.00E+00	9.11E+00	1.00E+00	2.00E+00	2.29E+03
1.00E+01	6.91E+00	1.00E+00	2.00E+00	2.29E+03
1.10E+01	9.29E+00	1.00E+00	2.00E+00	2.29E+03
1.20E+01	5.56E+00	1.00E+00	2.00E+00	2.29E+03
1.30E+01	3.68E+00	1.00E+00	2.00E+00	2.29E+03
1.40E+01	3.68E+00	1.00E+00	2.00E+00	2.29E+03
1.50E+01	6.94E+00	1.00E+00	2.00E+00	2.29E+03
1.60E+01	5.56E+00	1.00E+00	2.00E+00	2.29E+03
1.70E+01	4.88E+00	1.00E+00	2.00E+00	2.29E+03
1.80E+01	1.06E+01	1.00E+00	2.00E+00	2.29E+03
1.90E+01	6.34E+00	1.00E+00	2.00E+00	2.29E+03
2.00E+01	5.69E+00	1.00E+00	2.00E+00	2.29E+03
2.10E+01	5.40E+00	1.00E+00	2.00E+00	2.29E+03
2.20E+01	8.19E+00	1.00E+00	2.00E+00	2.29E+03
2.30E+01	8.63E+00	1.00E+00	2.00E+00	2.29E+03
2.40E+01	4.87E+00	1.00E+00	2.00E+00	2.29E+03
2.50E+01	7.31E+00	1.00E+00	2.00E+00	2.29E+03
2.60E+01	5.19E+00	1.00E+00	2.00E+00	2.29E+03
2.70E+01	9.25E+00	1.00E+00	2.00E+00	2.29E+03
2.80E+01	3.72E+00	1.00E+00	2.00E+00	2.29E+03
2.90E+01	3.42E+00	1.00E+00	2.00E+00	2.29E+03
3.00E+01	8.70E-01	2.00E+00	2.00E+00	2.29E+03
3.10E+01	1.16E+00	2.00E+00	2.00E+00	2.29E+03
3.20E+01	4.40E+00	2.00E+00	2.00E+00	2.29E+03
3.30E+01	7.22E+00	2.00E+00	2.00E+00	2.29E+03
3.40E+01	5.21E+00	2.00E+00	2.00E+00	2.29E+03
3.50E+01	7.68E+00	2.00E+00	2.00E+00	2.29E+03
3.60E+01	5.94E+00	2.00E+00	2.00E+00	2.29E+03
3.70E+01	7.53E+00	2.00E+00	2.00E+00	2.29E+03
3.80E+01	2.69E+00	2.00E+00	2.00E+00	2.29E+03
3.90E+01	3.44E+00	2.00E+00	2.00E+00	2.29E+03
4.00E+01	9.13E+00	2.00E+00	2.00E+00	2.29E+03
4.10E+01	8.48E+00	2.00E+00	2.00E+00	2.29E+03
4.20E+01	3.10E+00	2.00E+00	2.00E+00	2.29E+03
4.30E+01	5.18E+00	2.00E+00	2.00E+00	2.29E+03
4.40E+01	3.86E+00	2.00E+00	2.00E+00	2.29E+03
4.50E+01	8.08E+00	2.00E+00	2.00E+00	2.29E+03
4.60E+01	1.79E+00	2.00E+00	2.00E+00	2.29E+03
4.70E+01	2.03E+00	2.00E+00	2.00E+00	2.29E+03
4.80E+01	8.28E+00	2.00E+00	2.00E+00	2.29E+03
4.90E+01	3.24E+00	2.00E+00	2.00E+00	2.29E+03
5.00E+01	5.06E+00	2.00E+00	2.00E+00	2.29E+03
5.10E+01	4.34E+00	2.00E+00	2.00E+00	2.29E+03
5.20E+01	4.34E+00	2.00E+00	2.00E+00	2.29E+03
5.30E+01	9.75E+00	2.00E+00	2.00E+00	2.29E+03
5.40E+01	8.17E+00	2.00E+00	2.00E+00	2.29E+03
5.50E+01	6.91E+00	2.00E+00	2.00E+00	2.29E+03
5.60E+01	3.89E+00	2.00E+00	2.00E+00	2.29E+03
5.70E+01	1.14E+00	2.00E+00	2.00E+00	2.29E+03
5.80E+01	4.27E+00	2.00E+00	2.00E+00	2.29E+03
5.90E+01	9.66E+00	2.00E+00	2.00E+00	2.29E+03
1.00E+00	7.02E+00	1.00E+00	1.00E+00	2.21E+03
2.00E+00	9.86E+00	1.00E+00	1.00E+00	2.21E+03
3.00E+00	5.44E+00	1.00E+00	1.00E+00	2.21E+03
4.00E+00	7.02E+00	1.00E+00	1.00E+00	2.21E+03
5.00E+00	6.67E+00	1.00E+00	1.00E+00	2.21E+03
6.00E+00	7.34E+00	1.00E+00	1.00E+00	2.21E+03
7.00E+00	1.10E+01	1.00E+00	1.00E+00	2.21E+03
8.00E+00	9.86E+00	1.00E+00	1.00E+00	2.21E+03
9.00E+00	9.10E+00	1.00E+00	1.00E+00	2.21E+03
1.00E+01	6.43E+00	1.00E+00	1.00E+00	2.21E+03
1.10E+01	9.46E+00	1.00E+00	1.00E+00	2.21E+03
1.20E+01	9.20E+00	1.00E+00	1.00E+00	2.21E+03
1.30E+01	6.55E+00	1.00E+00	1.00E+00	2.21E+03
1.40E+01	8.06E+00	1.00E+00	1.00E+00	2.21E+03
1.50E+01	7.94E+00	1.00E+00	1.00E+00	2.21E+03
1.60E+01	9.74E+00	1.00E+00	1.00E+00	2.21E+03
1.70E+01	8.94E+00	1.00E+00	1.00E+00	2.21E+03
1.80E+01	8.77E+00	1.00E+00	1.00E+00	2.21E+03
1.90E+01	9.22E+00	1.00E+00	1.00E+00	2.21E+03
2.00E+01	1.04E+01	1.00E+00	1.00E+00	2.21E+03
2.10E+01	9.28E+00	1.00E+00	1.00E+00	2.21E+03
2.20E+01	7.87E+00	1.00E+00	1.00E+00	2.21E+03
2.30E+01	8.92E+00	1.00E+00	1.00E+00	2.21E+03
2.40E+01	6.60E+00	1.00E+00	1.00E+00	2.21E+03
2.50E+01	1.04E+01	1.00E+00	1.00E+00	2.21E+03
2.60E+01	8.80E+00	1.00E+00	1.00E+00	2.21E+03
2.70E+01	7.71E+00	1.00E+00	1.00E+00	2.21E+03
2.80E+01	8.88E+00	1.00E+00	1.00E+00	2.21E+03
2.90E+01	6.22E+00	1.00E+00	1.00E+00	2.21E+03
3.00E+01	6.72E+00	2.00E+00	1.00E+00	2.21E+03
3.10E+01	6.29E+00	2.00E+00	1.00E+00	2.21E+03
3.20E+01	1.00E+01	2.00E+00	1.00E+00	2.21E+03
3.30E+01	8.54E+00	2.00E+00	1.00E+00	2.21E+03
3.40E+01	7.92E+00	2.00E+00	1.00E+00	2.21E+03
3.50E+01	8.59E+00	2.00E+00	1.00E+00	2.21E+03
3.60E+01	7.94E+00	2.00E+00	1.00E+00	2.21E+03
3.70E+01	7.08E+00	2.00E+00	1.00E+00	2.21E+03
3.80E+01	5.91E+00	2.00E+00	1.00E+00	2.21E+03
3.90E+01	8.00E+00	2.00E+00	1.00E+00	2.21E+03
4.00E+01	7.05E+00	2.00E+00	1.00E+00	2.21E+03
4.10E+01	8.20E+00	2.00E+00	1.00E+00	2.21E+03
4.20E+01	8.48E+00	2.00E+00	1.00E+00	2.21E+03
4.30E+01	9.58E+00	2.00E+00	1.00E+00	2.21E+03
4.40E+01	5.91E+00	2.00E+00	1.00E+00	2.21E+03
4.50E+01	9.20E+00	2.00E+00	1.00E+00	2.21E+03
4.60E+01	9.17E+00	2.00E+00	1.00E+00	2.21E+03
4.70E+01	8.53E+00	2.00E+00	1.00E+00	2.21E+03
4.80E+01	6.37E+00	2.00E+00	1.00E+00	2.21E+03
4.90E+01	7.29E+00	2.00E+00	1.00E+00	2.21E+03
5.00E+01	1.13E+01	2.00E+00	1.00E+00	2.21E+03
5.10E+01	9.72E+00	2.00E+00	1.00E+00	2.21E+03
5.20E+01	9.72E+00	2.00E+00	1.00E+00	2.21E+03
5.30E+01	7.86E+00	2.00E+00	1.00E+00	2.21E+03
5.40E+01	8.64E+00	2.00E+00	1.00E+00	2.21E+03
5.50E+01	6.65E+00	2.00E+00	1.00E+00	2.21E+03
5.60E+01	4.88E+00	2.00E+00	1.00E+00	2.21E+03
5.70E+01	7.00E+00	2.00E+00	1.00E+00	2.21E+03
5.80E+01	8.63E+00	2.00E+00	1.00E+00	2.21E+03
5.90E+01	8.66E+00	2.00E+00	1.00E+00	2.21E+03
1.00E+00	8.26E+00	1.00E+00	2.00E+00	2.21E+03
2.00E+00	9.69E+00	1.00E+00	2.00E+00	2.21E+03
3.00E+00	2.09E+00	1.00E+00	2.00E+00	2.21E+03
4.00E+00	3.71E+00	1.00E+00	2.00E+00	2.21E+03
5.00E+00	7.92E+00	1.00E+00	2.00E+00	2.21E+03
6.00E+00	6.90E+00	1.00E+00	2.00E+00	2.21E+03
7.00E+00	6.24E+00	1.00E+00	2.00E+00	2.21E+03
8.00E+00	5.67E+00	1.00E+00	2.00E+00	2.21E+03
9.00E+00	7.89E+00	1.00E+00	2.00E+00	2.21E+03
1.00E+01	4.50E+00	1.00E+00	2.00E+00	2.21E+03
1.10E+01	7.93E+00	1.00E+00	2.00E+00	2.21E+03
1.20E+01	7.80E+00	1.00E+00	2.00E+00	2.21E+03
1.30E+01	2.59E+00	1.00E+00	2.00E+00	2.21E+03
1.40E+01	2.49E+00	1.00E+00	2.00E+00	2.21E+03
1.50E+01	7.36E+00	1.00E+00	2.00E+00	2.21E+03
1.60E+01	5.07E+00	1.00E+00	2.00E+00	2.21E+03
1.70E+01	4.67E+00	1.00E+00	2.00E+00	2.21E+03
1.80E+01	1.18E+01	1.00E+00	2.00E+00	2.21E+03
1.90E+01	9.30E+00	1.00E+00	2.00E+00	2.21E+03
2.00E+01	9.19E+00	1.00E+00	2.00E+00	2.21E+03
2.10E+01	8.82E+00	1.00E+00	2.00E+00	2.21E+03
2.20E+01	8.50E+00	1.00E+00	2.00E+00	2.21E+03
2.30E+01	5.01E+00	1.00E+00	2.00E+00	2.21E+03
2.40E+01	4.10E+00	1.00E+00	2.00E+00	2.21E+03
2.50E+01	8.08E+00	1.00E+00	2.00E+00	2.21E+03
2.60E+01	3.65E+00	1.00E+00	2.00E+00	2.21E+03
2.70E+01	8.12E+00	1.00E+00	2.00E+00	2.21E+03
2.80E+01	3.54E+00	1.00E+00	2.00E+00	2.21E+03
2.90E+01	2.46E+00	1.00E+00	2.00E+00	2.21E+03
3.00E+01	1.24E+00	2.00E+00	2.00E+00	2.21E+03
3.10E+01	1.17E+00	2.00E+00	2.00E+00	2.21E+03
3.20E+01	3.63E+00	2.00E+00	2.00E+00	2.21E+03
3.30E+01	4.28E+00	2.00E+00	2.00E+00	2.21E+03
3.40E+01	4.49E+00	2.00E+00	2.00E+00	2.21E+03
3.50E+01	6.88E+00	2.00E+00	2.00E+00	2.21E+03
3.60E+01	5.89E+00	2.00E+00	2.00E+00	2.21E+03
3.70E+01	1.70E+00	2.00E+00	2.00E+00	2.21E+03
3.80E+01	1.56E+00	2.00E+00	2.00E+00	2.21E+03
3.90E+01	3.66E+00	2.00E+00	2.00E+00	2.21E+03
4.00E+01	2.99E+00	2.00E+00	2.00E+00	2.21E+03
4.10E+01	7.43E+00	2.00E+00	2.00E+00	2.21E+03
4.20E+01	7.82E+00	2.00E+00	2.00E+00	2.21E+03
4.30E+01	5.64E+00	2.00E+00	2.00E+00	2.21E+03
4.40E+01	3.27E+00	2.00E+00	2.00E+00	2.21E+03
4.50E+01	8.65E+00	2.00E+00	2.00E+00	2.21E+03
4.60E+01	2.19E+00	2.00E+00	2.00E+00	2.21E+03
4.70E+01	1.84E+00	2.00E+00	2.00E+00	2.21E+03
4.80E+01	6.87E+00	2.00E+00	2.00E+00	2.21E+03
4.90E+01	2.63E+00	2.00E+00	2.00E+00	2.21E+03
5.00E+01	4.04E+00	2.00E+00	2.00E+00	2.21E+03
5.10E+01	3.67E+00	2.00E+00	2.00E+00	2.21E+03
5.20E+01	3.67E+00	2.00E+00	2.00E+00	2.21E+03
5.30E+01	6.99E+00	2.00E+00	2.00E+00	2.21E+03
5.40E+01	9.10E+00	2.00E+00	2.00E+00	2.21E+03
5.50E+01	5.55E+00	2.00E+00	2.00E+00	2.21E+03
5.60E+01	3.83E+00	2.00E+00	2.00E+00	2.21E+03
5.70E+01	1.35E+00	2.00E+00	2.00E+00	2.21E+03
5.80E+01	4.88E+00	2.00E+00	2.00E+00	2.21E+03
5.90E+01	6.73E+00	2.00E+00	2.00E+00	2.21E+03
1.00E+00	6.62E+00	1.00E+00	1.00E+00	2.12E+03
2.00E+00	7.40E+00	1.00E+00	1.00E+00	2.12E+03
3.00E+00	5.79E+00	1.00E+00	1.00E+00	2.12E+03
4.00E+00	6.73E+00	1.00E+00	1.00E+00	2.12E+03
5.00E+00	7.94E+00	1.00E+00	1.00E+00	2.12E+03
6.00E+00	6.62E+00	1.00E+00	1.00E+00	2.12E+03
7.00E+00	4.43E+00	1.00E+00	1.00E+00	2.12E+03
8.00E+00	8.22E+00	1.00E+00	1.00E+00	2.12E+03
9.00E+00	8.37E+00	1.00E+00	1.00E+00	2.12E+03
1.00E+01	3.60E+00	1.00E+00	1.00E+00	2.12E+03
1.10E+01	8.55E+00	1.00E+00	1.00E+00	2.12E+03
1.20E+01	9.11E+00	1.00E+00	1.00E+00	2.12E+03
1.30E+01	4.44E+00	1.00E+00	1.00E+00	2.12E+03
1.40E+01	6.28E+00	1.00E+00	1.00E+00	2.12E+03
1.50E+01	8.80E+00	1.00E+00	1.00E+00	2.12E+03
1.60E+01	7.73E+00	1.00E+00	1.00E+00	2.12E+03
1.70E+01	6.74E+00	1.00E+00	1.00E+00	2.12E+03
1.80E+01	8.47E+00	1.00E+00	1.00E+00	2.12E+03
1.90E+01	8.18E+00	1.00E+00	1.00E+00	2.12E+03
2.00E+01	6.18E+00	1.00E+00	1.00E+00	2.12E+03
2.10E+01	5.35E+00	1.00E+00	1.00E+00	2.12E+03
2.20E+01	9.50E+00	1.00E+00	1.00E+00	2.12E+03
2.30E+01	7.53E+00	1.00E+00	1.00E+00	2.12E+03
2.40E+01	6.08E+00	1.00E+00	1.00E+00	2.12E+03
2.50E+01	5.36E+00	1.00E+00	1.00E+00	2.12E+03
2.60E+01	4.68E+00	1.00E+00	1.00E+00	2.12E+03
2.70E+01	1.03E+01	1.00E+00	1.00E+00	2.12E+03
2.80E+01	6.84E+00	1.00E+00	1.00E+00	2.12E+03
2.90E+01	7.26E+00	1.00E+00	1.00E+00	2.12E+03
3.00E+01	7.86E+00	2.00E+00	1.00E+00	2.12E+03
3.10E+01	5.09E+00	2.00E+00	1.00E+00	2.12E+03
3.20E+01	8.38E+00	2.00E+00	1.00E+00	2.12E+03
3.30E+01	5.93E+00	2.00E+00	1.00E+00	2.12E+03
3.40E+01	8.06E+00	2.00E+00	1.00E+00	2.12E+03
3.50E+01	7.64E+00	2.00E+00	1.00E+00	2.12E+03
3.60E+01	7.94E+00	2.00E+00	1.00E+00	2.12E+03
3.70E+01	5.88E+00	2.00E+00	1.00E+00	2.12E+03
3.80E+01	6.94E+00	2.00E+00	1.00E+00	2.12E+03
3.90E+01	8.76E+00	2.00E+00	1.00E+00	2.12E+03
4.00E+01	6.19E+00	2.00E+00	1.00E+00	2.12E+03
4.10E+01	6.50E+00	2.00E+00	1.00E+00	2.12E+03
4.20E+01	5.85E+00	2.00E+00	1.00E+00	2.12E+03
4.30E+01	8.78E+00	2.00E+00	1.00E+00	2.12E+03
4.40E+01	6.80E+00	2.00E+00	1.00E+00	2.12E+03
4.50E+01	7.38E+00	2.00E+00	1.00E+00	2.12E+03
4.60E+01	4.52E+00	2.00E+00	1.00E+00	2.12E+03
4.70E+01	9.16E+00	2.00E+00	1.00E+00	2.12E+03
4.80E+01	7.84E+00	2.00E+00	1.00E+00	2.12E+03
4.90E+01	6.19E+00	2.00E+00	1.00E+00	2.12E+03
5.00E+01	8.16E+00	2.00E+00	1.00E+00	2.12E+03
5.10E+01	6.78E+00	2.00E+00	1.00E+00	2.12E+03
5.20E+01	6.78E+00	2.00E+00	1.00E+00	2.12E+03
5.30E+01	6.21E+00	2.00E+00	1.00E+00	2.12E+03
5.40E+01	1.15E+01	2.00E+00	1.00E+00	2.12E+03
5.50E+01	8.78E+00	2.00E+00	1.00E+00	2.12E+03
5.60E+01	7.36E+00	2.00E+00	1.00E+00	2.12E+03
5.70E+01	5.08E+00	2.00E+00	1.00E+00	2.12E+03
5.80E+01	7.73E+00	2.00E+00	1.00E+00	2.12E+03
5.90E+01	8.05E+00	2.00E+00	1.00E+00	2.12E+03
1.00E+00	3.94E+00	1.00E+00	2.00E+00	2.12E+03
2.00E+00	7.98E+00	1.00E+00	2.00E+00	2.12E+03
3.00E+00	4.25E+00	1.00E+00	2.00E+00	2.12E+03
4.00E+00	3.22E+00	1.00E+00	2.00E+00	2.12E+03
5.00E+00	1.00E+01	1.00E+00	2.00E+00	2.12E+03
6.00E+00	9.55E+00	1.00E+00	2.00E+00	2.12E+03
7.00E+00	4.17E+00	1.00E+00	2.00E+00	2.12E+03
8.00E+00	6.29E+00	1.00E+00	2.00E+00	2.12E+03
9.00E+00	8.46E+00	1.00E+00	2.00E+00	2.12E+03
1.00E+01	4.17E+00	1.00E+00	2.00E+00	2.12E+03
1.10E+01	6.68E+00	1.00E+00	2.00E+00	2.12E+03
1.20E+01	6.30E+00	1.00E+00	2.00E+00	2.12E+03
1.30E+01	3.99E+00	1.00E+00	2.00E+00	2.12E+03
1.40E+01	2.30E+00	1.00E+00	2.00E+00	2.12E+03
1.50E+01	8.39E+00	1.00E+00	2.00E+00	2.12E+03
1.60E+01	7.21E+00	1.00E+00	2.00E+00	2.12E+03
1.70E+01	1.62E+00	1.00E+00	2.00E+00	2.12E+03
1.80E+01	1.18E+01	1.00E+00	2.00E+00	2.12E+03
1.90E+01	1.06E+01	1.00E+00	2.00E+00	2.12E+03
2.00E+01	6.69E+00	1.00E+00	2.00E+00	2.12E+03
2.10E+01	7.48E+00	1.00E+00	2.00E+00	2.12E+03
2.20E+01	9.31E+00	1.00E+00	2.00E+00	2.12E+03
2.30E+01	7.47E+00	1.00E+00	2.00E+00	2.12E+03
2.40E+01	7.43E+00	1.00E+00	2.00E+00	2.12E+03
2.50E+01	7.41E+00	1.00E+00	2.00E+00	2.12E+03
2.60E+01	5.45E+00	1.00E+00	2.00E+00	2.12E+03
2.70E+01	8.53E+00	1.00E+00	2.00E+00	2.12E+03
2.80E+01	3.85E+00	1.00E+00	2.00E+00	2.12E+03
2.90E+01	6.69E+00	1.00E+00	2.00E+00	2.12E+03
3.00E+01	1.07E+00	2.00E+00	2.00E+00	2.12E+03
3.10E+01	4.31E-01	2.00E+00	2.00E+00	2.12E+03
3.20E+01	2.85E+00	2.00E+00	2.00E+00	2.12E+03
3.30E+01	3.33E+00	2.00E+00	2.00E+00	2.12E+03
3.40E+01	5.04E+00	2.00E+00	2.00E+00	2.12E+03
3.50E+01	7.19E+00	2.00E+00	2.00E+00	2.12E+03
3.60E+01	4.55E+00	2.00E+00	2.00E+00	2.12E+03
3.70E+01	4.15E+00	2.00E+00	2.00E+00	2.12E+03
3.80E+01	2.40E+00	2.00E+00	2.00E+00	2.12E+03
3.90E+01	2.53E+00	2.00E+00	2.00E+00	2.12E+03
4.00E+01	4.77E+00	2.00E+00	2.00E+00	2.12E+03
4.10E+01	1.00E+01	2.00E+00	2.00E+00	2.12E+03
4.20E+01	6.23E+00	2.00E+00	2.00E+00	2.12E+03
4.30E+01	6.25E+00	2.00E+00	2.00E+00	2.12E+03
4.40E+01	4.14E+00	2.00E+00	2.00E+00	2.12E+03
4.50E+01	7.37E+00	2.00E+00	2.00E+00	2.12E+03
4.60E+01	9.82E-01	2.00E+00	2.00E+00	2.12E+03
4.70E+01	2.31E+00	2.00E+00	2.00E+00	2.12E+03
4.80E+01	8.35E+00	2.00E+00	2.00E+00	2.12E+03
4.90E+01	3.65E+00	2.00E+00	2.00E+00	2.12E+03
5.00E+01	3.05E+00	2.00E+00	2.00E+00	2.12E+03
5.10E+01	2.27E+00	2.00E+00	2.00E+00	2.12E+03
5.20E+01	2.27E+00	2.00E+00	2.00E+00	2.12E+03
5.30E+01	6.30E+00	2.00E+00	2.00E+00	2.12E+03
5.40E+01	1.05E+01	2.00E+00	2.00E+00	2.12E+03
5.50E+01	7.61E+00	2.00E+00	2.00E+00	2.12E+03
5.60E+01	3.17E+00	2.00E+00	2.00E+00	2.12E+03
5.70E+01	9.01E-01	2.00E+00	2.00E+00	2.12E+03
5.80E+01	8.28E+00	2.00E+00	2.00E+00	2.12E+03
5.90E+01	8.01E+00	2.00E+00	2.00E+00	2.12E+03
1.00E+00	7.16E+00	1.00E+00	1.00E+00	2.04E+03
2.00E+00	8.11E+00	1.00E+00	1.00E+00	2.04E+03
3.00E+00	2.86E+00	1.00E+00	1.00E+00	2.04E+03
4.00E+00	5.16E+00	1.00E+00	1.00E+00	2.04E+03
5.00E+00	8.56E+00	1.00E+00	1.00E+00	2.04E+03
6.00E+00	5.80E+00	1.00E+00	1.00E+00	2.04E+03
7.00E+00	6.65E+00	1.00E+00	1.00E+00	2.04E+03
8.00E+00	6.41E+00	1.00E+00	1.00E+00	2.04E+03
9.00E+00	9.07E+00	1.00E+00	1.00E+00	2.04E+03
1.00E+01	6.43E+00	1.00E+00	1.00E+00	2.04E+03
1.10E+01	6.67E+00	1.00E+00	1.00E+00	2.04E+03
1.20E+01	5.16E+00	1.00E+00	1.00E+00	2.04E+03
1.30E+01	5.47E+00	1.00E+00	1.00E+00	2.04E+03
1.40E+01	3.25E+00	1.00E+00	1.00E+00	2.04E+03
1.50E+01	9.77E+00	1.00E+00	1.00E+00	2.04E+03
1.60E+01	8.58E+00	1.00E+00	1.00E+00	2.04E+03
1.70E+01	3.36E+00	1.00E+00	1.00E+00	2.04E+03
1.80E+01	1.14E+01	1.00E+00	1.00E+00	2.04E+03
1.90E+01	6.30E+00	1.00E+00	1.00E+00	2.04E+03
2.00E+01	6.79E+00	1.00E+00	1.00E+00	2.04E+03
2.10E+01	8.92E+00	1.00E+00	1.00E+00	2.04E+03
2.20E+01	8.68E+00	1.00E+00	1.00E+00	2.04E+03
2.30E+01	6.46E+00	1.00E+00	1.00E+00	2.04E+03
2.40E+01	7.07E+00	1.00E+00	1.00E+00	2.04E+03
2.50E+01	1.03E+01	1.00E+00	1.00E+00	2.04E+03
2.60E+01	8.17E+00	1.00E+00	1.00E+00	2.04E+03
2.70E+01	6.49E+00	1.00E+00	1.00E+00	2.04E+03
2.80E+01	5.10E+00	1.00E+00	1.00E+00	2.04E+03
2.90E+01	5.87E+00	1.00E+00	1.00E+00	2.04E+03
3.00E+01	6.11E+00	2.00E+00	1.00E+00	2.04E+03
3.10E+01	5.28E+00	2.00E+00	1.00E+00	2.04E+03
3.20E+01	6.29E+00	2.00E+00	1.00E+00	2.04E+03
3.30E+01	5.65E+00	2.00E+00	1.00E+00	2.04E+03
3.40E+01	7.25E+00	2.00E+00	1.00E+00	2.04E+03
3.50E+01	8.84E+00	2.00E+00	1.00E+00	2.04E+03
3.60E+01	4.83E+00	2.00E+00	1.00E+00	2.04E+03
3.70E+01	3.89E+00	2.00E+00	1.00E+00	2.04E+03
3.80E+01	7.66E+00	2.00E+00	1.00E+00	2.04E+03
3.90E+01	8.02E+00	2.00E+00	1.00E+00	2.04E+03
4.00E+01	4.93E+00	2.00E+00	1.00E+00	2.04E+03
4.10E+01	7.25E+00	2.00E+00	1.00E+00	2.04E+03
4.20E+01	6.56E+00	2.00E+00	1.00E+00	2.04E+03
4.30E+01	8.68E+00	2.00E+00	1.00E+00	2.04E+03
4.40E+01	7.62E+00	2.00E+00	1.00E+00	2.04E+03
4.50E+01	5.95E+00	2.00E+00	1.00E+00	2.04E+03
4.60E+01	3.93E+00	2.00E+00	1.00E+00	2.04E+03
4.70E+01	7.38E+00	2.00E+00	1.00E+00	2.04E+03
4.80E+01	5.94E+00	2.00E+00	1.00E+00	2.04E+03
4.90E+01	8.96E+00	2.00E+00	1.00E+00	2.04E+03
5.00E+01	5.30E+00	2.00E+00	1.00E+00	2.04E+03
5.10E+01	8.21E+00	2.00E+00	1.00E+00	2.04E+03
5.20E+01	8.21E+00	2.00E+00	1.00E+00	2.04E+03
5.30E+01	7.16E+00	2.00E+00	1.00E+00	2.04E+03
5.40E+01	6.11E+00	2.00E+00	1.00E+00	2.04E+03
5.50E+01	4.89E+00	2.00E+00	1.00E+00	2.04E+03
5.60E+01	4.45E+00	2.00E+00	1.00E+00	2.04E+03
5.70E+01	5.38E+00	2.00E+00	1.00E+00	2.04E+03
5.80E+01	8.37E+00	2.00E+00	1.00E+00	2.04E+03
5.90E+01	8.57E+00	2.00E+00	1.00E+00	2.04E+03
1.00E+00	5.77E+00	1.00E+00	2.00E+00	2.04E+03
2.00E+00	9.03E+00	1.00E+00	2.00E+00	2.04E+03
3.00E+00	2.81E+00	1.00E+00	2.00E+00	2.04E+03
4.00E+00	3.96E+00	1.00E+00	2.00E+00	2.04E+03
5.00E+00	9.47E+00	1.00E+00	2.00E+00	2.04E+03
6.00E+00	7.21E+00	1.00E+00	2.00E+00	2.04E+03
7.00E+00	3.37E+00	1.00E+00	2.00E+00	2.04E+03
8.00E+00	3.91E+00	1.00E+00	2.00E+00	2.04E+03
9.00E+00	6.29E+00	1.00E+00	2.00E+00	2.04E+03
1.00E+01	5.13E+00	1.00E+00	2.00E+00	2.04E+03
1.10E+01	5.18E+00	1.00E+00	2.00E+00	2.04E+03
1.20E+01	4.00E+00	1.00E+00	2.00E+00	2.04E+03
1.30E+01	2.47E+00	1.00E+00	2.00E+00	2.04E+03
1.40E+01	1.06E+00	1.00E+00	2.00E+00	2.04E+03
1.50E+01	7.79E+00	1.00E+00	2.00E+00	2.04E+03
1.60E+01	2.16E+00	1.00E+00	2.00E+00	2.04E+03
1.70E+01	2.30E+00	1.00E+00	2.00E+00	2.04E+03
1.80E+01	1.17E+01	1.00E+00	2.00E+00	2.04E+03
1.90E+01	7.58E+00	1.00E+00	2.00E+00	2.04E+03
2.00E+01	4.73E+00	1.00E+00	2.00E+00	2.04E+03
2.10E+01	8.37E+00	1.00E+00	2.00E+00	2.04E+03
2.20E+01	8.14E+00	1.00E+00	2.00E+00	2.04E+03
2.30E+01	7.81E+00	1.00E+00	2.00E+00	2.04E+03
2.40E+01	4.39E+00	1.00E+00	2.00E+00	2.04E+03
2.50E+01	6.38E+00	1.00E+00	2.00E+00	2.04E+03
2.60E+01	5.27E+00	1.00E+00	2.00E+00	2.04E+03
2.70E+01	4.19E+00	1.00E+00	2.00E+00	2.04E+03
2.80E+01	2.01E+00	1.00E+00	2.00E+00	2.04E+03
2.90E+01	2.66E+00	1.00E+00	2.00E+00	2.04E+03
3.00E+01	1.62E+00	2.00E+00	2.00E+00	2.04E+03
3.10E+01	1.99E-01	2.00E+00	2.00E+00	2.04E+03
3.20E+01	4.54E+00	2.00E+00	2.00E+00	2.04E+03
3.30E+01	2.51E+00	2.00E+00	2.00E+00	2.04E+03
3.40E+01	5.34E+00	2.00E+00	2.00E+00	2.04E+03
3.50E+01	4.44E+00	2.00E+00	2.00E+00	2.04E+03
3.60E+01	8.39E+00	2.00E+00	2.00E+00	2.04E+03
3.70E+01	2.54E+00	2.00E+00	2.00E+00	2.04E+03
3.80E+01	2.07E+00	2.00E+00	2.00E+00	2.04E+03
3.90E+01	4.30E+00	2.00E+00	2.00E+00	2.04E+03
4.00E+01	7.70E+00	2.00E+00	2.00E+00	2.04E+03
4.10E+01	7.15E+00	2.00E+00	2.00E+00	2.04E+03
4.20E+01	5.50E+00	2.00E+00	2.00E+00	2.04E+03
4.30E+01	5.73E+00	2.00E+00	2.00E+00	2.04E+03
4.40E+01	5.10E+00	2.00E+00	2.00E+00	2.04E+03
4.50E+01	9.24E+00	2.00E+00	2.00E+00	2.04E+03
4.60E+01	2.83E+00	2.00E+00	2.00E+00	2.04E+03
4.70E+01	9.01E-01	2.00E+00	2.00E+00	2.04E+03
4.80E+01	5.93E+00	2.00E+00	2.00E+00	2.04E+03
4.90E+01	3.50E+00	2.00E+00	2.00E+00	2.04E+03
5.00E+01	3.57E+00	2.00E+00	2.00E+00	2.04E+03
5.10E+01	6.85E+00	2.00E+00	2.00E+00	2.04E+03
5.20E+01	6.85E+00	2.00E+00	2.00E+00	2.04E+03
5.30E+01	8.95E+00	2.00E+00	2.00E+00	2.04E+03
5.40E+01	1.10E+01	2.00E+00	2.00E+00	2.04E+03
5.50E+01	5.26E+00	2.00E+00	2.00E+00	2.04E+03
5.60E+01	2.68E+00	2.00E+00	2.00E+00	2.04E+03
5.70E+01	1.98E+00	2.00E+00	2.00E+00	2.04E+03
5.80E+01	7.62E+00	2.00E+00	2.00E+00	2.04E+03
5.90E+01	5.80E+00	2.00E+00	2.00E+00	2.04E+03
1.00E+00	5.99E+00	1.00E+00	1.00E+00	1.95E+03
2.00E+00	6.86E+00	1.00E+00	1.00E+00	1.95E+03
3.00E+00	3.17E+00	1.00E+00	1.00E+00	1.95E+03
4.00E+00	3.56E+00	1.00E+00	1.00E+00	1.95E+03
5.00E+00	7.28E+00	1.00E+00	1.00E+00	1.95E+03
6.00E+00	5.14E+00	1.00E+00	1.00E+00	1.95E+03
7.00E+00	7.69E+00	1.00E+00	1.00E+00	1.95E+03
8.00E+00	8.78E+00	1.00E+00	1.00E+00	1.95E+03
9.00E+00	8.06E+00	1.00E+00	1.00E+00	1.95E+03
1.00E+01	5.72E+00	1.00E+00	1.00E+00	1.95E+03
1.10E+01	6.10E+00	1.00E+00	1.00E+00	1.95E+03
1.20E+01	6.28E+00	1.00E+00	1.00E+00	1.95E+03
1.30E+01	3.77E+00	1.00E+00	1.00E+00	1.95E+03
1.40E+01	4.37E+00	1.00E+00	1.00E+00	1.95E+03
1.50E+01	6.77E+00	1.00E+00	1.00E+00	1.95E+03
1.60E+01	6.31E+00	1.00E+00	1.00E+00	1.95E+03
1.70E+01	4.28E+00	1.00E+00	1.00E+00	1.95E+03
1.80E+01	9.01E+00	1.00E+00	1.00E+00	1.95E+03
1.90E+01	9.68E+00	1.00E+00	1.00E+00	1.95E+03
2.00E+01	5.03E+00	1.00E+00	1.00E+00	1.95E+03
2.10E+01	6.75E+00	1.00E+00	1.00E+00	1.95E+03
2.20E+01	9.10E+00	1.00E+00	1.00E+00	1.95E+03
2.30E+01	7.61E+00	1.00E+00	1.00E+00	1.95E+03
2.40E+01	5.12E+00	1.00E+00	1.00E+00	1.95E+03
2.50E+01	7.43E+00	1.00E+00	1.00E+00	1.95E+03
2.60E+01	7.68E+00	1.00E+00	1.00E+00	1.95E+03
2.70E+01	9.27E+00	1.00E+00	1.00E+00	1.95E+03
2.80E+01	4.18E+00	1.00E+00	1.00E+00	1.95E+03
2.90E+01	2.68E+00	1.00E+00	1.00E+00	1.95E+03
3.00E+01	6.07E+00	2.00E+00	1.00E+00	1.95E+03
3.10E+01	3.35E+00	2.00E+00	1.00E+00	1.95E+03
3.20E+01	5.26E+00	2.00E+00	1.00E+00	1.95E+03
3.30E+01	3.63E+00	2.00E+00	1.00E+00	1.95E+03
3.40E+01	6.81E+00	2.00E+00	1.00E+00	1.95E+03
3.50E+01	7.77E+00	2.00E+00	1.00E+00	1.95E+03
3.60E+01	5.89E+00	2.00E+00	1.00E+00	1.95E+03
3.70E+01	2.53E+00	2.00E+00	1.00E+00	1.95E+03
3.80E+01	5.62E+00	2.00E+00	1.00E+00	1.95E+03
3.90E+01	3.34E+00	2.00E+00	1.00E+00	1.95E+03
4.00E+01	6.85E+00	2.00E+00	1.00E+00	1.95E+03
4.10E+01	6.70E+00	2.00E+00	1.00E+00	1.95E+03
4.20E+01	7.25E+00	2.00E+00	1.00E+00	1.95E+03
4.30E+01	7.46E+00	2.00E+00	1.00E+00	1.95E+03
4.40E+01	7.06E+00	2.00E+00	1.00E+00	1.95E+03
4.50E+01	5.46E+00	2.00E+00	1.00E+00	1.95E+03
4.60E+01	6.24E+00	2.00E+00	1.00E+00	1.95E+03
4.70E+01	4.63E+00	2.00E+00	1.00E+00	1.95E+03
4.80E+01	7.81E+00	2.00E+00	1.00E+00	1.95E+03
4.90E+01	5.23E+00	2.00E+00	1.00E+00	1.95E+03
5.00E+01	5.67E+00	2.00E+00	1.00E+00	1.95E+03
5.10E+01	6.82E+00	2.00E+00	1.00E+00	1.95E+03
5.20E+01	6.82E+00	2.00E+00	1.00E+00	1.95E+03
5.30E+01	6.72E+00	2.00E+00	1.00E+00	1.95E+03
5.40E+01	8.31E+00	2.00E+00	1.00E+00	1.95E+03
5.50E+01	4.98E+00	2.00E+00	1.00E+00	1.95E+03
5.60E+01	3.57E+00	2.00E+00	1.00E+00	1.95E+03
5.70E+01	3.23E+00	2.00E+00	1.00E+00	1.95E+03
5.80E+01	6.00E+00	2.00E+00	1.00E+00	1.95E+03
5.90E+01	8.77E+00	2.00E+00	1.00E+00	1.95E+03
1.00E+00	6.87E+00	1.00E+00	2.00E+00	1.95E+03
2.00E+00	7.15E+00	1.00E+00	2.00E+00	1.95E+03
3.00E+00	2.38E+00	1.00E+00	2.00E+00	1.95E+03
4.00E+00	3.37E+00	1.00E+00	2.00E+00	1.95E+03
5.00E+00	9.22E+00	1.00E+00	2.00E+00	1.95E+03
6.00E+00	6.48E+00	1.00E+00	2.00E+00	1.95E+03
7.00E+00	6.48E+00	1.00E+00	2.00E+00	1.95E+03
8.00E+00	6.31E+00	1.00E+00	2.00E+00	1.95E+03
9.00E+00	7.77E+00	1.00E+00	2.00E+00	1.95E+03
1.00E+01	5.33E+00	1.00E+00	2.00E+00	1.95E+03
1.10E+01	6.95E+00	1.00E+00	2.00E+00	1.95E+03
1.20E+01	5.95E+00	1.00E+00	2.00E+00	1.95E+03
1.30E+01	2.78E+00	1.00E+00	2.00E+00	1.95E+03
1.40E+01	2.95E+00	1.00E+00	2.00E+00	1.95E+03
1.50E+01	9.39E+00	1.00E+00	2.00E+00	1.95E+03
1.60E+01	6.36E+00	1.00E+00	2.00E+00	1.95E+03
1.70E+01	3.37E+00	1.00E+00	2.00E+00	1.95E+03
1.80E+01	9.95E+00	1.00E+00	2.00E+00	1.95E+03
1.90E+01	8.57E+00	1.00E+00	2.00E+00	1.95E+03
2.00E+01	8.65E+00	1.00E+00	2.00E+00	1.95E+03
2.10E+01	5.02E+00	1.00E+00	2.00E+00	1.95E+03
2.20E+01	8.70E+00	1.00E+00	2.00E+00	1.95E+03
2.30E+01	6.25E+00	1.00E+00	2.00E+00	1.95E+03
2.40E+01	5.30E+00	1.00E+00	2.00E+00	1.95E+03
2.50E+01	6.80E+00	1.00E+00	2.00E+00	1.95E+03
2.60E+01	4.92E+00	1.00E+00	2.00E+00	1.95E+03
2.70E+01	1.10E+01	1.00E+00	2.00E+00	1.95E+03
2.80E+01	3.96E+00	1.00E+00	2.00E+00	1.95E+03
2.90E+01	1.93E+00	1.00E+00	2.00E+00	1.95E+03
3.00E+01	2.81E+00	2.00E+00	2.00E+00	1.95E+03
3.10E+01	1.28E+00	2.00E+00	2.00E+00	1.95E+03
3.20E+01	3.73E+00	2.00E+00	2.00E+00	1.95E+03
3.30E+01	2.90E+00	2.00E+00	2.00E+00	1.95E+03
3.40E+01	5.53E+00	2.00E+00	2.00E+00	1.95E+03
3.50E+01	7.74E+00	2.00E+00	2.00E+00	1.95E+03
3.60E+01	9.14E+00	2.00E+00	2.00E+00	1.95E+03
3.70E+01	4.97E+00	2.00E+00	2.00E+00	1.95E+03
3.80E+01	3.01E+00	2.00E+00	2.00E+00	1.95E+03
3.90E+01	4.97E+00	2.00E+00	2.00E+00	1.95E+03
4.00E+01	4.48E+00	2.00E+00	2.00E+00	1.95E+03
4.10E+01	1.18E+01	2.00E+00	2.00E+00	1.95E+03
4.20E+01	4.93E+00	2.00E+00	2.00E+00	1.95E+03
4.30E+01	5.82E+00	2.00E+00	2.00E+00	1.95E+03
4.40E+01	3.00E+00	2.00E+00	2.00E+00	1.95E+03
4.50E+01	8.26E+00	2.00E+00	2.00E+00	1.95E+03
4.60E+01	1.13E+00	2.00E+00	2.00E+00	1.95E+03
4.70E+01	1.96E+00	2.00E+00	2.00E+00	1.95E+03
4.80E+01	7.79E+00	2.00E+00	2.00E+00	1.95E+03
4.90E+01	2.75E+00	2.00E+00	2.00E+00	1.95E+03
5.00E+01	3.57E+00	2.00E+00	2.00E+00	1.95E+03
5.10E+01	5.83E+00	2.00E+00	2.00E+00	1.95E+03
5.20E+01	5.83E+00	2.00E+00	2.00E+00	1.95E+03
5.30E+01	5.41E+00	2.00E+00	2.00E+00	1.95E+03
5.40E+01	1.06E+01	2.00E+00	2.00E+00	1.95E+03
5.50E+01	6.98E+00	2.00E+00	2.00E+00	1.95E+03
5.60E+01	3.75E+00	2.00E+00	2.00E+00	1.95E+03
5.70E+01	1.55E+00	2.00E+00	2.00E+00	1.95E+03
5.80E+01	6.71E+00	2.00E+00	2.00E+00	1.95E+03
5.90E+01	8.01E+00	2.00E+00	2.00E+00	1.95E+03

Thank you!

Kazumi Maniwa
Postdoctoral Research Fellow
Linguistics Department
University of Konstanz


From jchernev at ntag.com  Fri Dec 21 16:14:56 2007
From: jchernev at ntag.com (jchernev)
Date: Fri, 21 Dec 2007 07:14:56 -0800 (PST)
Subject: [R]  Plotting multiple jpegs on a plot?
Message-ID: <14456941.post@talk.nabble.com>


Hello,

I have a set of data which I currently represent graphically on a plot. To
do that, I use the embedded functions from the graphics package (symbols,
rect, etc.) However, I would like to use static jpeg images to represent the
data if possible. Here's my progress so far:

1. Used the rimage library to obtain the read.jpeg func. 
2. Read in my jpeg image and assigned it to a variable.
3. I can plot the image itself if i just do plot(image). However, I have no
idea on how to plot a lot of images at the same time.
4. I've tried the subplot func, but it says that my margins are too small
(My jpeg is 40x40 rgb). The maximum margin that I've been able to create was
20. However, this does not help the cause, because I want to add images to
the plot itself, instead to the plot's margins.

Has someone wrestled this one before? If yes, can you please explain how?

Thanks!


-- 
View this message in context: http://www.nabble.com/Plotting-multiple-jpegs-on-a-plot--tp14456941p14456941.html
Sent from the R help mailing list archive at Nabble.com.


From liuwensui at gmail.com  Fri Dec 21 16:25:36 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 21 Dec 2007 10:25:36 -0500
Subject: [R] install R on Unix without Admin Privilege
Message-ID: <1115a2b00712210725t1b8cebdar910c395162c12e69@mail.gmail.com>

Good morning, Dear Lister,
I really like to use R on our unix server. However, I am not an admin
of the system.
I am wondering if it is possible to install R on unix without admin previlege.

Thank you so much! Have a nice Xmas season!


From jholtman at gmail.com  Fri Dec 21 16:32:24 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 21 Dec 2007 10:32:24 -0500
Subject: [R] Finding overlaps in vector
In-Reply-To: <fkfrm5$vj3$1@ger.gmane.org>
References: <fkfrm5$vj3$1@ger.gmane.org>
Message-ID: <644e1f320712210732p69c60969n827a7f0dd98942b9@mail.gmail.com>

Here is a modification of the algorithm to use a specified value for
the overlap:

> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
> # following add 0.5 as the overlap detection -- can be changed
> x <- rbind(cbind(value=vector, oper=1, id=seq_along(vector)),
+            cbind(value=vector+0.5, oper=-1, id=seq_along(vector)))
> x <- x[order(x[,'value'], -x[, 'oper']),]
> # determine which ones overlap
> x <- cbind(x, over=cumsum(x[, 'oper']))
> # now partition into groups and only use groups greater than or equal to 3
> # determine where the breaks are (0 values in cumsum(over))
> x <- cbind(x, breaks=cumsum(x[, 'over'] == 0))
> # delete entries with 'over' == 0
> x <- x[x[, 'over'] != 0,]
> # split into groupd
> x.groups <- split(x[, 'id'], x[, 'breaks'])
> # only keep those with more than 2
> x.subsets <- x.groups[sapply(x.groups, length) >= 3]
> # print out the subsets
> invisible(lapply(x.subsets, function(a) print(vector[unique(a)])))
[1] 0.00 0.45
[1] 3.00 3.25 3.33 3.75 4.10
[1] 6.00 6.45
[1] 7.0 7.1


On Dec 21, 2007 4:56 AM, Johannes Graumann <johannes_graumann at web.de> wrote:
> <posted & mailed>
>
> Dear all,
>
> I'm trying to solve the problem, of how to find clusters of values in a
> vector that are closer than a given value. Illustrated this might look as
> follows:
>
> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>
> When using '0.5' as the proximity requirement, the following groups would
> result:
> 0,0.45
> 3,3.25,3.33,3.75,4.1
> 6,6.45
> 7,7.1
>
> Jim Holtman proposed a very elegant solution in
> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I have
> modified and perused since he wrote it to me. The beauty of this approach
> is that it will not only work for constant proximity requirements as above,
> but also for overlap-windows defined in terms of ppm around each value.
> Now I have an additional need and have found no way (short of iteratively
> step through all the groups returned) to figure out how to do that with
> Jim's approach: how to figure out that 6,6.45 and 7,7.1 are separate
> clusters?
>
> Thanks for any hints, Joh
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jrkrideau at yahoo.ca  Fri Dec 21 16:33:21 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 21 Dec 2007 10:33:21 -0500 (EST)
Subject: [R] (no subject)
In-Reply-To: <OFBE2AD317.965FAC01-ON852573B6.0067196D-852573B6.00671F96@un.org>
Message-ID: <230620.82232.qm@web32801.mail.mud.yahoo.com>

Hi,

I don't think you have given enough information for
anyone to offer advice. 

A small working (or almost working ) exmple that
reproduces the problem would help. 


--- Jiaming Zuo <zuoj at un.org> wrote:

> Dear R Users,
> 
>     I am working for the United Nations to construct
> a complete life table 
> from an abridged table. 
>     I want to use the code of Hydman Filter by Rob J
> Hydman but an error 
> sentence always appears and it simply doesn't run--
>        source("C:/R/Jamie/HymanFilter.R")
>        Error in .C("spline_coef", method =
> as.integer(method), n = nx, x = 
> x,  : C symbol name "spline_coef" not in DLL for
> package "base"
>  
>     Can you please tell me what's the problem?
> 
>      I appreciate your help so much!
> 
> Sincerely,
>    Jamie
> 
> Jamie Zuo
> 
> Tel: 1-917-367-8000
> Cell: 1-917-226-7531
> Email: zjmgyx at hotmail.com
> MSN: zjmgyx at hotmail.com
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From csardi at rmki.kfki.hu  Fri Dec 21 16:42:59 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Fri, 21 Dec 2007 16:42:59 +0100
Subject: [R] install R on Unix without Admin Privilege
In-Reply-To: <1115a2b00712210725t1b8cebdar910c395162c12e69@mail.gmail.com>
References: <1115a2b00712210725t1b8cebdar910c395162c12e69@mail.gmail.com>
Message-ID: <20071221154259.GA25601@localdomain>

Well i don't know which Unix you have, but it shouldn't matter anyway.
If you're able to compile programs on the machine then download
the R source, compile it and install it into your local directory.

Gabor

On Fri, Dec 21, 2007 at 10:25:36AM -0500, Wensui Liu wrote:
> Good morning, Dear Lister,
> I really like to use R on our unix server. However, I am not an admin
> of the system.
> I am wondering if it is possible to install R on unix without admin previlege.
> 
> Thank you so much! Have a nice Xmas season!
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From tomfool at as220.org  Fri Dec 21 16:34:23 2007
From: tomfool at as220.org (tom sgouros)
Date: Fri, 21 Dec 2007 10:34:23 -0500
Subject: [R] Sorting factors
In-Reply-To: <799216B1-BA7C-4698-8332-2A516027A02B@gmail.com> 
References: <20071220140634.F1689FACA21@as220.org>
	<799216B1-BA7C-4698-8332-2A516027A02B@gmail.com>
Message-ID: <20071221153423.A10A8FAC9AB@as220.org>


Hello all:

I'm sorry to take up bandwidth with easy questions, but as an R
beginner, I continue to be surprised by factors.  If someone can shed
some light for me, I'd be grateful:

> d
 Total z02801 z02802 z02804 z02806 z02807 z02808 z02809 z02812 z02813 
 54813     29     51    169   2368    103     76   1328    112    501 
507 Levels: 0 10 1001 1004 1008 1016 1027 1028 103 1031 10318 1043 1045 ... Na
> sort(d)
z02807 z02812 z02809 z02804 z02806 z02801 z02813 z02802  Total z02808 
   103    112   1328    169   2368     29    501     51  54813     76 
507 Levels: 0 10 1001 1004 1008 1016 1027 1028 103 1031 10318 1043 1045 ... Na

Apparently this factor is sorted in alphabetic order, not numeric order.
I find no parameter of sort() that controls for this.

And yet: 

> mode(d)
[1] "numeric"
> length(d)
[1] 10

I would have thought that because d is a numeric list of ten values, I'd
get them sorted in numeric order.

Can someone help me understand why this is expected behavior, and also
what I should do in order to see the numerically sorted list of values I
really want?

Many thanks,

 -tom


-- 
 ------------------------
 tomfool at as220 dot org
 http://sgouros.com  
 http://whatcheer.net


From liuwensui at gmail.com  Fri Dec 21 16:46:39 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 21 Dec 2007 10:46:39 -0500
Subject: [R] install R on Unix without Admin Privilege
In-Reply-To: <20071221154259.GA25601@localdomain>
References: <1115a2b00712210725t1b8cebdar910c395162c12e69@mail.gmail.com>
	<20071221154259.GA25601@localdomain>
Message-ID: <1115a2b00712210746l65003281o61a102635626389f@mail.gmail.com>

HI, Gabor,
what program do i need to compile R on unix?
local dir? do you mean dir under my user name with my profile?

On Dec 21, 2007 10:42 AM, Gabor Csardi <csardi at rmki.kfki.hu> wrote:
> Well i don't know which Unix you have, but it shouldn't matter anyway.
> If you're able to compile programs on the machine then download
> the R source, compile it and install it into your local directory.
>
> Gabor
>
>
> On Fri, Dec 21, 2007 at 10:25:36AM -0500, Wensui Liu wrote:
> > Good morning, Dear Lister,
> > I really like to use R on our unix server. However, I am not an admin
> > of the system.
> > I am wondering if it is possible to install R on unix without admin previlege.
> >
> > Thank you so much! Have a nice Xmas season!
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK
>



-- 
===============================
WenSui Liu
Statistical Project Manager
ChoicePoint Precision Marketing
(http://spaces.msn.com/statcompute/blog)


From tlumley at u.washington.edu  Fri Dec 21 16:54:36 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 21 Dec 2007 07:54:36 -0800 (PST)
Subject: [R] "gam()" in "gam" package
In-Reply-To: <4b2e15cd0712191622l38a5f904n70ba3e6539fe6808@mail.gmail.com>
References: <s767a051.089@tedmail.lgc.co.uk>
	<4b2e15cd0712181710k68e930bdi938587229f6b6438@mail.gmail.com>
	<4b2e15cd0712191622l38a5f904n70ba3e6539fe6808@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0712210750320.16449@homer24.u.washington.edu>

On Thu, 20 Dec 2007, Kunio takezawa wrote:
> R-users
> E-mail: r-help at r-project.org
>
>  I found the answer myself.
>  '.Fortran("baklo",' in lo.wam() and  .Fortran("bakfit",in
> s.wam() may carry out backfitting. But I cannot
> create an R code which gives the same results as those of
> "bakfit". If someone knows the detail of "bakfit" algorithm,

The source code for both Fortran routines is in the package.  If I recal 
correctly the book by Hastie & Tibshirani has descriptions of 
the algorithms they used.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From xkneifl at mendelu.cz  Fri Dec 21 17:01:12 2007
From: xkneifl at mendelu.cz (Michal Kneifl)
Date: Fri, 21 Dec 2007 17:01:12 +0100
Subject: [R] Sorry for "off-topic"
References: <20071220140634.F1689FACA21@as220.org><799216B1-BA7C-4698-8332-2A516027A02B@gmail.com>
	<20071221153423.A10A8FAC9AB@as220.org>
Message-ID: <000201c843ea$b5be6060$a64bb2c3@Mistr1>

Mary Christmas and happy new year to all contributors:-)
Michael


From jholtman at gmail.com  Fri Dec 21 17:02:39 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 21 Dec 2007 11:02:39 -0500
Subject: [R] Sorting factors
In-Reply-To: <20071221153423.A10A8FAC9AB@as220.org>
References: <20071220140634.F1689FACA21@as220.org>
	<799216B1-BA7C-4698-8332-2A516027A02B@gmail.com>
	<20071221153423.A10A8FAC9AB@as220.org>
Message-ID: <644e1f320712210802q2d7b5158w636c9bc7a14120c@mail.gmail.com>

By default, factors are characters and sorted in alphabetical order.
It looks like somehow you numeric data was converted to factors.
Therefore '10' comes before '2'.  If you want the factors in numeric
order you have to convert them back.  Look in the FAQs.

> x <- c(1,2,3,4,5,10,11,20,21,22,30)
> sort(x)
 [1]  1  2  3  4  5 10 11 20 21 22 30
> x.f <- factor(as.character(x))
> str(x.f)
 Factor w/ 11 levels "1","10","11",..: 1 4 8 10 11 2 3 5 6 7 ...
> sort(x.f) # alphabetical order
 [1] 1  10 11 2  20 21 22 3  30 4  5
Levels: 1 10 11 2 20 21 22 3 30 4 5
> sort(as.numeric(as.character(x.f))) # 'expected' numeric order
 [1]  1  2  3  4  5 10 11 20 21 22 30
>


On Dec 21, 2007 10:34 AM, tom sgouros <tomfool at as220.org> wrote:
>
> Hello all:
>
> I'm sorry to take up bandwidth with easy questions, but as an R
> beginner, I continue to be surprised by factors.  If someone can shed
> some light for me, I'd be grateful:
>
> > d
>  Total z02801 z02802 z02804 z02806 z02807 z02808 z02809 z02812 z02813
>  54813     29     51    169   2368    103     76   1328    112    501
> 507 Levels: 0 10 1001 1004 1008 1016 1027 1028 103 1031 10318 1043 1045 ... Na
> > sort(d)
> z02807 z02812 z02809 z02804 z02806 z02801 z02813 z02802  Total z02808
>   103    112   1328    169   2368     29    501     51  54813     76
> 507 Levels: 0 10 1001 1004 1008 1016 1027 1028 103 1031 10318 1043 1045 ... Na
>
> Apparently this factor is sorted in alphabetic order, not numeric order.
> I find no parameter of sort() that controls for this.
>
> And yet:
>
> > mode(d)
> [1] "numeric"
> > length(d)
> [1] 10
>
> I would have thought that because d is a numeric list of ten values, I'd
> get them sorted in numeric order.
>
> Can someone help me understand why this is expected behavior, and also
> what I should do in order to see the numerically sorted list of values I
> really want?
>
> Many thanks,
>
>  -tom
>
>
> --
>  ------------------------
>  tomfool at as220 dot org
>  http://sgouros.com
>  http://whatcheer.net
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From David.John.Thompson at ontario.ca  Fri Dec 21 17:03:54 2007
From: David.John.Thompson at ontario.ca (Thompson, David (MNR))
Date: Fri, 21 Dec 2007 11:03:54 -0500
Subject: [R] Solution:  Available environment variables
In-Reply-To: <476AC42E.8070806@stats.uwo.ca>
References: <ECF21B71808ECF4F8918C57EDBEE121D0189F10A@CTSPITDCEMMVX11.cihs.ad.gov.on.ca>
	<476AC42E.8070806@stats.uwo.ca>
Message-ID: <ECF21B71808ECF4F8918C57EDBEE121D0189F1C4@CTSPITDCEMMVX11.cihs.ad.gov.on.ca>

Thank you Duncan (additional comments below),

>-----Original Message-----
>From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
>Sent: December 20, 2007 02:36 PM
>To: Thompson, David (MNR)
>Cc: r-help at r-project.org
>Subject: Re: [R] Available environment variables
>
>On 20/12/2007 2:13 PM, Thompson, David (MNR) wrote:
>> Hello,
>> 
>> I am trying to set my environment to streamline the downloading /
>> updating of packages. I have been through R-intro.html (10.8
>> Customizing-the-environment), R-FAQ (5.2 How can add-on packages be
>> installed?), rw-FAQ, and help pages for Sys.setenv {base},
>> download.packages {utils}, etc.,.
>> 
>> I am looking for something similar to the download.packages destdir
>> argument on a global environment level, e.g., destdir <- 'C:/Program
>> Files/_dave/stats/R/library/_download' in Rprofile.site. Any
>> suggestions?
>
>You could create your own function:
>
>download.packages <- function(pkgs, destdir='C:/Program 
>Files/_dave/stats/R/library/_download', ...)
>   utils::download.packages(pkgs, destdir, ...)
>
>> Also, in a more general context, is there a way of listing all
available
>> (or common, or default, or standard, . . .) environment variables?
>
>Sys.getenv() will return a list of the ones that are currently defined.
> 
>> I would like to know if there is an existing convention for any
arbitrary
>> process that I may try to do without reinventing the wheel. This same
>> question could apply to options() other than those included with a
>> standard installation.
>
>I don't know what you mean about an "arbitrary process".
>
>For options(), you could save the names that you consider standard
(e.g. 
>by starting an empty session and entering standardoptions <- 
>names(options()), then at a later time do
>
>newoptions <- names(options())
>newoptions[ !(newoptions %in% standardoptions) ]
>
>There's no easy way to know the options that would mean something in
the 
>current context but are not currently defined.
>Duncan Murdoch

Creating a local copy of the function with alternate argument defaults
works perfectly. I have done that in the past, don't know why I didn't
think of it this time. Thank you.

Saving initial- and post-session options (or their names) also nicely
addresses part of my concern. Thank you.

I am familiar with Sys.getenv() and was asking about potential
environment variables that are NOT currently defined. However, while
trying to rephrase my question, having reread the Startup {base} help
and related documentation, could not clearly refine the issue in my own
mind so, I am now content with the situation.

I think that I may be confusing the intended use of environment files
and profile files as noted in the details section of the Startup {base}
documentation. Part of my concern is to find a way of safeguarding
against my own errors and typos that may 'break the system' or having
something installed later (the "arbitrary process" part) breaking my
current setup. Also, I may be thinking of the startup process as
something 'different' from the interactive R session processing, which I
am beginning to see it is not.

Thanks again, DaveT.
*************************************
Silviculture Data Analyst
Ontario Forest Research Institute
Ontario Ministry of Natural Resources
david.john.thompson at ontario.ca
http://ofri.mnr.gov.on.ca


From ggrothendieck at gmail.com  Fri Dec 21 17:09:48 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 21 Dec 2007 11:09:48 -0500
Subject: [R] Finding overlaps in vector
In-Reply-To: <fkfrm5$vj3$1@ger.gmane.org>
References: <fkfrm5$vj3$1@ger.gmane.org>
Message-ID: <971536df0712210809w56b96e6arb20eee05368bef26@mail.gmail.com>

This may not be as direct as Jim's in terms of specifying granularity but
will uses conventional hierarchical clustering to create the clusters and also
draws a nice dendrogram for you.   I have split the dendrogram at a
height of 0.5
to define the clusters but you can change that to whatever granularity you like:

> v <- c(0, 0.45, 1, 2, 3, 3.25, 3.33, 3.75, 4.1, 5, 6, 6.45, 7, 7.1, 8)
>
> # cluster and plot
> hc <- hclust(dist(v), method = "single")
> plot(hc, lab = v)
> cl <- rect.hclust(hc, h = .5, border = "red")
>
> # each component of list cl is one cluster.  Print them out.
> for(idx in cl) print(unname(v[idx]))
[1] 8
[1] 7.0 7.1
[1] 6.00 6.45
[1] 5
[1] 3.00 3.25 3.33 3.75 4.10
[1] 2
[1] 1
[1] 0.00 0.45

> # a different representation of the clusters
> vv <- v
> names(vv) <- ct <- cutree(hc, h = .5)
> vv
   1    1    2    3    4    4    4    4    4    5    6    6    7    7    8
0.00 0.45 1.00 2.00 3.00 3.25 3.33 3.75 4.10 5.00 6.00 6.45 7.00 7.10 8.00


On Dec 21, 2007 4:56 AM, Johannes Graumann <johannes_graumann at web.de> wrote:
> <posted & mailed>
>
> Dear all,
>
> I'm trying to solve the problem, of how to find clusters of values in a
> vector that are closer than a given value. Illustrated this might look as
> follows:
>
> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>
> When using '0.5' as the proximity requirement, the following groups would
> result:
> 0,0.45
> 3,3.25,3.33,3.75,4.1
> 6,6.45
> 7,7.1
>
> Jim Holtman proposed a very elegant solution in
> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I have
> modified and perused since he wrote it to me. The beauty of this approach
> is that it will not only work for constant proximity requirements as above,
> but also for overlap-windows defined in terms of ppm around each value.
> Now I have an additional need and have found no way (short of iteratively
> step through all the groups returned) to figure out how to do that with
> Jim's approach: how to figure out that 6,6.45 and 7,7.1 are separate
> clusters?
>
> Thanks for any hints, Joh
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From csardi at rmki.kfki.hu  Fri Dec 21 17:25:54 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Fri, 21 Dec 2007 17:25:54 +0100
Subject: [R] install R on Unix without Admin Privilege
In-Reply-To: <1115a2b00712210746l65003281o61a102635626389f@mail.gmail.com>
References: <1115a2b00712210725t1b8cebdar910c395162c12e69@mail.gmail.com>
	<20071221154259.GA25601@localdomain>
	<1115a2b00712210746l65003281o61a102635626389f@mail.gmail.com>
Message-ID: <20071221162554.GB25601@localdomain>

On Fri, Dec 21, 2007 at 10:46:39AM -0500, Wensui Liu wrote:
> HI, Gabor,
> what program do i need to compile R on unix?

At least a C compiler, maybe a Fortran compiler too.
Some standard Unix tools like make as well. See also:
http://cran.r-project.org/doc/manuals/R-admin.html

> local dir? do you mean dir under my user name with my profile?

I mean any directory you've write access to, on a disk where you've 
enough space to install R.

Gabor

> On Dec 21, 2007 10:42 AM, Gabor Csardi <csardi at rmki.kfki.hu> wrote:
> > Well i don't know which Unix you have, but it shouldn't matter anyway.
> > If you're able to compile programs on the machine then download
> > the R source, compile it and install it into your local directory.
> >
> > Gabor
> >
> >
> > On Fri, Dec 21, 2007 at 10:25:36AM -0500, Wensui Liu wrote:
> > > Good morning, Dear Lister,
> > > I really like to use R on our unix server. However, I am not an admin
> > > of the system.
> > > I am wondering if it is possible to install R on unix without admin previlege.
> > >
> > > Thank you so much! Have a nice Xmas season!
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK
> >
> 
> 
> 
> -- 
> ===============================
> WenSui Liu
> Statistical Project Manager
> ChoicePoint Precision Marketing
> (http://spaces.msn.com/statcompute/blog)
> ===============================

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From philipp.fechteler at hhi.fraunhofer.de  Fri Dec 21 17:56:32 2007
From: philipp.fechteler at hhi.fraunhofer.de (Philipp Fechteler)
Date: Fri, 21 Dec 2007 17:56:32 +0100
Subject: [R] R script to start session (without automatically finishing)
Message-ID: <476BF040.609@hhi.fraunhofer.de>

Hello R friends

I am quite impressed by the power of R, I am using it only since some 
weeks now. But its visualizing capabilities are outstanding!

But one thing I couldn't solve: I have programs producing lots of data, 
most times 3D. In R I am using the library rgl to visualize nicely the 
3D data.

What I would like to do is to write R scipts which read in a data file 
and show me the 3D rgl plot. So that on the command line I just call my 
newly written R script, which pops up the rgl window. This session 
should run until I close the rgl window or press a button or something 
like this.

Currently, I can write R scripts
- using Rscript or
- http://tolstoy.newcastle.edu.au/R/help/04/05/0500.html
But both approaches contain the problem that the rgl window pops up for 
just a moment, and then the program terminates.


Does any body has an idea, what to do? Help would be very appreciated.

Thank's a lot and ... Marry Christmas + Happy New Year

Philipp


________________________________________________________________________
Dipl.-Ing. Philipp Fechteler
Department Image Processing
Fraunhofer Institute for Telecommunications
Heinrich-Hertz-Institut (HHI)
Einsteinufer 37
10587 Berlin, Germany
Phone	+49 30 31002 616
Fax	+49 30 392 72 00
Email	philipp.fechteler at hhi.fraunhofer.de
WWW	http://iphome.hhi.de/fechteler
________________________________________________________________________

----
Visit us at

FOE 2008 (8th Fiber Optics Expo) / Tokio Big Sight, Japan / 16 - 18 January 2008 / Booth 2 - 50
http://www.foe.jp/english


From phguardiol at aol.com  Fri Dec 21 19:29:28 2007
From: phguardiol at aol.com (phguardiol at aol.com)
Date: Fri, 21 Dec 2007 13:29:28 -0500
Subject: [R] R appearance under linux
Message-ID: <8CA12168E8485A4-AB8-A02@webmail-da15.sysops.aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071221/b42220ea/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Dec 21 19:35:21 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 21 Dec 2007 18:35:21 +0000 (GMT)
Subject: [R] R script to start session (without automatically finishing)
In-Reply-To: <476BF040.609@hhi.fraunhofer.de>
References: <476BF040.609@hhi.fraunhofer.de>
Message-ID: <Pine.LNX.4.64.0712211830570.12850@gannet.stats.ox.ac.uk>

So you have to write a script that will not terminate until you 'press a 
button or something like this'.  That's easy to do, but as you haven't 
told us your OS.  E.g. on Windows, call winDialogString at the end.

On Fri, 21 Dec 2007, Philipp Fechteler wrote:

> Hello R friends
>
> I am quite impressed by the power of R, I am using it only since some
> weeks now. But its visualizing capabilities are outstanding!
>
> But one thing I couldn't solve: I have programs producing lots of data,
> most times 3D. In R I am using the library rgl to visualize nicely the
> 3D data.
>
> What I would like to do is to write R scipts which read in a data file
> and show me the 3D rgl plot. So that on the command line I just call my
> newly written R script, which pops up the rgl window. This session
> should run until I close the rgl window or press a button or something
> like this.
>
> Currently, I can write R scripts
> - using Rscript or
> - http://tolstoy.newcastle.edu.au/R/help/04/05/0500.html
> But both approaches contain the problem that the rgl window pops up for
> just a moment, and then the program terminates.
>
>
> Does any body has an idea, what to do? Help would be very appreciated.
>
> Thank's a lot and ... Marry Christmas + Happy New Year
>
> Philipp
>
>
> ________________________________________________________________________
> Dipl.-Ing. Philipp Fechteler
> Department Image Processing
> Fraunhofer Institute for Telecommunications
> Heinrich-Hertz-Institut (HHI)
> Einsteinufer 37
> 10587 Berlin, Germany
> Phone	+49 30 31002 616
> Fax	+49 30 392 72 00
> Email	philipp.fechteler at hhi.fraunhofer.de
> WWW	http://iphome.hhi.de/fechteler
> ________________________________________________________________________


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From cberry at tajo.ucsd.edu  Fri Dec 21 19:41:46 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 21 Dec 2007 10:41:46 -0800
Subject: [R] Finding overlaps in vector
In-Reply-To: <fkfrm5$vj3$1@ger.gmane.org>
References: <fkfrm5$vj3$1@ger.gmane.org>
Message-ID: <Pine.LNX.4.64.0712211037480.20938@tajo.ucsd.edu>

On Fri, 21 Dec 2007, Johannes Graumann wrote:

> <posted & mailed>
>
> Dear all,
>
> I'm trying to solve the problem, of how to find clusters of values in a
> vector that are closer than a given value. Illustrated this might look as
> follows:
>
> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>
> When using '0.5' as the proximity requirement, the following groups would
> result:
> 0,0.45
> 3,3.25,3.33,3.75,4.1
> 6,6.45
> 7,7.1

Try this:

> tmp <- rle( diff(v)<.5 )
> ends <- 1+cumsum(tmp$lengths)[tmp$values]
> mapply(function(x,y) v[ seq(to=x,length=y) ], ends, 1+tmp$lengths[tmp$values])
[[1]]
[1] 0.00 0.45

[[2]]
[1] 3.00 3.25 3.33 3.75 4.10

[[3]]
[1] 6.00 6.45

[[4]]
[1] 7.0 7.1


HTH,

Chuck

>
> Jim Holtman proposed a very elegant solution in
> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I have
> modified and perused since he wrote it to me. The beauty of this approach
> is that it will not only work for constant proximity requirements as above,
> but also for overlap-windows defined in terms of ppm around each value.
> Now I have an additional need and have found no way (short of iteratively
> step through all the groups returned) to figure out how to do that with
> Jim's approach: how to figure out that 6,6.45 and 7,7.1 are separate
> clusters?
>
> Thanks for any hints, Joh
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From Jacqueline.Spilak at EC.gc.ca  Fri Dec 21 20:00:23 2007
From: Jacqueline.Spilak at EC.gc.ca (Spilak,Jacqueline [Edm])
Date: Fri, 21 Dec 2007 12:00:23 -0700
Subject: [R] adding lines to a barchart
In-Reply-To: <eb555e660712191231q74bb7992kc5e3dc63e75820e5@mail.gmail.com>
Message-ID: <4A6AB38B55B49C44A22E021A83CBEDDB021CA52E@sr-pnr-exch3.prairie.int.ec.gc.ca>

Thanks Deepayan, this helped a lot and gave me exactly what I wanted
however I want a few changes and am not sure how to do them.  In each
panel the bars for the years are the same color with the bottom axis
(the x-axis) labelled with the years.  I would like each year to have a
specific color with a legend for it at the side, is this possible?
And my other question is that there seems to be a gap between where the
bars start and the x-axis, it is kinda funny looking so is there anyway
to change this?
Thanks for all the help

On 12/19/07, Spilak,Jacqueline [Edm] <Jacqueline.Spilak at ec.gc.ca> wrote:
> Hi all
> I can't find what I am looking for so I am asking here.  I have a 
> dataset that looks something like this.
>
> Year   season  percent_below
> 2000 Winter     6.9179870
> 2000 Spring     1.6829436
> 2000 Summer     1.8463501
> 2000 Autumn     3.8184993
> 2001 Winter     2.8832806
> 2001 Spring     2.5870511
> 2001 Summer     0.0000000
> 2001 Autumn     4.7248240
> 2002 Winter     4.4532238
> 2002 Spring     3.7468846
> 2002 Summer     1.0784311
> 2002 Autumn     3.7061533
>
> I have plotted this nicely using
> barchart(percent_below ~ factor(Season, levels=Season), data= 
> dataset1,
> group=Year)
> This gives me a barplot by season with each year shown in that season.
> Now the tricky part.  My data is from 2000 to 2007 and 2007 is an 
> important year.  I have an upper and lower limit for percent_below for
> 2007 for each season.  I would like to add the upper and lower limit 
> of
> 2007 as a black line to each season.  The upper and lower limit will 
> be different for each season.  This is being done so for a comparison 
> so I need it done this way.

I would suggest a slightly different format. Here's an example with one
set of limits; you can add another set similarly. If you really want a
single panel, you could use panel.segments().

barchart(percent_below ~ factor(Year) | factor(season,
levels=unique(season)),
         data= dataset1, origin = 0, layout = c(4, 1),
         upper_2007 = c(6, 4, 5, 3),
         panel = function(..., upper_2007) {
             panel.abline(h = upper_2007[packet.number()])
             panel.barchart(...)
         })

-Deepayan


From blacertain at gmail.com  Fri Dec 21 20:20:18 2007
From: blacertain at gmail.com (bernardo lagos alvarez)
Date: Fri, 21 Dec 2007 16:20:18 -0300
Subject: [R] matrix to gal object
Message-ID: <b28bd39c0712211120n51490877w3e160b14b64d6464@mail.gmail.com>

useR's

I need transform the matrix

wdat
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    0    1    0    1    1    0
[2,]    1    0    0    1    1    0
[3,]    0    0    0    0    1    1
[4,]    1    1    0    0    1    0
[5,]    1    1    1    1    0    0
[6,]    0    0    1    0    0    0


to  gal object. How I do with spdep?


Thanks in advance for the help.

Bernardo.
University  of Concepci?.


From deepayan.sarkar at gmail.com  Fri Dec 21 20:31:18 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 21 Dec 2007 11:31:18 -0800
Subject: [R] adding lines to a barchart
In-Reply-To: <4A6AB38B55B49C44A22E021A83CBEDDB021CA52E@sr-pnr-exch3.prairie.int.ec.gc.ca>
References: <eb555e660712191231q74bb7992kc5e3dc63e75820e5@mail.gmail.com>
	<4A6AB38B55B49C44A22E021A83CBEDDB021CA52E@sr-pnr-exch3.prairie.int.ec.gc.ca>
Message-ID: <eb555e660712211131s896e1cck66c9aa01162228b7@mail.gmail.com>

On 12/21/07, Spilak,Jacqueline [Edm] <Jacqueline.Spilak at ec.gc.ca> wrote:
> Thanks Deepayan, this helped a lot and gave me exactly what I wanted
> however I want a few changes and am not sure how to do them.  In each
> panel the bars for the years are the same color with the bottom axis
> (the x-axis) labelled with the years.  I would like each year to have a
> specific color with a legend for it at the side, is this possible?

Sure, just use Year as a grouping variable as well.

barchart(percent_below ~ factor(Year) | factor(season, levels=unique(season)),
         data = dataset1, layout = c(4, 1),
         groups = factor(Year),
         auto.key = list(points = FALSE, rectangles = TRUE),
         stack = TRUE,
         upper_2007 = c(6, 4, 5, 3),
         panel = function(..., upper_2007) {
             panel.abline(h = upper_2007[packet.number()])
             panel.barchart(...)
         })


> And my other question is that there seems to be a gap between where the
> bars start and the x-axis, it is kinda funny looking so is there anyway
> to change this?

What exactly do you want to happen? I don't think a bar chart is an
appropriate visualization unless the lengths of the bars mean
something. If it does, and you just want to get rid of the space below
zero, the easiest way is to set an explicit ylim=c(0, 7) or something.
If the lengths don't really mean anything, you should consider using
dotplot() instead.

-Deepayan

> Thanks for all the help
>
> On 12/19/07, Spilak,Jacqueline [Edm] <Jacqueline.Spilak at ec.gc.ca> wrote:
> > Hi all
> > I can't find what I am looking for so I am asking here.  I have a
> > dataset that looks something like this.
> >
> > Year   season  percent_below
> > 2000 Winter     6.9179870
> > 2000 Spring     1.6829436
> > 2000 Summer     1.8463501
> > 2000 Autumn     3.8184993
> > 2001 Winter     2.8832806
> > 2001 Spring     2.5870511
> > 2001 Summer     0.0000000
> > 2001 Autumn     4.7248240
> > 2002 Winter     4.4532238
> > 2002 Spring     3.7468846
> > 2002 Summer     1.0784311
> > 2002 Autumn     3.7061533
> >
> > I have plotted this nicely using
> > barchart(percent_below ~ factor(Season, levels=Season), data=
> > dataset1,
> > group=Year)
> > This gives me a barplot by season with each year shown in that season.
> > Now the tricky part.  My data is from 2000 to 2007 and 2007 is an
> > important year.  I have an upper and lower limit for percent_below for
> > 2007 for each season.  I would like to add the upper and lower limit
> > of
> > 2007 as a black line to each season.  The upper and lower limit will
> > be different for each season.  This is being done so for a comparison
> > so I need it done this way.
>
> I would suggest a slightly different format. Here's an example with one
> set of limits; you can add another set similarly. If you really want a
> single panel, you could use panel.segments().
>
> barchart(percent_below ~ factor(Year) | factor(season,
> levels=unique(season)),
>          data= dataset1, origin = 0, layout = c(4, 1),
>          upper_2007 = c(6, 4, 5, 3),
>          panel = function(..., upper_2007) {
>              panel.abline(h = upper_2007[packet.number()])
>              panel.barchart(...)
>          })
>
> -Deepayan
>


From C.Rosa at lse.ac.uk  Fri Dec 21 20:52:29 2007
From: C.Rosa at lse.ac.uk (C.Rosa at lse.ac.uk)
Date: Fri, 21 Dec 2007 19:52:29 -0000
Subject: [R] Numerical precision problem
Message-ID: <8760D6386F7F784693F05256139A1FFB70F87A@EXCHF3.lse.ac.uk>

Dear All,

 

I have a numerical problem:

R, as other statistical software, can tell the difference between very small numbers and 0, while apparently cannot distinguish a number close to 1 and 1. In the example below, is it possible to instruct R to recognize that q is different from 1?

> p=.00000000000000000000000000000001

> p==0

[1] FALSE

> q=.99999999999999999999999999999

> q==1

[1] TRUE

Interestingly, 

> (1-p)==1

[1] TRUE

The main problem I have is that I need to evaluate the inverse of the normal Cumulative Distribution Function near 1 (but not exactly 1) and the PC cannot recognize it.

Thank you very much for your help!

Carlo


Please access the attached hyperlink for an important electronic communications disclaimer: http://www.lse.ac.uk/collections/secretariat/legal/disclaimer.htm


From murdoch at stats.uwo.ca  Fri Dec 21 21:15:02 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 21 Dec 2007 15:15:02 -0500
Subject: [R] Numerical precision problem
In-Reply-To: <8760D6386F7F784693F05256139A1FFB70F87A@EXCHF3.lse.ac.uk>
References: <8760D6386F7F784693F05256139A1FFB70F87A@EXCHF3.lse.ac.uk>
Message-ID: <476C1EC6.6000706@stats.uwo.ca>

On 21/12/2007 2:52 PM, C.Rosa at lse.ac.uk wrote:
> Dear All,
> 
>  
> 
> I have a numerical problem:
> 
> R, as other statistical software, can tell the difference between very small numbers and 0, while apparently cannot distinguish a number close to 1 and 1. In the example below, is it possible to instruct R to recognize that q is different from 1?

No, the limitations are inherent in the floating point representation 
that R uses.

>> p=.00000000000000000000000000000001
> 
>> p==0
> 
> [1] FALSE
> 
>> q=.99999999999999999999999999999
> 
>> q==1
> 
> [1] TRUE
> 
> Interestingly, 
> 
>> (1-p)==1
> 
> [1] TRUE
> 
> The main problem I have is that I need to evaluate the inverse of the normal Cumulative Distribution Function near 1 (but not exactly 1) and the PC cannot recognize it.

In this particular case, it's easy:  make use of the symmetry of the 
distribution.  If you want the 1-epsilon quantile of a normal with mean 
mu, find the epsilon quantile, and reflect it through mu:

mu <- 3
-qnorm(1e-30, mean=-mu)

More generally, you can make use of the log.p argument to the quantile 
functions, and the fact that log(1-epsilon) is close to -epsilon:

qnorm(-1e-30, mean=mu, log.p=TRUE)

Duncan Murdoch

> 
> Thank you very much for your help!
> 
> Carlo
> 
> 
> Please access the attached hyperlink for an important electronic communications disclaimer: http://www.lse.ac.uk/collections/secretariat/legal/disclaimer.htm
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From johannes_graumann at web.de  Fri Dec 21 21:04:07 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Fri, 21 Dec 2007 21:04:07 +0100
Subject: [R] Finding overlaps in vector
References: <fkfrm5$vj3$1@ger.gmane.org>
	<971536df0712210809w56b96e6arb20eee05368bef26@mail.gmail.com>
Message-ID: <fkh67k$dqc$2@ger.gmane.org>

Thank you very much for this elegant solution to the problem. The reason I
still hope for an extension of Jim's code (not the one re responded with in
this thread, but the one I actually reference) is that windows of overlap
can be asymetric with that: one can check e.g. whether values overlap given
the constraints that the closest allowed proximity 'down' is 0.5 and 'up'
is 0.75. I would highly cherish a solution that would allow for cluster
isolation with that requirement.

Thanks for your time and insight,

Joh

Gabor Grothendieck wrote:

> This may not be as direct as Jim's in terms of specifying granularity but
> will uses conventional hierarchical clustering to create the clusters and
> also
> draws a nice dendrogram for you.   I have split the dendrogram at a
> height of 0.5
> to define the clusters but you can change that to whatever granularity you
> like:
> 
>> v <- c(0, 0.45, 1, 2, 3, 3.25, 3.33, 3.75, 4.1, 5, 6, 6.45, 7, 7.1, 8)
>>
>> # cluster and plot
>> hc <- hclust(dist(v), method = "single")
>> plot(hc, lab = v)
>> cl <- rect.hclust(hc, h = .5, border = "red")
>>
>> # each component of list cl is one cluster.  Print them out.
>> for(idx in cl) print(unname(v[idx]))
> [1] 8
> [1] 7.0 7.1
> [1] 6.00 6.45
> [1] 5
> [1] 3.00 3.25 3.33 3.75 4.10
> [1] 2
> [1] 1
> [1] 0.00 0.45
> 
>> # a different representation of the clusters
>> vv <- v
>> names(vv) <- ct <- cutree(hc, h = .5)
>> vv
>    1    1    2    3    4    4    4    4    4    5    6    6    7    7    8
> 0.00 0.45 1.00 2.00 3.00 3.25 3.33 3.75 4.10 5.00 6.00 6.45 7.00 7.10 8.00
> 
> 
> On Dec 21, 2007 4:56 AM, Johannes Graumann <johannes_graumann at web.de>
> wrote:
>> <posted & mailed>
>>
>> Dear all,
>>
>> I'm trying to solve the problem, of how to find clusters of values in a
>> vector that are closer than a given value. Illustrated this might look as
>> follows:
>>
>> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>>
>> When using '0.5' as the proximity requirement, the following groups would
>> result:
>> 0,0.45
>> 3,3.25,3.33,3.75,4.1
>> 6,6.45
>> 7,7.1
>>
>> Jim Holtman proposed a very elegant solution in
>> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I have
>> modified and perused since he wrote it to me. The beauty of this approach
>> is that it will not only work for constant proximity requirements as
>> above, but also for overlap-windows defined in terms of ppm around each
>> value. Now I have an additional need and have found no way (short of
>> iteratively step through all the groups returned) to figure out how to do
>> that with Jim's approach: how to figure out that 6,6.45 and 7,7.1 are
>> separate clusters?
>>
>> Thanks for any hints, Joh
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From johannes_graumann at web.de  Fri Dec 21 20:59:29 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Fri, 21 Dec 2007 20:59:29 +0100
Subject: [R] Finding overlaps in vector
References: <fkfrm5$vj3$1@ger.gmane.org>
	<644e1f320712210732p69c60969n827a7f0dd98942b9@mail.gmail.com>
Message-ID: <fkh5uu$dqc$1@ger.gmane.org>

Jim,

Although I can't find the post this code stems from, I had come across it on
my prowling the NG. It's not the one you had shared with me to eliminate
overlaps (and which I referenced below:
http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html). That
particular solution you had come up with marked entries as overlapping or
not, and I am looking for an extension to that code which would also return
the actual "clusters" of consecutively overlapping values. While Gabor's
code in this thread does what I require for the example I still hope
somebody more cluefull than myself can extent your code since it carries
the - for me - significant advantage of being able to build the windows of
overlap with different values for 'up' and 'down', let's say check which
values overlap when the overlap-defining distance is 5ppm 'up' and
7.5ppm 'down' from each value. This is a generalization I would highly
cherish.

Thanks for your help and continuous patience on r-help.

Joh

jim holtman wrote:

> Here is a modification of the algorithm to use a specified value for
> the overlap:
> 
>> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>> # following add 0.5 as the overlap detection -- can be changed
>> x <- rbind(cbind(value=vector, oper=1, id=seq_along(vector)),
> +            cbind(value=vector+0.5, oper=-1, id=seq_along(vector)))
>> x <- x[order(x[,'value'], -x[, 'oper']),]
>> # determine which ones overlap
>> x <- cbind(x, over=cumsum(x[, 'oper']))
>> # now partition into groups and only use groups greater than or equal to
>> # 3 determine where the breaks are (0 values in cumsum(over))
>> x <- cbind(x, breaks=cumsum(x[, 'over'] == 0))
>> # delete entries with 'over' == 0
>> x <- x[x[, 'over'] != 0,]
>> # split into groupd
>> x.groups <- split(x[, 'id'], x[, 'breaks'])
>> # only keep those with more than 2
>> x.subsets <- x.groups[sapply(x.groups, length) >= 3]
>> # print out the subsets
>> invisible(lapply(x.subsets, function(a) print(vector[unique(a)])))
> [1] 0.00 0.45
> [1] 3.00 3.25 3.33 3.75 4.10
> [1] 6.00 6.45
> [1] 7.0 7.1
> 
> 
> On Dec 21, 2007 4:56 AM, Johannes Graumann <johannes_graumann at web.de>
> wrote:
>> <posted & mailed>
>>
>> Dear all,
>>
>> I'm trying to solve the problem, of how to find clusters of values in a
>> vector that are closer than a given value. Illustrated this might look as
>> follows:
>>
>> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>>
>> When using '0.5' as the proximity requirement, the following groups would
>> result:
>> 0,0.45
>> 3,3.25,3.33,3.75,4.1
>> 6,6.45
>> 7,7.1
>>
>> Jim Holtman proposed a very elegant solution in
>> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I have
>> modified and perused since he wrote it to me. The beauty of this approach
>> is that it will not only work for constant proximity requirements as
>> above, but also for overlap-windows defined in terms of ppm around each
>> value. Now I have an additional need and have found no way (short of
>> iteratively step through all the groups returned) to figure out how to do
>> that with Jim's approach: how to figure out that 6,6.45 and 7,7.1 are
>> separate clusters?
>>
>> Thanks for any hints, Joh
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
>>
> 
> 
>


From johannes_graumann at web.de  Fri Dec 21 21:24:43 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Fri, 21 Dec 2007 21:24:43 +0100
Subject: [R] Finding overlaps in vector
References: <fkfrm5$vj3$1@ger.gmane.org>
	<971536df0712210809w56b96e6arb20eee05368bef26@mail.gmail.com>
Message-ID: <fkh7e9$is7$1@ger.gmane.org>

Hm, hm, rect.hclust doesn't accept "plot=FALSE" and cutree doesn't retain
the indexes of membership ... anyway short of ripping out the guts of
rect.hclust to achieve the same result without an active graphics device?

Joh

>> # cluster and plot
>> hc <- hclust(dist(v), method = "single")
>> plot(hc, lab = v)
>> cl <- rect.hclust(hc, h = .5, border = "red")
>>
>> # each component of list cl is one cluster.  Print them out.
>> for(idx in cl) print(unname(v[idx]))
> [1] 8
> [1] 7.0 7.1
> [1] 6.00 6.45
> [1] 5
> [1] 3.00 3.25 3.33 3.75 4.10
> [1] 2
> [1] 1
> [1] 0.00 0.45
> 
>> # a different representation of the clusters
>> vv <- v
>> names(vv) <- ct <- cutree(hc, h = .5)
>> vv
>    1    1    2    3    4    4    4    4    4    5    6    6    7    7    8
> 0.00 0.45 1.00 2.00 3.00 3.25 3.33 3.75 4.10 5.00 6.00 6.45 7.00 7.10 8.00
> 
> 
> On Dec 21, 2007 4:56 AM, Johannes Graumann <johannes_graumann at web.de>
> wrote:
>> <posted & mailed>
>>
>> Dear all,
>>
>> I'm trying to solve the problem, of how to find clusters of values in a
>> vector that are closer than a given value. Illustrated this might look as
>> follows:
>>
>> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>>
>> When using '0.5' as the proximity requirement, the following groups would
>> result:
>> 0,0.45
>> 3,3.25,3.33,3.75,4.1
>> 6,6.45
>> 7,7.1
>>
>> Jim Holtman proposed a very elegant solution in
>> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I have
>> modified and perused since he wrote it to me. The beauty of this approach
>> is that it will not only work for constant proximity requirements as
>> above, but also for overlap-windows defined in terms of ppm around each
>> value. Now I have an additional need and have found no way (short of
>> iteratively step through all the groups returned) to figure out how to do
>> that with Jim's approach: how to figure out that 6,6.45 and 7,7.1 are
>> separate clusters?
>>
>> Thanks for any hints, Joh
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From francogrex at mail.com  Fri Dec 21 21:35:29 2007
From: francogrex at mail.com (francogrex)
Date: Fri, 21 Dec 2007 12:35:29 -0800 (PST)
Subject: [R] Forestplot
In-Reply-To: <14404133.post@talk.nabble.com>
References: <14404133.post@talk.nabble.com>
Message-ID: <14461981.post@talk.nabble.com>


Answering my own query here's a solution (if any reader was curious):

forestplot=function(mean,std,conf,threshold){
z=-qnorm((1-conf)/2)
CI.H <- mean+z*sqrt(std) # Calculate upper CI 
CI.L <- mean-z*sqrt(std) # Calculate lower CI 
plot(mean,1:length(mean),xlim=c(min(CI.L),max(CI.H)),pch=15,cex=1,
ylab="",xlab="mean & CIs",main="Forest plot")
arrows(mean,1:length(mean),CI.H,1:length(mean), angle=90,length=0.05)
arrows(mean,1:length(mean),CI.L,1:length(mean), angle=90,length=0.05)
abline(v=threshold,lty=3)
}


francogrex wrote:
> 
> I know there is a function forestplot from rmeta package and also the
> plot.meta from the meta package and maybe others, but they are rather
> complicated with extra plot parameters that I do not need and also they
> process only objects created with other package functions. 
> But I wonder if anyone has a much simpler function using the basic plot to
> make a forestplot with only a median (or mean) and just the confidence
> intervals...
> 

-- 
View this message in context: http://www.nabble.com/Forestplot-tp14404133p14461981.html
Sent from the R help mailing list archive at Nabble.com.


From jonas.malmros at gmail.com  Fri Dec 21 21:58:27 2007
From: jonas.malmros at gmail.com (Jonas Malmros)
Date: Fri, 21 Dec 2007 21:58:27 +0100
Subject: [R] Diagonal matrix with off diagonal elements
Message-ID: <fd3c7adf0712211258h3bcd83d8r1c4938ceb10510d4@mail.gmail.com>

Hi, everyone

I wonder if there is a function in R with which I can create a square
matrix with elements off main diagonal (for example one diagonal below
the main diagonal).

Thanks in advance!

-- 
Jonas Malmros
Stockholm University
Stockholm, Sweden


From dieter.menne at menne-biomed.de  Fri Dec 21 21:59:05 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 21 Dec 2007 20:59:05 +0000 (UTC)
Subject: [R] post hoc in repeated measures of anova
References: <20071221143043.xe5msd8ic8wo00cc@webmail.uni-konstanz.de>
Message-ID: <loom.20071221T203918-818@post.gmane.org>

Kazumi Maniwa <Kazumi.Maniwa <at> uni-konstanz.de> writes:

> 
> Hallo, I have this dataset with repeated measures. There are two  
> within-subject factors, "formant" (2 levels: 1 and 2) and "f2 Ref" (25  
> levels: 670, 729, 788, 846, 905, 1080, 1100, 1120, 1140, 1170, 1480,  
> 1470, 1450, 1440, 1430, 1890, 1840, 1790, 1740, 1690, 2290, 2210,  
> 2120, 2040, 1950), and one between-subject factor, lang (2 levels:1  
> and 2). The response variable is "thresh".
..
> 
> I ran a three-way ANOVA with repeated measures:
> 
> vThresh<-read.table("rRes.csv",header=T,sep=",")
> attach(vThresh)
>
vThresh.aov<-aov(thresh~factor(lang)*factor(formant)*factor(f2Ref)+
  Error(factor(sub)))

> It will be great if you could help me with codes for post hoc in  
> repeated measures ANOVA.

First, I suggest that you avoid attach(), and convert the numbers to factors 
in the data frame. The formula and the results are much easier to read, and 
the risk of making some mistake is reduced. lm and lme will work perfectly 
without factor(), but the results can be misleading or simply wrong, because 
numbers are treated as what one used to call continuous covariables in the 
old days (still persisting in psychology departments).

vThresh$formant = as.factor(vThresh$formant)

... same for the other factors. Better even, give you factors nice names 
(ok, with formants, numbers _are_ a natural order).

Then, use lm, which gives you a basic set of contrasts. Make sure that 
you understand that (Intercept) stands for the base level of all factors 
(e.g. 1), and the others are roughly differences, or differences of 
differences (interactions).

vThresh.lm = lm(thresh~lang*formant*f2Ref+Error(factor(sub),data=vThresh)

or, even better, use lme instead of lm when you have repeated measurements:

library(nlme)
vThresh.lme = lme(thresh~lang*formant*f2Ref, data=vThresh, random=~1|subject)

You can use stepAIC from MASS to prune down your model to relevant terms.

Then, use estimable in package gmodels, or glht in package multcomp to 
compute the post-hoc tests. 

Getting the contrasts right can be tricky; I use an Excel Spreadsheet to
construct the contrast with a bit of conditional formatting (0 white, yellow 
1) to easier spot errors, and read the named contrast ranges via ODBC.

Dieter

# some auxillary function to construct contrast in Excel and have the
# variable nicely named in estimable

"readContrasts" = 
function(cname,excelfile) {
  require(gdata)
  require(RODBC) # for trim
  channel=odbcConnectExcel(excelfile)
  cn=sqlQuery(channel,paste("select * from",cname),as.is=TRUE)
  odbcClose(channel)
  if (class(cn) != "data.frame") stop(cn[[2]],call.=FALSE)
  cn[,1] = trim(cn[,1]) # trim in gdata
  cn
}

"getContrasts" = 
function(cname,excelfile,rows=NULL) {
  cn=readContrasts(cname,excelfile)
  if (!is.null(rows)) cn = cn[rows,]
  colnames= cn[,1]
  rownames = gsub('#','.',colnames(cn))#[-1]
  # Use _ as placeholder for an empty field
  vars = do.call("rbind", strsplit(rownames,'\\.'))
  vars[vars=='_'] = ''
  # upper left corner must contain the names of the variables
  varnames = vars[1,]
  vars = vars[-1,,drop=FALSE]
  rownames(vars)=rownames[-1]
  colnames(vars)=varnames
  cn = t(as.matrix(cn[,-1]))
  rownames(cn) = rownames[-1]
  colnames(cn) = colnames
  attr(cn,"vars") = data.frame(vars)
  cn
}


From francogrex at mail.com  Fri Dec 21 22:15:10 2007
From: francogrex at mail.com (francogrex)
Date: Fri, 21 Dec 2007 13:15:10 -0800 (PST)
Subject: [R] Forestplot
In-Reply-To: <14461981.post@talk.nabble.com>
References: <14404133.post@talk.nabble.com> <14461981.post@talk.nabble.com>
Message-ID: <14462374.post@talk.nabble.com>


Sorry here's the good code:

forestplot=function(mean,std,conf,threshold){
z=-qnorm((1-conf)/2)
CI.H <- mean+z*std # Calculate upper CI 
CI.L <- mean-z*std # Calculate lower CI 
plot(mean,1:length(mean),xlim=c(min(CI.L),max(CI.H)),pch=15,cex=1,
ylab="",xlab="mean & CIs",main="Forest plot")
arrows(mean,1:length(mean),CI.H,1:length(mean), angle=90,length=0.05)
arrows(mean,1:length(mean),CI.L,1:length(mean), angle=90,length=0.05)
abline(v=threshold,lty=3)
}


-- 
View this message in context: http://www.nabble.com/Forestplot-tp14404133p14462374.html
Sent from the R help mailing list archive at Nabble.com.


From vistocco at unicas.it  Fri Dec 21 22:24:29 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Fri, 21 Dec 2007 22:24:29 +0100
Subject: [R] Diagonal matrix with off diagonal elements
In-Reply-To: <fd3c7adf0712211258h3bcd83d8r1c4938ceb10510d4@mail.gmail.com>
References: <fd3c7adf0712211258h3bcd83d8r1c4938ceb10510d4@mail.gmail.com>
Message-ID: <476C2F0D.3070005@unicas.it>

Jonas Malmros wrote:
> Hi, everyone
>
> I wonder if there is a function in R with which I can create a square
> matrix with elements off main diagonal (for example one diagonal below
> the main diagonal).
>
> Thanks in advance!
>
>   

You could combine rbind, diag and cbind:
rbind(rep(0,3),cbind(diag(2),rep(0,2)))
rbind(rep(0,4),cbind(diag(3),rep(0,3)))
.......

A simple function:

belowDiagonal = function(x)
{
    diagBelow=rbind(rep(0,length(x)+1),cbind(diag(x),rep(0,length(x))))
    return(diagBelow)
}


belowDiagonal(7)
belowDiagonal(1:5)

domenico


From groemping at tfh-berlin.de  Fri Dec 21 22:58:50 2007
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Fri, 21 Dec 2007 22:58:50 +0100
Subject: [R] New Cran Task View: ExperimentalDesign
Message-ID: <20071221213208.M42092@tfh-berlin.de>

Dear UseRs,

the new Task View "ExperimentalDesign" (Title: Design of Experiments (DoE) &
Analysis of Experimental Data) has just been uploaded to CRAN. It covers
packages that have functionalities for designing experiments or specific tools
for analyzing data from designed experiments. I hope that it will be useful
and will become more useful over time with a growing range of functionality in
R in this area.

Regards, Ulrike

 ****************************** 
 Prof. Dr. Ulrike Gr?mping 
 Fachbereich II 
 TFH Berlin 
 Luxemburger Str. 10 
 13353 Berlin 
 Tel.: +49 (0)30 4504 5127 
 Fax.: +49 (0)30 4504 66 5127 
 mail: groemping at tfh-berlin.de 
 www: www.tfh-berlin.de/~groemp/


From tlumley at u.washington.edu  Fri Dec 21 23:02:42 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 21 Dec 2007 14:02:42 -0800 (PST)
Subject: [R] Numerical precision problem
In-Reply-To: <8760D6386F7F784693F05256139A1FFB70F87A@EXCHF3.lse.ac.uk>
References: <8760D6386F7F784693F05256139A1FFB70F87A@EXCHF3.lse.ac.uk>
Message-ID: <Pine.LNX.4.64.0712211359540.32279@homer22.u.washington.edu>

On Fri, 21 Dec 2007, C.Rosa at lse.ac.uk wrote:

> Dear All,
>
>
>
> I have a numerical problem:
>
> R, as other statistical software, can tell the difference between very 
> small numbers and 0, while apparently cannot distinguish a number close 
> to 1 and 1. In the example below, is it possible to instruct R to 
> recognize that q is different from 1?

No, since q is not different from 1.
>
>> q==1
>
> [1] TRUE
>

>
> The main problem I have is that I need to evaluate the inverse of the 
> normal Cumulative Distribution Function near 1 (but not exactly 1) and 
> the PC cannot recognize it.
>

Use the lower.tail= argument to qnorm (or just use the fact that the 
standard Normal is symmetric)

> p=.00000000000000000000000000000001
> p
[1] 1e-32
> qnorm(p,lower.tail=FALSE)
[1] 11.85613
> -qnorm(p)
[1] 11.85613
>
 	-thomas


From jayoung at fhcrc.org  Fri Dec 21 23:27:01 2007
From: jayoung at fhcrc.org (Janet Young)
Date: Fri, 21 Dec 2007 14:27:01 -0800
Subject: [R] RMySQL installation
Message-ID: <29BC2192-BEFE-4567-A639-79D2DF2B2690@fhcrc.org>

Hi,

I am having trouble getting RMySQL running on a solaris machine.
[43] bedrock:/home/jayoung/source_codes/R/other_packages> uname -a
SunOS bedrock 5.10 Generic_118833-36 sun4v sparc SUNW,Sun-Fire-T200

I thought I had finally managed to get it installed, albeit with some  
warnings that I didn't understand (it took me a while to find where  
our mysql libraries were), but when I tried to load it within R I got  
an error:

 > library(RMySQL)
Error in dyn.load(file, ...) :
   unable to load shared library '/home/btrask/traskdata/lib/R/ 
library/RMySQL/libs/RMySQL.so':
   ld.so.1: R: fatal: relocation error: file /home/btrask/traskdata/ 
lib/R/library/RMySQL/libs/RMySQL.so: symbol mysql_more_results:  
referenced symbol not found
Error: package/namespace load failed for 'RMySQL'


I'm not a sysadmin and don't know any C, so I don't really understand  
this error, but I'm wondering whether we might have an older,  
incompatible version of mysql?
[42] bedrock:/home/jayoung/source_codes/R/other_packages> mysql -V
mysql  Ver 12.22 Distrib 4.0.24, for sun-solaris2.10 (sparc)

Or am I somehow failing to specify some of the necessary libraries in  
LD_LIBRARY_PATH or some other thing R is using to look for libraries?

I've pasted the command I used for installation, and the output of  
the build process below, as well as the output of sessionInfo.

Thanks in advance for any help,

Janet Young

-------------------------------------------------------------------

Dr. Janet Young (Trask lab)

Fred Hutchinson Cancer Research Center
1100 Fairview Avenue N., C3-168,
P.O. Box 19024, Seattle, WA 98109-1024, USA.

tel: (206) 667 1471 fax: (206) 667 6524
email: jayoung at fhcrc.org

http://www.fhcrc.org/labs/trask/

-------------------------------------------------------------------




[13] bedrock:/home/jayoung/source_codes/R/other_packages> R CMD  
INSTALL --configure-args='--with-mysql-inc=/usr/sfw/include/mysql -- 
with-mysql-lib=/usr/sfw/lib' RMySQL_0.6-0.tar.gz
* Installing to library '/home/btrask/traskdata/lib/R/library'
* Installing *source* package 'RMySQL' ...
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for compress in -lz... yes
checking for getopt_long in -lc... yes
checking for mysql_init in -lmysqlclient... no
checking for egrep... egrep
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking mysql.h usability... no
checking mysql.h presence... no
checking for mysql.h... no
configure: creating ./config.status
config.status: creating src/Makevars
** libs
cc -I/home/btrask/traskdata/lib/R/include -I/home/btrask/traskdata/ 
lib/R/include -I/usr/sfw/include/mysql -I/usr/local/include    -KPIC   
-g -c RS-DBI.c -o RS-DBI.o
"RS-DBI.c", line 1177: warning: assignment type mismatch:
         pointer to char "=" pointer to const char
"RS-DBI.c", line 1190: warning: implicit function declaration: isalpha
"RS-DBI.c", line 1228: warning: assignment type mismatch:
         pointer to char "=" pointer to const char
cc -I/home/btrask/traskdata/lib/R/include -I/home/btrask/traskdata/ 
lib/R/include -I/usr/sfw/include/mysql -I/usr/local/include    -KPIC   
-g -c RS-MySQL.c -o RS-MySQL.o
"RS-MySQL.c", line 134: warning: implicit function declaration:  
mysql_more_results
"RS-MySQL.c", line 161: warning: implicit function declaration:  
mysql_next_result
"RS-MySQL.c", line 387: warning: assignment type mismatch:
         pointer to char "=" pointer to const char
"RS-MySQL.c", line 389: warning: assignment type mismatch:
         pointer to char "=" pointer to const char
"RS-MySQL.c", line 391: warning: assignment type mismatch:
         pointer to char "=" pointer to const char
"RS-MySQL.c", line 393: warning: assignment type mismatch:
         pointer to char "=" pointer to const char
"RS-MySQL.c", line 395: warning: assignment type mismatch:
         pointer to char "=" pointer to const char
cc -G -L/usr/local/lib -o RMySQL.so RS-DBI.o RS-MySQL.o -L/usr/sfw/ 
lib -lmysqlclient -lz  -L/home/btrask/traskdata/lib/R/lib -lR
** R
** inst
** preparing package for lazy loading
Loading required package: DBI
Creating a new generic function for "format" in "RMySQL"
Creating a new generic function for "print" in "RMySQL"
** help
  >>> Building/Updating help pages for package 'RMySQL'
      Formats: text html latex example
   MySQL                             text    html    latex   example
   MySQLConnection-class             text    html    latex   example
   MySQLDriver-class                 text    html    latex   example
   MySQLObject-class                 text    html    latex   example
   MySQLResult-class                 text    html    latex   example
   RMySQL-package                    text    html    latex   example
   S4R                               text    html    latex   example
   dbApply-methods                   text    html    latex   example
   dbApply                           text    html    latex   example
   dbBuildTableDefinition            text    html    latex
   dbCallProc-methods                text    html    latex
   dbCommit-methods                  text    html    latex   example
   dbConnect-methods                 text    html    latex   example
Note: unmatched right brace in 'dbDataType-methods' on or after line 24
   dbDataType-methods                text    html    latex   example
   dbDriver-methods                  text    html    latex   example
   dbEscapeStrings-methods           text    html    latex   example
   dbEscapeStrings                   text    html    latex   example
   dbGetInfo-methods                 text    html    latex   example
   dbListTables-methods              text    html    latex   example
   dbNextResult-methods              text    html    latex   example
   dbNextResult                      text    html    latex   example
   dbObjectId-class                  text    html    latex   example
   dbReadTable-methods               text    html    latex   example
   dbSendQuery-methods               text    html    latex   example
   dbSetDataMappings-methods         text    html    latex   example
   fetch-methods                     text    html    latex   example
   isIdCurrent                       text    html    latex   example
   make.db.names-methods             text    html    latex   example
   mysqlDBApply                      text    html    latex   example
   mysqlSupport                      text    html    latex
   safe.write                        text    html    latex   example
   summary-methods                   text    html    latex
** building package indices ...
* DONE (RMySQL)

[1] bedrock:/home/jayoung> R

R version 2.6.1 Patched (2007-12-02 r43572)
Copyright (C) 2007 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

 > library(DBI)
 > library(RMySQL)
Error in dyn.load(file, ...) :
   unable to load shared library '/home/btrask/traskdata/lib/R/ 
library/RMySQL/libs/RMySQL.so':
   ld.so.1: R: fatal: relocation error: file /home/btrask/traskdata/ 
lib/R/library/RMySQL/libs/RMySQL.so: symbol mysql_more_results:  
referenced symbol not found
Error: package/namespace load failed for 'RMySQL'
 >  sessionInfo()
R version 2.6.1 Patched (2007-12-02 r43572)
sparc-sun-solaris2.10

locale:
/en_US.ISO8859-15/C/C/en_US.ISO8859-15/en_US.ISO8859-15/C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] DBI_0.2-4

 > q()
Save workspace image? [y/n/c]: n



[2] bedrock:/home/jayoung> printenv LD_LIBRARY_PATH
/opt/SUNWspro/lib:/usr/lib:/opt/SUNWspro/lib:/usr/openwin/lib:/usr/ 
lib:/usr/local/lib:/opt/hpnpl/lib:/opt/sfw/lib:/usr/local/emacs/lib:/ 
usr/local/ImageMagick:/home/btrask/traskdata/lib/R/library/RSPerl/ 
libs:/home/btrask/traskdata/lib/R/lib:/usr/sfw/lib:/opt/local/apps/ 
gdbm-1.7.3/lib:/home/jayoung/traskdata/lib/R/bin:/home/jayoung/ 
traskdata/lib/R/lib:/home/jayoung/traskdata/lib/RSPerl/libs:/home/ 
jayoung/traskdata/lib/RSPerl:/home/jayoung/traskdata/lib/RSPerl/scripts


From ggrothendieck at gmail.com  Fri Dec 21 23:42:55 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 21 Dec 2007 17:42:55 -0500
Subject: [R] Diagonal matrix with off diagonal elements
In-Reply-To: <fd3c7adf0712211258h3bcd83d8r1c4938ceb10510d4@mail.gmail.com>
References: <fd3c7adf0712211258h3bcd83d8r1c4938ceb10510d4@mail.gmail.com>
Message-ID: <971536df0712211442t1ab47096uec27160f05b95b04@mail.gmail.com>

On Dec 21, 2007 3:58 PM, Jonas Malmros <jonas.malmros at gmail.com> wrote:
> I wonder if there is a function in R with which I can create a square
> matrix with elements off main diagonal (for example one diagonal below
> the main diagonal).

Try this and adjust the formula for other patterns:

> mm <- matrix(nr = 5, nc = 5)
> (row(mm) == col(mm) + 1) + 0
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    1    0    0    0    0
[3,]    0    1    0    0    0
[4,]    0    0    1    0    0
[5,]    0    0    0    1    0


From h.wickham at gmail.com  Fri Dec 21 23:49:38 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 21 Dec 2007 22:49:38 +0000
Subject: [R] New Cran Task View: ExperimentalDesign
In-Reply-To: <20071221213208.M42092@tfh-berlin.de>
References: <20071221213208.M42092@tfh-berlin.de>
Message-ID: <f8e6ff050712211449o1613e72ame73612c5b0c2b625@mail.gmail.com>

And the url is:
http://cran.r-project.org/src/contrib/Views/ExperimentalDesign.html

Hadley

On 12/21/07, Ulrike Gr?mping <groemping at tfh-berlin.de> wrote:
> Dear UseRs,
>
> the new Task View "ExperimentalDesign" (Title: Design of Experiments (DoE) &
> Analysis of Experimental Data) has just been uploaded to CRAN. It covers
> packages that have functionalities for designing experiments or specific tools
> for analyzing data from designed experiments. I hope that it will be useful
> and will become more useful over time with a growing range of functionality in
> R in this area.
>
> Regards, Ulrike
>
>  ******************************
>  Prof. Dr. Ulrike Gr?mping
>  Fachbereich II
>  TFH Berlin
>  Luxemburger Str. 10
>  13353 Berlin
>  Tel.: +49 (0)30 4504 5127
>  Fax.: +49 (0)30 4504 66 5127
>  mail: groemping at tfh-berlin.de
>  www: www.tfh-berlin.de/~groemp/
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://had.co.nz/


From stubben at lanl.gov  Fri Dec 21 23:50:31 2007
From: stubben at lanl.gov (Chris Stubben)
Date: Fri, 21 Dec 2007 14:50:31 -0800 (PST)
Subject: [R] Diagonal matrix with off diagonal elements
In-Reply-To: <fd3c7adf0712211258h3bcd83d8r1c4938ceb10510d4@mail.gmail.com>
References: <fd3c7adf0712211258h3bcd83d8r1c4938ceb10510d4@mail.gmail.com>
Message-ID: <14463831.post@talk.nabble.com>


Also try the odiag function in the demogR package

odiag( 1:5, -1)
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    0    0    0    0    0    0
[2,]    1    0    0    0    0    0
[3,]    0    2    0    0    0    0
[4,]    0    0    3    0    0    0
[5,]    0    0    0    4    0    0
[6,]    0    0    0    0    5    0

Chris





Jonas Malmros wrote:
> 
> Hi, everyone
> 
> I wonder if there is a function in R with which I can create a square
> matrix with elements off main diagonal (for example one diagonal below
> the main diagonal).
> 
> Thanks in advance!
> 
> -- 
> Jonas Malmros
> Stockholm University
> Stockholm, Sweden
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Diagonal-matrix-with-off-diagonal-elements-tp14462310p14463831.html
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Fri Dec 21 23:51:27 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 21 Dec 2007 17:51:27 -0500
Subject: [R] Finding overlaps in vector
In-Reply-To: <fkh7e9$is7$1@ger.gmane.org>
References: <fkfrm5$vj3$1@ger.gmane.org>
	<971536df0712210809w56b96e6arb20eee05368bef26@mail.gmail.com>
	<fkh7e9$is7$1@ger.gmane.org>
Message-ID: <971536df0712211451h79513980ldf08edcb0ea28d3e@mail.gmail.com>

If we don't need any plotting we don't really need rect.hclust at
all.  Split the output of cutree, instead.  Continuing from the
prior code:

> for(el in split(unname(vv), names(vv))) print(el)
[1] 0.00 0.45
[1] 1
[1] 2
[1] 3.00 3.25 3.33 3.75 4.10
[1] 5
[1] 6.00 6.45
[1] 7.0 7.1
[1] 8

On Dec 21, 2007 3:24 PM, Johannes Graumann <johannes_graumann at web.de> wrote:
> Hm, hm, rect.hclust doesn't accept "plot=FALSE" and cutree doesn't retain
> the indexes of membership ... anyway short of ripping out the guts of
> rect.hclust to achieve the same result without an active graphics device?
>
> Joh
>
>
> >> # cluster and plot
> >> hc <- hclust(dist(v), method = "single")
> >> plot(hc, lab = v)
> >> cl <- rect.hclust(hc, h = .5, border = "red")
> >>
> >> # each component of list cl is one cluster.  Print them out.
> >> for(idx in cl) print(unname(v[idx]))
> > [1] 8
> > [1] 7.0 7.1
> > [1] 6.00 6.45
> > [1] 5
> > [1] 3.00 3.25 3.33 3.75 4.10
> > [1] 2
> > [1] 1
> > [1] 0.00 0.45
> >
> >> # a different representation of the clusters
> >> vv <- v
> >> names(vv) <- ct <- cutree(hc, h = .5)
> >> vv
> >    1    1    2    3    4    4    4    4    4    5    6    6    7    7    8
> > 0.00 0.45 1.00 2.00 3.00 3.25 3.33 3.75 4.10 5.00 6.00 6.45 7.00 7.10 8.00
> >
> >
> > On Dec 21, 2007 4:56 AM, Johannes Graumann <johannes_graumann at web.de>
> > wrote:
> >> <posted & mailed>
> >>
> >> Dear all,
> >>
> >> I'm trying to solve the problem, of how to find clusters of values in a
> >> vector that are closer than a given value. Illustrated this might look as
> >> follows:
> >>
> >> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
> >>
> >> When using '0.5' as the proximity requirement, the following groups would
> >> result:
> >> 0,0.45
> >> 3,3.25,3.33,3.75,4.1
> >> 6,6.45
> >> 7,7.1
> >>
> >> Jim Holtman proposed a very elegant solution in
> >> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I have
> >> modified and perused since he wrote it to me. The beauty of this approach
> >> is that it will not only work for constant proximity requirements as
> >> above, but also for overlap-windows defined in terms of ppm around each
> >> value. Now I have an additional need and have found no way (short of
> >> iteratively step through all the groups returned) to figure out how to do
> >> that with Jim's approach: how to figure out that 6,6.45 and 7,7.1 are
> >> separate clusters?
> >>
> >> Thanks for any hints, Joh
> >>


From mnevill at exitcheck.net  Fri Dec 21 23:08:51 2007
From: mnevill at exitcheck.net (Max)
Date: Fri, 21 Dec 2007 14:08:51 -0800
Subject: [R] Question about which kind of plot to use
References: <mn.9b737d7cee7588f6.83239@exitcheck.net>
	<200712191625.46587.dylan.beaudette@gmail.com>
	<eb555e660712191640g49709ac5hb84c0d0278d01fdb@mail.gmail.com>
	<4769CFEF.5090103@justemail.net>
	<f8e6ff050712200939i3a26b1dcx789562ae0dce5c2e@mail.gmail.com>
Message-ID: <mn.ab507d7ca7eec8bb.83239@exitcheck.net>

hadley wickham presented the following explanation :
>> Perhaps as long as you're learning a new plotting system, you might also
>> check out whether ggplot2 might be an option.
>> 
>> I did a quick and dirty version (which I'm sure Hadley can improve and
>> also remind me how to get rid of the legend that shows the "3" that I
>> set the size to).
>> 
>> Assuming your data is re-shaped, so it comes out something like mine in
>> the artificial example below, then it's a two-liner in ggplot:
>> 
>> 
>> maxdat.df <- data.frame (
>>     score1 =  rnorm(9, mean = rep(c(10,20,30), each = 3), sd = 1 ) ,
>>     SD = runif(9) * 2 + .5,
>>     Group = factor ( rep ( c("V", "W", "X"), each = 3 ) ),
>>     subGroup = rep( c("B","M","T"), 3) )
>> 
>> maxdat.df
>> 
>> library(ggplot2)
>> ggp <- ggplot ( maxdat.df, aes (y = score1, x = interaction(Group ,
>> subGroup), min = score1 - SD, max = score1 + SD, size = 3) )
>> ggp + geom_pointrange() + coord_flip()
>
> Take the size = 3 out of the aesthetic mappings, and put it directly
> in geom_pointrange(size = 3) - this way you are setting the size to 3
> (mm) rather than asking ggplot to map a variable containing only the
> value 3 to the size of the points/lines.  It's a subtle but important
> distinction, and I need to figure out how to explain it better.
>
> Hadley

Thanks everyone for all the help. I'll be playing around with the 
various suggestions I got. :)


From groemping at tfh-berlin.de  Sat Dec 22 00:29:35 2007
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Sat, 22 Dec 2007 00:29:35 +0100
Subject: [R] New Cran Task View: ExperimentalDesign
In-Reply-To: <f8e6ff050712211449o1613e72ame73612c5b0c2b625@mail.gmail.com>
References: <20071221213208.M42092@tfh-berlin.de>
	<f8e6ff050712211449o1613e72ame73612c5b0c2b625@mail.gmail.com>
Message-ID: <20071221232909.M79292@tfh-berlin.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071222/d02719a0/attachment.pl 

From rsposto at yahoo.com  Sat Dec 22 01:36:18 2007
From: rsposto at yahoo.com (rsposto at yahoo.com)
Date: Fri, 21 Dec 2007 19:36:18 -0500
Subject: [R] Apparent discordant solution using NLS() vs Optim or Excel for
	IWLS problem
In-Reply-To: <548b8d440709051643p69659404ude641b631b9eb25c@mail.gmail.com>
Message-ID: <22544730.1198283777869.JavaMail.root@wombat.diezmil.com>

I am writing a program for automated (i.e. no user intervention - for biologists) iteratively reweighted least-square fit to a function of the form "reading ~ exp(lm2)/(1 + (dose/exp(lm3))^exp(lm4)" using case weight proportional to the mean, e.g., E(reading).    Because for some datasets the solution is sensitive to starting values, I first use OPTIM() with Nelder-Mead to locate the solution, and then plug the solution into NLS() (default algorithm) using the appropriate weights as a way to retrieve SEs, deviance, etc rather than computing these from OPTIM results.

The problem I have discovered is the following.  OPTIM()  arrives at the correct solution that minimizes the objective function, and I have confirmed this in numerous examples in Excel.  When this solution is plugged into NLS(), the first evaluation yields the same value of the objective function as OPTIM().  NLS() then attempts steps, and apparently finds a slightly different solution with a smaller value for the objective function.  However, this new solution actually does not give a smaller objective function value, but rather a larger one, when verifed against an independent computation.  Hence, it appears that NLS is getting confused and taking a step based on erroneous computation of the objective function.  

An example is below, with an abridged OPTIM output followed by NLS output:

-- ************** START OUTPUT **************************
-- [1] "Drug = Melphelan"
-- [1] "Cell line = CHLA-266"
--   Nelder-Mead direct search function minimizer
-- function value for initial parameters = 3.052411
--   Scaled convergence tolerance is 3.05241e-12
-- Stepsize computed as 1.801225
-- BUILD              4 288782670697.238953 3.052411
-- LO-REDUCTION       6 40.478286 3.052411
-- HI-REDUCTION       8 27.015234 3.052411
-- .
-- .
-- .
-- LO-REDUCTION     208 0.763370 0.763370
-- LO-REDUCTION     210 0.763370 0.763370
-- Exiting from Nelder Mead minimizer
--     212 function evaluations used
--         m2           m3       m4
-- 1 80846342 1.190286e-05 1.049291

[***NOTE - the correct solution parameters values are the line above on the absolute scale, which are the values below on the ln scale, representing the first evaluation by NLS.  This gives the correct objective function value of 0.763370, and hence agrees with OPTIM at this point ***]

-- 0.7633697 :   18.20806090 -11.33873192   0.04811485 
--   It.   1, fac=           1, eval (no.,total): ( 1,  1): new dev = 0.7504
-- 0.7504 :   18.19909405 -11.34554803   0.05560241 
--   It.   2, fac=           1, eval (no.,total): ( 1,  2): new dev = 0.750387
-- 0.7503866 :   18.19933189 -11.34796180   0.05429375 
--   It.   3, fac=           1, eval (no.,total): ( 1,  3): new dev = 0.750387
-- .
-- . multiple step halving attempts
-- .
--   It.  14, fac=  3.8147e-06, eval (no.,total): ( 4, 39): new dev = 0.750387
--   It.  14, fac= 1.90735e-06, eval (no.,total): ( 5, 40): new dev = 0.750387
-- 
-- Formula: reading ~ exp(lm2)/(1 + (dose/exp(lm3))^exp(lm4))
-- 
-- Parameters:
--      Estimate Std. Error  t value Pr(>|t|)    
-- lm2  18.19934    0.02084  873.252   <2e-16 ***
-- lm3 -11.34795    0.08316 -136.459   <2e-16 ***
-- lm4   0.05435    0.04103    1.325    0.191    
-- ---
-- Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
-- 
-- Residual standard error: 0.1147 on 57 degrees of freedom
-- 
-- Number of iterations till stop: 13 
-- Achieved convergence tolerance: 2.611e-08 
-- Reason stopped: step factor 9.53674e-007 reduced below 'minFactor' of 1e-006 
-- 
-- Warning message:
-- In nls(reading ~ exp(lm2)/(1 + (dose/exp(lm3))^exp(lm4)), start = list(lm2 =
-- log(beta_nm$m2),  :
--   step factor 9.53674e-07 reduced below 'minFactor' of 1e-06
-- 
-- *** END OUTPUT *********************************************************

Note the objective function value of .750387 at parameter values 18.19934, -11.34795,  0.05435.  However, objective function value at this solution is actually 0.776426587, by Excel computation.

Can anyone suggest what I might be doing wrong, or is this a bug or anomaly of the NLS algorithm?

Thanks in advance

Richard Sposto (Los Angeles).



--
This message was sent on behalf of rsposto at yahoo.com at openSubscriber.com
http://www.opensubscriber.com/message/r-help at stat.math.ethz.ch/7530379.html


From dlakelan at street-artists.org  Sat Dec 22 01:48:07 2007
From: dlakelan at street-artists.org (dlakelan)
Date: Fri, 21 Dec 2007 16:48:07 -0800
Subject: [R] Numerical precision problem
In-Reply-To: <8760D6386F7F784693F05256139A1FFB70F87A@EXCHF3.lse.ac.uk>
References: <8760D6386F7F784693F05256139A1FFB70F87A@EXCHF3.lse.ac.uk>
Message-ID: <476C5EC7.9040603@street-artists.org>

C.Rosa at lse.ac.uk wrote:
> Dear All,
> ....

>> (1-p)==1
> 
> [1] TRUE
> 
> The main problem I have is that I need to evaluate the inverse of the normal Cumulative Distribution Function near 1 (but not exactly 1) and the PC cannot recognize it.

Evaluate it at 1, evaluate the density at 1, and use basic calculus to 
calculate the differential of the cumulative distribution for small 
differentials away from 1.


From tplate at acm.org  Sat Dec 22 01:55:10 2007
From: tplate at acm.org (Tony Plate)
Date: Fri, 21 Dec 2007 17:55:10 -0700
Subject: [R] assigning and saving datasets in a loop,
 with names changing with "i"
In-Reply-To: <WorldClient-F200712182124.AA24320054@epimgh.mcgill.ca>
References: <WorldClient-F200712182124.AA24320054@epimgh.mcgill.ca>
Message-ID: <476C606E.3040501@acm.org>

Marie Pierre Sylvestre wrote:
> Dear R users,
> 
> I am analysing a very large data set and I need to perform several data
> manipulations. The dataset is so big that the only way I can play with it
> without having memory problems (E.g. "cannot allocate vectors of size...")
> is to write a batch script to:
> 
> 1. cut the data into pieces 
> 2. save the pieces in seperate .RData files
> 3. Remove everything from the environment
> 4. load one of the piece
> 5. perform the manipulations on it
> 6. save it and remove it from the environment
> 7. Redo 4-6 for every piece
> 8. Merge everything together at the end
> 
> It works if coded line by line but since I'll have to perform these tasks
> on other data sets, I am trying to automate this as much as I can. 

The trackObjs package is designed to make it easy to work in approximately 
this manner -- it saves objects automatically to disk but they are still 
accessible as normal.

Here's how you could do the above - this example works with 10 8Mb objects 
in a R session with a limit of 40Mb.

# allow R only 40Mb of vector memory
mem.limits(vsize=40e6)
mem.limits()/1e6
library(trackObjs)
# start tracking to store data objects in the directory 'data'
# each object is 8Mb, and we store 10 of them
track.start("data")
n <- 10
m <- 1e6
constructObject <- function(i) i+rnorm(m)
# steps 1, 2 & 3:
for (i in 1:n) {
    xname <- paste("x", i, sep="")
    cat("", xname)
    assign(xname, constructObject(i))
    # store in a file, accessible by name:
    track(list=xname)
}
cat("\n")
gc(TRUE)
# accessing object by name
object.size(x1)/2^20 # In Mb
mean(x1)
mean(x2)
gc(TRUE)
# steps 4:6
# accessing object through a constructed name
result <- sapply(1:n, function(i) mean(get(paste("x", i, sep=""))))
result
# remove the data objects
track.remove(list=paste("x", 1:n, sep=""))
track.stop()

Here's the a full transcript of the above - note how whenever gc() is 
called there is hardly any vector memory in use.

 > # allow R only 40Mb of vector memory
 > mem.limits(vsize=40e6)
    nsize    vsize
       NA 40000000
 > mem.limits()/1e6
nsize vsize
    NA    40
 > library(trackObjs)
 > # start tracking to store data objects in the directory 'data'
 > # each object is 8Mb, and we store 10 of them
 > track.start("data")
 > n <- 10
 > m <- 1e6
 > constructObject <- function(i) i+rnorm(m)
 > # steps 1, 2 & 3:
 > for (i in 1:n) {
+    xname <- paste("x", i, sep="")
+    cat("", xname)
+    assign(xname, constructObject(i))
+    # store in a file, accessible by name:
+    track(list=xname)
+ }
  x1 x2 x3 x4 x5 x6 x7 x8 x9 x10> cat("\n")

 > gc(TRUE)
Garbage collection 19 = 6+0+13 (level 2) ...
4.0 Mbytes of cons cells used (42%)
0.7 Mbytes of vectors used (5%)
          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)
Ncells 148362  4.0     350000  9.4         NA   350000  9.4
Vcells  89973  0.7    1950935 14.9       38.2  2112735 16.2
 > # accessing object by name
 > object.size(x1)/2^20 # In Mb
[1] 7.629417
 > mean(x1)
[1] 0.998635
 > mean(x2)
[1] 1.999656
 > gc(TRUE)
Garbage collection 22 = 7+1+14 (level 2) ...
4.0 Mbytes of cons cells used (43%)
0.7 Mbytes of vectors used (6%)
          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)
Ncells 149264  4.0     350000  9.4         NA   350000  9.4
Vcells  90160  0.7    1560747 12.0       38.2  2112735 16.2
 > # steps 4:6
 > result <- sapply(1:n, function(i) mean(get(paste("x", i, sep=""))))
 > result
  [1]  0.998635  1.999656  2.997368  4.000197  5.000159  6.001216  6.999552
  [8]  7.999743  8.999982 10.001355
 > # remove the data objects
 > track.remove(list=paste("x", 1:n, sep=""))
  [1] "x1"  "x2"  "x3"  "x4"  "x5"  "x6"  "x7"  "x8"  "x9"  "x10"
 > track.stop()
 >



> 
> I am using a loop in which I used 'assign' and 'get' (pseudo code below).
> My problem is when I use 'get', it prints the whole object on the screen.
> I am wondering whether there is a more efficient way to do what I need to
> do. Any help would be appreciated. Please keep in mind that the whole
> process is quite computer-intensive, so I can't keep everything in the
> environment while R performs calculations.
> 
> Say I have 1 big dataframe called data. I use 'split' to divide it into a
> list of 12 dataframes (call this list my.list)
> 
> my.fun is a function that takes a dataframe, performs several
> manipulations on it and returns a dataframe.
> 
> 
> for (i in 1:12){
>   assign( paste( "data", i, sep=""),  my.fun(my.list[i]))   # this works
>   # now I need to save this new object as a RData. 
> 
>   # The following line does not work
>   save(paste("data", i, sep = ""),  file = paste(  paste("data", i, sep =
> ""), "RData", sep="."))
> }
> 
>   # This works but it is a bit convoluted!!!
>   temp <- get(paste("data", i, sep = ""))
>   save(temp,  file = "lala.RData")
> }
> 
> 
> I am *sure* there is something more clever to do but I can't find it. Any
> help would be appreciated.
> 
> best regards,
> 
> MP
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tplate at acm.org  Sat Dec 22 02:17:18 2007
From: tplate at acm.org (Tony Plate)
Date: Fri, 21 Dec 2007 18:17:18 -0700
Subject: [R] Efficient way to find consecutive integers in vector?
In-Reply-To: <18283.54174.973381.750616@stat.math.ethz.ch>
References: <fkenl8$bt4$1@ger.gmane.org>	<1198190034.4557.4.camel@Bellerophon.localdomain>
	<18283.54174.973381.750616@stat.math.ethz.ch>
Message-ID: <476C659E.1030005@acm.org>

Martin Maechler wrote:
>>>>>> "MS" == Marc Schwartz <marc_schwartz at comcast.net>
>>>>>>     on Thu, 20 Dec 2007 16:33:54 -0600 writes:
> 
>     MS> On Thu, 2007-12-20 at 22:43 +0100, Johannes Graumann wrote:
>     >> Hi all,
>     >> 
>     >> Does anybody have a magic trick handy to isolate directly consecutive
>     >> integers from something like this:
>     >> c(1,2,3,4,7,8,9,10,12,13)
>     >> 
>     >> The result should be, that groups 1-4, 7-10 and 12-13 are consecutive
>     >> integers ...
>     >> 
>     >> Thanks for any hints, Joh
> 
>     MS> Not fully tested, but here is one possible approach:
> 
>     >> Vec
>     MS> [1]  1  2  3  4  7  8  9 10 12 13
> 
>     MS> Breaks <- c(0, which(diff(Vec) != 1), length(Vec))
> 
>     >> Breaks
>     MS> [1]  0  4  8 10
> 
>     >> sapply(seq(length(Breaks) - 1), 
>     MS> function(i) Vec[(Breaks[i] + 1):Breaks[i+1]])
>     MS> [[1]]
>     MS> [1] 1 2 3 4
> 
>     MS> [[2]]
>     MS> [1]  7  8  9 10
> 
>     MS> [[3]]
>     MS> [1] 12 13
> 
> 
> 
>     MS> For a quick test, I tried it on another vector:
> 
> 
>     MS> set.seed(1)
>     MS> Vec <- sort(sample(20, 15))
> 
>     >> Vec
>     MS> [1]  1  2  3  4  5  6  8  9 10 11 14 15 16 19 20
> 
>     MS> Breaks <- c(0, which(diff(Vec) != 1), length(Vec))
> 
>     >> Breaks
>     MS> [1]  0  6 10 13 15
> 
>     >> sapply(seq(length(Breaks) - 1), 
>     MS> function(i) Vec[(Breaks[i] + 1):Breaks[i+1]])
>     MS> [[1]]
>     MS> [1] 1 2 3 4 5 6
> 
>     MS> [[2]]
>     MS> [1]  8  9 10 11
> 
>     MS> [[3]]
>     MS> [1] 14 15 16
> 
>     MS> [[4]]
>     MS> [1] 19 20
> 
> Seems ok, but ``only works for increasing sequences''.
> More than 12 years ago, I had encountered the same problem and
> solved it like this:
> 
> In package 'sfsmisc', there has been the function  inv.seq(),
> named for "inversion of seq()",
> which does this too, currently returning an expression,
> but returning a call in the development version of sfsmisc:
> 
> Its definition is currently
> 
> inv.seq <- function(i) {
>   ## Purpose: 'Inverse seq': Return a short expression for the 'index'  `i'
>   ## --------------------------------------------------------------------
>   ## Arguments: i: vector of (usually increasing) integers.
>   ## --------------------------------------------------------------------
>   ## Author: Martin Maechler, Date:  3 Oct 95, 18:08
>   ## --------------------------------------------------------------------
>   ## EXAMPLES: cat(rr <- inv.seq(c(3:12, 20:24, 27, 30:33)),"\n"); eval(rr)
>   ##           r2 <- inv.seq(c(20:13, 3:12, -1:-4, 27, 30:31)); eval(r2); r2
>   li <- length(i <- as.integer(i))
>   if(li == 0) return(expression(NULL))
>   else if(li == 1) return(as.expression(i))
>   ##-- now have: length(i) >= 2
>   di1 <- abs(diff(i)) == 1	#-- those are just simple sequences  n1:n2 !
>   s1 <- i[!c(FALSE,di1)] # beginnings
>   s2 <- i[!c(di1,FALSE)] # endings
> 
>   ## using text & parse {cheap and dirty} :
>   mkseq <- function(i,j) if(i == j) i else paste(i,":",j, sep="")
>   parse(text =
>         paste("c(", paste(mapply(mkseq, s1,s2), collapse = ","), ")", sep = ""),
>         srcfile = NULL)[[1]]
> }
> 
> with example code
> 
>  > v <- c(1:10,11,6,5,4,0,1)
>  > (iv <- inv.seq(v))
>  c(1:11, 6:4, 0:1)
>  > stopifnot(identical(eval(iv), as.integer(v)))
>  > iv[[2]]
>  1:11
>  > str(iv)
>   language c(1:11, 6:4, 0:1)
>  > str(iv[[2]])
>   language 1:11
>  > 
> 
> 
> Now, given that this stems from  1995,  I should be excused for
> using   parse(text = *)  [see  fortune(106) if you don't understand].
> 
> However, doing this differently by constructing the resulting
> language object directly {using substitute(), as.symbol(),
> 	 		  as.expression() ... etc}
> seems not quite trivial.
> 
> So here's the Friday afternoon /  Christmas break quizz:  
> 
>   What's the most elegant way
>   to replace the last statements in  inv.seq()
>   ------------------------------------------------------------------------
>   ## using text & parse {cheap and dirty} :
>   mkseq <- function(i,j) if(i == j) i else paste(i,":",j, sep="")
>   parse(text =
>         paste("c(", paste(mapply(mkseq, s1,s2), collapse = ","), ")", sep = ""),
> 	      srcfile = NULL)[[1]]
>   ------------------------------------------------------------------------
> 
>   by code that does not use parse (or source() or similar) ???
> 
> I don't have an answer yet, at least not at all an elegant one.
> And maybe, the solution to the quiz is that there is no elegant
> solution.

How about this ? :

 > i <- c(1, 10, 12)
 > j <- c(5, 10, 14)
 > mkseq <- function(i, j) if (i==j) i else call(':', i, j)
 > as.call(c(list(as.name('c')), mapply(i, j, FUN=mkseq)))
c(1:5, 10, 12:14)
 > eval(.Last.value)
[1]  1  2  3  4  5 10 12 13 14
 >

-- Tony Plate

> 
> Martin
> 
> 
>     MS> HTH,
> 
>     MS> Marc Schwartz
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From markleeds at verizon.net  Sat Dec 22 02:38:19 2007
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 21 Dec 2007 19:38:19 -0600 (CST)
Subject: [R] using solve.qp without a quadratic term
Message-ID: <1808201.508981198287499445.JavaMail.root@vms125.mailsrvcs.net>

I was playing around with a simple example using solve.qp ( function is in the quadprog package ) and the code is below. ( I'm not even sure there if there is a reasonable solution because I made the problem up  ). 

But, when I try to use solve.QP to solve it, I get the error that D in the quadratic function is not positive
definite. This is because Dmat is zero
because I don't have a quadratic term in my
objective function. So, I was wondering if
it was possible to use solve.QP when there isn't
a quadratic term in the objective function.  

I imagine that there are other functions in R that can be used but I would like to use solve.QP because, in my real problem,
I will have a lot of fairly complex constraints
and solve.QP provides a very nice way for implementing
them. Maybe there is another linear solver that allows you to implement hundreds of constraints just solve.QP that I am unaware of ? Thanks for any suggestions.

# IN THE CODE BELOW, WE MINIMIZE 

# -3*b1 + 4*b2 + 6*b3

# SUBJECT TO

# b1 + b2 + b3 >=0
# -(b1  b2 + b3) >= 0 
# IE : b1 + b2 + b3 = 0.

Dmat <- matrix(0,3,3)          # QUADRATIC TERM
dvec <- c(-3,4,6)              # LINEAR TERM
Amat <- matrix(c(1,-1,0,1,-1,0,1,-1,0),3,3)

#print(Amat)

bvec = c(0,0,0)                  # THIRD ZERO IS SAME AS NO CONSTRAINT

result <- solve.QP(Dmat, dvec, Amat)


From hb at stat.berkeley.edu  Sat Dec 22 02:39:41 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Fri, 21 Dec 2007 17:39:41 -0800
Subject: [R] Efficient way to find consecutive integers in vector?
In-Reply-To: <476C659E.1030005@acm.org>
References: <fkenl8$bt4$1@ger.gmane.org>
	<1198190034.4557.4.camel@Bellerophon.localdomain>
	<18283.54174.973381.750616@stat.math.ethz.ch>
	<476C659E.1030005@acm.org>
Message-ID: <59d7961d0712211739q57e4023xd1fd6b84b1138373@mail.gmail.com>

In the R.utils package there is seqToIntervals(), e.g.

print(seqToIntervals(1:10))
##      from to
## [1,]    1 10
print(seqToIntervals(c(1:10, 15:18, 20)))
##      from to
## [1,]    1 10
## [2,]   15 18
## [3,]   20 20

There is also seqToIntervals(), which uses the above, e.g.

print(seqToHumanReadable(1:10))
## [1] "1-10"
print(seqToHumanReadable(c(1:10, 15:18, 20)))
## [1] "1-10, 15-18, 20"

/Henrik

On 21/12/2007, Tony Plate <tplate at acm.org> wrote:
> Martin Maechler wrote:
> >>>>>> "MS" == Marc Schwartz <marc_schwartz at comcast.net>
> >>>>>>     on Thu, 20 Dec 2007 16:33:54 -0600 writes:
> >
> >     MS> On Thu, 2007-12-20 at 22:43 +0100, Johannes Graumann wrote:
> >     >> Hi all,
> >     >>
> >     >> Does anybody have a magic trick handy to isolate directly consecutive
> >     >> integers from something like this:
> >     >> c(1,2,3,4,7,8,9,10,12,13)
> >     >>
> >     >> The result should be, that groups 1-4, 7-10 and 12-13 are consecutive
> >     >> integers ...
> >     >>
> >     >> Thanks for any hints, Joh
> >
> >     MS> Not fully tested, but here is one possible approach:
> >
> >     >> Vec
> >     MS> [1]  1  2  3  4  7  8  9 10 12 13
> >
> >     MS> Breaks <- c(0, which(diff(Vec) != 1), length(Vec))
> >
> >     >> Breaks
> >     MS> [1]  0  4  8 10
> >
> >     >> sapply(seq(length(Breaks) - 1),
> >     MS> function(i) Vec[(Breaks[i] + 1):Breaks[i+1]])
> >     MS> [[1]]
> >     MS> [1] 1 2 3 4
> >
> >     MS> [[2]]
> >     MS> [1]  7  8  9 10
> >
> >     MS> [[3]]
> >     MS> [1] 12 13
> >
> >
> >
> >     MS> For a quick test, I tried it on another vector:
> >
> >
> >     MS> set.seed(1)
> >     MS> Vec <- sort(sample(20, 15))
> >
> >     >> Vec
> >     MS> [1]  1  2  3  4  5  6  8  9 10 11 14 15 16 19 20
> >
> >     MS> Breaks <- c(0, which(diff(Vec) != 1), length(Vec))
> >
> >     >> Breaks
> >     MS> [1]  0  6 10 13 15
> >
> >     >> sapply(seq(length(Breaks) - 1),
> >     MS> function(i) Vec[(Breaks[i] + 1):Breaks[i+1]])
> >     MS> [[1]]
> >     MS> [1] 1 2 3 4 5 6
> >
> >     MS> [[2]]
> >     MS> [1]  8  9 10 11
> >
> >     MS> [[3]]
> >     MS> [1] 14 15 16
> >
> >     MS> [[4]]
> >     MS> [1] 19 20
> >
> > Seems ok, but ``only works for increasing sequences''.
> > More than 12 years ago, I had encountered the same problem and
> > solved it like this:
> >
> > In package 'sfsmisc', there has been the function  inv.seq(),
> > named for "inversion of seq()",
> > which does this too, currently returning an expression,
> > but returning a call in the development version of sfsmisc:
> >
> > Its definition is currently
> >
> > inv.seq <- function(i) {
> >   ## Purpose: 'Inverse seq': Return a short expression for the 'index'  `i'
> >   ## --------------------------------------------------------------------
> >   ## Arguments: i: vector of (usually increasing) integers.
> >   ## --------------------------------------------------------------------
> >   ## Author: Martin Maechler, Date:  3 Oct 95, 18:08
> >   ## --------------------------------------------------------------------
> >   ## EXAMPLES: cat(rr <- inv.seq(c(3:12, 20:24, 27, 30:33)),"\n"); eval(rr)
> >   ##           r2 <- inv.seq(c(20:13, 3:12, -1:-4, 27, 30:31)); eval(r2); r2
> >   li <- length(i <- as.integer(i))
> >   if(li == 0) return(expression(NULL))
> >   else if(li == 1) return(as.expression(i))
> >   ##-- now have: length(i) >= 2
> >   di1 <- abs(diff(i)) == 1    #-- those are just simple sequences  n1:n2 !
> >   s1 <- i[!c(FALSE,di1)] # beginnings
> >   s2 <- i[!c(di1,FALSE)] # endings
> >
> >   ## using text & parse {cheap and dirty} :
> >   mkseq <- function(i,j) if(i == j) i else paste(i,":",j, sep="")
> >   parse(text =
> >         paste("c(", paste(mapply(mkseq, s1,s2), collapse = ","), ")", sep = ""),
> >         srcfile = NULL)[[1]]
> > }
> >
> > with example code
> >
> >  > v <- c(1:10,11,6,5,4,0,1)
> >  > (iv <- inv.seq(v))
> >  c(1:11, 6:4, 0:1)
> >  > stopifnot(identical(eval(iv), as.integer(v)))
> >  > iv[[2]]
> >  1:11
> >  > str(iv)
> >   language c(1:11, 6:4, 0:1)
> >  > str(iv[[2]])
> >   language 1:11
> >  >
> >
> >
> > Now, given that this stems from  1995,  I should be excused for
> > using   parse(text = *)  [see  fortune(106) if you don't understand].
> >
> > However, doing this differently by constructing the resulting
> > language object directly {using substitute(), as.symbol(),
> >                         as.expression() ... etc}
> > seems not quite trivial.
> >
> > So here's the Friday afternoon /  Christmas break quizz:
> >
> >   What's the most elegant way
> >   to replace the last statements in  inv.seq()
> >   ------------------------------------------------------------------------
> >   ## using text & parse {cheap and dirty} :
> >   mkseq <- function(i,j) if(i == j) i else paste(i,":",j, sep="")
> >   parse(text =
> >         paste("c(", paste(mapply(mkseq, s1,s2), collapse = ","), ")", sep = ""),
> >             srcfile = NULL)[[1]]
> >   ------------------------------------------------------------------------
> >
> >   by code that does not use parse (or source() or similar) ???
> >
> > I don't have an answer yet, at least not at all an elegant one.
> > And maybe, the solution to the quiz is that there is no elegant
> > solution.
>
> How about this ? :
>
>  > i <- c(1, 10, 12)
>  > j <- c(5, 10, 14)
>  > mkseq <- function(i, j) if (i==j) i else call(':', i, j)
>  > as.call(c(list(as.name('c')), mapply(i, j, FUN=mkseq)))
> c(1:5, 10, 12:14)
>  > eval(.Last.value)
> [1]  1  2  3  4  5 10 12 13 14
>  >
>
> -- Tony Plate
>
> >
> > Martin
> >
> >
> >     MS> HTH,
> >
> >     MS> Marc Schwartz
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Antonio.Gasparrini at lshtm.ac.uk  Sat Dec 22 03:06:17 2007
From: Antonio.Gasparrini at lshtm.ac.uk (Antonio Gasparrini)
Date: Sat, 22 Dec 2007 02:06:17 +0000
Subject: [R] mle
Message-ID: <476C710E020000B200035FE7@smtp-a.lshtm.ac.uk>

Dear all,

I'm trying to estimate the parameters of a special case of a poisson
model, where the specified equation has an integral and several fixed
parameters.
I think that the MLE command in STATS4 package could be a good choice,
but it's a little complicated. I've got some problems with the offset
and I don't understand some of the functions. Do you know where can I
find some explicit tutorial about a poisson regression with MLE or,
maybe better, about maximum likelyhood estimation?

Best wishes

Antonio Gasparrini
Public and Environmental Health Research Unit (PEHRU)
London School of Hygiene & Tropical Medicine
Keppel Street, London WC1E 7HT, UK
Office: 0044 (0)20 79272406
Mobile: 0044 (0)79 64925523
www.lshtm.ac.uk/pehru/
antonio.gasparrini at lshtm.ac.uk


From Antonio.Gasparrini at lshtm.ac.uk  Sat Dec 22 03:14:15 2007
From: Antonio.Gasparrini at lshtm.ac.uk (Antonio Gasparrini)
Date: Sat, 22 Dec 2007 02:14:15 +0000
Subject: [R] mle
Message-ID: <476C72ED020000B200035FEB@smtp-a.lshtm.ac.uk>

Dear all,

I'm trying to estimate the parameters of a special case of a poisson
model, where the specified equation has an integral and several fixed
parameters.
I think that the MLE command in STATS4 package could be a good choice,
but it's a little complicated. I've got some problems with the offset
and I don't understand some of the functions. Do you know where can I
find some explicit tutorial about a poisson regression with MLE and/or
about maximum likelihood estimation with R?

Best wishes

Antonio Gasparrini
Public and Environmental Health Research Unit (PEHRU)
London School of Hygiene & Tropical Medicine
Keppel Street, London WC1E 7HT, UK
Office: 0044 (0)20 79272406
Mobile: 0044 (0)79 64925523
www.lshtm.ac.uk/pehru/
antonio.gasparrini at lshtm.ac.uk


From ggrothendieck at gmail.com  Sat Dec 22 03:26:37 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 21 Dec 2007 21:26:37 -0500
Subject: [R] Apparent discordant solution using NLS() vs Optim or Excel
	for IWLS problem
In-Reply-To: <22544730.1198283777869.JavaMail.root@wombat.diezmil.com>
References: <548b8d440709051643p69659404ude641b631b9eb25c@mail.gmail.com>
	<22544730.1198283777869.JavaMail.root@wombat.diezmil.com>
Message-ID: <971536df0712211826u64beacf5o9afc5f14f7d7ab63@mail.gmail.com>

Check out the drc package.

Also, please read the last line to every message to r-help regarding
reproducible code
and note that function names are case sensitive in R so NLS is not the
same as nls.

On Dec 21, 2007 7:36 PM,  <rsposto at yahoo.com> wrote:
> I am writing a program for automated (i.e. no user intervention - for biologists) iteratively reweighted least-square fit to a function of the form "reading ~ exp(lm2)/(1 + (dose/exp(lm3))^exp(lm4)" using case weight proportional to the mean, e.g., E(reading).    Because for some datasets the solution is sensitive to starting values, I first use OPTIM() with Nelder-Mead to locate the solution, and then plug the solution into NLS() (default algorithm) using the appropriate weights as a way to retrieve SEs, deviance, etc rather than computing these from OPTIM results.
>
> The problem I have discovered is the following.  OPTIM()  arrives at the correct solution that minimizes the objective function, and I have confirmed this in numerous examples in Excel.  When this solution is plugged into NLS(), the first evaluation yields the same value of the objective function as OPTIM().  NLS() then attempts steps, and apparently finds a slightly different solution with a smaller value for the objective function.  However, this new solution actually does not give a smaller objective function value, but rather a larger one, when verifed against an independent computation.  Hence, it appears that NLS is getting confused and taking a step based on erroneous computation of the objective function.
>
> An example is below, with an abridged OPTIM output followed by NLS output:
>
> -- ************** START OUTPUT **************************
> -- [1] "Drug = Melphelan"
> -- [1] "Cell line = CHLA-266"
> --   Nelder-Mead direct search function minimizer
> -- function value for initial parameters = 3.052411
> --   Scaled convergence tolerance is 3.05241e-12
> -- Stepsize computed as 1.801225
> -- BUILD              4 288782670697.238953 3.052411
> -- LO-REDUCTION       6 40.478286 3.052411
> -- HI-REDUCTION       8 27.015234 3.052411
> -- .
> -- .
> -- .
> -- LO-REDUCTION     208 0.763370 0.763370
> -- LO-REDUCTION     210 0.763370 0.763370
> -- Exiting from Nelder Mead minimizer
> --     212 function evaluations used
> --         m2           m3       m4
> -- 1 80846342 1.190286e-05 1.049291
>
> [***NOTE - the correct solution parameters values are the line above on the absolute scale, which are the values below on the ln scale, representing the first evaluation by NLS.  This gives the correct objective function value of 0.763370, and hence agrees with OPTIM at this point ***]
>
> -- 0.7633697 :   18.20806090 -11.33873192   0.04811485
> --   It.   1, fac=           1, eval (no.,total): ( 1,  1): new dev = 0.7504
> -- 0.7504 :   18.19909405 -11.34554803   0.05560241
> --   It.   2, fac=           1, eval (no.,total): ( 1,  2): new dev = 0.750387
> -- 0.7503866 :   18.19933189 -11.34796180   0.05429375
> --   It.   3, fac=           1, eval (no.,total): ( 1,  3): new dev = 0.750387
> -- .
> -- . multiple step halving attempts
> -- .
> --   It.  14, fac=  3.8147e-06, eval (no.,total): ( 4, 39): new dev = 0.750387
> --   It.  14, fac= 1.90735e-06, eval (no.,total): ( 5, 40): new dev = 0.750387
> --
> -- Formula: reading ~ exp(lm2)/(1 + (dose/exp(lm3))^exp(lm4))
> --
> -- Parameters:
> --      Estimate Std. Error  t value Pr(>|t|)
> -- lm2  18.19934    0.02084  873.252   <2e-16 ***
> -- lm3 -11.34795    0.08316 -136.459   <2e-16 ***
> -- lm4   0.05435    0.04103    1.325    0.191
> -- ---
> -- Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> --
> -- Residual standard error: 0.1147 on 57 degrees of freedom
> --
> -- Number of iterations till stop: 13
> -- Achieved convergence tolerance: 2.611e-08
> -- Reason stopped: step factor 9.53674e-007 reduced below 'minFactor' of 1e-006
> --
> -- Warning message:
> -- In nls(reading ~ exp(lm2)/(1 + (dose/exp(lm3))^exp(lm4)), start = list(lm2 =
> -- log(beta_nm$m2),  :
> --   step factor 9.53674e-07 reduced below 'minFactor' of 1e-06
> --
> -- *** END OUTPUT *********************************************************
>
> Note the objective function value of .750387 at parameter values 18.19934, -11.34795,  0.05435.  However, objective function value at this solution is actually 0.776426587, by Excel computation.
>
> Can anyone suggest what I might be doing wrong, or is this a bug or anomaly of the NLS algorithm?
>
> Thanks in advance
>
> Richard Sposto (Los Angeles).
>
>
>
> --
> This message was sent on behalf of rsposto at yahoo.com at openSubscriber.com
> http://www.opensubscriber.com/message/r-help at stat.math.ethz.ch/7530379.html
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From markleeds at verizon.net  Sat Dec 22 04:01:40 2007
From: markleeds at verizon.net (Mark Leeds)
Date: Fri, 21 Dec 2007 22:01:40 -0500
Subject: [R] regarding lack of quadratic term in solve.qp
Message-ID: <000601c84446$f9bde3b0$2f01a8c0@coresystem>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071221/e6bb0378/attachment.pl 

From tyler.smith at mail.mcgill.ca  Sat Dec 22 06:31:15 2007
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Sat, 22 Dec 2007 05:31:15 +0000 (UTC)
Subject: [R] R appearance under linux
References: <8CA12168E8485A4-AB8-A02@webmail-da15.sysops.aol.com>
Message-ID: <slrnfmp894.m5q.tyler.smith@blackbart.sedgenet>

On 2007-12-21, phguardiol at aol.com <phguardiol at aol.com> wrote:
>
>  Dear R users,
>
> I have just moved to R2.6.1 under Opensuse linux 10.3. I used to
> work with R under XPpro. Is it "normal" to have a visual aspect of R
> under linux different ? 

Yes, that's normal. Under windows, you get a GUI interface with menus
and separate windows etc. by default. In Linux you get to choose for
yourself how you interact with R. One of the better options is Emacs
with ESS. Details can be found at http://ESS.R-project.org/ including
rpms for suse. Other options are presented here:
http://www.sciviews.org/_rgui/

There will be a bit of a learning curve here, but eventually you'll
find you can do everything that you could do with the Windows GUI, and
at least with Emacs and ESS, a whole lot more.

Tyler


From chs23 at student.open.ac.uk  Sat Dec 22 08:21:25 2007
From: chs23 at student.open.ac.uk (Chris.H. Snow)
Date: Sat, 22 Dec 2007 07:21:25 GMT
Subject: [R] odfWeave cross-reference
Message-ID: <fc.004c540106079503004c540106079503.607951a@oufcnt2.open.ac.uk>

How can I insert cross-references to odfWeave generated figures in my
source odf before the graphic has been created with odfWeave?

Many thanks,

Chris


From johannes_graumann at web.de  Sat Dec 22 11:12:45 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Sat, 22 Dec 2007 11:12:45 +0100
Subject: [R] Finding overlaps in vector
References: <fkfrm5$vj3$1@ger.gmane.org>
	<971536df0712210809w56b96e6arb20eee05368bef26@mail.gmail.com>
	<fkh7e9$is7$1@ger.gmane.org>
	<971536df0712211451h79513980ldf08edcb0ea28d3e@mail.gmail.com>
Message-ID: <fkinut$re4$1@ger.gmane.org>

But cutree does away with the indexes from the original input, which
rect.hclust retains.
I will have no other choice and match that input with the 'values' contained
in the clusters ... 

Joh

Gabor Grothendieck wrote:

> If we don't need any plotting we don't really need rect.hclust at
> all.  Split the output of cutree, instead.  Continuing from the
> prior code:
> 
>> for(el in split(unname(vv), names(vv))) print(el)
> [1] 0.00 0.45
> [1] 1
> [1] 2
> [1] 3.00 3.25 3.33 3.75 4.10
> [1] 5
> [1] 6.00 6.45
> [1] 7.0 7.1
> [1] 8
> 
> On Dec 21, 2007 3:24 PM, Johannes Graumann <johannes_graumann at web.de>
> wrote:
>> Hm, hm, rect.hclust doesn't accept "plot=FALSE" and cutree doesn't retain
>> the indexes of membership ... anyway short of ripping out the guts of
>> rect.hclust to achieve the same result without an active graphics device?
>>
>> Joh
>>
>>
>> >> # cluster and plot
>> >> hc <- hclust(dist(v), method = "single")
>> >> plot(hc, lab = v)
>> >> cl <- rect.hclust(hc, h = .5, border = "red")
>> >>
>> >> # each component of list cl is one cluster.  Print them out.
>> >> for(idx in cl) print(unname(v[idx]))
>> > [1] 8
>> > [1] 7.0 7.1
>> > [1] 6.00 6.45
>> > [1] 5
>> > [1] 3.00 3.25 3.33 3.75 4.10
>> > [1] 2
>> > [1] 1
>> > [1] 0.00 0.45
>> >
>> >> # a different representation of the clusters
>> >> vv <- v
>> >> names(vv) <- ct <- cutree(hc, h = .5)
>> >> vv
>> >    1    1    2    3    4    4    4    4    4    5    6    6    7    7  
>> >     8
>> > 0.00 0.45 1.00 2.00 3.00 3.25 3.33 3.75 4.10 5.00 6.00 6.45 7.00 7.10
>> > 8.00
>> >
>> >
>> > On Dec 21, 2007 4:56 AM, Johannes Graumann <johannes_graumann at web.de>
>> > wrote:
>> >> <posted & mailed>
>> >>
>> >> Dear all,
>> >>
>> >> I'm trying to solve the problem, of how to find clusters of values in
>> >> a vector that are closer than a given value. Illustrated this might
>> >> look as follows:
>> >>
>> >> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>> >>
>> >> When using '0.5' as the proximity requirement, the following groups
>> >> would result:
>> >> 0,0.45
>> >> 3,3.25,3.33,3.75,4.1
>> >> 6,6.45
>> >> 7,7.1
>> >>
>> >> Jim Holtman proposed a very elegant solution in
>> >> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I
>> >> have modified and perused since he wrote it to me. The beauty of this
>> >> approach is that it will not only work for constant proximity
>> >> requirements as above, but also for overlap-windows defined in terms
>> >> of ppm around each value. Now I have an additional need and have found
>> >> no way (short of iteratively step through all the groups returned) to
>> >> figure out how to do that with Jim's approach: how to figure out that
>> >> 6,6.45 and 7,7.1 are separate clusters?
>> >>
>> >> Thanks for any hints, Joh
>> >>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Sat Dec 22 11:29:30 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sat, 22 Dec 2007 11:29:30 +0100
Subject: [R] Diagonal matrix with off diagonal elements
In-Reply-To: <14463831.post@talk.nabble.com>
References: <fd3c7adf0712211258h3bcd83d8r1c4938ceb10510d4@mail.gmail.com>
	<14463831.post@talk.nabble.com>
Message-ID: <476CE70A.7010903@biostat.ku.dk>

Chris Stubben wrote:
> Also try the odiag function in the demogR package
>
> odiag( 1:5, -1)
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    0    0    0    0    0    0
> [2,]    1    0    0    0    0    0
> [3,]    0    2    0    0    0    0
> [4,]    0    0    3    0    0    0
> [5,]    0    0    0    4    0    0
> [6,]    0    0    0    0    5    0
>
> Chris
>
>   
Also, this sort of pattern works

 > m <- matrix(0,6,6)
 > diag(m[-1,])<-1:5
 > m
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    0    0    0    0    0    0
[2,]    1    0    0    0    0    0
[3,]    0    2    0    0    0    0
[4,]    0    0    3    0    0    0
[5,]    0    0    0    4    0    0
[6,]    0    0    0    0    5    0



>
>
>
> Jonas Malmros wrote:
>   
>> Hi, everyone
>>
>> I wonder if there is a function in R with which I can create a square
>> matrix with elements off main diagonal (for example one diagonal below
>> the main diagonal).
>>
>> Thanks in advance!
>>
>> -- 
>> Jonas Malmros
>> Stockholm University
>> Stockholm, Sweden
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>     
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From maechler at stat.math.ethz.ch  Sat Dec 22 12:29:05 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 22 Dec 2007 12:29:05 +0100
Subject: [R] Efficient way to find consecutive integers in vector?
In-Reply-To: <476C659E.1030005@acm.org>
References: <fkenl8$bt4$1@ger.gmane.org>
	<1198190034.4557.4.camel@Bellerophon.localdomain>
	<18283.54174.973381.750616@stat.math.ethz.ch>
	<476C659E.1030005@acm.org>
Message-ID: <18284.62721.59841.726166@ada-stat.math.ethz.ch>

>>>>> "TP" == Tony Plate <tplate at acm.org>
>>>>>     on Fri, 21 Dec 2007 18:17:18 -0700 writes:

    TP> Martin Maechler wrote:
    >>>>>>> "MS" == Marc Schwartz <marc_schwartz at comcast.net>
    >>>>>>> on Thu, 20 Dec 2007 16:33:54 -0600 writes:
    >> 
    MS> On Thu, 2007-12-20 at 22:43 +0100, Johannes Graumann wrote:
    >> >> Hi all,
    >> >> 
    >> >> Does anybody have a magic trick handy to isolate directly consecutive
    >> >> integers from something like this:
    >> >> c(1,2,3,4,7,8,9,10,12,13)
    >> >> 
    >> >> The result should be, that groups 1-4, 7-10 and 12-13 are consecutive
    >> >> integers ...
    >> >> 
    >> >> Thanks for any hints, Joh
    >> 
    MS> Not fully tested, but here is one possible approach:
    >> 
    >> >> Vec
    MS> [1]  1  2  3  4  7  8  9 10 12 13
    >> 
    MS> Breaks <- c(0, which(diff(Vec) != 1), length(Vec))
    >> 
    >> >> Breaks
    MS> [1]  0  4  8 10
    >> 
    >> >> sapply(seq(length(Breaks) - 1), 
    MS> function(i) Vec[(Breaks[i] + 1):Breaks[i+1]])
    MS> [[1]]
    MS> [1] 1 2 3 4
    >> 
    MS> [[2]]
    MS> [1]  7  8  9 10
    >> 
    MS> [[3]]
    MS> [1] 12 13
    >> 
    >> 
    >> 
    MS> For a quick test, I tried it on another vector:
    >> 
    >> 
    MS> set.seed(1)
    MS> Vec <- sort(sample(20, 15))
    >> 
    >> >> Vec
    MS> [1]  1  2  3  4  5  6  8  9 10 11 14 15 16 19 20
    >> 
    MS> Breaks <- c(0, which(diff(Vec) != 1), length(Vec))
    >> 
    >> >> Breaks
    MS> [1]  0  6 10 13 15
    >> 
    >> >> sapply(seq(length(Breaks) - 1), 
    MS> function(i) Vec[(Breaks[i] + 1):Breaks[i+1]])
    MS> [[1]]
    MS> [1] 1 2 3 4 5 6
    >> 
    MS> [[2]]
    MS> [1]  8  9 10 11
    >> 
    MS> [[3]]
    MS> [1] 14 15 16
    >> 
    MS> [[4]]
    MS> [1] 19 20
    >> 
    >> Seems ok, but ``only works for increasing sequences''.
    >> More than 12 years ago, I had encountered the same problem and
    >> solved it like this:
    >> 
    >> In package 'sfsmisc', there has been the function  inv.seq(),
    >> named for "inversion of seq()",
    >> which does this too, currently returning an expression,
    >> but returning a call in the development version of sfsmisc:
    >> 
    >> Its definition is currently
    >> 
    >> inv.seq <- function(i) {
    >> ## Purpose: 'Inverse seq': Return a short expression for the 'index'  `i'
    >> ## --------------------------------------------------------------------
    >> ## Arguments: i: vector of (usually increasing) integers.
    >> ## --------------------------------------------------------------------
    >> ## Author: Martin Maechler, Date:  3 Oct 95, 18:08
    >> ## --------------------------------------------------------------------
    >> ## EXAMPLES: cat(rr <- inv.seq(c(3:12, 20:24, 27, 30:33)),"\n"); eval(rr)
    >> ##           r2 <- inv.seq(c(20:13, 3:12, -1:-4, 27, 30:31)); eval(r2); r2
    >> li <- length(i <- as.integer(i))
    >> if(li == 0) return(expression(NULL))
    >> else if(li == 1) return(as.expression(i))
    >> ##-- now have: length(i) >= 2
    >> di1 <- abs(diff(i)) == 1	#-- those are just simple sequences  n1:n2 !
    >> s1 <- i[!c(FALSE,di1)] # beginnings
    >> s2 <- i[!c(di1,FALSE)] # endings
    >> 
    >> ## using text & parse {cheap and dirty} :
    >> mkseq <- function(i,j) if(i == j) i else paste(i,":",j, sep="")
    >> parse(text =
    >> paste("c(", paste(mapply(mkseq, s1,s2), collapse = ","), ")", sep = ""),
    >> srcfile = NULL)[[1]]
    >> }
    >> 
    >> with example code
    >> 
    >> > v <- c(1:10,11,6,5,4,0,1)
    >> > (iv <- inv.seq(v))
    >> c(1:11, 6:4, 0:1)
    >> > stopifnot(identical(eval(iv), as.integer(v)))
    >> > iv[[2]]
    >> 1:11
    >> > str(iv)
    >> language c(1:11, 6:4, 0:1)
    >> > str(iv[[2]])
    >> language 1:11
    >> > 
    >> 
    >> 
    >> Now, given that this stems from  1995,  I should be excused for
    >> using   parse(text = *)  [see  fortune(106) if you don't understand].
    >> 
    >> However, doing this differently by constructing the resulting
    >> language object directly {using substitute(), as.symbol(),
    >> as.expression() ... etc}
    >> seems not quite trivial.
    >> 
    >> So here's the Friday afternoon /  Christmas break quizz:  
    >> 
    >> What's the most elegant way
    >> to replace the last statements in  inv.seq()
    >> ------------------------------------------------------------------------
    >> ## using text & parse {cheap and dirty} :
    >> mkseq <- function(i,j) if(i == j) i else paste(i,":",j, sep="")
    >> parse(text =
    >> paste("c(", paste(mapply(mkseq, s1,s2), collapse = ","), ")", sep = ""),
    >> srcfile = NULL)[[1]]
    >> ------------------------------------------------------------------------
    >> 
    >> by code that does not use parse (or source() or similar) ???
    >> 
    >> I don't have an answer yet, at least not at all an elegant one.
    >> And maybe, the solution to the quiz is that there is no elegant
    >> solution.

    TP> How about this ? :

    >> i <- c(1, 10, 12)
    >> j <- c(5, 10, 14)
    >> mkseq <- function(i, j) if (i==j) i else call(':', i, j)
    >> as.call(c(list(as.name('c')), mapply(i, j, FUN=mkseq)))

Excellent, Tony!
That's just about what I had tried to do myself for half an hour
and didn't get around to..

So, I'd say you've clearly won the quiz. 
Congratulations!

If you can think of an appropriate price, please say so.
Otherwise, if we meet at the next useR! conference in
Dortmund.. it will be a beer or something like that..

Martin

    TP> c(1:5, 10, 12:14)
    >> eval(.Last.value)
    TP> [1]  1  2  3  4  5 10 12 13 14
    >> 

    TP> -- Tony Plate


From Roger.Bivand at nhh.no  Sat Dec 22 14:11:04 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 22 Dec 2007 13:11:04 +0000 (UTC)
Subject: [R] matrix to gal object
References: <b28bd39c0712211120n51490877w3e160b14b64d6464@mail.gmail.com>
Message-ID: <loom.20071222T130645-280@post.gmane.org>

bernardo lagos alvarez <blacertain <at> gmail.com> writes:

> 
> useR's
> 
> I need transform the matrix
> 
> wdat
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    0    1    0    1    1    0
> [2,]    1    0    0    1    1    0
> [3,]    0    0    0    0    1    1
> [4,]    1    1    0    0    1    0
> [5,]    1    1    1    1    0    0
> [6,]    0    0    1    0    0    0
> 
> to  gal object. How I do with spdep?

RSiteSearch("matrix listw")

gets you to several threads on the R-sig-geo list, suggesting in your case:

w <- mat2listw(wdat)
# convert matrix to spatial weights list
write.nb.gal(w$neighbours, "wdat.gal")
# write neighbours component of spatial weights list as GAL file

Roger Bivand



> 
> Thanks in advance for the help.
> 
> Bernardo.
> University  of Concepci?.
> 
> ______________________________________________
> R-help <at> r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From johannes_graumann at web.de  Sat Dec 22 14:38:21 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Sat, 22 Dec 2007 14:38:21 +0100
Subject: [R] Finding overlaps in vector
References: <fkfrm5$vj3$1@ger.gmane.org>
	<971536df0712210809w56b96e6arb20eee05368bef26@mail.gmail.com>
	<fkh7e9$is7$1@ger.gmane.org>
	<971536df0712211451h79513980ldf08edcb0ea28d3e@mail.gmail.com>
Message-ID: <fkj40c$o3t$1@ger.gmane.org>

Here's what I finally came up with. Thanks for your help!

Joh

MQUSpotOverlapClusters <- function(
  Series,# Vector of data to be evaluated
  distance=0.5,# Maximum distance of clustered data points
  minSize=2# Minimum size of clusters returned
){
############################################################################################
  # Check prerequisites
  #####################
  # Check prerequisites: Series
  if(!(is.numeric(Series) & length(Series) > 1)){
    stop("'Series' must be a vector of numerical data.")
  }
  # Check prerequisites: distance
  if(!(is.numeric(distance) & distance > 0)){
    stop("'distance' must be a positive number.")
  }
############################################################################################
  # Perform clustering
  ####################
  hc <- hclust(dist(Series), method = "single")
  hcut <- cutree(hc,h=distance)
  cluster.idx <- c()
  for(i in unique(hcut)){
    members <- which(hcut == i)
    if(length(members) >= minSize){
      cluster.idx <- append(cluster.idx,list(members))
    }
  }
  return(cluster.idx)
}

Gabor Grothendieck wrote:

> If we don't need any plotting we don't really need rect.hclust at
> all.  Split the output of cutree, instead.  Continuing from the
> prior code:
> 
>> for(el in split(unname(vv), names(vv))) print(el)
> [1] 0.00 0.45
> [1] 1
> [1] 2
> [1] 3.00 3.25 3.33 3.75 4.10
> [1] 5
> [1] 6.00 6.45
> [1] 7.0 7.1
> [1] 8
> 
> On Dec 21, 2007 3:24 PM, Johannes Graumann <johannes_graumann at web.de>
> wrote:
>> Hm, hm, rect.hclust doesn't accept "plot=FALSE" and cutree doesn't retain
>> the indexes of membership ... anyway short of ripping out the guts of
>> rect.hclust to achieve the same result without an active graphics device?
>>
>> Joh
>>
>>
>> >> # cluster and plot
>> >> hc <- hclust(dist(v), method = "single")
>> >> plot(hc, lab = v)
>> >> cl <- rect.hclust(hc, h = .5, border = "red")
>> >>
>> >> # each component of list cl is one cluster.  Print them out.
>> >> for(idx in cl) print(unname(v[idx]))
>> > [1] 8
>> > [1] 7.0 7.1
>> > [1] 6.00 6.45
>> > [1] 5
>> > [1] 3.00 3.25 3.33 3.75 4.10
>> > [1] 2
>> > [1] 1
>> > [1] 0.00 0.45
>> >
>> >> # a different representation of the clusters
>> >> vv <- v
>> >> names(vv) <- ct <- cutree(hc, h = .5)
>> >> vv
>> >    1    1    2    3    4    4    4    4    4    5    6    6    7    7  
>> >     8
>> > 0.00 0.45 1.00 2.00 3.00 3.25 3.33 3.75 4.10 5.00 6.00 6.45 7.00 7.10
>> > 8.00
>> >
>> >
>> > On Dec 21, 2007 4:56 AM, Johannes Graumann <johannes_graumann at web.de>
>> > wrote:
>> >> <posted & mailed>
>> >>
>> >> Dear all,
>> >>
>> >> I'm trying to solve the problem, of how to find clusters of values in
>> >> a vector that are closer than a given value. Illustrated this might
>> >> look as follows:
>> >>
>> >> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>> >>
>> >> When using '0.5' as the proximity requirement, the following groups
>> >> would result:
>> >> 0,0.45
>> >> 3,3.25,3.33,3.75,4.1
>> >> 6,6.45
>> >> 7,7.1
>> >>
>> >> Jim Holtman proposed a very elegant solution in
>> >> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I
>> >> have modified and perused since he wrote it to me. The beauty of this
>> >> approach is that it will not only work for constant proximity
>> >> requirements as above, but also for overlap-windows defined in terms
>> >> of ppm around each value. Now I have an additional need and have found
>> >> no way (short of iteratively step through all the groups returned) to
>> >> figure out how to do that with Jim's approach: how to figure out that
>> >> 6,6.45 and 7,7.1 are separate clusters?
>> >>
>> >> Thanks for any hints, Joh
>> >>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sat Dec 22 15:04:05 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 22 Dec 2007 14:04:05 +0000 (UTC)
Subject: [R] Finding overlaps in vector
References: <fkfrm5$vj3$1@ger.gmane.org>
	<971536df0712210809w56b96e6arb20eee05368bef26@mail.gmail.com>
	<fkh7e9$is7$1@ger.gmane.org>
	<971536df0712211451h79513980ldf08edcb0ea28d3e@mail.gmail.com>
	<fkinut$re4$1@ger.gmane.org>
Message-ID: <Xns9A0E5C416B76DdNOTwinscomcast@80.91.229.13>

Johannes Graumann <johannes_graumann at web.de> wrote in
news:fkinut$re4$1 at ger.gmane.org: 

> But cutree does away with the indexes from the original input, which
> rect.hclust retains.
> I will have no other choice and match that input with the 'values'
> contained in the clusters ... 

If you want to retain the original rownames, then try:

> vector
 [1] 0.00 0.45 1.00 2.00 3.00 3.25 3.33 3.75 4.10 5.00 6.00 6.45 7.00 
7.10 8.00

#-----start cut-and-pastable-----
#this will "label" individual group membership
#diff(.) returns a vector that is smaller by one than its input
#so it needs to be augmented with c(1,fn(diff((.))

grp.v<-cbind(vector,(c(1,1+cumsum(as.numeric(diff(vector)>0.5)))))

#You can then tally up the counts in groups

tb<-table(grp.v[,2])
tb

#1 2 3 4 5 6 7 8 
#2 1 1 5 1 2 2 1 
# And apply the counts to the rows by doing a 
# "row count" lookup into tb[.]

grp.v<-cbind(grp.v,tb[grp.v[,2]])
grp.v

-----end cut and pastable------
  vector    
1   0.00 1 2
1   0.45 1 2
2   1.00 2 1
3   2.00 3 1
4   3.00 4 5
4   3.25 4 5
4   3.33 4 5
4   3.75 4 5
4   4.10 4 5
5   5.00 5 1
6   6.00 6 2
6   6.45 6 2
7   7.00 7 2
7   7.10 7 2
8   8.00 8 1

Further processing of the membership "label" might better be accomplished 
by converting the matrix to a dataframe, and then working with the 
membership "label" as a factor. If you only want to deal with the 
rownames and values of vector that have more than <x> values, that should 
be straightforward.

-- 
David Winsemius
 
> Gabor Grothendieck wrote:
> 
>> If we don't need any plotting we don't really need rect.hclust at
>> all.  Split the output of cutree, instead.  Continuing from the
>> prior code:
>> 
>>> for(el in split(unname(vv), names(vv))) print(el)
>> [1] 0.00 0.45
>> [1] 1
>> [1] 2
>> [1] 3.00 3.25 3.33 3.75 4.10
>> [1] 5
>> [1] 6.00 6.45
>> [1] 7.0 7.1
>> [1] 8
>> 
>> On Dec 21, 2007 3:24 PM, Johannes Graumann <johannes_graumann at web.de>
>> wrote:
>>> Hm, hm, rect.hclust doesn't accept "plot=FALSE" and cutree doesn't
>>> retain the indexes of membership ... anyway short of ripping out the
>>> guts of rect.hclust to achieve the same result without an active
>>> graphics device? 
>>>
>>> Joh
>>>
>>>
>>> >> # cluster and plot
>>> >> hc <- hclust(dist(v), method = "single")
>>> >> plot(hc, lab = v)
>>> >> cl <- rect.hclust(hc, h = .5, border = "red")
>>> >>
>>> >> # each component of list cl is one cluster.  Print them out.
>>> >> for(idx in cl) print(unname(v[idx]))
>>> > [1] 8
>>> > [1] 7.0 7.1
>>> > [1] 6.00 6.45
>>> > [1] 5
>>> > [1] 3.00 3.25 3.33 3.75 4.10
>>> > [1] 2
>>> > [1] 1
>>> > [1] 0.00 0.45
>>> >
>>> >> # a different representation of the clusters
>>> >> vv <- v
>>> >> names(vv) <- ct <- cutree(hc, h = .5)
>>> >> vv
>>> >    1    1    2    3    4    4    4    4    4    5    6    6    7  
>>> >     7  
>>> >     8
>>> > 0.00 0.45 1.00 2.00 3.00 3.25 3.33 3.75 4.10 5.00 6.00 6.45 7.00
>>> > 7.10 8.00
>>> >
>>> >
>>> > On Dec 21, 2007 4:56 AM, Johannes Graumann
>>> > <johannes_graumann at web.de> wrote:
>>> >> <posted & mailed>
>>> >>
>>> >> Dear all,
>>> >>
>>> >> I'm trying to solve the problem, of how to find clusters of
>>> >> values in a vector that are closer than a given value.
>>> >> Illustrated this might look as follows:
>>> >>
>>> >> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>>> >>
>>> >> When using '0.5' as the proximity requirement, the following
>>> >> groups would result:
>>> >> 0,0.45
>>> >> 3,3.25,3.33,3.75,4.1
>>> >> 6,6.45
>>> >> 7,7.1
>>> >>
>>> >> Jim Holtman proposed a very elegant solution in
>>> >> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which
>>> >> I have modified and perused since he wrote it to me. The beauty
>>> >> of this approach is that it will not only work for constant
>>> >> proximity requirements as above, but also for overlap-windows
>>> >> defined in terms of ppm around each value. Now I have an
>>> >> additional need and have found no way (short of iteratively step
>>> >> through all the groups returned) to figure out how to do that
>>> >> with Jim's approach: how to figure out that 6,6.45 and 7,7.1 are
>>> >> separate clusters? 
>>> >>
>>> >> Thanks for any hints, Joh
>>> >>


From ggrothendieck at gmail.com  Sat Dec 22 17:33:02 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 22 Dec 2007 11:33:02 -0500
Subject: [R] Finding overlaps in vector
In-Reply-To: <fkinut$re4$1@ger.gmane.org>
References: <fkfrm5$vj3$1@ger.gmane.org>
	<971536df0712210809w56b96e6arb20eee05368bef26@mail.gmail.com>
	<fkh7e9$is7$1@ger.gmane.org>
	<971536df0712211451h79513980ldf08edcb0ea28d3e@mail.gmail.com>
	<fkinut$re4$1@ger.gmane.org>
Message-ID: <971536df0712220833j362e044ej5f7f233bfae061c@mail.gmail.com>

If you want indexes, i.e. 1, 2, 3, ... instead of the values in v you
can still use split -- just split on seq_along(v) instead of v (or if
v had names you might want to split along names(v)):

split(seq_along(v), ct)

and if you only want to retain groups with 2+ elements then
you can just Filter then out:

twoplus <- function(x) length(x) >= 2
Filter(twoplus, split(seq_along(v), ct))

On Dec 22, 2007 5:12 AM, Johannes Graumann <johannes_graumann at web.de> wrote:
> But cutree does away with the indexes from the original input, which
> rect.hclust retains.
> I will have no other choice and match that input with the 'values' contained
> in the clusters ...
>
> Joh
>
>
> Gabor Grothendieck wrote:
>
> > If we don't need any plotting we don't really need rect.hclust at
> > all.  Split the output of cutree, instead.  Continuing from the
> > prior code:
> >
> >> for(el in split(unname(vv), names(vv))) print(el)
> > [1] 0.00 0.45
> > [1] 1
> > [1] 2
> > [1] 3.00 3.25 3.33 3.75 4.10
> > [1] 5
> > [1] 6.00 6.45
> > [1] 7.0 7.1
> > [1] 8
> >
> > On Dec 21, 2007 3:24 PM, Johannes Graumann <johannes_graumann at web.de>
> > wrote:
> >> Hm, hm, rect.hclust doesn't accept "plot=FALSE" and cutree doesn't retain
> >> the indexes of membership ... anyway short of ripping out the guts of
> >> rect.hclust to achieve the same result without an active graphics device?
> >>
> >> Joh
> >>
> >>
> >> >> # cluster and plot
> >> >> hc <- hclust(dist(v), method = "single")
> >> >> plot(hc, lab = v)
> >> >> cl <- rect.hclust(hc, h = .5, border = "red")
> >> >>
> >> >> # each component of list cl is one cluster.  Print them out.
> >> >> for(idx in cl) print(unname(v[idx]))
> >> > [1] 8
> >> > [1] 7.0 7.1
> >> > [1] 6.00 6.45
> >> > [1] 5
> >> > [1] 3.00 3.25 3.33 3.75 4.10
> >> > [1] 2
> >> > [1] 1
> >> > [1] 0.00 0.45
> >> >
> >> >> # a different representation of the clusters
> >> >> vv <- v
> >> >> names(vv) <- ct <- cutree(hc, h = .5)
> >> >> vv
> >> >    1    1    2    3    4    4    4    4    4    5    6    6    7    7
> >> >     8
> >> > 0.00 0.45 1.00 2.00 3.00 3.25 3.33 3.75 4.10 5.00 6.00 6.45 7.00 7.10
> >> > 8.00
> >> >
> >> >
> >> > On Dec 21, 2007 4:56 AM, Johannes Graumann <johannes_graumann at web.de>
> >> > wrote:
> >> >> <posted & mailed>
> >> >>
> >> >> Dear all,
> >> >>
> >> >> I'm trying to solve the problem, of how to find clusters of values in
> >> >> a vector that are closer than a given value. Illustrated this might
> >> >> look as follows:
> >> >>
> >> >> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
> >> >>
> >> >> When using '0.5' as the proximity requirement, the following groups
> >> >> would result:
> >> >> 0,0.45
> >> >> 3,3.25,3.33,3.75,4.1
> >> >> 6,6.45
> >> >> 7,7.1
> >> >>
> >> >> Jim Holtman proposed a very elegant solution in
> >> >> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I
> >> >> have modified and perused since he wrote it to me. The beauty of this
> >> >> approach is that it will not only work for constant proximity
> >> >> requirements as above, but also for overlap-windows defined in terms
> >> >> of ppm around each value. Now I have an additional need and have found
> >> >> no way (short of iteratively step through all the groups returned) to
> >> >> figure out how to do that with Jim's approach: how to figure out that
> >> >> 6,6.45 and 7,7.1 are separate clusters?
> >> >>
> >> >> Thanks for any hints, Joh
> >> >>
> >
>
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From johannes_graumann at web.de  Sat Dec 22 21:16:28 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Sat, 22 Dec 2007 21:16:28 +0100
Subject: [R] Finding overlaps in vector
References: <fkfrm5$vj3$1@ger.gmane.org>
	<971536df0712210809w56b96e6arb20eee05368bef26@mail.gmail.com>
	<fkh7e9$is7$1@ger.gmane.org>
	<971536df0712211451h79513980ldf08edcb0ea28d3e@mail.gmail.com>
	<fkinut$re4$1@ger.gmane.org>
	<971536df0712220833j362e044ej5f7f233bfae061c@mail.gmail.com>
Message-ID: <fkjraq$mrj$1@ger.gmane.org>

Enlightening. Thanks.

Joh

Gabor Grothendieck wrote:

> If you want indexes, i.e. 1, 2, 3, ... instead of the values in v you
> can still use split -- just split on seq_along(v) instead of v (or if
> v had names you might want to split along names(v)):
> 
> split(seq_along(v), ct)
> 
> and if you only want to retain groups with 2+ elements then
> you can just Filter then out:
> 
> twoplus <- function(x) length(x) >= 2
> Filter(twoplus, split(seq_along(v), ct))
> 
> On Dec 22, 2007 5:12 AM, Johannes Graumann <johannes_graumann at web.de>
> wrote:
>> But cutree does away with the indexes from the original input, which
>> rect.hclust retains.
>> I will have no other choice and match that input with the 'values'
>> contained in the clusters ...
>>
>> Joh
>>
>>
>> Gabor Grothendieck wrote:
>>
>> > If we don't need any plotting we don't really need rect.hclust at
>> > all.  Split the output of cutree, instead.  Continuing from the
>> > prior code:
>> >
>> >> for(el in split(unname(vv), names(vv))) print(el)
>> > [1] 0.00 0.45
>> > [1] 1
>> > [1] 2
>> > [1] 3.00 3.25 3.33 3.75 4.10
>> > [1] 5
>> > [1] 6.00 6.45
>> > [1] 7.0 7.1
>> > [1] 8
>> >
>> > On Dec 21, 2007 3:24 PM, Johannes Graumann <johannes_graumann at web.de>
>> > wrote:
>> >> Hm, hm, rect.hclust doesn't accept "plot=FALSE" and cutree doesn't
>> >> retain the indexes of membership ... anyway short of ripping out the
>> >> guts of rect.hclust to achieve the same result without an active
>> >> graphics device?
>> >>
>> >> Joh
>> >>
>> >>
>> >> >> # cluster and plot
>> >> >> hc <- hclust(dist(v), method = "single")
>> >> >> plot(hc, lab = v)
>> >> >> cl <- rect.hclust(hc, h = .5, border = "red")
>> >> >>
>> >> >> # each component of list cl is one cluster.  Print them out.
>> >> >> for(idx in cl) print(unname(v[idx]))
>> >> > [1] 8
>> >> > [1] 7.0 7.1
>> >> > [1] 6.00 6.45
>> >> > [1] 5
>> >> > [1] 3.00 3.25 3.33 3.75 4.10
>> >> > [1] 2
>> >> > [1] 1
>> >> > [1] 0.00 0.45
>> >> >
>> >> >> # a different representation of the clusters
>> >> >> vv <- v
>> >> >> names(vv) <- ct <- cutree(hc, h = .5)
>> >> >> vv
>> >> >    1    1    2    3    4    4    4    4    4    5    6    6    7   
>> >> >    7
>> >> >     8
>> >> > 0.00 0.45 1.00 2.00 3.00 3.25 3.33 3.75 4.10 5.00 6.00 6.45 7.00
>> >> > 7.10 8.00
>> >> >
>> >> >
>> >> > On Dec 21, 2007 4:56 AM, Johannes Graumann
>> >> > <johannes_graumann at web.de> wrote:
>> >> >> <posted & mailed>
>> >> >>
>> >> >> Dear all,
>> >> >>
>> >> >> I'm trying to solve the problem, of how to find clusters of values
>> >> >> in a vector that are closer than a given value. Illustrated this
>> >> >> might look as follows:
>> >> >>
>> >> >> vector <- c(0,0.45,1,2,3,3.25,3.33,3.75,4.1,5,6,6.45,7,7.1,8)
>> >> >>
>> >> >> When using '0.5' as the proximity requirement, the following groups
>> >> >> would result:
>> >> >> 0,0.45
>> >> >> 3,3.25,3.33,3.75,4.1
>> >> >> 6,6.45
>> >> >> 7,7.1
>> >> >>
>> >> >> Jim Holtman proposed a very elegant solution in
>> >> >> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21286.html, which I
>> >> >> have modified and perused since he wrote it to me. The beauty of
>> >> >> this approach is that it will not only work for constant proximity
>> >> >> requirements as above, but also for overlap-windows defined in
>> >> >> terms of ppm around each value. Now I have an additional need and
>> >> >> have found no way (short of iteratively step through all the groups
>> >> >> returned) to figure out how to do that with Jim's approach: how to
>> >> >> figure out that 6,6.45 and 7,7.1 are separate clusters?
>> >> >>
>> >> >> Thanks for any hints, Joh
>> >> >>
>> >
>>
>> > ______________________________________________
>> > R-help at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html and provide commented,
>> > minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From cskiadas at gmail.com  Sat Dec 22 21:30:34 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Sat, 22 Dec 2007 15:30:34 -0500
Subject: [R] Understanding eval
Message-ID: <0B375093-6247-4700-A793-2C46C5462E05@gmail.com>

After many hours of debugging code, I came to the conclusion that I  
have a fundamental misunderstanding regarding eval, and hope that  
someone here can explain to me, why the following code acts as it does:

foo <- function(expr) {
   eval(substitute(expr), envir=list(a=5), enclos=parent.frame())
}
bar <- function(er) {
   foo(er)
}

 > foo(a)
[1] 5
 > bar(a)
Error in eval(expr, envir, enclos) : object "a" not found


Now, regarding the "bar(a)" call, this is my understanding of what  
happens, hoping someone will correct me where I'm wrong.

1) bar is called. Its evaluation frame contains the association "er=a".
2) bar calls foo. So foo is called, and its evaluation frame contains  
the association "expr=er", with enclosing environment the local  
environment of bar.
3) foo calls eval.
4) eval starts by evaluating "substitute(expr)" in foo's environment.  
"substitute" then locates expr in foo's environment, and replaces it  
with er. So the result of this process is the symbol er, which is  
what will now be evaluated by eval.
5) eval then creates the environment where this evaluation will take  
place. It does that by creating an environment containing the frame  
"a=5", and with enclosing environment the parent frame of foo, which  
is bar's environment.
6) So, as I understand it, the symbol "er" is going to now be  
evaluated in an environment where a is set to 5 and er is set to a,  
along with whatever is in the user's workspace.
7) So the first step now is looking up a definition for er. Nothing  
is found in the current frame, so the evaluation proceeds to bar's  
environment, where the association "er=a" is found, so er is replaced  
by a.
8) Now, and perhaps this is where I misunderstand things, the lookup  
for a will take place. My thinking was that the lookup would start  
from the evaluation environment that eval created, and hence would  
locate the a=5 value. But this is clearly not what happens.

Anyway, hope someone will correct me where I'm wrong, and explain to  
me what I am doing wrong, and ideally how to diagnose such things.

Thanks,
Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From murdoch at stats.uwo.ca  Sat Dec 22 22:44:46 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 22 Dec 2007 16:44:46 -0500
Subject: [R] Understanding eval
In-Reply-To: <0B375093-6247-4700-A793-2C46C5462E05@gmail.com>
References: <0B375093-6247-4700-A793-2C46C5462E05@gmail.com>
Message-ID: <476D854E.3020607@stats.uwo.ca>

On 22/12/2007 3:30 PM, Charilaos Skiadas wrote:
> After many hours of debugging code, I came to the conclusion that I  
> have a fundamental misunderstanding regarding eval, and hope that  
> someone here can explain to me, why the following code acts as it does:
> 
> foo <- function(expr) {
>    eval(substitute(expr), envir=list(a=5), enclos=parent.frame())
> }
> bar <- function(er) {
>    foo(er)
> }
> 
>  > foo(a)
> [1] 5
>  > bar(a)
> Error in eval(expr, envir, enclos) : object "a" not found
> 
> 
> Now, regarding the "bar(a)" call, this is my understanding of what  
> happens, hoping someone will correct me where I'm wrong.
> 
> 1) bar is called. Its evaluation frame contains the association "er=a".
> 2) bar calls foo. So foo is called, and its evaluation frame contains  
> the association "expr=er", with enclosing environment the local  
> environment of bar.
> 3) foo calls eval.
> 4) eval starts by evaluating "substitute(expr)" in foo's environment.  
> "substitute" then locates expr in foo's environment, and replaces it  
> with er. So the result of this process is the symbol er, which is  
> what will now be evaluated by eval.
> 5) eval then creates the environment where this evaluation will take  
> place. It does that by creating an environment containing the frame  
> "a=5", and with enclosing environment the parent frame of foo, which  
> is bar's environment.
> 6) So, as I understand it, the symbol "er" is going to now be  
> evaluated in an environment where a is set to 5 and er is set to a,  
> along with whatever is in the user's workspace.

I think this part is wrong.  A better description is:

er is going to be evaluated in an environment where a is set to 5.  The 
parent of that environment is the bar evaluation frame, where er is set 
to be a promise to evaluate a in the global environment.

> 7) So the first step now is looking up a definition for er. Nothing  
> is found in the current frame, so the evaluation proceeds to bar's  
> environment, where the association "er=a" is found, so er is replaced  
> by a.

No, at this point an attempt is made to force the promise.  Promises 
have their own associated environments, and that's where the evaluation 
takes place.  In the case of the er object, the associated environment 
is the one where bar(a) was called, i.e. the global environment.

> 8) Now, and perhaps this is where I misunderstand things, the lookup  
> for a will take place. My thinking was that the lookup would start  
> from the evaluation environment that eval created, and hence would  
> locate the a=5 value. But this is clearly not what happens.
> 
> Anyway, hope someone will correct me where I'm wrong, and explain to  
> me what I am doing wrong, and ideally how to diagnose such things.

Diagnosing things like this is hard.  Promises are very difficult things 
to look at:  as soon as you try to do anything with them they get 
evaluated, and there's no way in R code to display them without that.
You can use substitute() to extract the expression part, but there's no 
way to extract the environment part.  Maybe there should be, but it's 
tricky to get the semantics right.  If the function environment() worked 
to extract the environment of a promise, then all sorts of code would 
fail where I really wanted to evaluate the arg before extracting the 
environment.

Duncan Murdoch


From Cyonora at gmail.com  Sat Dec 22 15:28:23 2007
From: Cyonora at gmail.com (-Halcyon-)
Date: Sat, 22 Dec 2007 06:28:23 -0800 (PST)
Subject: [R]  vector distribution
Message-ID: <14469865.post@talk.nabble.com>


Hi everyone, 

say i have a population (stable) with different amounts of animals in every
ageclass (80 of age 1, 60 of age 2, etc) in a vector.
Can anybody tell me how i can add gender (male or female) to all ageclasses?
I want a 1:1 ratio of males and females within the population (as a starting
value). So, 40 males and 40 females of age 1, 30m-30f of age 2, etc etc... 

Any help would be greatly appreciated!! 

Kind regards
-- 
View this message in context: http://www.nabble.com/vector-distribution-tp14469865p14469865.html
Sent from the R help mailing list archive at Nabble.com.


From louismartinbis at yahoo.fr  Fri Dec 21 23:03:54 2007
From: louismartinbis at yahoo.fr (Louis Martin)
Date: Fri, 21 Dec 2007 23:03:54 +0100 (CET)
Subject: [R] number of count of each unique row
Message-ID: <135962.65084.qm@web27415.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071221/195e4a12/attachment.pl 

From cskiadas at gmail.com  Sat Dec 22 23:45:52 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Sat, 22 Dec 2007 17:45:52 -0500
Subject: [R] Understanding eval
In-Reply-To: <476D854E.3020607@stats.uwo.ca>
References: <0B375093-6247-4700-A793-2C46C5462E05@gmail.com>
	<476D854E.3020607@stats.uwo.ca>
Message-ID: <E14592DA-CB39-4A18-A93C-36268E6DA078@gmail.com>

On Dec 22, 2007, at 4:44 PM, Duncan Murdoch wrote:
>> 5) eval then creates the environment where this evaluation will  
>> take  place. It does that by creating an environment containing  
>> the frame  "a=5", and with enclosing environment the parent frame  
>> of foo, which  is bar's environment.
>> 6) So, as I understand it, the symbol "er" is going to now be   
>> evaluated in an environment where a is set to 5 and er is set to  
>> a,  along with whatever is in the user's workspace.
>
> I think this part is wrong.  A better description is:
>
> er is going to be evaluated in an environment where a is set to 5.   
> The parent of that environment is the bar evaluation frame, where  
> er is set to be a promise to evaluate a in the global environment.
>
>> 7) So the first step now is looking up a definition for er.  
>> Nothing  is found in the current frame, so the evaluation proceeds  
>> to bar's  environment, where the association "er=a" is found, so  
>> er is replaced  by a.
>
> No, at this point an attempt is made to force the promise.   
> Promises have their own associated environments, and that's where  
> the evaluation takes place.  In the case of the er object, the  
> associated environment is the one where bar(a) was called, i.e. the  
> global environment.
>
>> 8) Now, and perhaps this is where I misunderstand things, the  
>> lookup  for a will take place. My thinking was that the lookup  
>> would start  from the evaluation environment that eval created,  
>> and hence would  locate the a=5 value. But this is clearly not  
>> what happens.
>> Anyway, hope someone will correct me where I'm wrong, and explain  
>> to  me what I am doing wrong, and ideally how to diagnose such  
>> things.
>
> Diagnosing things like this is hard.  Promises are very difficult  
> things to look at:  as soon as you try to do anything with them  
> they get evaluated, and there's no way in R code to display them  
> without that.
> You can use substitute() to extract the expression part, but  
> there's no way to extract the environment part.  Maybe there should  
> be, but it's tricky to get the semantics right.  If the function  
> environment() worked to extract the environment of a promise, then  
> all sorts of code would fail where I really wanted to evaluate the  
> arg before extracting the environment.

Thank you Duncan, for the very clear explanation.

Ok, so the substitute "breaks through" the promise of expr, returning  
as a language object the promise of er, and there's no easy way to  
break through that. I ended up with the following, somewhat uglier  
than I wanted, code, which seems to do what I need in this case, and  
hopefully will still work in the more general case I want it to. The  
idea was to break through the er promise in bar, before sending it  
over to foo. Then foo receives simply an expression, which it can  
then evaluate. Though I seem to have had to work a bit harder on that  
part than I expected to. Perhaps there's an easier way? Or things  
that can go seriously wrong with this way?

foo <- function(fr, expr) {
   ..obj <- list(.=fr)
   ..expr <- substitute(expr)
   ..txt <- parse( text=paste("substitute(",..expr,")") )
   ..expr <- eval(..txt, ..obj, parent.frame())
   ..expr <- eval(..expr, parent.frame())
   eval(..expr, ..obj)
}
bar <- function(parent, er, ...) {
   .fr=parent
   g <- substitute(er)
   foo(.fr, g)
}

 > foo(5,.)
[1] 5
 > bar(5,.)
[1] 5


> Duncan Murdoch

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From gkerns at ysu.edu  Sun Dec 23 00:55:06 2007
From: gkerns at ysu.edu (G. Jay Kerns)
Date: Sat, 22 Dec 2007 18:55:06 -0500
Subject: [R] number of count of each unique row
In-Reply-To: <135962.65084.qm@web27415.mail.ukl.yahoo.com>
References: <135962.65084.qm@web27415.mail.ukl.yahoo.com>
Message-ID: <a695148b0712221555x41f7d65en580a2dbff3c82809@mail.gmail.com>

Hi Louis,

If I am understanding your question correctly, here is one way:
suppose your matrix is M of dimension n x k.

D <- data.frame(M)  # convert to data frame
ones <- rep(1, n)   # a column of 1s

Now you can count the number of repeats of each unique row.

aggregate( ones, by = as.list(D), FUN = sum)

The output will be a data frame with the unique rows, and a column at
the end labeled "x" with the frequency of each unique row.

Once you get this you can convert to a list, manipulate, etc.  I am
sure that there exist faster/better methods.

Best,
Jay




On Dec 21, 2007 5:03 PM, Louis Martin <louismartinbis at yahoo.fr> wrote:
> Hi,
>
> I have a matrix of duplicate rows. How to output a list the unique rows with their count? I have used "unique" to have the unique rows, but can't produce the occurences of each unique row.
>
> Thanks
>
> Louis
>
>
>
> ---------------------------------
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 



***************************************************
G. Jay Kerns, Ph.D.
Assistant Professor / Statistics Coordinator
Department of Mathematics & Statistics
Youngstown State University
Youngstown, OH 44555-0002 USA
Office: 1035 Cushwa Hall
Phone: (330) 941-3310 Office (voice mail)
-3302 Department
-3170 FAX
E-mail: gkerns at ysu.edu
http://www.cc.ysu.edu/~gjkerns/


From tplate at acm.org  Sun Dec 23 02:33:06 2007
From: tplate at acm.org (Tony Plate)
Date: Sat, 22 Dec 2007 18:33:06 -0700
Subject: [R] [R-pkgs] new version of trackObjs
Message-ID: <476DBAD2.1020700@acm.org>

The trackObjs package stores objects in files on disk so that files are
automatically rewritten when objects are changed, and so
that objects are accessible but do not occupy memory until
they are accessed. Also tracks times when objects are created
and modified, and caches some basic characteristics of objects
to allow for fast summaries of objects.

This version trackObjs_0.8-0 fixes some bugs:

     o   Fixed faulty detection of conflicting existing objects
         when starting to track to an existing directory.

     o   Replaced environment on function that is in the active
         binding for a tracked object.  Previously, that function
         could, if constructed via track(obj <- value), have a
         copy of the tracked object in its environment, which would
         stay present taking up memory even if the object was
         flushed out of the tracking environment.

     o   Fixed bug that stopped track.stop(all=TRUE) from working

-- Tony Plate

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From ggrothendieck at gmail.com  Sun Dec 23 02:51:57 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 22 Dec 2007 20:51:57 -0500
Subject: [R] Understanding eval
In-Reply-To: <0B375093-6247-4700-A793-2C46C5462E05@gmail.com>
References: <0B375093-6247-4700-A793-2C46C5462E05@gmail.com>
Message-ID: <971536df0712221751g5b244c24pbb42057e65147d46@mail.gmail.com>

Duncan has already pointed out that consideration of promises is
what is missing in the description but in addition the way lm and
other functions in R get around it is to use match.call like this:

bar2 <- function(er) {
	mc <- match.call()
	mc[[1]] <- as.name("foo")
	names(mc)[[2]] <- "expr"
	eval.parent(mc)
}
bar2(a)

On Dec 22, 2007 3:30 PM, Charilaos Skiadas <cskiadas at gmail.com> wrote:
> After many hours of debugging code, I came to the conclusion that I
> have a fundamental misunderstanding regarding eval, and hope that
> someone here can explain to me, why the following code acts as it does:
>
> foo <- function(expr) {
>   eval(substitute(expr), envir=list(a=5), enclos=parent.frame())
> }
> bar <- function(er) {
>   foo(er)
> }
>
>  > foo(a)
> [1] 5
>  > bar(a)
> Error in eval(expr, envir, enclos) : object "a" not found
>
>
> Now, regarding the "bar(a)" call, this is my understanding of what
> happens, hoping someone will correct me where I'm wrong.
>
> 1) bar is called. Its evaluation frame contains the association "er=a".
> 2) bar calls foo. So foo is called, and its evaluation frame contains
> the association "expr=er", with enclosing environment the local
> environment of bar.
> 3) foo calls eval.
> 4) eval starts by evaluating "substitute(expr)" in foo's environment.
> "substitute" then locates expr in foo's environment, and replaces it
> with er. So the result of this process is the symbol er, which is
> what will now be evaluated by eval.
> 5) eval then creates the environment where this evaluation will take
> place. It does that by creating an environment containing the frame
> "a=5", and with enclosing environment the parent frame of foo, which
> is bar's environment.
> 6) So, as I understand it, the symbol "er" is going to now be
> evaluated in an environment where a is set to 5 and er is set to a,
> along with whatever is in the user's workspace.
> 7) So the first step now is looking up a definition for er. Nothing
> is found in the current frame, so the evaluation proceeds to bar's
> environment, where the association "er=a" is found, so er is replaced
> by a.
> 8) Now, and perhaps this is where I misunderstand things, the lookup
> for a will take place. My thinking was that the lookup would start
> from the evaluation environment that eval created, and hence would
> locate the a=5 value. But this is clearly not what happens.
>
> Anyway, hope someone will correct me where I'm wrong, and explain to
> me what I am doing wrong, and ideally how to diagnose such things.
>
> Thanks,
> Haris Skiadas
> Department of Mathematics and Computer Science
> Hanover College
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cskiadas at gmail.com  Sun Dec 23 03:45:30 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Sat, 22 Dec 2007 21:45:30 -0500
Subject: [R] Understanding eval
In-Reply-To: <971536df0712221751g5b244c24pbb42057e65147d46@mail.gmail.com>
References: <0B375093-6247-4700-A793-2C46C5462E05@gmail.com>
	<971536df0712221751g5b244c24pbb42057e65147d46@mail.gmail.com>
Message-ID: <FC41D53A-7284-4CB2-9849-1F551296B2E3@gmail.com>

That's a very good idea. I'll have to see if and how it applies to  
the more general case I'm dealing with.  ( Essentially I am trying to  
create "tkcbind" and "tkrbind" kind of functions, that group together  
tcltk widgets under a new frame they create, and the frame has to be  
created before the widgets are and the widgets should be created with  
the frame as their parent  etc. ... ). Anyway, that's another story  
for another day ;).

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College

On Dec 22, 2007, at 8:51 PM, Gabor Grothendieck wrote:

> Duncan has already pointed out that consideration of promises is
> what is missing in the description but in addition the way lm and
> other functions in R get around it is to use match.call like this:
>
> bar2 <- function(er) {
> 	mc <- match.call()
> 	mc[[1]] <- as.name("foo")
> 	names(mc)[[2]] <- "expr"
> 	eval.parent(mc)
> }
> bar2(a)
>
> On Dec 22, 2007 3:30 PM, Charilaos Skiadas <cskiadas at gmail.com> wrote:
>> After many hours of debugging code, I came to the conclusion that I
>> have a fundamental misunderstanding regarding eval, and hope that
>> someone here can explain to me, why the following code acts as it  
>> does:
>>
>> foo <- function(expr) {
>>   eval(substitute(expr), envir=list(a=5), enclos=parent.frame())
>> }
>> bar <- function(er) {
>>   foo(er)
>> }
>>
>>> foo(a)
>> [1] 5
>>> bar(a)
>> Error in eval(expr, envir, enclos) : object "a" not found
>>
>>
>> Now, regarding the "bar(a)" call, this is my understanding of what
>> happens, hoping someone will correct me where I'm wrong.
>>
>> 1) bar is called. Its evaluation frame contains the association  
>> "er=a".
>> 2) bar calls foo. So foo is called, and its evaluation frame contains
>> the association "expr=er", with enclosing environment the local
>> environment of bar.
>> 3) foo calls eval.
>> 4) eval starts by evaluating "substitute(expr)" in foo's environment.
>> "substitute" then locates expr in foo's environment, and replaces it
>> with er. So the result of this process is the symbol er, which is
>> what will now be evaluated by eval.
>> 5) eval then creates the environment where this evaluation will take
>> place. It does that by creating an environment containing the frame
>> "a=5", and with enclosing environment the parent frame of foo, which
>> is bar's environment.
>> 6) So, as I understand it, the symbol "er" is going to now be
>> evaluated in an environment where a is set to 5 and er is set to a,
>> along with whatever is in the user's workspace.
>> 7) So the first step now is looking up a definition for er. Nothing
>> is found in the current frame, so the evaluation proceeds to bar's
>> environment, where the association "er=a" is found, so er is replaced
>> by a.
>> 8) Now, and perhaps this is where I misunderstand things, the lookup
>> for a will take place. My thinking was that the lookup would start
>> from the evaluation environment that eval created, and hence would
>> locate the a=5 value. But this is clearly not what happens.
>>
>> Anyway, hope someone will correct me where I'm wrong, and explain to
>> me what I am doing wrong, and ideally how to diagnose such things.
>>
>> Thanks,
>> Haris Skiadas
>> Department of Mathematics and Computer Science
>> Hanover College
>>


From jim at bitwrit.com.au  Sun Dec 23 11:01:31 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sun, 23 Dec 2007 21:01:31 +1100
Subject: [R] number of count of each unique row
In-Reply-To: <135962.65084.qm@web27415.mail.ukl.yahoo.com>
References: <135962.65084.qm@web27415.mail.ukl.yahoo.com>
Message-ID: <476E31FB.4000909@bitwrit.com.au>

Louis Martin wrote:
> Hi,
> 
> I have a matrix of duplicate rows. How to output a list the unique rows with their count? I have used "unique" to have the unique rows, but can't produce the occurences of each unique row.
> 
Hi Louis,
If you want the unique rows returned, this might do the job.

unique.rows<-function(x) {
  nrows<-dim(x)[1]
  urows<-1:nrows
  for(i in 1:(nrows-1)) {
   for(j in (i+1):nrows) {
    if(!is.na(urows[j])) if(all(x[i,]==x[j,])) urows[j]<-NA
   }
  }
  return(x[urows[!is.na(urows)],])
}

Jim


From bcarvalh at jhsph.edu  Sun Dec 23 11:04:48 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Sun, 23 Dec 2007 05:04:48 -0500
Subject: [R] number of count of each unique row
In-Reply-To: <476E31FB.4000909@bitwrit.com.au>
References: <135962.65084.qm@web27415.mail.ukl.yahoo.com>
	<476E31FB.4000909@bitwrit.com.au>
Message-ID: <7C53C1BC-A960-4D5A-A029-468FF8EA1B17@jhsph.edu>

i didn't test, but i think you want something like:

table(apply(x, 1, paste, collapse=","))

where "x" is your matrix...

b

On Dec 23, 2007, at 5:01 AM, Jim Lemon wrote:

> Louis Martin wrote:
>> Hi,
>>
>> I have a matrix of duplicate rows. How to output a list the unique  
>> rows with their count? I have used "unique" to have the unique  
>> rows, but can't produce the occurences of each unique row.
>>
> Hi Louis,
> If you want the unique rows returned, this might do the job.
>
> unique.rows<-function(x) {
>  nrows<-dim(x)[1]
>  urows<-1:nrows
>  for(i in 1:(nrows-1)) {
>   for(j in (i+1):nrows) {
>    if(!is.na(urows[j])) if(all(x[i,]==x[j,])) urows[j]<-NA
>   }
>  }
>  return(x[urows[!is.na(urows)],])
> }
>
> Jim
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From charpent at bacbuc.dyndns.org  Sun Dec 23 11:51:49 2007
From: charpent at bacbuc.dyndns.org (Emmanuel Charpentier)
Date: Sun, 23 Dec 2007 11:51:49 +0100
Subject: [R] odfWeave cross-reference
In-Reply-To: <fc.004c540106079503004c540106079503.607951a@oufcnt2.open.ac.uk>
References: <fc.004c540106079503004c540106079503.607951a@oufcnt2.open.ac.uk>
Message-ID: <fklekc$u4t$1@ger.gmane.org>

Chris.H. Snow a ?crit :
> How can I insert cross-references to odfWeave generated figures in my
> source odf before the graphic has been created with odfWeave?

In the current (0.6.0) version, you can't. I wrote some modifications to
odfWeave in this direction, planning to send them back to Max Kuhn after
some testing, but am currently unable to work on them.

Basically, you have to separate the creation of a reference to a
table|figure from the creation of the table|figure itself. The simplest
way is to be able 1) to create some forme of R object containing the
necessary reference info, 2) to use such an R object in ordre to insert
the XML code for reference and|or 3) to pass such an object as an
optional argument to table|figure caption creation.

My current set of functions allows all of the above, but is not
correctly integrated in the package, and has not been tested extensively.

HTH,

					Emmanuel Charpentier


From murdoch at stats.uwo.ca  Sun Dec 23 15:15:13 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 23 Dec 2007 09:15:13 -0500
Subject: [R] Understanding eval
In-Reply-To: <E14592DA-CB39-4A18-A93C-36268E6DA078@gmail.com>
References: <0B375093-6247-4700-A793-2C46C5462E05@gmail.com>	<476D854E.3020607@stats.uwo.ca>
	<E14592DA-CB39-4A18-A93C-36268E6DA078@gmail.com>
Message-ID: <476E6D71.2000303@stats.uwo.ca>

On 22/12/2007 5:45 PM, Charilaos Skiadas wrote:
> On Dec 22, 2007, at 4:44 PM, Duncan Murdoch wrote:
>>> 5) eval then creates the environment where this evaluation will  
>>> take  place. It does that by creating an environment containing  
>>> the frame  "a=5", and with enclosing environment the parent frame  
>>> of foo, which  is bar's environment.
>>> 6) So, as I understand it, the symbol "er" is going to now be   
>>> evaluated in an environment where a is set to 5 and er is set to  
>>> a,  along with whatever is in the user's workspace.
>> I think this part is wrong.  A better description is:
>>
>> er is going to be evaluated in an environment where a is set to 5.   
>> The parent of that environment is the bar evaluation frame, where  
>> er is set to be a promise to evaluate a in the global environment.
>>
>>> 7) So the first step now is looking up a definition for er.  
>>> Nothing  is found in the current frame, so the evaluation proceeds  
>>> to bar's  environment, where the association "er=a" is found, so  
>>> er is replaced  by a.
>> No, at this point an attempt is made to force the promise.   
>> Promises have their own associated environments, and that's where  
>> the evaluation takes place.  In the case of the er object, the  
>> associated environment is the one where bar(a) was called, i.e. the  
>> global environment.
>>
>>> 8) Now, and perhaps this is where I misunderstand things, the  
>>> lookup  for a will take place. My thinking was that the lookup  
>>> would start  from the evaluation environment that eval created,  
>>> and hence would  locate the a=5 value. But this is clearly not  
>>> what happens.
>>> Anyway, hope someone will correct me where I'm wrong, and explain  
>>> to  me what I am doing wrong, and ideally how to diagnose such  
>>> things.
>> Diagnosing things like this is hard.  Promises are very difficult  
>> things to look at:  as soon as you try to do anything with them  
>> they get evaluated, and there's no way in R code to display them  
>> without that.
>> You can use substitute() to extract the expression part, but  
>> there's no way to extract the environment part.  Maybe there should  
>> be, but it's tricky to get the semantics right.  If the function  
>> environment() worked to extract the environment of a promise, then  
>> all sorts of code would fail where I really wanted to evaluate the  
>> arg before extracting the environment.
> 
> Thank you Duncan, for the very clear explanation.
> 
> Ok, so the substitute "breaks through" the promise of expr, returning  
> as a language object the promise of er, and there's no easy way to  
> break through that. I ended up with the following, somewhat uglier  
> than I wanted, code, which seems to do what I need in this case, and  
> hopefully will still work in the more general case I want it to. The  
> idea was to break through the er promise in bar, before sending it  
> over to foo. Then foo receives simply an expression, which it can  
> then evaluate. Though I seem to have had to work a bit harder on that  
> part than I expected to. Perhaps there's an easier way? Or things  
> that can go seriously wrong with this way?
> 
> foo <- function(fr, expr) {
>    ..obj <- list(.=fr)
>    ..expr <- substitute(expr)
>    ..txt <- parse( text=paste("substitute(",..expr,")") )

I think you want

..txt <- parse( text=paste("substitute(",deparse(..expr),")") )

here, but it's even better not to go through the deparse-parse cycle:

..txt <- bquote( substitute( .(..expr) ) )

The main thing that could go wrong is the evaluation of er might not be 
right.  Just because it is an argument to bar doesn't mean bar's frame 
or parent.frame() is the right place to evaluate it.

To check for errors, I'd introduce like-named variables at lots of 
levels, and then put them into the expression you were evaluating in 
such a way that you can tell which one was found.  For example, put
x <- "foo" into foo(), x <- "bar" into bar(), and x <- "global" into the 
global environment.  Then evaluate some expression that prints x and 
make sure you see the right one.

Duncan Murdoch

>    ..expr <- eval(..txt, ..obj, parent.frame())
>    ..expr <- eval(..expr, parent.frame())
>    eval(..expr, ..obj)
> }
> bar <- function(parent, er, ...) {
>    .fr=parent
>    g <- substitute(er)
>    foo(.fr, g)
> }
> 
>  > foo(5,.)
> [1] 5
>  > bar(5,.)
> [1] 5
> 
> 
>> Duncan Murdoch
> 
> Haris Skiadas
> Department of Mathematics and Computer Science
> Hanover College
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Sun Dec 23 15:21:31 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 23 Dec 2007 09:21:31 -0500
Subject: [R] Understanding eval
In-Reply-To: <476E6D71.2000303@stats.uwo.ca>
References: <0B375093-6247-4700-A793-2C46C5462E05@gmail.com>	<476D854E.3020607@stats.uwo.ca>	<E14592DA-CB39-4A18-A93C-36268E6DA078@gmail.com>
	<476E6D71.2000303@stats.uwo.ca>
Message-ID: <476E6EEB.3050804@stats.uwo.ca>

On 23/12/2007 9:15 AM, Duncan Murdoch wrote:
> On 22/12/2007 5:45 PM, Charilaos Skiadas wrote:
>> On Dec 22, 2007, at 4:44 PM, Duncan Murdoch wrote:
>>>> 5) eval then creates the environment where this evaluation will  
>>>> take  place. It does that by creating an environment containing  
>>>> the frame  "a=5", and with enclosing environment the parent frame  
>>>> of foo, which  is bar's environment.
>>>> 6) So, as I understand it, the symbol "er" is going to now be   
>>>> evaluated in an environment where a is set to 5 and er is set to  
>>>> a,  along with whatever is in the user's workspace.
>>> I think this part is wrong.  A better description is:
>>>
>>> er is going to be evaluated in an environment where a is set to 5.   
>>> The parent of that environment is the bar evaluation frame, where  
>>> er is set to be a promise to evaluate a in the global environment.
>>>
>>>> 7) So the first step now is looking up a definition for er.  
>>>> Nothing  is found in the current frame, so the evaluation proceeds  
>>>> to bar's  environment, where the association "er=a" is found, so  
>>>> er is replaced  by a.
>>> No, at this point an attempt is made to force the promise.   
>>> Promises have their own associated environments, and that's where  
>>> the evaluation takes place.  In the case of the er object, the  
>>> associated environment is the one where bar(a) was called, i.e. the  
>>> global environment.
>>>
>>>> 8) Now, and perhaps this is where I misunderstand things, the  
>>>> lookup  for a will take place. My thinking was that the lookup  
>>>> would start  from the evaluation environment that eval created,  
>>>> and hence would  locate the a=5 value. But this is clearly not  
>>>> what happens.
>>>> Anyway, hope someone will correct me where I'm wrong, and explain  
>>>> to  me what I am doing wrong, and ideally how to diagnose such  
>>>> things.
>>> Diagnosing things like this is hard.  Promises are very difficult  
>>> things to look at:  as soon as you try to do anything with them  
>>> they get evaluated, and there's no way in R code to display them  
>>> without that.
>>> You can use substitute() to extract the expression part, but  
>>> there's no way to extract the environment part.  Maybe there should  
>>> be, but it's tricky to get the semantics right.  If the function  
>>> environment() worked to extract the environment of a promise, then  
>>> all sorts of code would fail where I really wanted to evaluate the  
>>> arg before extracting the environment.
>> Thank you Duncan, for the very clear explanation.
>>
>> Ok, so the substitute "breaks through" the promise of expr, returning  
>> as a language object the promise of er, and there's no easy way to  
>> break through that. I ended up with the following, somewhat uglier  
>> than I wanted, code, which seems to do what I need in this case, and  
>> hopefully will still work in the more general case I want it to. The  
>> idea was to break through the er promise in bar, before sending it  
>> over to foo. Then foo receives simply an expression, which it can  
>> then evaluate. Though I seem to have had to work a bit harder on that  
>> part than I expected to. Perhaps there's an easier way? Or things  
>> that can go seriously wrong with this way?
>>
>> foo <- function(fr, expr) {
>>    ..obj <- list(.=fr)
>>    ..expr <- substitute(expr)
>>    ..txt <- parse( text=paste("substitute(",..expr,")") )
> 
> I think you want
> 
> ..txt <- parse( text=paste("substitute(",deparse(..expr),")") )
> 
> here, but it's even better not to go through the deparse-parse cycle:
> 
> ..txt <- bquote( substitute( .(..expr) ) )
> 
> The main thing that could go wrong ...

Sorry, this wasn't written clearly:  This was a response to your second 
question, not a continuation of the bquote() suggestion.

Duncan Murdoch


is the evaluation of er might not be
> right.  Just because it is an argument to bar doesn't mean bar's frame 
> or parent.frame() is the right place to evaluate it.
> 
> To check for errors, I'd introduce like-named variables at lots of 
> levels, and then put them into the expression you were evaluating in 
> such a way that you can tell which one was found.  For example, put
> x <- "foo" into foo(), x <- "bar" into bar(), and x <- "global" into the 
> global environment.  Then evaluate some expression that prints x and 
> make sure you see the right one.
> 
> Duncan Murdoch
> 
>>    ..expr <- eval(..txt, ..obj, parent.frame())
>>    ..expr <- eval(..expr, parent.frame())
>>    eval(..expr, ..obj)
>> }
>> bar <- function(parent, er, ...) {
>>    .fr=parent
>>    g <- substitute(er)
>>    foo(.fr, g)
>> }
>>
>>  > foo(5,.)
>> [1] 5
>>  > bar(5,.)
>> [1] 5
>>
>>
>>> Duncan Murdoch
>> Haris Skiadas
>> Department of Mathematics and Computer Science
>> Hanover College
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From louismartinbis at yahoo.fr  Sun Dec 23 19:16:23 2007
From: louismartinbis at yahoo.fr (Louis Martin)
Date: Sun, 23 Dec 2007 19:16:23 +0100 (CET)
Subject: [R] RE : Re:  number of count of each unique row
In-Reply-To: <a695148b0712221554p49924e7ci8c05808ad782284e@mail.gmail.com>
Message-ID: <605601.86978.qm@web27408.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071223/c0183bea/attachment.pl 

From gregor.gorjanc at bfro.uni-lj.si  Sun Dec 23 20:55:56 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Sun, 23 Dec 2007 19:55:56 +0000 (UTC)
Subject: [R] Sweave and Scientific Workplace
References: <476808C9.80002@uni-osnabrueck.de>
Message-ID: <loom.20071223T195445-405@post.gmane.org>

Dietrich Trenkler <Dietrich.Trenkler <at> uni-osnabrueck.de> writes:
> Dear HelpeRs,
> 
> a colleague of mine uses Scientific Workplace to write his LaTeX documents.
> I made his mouth water mentioning the advantages of using Sweave.
> 
> Not using SW myself I wonder if anyone out there has gathered some 
> experiences
> in using the combination of both.

I can not say anything about SW, but LyX can do the same. I have a paper, that
will probably appear in next issues of Rnews about using Sweave in LyX.

Gregor


From affysnp at gmail.com  Sun Dec 23 22:28:40 2007
From: affysnp at gmail.com (affy snp)
Date: Sun, 23 Dec 2007 16:28:40 -0500
Subject: [R] How to remove some rows from a data.frame
Message-ID: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071223/27253658/attachment.pl 

From ggrothendieck at gmail.com  Sun Dec 23 23:01:53 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 23 Dec 2007 17:01:53 -0500
Subject: [R] How to remove some rows from a data.frame
In-Reply-To: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>
References: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>
Message-ID: <971536df0712231401l3c0e4811r762c562c033d7c01@mail.gmail.com>

On Dec 23, 2007 4:28 PM, affy snp <affysnp at gmail.com> wrote:
> Hello list,
>
> I have a data frame M like:
>
> BAC                 chr    pos          s1   s2
> RP11-80G24    1    77465510    -1    0
> RP11-198H14    1    78696291    -1    0
> RP11-267M21    1    79681704    -1    0
> RP11-89A19      1    80950808    -1    0
> RP11-6B16        1    82255496    -1    0
> RP11-210E16    1    228801510    0    -1
> RP11-155C15    1    230957584    0    -1
> RP11-210F8      1    237932418    0    -1
> RP11-263L17     2    65724492    0    1
> RP11-340F16     2    65879898    0    1
> RP11-68A1        2    67718674    0    0
> RP11-474G23    2    68318411    0    0
> RP11-218N6      2    68454651    0    0
> CTD-2003M22    2    68567494    0    0
> .....
>
> how to remove those rows which have 0 for both of columns s1,s2?
> sth like M[!M$21=0&!M$s2=0]?
>
> Moreover, I want to get a list which could find a subset of rows which have
> the same pattern of data. For example, the first 8 rows in M can be
> clustered
> into 2 groups (represented below in 2 rows) and shown as:
>
> chr             Start       End             # of rows     Pattern
> 1             77465510   82255496       5              (-1 0)
> 1            228801510  237932418     3              (0 -1)
>

Using:

M <- structure(list(BAC = structure(c(13L, 3L, 8L, 14L, 12L, 4L, 2L,
5L, 7L, 9L, 11L, 10L, 6L, 1L), .Label = c("CTD-2003M22", "RP11-155C15",
"RP11-198H14", "RP11-210E16", "RP11-210F8", "RP11-218N6", "RP11-263L17",
"RP11-267M21", "RP11-340F16", "RP11-474G23", "RP11-68A1", "RP11-6B16",
"RP11-80G24", "RP11-89A19"), class = "factor"), chr = c(1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L), pos = c(77465510L,
78696291L, 79681704L, 80950808L, 82255496L, 228801510L, 230957584L,
237932418L, 65724492L, 65879898L, 67718674L, 68318411L, 68454651L,
68567494L), s1 = c(-1L, -1L, -1L, -1L, -1L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L), s2 = c(0L, 0L, 0L, 0L, 0L, -1L, -1L, -1L, 1L,
1L, 0L, 0L, 0L, 0L)), .Names = c("BAC", "chr", "pos", "s1", "s2"
), class = "data.frame", row.names = c(NA, -14L))

# try this

subset(M, s1 | s2)  # as 0 regarded as FALSE and others as TRUE

# and for second question:

f <- function(x) with(x,
  c(start = pos[1], end = tail(pos, 1),
     chr = chr[1], nrow = NROW(x), s1 = s1[1], s2 = s2[1])
)
do.call(rbind, by(M, M[4:5], f))


From macq at llnl.gov  Sun Dec 23 23:14:00 2007
From: macq at llnl.gov (Don MacQueen)
Date: Sun, 23 Dec 2007 14:14:00 -0800
Subject: [R] How to remove some rows from a data.frame
In-Reply-To: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>
References: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>
Message-ID: <p06240800c3948c7bd0ee@[192.168.11.8]>

At 4:28 PM -0500 12/23/07, affy snp wrote:
>Hello list,
>
>I have a data frame M like:
>
>BAC                 chr    pos          s1   s2
>RP11-80G24    1    77465510    -1    0
>RP11-198H14    1    78696291    -1    0
>RP11-267M21    1    79681704    -1    0
>RP11-89A19      1    80950808    -1    0
>RP11-6B16        1    82255496    -1    0
>RP11-210E16    1    228801510    0    -1
>RP11-155C15    1    230957584    0    -1
>RP11-210F8      1    237932418    0    -1
>RP11-263L17     2    65724492    0    1
>RP11-340F16     2    65879898    0    1
>RP11-68A1        2    67718674    0    0
>RP11-474G23    2    68318411    0    0
>RP11-218N6      2    68454651    0    0
>CTD-2003M22    2    68567494    0    0
>.....
>
>how to remove those rows which have 0 for both of columns s1,s2?
>sth like M[!M$21=0&!M$s2=0]?

M[ !(M$s1==0 & M$s2==0) , ]

>
>Moreover, I want to get a list which could find a subset of rows which have
>the same pattern of data. For example, the first 8 rows in M can be
>clustered
>into 2 groups (represented below in 2 rows) and shown as:
>
>chr             Start       End             # of rows     Pattern
>1             77465510   82255496       5              (-1 0)
>1            228801510  237932418     3              (0 -1)
>
>Can anybody help me out of this? Thank you very much and happy holiday!

pat <- paste(M$s1,M$s2)

## to find the first subset:
M[ pat == pat[1] ,]

## to find the second subset:
M[ pat == pat[2], ]

## and so on, for however many unique patterns there are.

## also try
table(pat)

Of course, your example does more than just "find" the subsets. It 
also does some summarizing of them. That's a little more complicated. 
I might start with the summarize() function in the Hmisc package, but 
there are potentially many ways to also do the summarizing.

-Don

>Best,
>     Allen
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
---------------------------------
Don MacQueen
Lawrence Livermore National Laboratory
Livermore, CA, USA
925-423-1062
macq at llnl.gov


From dwinsemius at comcast.net  Sun Dec 23 23:49:33 2007
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 23 Dec 2007 22:49:33 +0000 (UTC)
Subject: [R] (no subject)
References: <OFBE2AD317.965FAC01-ON852573B6.0067196D-852573B6.00671F96@un.org>
Message-ID: <Xns9A0FB55988F60dNOTwinscomcast@80.91.229.13>

Jiaming Zuo <zuoj at un.org> wrote in
news:OFBE2AD317.965FAC01-ON852573B6.0067196D-852573B6.00671F96 at un.org: 

>     I am working for the United Nations to construct a complete life
>     table 
> from an abridged table. 
>     I want to use the code of Hydman Filter by Rob J Hydman but an
>     error 
> sentence always appears and it simply doesn't run--
>        source("C:/R/Jamie/HymanFilter.R")

Considering that the last name of the person with the most publications 
in this area is "Hyndman" (and is author of the R packages, deomgraphy 
and forecasting), I am wondering if the problem is simply spelling.
See:
<http://www-personal.buseco.monash.edu.au/~hyndman/index.php?option=com_content&task=view&id=44&Itemid=71>

>        Error in .C("spline_coef", method = as.integer(method), n = nx,
>        x = 
> x,  : C symbol name "spline_coef" not in DLL for package "base"
>  
>     Can you please tell me what's the problem?

-- 
David Winsemius


From ohitsabid at yahoo.com  Mon Dec 24 01:48:47 2007
From: ohitsabid at yahoo.com (Syed Abid Hussaini)
Date: Sun, 23 Dec 2007 16:48:47 -0800 (PST)
Subject: [R] saving while loop values to one vector
Message-ID: <36979.56041.qm@web35613.mail.mud.yahoo.com>

Hi all,
  I am pretty new to R and even new to programming in general. Right now i get only one value for
j below (print(j)), how do i save all j values to one vector or matrix? Sorry for this very basic
question. thanks in advance.

wave2 <- abs(Re(rnorm(100)))
i <- findInterval((wave2[1]), wave2)
i <- i+1

while (i < length(wave2))  {
j <- (wave2 [i] - wave2[i-1])
print(j)
i<-i+1
}

abid



      ____________________________________________________________________________________
Looking for last minute shopping deals?


From cskiadas at gmail.com  Mon Dec 24 04:30:16 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Sun, 23 Dec 2007 22:30:16 -0500
Subject: [R] Understanding eval
In-Reply-To: <476E6EEB.3050804@stats.uwo.ca>
References: <0B375093-6247-4700-A793-2C46C5462E05@gmail.com>	<476D854E.3020607@stats.uwo.ca>	<E14592DA-CB39-4A18-A93C-36268E6DA078@gmail.com>
	<476E6D71.2000303@stats.uwo.ca> <476E6EEB.3050804@stats.uwo.ca>
Message-ID: <F5F9F10C-CF13-46CA-9AD5-68E03C76FA30@gmail.com>

On Dec 23, 2007, at 9:21 AM, Duncan Murdoch wrote:
> On 23/12/2007 9:15 AM, Duncan Murdoch wrote:
>> On 22/12/2007 5:45 PM, Charilaos Skiadas wrote:
>>> On Dec 22, 2007, at 4:44 PM, Duncan Murdoch wrote:
>>>>> 5) eval then creates the environment where this evaluation  
>>>>> will  take  place. It does that by creating an environment  
>>>>> containing  the frame  "a=5", and with enclosing environment  
>>>>> the parent frame  of foo, which  is bar's environment.
>>>>> 6) So, as I understand it, the symbol "er" is going to now be    
>>>>> evaluated in an environment where a is set to 5 and er is set  
>>>>> to  a,  along with whatever is in the user's workspace.
>>>> I think this part is wrong.  A better description is:
>>>>
>>>> er is going to be evaluated in an environment where a is set to  
>>>> 5.   The parent of that environment is the bar evaluation frame,  
>>>> where  er is set to be a promise to evaluate a in the global  
>>>> environment.
>>>>
>>>>> 7) So the first step now is looking up a definition for er.   
>>>>> Nothing  is found in the current frame, so the evaluation  
>>>>> proceeds  to bar's  environment, where the association "er=a"  
>>>>> is found, so  er is replaced  by a.
>>>> No, at this point an attempt is made to force the promise.    
>>>> Promises have their own associated environments, and that's  
>>>> where  the evaluation takes place.  In the case of the er  
>>>> object, the  associated environment is the one where bar(a) was  
>>>> called, i.e. the  global environment.
>>>>
>>>>> 8) Now, and perhaps this is where I misunderstand things, the   
>>>>> lookup  for a will take place. My thinking was that the lookup   
>>>>> would start  from the evaluation environment that eval  
>>>>> created,  and hence would  locate the a=5 value. But this is  
>>>>> clearly not  what happens.
>>>>> Anyway, hope someone will correct me where I'm wrong, and  
>>>>> explain  to  me what I am doing wrong, and ideally how to  
>>>>> diagnose such  things.
>>>> Diagnosing things like this is hard.  Promises are very  
>>>> difficult  things to look at:  as soon as you try to do anything  
>>>> with them  they get evaluated, and there's no way in R code to  
>>>> display them  without that.
>>>> You can use substitute() to extract the expression part, but   
>>>> there's no way to extract the environment part.  Maybe there  
>>>> should  be, but it's tricky to get the semantics right.  If the  
>>>> function  environment() worked to extract the environment of a  
>>>> promise, then  all sorts of code would fail where I really  
>>>> wanted to evaluate the  arg before extracting the environment.
>>> Thank you Duncan, for the very clear explanation.
>>>
>>> Ok, so the substitute "breaks through" the promise of expr,  
>>> returning  as a language object the promise of er, and there's no  
>>> easy way to  break through that. I ended up with the following,  
>>> somewhat uglier  than I wanted, code, which seems to do what I  
>>> need in this case, and  hopefully will still work in the more  
>>> general case I want it to. The  idea was to break through the er  
>>> promise in bar, before sending it  over to foo. Then foo receives  
>>> simply an expression, which it can  then evaluate. Though I seem  
>>> to have had to work a bit harder on that  part than I expected  
>>> to. Perhaps there's an easier way? Or things  that can go  
>>> seriously wrong with this way?
>>>
>>> foo <- function(fr, expr) {
>>>    ..obj <- list(.=fr)
>>>    ..expr <- substitute(expr)
>>>    ..txt <- parse( text=paste("substitute(",..expr,")") )
>> I think you want
>> ..txt <- parse( text=paste("substitute(",deparse(..expr),")") )
>> here, but it's even better not to go through the deparse-parse cycle:
>> ..txt <- bquote( substitute( .(..expr) ) )
>> The main thing that could go wrong ...
>
> Sorry, this wasn't written clearly:  This was a response to your  
> second question, not a continuation of the bquote() suggestion.

It was clear enough ;). I need to learn to use bquote more often ....

Thanks,
Haris Skiadas
Department of Mathematics and Computer Science
Hanover College

> Duncan Murdoch
>
>
> is the evaluation of er might not be
>> right.  Just because it is an argument to bar doesn't mean bar's  
>> frame or parent.frame() is the right place to evaluate it.
>> To check for errors, I'd introduce like-named variables at lots of  
>> levels, and then put them into the expression you were evaluating  
>> in such a way that you can tell which one was found.  For example,  
>> put
>> x <- "foo" into foo(), x <- "bar" into bar(), and x <- "global"  
>> into the global environment.  Then evaluate some expression that  
>> prints x and make sure you see the right one.
>> Duncan Murdoch
>>>    ..expr <- eval(..txt, ..obj, parent.frame())
>>>    ..expr <- eval(..expr, parent.frame())
>>>    eval(..expr, ..obj)
>>> }
>>> bar <- function(parent, er, ...) {
>>>    .fr=parent
>>>    g <- substitute(er)
>>>    foo(.fr, g)
>>> }
>>>
>>>  > foo(5,.)
>>> [1] 5
>>>  > bar(5,.)
>>> [1] 5
>>>


From dxc13 at health.state.ny.us  Mon Dec 24 04:59:49 2007
From: dxc13 at health.state.ny.us (dxc13)
Date: Sun, 23 Dec 2007 19:59:49 -0800 (PST)
Subject: [R]  expand.grid function
Message-ID: <14484403.post@talk.nabble.com>


useR's,

I have used expand.grid() several times and like the results it gives me.  I
am now trying something with it that I have not been able to get to work. 
For any n column matrix I would like to run this function on those n columns
and store the results.
For example, if my matrix has 1 column then this is just expand.grid(x =
column1).  If my matrix has two columns, then I want expand.grid(x =
column1, y = column2), and so on for any number of columns...

In a program I am writing, the user can specify any matrix.  Does anyone
know of a way for R to calculate this based on what the input matrix is? 
e.g. if this user gives a 3 column matrix, I want to be able to perform
expand.grid() on these 3 columns without having to hard code it b/c I want
to have this small function embedded in my code and the results stored as a
variable.

If this isn't clear, I can try to be more detailed.  Thank you for any
thoughts.

Derek 
-- 
View this message in context: http://www.nabble.com/expand.grid-function-tp14484403p14484403.html
Sent from the R help mailing list archive at Nabble.com.


From cskiadas at gmail.com  Mon Dec 24 05:18:08 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Sun, 23 Dec 2007 23:18:08 -0500
Subject: [R] expand.grid function
In-Reply-To: <14484403.post@talk.nabble.com>
References: <14484403.post@talk.nabble.com>
Message-ID: <8280D62F-0D2F-4FDA-AEBF-BB605AF0DFFD@gmail.com>

Hi Derek,
On Dec 23, 2007, at 10:59 PM, dxc13 wrote:

>
> useR's,
>
> I have used expand.grid() several times and like the results it  
> gives me.  I
> am now trying something with it that I have not been able to get to  
> work.
> For any n column matrix I would like to run this function on those  
> n columns
> and store the results.
> For example, if my matrix has 1 column then this is just expand.grid 
> (x =
> column1).  If my matrix has two columns, then I want expand.grid(x =
> column1, y = column2), and so on for any number of columns...
>

Does this do what you want  (x is the matrix)?

do.call(expand.grid, as.data.frame(x))


> In a program I am writing, the user can specify any matrix.  Does  
> anyone
> know of a way for R to calculate this based on what the input  
> matrix is?
> e.g. if this user gives a 3 column matrix, I want to be able to  
> perform
> expand.grid() on these 3 columns without having to hard code it b/c  
> I want
> to have this small function embedded in my code and the results  
> stored as a
> variable.
>
> If this isn't clear, I can try to be more detailed.  Thank you for any
> thoughts.
>
> Derek

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From dxc13 at health.state.ny.us  Mon Dec 24 05:41:16 2007
From: dxc13 at health.state.ny.us (dxc13)
Date: Sun, 23 Dec 2007 20:41:16 -0800 (PST)
Subject: [R] expand.grid function
In-Reply-To: <8280D62F-0D2F-4FDA-AEBF-BB605AF0DFFD@gmail.com>
References: <14484403.post@talk.nabble.com>
	<8280D62F-0D2F-4FDA-AEBF-BB605AF0DFFD@gmail.com>
Message-ID: <14484572.post@talk.nabble.com>


Yes, that works perfectly.  Thank you for your help!

Derek



Charilaos Skiadas-3 wrote:
> 
> Hi Derek,
> On Dec 23, 2007, at 10:59 PM, dxc13 wrote:
> 
>>
>> useR's,
>>
>> I have used expand.grid() several times and like the results it  
>> gives me.  I
>> am now trying something with it that I have not been able to get to  
>> work.
>> For any n column matrix I would like to run this function on those  
>> n columns
>> and store the results.
>> For example, if my matrix has 1 column then this is just expand.grid 
>> (x =
>> column1).  If my matrix has two columns, then I want expand.grid(x =
>> column1, y = column2), and so on for any number of columns...
>>
> 
> Does this do what you want  (x is the matrix)?
> 
> do.call(expand.grid, as.data.frame(x))
> 
> 
>> In a program I am writing, the user can specify any matrix.  Does  
>> anyone
>> know of a way for R to calculate this based on what the input  
>> matrix is?
>> e.g. if this user gives a 3 column matrix, I want to be able to  
>> perform
>> expand.grid() on these 3 columns without having to hard code it b/c  
>> I want
>> to have this small function embedded in my code and the results  
>> stored as a
>> variable.
>>
>> If this isn't clear, I can try to be more detailed.  Thank you for any
>> thoughts.
>>
>> Derek
> 
> Haris Skiadas
> Department of Mathematics and Computer Science
> Hanover College
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/expand.grid-function-tp14484403p14484572.html
Sent from the R help mailing list archive at Nabble.com.


From ohitsabid at yahoo.com  Mon Dec 24 07:54:50 2007
From: ohitsabid at yahoo.com (Syed Abid Hussaini)
Date: Sun, 23 Dec 2007 22:54:50 -0800 (PST)
Subject: [R] saving while loop values to one vector
Message-ID: <892880.33148.qm@web35610.mail.mud.yahoo.com>

I solved it myself by reading the help files which i should have done prior to my posting. 

Solution:

wave2 <- abs(Re(rnorm(100)))
wave2 <- sort (wave2, decreasing=F)
hist(wave2)
x <- length (wave2)
i <- findInterval((wave2[1]), wave2)
i <- i+1
out <- numeric(x)
for (i in i:x)  {
out[i] <- (wave2 [i] - wave2[i-1])
}
out

As you see i trimmed the code and alloted a new variable called out which i later save my values
to. 



--- Syed Abid Hussaini <ohitsabid at yahoo.com> wrote:

> Hi all,
>   I am pretty new to R and even new to programming in general. Right now i get only one value
> for
> j below (print(j)), how do i save all j values to one vector or matrix? Sorry for this very
> basic
> question. thanks in advance.
> 
> wave2 <- abs(Re(rnorm(100)))
> i <- findInterval((wave2[1]), wave2)
> i <- i+1
> 
> while (i < length(wave2))  {
> j <- (wave2 [i] - wave2[i-1])
> print(j)
> i<-i+1
> }
> 
> abid
> 
> 
> 
>       ____________________________________________________________________________________
> Looking for last minute shopping deals?  

> http://tools.search.yahoo.com/newsearch/category.php?category=shopping
> 



      ____________________________________________________________________________________
Looking for last minute shopping deals?


From ggrothendieck at gmail.com  Mon Dec 24 08:30:57 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Dec 2007 02:30:57 -0500
Subject: [R] saving while loop values to one vector
In-Reply-To: <892880.33148.qm@web35610.mail.mud.yahoo.com>
References: <892880.33148.qm@web35610.mail.mud.yahoo.com>
Message-ID: <971536df0712232330s2788b3c8h30c8c602ca872b8@mail.gmail.com>

Its not clear what your code is intended to do but

- put set.seed(123) before your code so you can
run it reproducibly.

- wave2 <- sort(abs(rnorm(100))) is the same as your first
two lines

- your findInterval line always returns 1 so its probably not what you want

- i in i:x is confusing. Use j in i:x or even better use diff(wave2).

On Dec 24, 2007 1:54 AM, Syed Abid Hussaini <ohitsabid at yahoo.com> wrote:
> I solved it myself by reading the help files which i should have done prior to my posting.
>
> Solution:
>
> wave2 <- abs(Re(rnorm(100)))
> wave2 <- sort (wave2, decreasing=F)
> hist(wave2)
> x <- length (wave2)
> i <- findInterval((wave2[1]), wave2)
> i <- i+1
> out <- numeric(x)
> for (i in i:x)  {
> out[i] <- (wave2 [i] - wave2[i-1])
> }
> out
>
> As you see i trimmed the code and alloted a new variable called out which i later save my values
> to.
>
>
>
> --- Syed Abid Hussaini <ohitsabid at yahoo.com> wrote:
>
> > Hi all,
> >   I am pretty new to R and even new to programming in general. Right now i get only one value
> > for
> > j below (print(j)), how do i save all j values to one vector or matrix? Sorry for this very
> > basic
> > question. thanks in advance.
> >
> > wave2 <- abs(Re(rnorm(100)))
> > i <- findInterval((wave2[1]), wave2)
> > i <- i+1
> >
> > while (i < length(wave2))  {
> > j <- (wave2 [i] - wave2[i-1])
> > print(j)
> > i<-i+1
> > }
> >
> > abid
> >
> >
> >
> >       ____________________________________________________________________________________
> > Looking for last minute shopping deals?
>
> > http://tools.search.yahoo.com/newsearch/category.php?category=shopping
> >
>
>
>
>      ____________________________________________________________________________________
> Looking for last minute shopping deals?
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jabbadhutt at libero.it  Mon Dec 24 03:48:32 2007
From: jabbadhutt at libero.it (Jabba)
Date: Mon, 24 Dec 2007 03:48:32 +0100
Subject: [R] R appearance under linux
In-Reply-To: <slrnfmp894.m5q.tyler.smith@blackbart.sedgenet>
References: <8CA12168E8485A4-AB8-A02@webmail-da15.sysops.aol.com>
	<slrnfmp894.m5q.tyler.smith@blackbart.sedgenet>
Message-ID: <476F1E00.2000301@libero.it>

Tyler Smith ha scritto:
> On 2007-12-21, phguardiol a aol.com <phguardiol a aol.com> wrote:
>>  Dear R users,
>>
>> I have just moved to R2.6.1 under Opensuse linux 10.3. I used to
>> work with R under XPpro. Is it "normal" to have a visual aspect of R
>> under linux different ? 
> 
> Yes, that's normal. Under windows, you get a GUI interface with menus
> and separate windows etc. by default. In Linux you get to choose for
> yourself how you interact with R. One of the better options is Emacs
> with ESS. Details can be found at http://ESS.R-project.org/ including
> rpms for suse. Other options are presented here:
> http://www.sciviews.org/_rgui/
> 
> There will be a bit of a learning curve here, but eventually you'll
> find you can do everything that you could do with the Windows GUI, and
> at least with Emacs and ESS, a whole lot more.
> 
> Tyler

I agree. In the meantime you learn emacs ;-), cosider using this functions:

getwd()
setwd()
save.image()
install.packages()

and  you can install Rmdr and run

library(Rcmdr)

Anyway, I prefer GNU Emacs + ESS, but it can take a bit more time to learn.

Jabba.


From m_olshansky at yahoo.com  Mon Dec 24 09:54:56 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Mon, 24 Dec 2007 00:54:56 -0800 (PST)
Subject: [R] How to remove some rows from a data.frame
In-Reply-To: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>
Message-ID: <671623.2094.qm@web32203.mail.mud.yahoo.com>

To answer your firs question try

M[-which( M$s1 == 0 & M$s2 == 0),]

For the second question, you must start with the more
precise definition of the grouping criterion.

--- affy snp <affysnp at gmail.com> wrote:

> Hello list,
> 
> I have a data frame M like:
> 
> BAC                 chr    pos          s1   s2
> RP11-80G24    1    77465510    -1    0
> RP11-198H14    1    78696291    -1    0
> RP11-267M21    1    79681704    -1    0
> RP11-89A19      1    80950808    -1    0
> RP11-6B16        1    82255496    -1    0
> RP11-210E16    1    228801510    0    -1
> RP11-155C15    1    230957584    0    -1
> RP11-210F8      1    237932418    0    -1
> RP11-263L17     2    65724492    0    1
> RP11-340F16     2    65879898    0    1
> RP11-68A1        2    67718674    0    0
> RP11-474G23    2    68318411    0    0
> RP11-218N6      2    68454651    0    0
> CTD-2003M22    2    68567494    0    0
> .....
> 
> how to remove those rows which have 0 for both of
> columns s1,s2?
> sth like M[!M$21=0&!M$s2=0]?
> 
> Moreover, I want to get a list which could find a
> subset of rows which have
> the same pattern of data. For example, the first 8
> rows in M can be
> clustered
> into 2 groups (represented below in 2 rows) and
> shown as:
> 
> chr             Start       End             # of
> rows     Pattern
> 1             77465510   82255496       5           
>   (-1 0)
> 1            228801510  237932418     3             
> (0 -1)
> 
> Can anybody help me out of this? Thank you very much
> and happy holiday!
> 
> Best,
>     Allen
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From deleeuw at stat.ucla.edu  Sun Dec 23 18:57:08 2007
From: deleeuw at stat.ucla.edu (Jan de Leeuw)
Date: Sun, 23 Dec 2007 09:57:08 -0800
Subject: [R] [R-pkgs] anacor: yet another ca package
Message-ID: <E99D0798-917F-4C4B-A951-C25D86A3175F@stat.ucla.edu>

anacor-0.9.0 is on CRAN (by De Leeuw and Mair)

anacor does correspondence analysis and canonical correspondence
analysis. It can make row plots, column plots,
joint plots, Benz?cri plots, regression plots,
and transformation plots. Where appropriate, plots can
be in 3d using either rgl or scatterplot3d. Row and
column points can be in standard scaling, Benz?cri
scaling, Goodman scaling, row-centroid, or column-centroid
scaling.

The summary method writes out a table with the chi-square
(inertia) decomposition, it also writes out the singular
values, and their asymptotic standard errors under multinomial
sampling. Plots of the category quantifications (row scores
and column scores) can be made optionally with asymptotic
confidence ellipses, again based on multinomial sampling.

The package contain various utilities to switch data formats,
in particular to transform data frames to Burt matrices,
to indicator matrices, and even to fuzzy indicator matrices
using B-spline bases.

The vignette for the package (not added yet) is a paper
also submitted to the special psychoR issue of JSS. You
can get a preprint at

http://idisk.mac.com/jdeleeuw-Public/psychoR/anacor.zip

The psychoR directory also has the anacor package, the
homals package, and the homals paper. smacof (many forms
of multidimensional scaling) is next.

==========================================================
Jan de Leeuw, 11667 Steinhoff Rd, Frazier Park, CA 93225
home 661-245-1725 mobile 661-231-5416 work 310-825-9550
.mac: jdeleeuw +++  aim: deleeuwjan +++ skype: j_deleeuw
==========================================================
          There is no worse screen to block out
    the spirit than confidence in our own intelligence.
                                    ---- John Calvin.

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From maechler at stat.math.ethz.ch  Mon Dec 24 14:00:22 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 24 Dec 2007 14:00:22 +0100
Subject: [R] Diagonal matrix with off diagonal elements
In-Reply-To: <476CE70A.7010903@biostat.ku.dk>
References: <fd3c7adf0712211258h3bcd83d8r1c4938ceb10510d4@mail.gmail.com>
	<14463831.post@talk.nabble.com> <476CE70A.7010903@biostat.ku.dk>
Message-ID: <18287.44390.415297.390319@ada-stat.math.ethz.ch>

>>>>> "PD" == Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>>>>>     on Sat, 22 Dec 2007 11:29:30 +0100 writes:

    PD> Chris Stubben wrote:
    >> Also try the odiag function in the demogR package
    >> 
    >> odiag( 1:5, -1)
    >> [,1] [,2] [,3] [,4] [,5] [,6]
    >> [1,]    0    0    0    0    0    0
    >> [2,]    1    0    0    0    0    0
    >> [3,]    0    2    0    0    0    0
    >> [4,]    0    0    3    0    0    0
    >> [5,]    0    0    0    4    0    0
    >> [6,]    0    0    0    0    5    0
    >> 
    >> Chris
    >> 
    >> 
    PD> Also, this sort of pattern works

    >> m <- matrix(0,6,6)
    >> diag(m[-1,])<-1:5
    >> m
    PD> [,1] [,2] [,3] [,4] [,5] [,6]
    PD> [1,]    0    0    0    0    0    0
    PD> [2,]    1    0    0    0    0    0
    PD> [3,]    0    2    0    0    0    0
    PD> [4,]    0    0    3    0    0    0
    PD> [5,]    0    0    0    4    0    0
    PD> [6,]    0    0    0    0    5    0


Yes, indeed, *the* S- / R- answer.

and note that this also works with sparse matrices:

> library(Matrix)
> m <- Matrix(0,6,6)
> diag(m[-1,]) <- 1:5
> m
6 x 6 sparse Matrix of class "dgCMatrix"
                
[1,] . . . . . .
[2,] 1 . . . . .
[3,] . 2 . . . .
[4,] . . 3 . . .
[5,] . . . 4 . .
[6,] . . . . 5 .

- - - - - - - - - - - 

Merry Christmas' Eve  to all R-helpers!

Martin Maechler


From alex.park1 at ntlworld.com  Mon Dec 24 14:30:44 2007
From: alex.park1 at ntlworld.com (Alex Park)
Date: Mon, 24 Dec 2007 13:30:44 -0000
Subject: [R] Help with read.zoo and transform
Message-ID: <000001c84631$30d81af0$9dc80d56@Paula>

R 

I get a daily feed of data over the internet that I keep in various .csv
files. 

I have built a function that reads that data into R for me:

getMarketData<-function(market)
{
	library(zoo)
	pathname<- "C:/DATA/"
	files<-c("AN_REV.csv","AX_REV.csv","BN_REV.csv")
	markets<-c("AUS","DAX","GBP")
	df<-read.zoo(paste(pathname,files[match(market,markets)],sep=""),
index.column=1, format="%m/%d/%Y", header=F, sep=",")
	df
}

This works fine and returns the dataset as planned:

              V2    V3    V4    V5   V6   V7
1990-01-02 54.89 54.99 54.61 54.61  125 2576
1990-01-03 54.48 54.62 54.29 54.33 1495 3232
1990-01-04 54.67 55.20 54.59 55.08  932 3145
1990-01-05 54.64 54.87 54.57 54.57  272 2567
1990-01-08 54.87 54.89 54.68 54.79  177 2456
1990-01-09 54.87 54.96 54.80 54.88  106 2403

Also if I type the following:

mode(df[1,1])

>"numeric"

That is, my data is treated as numeric (which is as it should be).

Here is where I get a problem. I'd like to add some more columns on to the
end of the dataset to represent the year, quarter, month, and day.

If I try the following within the function it won't work:

df<-transform(df, "Year"=format(time(df), "%Y"))

Curiously, if I do exactly the same line by line in R (i.e. without
specifying a function) then it works fine. Is there any reason why I cannot
use transform in a function?

I also tried a different approach as shown below i.e within my original
function I added the following lines after I had created df:

df[,8:11]<-0
df[,7]<-format(time(df), format="%Y")
df[,8]<-quarters(time(df))
df[,9]<-months(time(df))
df[,10]<-weekdays(time(df))

This worked fine however it changed all my data from numeric to character:

           Open  High  Low   Close Volume OI   Year Quarter Month   Day

1990-01-02 54.89 54.99 54.61 54.61 125    2576 1990 Q1      January Tuesday

1990-01-03 54.48 54.62 54.29 54.33 1495   3232 1990 Q1      January Wedesday
1990-01-04 54.67 55.2  54.59 55.08 932    3145 1990 Q1      January Thursday

1990-01-05 54.64 54.87 54.57 54.57 272    2567 1990 Q1      January Friday

1990-01-08 54.87 54.89 54.68 54.79 177    2456 1990 Q1      January Monday

1990-01-09 54.87 54.96 54.8  54.88 106    2403 1990 Q1      January Tuesday


mode(df[1,1])

>"character"

Why does my numeric data get changed into character?

Can anybody see a simple way to add the data I require whilst retaining the
"numeric" format?

Regards


Alex Park


From ggrothendieck at gmail.com  Mon Dec 24 14:58:09 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Dec 2007 08:58:09 -0500
Subject: [R] Help with read.zoo and transform
In-Reply-To: <000001c84631$30d81af0$9dc80d56@Paula>
References: <000001c84631$30d81af0$9dc80d56@Paula>
Message-ID: <971536df0712240558j21a0c70ep44441d368c79c6ee@mail.gmail.com>

zoo objects are intended to represent time series and are
based on vectors and matrices, like ts objects, not data frames.
See ?zoo

Create your new columns as numeric variables:

> library(zoo)
> library(chron) # need chron 2.3-16 for month.day.year
> z <- zoo(matrix(1:24, 6, 4), Sys.Date() + seq(0, length = 6, by = 32)) # test data

> # add year, month and day as columns to z
> with(month.day.year(time(z)), cbind(z, year, month, day, quarter = (month - 1) %/% 3 + 1, dow = as.numeric(format(time(z), "%w"))))
           z.1 z.2 z.3 z.4 year month day quarter dow
2007-12-24   1   7  13  19 2007    12  24       4   1
2008-01-25   2   8  14  20 2008     1  25       1   5
2008-02-26   3   9  15  21 2008     2  26       1   2
2008-03-29   4  10  16  22 2008     3  29       1   6
2008-04-30   5  11  17  23 2008     4  30       2   3
2008-06-01   6  12  18  24 2008     6   1       2   0





On Dec 24, 2007 8:30 AM, Alex Park <alex.park1 at ntlworld.com> wrote:
> R
>
> I get a daily feed of data over the internet that I keep in various .csv
> files.
>
> I have built a function that reads that data into R for me:
>
> getMarketData<-function(market)
> {
>        library(zoo)
>        pathname<- "C:/DATA/"
>        files<-c("AN_REV.csv","AX_REV.csv","BN_REV.csv")
>        markets<-c("AUS","DAX","GBP")
>        df<-read.zoo(paste(pathname,files[match(market,markets)],sep=""),
> index.column=1, format="%m/%d/%Y", header=F, sep=",")
>        df
> }
>
> This works fine and returns the dataset as planned:
>
>              V2    V3    V4    V5   V6   V7
> 1990-01-02 54.89 54.99 54.61 54.61  125 2576
> 1990-01-03 54.48 54.62 54.29 54.33 1495 3232
> 1990-01-04 54.67 55.20 54.59 55.08  932 3145
> 1990-01-05 54.64 54.87 54.57 54.57  272 2567
> 1990-01-08 54.87 54.89 54.68 54.79  177 2456
> 1990-01-09 54.87 54.96 54.80 54.88  106 2403
>
> Also if I type the following:
>
> mode(df[1,1])
>
> >"numeric"
>
> That is, my data is treated as numeric (which is as it should be).
>
> Here is where I get a problem. I'd like to add some more columns on to the
> end of the dataset to represent the year, quarter, month, and day.
>
> If I try the following within the function it won't work:
>
> df<-transform(df, "Year"=format(time(df), "%Y"))
>
> Curiously, if I do exactly the same line by line in R (i.e. without
> specifying a function) then it works fine. Is there any reason why I cannot
> use transform in a function?
>
> I also tried a different approach as shown below i.e within my original
> function I added the following lines after I had created df:
>
> df[,8:11]<-0
> df[,7]<-format(time(df), format="%Y")
> df[,8]<-quarters(time(df))
> df[,9]<-months(time(df))
> df[,10]<-weekdays(time(df))
>
> This worked fine however it changed all my data from numeric to character:
>
>           Open  High  Low   Close Volume OI   Year Quarter Month   Day
>
> 1990-01-02 54.89 54.99 54.61 54.61 125    2576 1990 Q1      January Tuesday
>
> 1990-01-03 54.48 54.62 54.29 54.33 1495   3232 1990 Q1      January Wedesday
> 1990-01-04 54.67 55.2  54.59 55.08 932    3145 1990 Q1      January Thursday
>
> 1990-01-05 54.64 54.87 54.57 54.57 272    2567 1990 Q1      January Friday
>
> 1990-01-08 54.87 54.89 54.68 54.79 177    2456 1990 Q1      January Monday
>
> 1990-01-09 54.87 54.96 54.8  54.88 106    2403 1990 Q1      January Tuesday
>
>
> mode(df[1,1])
>
> >"character"
>
> Why does my numeric data get changed into character?
>
> Can anybody see a simple way to add the data I require whilst retaining the
> "numeric" format?
>
> Regards
>
>
> Alex Park
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maechler at stat.math.ethz.ch  Mon Dec 24 15:02:17 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 24 Dec 2007 15:02:17 +0100
Subject: [R] using solve.qp without a quadratic term
In-Reply-To: <1808201.508981198287499445.JavaMail.root@vms125.mailsrvcs.net>
References: <1808201.508981198287499445.JavaMail.root@vms125.mailsrvcs.net>
Message-ID: <18287.48105.379507.313522@ada-stat.math.ethz.ch>

>>>>> "ML" == Mark Leeds <markleeds at verizon.net>
>>>>>     on Fri, 21 Dec 2007 19:38:19 -0600 (CST) writes:

    ML> I was playing around with a simple example using solve.qp ( function is in the quadprog package ) and the code is below. ( I'm not even sure there if there is a reasonable solution because I made the problem up  ). 
    ML> But, when I try to use solve.QP to solve it, I get the error that D in the quadratic function is not positive
    ML> definite. This is because Dmat is zero
    ML> because I don't have a quadratic term in my
    ML> objective function. So, I was wondering if
    ML> it was possible to use solve.QP when there isn't
    ML> a quadratic term in the objective function.  

    ML> I imagine that there are other functions in R that can be used but I would like to use solve.QP because, in my real problem,
    ML> I will have a lot of fairly complex constraints
    ML> and solve.QP provides a very nice way for implementing
    ML> them. Maybe there is another linear solver that allows you to implement hundreds of constraints just solve.QP that I am unaware of ? Thanks for any suggestions.

    ML> # IN THE CODE BELOW, WE MINIMIZE 

    ML> # -3*b1 + 4*b2 + 6*b3

    ML> # SUBJECT TO

    ML> # b1 + b2 + b3 >=0
    ML> # -(b1  b2 + b3) >= 0 
    ML> # IE : b1 + b2 + b3 = 0.

So you want to solve a *linear* programming problem,
not a quadratic.  Linear is typically considerably easier.

The recommended (and hence always installed) package 'boot'
has function simplex() to do this
and I see two other CRAN packages 'linprog' and 'lpSolve' also
for the same problem;  since ?simplex says that it may not be
very efficient for large problems, you would e.g. lpSolve
instead.

Regards,
Martin

    ML> Dmat <- matrix(0,3,3)          # QUADRATIC TERM
    ML> dvec <- c(-3,4,6)              # LINEAR TERM
    ML> Amat <- matrix(c(1,-1,0,1,-1,0,1,-1,0),3,3)

    ML> #print(Amat)

    ML> bvec = c(0,0,0)                  # THIRD ZERO IS SAME AS NO CONSTRAINT

    ML> result <- solve.QP(Dmat, dvec, Amat)

    ML> ______________________________________________
    ML> R-help at r-project.org mailing list
    ML> https://stat.ethz.ch/mailman/listinfo/r-help
    ML> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    ML> and provide commented, minimal, self-contained, reproducible code.


From pieterprovoost at gmail.com  Mon Dec 24 15:04:09 2007
From: pieterprovoost at gmail.com (pieterprovoost at gmail.com)
Date: Mon, 24 Dec 2007 09:04:09 -0500
Subject: [R] curve fitting problem
Message-ID: <29494967.1198505049212.JavaMail.root@wombat.diezmil.com>

I'm trying to fit a function y=k*l^(m*x) to some data points, with  reasonable starting value estimates (I think). I keep getting "singular matrix 'a' in solve".

This is the code:

ox <- c(-600,-300,-200,1,100,200)
ir <- c(1,2.5,4,9,14,20)
model <- nls(ir ~ k*l^(m*ox),start=list(k=10,l=3,m=0.004),algorithm="plinear")
summary(model)
plot(ox,ir)
testox <- seq(-600,200,length=100)
k <- 10
l <- 3
m <- 0.004
testir <- k*l^(m*testox)
lines(testox,testir)

Any thoughts?
Thanks!

--
This message was sent on behalf of pieterprovoost at gmail.com at openSubscriber.com
http://www.opensubscriber.com/messages/r-help at stat.math.ethz.ch/topic.html


From ggrothendieck at gmail.com  Mon Dec 24 16:02:48 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Dec 2007 10:02:48 -0500
Subject: [R] curve fitting problem
In-Reply-To: <29494967.1198505049212.JavaMail.root@wombat.diezmil.com>
References: <29494967.1198505049212.JavaMail.root@wombat.diezmil.com>
Message-ID: <971536df0712240702u6b2c7bfcs28fb3ad665b70a91@mail.gmail.com>

Your model is singular.  Varying m and log(l) have the same
effect: log(ir) = log(k) + m * log(l) * ox

Also with plinear you don't specify the linear coefficients but
rather an X matrix whose coefficients represent them:
If we use this model instead:

ir = k * exp(m * ox)

Then:

> mod0 <- lm(log(ir) ~ ox)
> mod0

Call:
lm(formula = log(ir) ~ ox)

Coefficients:
(Intercept)           ox
   2.199743     0.003835

> nls(ir ~ exp(m * ox), start = list(m = coef(mod0)[2]), algorithm = "plinear")
Nonlinear regression model
  model:  ir ~ exp(m * ox)
   data:  parent.frame()
       m     .lin
0.003991 9.091758
 residual sum-of-squares: 0.3551

Number of iterations to convergence: 3
Achieved convergence tolerance: 5.289e-07

On Dec 24, 2007 9:04 AM,  <pieterprovoost at gmail.com> wrote:
> I'm trying to fit a function y=k*l^(m*x) to some data points, with  reasonable starting value estimates (I think). I keep getting "singular matrix 'a' in solve".
>
> This is the code:
>
> ox <- c(-600,-300,-200,1,100,200)
> ir <- c(1,2.5,4,9,14,20)
> model <- nls(ir ~ k*l^(m*ox),start=list(k=10,l=3,m=0.004),algorithm="plinear")
> summary(model)
> plot(ox,ir)
> testox <- seq(-600,200,length=100)
> k <- 10
> l <- 3
> m <- 0.004
> testir <- k*l^(m*testox)
> lines(testox,testir)
>
> Any thoughts?
> Thanks!
>
> --
> This message was sent on behalf of pieterprovoost at gmail.com at openSubscriber.com
> http://www.opensubscriber.com/messages/r-help at stat.math.ethz.ch/topic.html
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From affysnp at gmail.com  Mon Dec 24 16:57:36 2007
From: affysnp at gmail.com (affy snp)
Date: Mon, 24 Dec 2007 10:57:36 -0500
Subject: [R] How to remove some rows from a data.frame
In-Reply-To: <671623.2094.qm@web32203.mail.mud.yahoo.com>
References: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>
	<671623.2094.qm@web32203.mail.mud.yahoo.com>
Message-ID: <5032046e0712240757s63f00414wc163a48a9325d2d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071224/85f74ae8/attachment.pl 

From wwwhsd at gmail.com  Mon Dec 24 17:27:10 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 24 Dec 2007 14:27:10 -0200
Subject: [R] How to remove some rows from a data.frame
In-Reply-To: <5032046e0712240757s63f00414wc163a48a9325d2d@mail.gmail.com>
References: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>
	<671623.2094.qm@web32203.mail.mud.yahoo.com>
	<5032046e0712240757s63f00414wc163a48a9325d2d@mail.gmail.com>
Message-ID: <da79af330712240827m71d1037alfe4f19c00b4dff7e@mail.gmail.com>

Try this:

f <- function(x)
{
cbind.data.frame(chr=unique(x$chr),
                 Start=min(x$pos),
                 End=max(x$pos),
                 Rows=nrow(x),
                 Pattern=paste("(", x$s1, x$s2, ")")
                 )
}

do.call("rbind", lapply(lapply(split(df, paste(df$s1, df$s2)), f), unique))


On 24/12/2007, affy snp <affysnp at gmail.com> wrote:
> Thanks Moshe! I apologize for not being so clear about the
> second part. Again, below is how the data looks like. The
> pattern for columns s1 and s2 will be:
>
> (-1 -1)  (-1 0)  (-1 1)  (0 -1)   (0 0)   (0 1)  (1 -1)   (1 0)   (1 1)
>  104    131     57      631     305    668    33       15     107
>
> There are 9 patterns, in other words, 9 combinations of -1,1, 0
> given in the parenthesis. The occurring numbers are underneath.
> What I wish to have is that: scan the data from the begin,
> if any consecutive rows are of the same pattern (one of the 9
> combinations in the above), we will 'memorize' the following information:
>
> the number in 'chr' column, the number in 'pos' column for the first
> row in the consecutive rows, the number in 'pos' column for the
> last row in the consecutive rows, how many rows of the consecutive
> rows, the corresponding pattern for them.
>
> I forgot to reinforce one requirement before for definition of
> the consecutive rows, which is that they are in the consecutive
> orders and are of the same number of 'chr'.
>
> Just to illustrate this, an example could be that, based on the data:
>
> BAC                 chr    pos          s1   s2
> RP11-80G24    1    77465510    0    0
> RP11-198H14    1    78696291    -1    0
> RP11-267M21    1    79681704    -1    0
> RP11-89A19      1    80950808    -1    0
> RP11-6B16        1    82255496    -1    0
> RP11-210E16    2    228801510    -1   0
>
> even though row 2---6 are of the same pattern, which is -1 0
> and are in the consecutive order, but row 6 is of different number
> of 'chr' than other rows. Therefore, we will not count row 6 and
> end up with:
> chr    Start           End        #of_rows          pattern
> 1    78696291    82255496   4                    (-1 0)
>
> Hope this is clear. Thank you once again and Merry X'mas!
>
> Best,
>     Allen
>
>
>
>
>
> > BAC                 chr    pos          s1   s2
> > RP11-80G24    1    77465510    -1    0
> > RP11-198H14    1    78696291    -1    0
> > RP11-267M21    1    79681704    -1    0
> > RP11-89A19      1    80950808    -1    0
> > RP11-6B16        1    82255496    -1    0
> > RP11-210E16    1    228801510    0    -1
> > RP11-155C15    1    230957584    0    -1
> > RP11-210F8      1    237932418    0    -1
> > RP11-263L17     2    65724492    0    1
> > RP11-340F16     2    65879898    0    1
> > RP11-68A1        2    67718674    0    0
> > RP11-474G23    2    68318411    0    0
> > RP11-218N6      2    68454651    0    0
> > CTD-2003M22    2    68567494    0    0
> > .....
> >
>
> On Dec 24, 2007 3:54 AM, Moshe Olshansky <m_olshansky at yahoo.com> wrote:
>
> > To answer your firs question try
> >
> > M[-which( M$s1 == 0 & M$s2 == 0),]
> >
> > For the second question, you must start with the more
> > precise definition of the grouping criterion.
> >
> > --- affy snp <affysnp at gmail.com> wrote:
> >
> > > Hello list,
> > >
> > > I have a data frame M like:
> > >
> > > BAC                 chr    pos          s1   s2
> > > RP11-80G24    1    77465510    -1    0
> > > RP11-198H14    1    78696291    -1    0
> > > RP11-267M21    1    79681704    -1    0
> > > RP11-89A19      1    80950808    -1    0
> > > RP11-6B16        1    82255496    -1    0
> > > RP11-210E16    1    228801510    0    -1
> > > RP11-155C15    1    230957584    0    -1
> > > RP11-210F8      1    237932418    0    -1
> > > RP11-263L17     2    65724492    0    1
> > > RP11-340F16     2    65879898    0    1
> > > RP11-68A1        2    67718674    0    0
> > > RP11-474G23    2    68318411    0    0
> > > RP11-218N6      2    68454651    0    0
> > > CTD-2003M22    2    68567494    0    0
> > > .....
> > >
> > > how to remove those rows which have 0 for both of
> > > columns s1,s2?
> > > sth like M[!M$21=0&!M$s2=0]?
> > >
> > > Moreover, I want to get a list which could find a
> > > subset of rows which have
> > > the same pattern of data. For example, the first 8
> > > rows in M can be
> > > clustered
> > > into 2 groups (represented below in 2 rows) and
> > > shown as:
> > >
> > > chr             Start       End             # of
> > > rows     Pattern
> > > 1             77465510   82255496       5
> > >   (-1 0)
> > > 1            228801510  237932418     3
> > > (0 -1)
> > >
> > > Can anybody help me out of this? Thank you very much
> > > and happy holiday!
> > >
> > > Best,
> > >     Allen
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained,
> > > reproducible code.
> > >
> >
> >
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From affysnp at gmail.com  Mon Dec 24 17:30:04 2007
From: affysnp at gmail.com (affy snp)
Date: Mon, 24 Dec 2007 11:30:04 -0500
Subject: [R] How to remove some rows from a data.frame
In-Reply-To: <da79af330712240827m71d1037alfe4f19c00b4dff7e@mail.gmail.com>
References: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>
	<671623.2094.qm@web32203.mail.mud.yahoo.com>
	<5032046e0712240757s63f00414wc163a48a9325d2d@mail.gmail.com>
	<da79af330712240827m71d1037alfe4f19c00b4dff7e@mail.gmail.com>
Message-ID: <5032046e0712240830w2ea6349ar4f8da0eab6133713@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071224/94e1f119/attachment.pl 

From wwwhsd at gmail.com  Mon Dec 24 17:32:09 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 24 Dec 2007 14:32:09 -0200
Subject: [R] How to remove some rows from a data.frame
In-Reply-To: <5032046e0712240830w2ea6349ar4f8da0eab6133713@mail.gmail.com>
References: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>
	<671623.2094.qm@web32203.mail.mud.yahoo.com>
	<5032046e0712240757s63f00414wc163a48a9325d2d@mail.gmail.com>
	<da79af330712240827m71d1037alfe4f19c00b4dff7e@mail.gmail.com>
	<5032046e0712240830w2ea6349ar4f8da0eab6133713@mail.gmail.com>
Message-ID: <da79af330712240832g2d8d18ccq8412ed03d8ec343f@mail.gmail.com>

My object df is this:

 df <- structure(list(BAC = structure(c(13L, 3L, 8L, 14L, 12L, 4L, 2L,
5L, 7L, 9L, 11L, 10L, 6L, 1L), .Label = c("CTD-2003M22", "RP11-155C15",
"RP11-198H14", "RP11-210E16", "RP11-210F8", "RP11-218N6", "RP11-263L17",
"RP11-267M21", "RP11-340F16", "RP11-474G23", "RP11-68A1", "RP11-6B16",
"RP11-80G24", "RP11-89A19"), class = "factor"), chr = c(1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L), pos = c(77465510L,
78696291L, 79681704L, 80950808L, 82255496L, 228801510L, 230957584L,
237932418L, 65724492L, 65879898L, 67718674L, 68318411L, 68454651L,
68567494L), s1 = c(-1L, -1L, -1L, -1L, -1L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L), s2 = c(0L, 0L, 0L, 0L, 0L, -1L, -1L, -1L, 1L,
1L, 0L, 0L, 0L, 0L), Count = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1)), .Names = c("BAC", "chr", "pos", "s1", "s2", "Count"
), row.names = c(NA, -14L), class = "data.frame")

works for me

On 24/12/2007, affy snp <affysnp at gmail.com> wrote:
> Dear Henrique,
>
> Thanks a lot for the help! I got this:
>
> > f <- function(x)
> + {
> + cbind.data.frame(chr=unique(x$chr),
> +                 Start=min(x$pos),
> +                 End=max(x$pos),
> +                 Rows=nrow(x),
> +                 Pattern=paste("(", x$s1, x$s2, ")")
> +                 )
> + }
> >
> > do.call("rbind", lapply(lapply(split(df, paste(df$s1, df$s2)), f),
> unique))
> Error in split.default(df, paste(df$s1, df$s2)) :
>   first argument must be a vector
>
>
> Any clue?
>
> Best,
>      Allen
>
>
> On Dec 24, 2007 11:27 AM, Henrique Dallazuanna < wwwhsd at gmail.com> wrote:
> > Try this:
> >
> > f <- function(x)
> > {
> > cbind.data.frame (chr=unique(x$chr),
> >                 Start=min(x$pos),
> >                 End=max(x$pos),
> >                 Rows=nrow(x),
> >                 Pattern=paste("(", x$s1, x$s2, ")")
> >                 )
> > }
> >
> > do.call("rbind", lapply(lapply(split(df, paste(df$s1, df$s2)), f),
> unique))
> >
> >
> >
> >
> >
> > On 24/12/2007, affy snp <affysnp at gmail.com > wrote:
> > > Thanks Moshe! I apologize for not being so clear about the
> > > second part. Again, below is how the data looks like. The
> > > pattern for columns s1 and s2 will be:
> > >
> > > (-1 -1)  (-1 0)  (-1 1)  (0 -1)   (0 0)   (0 1)  (1 -1)   (1 0)   (1 1)
> > >  104    131     57      631     305    668    33       15     107
> > >
> > > There are 9 patterns, in other words, 9 combinations of -1,1, 0
> > > given in the parenthesis. The occurring numbers are underneath.
> > > What I wish to have is that: scan the data from the begin,
> > > if any consecutive rows are of the same pattern (one of the 9
> > > combinations in the above), we will 'memorize' the following
> information:
> > >
> > > the number in 'chr' column, the number in 'pos' column for the first
> > > row in the consecutive rows, the number in 'pos' column for the
> > > last row in the consecutive rows, how many rows of the consecutive
> > > rows, the corresponding pattern for them.
> > >
> > > I forgot to reinforce one requirement before for definition of
> > > the consecutive rows, which is that they are in the consecutive
> > > orders and are of the same number of 'chr'.
> > >
> > > Just to illustrate this, an example could be that, based on the data:
> > >
> > > BAC                 chr    pos          s1   s2
> > > RP11-80G24    1    77465510    0    0
> > > RP11-198H14    1    78696291    -1    0
> > > RP11-267M21    1    79681704    -1    0
> > > RP11-89A19      1    80950808    -1    0
> > > RP11-6B16        1    82255496    -1    0
> > > RP11-210E16    2    228801510    -1   0
> > >
> > > even though row 2---6 are of the same pattern, which is -1 0
> > > and are in the consecutive order, but row 6 is of different number
> > > of 'chr' than other rows. Therefore, we will not count row 6 and
> > > end up with:
> > > chr    Start           End        #of_rows          pattern
> > > 1    78696291    82255496   4                    (-1 0)
> > >
> > > Hope this is clear. Thank you once again and Merry X'mas!
> > >
> > > Best,
> > >     Allen
> > >
> > >
> > >
> > >
> > >
> > > > BAC                 chr    pos          s1   s2
> > > > RP11-80G24    1    77465510    -1    0
> > > > RP11-198H14    1    78696291    -1    0
> > > > RP11-267M21    1    79681704    -1    0
> > > > RP11-89A19      1    80950808    -1    0
> > > > RP11-6B16        1    82255496    -1    0
> > > > RP11-210E16    1    228801510    0    -1
> > > > RP11-155C15    1    230957584    0    -1
> > > > RP11-210F8      1    237932418    0    -1
> > > > RP11-263L17     2    65724492    0    1
> > > > RP11-340F16     2    65879898    0    1
> > > > RP11-68A1        2    67718674    0    0
> > > > RP11-474G23    2    68318411    0    0
> > > > RP11-218N6      2    68454651    0    0
> > > > CTD-2003M22    2    68567494    0    0
> > > > .....
> > > >
> > >
> > > On Dec 24, 2007 3:54 AM, Moshe Olshansky <m_olshansky at yahoo.com> wrote:
> > >
> > > > To answer your firs question try
> > > >
> > > > M[-which( M$s1 == 0 & M$s2 == 0),]
> > > >
> > > > For the second question, you must start with the more
> > > > precise definition of the grouping criterion.
> > > >
> > > > --- affy snp <affysnp at gmail.com> wrote:
> > > >
> > > > > Hello list,
> > > > >
> > > > > I have a data frame M like:
> > > > >
> > > > > BAC                 chr    pos          s1   s2
> > > > > RP11-80G24    1    77465510    -1    0
> > > > > RP11-198H14    1    78696291    -1    0
> > > > > RP11-267M21    1    79681704    -1    0
> > > > > RP11-89A19      1    80950808    -1    0
> > > > > RP11-6B16        1    82255496    -1    0
> > > > > RP11-210E16    1    228801510    0    -1
> > > > > RP11-155C15    1    230957584    0    -1
> > > > > RP11-210F8      1    237932418    0    -1
> > > > > RP11-263L17     2    65724492    0    1
> > > > > RP11-340F16     2    65879898    0    1
> > > > > RP11-68A1        2    67718674    0    0
> > > > > RP11-474G23    2    68318411    0    0
> > > > > RP11-218N6      2    68454651    0    0
> > > > > CTD-2003M22    2    68567494    0    0
> > > > > .....
> > > > >
> > > > > how to remove those rows which have 0 for both of
> > > > > columns s1,s2?
> > > > > sth like M[!M$21=0&!M$s2=0]?
> > > > >
> > > > > Moreover, I want to get a list which could find a
> > > > > subset of rows which have
> > > > > the same pattern of data. For example, the first 8
> > > > > rows in M can be
> > > > > clustered
> > > > > into 2 groups (represented below in 2 rows) and
> > > > > shown as:
> > > > >
> > > > > chr             Start       End             # of
> > > > > rows     Pattern
> > > > > 1             77465510   82255496       5
> > > > >   (-1 0)
> > > > > 1            228801510  237932418     3
> > > > > (0 -1)
> > > > >
> > > > > Can anybody help me out of this? Thank you very much
> > > > > and happy holiday!
> > > > >
> > > > > Best,
> > > > >     Allen
> > > > >
> > > > >       [[alternative HTML version deleted]]
> > > > >
> > > > > ______________________________________________
> > > > > R-help at r-project.org mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > > http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained,
> > > > > reproducible code.
> > > > >
> > > >
> > > >
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> > --
> > Henrique Dallazuanna
> > Curitiba-Paran?-Brasil
> > 25? 25' 40" S 49? 16' 22" O
> >
>
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From affysnp at gmail.com  Mon Dec 24 17:38:02 2007
From: affysnp at gmail.com (affy snp)
Date: Mon, 24 Dec 2007 11:38:02 -0500
Subject: [R] How to remove some rows from a data.frame
In-Reply-To: <da79af330712240832g2d8d18ccq8412ed03d8ec343f@mail.gmail.com>
References: <5032046e0712231328g66a6231x809ac150d2eb610c@mail.gmail.com>
	<671623.2094.qm@web32203.mail.mud.yahoo.com>
	<5032046e0712240757s63f00414wc163a48a9325d2d@mail.gmail.com>
	<da79af330712240827m71d1037alfe4f19c00b4dff7e@mail.gmail.com>
	<5032046e0712240830w2ea6349ar4f8da0eab6133713@mail.gmail.com>
	<da79af330712240832g2d8d18ccq8412ed03d8ec343f@mail.gmail.com>
Message-ID: <5032046e0712240838r18e084f3j3eb6ea6e848d290d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071224/4964b78c/attachment.pl 

From mazatlanmexico at yahoo.com  Mon Dec 24 19:01:17 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Mon, 24 Dec 2007 10:01:17 -0800 (PST)
Subject: [R] geom_hline
Message-ID: <642070.60165.qm@web56603.mail.re3.yahoo.com>

 Hi all:
I am trying to draw a horizontal line along the zero
"Y axis" value but since zero isn't showing therefore
the line is not drawn. If I set my intercept to 15
then it'll work but I dont want to hardcode it because
the Y axis parameters could be different for another
variable. I would like to alway set my hline along the
zero as default. Thanks

p <- ggplot(mtcars, aes(x = wt, y=mpg)) + geom_point()
p + geom_hline(intercept=0)

Felipe D. Carrillo
  Fishery Biologist
  US Fish & Wildlife Service
  California, USA



      ____________________________________________________________________________________
Looking for last minute shopping deals?


From ohitsabid at yahoo.com  Mon Dec 24 19:08:39 2007
From: ohitsabid at yahoo.com (Syed Abid Hussaini)
Date: Mon, 24 Dec 2007 10:08:39 -0800 (PST)
Subject: [R] saving while loop values to one vector
In-Reply-To: <mailman.16.1198494003.12824.r-help@r-project.org>
Message-ID: <859368.45440.qm@web35615.mail.mud.yahoo.com>

Thanks Gabor!
   I never realized there could be a "diff" function. all i was trying to do was to calculate the
iterated differences in a vector. I feel so embarassed, but so happy for using R. 
by the way, i used 
"wave2 <- sort (wave2, decreasing=F)" to sort the values non-decreasingly because the code "i <-
findInterval((wave2[1]), wave2)" doesnt work without this sorting. 
But all that is past now since i found the diff function. 

Thanks again.

abid

> > From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
> CC: R-help at stat.math.ethz.ch
> To: "Syed Abid Hussaini" <ohitsabid at yahoo.com>
> Date: Mon, 24 Dec 2007 02:30:57 -0500
> Subject: Re: [R] saving while loop values to one vector
> 
> Its not clear what your code is intended to do but
> 
> - put set.seed(123) before your code so you can
> run it reproducibly.
> 
> - wave2 <- sort(abs(rnorm(100))) is the same as your first
> two lines
> 
> - your findInterval line always returns 1 so its probably not what you want
> 
> - i in i:x is confusing. Use j in i:x or even better use diff(wave2).
> 
> On Dec 24, 2007 1:54 AM, Syed Abid Hussaini <ohitsabid at yahoo.com> wrote:
> > I solved it myself by reading the help files which i should have done prior to my posting.
> >
> > Solution:
> >
> > wave2 <- abs(Re(rnorm(100)))
> > wave2 <- sort (wave2, decreasing=F)
> > hist(wave2)
> > x <- length (wave2)
> > i <- findInterval((wave2[1]), wave2)
> > i <- i+1
> > out <- numeric(x)
> > for (i in i:x)  {
> > out[i] <- (wave2 [i] - wave2[i-1])
> > }
> > out
> >
> > As you see i trimmed the code and alloted a new variable called out which i later save my
> values
> > to.
> >
> >
> >
> > --- Syed Abid Hussaini <ohitsabid at yahoo.com> wrote:
> >
> > > Hi all,
> > >   I am pretty new to R and even new to programming in general. Right now i get only one
> value
> > > for
> > > j below (print(j)), how do i save all j values to one vector or matrix? Sorry for this very
> > > basic
> > > question. thanks in advance.
> > >
> > > wave2 <- abs(Re(rnorm(100)))
> > > i <- findInterval((wave2[1]), wave2)
> > > i <- i+1
> > >
> > > while (i < length(wave2))  {
> > > j <- (wave2 [i] - wave2[i-1])
> > > print(j)
> > > i<-i+1
> > > }
> > >
> > > abid
> > >
> > >
> > >



      ____________________________________________________________________________________
Looking for last minute shopping deals?


From tkobayas at indiana.edu  Mon Dec 24 19:52:11 2007
From: tkobayas at indiana.edu (Takatsugu Kobayashi)
Date: Mon, 24 Dec 2007 13:52:11 -0500
Subject: [R] R Compilation error on Ubuntu
Message-ID: <1198522331.13205.8.camel@2WIRE387.gateway.2wire.net>

Hi

I bought a new laptop HP dv9500 just a week ago and installed a Ubuntu
gutsy ribbon on this laptop. I wanted to install Fedora but there are
more threads on Ubuntu, so I decided to install Ubuntu. After hours of
struggle in configuring x server/graphic card stuff, I installed R for
Gutsy ribbon using "sudo apt-get install g77 r-core'. 

Now when I tried to install 'sp' package I got this error message: 

Warning in install.packages("sp") : argument 'lib' is missing: using
'/usr/local/lib/R/site-library'
--- Please select a CRAN mirror for use in this session ---
Loading Tcl/Tk interface ... done
trying URL 'http://cran.mtu.edu/src/contrib/sp_0.9-17.tar.gz'
Content type 'application/x-gzip' length 359194 bytes
opened URL
==================================================
downloaded 350Kb

* Installing *source* package 'sp' ...
** libs
gcc-4.2 -std=gnu99 -I/usr/share/R/include -I/usr/share/R/include
-fpic  -g -O2 -c gcdist.c -o gcdist.o
/bin/bash: gcc-4.2: command not found
make: *** [gcdist.o] Error 127
ERROR: compilation failed for package 'sp'
** Removing '/usr/local/lib/R/site-library/sp'

The downloaded packages are in
        /tmp/RtmpfOk3H7/downloaded_packages
Warning message:
installation of package 'sp' had non-zero exit status in:
install.packages("sp") 

Then, I removed this gutsy ribbon version of R and attempted to install
fresh R-2.6.1.tar.gz. When I tried to build it, I got this error
message:

configure: error: --with-readline=yes (default) and headers/libs are not
available

I installed xserver-xorg-dev ok. 

I am using Fedora 6 on my desktop and have never seen these above error
messages. Should I install a Fedora and R on it? Or should I just create
a link using 'ln -s'? 

I appreciate your help.

Have a festive holiday!

taka


From ggrothendieck at gmail.com  Mon Dec 24 19:57:54 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Dec 2007 13:57:54 -0500
Subject: [R] saving while loop values to one vector
In-Reply-To: <859368.45440.qm@web35615.mail.mud.yahoo.com>
References: <mailman.16.1198494003.12824.r-help@r-project.org>
	<859368.45440.qm@web35615.mail.mud.yahoo.com>
Message-ID: <971536df0712241057q1ae7aa2egcedaff7b1b343270@mail.gmail.com>

On Dec 24, 2007 1:08 PM, Syed Abid Hussaini <ohitsabid at yahoo.com> wrote:
> Thanks Gabor!
>   I never realized there could be a "diff" function. all i was trying to do was to calculate the
> iterated differences in a vector. I feel so embarassed, but so happy for using R.
> by the way, i used
> "wave2 <- sort (wave2, decreasing=F)" to sort the values non-decreasingly because the code "i <-

That is the default for sort.   Its not necessary to specify defaults.
 Also its better practice to
write out F as FALSE since F can be a variable in R but FALSE cannot be.

> findInterval((wave2[1]), wave2)" doesnt work without this sorting.
> But all that is past now since i found the diff function.
>
> Thanks again.
>
> abid
>
> > > From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
> > CC: R-help at stat.math.ethz.ch
> > To: "Syed Abid Hussaini" <ohitsabid at yahoo.com>
> > Date: Mon, 24 Dec 2007 02:30:57 -0500
> > Subject: Re: [R] saving while loop values to one vector
>
> >
> > Its not clear what your code is intended to do but
> >
> > - put set.seed(123) before your code so you can
> > run it reproducibly.
> >
> > - wave2 <- sort(abs(rnorm(100))) is the same as your first
> > two lines
> >
> > - your findInterval line always returns 1 so its probably not what you want
> >
> > - i in i:x is confusing. Use j in i:x or even better use diff(wave2).
> >
> > On Dec 24, 2007 1:54 AM, Syed Abid Hussaini <ohitsabid at yahoo.com> wrote:
> > > I solved it myself by reading the help files which i should have done prior to my posting.
> > >
> > > Solution:
> > >
> > > wave2 <- abs(Re(rnorm(100)))
> > > wave2 <- sort (wave2, decreasing=F)
> > > hist(wave2)
> > > x <- length (wave2)
> > > i <- findInterval((wave2[1]), wave2)
> > > i <- i+1
> > > out <- numeric(x)
> > > for (i in i:x)  {
> > > out[i] <- (wave2 [i] - wave2[i-1])
> > > }
> > > out
> > >
> > > As you see i trimmed the code and alloted a new variable called out which i later save my
> > values
> > > to.
> > >
> > >
> > >
> > > --- Syed Abid Hussaini <ohitsabid at yahoo.com> wrote:
> > >
> > > > Hi all,
> > > >   I am pretty new to R and even new to programming in general. Right now i get only one
> > value
> > > > for
> > > > j below (print(j)), how do i save all j values to one vector or matrix? Sorry for this very
> > > > basic
> > > > question. thanks in advance.
> > > >
> > > > wave2 <- abs(Re(rnorm(100)))
> > > > i <- findInterval((wave2[1]), wave2)
> > > > i <- i+1
> > > >
> > > > while (i < length(wave2))  {
> > > > j <- (wave2 [i] - wave2[i-1])
> > > > print(j)
> > > > i<-i+1
> > > > }
> > > >
> > > > abid
> > > >
> > > >
> > > >
>
>
>
>      ____________________________________________________________________________________
> Looking for last minute shopping deals?
> Find them fast with Yahoo! Search.  http://tools.search.yahoo.com/newsearch/category.php?category=shopping
>


From young.stat at gmail.com  Mon Dec 24 20:43:22 2007
From: young.stat at gmail.com (Young Cho)
Date: Mon, 24 Dec 2007 11:43:22 -0800
Subject: [R] mgarch
Message-ID: <b44da9db0712241143q1d609dfcye8ef701efb951327@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071224/b0369ce9/attachment.pl 

From pgilbert at bank-banque-canada.ca  Mon Dec 24 21:22:41 2007
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 24 Dec 2007 15:22:41 -0500
Subject: [R] R Compilation error on Ubuntu
In-Reply-To: <1198522331.13205.8.camel@2WIRE387.gateway.2wire.net>
References: <1198522331.13205.8.camel@2WIRE387.gateway.2wire.net>
Message-ID: <47701511.9050602@bank-banque-canada.ca>

Taka

You might want to try R-SIG-Debian for this kind of question. Here are 
instructions from Dirk in response to my similar question a few days ago.

HTH,
Paul
_________________

On 12 December 2007 at 18:17, Paul Gilbert wrote:
| I'm trying to build R from source on Ubuntu Gutsy Gibbon. I've done
| apt-get install r-base-dev and  apt-get libX11-dev,  but R configure is

You're close: 'r-base-dev' gives you (about) everything to run
'install.package()' once you have R; libX11-dev ought to give you everything
but somehow doesn't so ...

| still complaining about X11 headers/libs are not available.  What else
| do I need?

...  use the 'apt-get build-dep $package' command, ie

	$ sudo apt-get build-dep r-base

which will pull in this list (as per the current Debian unstable package):

Build-Depends: gcc (>= 4:4.1.0), g++ (>= 4:4.1.0), gfortran (>= 
4:4.1.0), refblas3-dev | atlas3-base-dev, tcl8.4-dev, tk8.4-dev, bison, 
groff-base, libncurses5-dev, libreadline5-dev, debhelper (>= 5.0.0), 
texi2html, texinfo (>= 4.1-2), libbz2-dev, libpcre3-dev, tetex-bin, 
tetex-extra, xpdf-reader, zlib1g-dev, libpng12-dev, libjpeg62-dev, 
libx11-dev, libxt-dev, x-dev

So in this particular case, libxt-dev and x-dev were still missing.

'apt-get build-dep $foo' is really convenient. Try 'apt-get source $foo' as
well to get sources (if you have entries like

	deb-src http://ftp.us.debian.org/debian/ unstable main contrib non-free

I use that all the time to fetch Debian sources onto Ubuntu machines at
work.  It may at times conflict with what packages Ubuntu has, or what names
they used -- but for R and one or two dozen other Debian packages it has
always been useful for me on Ubuntu too.

Cheers, Dirk

Takatsugu Kobayashi wrote:
> Hi
> 
> I bought a new laptop HP dv9500 just a week ago and installed a Ubuntu
> gutsy ribbon on this laptop. I wanted to install Fedora but there are
> more threads on Ubuntu, so I decided to install Ubuntu. After hours of
> struggle in configuring x server/graphic card stuff, I installed R for
> Gutsy ribbon using "sudo apt-get install g77 r-core'. 
> 
> Now when I tried to install 'sp' package I got this error message: 
> 
> Warning in install.packages("sp") : argument 'lib' is missing: using
> '/usr/local/lib/R/site-library'
> --- Please select a CRAN mirror for use in this session ---
> Loading Tcl/Tk interface ... done
> trying URL 'http://cran.mtu.edu/src/contrib/sp_0.9-17.tar.gz'
> Content type 'application/x-gzip' length 359194 bytes
> opened URL
> ==================================================
> downloaded 350Kb
> 
> * Installing *source* package 'sp' ...
> ** libs
> gcc-4.2 -std=gnu99 -I/usr/share/R/include -I/usr/share/R/include
> -fpic  -g -O2 -c gcdist.c -o gcdist.o
> /bin/bash: gcc-4.2: command not found
> make: *** [gcdist.o] Error 127
> ERROR: compilation failed for package 'sp'
> ** Removing '/usr/local/lib/R/site-library/sp'
> 
> The downloaded packages are in
>         /tmp/RtmpfOk3H7/downloaded_packages
> Warning message:
> installation of package 'sp' had non-zero exit status in:
> install.packages("sp") 
> 
> Then, I removed this gutsy ribbon version of R and attempted to install
> fresh R-2.6.1.tar.gz. When I tried to build it, I got this error
> message:
> 
> configure: error: --with-readline=yes (default) and headers/libs are not
> available
> 
> I installed xserver-xorg-dev ok. 
> 
> I am using Fedora 6 on my desktop and have never seen these above error
> messages. Should I install a Fedora and R on it? Or should I just create
> a link using 'ln -s'? 
> 
> I appreciate your help.
> 
> Have a festive holiday!
> 
> taka
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential in...{{dropped:26}}


From Roger.Vallejo at ARS.USDA.GOV  Mon Dec 24 21:56:24 2007
From: Roger.Vallejo at ARS.USDA.GOV (Vallejo, Roger)
Date: Mon, 24 Dec 2007 15:56:24 -0500
Subject: [R] Affy Package
Message-ID: <F649B8DBD3DEAD4BB163AC4D9AC18B5FB13C88@MD-MAIL-01.ARSNET.ARS.USDA.GOV>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071224/d9051815/attachment.pl 

From ralf.pfeiffer at agrarimmobilien.info  Mon Dec 24 22:27:46 2007
From: ralf.pfeiffer at agrarimmobilien.info (Agrarimmobilien)
Date: Mon, 24 Dec 2007 22:27:46 +0100
Subject: [R] aggregation with two statistical functions - mean and variance
Message-ID: <003101c84673$d42a3b60$6702a8c0@PFEIFFER>

Hello, 

using the syntax 

aggregate(daten[,c(3,4)], list(A,B), mean)

I'm getting the following data.frame:


    A     B        C                    D       
1  35    1         6.16000          5
2  47    1        31.24333         20
3  54    1        26.81773         2
4   3     2        12.99000         7
5   4     2         6.49000          1

C and D are both means. But now I want to have: C is mean, D is variance. 

How can I do this by using/ changing the aggregation syntax?

Thanks! 

Macki


From ggrothendieck at gmail.com  Mon Dec 24 22:48:50 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Dec 2007 16:48:50 -0500
Subject: [R] aggregation with two statistical functions - mean and
	variance
In-Reply-To: <003101c84673$d42a3b60$6702a8c0@PFEIFFER>
References: <003101c84673$d42a3b60$6702a8c0@PFEIFFER>
Message-ID: <971536df0712241348l6c650a2ex15897a59d0855d07@mail.gmail.com>

See summaryBy in the doBy package.

On Dec 24, 2007 4:27 PM, Agrarimmobilien
<ralf.pfeiffer at agrarimmobilien.info> wrote:
> Hello,
>
> using the syntax
>
> aggregate(daten[,c(3,4)], list(A,B), mean)
>
> I'm getting the following data.frame:
>
>
>    A     B        C                    D
> 1  35    1         6.16000          5
> 2  47    1        31.24333         20
> 3  54    1        26.81773         2
> 4   3     2        12.99000         7
> 5   4     2         6.49000          1
>
> C and D are both means. But now I want to have: C is mean, D is variance.
>
> How can I do this by using/ changing the aggregation syntax?
>
> Thanks!
>
> Macki
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spndeep at googlemail.com  Mon Dec 24 23:53:11 2007
From: spndeep at googlemail.com (Sandeep Nikam)
Date: Mon, 24 Dec 2007 23:53:11 +0100
Subject: [R] ARIMA problem
Message-ID: <6cb8ca860712241453u3d2dad40yd6607d1ddc7f339e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071224/b411c837/attachment.pl 

From liuwensui at gmail.com  Tue Dec 25 00:31:44 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 24 Dec 2007 18:31:44 -0500
Subject: [R] ARIMA problem
In-Reply-To: <6cb8ca860712241453u3d2dad40yd6607d1ddc7f339e@mail.gmail.com>
References: <6cb8ca860712241453u3d2dad40yd6607d1ddc7f339e@mail.gmail.com>
Message-ID: <1115a2b00712241531m7a75084cj71a112e7fde54b17@mail.gmail.com>

Hi, Sandeep,
what is your specification of p, d, q? it is not surprising to have
all forecasted with same value.


On 12/24/07, Sandeep Nikam <spndeep at googlemail.com> wrote:
> Hi,
>
> This is regarding the ARIMA model.
>
> I am having time series data of stock of  2000 values. Using the ARIMA model
> in R, I want the forcasted values for next 36 time points.
>
> However when I run this model in R, I am getting same value for all 36 time
> points.
> I have tried to fit the data with ARIMA model by changing the parameters
> p,d,q after looking at the errors and other criteria for selecting the p,d,q
> value.
>
> So could you please help me in this regard, and if require I can send the
> data file too.
>
> With rgds,
>
> Sandeep
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
===============================
WenSui Liu
Statistical Project Manager
ChoicePoint Precision Marketing
(http://spaces.msn.com/statcompute/blog)


From liuwensui at gmail.com  Tue Dec 25 00:33:23 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 24 Dec 2007 18:33:23 -0500
Subject: [R] mgarch
In-Reply-To: <b44da9db0712241143q1d609dfcye8ef701efb951327@mail.gmail.com>
References: <b44da9db0712241143q1d609dfcye8ef701efb951327@mail.gmail.com>
Message-ID: <1115a2b00712241533l3257aefble9568cb9d330bf48@mail.gmail.com>

Hi, Young,
pls see if attached is what you are looking for.
http://www.google.com/search?q=m-garch&domains=r-project.org&sitesearch=r-project.org&btnG=Google+Search

On 12/24/07, Young Cho <young.stat at gmail.com> wrote:
> Is there a package or function for multivariate garch model in R? I am
> having a hard time in locating one. Thanks for help in advance.
>
> -Young
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
===============================
WenSui Liu
Statistical Project Manager
ChoicePoint Precision Marketing
(http://spaces.msn.com/statcompute/blog)


From r.otasuke at gmail.com  Tue Dec 25 00:59:37 2007
From: r.otasuke at gmail.com (Kunio takezawa)
Date: Tue, 25 Dec 2007 08:59:37 +0900
Subject: [R] "gam()" in "gam" package
In-Reply-To: <Pine.LNX.4.64.0712210750320.16449@homer24.u.washington.edu>
References: <s767a051.089@tedmail.lgc.co.uk>
	<4b2e15cd0712181710k68e930bdi938587229f6b6438@mail.gmail.com>
	<4b2e15cd0712191622l38a5f904n70ba3e6539fe6808@mail.gmail.com>
	<Pine.LNX.4.64.0712210750320.16449@homer24.u.washington.edu>
Message-ID: <4b2e15cd0712241559i5584ac9bxfbd31d0f2e4582d8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071225/9dfbd5d7/attachment.pl 

From murdoch at stats.uwo.ca  Tue Dec 25 01:46:15 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 24 Dec 2007 19:46:15 -0500
Subject: [R] "gam()" in "gam" package
In-Reply-To: <4b2e15cd0712241559i5584ac9bxfbd31d0f2e4582d8@mail.gmail.com>
References: <s767a051.089@tedmail.lgc.co.uk>	<4b2e15cd0712181710k68e930bdi938587229f6b6438@mail.gmail.com>	<4b2e15cd0712191622l38a5f904n70ba3e6539fe6808@mail.gmail.com>	<Pine.LNX.4.64.0712210750320.16449@homer24.u.washington.edu>
	<4b2e15cd0712241559i5584ac9bxfbd31d0f2e4582d8@mail.gmail.com>
Message-ID: <477052D7.5070805@stats.uwo.ca>

On 24/12/2007 6:59 PM, Kunio takezawa wrote:
> R-users
> E-mail: r-help at r-project.org
> 
>>>  I found the answer myself.
>>>  '.Fortran("baklo",' in lo.wam() and  .Fortran("bakfit",in
>>> s.wam() may carry out backfitting. But I cannot
>>> create an R code which gives the same results as those of
>>> "bakfit". If someone knows the detail of "bakfit" algorithm,
> 
> Thomas>The source code for both Fortran routines is in the package.  If I
> recal
> Thomas>correctly the book by Hastie & Tibshirani has descriptions of
> Thomas>the algorithms they used.
> 
>    I failed to find the Fortran source codes. Please specify the names
> of folder and files.

You may have a binary install of the package; you need the source.  Look 
in http://cran.r-project.org/src/contrib/gam_0.98.tar.gz, files 
gam/src/backfit.f and gam/src/backlo.f

Duncan Murdoch


From booboo at gforcecable.com  Tue Dec 25 02:06:54 2007
From: booboo at gforcecable.com (Tom La Bone)
Date: Mon, 24 Dec 2007 17:06:54 -0800 (PST)
Subject: [R]  Problems with EMACS-ESS on Ubuntu
Message-ID: <14492497.post@talk.nabble.com>


I installed EMACS-ESS on Windows XP-Pro and it worked with R perfectly right
out of the box. I was impressed with how easy it is to use (I normally use
Tinn-R). I then switched over to Ubuntu 7.10 and installed EMACS-ESS.
Everything worked the same as in Windows (which is my main reason for using
EMACS) until I tried to send a line of code from the EMACS buffer to R -- R
is running but the code just will not go to it. All I get is a line at the
bottom of the editor window that says "Process to load into: R". I am new to
EMACS and did not see any fixes that looked promising in the ESS manual. Can
anyone offer some suggestions on things to check? Thanks.

Tom
  
-- 
View this message in context: http://www.nabble.com/Problems-with-EMACS-ESS-on-Ubuntu-tp14492497p14492497.html
Sent from the R help mailing list archive at Nabble.com.


From r.otasuke at gmail.com  Tue Dec 25 02:51:35 2007
From: r.otasuke at gmail.com (Kunio takezawa)
Date: Tue, 25 Dec 2007 10:51:35 +0900
Subject: [R] fields 4.1
Message-ID: <4b2e15cd0712241751p780fb224xcdd6ec23be0c1309@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071225/8a01395b/attachment.pl 

From r.otasuke at gmail.com  Tue Dec 25 02:59:25 2007
From: r.otasuke at gmail.com (Kunio takezawa)
Date: Tue, 25 Dec 2007 10:59:25 +0900
Subject: [R] "gam()" in "gam" package
In-Reply-To: <477052D7.5070805@stats.uwo.ca>
References: <s767a051.089@tedmail.lgc.co.uk>
	<4b2e15cd0712181710k68e930bdi938587229f6b6438@mail.gmail.com>
	<4b2e15cd0712191622l38a5f904n70ba3e6539fe6808@mail.gmail.com>
	<Pine.LNX.4.64.0712210750320.16449@homer24.u.washington.edu>
	<4b2e15cd0712241559i5584ac9bxfbd31d0f2e4582d8@mail.gmail.com>
	<477052D7.5070805@stats.uwo.ca>
Message-ID: <4b2e15cd0712241759q133c281di9db33f276619f889@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071225/6dc33e88/attachment.pl 

From bolker at ufl.edu  Tue Dec 25 03:51:48 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 25 Dec 2007 02:51:48 +0000 (UTC)
Subject: [R] Problems with EMACS-ESS on Ubuntu
References: <14492497.post@talk.nabble.com>
Message-ID: <loom.20071225T025032-478@post.gmane.org>

Tom La Bone <booboo <at> gforcecable.com> writes:

> 
> 
> I installed EMACS-ESS on Windows XP-Pro and it worked with R perfectly right
> out of the box. I was impressed with how easy it is to use (I normally use
> Tinn-R). I then switched over to Ubuntu 7.10 and installed EMACS-ESS.
> Everything worked the same as in Windows (which is my main reason for using
> EMACS) until I tried to send a line of code from the EMACS buffer to R -- R
> is running but the code just will not go to it. All I get is a line at the
> bottom of the editor window that says "Process to load into: R". I am new to
> EMACS and did not see any fixes that looked promising in the ESS manual. Can
> anyone offer some suggestions on things to check? Thanks.

  Emacs just wants you to confirm which buffer the code is getting
sent to (and the buffer called "R" is its default).  The focus should
automatically have gotten set to the mini-buffer at the bottom of the
window, so if you just hit ENTER in response to this query everything
should work fine!

  Ben Bolker


From bolker at ufl.edu  Tue Dec 25 03:55:21 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 25 Dec 2007 02:55:21 +0000 (UTC)
Subject: [R] vector distribution
References: <14469865.post@talk.nabble.com>
Message-ID: <loom.20071225T025443-2@post.gmane.org>

-Halcyon- <Cyonora <at> gmail.com> writes:

> 
> 
> Hi everyone, 
> 
> say i have a population (stable) with different amounts of animals in every
> ageclass (80 of age 1, 60 of age 2, etc) in a vector.
> Can anybody tell me how i can add gender (male or female) to all ageclasses?
> I want a 1:1 ratio of males and females within the population (as a starting
> value). So, 40 males and 40 females of age 1, 30m-30f of age 2, etc etc... 
> 
> Any help would be greatly appreciated!! 
> 
> Kind regards

  Do you mean something like

> x <- c(age1=80,age2=60,age3=40)
> x
age1 age2 age3 
  80   60   40 
> y <- rep(x/2,2)
> names(y) <- c(paste("M",names(x),sep="."),paste("F",names(x),sep="."))
> y
M.age1 M.age2 M.age3 F.age1 F.age2 F.age3 
    40     30     20     40     30     20 
> 

  ?

  Ben Bolker


From young.stat at gmail.com  Tue Dec 25 05:22:39 2007
From: young.stat at gmail.com (YOUNG CHO)
Date: Mon, 24 Dec 2007 20:22:39 -0800
Subject: [R] mgarch
In-Reply-To: <1115a2b00712241533l3257aefble9568cb9d330bf48@mail.gmail.com>
References: <b44da9db0712241143q1d609dfcye8ef701efb951327@mail.gmail.com>
	<1115a2b00712241533l3257aefble9568cb9d330bf48@mail.gmail.com>
Message-ID: <454F272A-3D5D-4B47-9465-8F84FB97DA05@gmail.com>

First of all, thanks so much! In fact I had looked at the site  
before. But, that site leads me to the following site:

http://www.vsthost.com/vstDocs/mgarchBEKK/

but it was not clear to me where I can find the package to download &  
compile. Is there some special website I need go to in order to  
download the package? Thanks again!

-Young

On Dec 24, 2007, at 3:33 PM, Wensui Liu wrote:

> Hi, Young,
> pls see if attached is what you are looking for.
> http://www.google.com/search?q=m-garch&domains=r- 
> project.org&sitesearch=r-project.org&btnG=Google+Search
>
> On 12/24/07, Young Cho <young.stat at gmail.com> wrote:
>> Is there a package or function for multivariate garch model in R?  
>> I am
>> having a hard time in locating one. Thanks for help in advance.
>>
>> -Young
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> -- 
> ===============================
> WenSui Liu
> Statistical Project Manager
> ChoicePoint Precision Marketing
> (http://spaces.msn.com/statcompute/blog)
> ===============================


From jim at bitwrit.com.au  Tue Dec 25 10:16:06 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 25 Dec 2007 20:16:06 +1100
Subject: [R] geom_hline
In-Reply-To: <642070.60165.qm@web56603.mail.re3.yahoo.com>
References: <642070.60165.qm@web56603.mail.re3.yahoo.com>
Message-ID: <4770CA56.4080002@bitwrit.com.au>

Felipe Carrillo wrote:
>  Hi all:
> I am trying to draw a horizontal line along the zero
> "Y axis" value but since zero isn't showing therefore
> the line is not drawn. If I set my intercept to 15
> then it'll work but I dont want to hardcode it because
> the Y axis parameters could be different for another
> variable. I would like to alway set my hline along the
> zero as default. Thanks
> 
> p <- ggplot(mtcars, aes(x = wt, y=mpg)) + geom_point()
> p + geom_hline(intercept=0)
> 
Hi Felipe,
It sounds like you want to include zero in the axis. In base graphics, 
you would use:

ylim=c(0,maxy)

where maxy specifies the upper end of the ordinate. I had a quick look 
at the ggplot2 docs, but I haven't yet found whether this works in that 
system.

Jim


From brown_emu at yahoo.com  Tue Dec 25 10:35:19 2007
From: brown_emu at yahoo.com (Satoshi Takahama)
Date: Tue, 25 Dec 2007 01:35:19 -0800 (PST)
Subject: [R] R Compilation error on Ubuntu
Message-ID: <972178.28791.qm@web39712.mail.mud.yahoo.com>

Hi Taka,

I was just trying to do this yesterday and ran into the same problem (compiling R 2.6.1 on Gutsy Gibbon). Apparently this happens on Debian/Ubuntu distributions because the developer install is separate from the user install.

Basically, to address the configure error:

"--with-readline=yes (default) and headers/libs are not available"

you need the development version of readline:
  sudo apt-get install libreadline5-dev
(see http://cran.r-project.org/doc/manuals/R-admin.html)

Later, I ran into another configure error:
"--with-x=yes (default) and X11 headers/libs are not available"
so I installed xorg-dev:
  sudo apt-get install xorg-dev
(see http://tolstoy.newcastle.edu.au/R/e2/help/06/11/5193.html)

At the end, I also got a configure WARNING:
"you cannot build info or HTML versions of the R manuals "
so then I installed texinfo:
  sudo apt-get install texinfo
(see http://tolstoy.newcastle.edu.au/R/e2/help/07/04/15498.html)

Hope this helps,

ST


----- Original Message ----
From: Takatsugu Kobayashi <tkobayas at indiana.edu>
To: r-help at stat.math.ethz.ch
Sent: Monday, December 24, 2007 10:52:11 AM
Subject: Re: [R] R Compilation error on Ubuntu


Hi

I bought a new laptop HP dv9500 just a week ago and installed a Ubuntu
gutsy ribbon on this laptop. I wanted to install Fedora but there are
more threads on Ubuntu, so I decided to install Ubuntu. After hours of
struggle in configuring x server/graphic card stuff, I installed R for
Gutsy ribbon using "sudo apt-get install g77 r-core'. 

Now when I tried to install 'sp' package I got this error message: 

Warning in install.packages("sp") : argument 'lib' is missing: using
'/usr/local/lib/R/site-library'
--- Please select a CRAN mirror for use in this session ---
Loading Tcl/Tk interface ... done
trying URL 'http://cran.mtu.edu/src/contrib/sp_0.9-17.tar.gz'
Content type 'application/x-gzip' length 359194 bytes
opened URL
==================================================
downloaded 350Kb

* Installing *source* package 'sp' ...
** libs
gcc-4.2 -std=gnu99 -I/usr/share/R/include -I/usr/share/R/include
-fpic  -g -O2 -c gcdist.c -o gcdist.o
/bin/bash: gcc-4.2: command not found
make: *** [gcdist.o] Error 127
ERROR: compilation failed for package 'sp'
** Removing '/usr/local/lib/R/site-library/sp'

The downloaded packages are in
        /tmp/RtmpfOk3H7/downloaded_packages
Warning message:
installation of package 'sp' had non-zero exit status in:
install.packages("sp") 

Then, I removed this gutsy ribbon version of R and attempted to install
fresh R-2.6.1.tar.gz. When I tried to build it, I got this error
message:

configure: error: --with-readline=yes (default) and headers/libs are
 not
available

I installed xserver-xorg-dev ok. 

I am using Fedora 6 on my desktop and have never seen these above error
messages. Should I install a Fedora and R on it? Or should I just
 create
a link using 'ln -s'? 

I appreciate your help.

[[replacing trailing spam]]

taka

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
 http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.





      ____________________________________________________________________________________
Never miss a thing.  Make Yahoo your home page.


From ralf.pfeiffer at pfeiffer-koberstein-immobilien.de  Tue Dec 25 10:41:56 2007
From: ralf.pfeiffer at pfeiffer-koberstein-immobilien.de (Pfeiffer & Koberstein Immobilien GmbH - Ralf Pfeiffer)
Date: Tue, 25 Dec 2007 10:41:56 +0100
Subject: [R] aggregation with two statistical functions - mean and
	variance
References: <003101c84673$d42a3b60$6702a8c0@PFEIFFER>
	<971536df0712241348l6c650a2ex15897a59d0855d07@mail.gmail.com>
Message-ID: <001e01c846da$63b68520$6702a8c0@PFEIFFER>

thank you,  I downloaded and installed the package. With library(help="doBy") 
I got the information about this package.

But when I use the syntax

summaryBy(daten[,c(3:4)], list(A,B), Fun=c(var,mean))

I got the following error:
<<Fehler: konnte Funktion "summaryBy" nicht finden>> (error: couldn't find 
function summaryBy)

I tried before
require(doBy), but this doesn't work neither.

What can I do?

----- Original Message ----- 
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
To: "Agrarimmobilien" <ralf.pfeiffer at agrarimmobilien.info>
Cc: <r-help at r-project.org>
Sent: Monday, December 24, 2007 10:48 PM
Subject: Re: [R] aggregation with two statistical functions - mean and 
variance


> See summaryBy in the doBy package.
>
> On Dec 24, 2007 4:27 PM, Agrarimmobilien
> <ralf.pfeiffer at agrarimmobilien.info> wrote:
>> Hello,
>>
>> using the syntax
>>
>> aggregate(daten[,c(3,4)], list(A,B), mean)
>>
>> I'm getting the following data.frame:
>>
>>
>>    A     B        C                    D
>> 1  35    1         6.16000          5
>> 2  47    1        31.24333         20
>> 3  54    1        26.81773         2
>> 4   3     2        12.99000         7
>> 5   4     2         6.49000          1
>>
>> C and D are both means. But now I want to have: C is mean, D is variance.
>>
>> How can I do this by using/ changing the aggregation syntax?
>>
>> Thanks!
>>
>> Macki
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From brown_emu at yahoo.com  Tue Dec 25 10:46:12 2007
From: brown_emu at yahoo.com (Satoshi Takahama)
Date: Tue, 25 Dec 2007 01:46:12 -0800 (PST)
Subject: [R] expand.grid function
Message-ID: <923825.45330.qm@web39703.mail.mud.yahoo.com>

Hi Derek,

I love this function as well. If 'mat' is your matrix,
> (mat <- matrix(1:15,ncol=3))
     [,1] [,2] [,3]
[1,]    1    6   11
[2,]    2    7   12
[3,]    3    8   13
[4,]    4    9   14
[5,]    5   10   15

how about
> expandedDF <- do.call(`expand.grid`,as.data.frame(mat))
> head(expandedDF)
  V1 V2 V3
1  1  6 11
2  2  6 11
3  3  6 11
4  4  6 11
5  5  6 11
6  1  7 11
[...]

(the function name can be in backticks, single or double quotes, or passed with no quotes). Hope this helps,

ST


----- Original Message ----
From: dxc13 <dxc13 at health.state.ny.us>
To: r-help at r-project.org
Sent: Sunday, December 23, 2007 7:59:49 PM
Subject: [R]  expand.grid function



useR's,

I have used expand.grid() several times and like the results it gives
 me.  I
am now trying something with it that I have not been able to get to
 work. 
For any n column matrix I would like to run this function on those n
 columns
and store the results.
For example, if my matrix has 1 column then this is just expand.grid(x
 =
column1).  If my matrix has two columns, then I want expand.grid(x =
column1, y = column2), and so on for any number of columns...

In a program I am writing, the user can specify any matrix.  Does
 anyone
know of a way for R to calculate this based on what the input matrix
 is? 
e.g. if this user gives a 3 column matrix, I want to be able to perform
expand.grid() on these 3 columns without having to hard code it b/c I
 want
to have this small function embedded in my code and the results stored
 as a
variable.

If this isn't clear, I can try to be more detailed.  Thank you for any
thoughts.

Derek 
-- 
View this message in context:
 http://www.nabble.com/expand.grid-function-tp14484403p14484403.html
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
 http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.





      ____________________________________________________________________________________
Looking for last minute shopping deals?


From ggrothendieck at gmail.com  Tue Dec 25 12:49:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 25 Dec 2007 06:49:41 -0500
Subject: [R] aggregation with two statistical functions - mean and
	variance
In-Reply-To: <001e01c846da$63b68520$6702a8c0@PFEIFFER>
References: <003101c84673$d42a3b60$6702a8c0@PFEIFFER>
	<971536df0712241348l6c650a2ex15897a59d0855d07@mail.gmail.com>
	<001e01c846da$63b68520$6702a8c0@PFEIFFER>
Message-ID: <971536df0712250349ke13db6dr16b47cd7c2f1806@mail.gmail.com>

On Dec 25, 2007 4:41 AM, Pfeiffer & Koberstein Immobilien GmbH - Ralf
Pfeiffer <ralf.pfeiffer at pfeiffer-koberstein-immobilien.de> wrote:
> thank you,  I downloaded and installed the package. With library(help="doBy")
> I got the information about this package.
>
> But when I use the syntax
>
> summaryBy(daten[,c(3:4)], list(A,B), Fun=c(var,mean))
>
> I got the following error:
> <<Fehler: konnte Funktion "summaryBy" nicht finden>> (error: couldn't find
> function summaryBy)
>
> I tried before
> require(doBy), but this doesn't work neither.
>
> What can I do?

Make sure you did this:

install.packages("doBy")
library(doBy)
library(help = doBy)
?summaryBy
example(summaryBy)

which installs the package on your computer, loads the package into your
session, shows help for the package, shows help
for the summaryBy function and runs the examples, respectively. Read
the help page carefully and follow the examples.

Also read the posting guide and follow the instructions on the
last line of every message to r-help.


From m_hofert at web.de  Tue Dec 25 15:56:03 2007
From: m_hofert at web.de (Hofert Marius)
Date: Tue, 25 Dec 2007 15:56:03 +0100
Subject: [R] both lines and plot characters in a lattice plot key
Message-ID: <33F086AA-0408-4904-B471-00C391C2E281@web.de>

Hello,

I have two short questions concerning a key (legend) in a lattice  
plot. I have points represented by two different plot characters  
(crosses and circles) and they are colored from light to dark gray. I  
would like to have 3 parts in my legend showing the two different  
plot characters in the first two lines and the last line should  
consist of a line segment for explaining the different shades of  
gray. Below you can see what I reached so far.
My first problem is, that I do not get the 2 plot characters and the  
line segment vertically aligned to each other (I chose the vector of  
plotting characters to contain "26" as last element, because this  
leaves out a plotting character for the last line, where the line  
segment should be placed). So how do I get these aligned?
My second question is, if it's possible to have a line segment shown  
in the key, which is colored from light to dark gray (instead of a  
single color), matching the colors in my specified variable "colors"  
if possible, i.e. a single line segment starting from light gray and  
ending with a dark gray.

Thanks very much in advance!

Jan

library(lattice)
x=c(1,2,3,4,5)
y=c(5,4,3,2,1)
symbolvec=c(3,3,3,1,3)
colorseq=seq(0.25,0.88,length=5)
colors=gray(colorseq)
colorvec=colors
xyplot(y~x,
	panel=function(...){
		panel.xyplot(...,type="n")
		lpoints(x,y,pch=symbolvec,col=colorvec)
	},
key=list(x=0.2,y=0.2,lines=list(lty=c(0,0,1)),points=list(pch=c 
(1,3,26)),text=list(c("key 1","key 2","key 3")),align=T,transparent=T))


From villegas.ro at gmail.com  Tue Dec 25 18:50:50 2007
From: villegas.ro at gmail.com (Rod)
Date: Tue, 25 Dec 2007 18:50:50 +0100
Subject: [R] R Compilation error on Ubuntu
In-Reply-To: <972178.28791.qm@web39712.mail.mud.yahoo.com>
References: <972178.28791.qm@web39712.mail.mud.yahoo.com>
Message-ID: <29cf68350712250950q6edff825y5b78cfd78a69c5fa@mail.gmail.com>

On Dec 25, 2007 10:35 AM, Satoshi Takahama <brown_emu at yahoo.com> wrote:
> Hi Taka,
>
> I was just trying to do this yesterday and ran into the same problem (compiling R 2.6.1 on Gutsy Gibbon). Apparently this happens on Debian/Ubuntu distributions because the developer install is separate from the user install.
>
> Basically, to address the configure error:
>
> "--with-readline=yes (default) and headers/libs are not available"
>
> you need the development version of readline:
>   sudo apt-get install libreadline5-dev
> (see http://cran.r-project.org/doc/manuals/R-admin.html)
>
> Later, I ran into another configure error:
> "--with-x=yes (default) and X11 headers/libs are not available"
> so I installed xorg-dev:
>   sudo apt-get install xorg-dev
> (see http://tolstoy.newcastle.edu.au/R/e2/help/06/11/5193.html)
>
> At the end, I also got a configure WARNING:
> "you cannot build info or HTML versions of the R manuals "
> so then I installed texinfo:
>   sudo apt-get install texinfo
> (see http://tolstoy.newcastle.edu.au/R/e2/help/07/04/15498.html)
>
> Hope this helps,
>
> ST
>
>
>
> ----- Original Message ----
> From: Takatsugu Kobayashi <tkobayas at indiana.edu>
> To: r-help at stat.math.ethz.ch
> Sent: Monday, December 24, 2007 10:52:11 AM
> Subject: Re: [R] R Compilation error on Ubuntu
>
>
> Hi
>
> I bought a new laptop HP dv9500 just a week ago and installed a Ubuntu
> gutsy ribbon on this laptop. I wanted to install Fedora but there are
> more threads on Ubuntu, so I decided to install Ubuntu. After hours of
> struggle in configuring x server/graphic card stuff, I installed R for
> Gutsy ribbon using "sudo apt-get install g77 r-core'.
>
> Now when I tried to install 'sp' package I got this error message:
>
> Warning in install.packages("sp") : argument 'lib' is missing: using
> '/usr/local/lib/R/site-library'
> --- Please select a CRAN mirror for use in this session ---
> Loading Tcl/Tk interface ... done
> trying URL 'http://cran.mtu.edu/src/contrib/sp_0.9-17.tar.gz'
> Content type 'application/x-gzip' length 359194 bytes
> opened URL
> ==================================================
> downloaded 350Kb
>
> * Installing *source* package 'sp' ...
> ** libs
> gcc-4.2 -std=gnu99 -I/usr/share/R/include -I/usr/share/R/include
> -fpic  -g -O2 -c gcdist.c -o gcdist.o
> /bin/bash: gcc-4.2: command not found
> make: *** [gcdist.o] Error 127
> ERROR: compilation failed for package 'sp'
> ** Removing '/usr/local/lib/R/site-library/sp'
>
> The downloaded packages are in
>         /tmp/RtmpfOk3H7/downloaded_packages
> Warning message:
> installation of package 'sp' had non-zero exit status in:
> install.packages("sp")
>
> Then, I removed this gutsy ribbon version of R and attempted to install
> fresh R-2.6.1.tar.gz. When I tried to build it, I got this error
> message:
>
> configure: error: --with-readline=yes (default) and headers/libs are
>  not
> available
>
> I installed xserver-xorg-dev ok.
>
> I am using Fedora 6 on my desktop and have never seen these above error
> messages. Should I install a Fedora and R on it? Or should I just
>  create
> a link using 'ln -s'?
>
> I appreciate your help.
>
> [[replacing trailing spam]]
>
> taka
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
>  http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>
>       ____________________________________________________________________________________
> Never miss a thing.  Make Yahoo your home page.
>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

I tried google and I find
http://help.nceas.ucsb.edu/index.php/Installing_R_on_Ubuntu
Perhaps it's more easy to install this manner.

Happy Xmas,

Rod.


From booboo at gforcecable.com  Tue Dec 25 22:30:39 2007
From: booboo at gforcecable.com (Tom La Bone)
Date: Tue, 25 Dec 2007 13:30:39 -0800 (PST)
Subject: [R] Problems with EMACS-ESS on Ubuntu
In-Reply-To: <loom.20071225T025032-478@post.gmane.org>
References: <14492497.post@talk.nabble.com>
	<loom.20071225T025032-478@post.gmane.org>
Message-ID: <14497587.post@talk.nabble.com>


Yes indeed. As usual, I found the answer to my question shortly after I
posted it to the forum. John Fox clearly describes this behavior in his
paper "An Introduction to ESS + XEmacs for Windows Users of R". I guess I
was confused because the version of Emacs/ESS I installed on Windows
(courtesy of Vincent Goulet) did not require me to confirm that I wanted to
use R. Thanks everyone for the assistance.

Tom



Ben Bolker wrote:
> 
> Tom La Bone <booboo <at> gforcecable.com> writes:
> 
>> 
>> 
>> I installed EMACS-ESS on Windows XP-Pro and it worked with R perfectly
>> right
>> out of the box. I was impressed with how easy it is to use (I normally
>> use
>> Tinn-R). I then switched over to Ubuntu 7.10 and installed EMACS-ESS.
>> Everything worked the same as in Windows (which is my main reason for
>> using
>> EMACS) until I tried to send a line of code from the EMACS buffer to R --
>> R
>> is running but the code just will not go to it. All I get is a line at
>> the
>> bottom of the editor window that says "Process to load into: R". I am new
>> to
>> EMACS and did not see any fixes that looked promising in the ESS manual.
>> Can
>> anyone offer some suggestions on things to check? Thanks.
> 
>   Emacs just wants you to confirm which buffer the code is getting
> sent to (and the buffer called "R" is its default).  The focus should
> automatically have gotten set to the mini-buffer at the bottom of the
> window, so if you just hit ENTER in response to this query everything
> should work fine!
> 
>   Ben Bolker
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Problems-with-EMACS-ESS-on-Ubuntu-tp14492497p14497587.html
Sent from the R help mailing list archive at Nabble.com.


From Thierry.ONKELINX at inbo.be  Tue Dec 25 22:32:17 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 25 Dec 2007 22:32:17 +0100
Subject: [R] geom_hline
References: <642070.60165.qm@web56603.mail.re3.yahoo.com>
	<4770CA56.4080002@bitwrit.com.au>
Message-ID: <2E9C414912813E4EB981326983E0A1042E49F1@inboexch.inbo.be>

Felipe,
 
You'll need to define the limits yourself with scale_y_continuous.
 
library(ggplot2)
p <- ggplot(mtcars, aes(x = wt, y=mpg)) + geom_point()
p + geom_hline(intercept=0) + scale_y_continuous(limits = c(0, max(mtcars$mpg)))

Cheers,
 
Thierry

________________________________

Van: r-help-bounces op r-project.org namens Jim Lemon
Verzonden: di 25-12-2007 10:16
Aan: Felipe Carrillo
CC: r-help op stat.math.ethz.ch
Onderwerp: Re: [R] geom_hline



Felipe Carrillo wrote:
>  Hi all:
> I am trying to draw a horizontal line along the zero
> "Y axis" value but since zero isn't showing therefore
> the line is not drawn. If I set my intercept to 15
> then it'll work but I dont want to hardcode it because
> the Y axis parameters could be different for another
> variable. I would like to alway set my hline along the
> zero as default. Thanks
>
> p <- ggplot(mtcars, aes(x = wt, y=mpg)) + geom_point()
> p + geom_hline(intercept=0)
>
Hi Felipe,
It sounds like you want to include zero in the axis. In base graphics,
you would use:

ylim=c(0,maxy)

where maxy specifies the upper end of the ordinate. I had a quick look
at the ggplot2 docs, but I haven't yet found whether this works in that
system.

Jim

______________________________________________
R-help op r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From deepayan.sarkar at gmail.com  Tue Dec 25 22:45:39 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 25 Dec 2007 13:45:39 -0800
Subject: [R] both lines and plot characters in a lattice plot key
In-Reply-To: <33F086AA-0408-4904-B471-00C391C2E281@web.de>
References: <33F086AA-0408-4904-B471-00C391C2E281@web.de>
Message-ID: <eb555e660712251345o5d60693bv35b982cd78395338@mail.gmail.com>

On 12/25/07, Hofert Marius <m_hofert at web.de> wrote:
> Hello,
>
> I have two short questions concerning a key (legend) in a lattice
> plot. I have points represented by two different plot characters
> (crosses and circles) and they are colored from light to dark gray. I
> would like to have 3 parts in my legend showing the two different
> plot characters in the first two lines and the last line should
> consist of a line segment for explaining the different shades of
> gray. Below you can see what I reached so far.
> My first problem is, that I do not get the 2 plot characters and the
> line segment vertically aligned to each other (I chose the vector of
> plotting characters to contain "26" as last element, because this
> leaves out a plotting character for the last line, where the line
> segment should be placed). So how do I get these aligned?

The 'lines' components allow a 'type' specification, so:

       key =
       list(x=0.2,y=0.2,
            lines=list(pch=c(1,3,1), type = c("p", "p", "l")),
            text=list(c("key 1","key 2","key 3")),
            align=T,transparent=T)

> My second question is, if it's possible to have a line segment shown
> in the key, which is colored from light to dark gray (instead of a
> single color), matching the colors in my specified variable "colors"
> if possible, i.e. a single line segment starting from light gray and
> ending with a dark gray.

Unfortunately, no, at least not using the 'key' argument.  However,
using the 'legend' argument gives you more flexibility, especially if
you are willing to write some grid code. Even without grid, you can
use draw.key() and draw.colorkey() together to produce one legend for
the plotting characters and a different one for the color gradient.

-Deepayan

>
> Thanks very much in advance!
>
> Jan
>
> library(lattice)
> x=c(1,2,3,4,5)
> y=c(5,4,3,2,1)
> symbolvec=c(3,3,3,1,3)
> colorseq=seq(0.25,0.88,length=5)
> colors=gray(colorseq)
> colorvec=colors
> xyplot(y~x,
>         panel=function(...){
>                 panel.xyplot(...,type="n")
>                 lpoints(x,y,pch=symbolvec,col=colorvec)
>         },
> key=list(x=0.2,y=0.2,lines=list(lty=c(0,0,1)),points=list(pch=c
> (1,3,26)),text=list(c("key 1","key 2","key 3")),align=T,transparent=T))
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Tue Dec 25 22:46:58 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 25 Dec 2007 21:46:58 +0000
Subject: [R] geom_hline
In-Reply-To: <2E9C414912813E4EB981326983E0A1042E49F1@inboexch.inbo.be>
References: <642070.60165.qm@web56603.mail.re3.yahoo.com>
	<4770CA56.4080002@bitwrit.com.au>
	<2E9C414912813E4EB981326983E0A1042E49F1@inboexch.inbo.be>
Message-ID: <f8e6ff050712251346q57d74b57ya444a43f3188d243@mail.gmail.com>

On Dec 25, 2007 9:32 PM, ONKELINX, Thierry <Thierry.ONKELINX at inbo.be> wrote:
> Felipe,
>
> You'll need to define the limits yourself with scale_y_continuous.
>
> library(ggplot2)
> p <- ggplot(mtcars, aes(x = wt, y=mpg)) + geom_point()
> p + geom_hline(intercept=0) + scale_y_continuous(limits = c(0, max(mtcars$mpg)))

Or

p + geom_hline(intercept=0) + scale_y_continuous(limits = c(0, NA))

-  the NA means that it will calculate the maximum from the data.

Hadley

-- 
http://had.co.nz/


From r.otasuke at gmail.com  Wed Dec 26 05:14:06 2007
From: r.otasuke at gmail.com (Kunio takezawa)
Date: Wed, 26 Dec 2007 13:14:06 +0900
Subject: [R] Cubic splines in package "mgcv"
Message-ID: <4b2e15cd0712252014g287eb219o44a627beeaa19752@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071226/28fe0a5e/attachment.pl 

From pmilin at ff.ns.ac.yu  Wed Dec 26 09:08:20 2007
From: pmilin at ff.ns.ac.yu (Petar Milin)
Date: Wed, 26 Dec 2007 09:08:20 +0100
Subject: [R] Removing duplicate packages...
Message-ID: <47720BF4.7070308@ff.ns.ac.yu>

Hello!
I am using R version 2.6.1, under Ubuntu 6.06 (Dapper Drake). Recently I 
have realized that I have, in some cases, two or even three instances of 
the same package; for example: cluster, foreign etc. Typically, two 
instances occur in one of the three location: 
"/usr/local/lib/R/site-library", "/usr/lib/R/site-library", or 
"/usr/lib/R/library", and the third is, I suppose, part of the core system.
I would really like to clean up my system, but the question is how to do 
that. Can anyone help me, please?

Sincerely,
PM


From wwwhsd at gmail.com  Wed Dec 26 11:18:01 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 26 Dec 2007 08:18:01 -0200
Subject: [R] Removing duplicate packages...
In-Reply-To: <47720BF4.7070308@ff.ns.ac.yu>
References: <47720BF4.7070308@ff.ns.ac.yu>
Message-ID: <da79af330712260218n3a2805d4q78cbca3e93f78253@mail.gmail.com>

Try this:

(tested on Windows)

remove.packages(installed.packages()[duplicated(rownames(installed.packages())),1],
lib=.libPaths()[.libPaths() != .Library])

On 26/12/2007, Petar Milin <pmilin at ff.ns.ac.yu> wrote:
> Hello!
> I am using R version 2.6.1, under Ubuntu 6.06 (Dapper Drake). Recently I
> have realized that I have, in some cases, two or even three instances of
> the same package; for example: cluster, foreign etc. Typically, two
> instances occur in one of the three location:
> "/usr/local/lib/R/site-library", "/usr/lib/R/site-library", or
> "/usr/lib/R/library", and the third is, I suppose, part of the core system.
> I would really like to clean up my system, but the question is how to do
> that. Can anyone help me, please?
>
> Sincerely,
> PM
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From arturmatos78 at gmail.com  Wed Dec 26 14:07:44 2007
From: arturmatos78 at gmail.com (Artur Matos)
Date: Wed, 26 Dec 2007 22:07:44 +0900
Subject: [R] persp(): how to control the distance between the axis and the
	axis labels
Message-ID: <AC81D974-454E-4254-A877-A8C7D8159BAC@gmail.com>

Hi,

I have been using persp() to draw 3D surface plots of my data,
but I always run into a small problem: If use the standard settings,  
the axis
labels always end up too close and overlapping the the axis numbers.  
Is there
any setting for controlling the distance between the axis labels and the
axis themselves? I have been going through the persp() and par()  
documentation,
and I could not find anything.

In case it is of any help, this is the call I am using:

est <- bkde2D(x, bandwidth=c(2,0.1))
persp(x=est$x1, y=est$x2, z=est$fhat, phi =30, theta =20, d=  
5,ticktype="detailed", xlab="Beta'", ylab="K'", zlab="Density",  
main="Grammar Based")

Best regards,

Artur.


From lbacon at lba.com  Tue Dec 25 03:07:28 2007
From: lbacon at lba.com (Lynd Bacon)
Date: Mon, 24 Dec 2007 18:07:28 -0800
Subject: [R] how to remove paths from where library() looks for packages
Message-ID: <477065E0.9090709@lba.com>

Greetings.  My current installation of R 2.6.1 on Unbuntu 7.10 ("Gutsy") 
is looking for packages in a directory that doesn't contain any:

 > library()
Warning message:
In library() : library "/usr/lib/R/site-library" contains no packages

 > .libPaths()
[1] "/usr/local/lib/R/site-library" "/usr/lib/R/site-library"
[3] "/usr/lib/R/library"

I'd like to eliminate the path to the empty directory so I don't get 
this vexsome warning message.  Any and all suggestions will be greatly 
appreciated!

Cheers,

Lynd

-- 
_________________________________________
Lynd Bacon
Woodside CA USA GMT-08:00


From ralf.pfeiffer at pfeiffer-koberstein-immobilien.de  Wed Dec 26 20:08:12 2007
From: ralf.pfeiffer at pfeiffer-koberstein-immobilien.de (Pfeiffer & Koberstein Immobilien GmbH - Ralf Pfeiffer)
Date: Wed, 26 Dec 2007 20:08:12 +0100
Subject: [R] data.frame - how to calculate the number of rows
Message-ID: <004701c847f2$ae9335c0$6702a8c0@PFEIFFER>

Hello,

it seems to be a simple problem, but  I couldn't find an answer in the 
archiv. (I think, it must has something to do with the group-select, like in 
php)

I've the following data.frame:

        A            B                C
1      3             6                 5
2      4             4                 20
3      5             8                 2

I want to get the number of the rows in the 4th column, like:

        A            B                C                      num rows
1      3            6                 6.16                  3


(B is mean, C is variance by using summaryBy(B+C~ A,  data=daten, 
FUN=c(mean,var))  from the doBy-package)

But, how can I add the column 'numrow' and count the number of the rows?

Thanks!

Macki


From ralf.pfeiffer at agrarimmobilien.info  Wed Dec 26 20:44:50 2007
From: ralf.pfeiffer at agrarimmobilien.info (Agrarimmobilien)
Date: Wed, 26 Dec 2007 20:44:50 +0100
Subject: [R] data.frame - how to calculate the number of rows
References: <004701c847f2$ae9335c0$6702a8c0@PFEIFFER>
Message-ID: <003001c847f7$c7c5eab0$6702a8c0@PFEIFFER>

Hello,

I had to give some more information to my posting:

as ouput there are more than one row, depending on the values of column A 
(transformierung to distinct values, here 3 and 4, but there are much more)

>        A            B                C
> 1      3             6                 5
> 2      3             4                 20
> 3      3             8                 2
> 4      4             8                 3
> 5      4             2                 6

Output:

>        A            B                C                      num rows
> 1      3            6                 6.16                  3
> 2      4            5                 4.00                  3


B is mean and C variance.


thank you
Macki



> Hello,
>
> it seems to be a simple problem, but  I couldn't find an answer in the
> archiv. (I think, it must has something to do with the group-select, like 
> in
> php)
>
> I've the following data.frame:
>
>        A            B                C
> 1      3             6                 5
> 2      4             4                 20
> 3      5             8                 2
>
> I want to get the number of the rows in the 4th column, like:
>
>        A            B                C                      num rows
> 1      3            6                 6.16                  3
>
>
> (B is mean, C is variance by using summaryBy(B+C~ A,  data=daten,
> FUN=c(mean,var))  from the doBy-package)
>
> But, how can I add the column 'numrow' and count the number of the rows?
>
> Thanks!
>
> Macki
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wwwhsd at gmail.com  Wed Dec 26 20:58:03 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 26 Dec 2007 17:58:03 -0200
Subject: [R] data.frame - how to calculate the number of rows
In-Reply-To: <003001c847f7$c7c5eab0$6702a8c0@PFEIFFER>
References: <004701c847f2$ae9335c0$6702a8c0@PFEIFFER>
	<003001c847f7$c7c5eab0$6702a8c0@PFEIFFER>
Message-ID: <da79af330712261158s40408b48m6a94a2a7b886520a@mail.gmail.com>

If i understand your question, you can try this:

cbind(summaryBy(B+C~ A,  data=df,FUN=c(mean, var)), 'num
rows'=as.numeric(table(df$A)))

On 26/12/2007, Agrarimmobilien <ralf.pfeiffer at agrarimmobilien.info> wrote:
> Hello,
>
> I had to give some more information to my posting:
>
> as ouput there are more than one row, depending on the values of column A
> (transformierung to distinct values, here 3 and 4, but there are much more)
>
> >        A            B                C
> > 1      3             6                 5
> > 2      3             4                 20
> > 3      3             8                 2
> > 4      4             8                 3
> > 5      4             2                 6
>
> Output:
>
> >        A            B                C                      num rows
> > 1      3            6                 6.16                  3
> > 2      4            5                 4.00                  3
>
>
> B is mean and C variance.
>
>
> thank you
> Macki
>
>
>
> > Hello,
> >
> > it seems to be a simple problem, but  I couldn't find an answer in the
> > archiv. (I think, it must has something to do with the group-select, like
> > in
> > php)
> >
> > I've the following data.frame:
> >
> >        A            B                C
> > 1      3             6                 5
> > 2      4             4                 20
> > 3      5             8                 2
> >
> > I want to get the number of the rows in the 4th column, like:
> >
> >        A            B                C                      num rows
> > 1      3            6                 6.16                  3
> >
> >
> > (B is mean, C is variance by using summaryBy(B+C~ A,  data=daten,
> > FUN=c(mean,var))  from the doBy-package)
> >
> > But, how can I add the column 'numrow' and count the number of the rows?
> >
> > Thanks!
> >
> > Macki
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From Greg.Snow at imail.org  Wed Dec 26 21:05:55 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Wed, 26 Dec 2007 13:05:55 -0700
Subject: [R] Reminiscing on 20 years using S
Message-ID: <07E228A5BE53C24CAD490193A7381BBBDB7058@LP-EXCHVS07.CO.IHC.COM>


I realized earlier this year (2007) that it was in 1987 that I first
started using an early version of S (it was ported to VMS and was called
success).  That means that I have been using some variant of S (to
various degrees) for over 20 years now (I don't feel that old).  

Since things are a bit slow this time of year I thought I would take a
few minutes and reminisce on some of the changes I have observed since I
first encountered that early version.  Some of the newer users may enjoy
seeing how much things have improved and maybe some of the other more
experienced users would like to chime in with their observations.

We would access that old version using dumb terminals (the computer that
actually did the work was down the hall in a locked air-conditioned room
(and it took up a good chunk of the room)).  We did graphics using
essentially ASCII art.  There was not a default graphics driver in those
days and we would specify the "printer()" driver, then do the graphics
commands, then use the "show()" command to actually print out the graph
(with '*' for points and - for the x axis, | for the y axis and + for
tick marks), we thought it was pretty impressive at the time that the
computer would do that for us.  Later I learned that our terminals could
understand both vt100 and tek4010 protocols so we would do all the
typing and textual output in vt100 mode, then switch to tek for the
graphs, having actual lines made the graphs look a lot better, but it
was a lot slower, you could see each point added to the graph.  And if
you forgot to switch back to text mode, then you next output would be
plotted (1 character at a time) over the top of the graph.

In order to get color graphs or other high quality graphs we had to send
the plots to the old hp pen plotter which had a mechanical arm that
would pick up the pens and move them around the paper (fun to watch, but
slow, very slow on curves).  That is where the origin of using color
numbers came from, color 1 matched with whichever pen was in slot #1,
color 2 with slot 2, ... And the computer did not know which color was
which (we tried to keep black in position 1, but someone could always
change that).  If you wanted to use colors 5-8 then it would stop and
beep, then you would change the pens to the other set of colors.  A line
width of 2 meant drawing the line 2 times, etc.  And there were no
filled rectangles, only density and angle of hash lines in them (if the
density was high enough it would look like a fill, but it would use up
the pens to fast and make you unpopular).

The current 600+ colors in R are a luxury now.

I remember before I learned about attaching to the search list (I don't
know if it was because attaching was not mature yet, or I just had not
learned it) we would name our objects using a pattern like: proj1.x,
proj1.y, proj2.x, proj2.y, ... where proj1 and proj2 were identifiers
for the projects (what I would name a data frame now) then there was a
prefix function that you could set proj1 as a prefix and just type x and
it would use proj1.x.  Attaching was a huge jump forward, and the 'with'
command is pure luxury in comparison (I just need to remember to
appreciate little things like that more).

It is amazing how much the S language (S-plus and R) have
changed/improved in the last 20 years.  Thanks to all the great people
who have contributed and thanks for letting me wax nostalgic for a bit.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
(801) 408-8111
 


From mazatlanmexico at yahoo.com  Wed Dec 26 21:45:19 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Wed, 26 Dec 2007 12:45:19 -0800 (PST)
Subject: [R] scale_x_continuous
Message-ID: <300154.18746.qm@web56609.mail.re3.yahoo.com>

Hello all: Any ideas on how can I set the X axis
limits to week 1? I tried the
scale_x_continuous(limits=c(1,NA)
but I get an error message. It works for the Y axis
though.

library(ggplot2)
df <- data.frame(date = seq(Sys.Date(), len=100, by="1
day")[sample(100, 50)],price = runif(50))
df <- df[order(df$date), ]
dt <- qplot(date, price, data=df, geom="line")
dt$aspect.ratio <- 1/4
dt + scale_x_date()
 dt + scale_x_date(format="%m/%d")
 dt + scale_x_date(format="%W")+
scale_y_continuous(limits = c(0, NA))

Felipe D. Carrillo
  Fishery Biologist
  US Fish & Wildlife Service
  California, USA



      ____________________________________________________________________________________
Be a better friend, newshound, and


From arjun_kondamani at ml.com  Wed Dec 26 22:23:51 2007
From: arjun_kondamani at ml.com (Kondamani, Arjun (GMI - NY Corporate Bonds))
Date: Wed, 26 Dec 2007 16:23:51 -0500
Subject: [R] Rbind-ing a list into one item
Message-ID: <2893E11BC50DB445BC9E97835D9C219D357FF7@MLNYC727MB.amrs.win.ml.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071226/326102b2/attachment.pl 

From markleeds at verizon.net  Wed Dec 26 22:36:40 2007
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 26 Dec 2007 15:36:40 -0600 (CST)
Subject: [R] Rbind-ing a list into one item
Message-ID: <14160363.1588051198705000706.JavaMail.root@vms124.mailsrvcs.net>

>From: "Kondamani, Arjun (GMI - NY Corporate Bonds)" <arjun_kondamani at ml.com>
>Date: 2007/12/26 Wed PM 03:23:51 CST
>To: r-help at r-project.org
>Subject: [R] Rbind-ing a list into one item

try do.call(rbind,res)

>Hi,
>
>I am doing the following: 
>
>1. I have a list of files.. Files1=list.files("some
>directory",pattern="some pattern")
>2. I define a list as res=vector("list", length(files1))
>3. I read all the files into this list: res=lapply(files1, read.csv)
>
>I now want to rowbind all the items in the list into one big mass (all
>files have same number of columns). I tried lapply(res, rbind) but that
>did not work. Suggestions?
>--------------------------------------------------------
>
>This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
>--------------------------------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From A.Robinson at ms.unimelb.edu.au  Wed Dec 26 22:37:56 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 27 Dec 2007 08:37:56 +1100
Subject: [R] Rbind-ing a list into one item
In-Reply-To: <2893E11BC50DB445BC9E97835D9C219D357FF7@MLNYC727MB.amrs.win.ml.com>
References: <2893E11BC50DB445BC9E97835D9C219D357FF7@MLNYC727MB.amrs.win.ml.com>
Message-ID: <20071226213756.GC1368@ms.unimelb.edu.au>

Hi Arjun,

try do.call()

Cheers

Andrew

On Wed, Dec 26, 2007 at 04:23:51PM -0500, Kondamani, Arjun (GMI - NY Corporate Bonds) wrote:
> Hi,
> 
> I am doing the following: 
> 
> 1. I have a list of files.. Files1=list.files("some
> directory",pattern="some pattern")
> 2. I define a list as res=vector("list", length(files1))
> 3. I read all the files into this list: res=lapply(files1, read.csv)
> 
> I now want to rowbind all the items in the list into one big mass (all
> files have same number of columns). I tried lapply(res, rbind) but that
> did not work. Suggestions?
> --------------------------------------------------------
> 
> This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
> --------------------------------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> This message has been scanned for viruses and
> dangerous content by MailScanner, and is
> believed to be clean.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From topkatz at msn.com  Wed Dec 26 22:42:46 2007
From: topkatz at msn.com (Talbot Katz)
Date: Wed, 26 Dec 2007 16:42:46 -0500
Subject: [R] Can you recover default argument values of a function?
Message-ID: <BAY108-W340F37C3FDD3F6B35EF7E5AA5B0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071226/29e229f6/attachment.pl 

From h.wickham at gmail.com  Wed Dec 26 22:47:28 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 26 Dec 2007 21:47:28 +0000
Subject: [R] scale_x_continuous
In-Reply-To: <300154.18746.qm@web56609.mail.re3.yahoo.com>
References: <300154.18746.qm@web56609.mail.re3.yahoo.com>
Message-ID: <f8e6ff050712261347m3d1d9d52y6b37eadf0dbd2de7@mail.gmail.com>

> dt + scale_x_date()
>  dt + scale_x_date(format="%m/%d")
>  dt + scale_x_date(format="%W")+

You can only have one x scale - the last one overrides all the others.
 The date scale (currently) doesn't have a limits setting (what units
would you specify it in?), but you could use subset to plot only the
subject of interest.

Hadley

-- 
http://had.co.nz/


From ralf.pfeiffer at agrarimmobilien.info  Wed Dec 26 22:56:27 2007
From: ralf.pfeiffer at agrarimmobilien.info (Agrarimmobilien)
Date: Wed, 26 Dec 2007 22:56:27 +0100
Subject: [R] data.frame - how to calculate the number of rows
References: <004701c847f2$ae9335c0$6702a8c0@PFEIFFER>
	<003001c847f7$c7c5eab0$6702a8c0@PFEIFFER>
	<da79af330712261158s40408b48m6a94a2a7b886520a@mail.gmail.com>
Message-ID: <005201c8480a$2a99e260$6702a8c0@PFEIFFER>

this works now,

cbind(summaryBy(B+C~ A,  data=df, FUN=c(mean, var)), 
'numrows'=as.numeric(table(df$A)))


but now, I cannot add another column D beside A, as I do before, like

cbind(summaryBy(B+C~ A+D,  data=df,FUN=c(mean, var)), 
'numrows'=as.numeric(table(df$A)))

because of this D, I got the following error message in german:
<<Fehler in data.frame(..., check.names = FALSE) :  Argumente implizieren 
unterschiedliche Anzahl Zeilen: 21, 18>>

<<error in data.frame(...., check.names = FALSE): arguments has differents 
counts of rows: 21, 18>>

has anybody an idea, what I can do ?




On 26/12/2007, Agrarimmobilien <ralf.pfeiffer at agrarimmobilien.info> wrote:
> Hello,
>
> I had to give some more information to my posting:
>
> as ouput there are more than one row, depending on the values of column A
> (transformierung to distinct values, here 3 and 4, but there are much 
> more)
>
> >        A            B                C
> > 1      3             6                 5
> > 2      3             4                 20
> > 3      3             8                 2
> > 4      4             8                 3
> > 5      4             2                 6
>
> Output:
>
> >        A            B                C                      num rows
> > 1      3            6                 6.16                  3
> > 2      4            5                 4.00                  3
>
>
> B is mean and C variance.
>
>
> thank you
> Macki
>
>
>
> > Hello,
> >
> > it seems to be a simple problem, but  I couldn't find an answer in the
> > archiv. (I think, it must has something to do with the group-select, 
> > like
> > in
> > php)
> >
> > I've the following data.frame:
> >
> >        A            B                C
> > 1      3             6                 5
> > 2      4             4                 20
> > 3      5             8                 2
> >
> > I want to get the number of the rows in the 4th column, like:
> >
> >        A            B                C                      num rows
> > 1      3            6                 6.16                  3
> >
> >
> > (B is mean, C is variance by using summaryBy(B+C~ A,  data=daten,
> > FUN=c(mean,var))  from the doBy-package)
> >
> > But, how can I add the column 'numrow' and count the number of the rows?
> >
> > Thanks!
> >
> > Macki
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From ggrothendieck at gmail.com  Wed Dec 26 23:21:46 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Dec 2007 17:21:46 -0500
Subject: [R] Can you recover default argument values of a function?
In-Reply-To: <BAY108-W340F37C3FDD3F6B35EF7E5AA5B0@phx.gbl>
References: <BAY108-W340F37C3FDD3F6B35EF7E5AA5B0@phx.gbl>
Message-ID: <971536df0712261421m5f508e1fr6977c3b8ebfa6cec@mail.gmail.com>

If the default value is a constant such as in

f <- function(x = 1) x

then formals(f)[[1]] will give it to you but it could be an expression
referring to other variables (other arguments, other variables in
the function, free variables) such as

f <- function(x = u, u) x

or

f <- function(x = u+v+w, u) { v <- u+1; x }

in which case you would get an object of class
"name" in the first case or "call" in the second.


On Dec 26, 2007 4:42 PM, Talbot Katz <topkatz at msn.com> wrote:
>
> Hi.
>
> Maybe this is a stupid question.  If so, I apologize, but here goes.  Suppose I have a function f1(x,...) that calls a function f2(y1,y2,...,yn) in the following way: if x satisfies a certain condition, then I want to call f2(x,y2,...,yn); otherwise I want to use the default value of y1, if there is one.  I could do something like the following:
>
> v <- ifelse ( is.null(x), f2( , y2,..., yn), f2( x, y2,..., yn) )
>
> but I'm doing this in a loop (where the y2,...,yn variables may change), and I'd prefer not to execute the ifelse statement each time, so I'd like an initial pre-loop ifelse such as the following:
>
> y0 <- ifelse ( is.null(x), default(y1), x )
>
> where default(y1) is the default value of the y1 argument of f2.  Then, inside the loop I'd have
>
> v <- f2( y0, y2,..., yn )
>
> Is there any mechanism that tells me how many arguments a function has, and the default values of each one, if there are default values?
>
> Thanks!
>
> --  TMK  --212-460-5430 home917-656-5351 cell
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Roger.Vallejo at ARS.USDA.GOV  Wed Dec 26 23:55:37 2007
From: Roger.Vallejo at ARS.USDA.GOV (Vallejo, Roger)
Date: Wed, 26 Dec 2007 17:55:37 -0500
Subject: [R] Affy Package
Message-ID: <F649B8DBD3DEAD4BB163AC4D9AC18B5FB13C8B@MD-MAIL-01.ARSNET.ARS.USDA.GOV>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071226/d87adaca/attachment.pl 

From roger at ysidro.econ.uiuc.edu  Wed Dec 26 23:56:45 2007
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Wed, 26 Dec 2007 16:56:45 -0600
Subject: [R] Reminiscing on 20 years using S
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBBDB7058@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBBDB7058@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <C85E19E9-FCBE-453F-B98F-360E4689B86F@ysidro.econ.uiuc.edu>


On Dec 26, 2007, at 2:05 PM, Greg Snow wrote:

>
> I realized earlier this year (2007) that it was in 1987 that I first
> started using an early version of S (it was ported to VMS and was  
> called
> success).  That means that I have been using some variant of S (to
> various degrees) for over 20 years now (I don't feel that old).
>

Boxing day somehow seems appropriate for this thread.  R.I.P. to all  
those old boxes
of yesteryore and the software that ran on them -- and yet there is  
always a residual  archaeological curiosity.

I discovered recently that the MIT athena network contains a circa  
1989 version
of S:  http://stuff.mit.edu/afs/athena/astaff/project/Sdev/S/  which  
made me wonder
whether there was any likelihood that one could recreate "S Thu Dec  7  
16:49:47 EST 1989".
Curiosity is one thing, time to dig through the layers of ancient  
civilizations is quite another.
But if anyone would like to offer a (preferably educated) guess about  
the feasibility of  such a project, like I said, I would be curious.


url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


From gunter.berton at gene.com  Thu Dec 27 00:02:25 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 26 Dec 2007 15:02:25 -0800
Subject: [R] Rbind-ing a list into one item
In-Reply-To: <14160363.1588051198705000706.JavaMail.root@vms124.mailsrvcs.net>
References: <14160363.1588051198705000706.JavaMail.root@vms124.mailsrvcs.net>
Message-ID: <000901c84813$618d7c60$6701a8c0@gne.windows.gene.com>

There are at least a couple of "gotchas" that should be noted, however:

1. The files = data.frames can only be rbind-ed (?is this the past tense of
rbind?) if the columns have the same names as well as numbers.

2. The corresponding column classes must be the same for all data.frames.
You may not get errors if this is not true, but silent coercions may end up
changing things from what you expect.

Editorial Comment: I find that both these issues often cause problems in
reading in spreadsheets due to a)typos and inconsistencies in column names
and character columns (especially with spaces and caps)-- which by default
become factors --  and stray characters (e.g., commas or "$" in American
numeric and financial columns). It's very easy for such little problems to
slip into Excel spreadsheets unless the creators are very fastidious, which
most of us aren't. My subjective estimate is that >50% -- and probably
closer to 90% -- of "uncurated" spreadsheets (as opposed to e.g. structured
databases) contain such problems.

Hopefully these issues will not bedevil you.

Cheers,
Bert Gunter
Genentech

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of markleeds at verizon.net
Sent: Wednesday, December 26, 2007 1:37 PM
To: r-help at r-project.org; Kondamani, Arjun (GMI - NY Corporate Bonds)
Subject: Re: [R] Rbind-ing a list into one item

>From: "Kondamani, Arjun (GMI - NY Corporate Bonds)"
<arjun_kondamani at ml.com>
>Date: 2007/12/26 Wed PM 03:23:51 CST
>To: r-help at r-project.org
>Subject: [R] Rbind-ing a list into one item

try do.call(rbind,res)

>Hi,
>
>I am doing the following: 
>
>1. I have a list of files.. Files1=list.files("some
>directory",pattern="some pattern")
>2. I define a list as res=vector("list", length(files1))
>3. I read all the files into this list: res=lapply(files1, read.csv)
>
>I now want to rowbind all the items in the list into one big mass (all
>files have same number of columns). I tried lapply(res, rbind) but that
>did not work. Suggestions?
>--------------------------------------------------------
>
>This message w/attachments (message) may be privileged, confidential or
proprietary, and if you are not an intended recipient, please notify the
sender, do not use or share it and delete it. Unless specifically indicated,
this message is not an offer to sell or a solicitation of any investment
products or other financial product or service, an official confirmation of
any transaction, or an official statement of Merrill Lynch. Subject to
applicable law, Merrill Lynch may monitor, review and retain
e-communications (EC) traveling through its networks/systems. The laws of
the country of each sender/recipient may impact the handling of EC, and EC
may be archived, supervised and produced in countries other than the country
in which you are located. This message cannot be guaranteed to be secure or
error-free. This message is subject to terms available at the following
link: http://www.ml.com/e-communications_terms/. By messaging with Merrill
Lynch you consent to the foregoing.
>--------------------------------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From topkatz at msn.com  Thu Dec 27 00:09:04 2007
From: topkatz at msn.com (Talbot Katz)
Date: Wed, 26 Dec 2007 18:09:04 -0500
Subject: [R] Can you recover default argument values of a function?
In-Reply-To: <971536df0712261421m5f508e1fr6977c3b8ebfa6cec@mail.gmail.com>
References: <BAY108-W340F37C3FDD3F6B35EF7E5AA5B0@phx.gbl>
	<971536df0712261421m5f508e1fr6977c3b8ebfa6cec@mail.gmail.com>
Message-ID: <BAY108-W334F34D5BC6A7DDCB0EB13AA5B0@phx.gbl>


Thank you, that's just what I wanted.  By the way, I found an interesting "gotcha" that can occur with expression arguments:

> x = 7
> z = 2
> mxy <- function( x = 4, y = x + z ) { return(x*y) }
> eval( formals( mxy )[[1]] )
[1] 4
> eval( formals( mxy )[[2]] )
[1] 9
> mxy()
[1] 24
> mxy( eval( formals( mxy )[[1]] ), eval( formals( mxy )[[2]] ) )
[1] 36
>

The problem is "confusion" about whether the "x" in the second argument expression refers to the first argument, or the environment variable.  When the function is evaluated, the argument value of x is used, but when the argument is evaluated (using eval and formals) the environment value of x is used.  This is a reasonable choice, and mixing up arguments and environment variables in a function definition probably should be considered bad programming.

--  TMK  --
212-460-5430	home
917-656-5351	cell


> Date: Wed, 26 Dec 2007 17:21:46 -0500
> From: ggrothendieck at gmail.com
> To: topkatz at msn.com
> Subject: Re: [R] Can you recover default argument values of a function?
> CC: r-help at stat.math.ethz.ch
>
> If the default value is a constant such as in
>
> f <- function(x = 1) x
>
> then formals(f)[[1]] will give it to you but it could be an expression
> referring to other variables (other arguments, other variables in
> the function, free variables) such as
>
> f <- function(x = u, u) x
>
> or
>
> f <- function(x = u+v+w, u) { v <- u+1; x }
>
> in which case you would get an object of class
> "name" in the first case or "call" in the second.
>
>
> On Dec 26, 2007 4:42 PM, Talbot Katz  wrote:
>>
>> Hi.
>>
>> Maybe this is a stupid question. If so, I apologize, but here goes. Suppose I have a function f1(x,...) that calls a function f2(y1,y2,...,yn) in the following way: if x satisfies a certain condition, then I want to call f2(x,y2,...,yn); otherwise I want to use the default value of y1, if there is one. I could do something like the following:
>>
>> v <- ifelse ( is.null(x), f2( , y2,..., yn), f2( x, y2,..., yn) )
>>
>> but I'm doing this in a loop (where the y2,...,yn variables may change), and I'd prefer not to execute the ifelse statement each time, so I'd like an initial pre-loop ifelse such as the following:
>>
>> y0 <- ifelse ( is.null(x), default(y1), x )
>>
>> where default(y1) is the default value of the y1 argument of f2. Then, inside the loop I'd have
>>
>> v <- f2( y0, y2,..., yn )
>>
>> Is there any mechanism that tells me how many arguments a function has, and the default values of each one, if there are default values?
>>
>> Thanks!
>>
>> -- TMK --212-460-5430 home917-656-5351 cell
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From Roger.Vallejo at ARS.USDA.GOV  Thu Dec 27 00:08:41 2007
From: Roger.Vallejo at ARS.USDA.GOV (Vallejo, Roger)
Date: Wed, 26 Dec 2007 18:08:41 -0500
Subject: [R] nlme package
Message-ID: <F649B8DBD3DEAD4BB163AC4D9AC18B5FB13C8C@MD-MAIL-01.ARSNET.ARS.USDA.GOV>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071226/0f0ade13/attachment.pl 

From ggrothendieck at gmail.com  Thu Dec 27 00:27:27 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Dec 2007 18:27:27 -0500
Subject: [R] Can you recover default argument values of a function?
In-Reply-To: <BAY108-W334F34D5BC6A7DDCB0EB13AA5B0@phx.gbl>
References: <BAY108-W340F37C3FDD3F6B35EF7E5AA5B0@phx.gbl>
	<971536df0712261421m5f508e1fr6977c3b8ebfa6cec@mail.gmail.com>
	<BAY108-W334F34D5BC6A7DDCB0EB13AA5B0@phx.gbl>
Message-ID: <971536df0712261527g5a50bf38jc85d4969db16bf3a@mail.gmail.com>

Yes, that is what I was referring to. In fact my second example shows it can be
even worse than in your example since v is computed by f so you
can't get v's value without running f.

On Dec 26, 2007 6:09 PM, Talbot Katz <topkatz at msn.com> wrote:
>
> Thank you, that's just what I wanted.  By the way, I found an interesting "gotcha" that can occur with expression arguments:
>
> > x = 7
> > z = 2
> > mxy <- function( x = 4, y = x + z ) { return(x*y) }
> > eval( formals( mxy )[[1]] )
> [1] 4
> > eval( formals( mxy )[[2]] )
> [1] 9
> > mxy()
> [1] 24
> > mxy( eval( formals( mxy )[[1]] ), eval( formals( mxy )[[2]] ) )
> [1] 36
> >
>
> The problem is "confusion" about whether the "x" in the second argument expression refers to the first argument, or the environment variable.  When the function is evaluated, the argument value of x is used, but when the argument is evaluated (using eval and formals) the environment value of x is used.  This is a reasonable choice, and mixing up arguments and environment variables in a function definition probably should be considered bad programming.
>
> --  TMK  --
> 212-460-5430    home
> 917-656-5351    cell
>
>
> > Date: Wed, 26 Dec 2007 17:21:46 -0500
> > From: ggrothendieck at gmail.com
> > To: topkatz at msn.com
> > Subject: Re: [R] Can you recover default argument values of a function?
> > CC: r-help at stat.math.ethz.ch
> >
> > If the default value is a constant such as in
> >
> > f <- function(x = 1) x
> >
> > then formals(f)[[1]] will give it to you but it could be an expression
> > referring to other variables (other arguments, other variables in
> > the function, free variables) such as
> >
> > f <- function(x = u, u) x
> >
> > or
> >
> > f <- function(x = u+v+w, u) { v <- u+1; x }
> >
> > in which case you would get an object of class
> > "name" in the first case or "call" in the second.
> >
> >
>
> > On Dec 26, 2007 4:42 PM, Talbot Katz  wrote:
> >>
> >> Hi.
> >>
> >> Maybe this is a stupid question. If so, I apologize, but here goes. Suppose I have a function f1(x,...) that calls a function f2(y1,y2,...,yn) in the following way: if x satisfies a certain condition, then I want to call f2(x,y2,...,yn); otherwise I want to use the default value of y1, if there is one. I could do something like the following:
> >>
> >> v <- ifelse ( is.null(x), f2( , y2,..., yn), f2( x, y2,..., yn) )
> >>
> >> but I'm doing this in a loop (where the y2,...,yn variables may change), and I'd prefer not to execute the ifelse statement each time, so I'd like an initial pre-loop ifelse such as the following:
> >>
> >> y0 <- ifelse ( is.null(x), default(y1), x )
> >>
> >> where default(y1) is the default value of the y1 argument of f2. Then, inside the loop I'd have
> >>
> >> v <- f2( y0, y2,..., yn )
> >>
> >> Is there any mechanism that tells me how many arguments a function has, and the default values of each one, if there are default values?
> >>
> >> Thanks!
> >>
> >> -- TMK --212-460-5430 home917-656-5351 cell
> >> [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
>


From tom.soyer at gmail.com  Thu Dec 27 04:55:28 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Wed, 26 Dec 2007 21:55:28 -0600
Subject: [R] questions about plot.zoo
Message-ID: <65cc7bdf0712261955t5e1db722u7bd3e3417b6f5016@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071226/c4cc7a64/attachment.pl 

From ggrothendieck at gmail.com  Thu Dec 27 05:40:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Dec 2007 23:40:41 -0500
Subject: [R] questions about plot.zoo
In-Reply-To: <65cc7bdf0712261955t5e1db722u7bd3e3417b6f5016@mail.gmail.com>
References: <65cc7bdf0712261955t5e1db722u7bd3e3417b6f5016@mail.gmail.com>
Message-ID: <971536df0712262040o2b0681baqb2d2818d6b07ec6c@mail.gmail.com>

On Dec 26, 2007 10:55 PM, tom soyer <tom.soyer at gmail.com> wrote:
> I have been having very good results using plot.zoo to chart time series
>[...]
> (1) when I tried to use semi-log scale, via log="y", R issued a warning,
> although it looked like plot.zoo plotted in semi-log scale anyway:
>
> Warning message:
> In plot.xy(xy.coords(x, y), type = type, ...) :
>  "log" is not a graphical parameter
> Does anyone know if I should ignore the warning, or discard the semi-log
> looking graph instead?

1. plot.zoo is passing the log= argument to the panel and in this case
panel=lines.  You can (a) ignore the warning, (b) use suppressWarnings
or (c) use a custom panel which excludes log:

lines2 <- function(...) { L <- list(...); L$log <- NULL; do.call(lines, L) }
z <- zoo(1:100)
plot(z, log = "y", panel = lines2)

> (2) I noticed that plot.zoo automatically generates major tick marks for the
> user. But does anyone know how to customize the major tick marks on the
> y-axis, and adding minor tick marks between them? I tried both axis() and
> minor.tick in the Hmisc package, but could not make them work. It seems that
> axis() and minor.tick do not work well with dates.

2.  If you are using a single panel then try this:
plot(z, yaxt = "n")
axis(2, c(1, 10, 100))

>
> (3) I tried to add horizontal grid like this:
>
> x=axis(4)
> abline(h=x,col="light gray")
>
> It works... but unfortunately, the grid lines are all drawn on top, or in
> front, of the lines or bars of data on the chart. As a result, the grid
> lines cover up the lines of data when they cross each other. Does anyone
> know if it is possible to specify the order of lines on a chart, i.e.,
> something like the z-index, so that the grid lines are behind the data
> lines? If not, then what would be the proper way of adding gridlines so that
> thay are consistent with the major and minor tick marks?

Try using xyplot.zoo.  See:
  ?xyplot.zoo
for an example which specifically shows how to do it.


From tom.soyer at gmail.com  Thu Dec 27 06:28:03 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Wed, 26 Dec 2007 23:28:03 -0600
Subject: [R] questions about plot.zoo
In-Reply-To: <971536df0712262040o2b0681baqb2d2818d6b07ec6c@mail.gmail.com>
References: <65cc7bdf0712261955t5e1db722u7bd3e3417b6f5016@mail.gmail.com>
	<971536df0712262040o2b0681baqb2d2818d6b07ec6c@mail.gmail.com>
Message-ID: <65cc7bdf0712262128j6e674a49ibdde22a7139d6e22@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071226/b251879a/attachment.pl 

From ggrothendieck at gmail.com  Thu Dec 27 06:33:26 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Dec 2007 00:33:26 -0500
Subject: [R] questions about plot.zoo
In-Reply-To: <65cc7bdf0712262128j6e674a49ibdde22a7139d6e22@mail.gmail.com>
References: <65cc7bdf0712261955t5e1db722u7bd3e3417b6f5016@mail.gmail.com>
	<971536df0712262040o2b0681baqb2d2818d6b07ec6c@mail.gmail.com>
	<65cc7bdf0712262128j6e674a49ibdde22a7139d6e22@mail.gmail.com>
Message-ID: <971536df0712262133t2a267362kee440ab0b7ffec80@mail.gmail.com>

There is an example in the example section of ?plot.zoo .  That
example is for a multipanel plot but the same idea works for
a single panel plot -- in that case you don't have to define a panel
but can just use axis directly.

On Dec 27, 2007 12:28 AM, tom soyer <tom.soyer at gmail.com> wrote:
> Thanks Gabor!
>
> Also, can you give an example of customizing the major tick marks on the
> x-axis and adding minor tick marks between major tick marks?
>
>
>
> On 12/26/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On Dec 26, 2007 10:55 PM, tom soyer <tom.soyer at gmail.com> wrote:
> > > I have been having very good results using plot.zoo to chart time series
> > >[...]
> > > (1) when I tried to use semi-log scale, via log="y", R issued a warning,
> > > although it looked like plot.zoo plotted in semi-log scale anyway:
> > >
> > > Warning message:
> > > In plot.xy(xy.coords(x, y), type = type, ...) :
> > >  "log" is not a graphical parameter
> > > Does anyone know if I should ignore the warning, or discard the semi-log
> > > looking graph instead?
> >
> > 1. plot.zoo is passing the log= argument to the panel and in this case
> > panel=lines.  You can (a) ignore the warning, (b) use suppressWarnings
> > or (c) use a custom panel which excludes log:
> >
> > lines2 <- function(...) { L <- list(...); L$log <- NULL; do.call(lines, L)
> }
> > z <- zoo(1:100)
> > plot(z, log = "y", panel = lines2)
> >
> > > (2) I noticed that plot.zoo automatically generates major tick marks for
> the
> > > user. But does anyone know how to customize the major tick marks on the
> > > y-axis, and adding minor tick marks between them? I tried both axis()
> and
> > > minor.tick in the Hmisc package, but could not make them work. It seems
> that
> > > axis() and minor.tick do not work well with dates.
> >
> > 2.  If you are using a single panel then try this:
> > plot(z, yaxt = "n")
> > axis(2, c(1, 10, 100))
> >
> > >
> > > (3) I tried to add horizontal grid like this:
> > >
> > > x=axis(4)
> > > abline(h=x,col="light gray")
> > >
> > > It works... but unfortunately, the grid lines are all drawn on top, or
> in
> > > front, of the lines or bars of data on the chart. As a result, the grid
> > > lines cover up the lines of data when they cross each other. Does anyone
> > > know if it is possible to specify the order of lines on a chart, i.e.,
> > > something like the z-index, so that the grid lines are behind the data
> > > lines? If not, then what would be the proper way of adding gridlines so
> that
> > > thay are consistent with the major and minor tick marks?
> >
> > Try using xyplot.zoo.  See:
> > ?xyplot.zoo
> > for an example which specifically shows how to do it.
> >
>
>
>
> --
> Tom


From ronggui.huang at gmail.com  Thu Dec 27 09:19:00 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 27 Dec 2007 16:19:00 +0800
Subject: [R] Problem of lmer under FreeBSD
Message-ID: <38b9f0350712270019h59649764n8091f410b0d82476@mail.gmail.com>

I encounter such problem with lmer under FreeBSD, but not under
Windows. Anyone knows why? Thanks.

> example(lmer)

lmer> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
Error in UseMethod("as.logical") : no applicable method for "as.logical"
> traceback()
9: as.logical(EMverbose)
8: as.logical(EMverbose)
7: lmerControl()
6: do.call("lmerControl", control)
5: lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
4: eval.with.vis(expr, envir, enclos)
3: eval.with.vis(ei, envir)
2: source(zfile, local, echo = echo, prompt.echo = paste(prompt.prefix,
       getOption("prompt"), sep = ""), continue.echo = paste(prompt.prefix,
       getOption("continue"), sep = ""), verbose = verbose,
max.deparse.length = Inf,
       encoding = encoding, skip.echo = skips)
1: example(lmer)
>

> sessionInfo()
R version 2.6.0 (2007-10-03)
i386-portbld-freebsd6.3

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.99875-9    Matrix_0.999375-2 lattice_0.16-5

loaded via a namespace (and not attached):
[1] grid_2.6.0      rcompgen_0.1-15 tools_2.6.0

-- 
Ronggui Huang

Department of Sociology, Fudan University, Shanghai, China

Department of Public and Social Administration, CityU, HK


From ronggui.huang at gmail.com  Thu Dec 27 09:19:00 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 27 Dec 2007 16:19:00 +0800
Subject: [R] Problem of lmer under FreeBSD
Message-ID: <38b9f0350712270019h59649764n8091f410b0d82476@mail.gmail.com>

I encounter such problem with lmer under FreeBSD, but not under
Windows. Anyone knows why? Thanks.

> example(lmer)

lmer> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
Error in UseMethod("as.logical") : no applicable method for "as.logical"
> traceback()
9: as.logical(EMverbose)
8: as.logical(EMverbose)
7: lmerControl()
6: do.call("lmerControl", control)
5: lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
4: eval.with.vis(expr, envir, enclos)
3: eval.with.vis(ei, envir)
2: source(zfile, local, echo = echo, prompt.echo = paste(prompt.prefix,
       getOption("prompt"), sep = ""), continue.echo = paste(prompt.prefix,
       getOption("continue"), sep = ""), verbose = verbose,
max.deparse.length = Inf,
       encoding = encoding, skip.echo = skips)
1: example(lmer)
>

> sessionInfo()
R version 2.6.0 (2007-10-03)
i386-portbld-freebsd6.3

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.99875-9    Matrix_0.999375-2 lattice_0.16-5

loaded via a namespace (and not attached):
[1] grid_2.6.0      rcompgen_0.1-15 tools_2.6.0

-- 
Ronggui Huang

Department of Sociology, Fudan University, Shanghai, China

Department of Public and Social Administration, CityU, HK


From maura.monville at gmail.com  Thu Dec 27 09:26:58 2007
From: maura.monville at gmail.com (Maura E Monville)
Date: Thu, 27 Dec 2007 02:26:58 -0600
Subject: [R] Error: cannot allocate vector of size ... Bcc: Add Cc | Add Bcc
Message-ID: <36d691950712270026w76d3ea32r9910bab5c8bf6db6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/10bc5fdc/attachment.pl 

From p.dalgaard at biostat.ku.dk  Thu Dec 27 09:48:23 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 27 Dec 2007 09:48:23 +0100
Subject: [R] Can you recover default argument values of a function?
In-Reply-To: <BAY108-W334F34D5BC6A7DDCB0EB13AA5B0@phx.gbl>
References: <BAY108-W340F37C3FDD3F6B35EF7E5AA5B0@phx.gbl>	<971536df0712261421m5f508e1fr6977c3b8ebfa6cec@mail.gmail.com>
	<BAY108-W334F34D5BC6A7DDCB0EB13AA5B0@phx.gbl>
Message-ID: <477366D7.3090100@biostat.ku.dk>

Talbot Katz wrote:
> Thank you, that's just what I wanted.  By the way, I found an interesting "gotcha" that can occur with expression arguments:
>
>   
>> x = 7
>> z = 2
>> mxy <- function( x = 4, y = x + z ) { return(x*y) }
>> eval( formals( mxy )[[1]] )
>>     
> [1] 4
>   
>> eval( formals( mxy )[[2]] )
>>     
> [1] 9
>   
>> mxy()
>>     
> [1] 24
>   
>> mxy( eval( formals( mxy )[[1]] ), eval( formals( mxy )[[2]] ) )
>>     
> [1] 36
>   
>
> The problem is "confusion" about whether the "x" in the second argument expression refers to the first argument, or the environment variable.  When the function is evaluated, the argument value of x is used, but when the argument is evaluated (using eval and formals) the environment value of x is used.  This is a reasonable choice, and mixing up arguments and environment variables in a function definition probably should be considered bad programming.
>
>   
Yes, argument expressions are always evaluated in the evaluation frame 
of the function. Sometimes they even refer to quantities computed well 
into the evaluation of the function, like "p" in

 > anova.mlm
function (object, ..., test = c("Pillai", "Wilks", "Hotelling-Lawley",
    "Roy", "Spherical"), Sigma = diag(nrow = p), T = Thin.row(proj(M) -
    proj(X)), M = diag(nrow = p), X = ~0, idata = data.frame(index = 
seq_len(p)))
.....
        p <- ncol(SSD(object)$SSD)
.....

(p is the dimension of the covariance matrix, aka the number of columns 
in the response matrix. It makes no sense to pass it as a separate 
parameter, and obtaining it via extraction from "object" is kludgy and 
inefficient as it is needed three times.)

Another gotcha is if you modify a variable referred to in a default 
expression before using the expression.

The upshot is that you pretty much cannot in general figure out what the 
argument defaults will evaluate to without actually running the function.


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From haitian at ust.hk  Thu Dec 27 10:20:08 2007
From: haitian at ust.hk (Maggie Wang)
Date: Thu, 27 Dec 2007 17:20:08 +0800
Subject: [R] (package e1071) SVM tune for best parameters: why they are
	different everytime i run?
Message-ID: <2e7931b00712270120v49691ba0x507b5ef80e849a5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/987e9d84/attachment.pl 

From ligges at statistik.uni-dortmund.de  Thu Dec 27 11:04:55 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 27 Dec 2007 11:04:55 +0100
Subject: [R] Affy Package
In-Reply-To: <F649B8DBD3DEAD4BB163AC4D9AC18B5FB13C8B@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
References: <F649B8DBD3DEAD4BB163AC4D9AC18B5FB13C8B@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
Message-ID: <477378C7.3000208@statistik.uni-dortmund.de>



Vallejo, Roger wrote:
> Dear R Users,
> In the expresso function, which combination of these methods for data pre-processing (when using affymetrix oligo arrays) is the best:
> 
> bgcorrect.metod = rma rma2 mas
> 
> normalize.method = qspline quantiles loess
> 
> pmcorrect.method = pmonly subtractmm mas
> 
> summary.method = liwong avgdiff medianpolish mas
> 
> There are many options within each method. I would appreciate a hint on the best combination. 


There is more than one implemented, hence we can guess that there is no 
generally best one among these, i.e. it depends on your data what to use.

Uwe Ligges


> Thank you very much in advance.
> Roger
>  
>  
> Roger L. Vallejo, Ph.D.
> Computational Biologist & Geneticist
> U.S. Department of Agriculture, ARS
> National Center for Cool & Cold Water Aquaculture
> 11861 Leetown Road
> Kearneysville, WV 25430
> Voice:  (304) 724-8340 Ext. 2141
> Email:  roger.vallejo at ars.usda.gov
> http://www.ars.usda.gov/pandp/people/people.htm?personid=37662
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Thu Dec 27 11:06:46 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 27 Dec 2007 11:06:46 +0100
Subject: [R] Affy Package
In-Reply-To: <477378C7.3000208@statistik.uni-dortmund.de>
References: <F649B8DBD3DEAD4BB163AC4D9AC18B5FB13C8B@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
	<477378C7.3000208@statistik.uni-dortmund.de>
Message-ID: <47737936.2020002@statistik.uni-dortmund.de>

Damn, this was a cross-post (and I replied to all), apologies for my 
re-cross-posting.

To Roger, the original poster: PLEASE do read the posting guide of 
R-help before posting and NEVER EVER do crossposting again.

Thanks,
Uwe Ligges

Uwe Ligges wrote:
> 
> 
> Vallejo, Roger wrote:
>> Dear R Users,
>> In the expresso function, which combination of these methods for data 
>> pre-processing (when using affymetrix oligo arrays) is the best:
>>
>> bgcorrect.metod = rma rma2 mas
>>
>> normalize.method = qspline quantiles loess
>>
>> pmcorrect.method = pmonly subtractmm mas
>>
>> summary.method = liwong avgdiff medianpolish mas
>>
>> There are many options within each method. I would appreciate a hint 
>> on the best combination. 
> 
> 
> There is more than one implemented, hence we can guess that there is no 
> generally best one among these, i.e. it depends on your data what to use.
> 
> Uwe Ligges
> 
> 
>> Thank you very much in advance.
>> Roger
>>  
>>  
>> Roger L. Vallejo, Ph.D.
>> Computational Biologist & Geneticist
>> U.S. Department of Agriculture, ARS
>> National Center for Cool & Cold Water Aquaculture
>> 11861 Leetown Road
>> Kearneysville, WV 25430
>> Voice:  (304) 724-8340 Ext. 2141
>> Email:  roger.vallejo at ars.usda.gov
>> http://www.ars.usda.gov/pandp/people/people.htm?personid=37662
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Thu Dec 27 11:10:16 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 27 Dec 2007 11:10:16 +0100
Subject: [R] nlme package
In-Reply-To: <F649B8DBD3DEAD4BB163AC4D9AC18B5FB13C8C@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
References: <F649B8DBD3DEAD4BB163AC4D9AC18B5FB13C8C@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
Message-ID: <47737A08.5010404@statistik.uni-dortmund.de>


AGAIN, please do not crosspost!


Vallejo, Roger wrote:
> In using the NLME package (R 2.6.1 for Windows),

Which version of nlme? nlme is a contributed package that can be updated 
independently of R releases.


> I am having a problem in running an R script that used to run with no problems using a Linux OS in 2004. So I am wondering if during these last ~3 yrs we had major changes in the syntax of the NLME package that I am not aware. 

For sure you are not aware. And which version of nlme have you used in 2004?


> This is the R script:
>  
> library(nlme)
>  
> treat=as.factor(c(1,2,1,2,1,2,1,2))
> mouse=as.factor(c(1,1,2,2,3,3,4,4))
> time=as.factor(c(1,1,1,1,2,2,2,2))
> 
> mouse.anova=function(vec){lme(vec~treat+time+treat*time,random=~1|mouse/time)} 
>  
> lme.out=apply(exprmat,1,mouse.anova)


Well, out error message is:

Error in apply(exprmat, 1, mouse.anova) : object "exprmat" not found


Guess why ... and re-read the posting guide, please.


Uwe Ligges



> This is the ERROR message:
>  
> Error in lme.formula(vec ~ treat + time + treat * time, random = ~1 |  : 
>   nlminb problem, convergence error code = 1
>   message = singular convergence (7)
>  
> I will appreciate your help in pointing mistakes in this R script. 
> Thank you very much.
> Roger
> 
> Roger L. Vallejo, Ph.D.
> Computational Biologist & Geneticist
> U.S. Department of Agriculture, ARS
> National Center for Cool & Cold Water Aquaculture
> 11861 Leetown Road
> Kearneysville, WV 25430
> Voice:  (304) 724-8340 Ext. 2141
> Email:  roger.vallejo at ars.usda.gov
> http://www.ars.usda.gov/pandp/people/people.htm?personid=37662
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Thu Dec 27 11:14:24 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 27 Dec 2007 11:14:24 +0100
Subject: [R] Error: cannot allocate vector of size ...
In-Reply-To: <36d691950712270026w76d3ea32r9910bab5c8bf6db6@mail.gmail.com>
References: <36d691950712270026w76d3ea32r9910bab5c8bf6db6@mail.gmail.com>
Message-ID: <47737B00.1060504@statistik.uni-dortmund.de>



Maura E Monville wrote:
> I read the subject message in a number of R archived emails.
> Since I am experiencing the same problem:
> 
>> upfmla
> A ~ T + cosP + cos2P + cos4P + cos5P + sin3P + sin5P + cosP2 +
>     sinP3 + P2
>> glmod <- gls(upfmla,correlation=corAR1(),method="ML")
> Error: cannot allocate vector of size 491.3 Mb
> 
>> dim(xx)
> [1] 8025   14

How much memory is consumed in your workspace? If so, remove huge 
objects from your workspace
How big is upfmla? Are there factors in it? If so, how many levels? Then 
you can calculate the size of your design matrix.
64-bit is nice, but how much RAM is in your machine?

Uwe Ligges


> and since I'm running R on a 64-bit Linux platform ... I wonder whether this
> problem has
> been solved or I'm better off giving up the Generalized Least Squares
> models.
> Would it make sense to break the data into two halves and fit one half at a
> time ?
> 
> Thank you .
> 
> Happy new year,


From wwwhsd at gmail.com  Thu Dec 27 11:16:46 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 27 Dec 2007 08:16:46 -0200
Subject: [R] data.frame - how to calculate the number of rows
In-Reply-To: <005201c8480a$2a99e260$6702a8c0@PFEIFFER>
References: <004701c847f2$ae9335c0$6702a8c0@PFEIFFER>
	<003001c847f7$c7c5eab0$6702a8c0@PFEIFFER>
	<da79af330712261158s40408b48m6a94a2a7b886520a@mail.gmail.com>
	<005201c8480a$2a99e260$6702a8c0@PFEIFFER>
Message-ID: <da79af330712270216i68206f35v7a5f0c4316a29773@mail.gmail.com>

Try this:

summaryBy(B+C~ A+D,  data=daten,FUN=c(mean, var, length))

On 26/12/2007, Agrarimmobilien <ralf.pfeiffer at agrarimmobilien.info> wrote:
> this works now,
>
> cbind(summaryBy(B+C~ A,  data=df, FUN=c(mean, var)),
> 'numrows'=as.numeric(table(df$A)))
>
>
> but now, I cannot add another column D beside A, as I do before, like
>
> cbind(summaryBy(B+C~ A+D,  data=df,FUN=c(mean, var)),
> 'numrows'=as.numeric(table(df$A)))
>
> because of this D, I got the following error message in german:
> <<Fehler in data.frame(..., check.names = FALSE) :  Argumente implizieren
> unterschiedliche Anzahl Zeilen: 21, 18>>
>
> <<error in data.frame(...., check.names = FALSE): arguments has differents
> counts of rows: 21, 18>>
>
> has anybody an idea, what I can do ?
>
>
>
>
> On 26/12/2007, Agrarimmobilien <ralf.pfeiffer at agrarimmobilien.info> wrote:
> > Hello,
> >
> > I had to give some more information to my posting:
> >
> > as ouput there are more than one row, depending on the values of column A
> > (transformierung to distinct values, here 3 and 4, but there are much
> > more)
> >
> > >        A            B                C
> > > 1      3             6                 5
> > > 2      3             4                 20
> > > 3      3             8                 2
> > > 4      4             8                 3
> > > 5      4             2                 6
> >
> > Output:
> >
> > >        A            B                C                      num rows
> > > 1      3            6                 6.16                  3
> > > 2      4            5                 4.00                  3
> >
> >
> > B is mean and C variance.
> >
> >
> > thank you
> > Macki
> >
> >
> >
> > > Hello,
> > >
> > > it seems to be a simple problem, but  I couldn't find an answer in the
> > > archiv. (I think, it must has something to do with the group-select,
> > > like
> > > in
> > > php)
> > >
> > > I've the following data.frame:
> > >
> > >        A            B                C
> > > 1      3             6                 5
> > > 2      4             4                 20
> > > 3      5             8                 2
> > >
> > > I want to get the number of the rows in the 4th column, like:
> > >
> > >        A            B                C                      num rows
> > > 1      3            6                 6.16                  3
> > >
> > >
> > > (B is mean, C is variance by using summaryBy(B+C~ A,  data=daten,
> > > FUN=c(mean,var))  from the doBy-package)
> > >
> > > But, how can I add the column 'numrow' and count the number of the rows?
> > >
> > > Thanks!
> > >
> > > Macki
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Henrique Dallazuanna
> Curitiba-Paran?-Brasil
> 25? 25' 40" S 49? 16' 22" O
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From ligges at statistik.uni-dortmund.de  Thu Dec 27 11:17:44 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 27 Dec 2007 11:17:44 +0100
Subject: [R] (package e1071) SVM tune for best parameters: why they are
 different everytime i run?
In-Reply-To: <2e7931b00712270120v49691ba0x507b5ef80e849a5@mail.gmail.com>
References: <2e7931b00712270120v49691ba0x507b5ef80e849a5@mail.gmail.com>
Message-ID: <47737BC8.8030502@statistik.uni-dortmund.de>



Maggie Wang wrote:
> Hi,
> 
> I run the following tuning function for svm. It's very strange that every
> time i run this function, the best.parameters give different values.
> 
> [A]
> 
>> svm.tune <- tune(svm, train.x, train.y,
> 
>                     validation.x=train.x, validation.y=train.y,
> 
>                  ranges = list(gamma = 2^(-1:2),
> 
>                  cost = 2^(-3:2)))
> 
> 
> 
> # where train.x and train.y are matrix specified.
> 
> 
> 
> # output command:
> 
> 
> 
>> svm.tune$best.parameters$cost
> 
>> svm.tune$best.parameters$gamma
> 
> 
> 
> result:
> 
>  cost gamma
>  0.25  4.00
> 
> 
> 
> run A again:
> 
>  cost gamma
>     1     4
> 
> 
> 
> again:
> 
>   cost gamma
>  0.25  4.00
> 
> 
> 
> The result is so unstable, if it varies so much, why do we need to tune? Do
> you know if this behavior is normal? Can we trust the best.parameters for
> prediction?

I guess you do not have really many observations in your dataset. Then 
it highly depends ion the cross validation sets which parameter is best. 
And therefore you get quite different results.

Uwe Ligges



> 
> 
> Thank you so much to help out!!
> 
> 
> 
> Best Regards,
> 
> Maggie
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wwwhsd at gmail.com  Thu Dec 27 11:23:07 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 27 Dec 2007 08:23:07 -0200
Subject: [R] how to remove paths from where library() looks for packages
In-Reply-To: <477065E0.9090709@lba.com>
References: <477065E0.9090709@lba.com>
Message-ID: <da79af330712270223n5feac4c7x43fbf515495c168f@mail.gmail.com>

Perhaps this:

.libPaths(unique(installed.packages()[,2]))

On 25/12/2007, Lynd Bacon <lbacon at lba.com> wrote:
> Greetings.  My current installation of R 2.6.1 on Unbuntu 7.10 ("Gutsy")
> is looking for packages in a directory that doesn't contain any:
>
>  > library()
> Warning message:
> In library() : library "/usr/lib/R/site-library" contains no packages
>
>  > .libPaths()
> [1] "/usr/local/lib/R/site-library" "/usr/lib/R/site-library"
> [3] "/usr/lib/R/library"
>
> I'd like to eliminate the path to the empty directory so I don't get
> this vexsome warning message.  Any and all suggestions will be greatly
> appreciated!
>
> Cheers,
>
> Lynd
>
> --
> _________________________________________
> Lynd Bacon
> Woodside CA USA GMT-08:00
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From haitian at ust.hk  Thu Dec 27 11:39:07 2007
From: haitian at ust.hk (Maggie Wang)
Date: Thu, 27 Dec 2007 18:39:07 +0800
Subject: [R] (package e1071) SVM tune for best parameters: why they are
	different everytime i run?
In-Reply-To: <47737BC8.8030502@statistik.uni-dortmund.de>
References: <2e7931b00712270120v49691ba0x507b5ef80e849a5@mail.gmail.com>
	<47737BC8.8030502@statistik.uni-dortmund.de>
Message-ID: <2e7931b00712270239q3a54704ew933268a22c103709@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/360acf29/attachment.pl 

From ligges at statistik.uni-dortmund.de  Thu Dec 27 11:43:32 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 27 Dec 2007 11:43:32 +0100
Subject: [R] (package e1071) SVM tune for best parameters: why they are
 different everytime i run?
In-Reply-To: <2e7931b00712270239q3a54704ew933268a22c103709@mail.gmail.com>
References: <2e7931b00712270120v49691ba0x507b5ef80e849a5@mail.gmail.com>	
	<47737BC8.8030502@statistik.uni-dortmund.de>
	<2e7931b00712270239q3a54704ew933268a22c103709@mail.gmail.com>
Message-ID: <477381D4.5050701@statistik.uni-dortmund.de>



Maggie Wang wrote:
> Hi, Uwe,
> 
> Thanks for the reply!!  I have 87 observations in total. If this amount
> causes the different best.parameters, is there a better way than cross
> validation to tune them?


In order to get stable (I do not say "best") results, you could try some 
bootstrap with many replications or leave-one-out crossvalidation.

Uwe

> Thank you so much for the help!
> 
> Best Regards,
> Maggie
> 
> On Dec 27, 2007 6:17 PM, Uwe Ligges <ligges at statistik.uni-dortmund.de>
> wrote:
> 
>>
>> Maggie Wang wrote:
>>> Hi,
>>>
>>> I run the following tuning function for svm. It's very strange that
>> every
>>> time i run this function, the best.parameters give different values.
>>>
>>> [A]
>>>
>>>> svm.tune <- tune(svm, train.x, train.y,
>>>                     validation.x=train.x, validation.y=train.y,
>>>
>>>                  ranges = list(gamma = 2^(-1:2),
>>>
>>>                  cost = 2^(-3:2)))
>>>
>>>
>>>
>>> # where train.x and train.y are matrix specified.
>>>
>>>
>>>
>>> # output command:
>>>
>>>
>>>
>>>> svm.tune$best.parameters$cost
>>>> svm.tune$best.parameters$gamma
>>>
>>>
>>> result:
>>>
>>>  cost gamma
>>>  0.25  4.00
>>>
>>>
>>>
>>> run A again:
>>>
>>>  cost gamma
>>>     1     4
>>>
>>>
>>>
>>> again:
>>>
>>>   cost gamma
>>>  0.25  4.00
>>>
>>>
>>>
>>> The result is so unstable, if it varies so much, why do we need to tune?
>> Do
>>> you know if this behavior is normal? Can we trust the best.parametersfor
>>> prediction?
>> I guess you do not have really many observations in your dataset. Then
>> it highly depends ion the cross validation sets which parameter is best.
>> And therefore you get quite different results.
>>
>> Uwe Ligges
>>
>>
>>
>>>
>>> Thank you so much to help out!!
>>>
>>>
>>>
>>> Best Regards,
>>>
>>> Maggie
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>>> and provide commented, minimal, self-contained, reproducible code.
>


From haitian at ust.hk  Thu Dec 27 11:49:31 2007
From: haitian at ust.hk (Maggie Wang)
Date: Thu, 27 Dec 2007 18:49:31 +0800
Subject: [R] (package e1071) SVM tune for best parameters: why they are
	different everytime i run?
In-Reply-To: <477381D4.5050701@statistik.uni-dortmund.de>
References: <2e7931b00712270120v49691ba0x507b5ef80e849a5@mail.gmail.com>
	<47737BC8.8030502@statistik.uni-dortmund.de>
	<2e7931b00712270239q3a54704ew933268a22c103709@mail.gmail.com>
	<477381D4.5050701@statistik.uni-dortmund.de>
Message-ID: <2e7931b00712270249w64d0df24m15a0ec676a1c2b8c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/ae9863e7/attachment.pl 

From ralf.pfeiffer at agrarimmobilien.info  Thu Dec 27 11:53:29 2007
From: ralf.pfeiffer at agrarimmobilien.info (Agrarimmobilien)
Date: Thu, 27 Dec 2007 11:53:29 +0100
Subject: [R] data.frame - how to calculate the number of rows
References: <004701c847f2$ae9335c0$6702a8c0@PFEIFFER>
	<003001c847f7$c7c5eab0$6702a8c0@PFEIFFER>
	<da79af330712261158s40408b48m6a94a2a7b886520a@mail.gmail.com>
	<005201c8480a$2a99e260$6702a8c0@PFEIFFER>
	<da79af330712270216i68206f35v7a5f0c4316a29773@mail.gmail.com>
Message-ID: <002201c84876$b7c57110$6402a8c0@PFEIFFER>

now it works again,

length gave the same resultat as 'numrows'=as.numeric(table(df$A)))

thanks for your help

Macki

----- Original Message ----- 
From: "Henrique Dallazuanna" <wwwhsd at gmail.com>
To: "Agrarimmobilien" <ralf.pfeiffer at agrarimmobilien.info>
Cc: <r-help at r-project.org>
Sent: Thursday, December 27, 2007 11:16 AM
Subject: Re: [R] data.frame - how to calculate the number of rows


Try this:

summaryBy(B+C~ A+D,  data=daten,FUN=c(mean, var, length))

On 26/12/2007, Agrarimmobilien <ralf.pfeiffer at agrarimmobilien.info> wrote:
> this works now,
>
> cbind(summaryBy(B+C~ A,  data=df, FUN=c(mean, var)),
> 'numrows'=as.numeric(table(df$A)))
>
>
> but now, I cannot add another column D beside A, as I do before, like
>
> cbind(summaryBy(B+C~ A+D,  data=df,FUN=c(mean, var)),
> 'numrows'=as.numeric(table(df$A)))
>
> because of this D, I got the following error message in german:
> <<Fehler in data.frame(..., check.names = FALSE) :  Argumente implizieren
> unterschiedliche Anzahl Zeilen: 21, 18>>
>
> <<error in data.frame(...., check.names = FALSE): arguments has differents
> counts of rows: 21, 18>>
>
> has anybody an idea, what I can do ?
>
>
>
>
> On 26/12/2007, Agrarimmobilien <ralf.pfeiffer at agrarimmobilien.info> wrote:
> > Hello,
> >
> > I had to give some more information to my posting:
> >
> > as ouput there are more than one row, depending on the values of column 
> > A
> > (transformierung to distinct values, here 3 and 4, but there are much
> > more)
> >
> > >        A            B                C
> > > 1      3             6                 5
> > > 2      3             4                 20
> > > 3      3             8                 2
> > > 4      4             8                 3
> > > 5      4             2                 6
> >
> > Output:
> >
> > >        A            B                C                      num rows
> > > 1      3            6                 6.16                  3
> > > 2      4            5                 4.00                  3
> >
> >
> > B is mean and C variance.
> >
> >
> > thank you
> > Macki
> >
> >
> >
> > > Hello,
> > >
> > > it seems to be a simple problem, but  I couldn't find an answer in the
> > > archiv. (I think, it must has something to do with the group-select,
> > > like
> > > in
> > > php)
> > >
> > > I've the following data.frame:
> > >
> > >        A            B                C
> > > 1      3             6                 5
> > > 2      4             4                 20
> > > 3      5             8                 2
> > >
> > > I want to get the number of the rows in the 4th column, like:
> > >
> > >        A            B                C                      num rows
> > > 1      3            6                 6.16                  3
> > >
> > >
> > > (B is mean, C is variance by using summaryBy(B+C~ A,  data=daten,
> > > FUN=c(mean,var))  from the doBy-package)
> > >
> > > But, how can I add the column 'numrow' and count the number of the 
> > > rows?
> > >
> > > Thanks!
> > >
> > > Macki
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Henrique Dallazuanna
> Curitiba-Paran?-Brasil
> 25? 25' 40" S 49? 16' 22" O
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From rhurlin at gwdg.de  Thu Dec 27 12:05:11 2007
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Thu, 27 Dec 2007 12:05:11 +0100
Subject: [R] Problem of lmer under FreeBSD
In-Reply-To: <38b9f0350712270019h59649764n8091f410b0d82476@mail.gmail.com>
References: <38b9f0350712270019h59649764n8091f410b0d82476@mail.gmail.com>
Message-ID: <477386E7.8010805@gwdg.de>

Hello Ronggui Huang,

I am working with FreeBSD 8.0-CURRENT (i386) and R-2.6.1 (not built from 
ports). After loading version 0.99875-9 of lme4 and then executing the 
example

   library(lme4)
   example(lmer)

all works fine for me.


On 27.12.2007 09:19 (UTC+1), Ronggui wrote:
> I encounter such problem with lmer under FreeBSD, but not under
> Windows. Anyone knows why? Thanks.

Assuming, that your R installation is correct (from math/R ?), you 
should consider to reinstall lme4 and its dependencies. It is possible 
that these package have been built on a prior version of R.

For installing the package 'Matrix' on FreeBSD you have to use
gmake instead of make. The following example will do it

setenv MAKE gmake
R CMD INSTALL Matrix_0.999375-3.tar.gz
unsetenv MAKE

If reinstalling packages not solves your problem, please try if 
reinstalling R does. (Under FreeBSD the R sources build and install 
fine, so there is no need to use the port at math/R ...)

Hope this helps,
Rainer


>> example(lmer)
> 
> lmer> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> Error in UseMethod("as.logical") : no applicable method for "as.logical"
>> traceback()
> 9: as.logical(EMverbose)
> 8: as.logical(EMverbose)
> 7: lmerControl()
> 6: do.call("lmerControl", control)
> 5: lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
> 4: eval.with.vis(expr, envir, enclos)
> 3: eval.with.vis(ei, envir)
> 2: source(zfile, local, echo = echo, prompt.echo = paste(prompt.prefix,
>        getOption("prompt"), sep = ""), continue.echo = paste(prompt.prefix,
>        getOption("continue"), sep = ""), verbose = verbose,
> max.deparse.length = Inf,
>        encoding = encoding, skip.echo = skips)
> 1: example(lmer)
> 
>> sessionInfo()
> R version 2.6.0 (2007-10-03)
> i386-portbld-freebsd6.3
> 
> locale:
> C
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] lme4_0.99875-9    Matrix_0.999375-2 lattice_0.16-5
> 
> loaded via a namespace (and not attached):
> [1] grid_2.6.0      rcompgen_0.1-15 tools_2.6.0


From ronggui.huang at gmail.com  Thu Dec 27 13:07:49 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 27 Dec 2007 20:07:49 +0800
Subject: [R] Problem of lmer under FreeBSD
In-Reply-To: <477386E7.8010805@gwdg.de>
References: <38b9f0350712270019h59649764n8091f410b0d82476@mail.gmail.com>
	<477386E7.8010805@gwdg.de>
Message-ID: <38b9f0350712270407x699e08d5j6040d13198fd66e4@mail.gmail.com>

Dear Rainer,

Thanks for your suggestion.

The problem lies in the Matrix package. I installed it under R-2.5.0.
When I upgraded R to R-2.6.0, I didn't upgrade all the add-on
packages. I reinstall Matrix and lme4 packages, things are going fine.

Regards

Ronggui HUANG

2007/12/27, Rainer Hurling <rhurlin at gwdg.de>:
> Hello Ronggui Huang,
>
> I am working with FreeBSD 8.0-CURRENT (i386) and R-2.6.1 (not built from
> ports). After loading version 0.99875-9 of lme4 and then executing the
> example
>
>    library(lme4)
>    example(lmer)
>
> all works fine for me.
>
>
> On 27.12.2007 09:19 (UTC+1), Ronggui wrote:
> > I encounter such problem with lmer under FreeBSD, but not under
> > Windows. Anyone knows why? Thanks.
>
> Assuming, that your R installation is correct (from math/R ?), you
> should consider to reinstall lme4 and its dependencies. It is possible
> that these package have been built on a prior version of R.
>
> For installing the package 'Matrix' on FreeBSD you have to use
> gmake instead of make. The following example will do it
>
> setenv MAKE gmake
> R CMD INSTALL Matrix_0.999375-3.tar.gz
> unsetenv MAKE
>
> If reinstalling packages not solves your problem, please try if
> reinstalling R does. (Under FreeBSD the R sources build and install
> fine, so there is no need to use the port at math/R ...)
>
> Hope this helps,
> Rainer
>
>
> >> example(lmer)
> >
> > lmer> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> > Error in UseMethod("as.logical") : no applicable method for "as.logical"
> >> traceback()
> > 9: as.logical(EMverbose)
> > 8: as.logical(EMverbose)
> > 7: lmerControl()
> > 6: do.call("lmerControl", control)
> > 5: lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
> > 4: eval.with.vis(expr, envir, enclos)
> > 3: eval.with.vis(ei, envir)
> > 2: source(zfile, local, echo = echo, prompt.echo = paste(prompt.prefix,
> >        getOption("prompt"), sep = ""), continue.echo = paste(prompt.prefix,
> >        getOption("continue"), sep = ""), verbose = verbose,
> > max.deparse.length = Inf,
> >        encoding = encoding, skip.echo = skips)
> > 1: example(lmer)
> >
> >> sessionInfo()
> > R version 2.6.0 (2007-10-03)
> > i386-portbld-freebsd6.3
> >
> > locale:
> > C
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> > other attached packages:
> > [1] lme4_0.99875-9    Matrix_0.999375-2 lattice_0.16-5
> >
> > loaded via a namespace (and not attached):
> > [1] grid_2.6.0      rcompgen_0.1-15 tools_2.6.0
>
>
>


-- 
Ronggui Huang

Department of Sociology, Fudan University, Shanghai, China

Department of Public and Social Administration, CityU, HK


From perrone at rocketmail.com  Wed Dec 26 20:55:23 2007
From: perrone at rocketmail.com (Ricardo Perrone)
Date: Wed, 26 Dec 2007 11:55:23 -0800 (PST)
Subject: [R] probability from different values
Message-ID: <429134.34206.qm@web34308.mail.mud.yahoo.com>

Hi all,

i'm new R user and i need some help:

i have two vectors (A and B) and i need create another vector (C) from the subtraction of A's values with the B's values. How can i estimate the probability of C's values if i have differents values combinations of A and B that can result in the same value? like 90 -20 = 70 and 91 - 21 = 70.

example:
B = {18      18    18     19      20      21      22      23      24      25      26      27   28....} 
A = {82     83     84     85      85     86       87     88       89      90      91     91    92 ....}

Considering C's values that were computed from different frequencies of the A and B, is it necessary any mathematical convolution to estimated the  probability more precisely?

Thanks a lot

Ricardo




      ____________________________________________________________________________________
Be a better friend, newshound, and


From s.nancy1 at yahoo.com  Wed Dec 26 23:49:13 2007
From: s.nancy1 at yahoo.com (SNN)
Date: Wed, 26 Dec 2007 14:49:13 -0800 (PST)
Subject: [R]  Principal  Components Analysis
Message-ID: <14507365.post@talk.nabble.com>


Hi,

I do have a file that has 500000 columns and 40 rows. I want to apply PCA on
that data and this is what I did

h1<-read.table("Ccode.txt", sep='\t', header=F) # reads the data from the
file Ccode.txt
 h2<-prcomp(na.omit(h1),center=T)
 
but I am getting the following error
 
"Error in svd(x, nu = 0) : 0 extent dimensions"


I appreciate if someone can help

Thanks

-- 
View this message in context: http://www.nabble.com/Principal--Components-Analysis-tp14507365p14507365.html
Sent from the R help mailing list archive at Nabble.com.


From edd at debian.org  Thu Dec 27 13:29:47 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 27 Dec 2007 06:29:47 -0600
Subject: [R] R Compilation error on Ubuntu
In-Reply-To: <972178.28791.qm@web39712.mail.mud.yahoo.com>
References: <972178.28791.qm@web39712.mail.mud.yahoo.com>
Message-ID: <20071227122947.GA26242@eddelbuettel.com>

On Tue, Dec 25, 2007 at 01:35:19AM -0800, Satoshi Takahama wrote:
> Hi Taka,
> 
> I was just trying to do this yesterday and ran into the same problem (compiling R 2.6.1 on Gutsy Gibbon). Apparently this happens on Debian/Ubuntu distributions because the developer install is separate from the user install.
> 
> Basically, to address the configure error:
> 
> "--with-readline=yes (default) and headers/libs are not available"
> 
> you need the development version of readline:
>   sudo apt-get install libreadline5-dev
> (see http://cran.r-project.org/doc/manuals/R-admin.html)
> 
> Later, I ran into another configure error:
> "--with-x=yes (default) and X11 headers/libs are not available"
> so I installed xorg-dev:
>   sudo apt-get install xorg-dev
> (see http://tolstoy.newcastle.edu.au/R/e2/help/06/11/5193.html)
> 
> At the end, I also got a configure WARNING:
> "you cannot build info or HTML versions of the R manuals "
> so then I installed texinfo:
>   sudo apt-get install texinfo
> (see http://tolstoy.newcastle.edu.au/R/e2/help/07/04/15498.html)


If you do 

      sudo apt-get build-dep r-base

you get all build-dependencies automatically filled in by the system.

Dirk


> Hope this helps,
> 
> ST
> 
> 
> ----- Original Message ----
> From: Takatsugu Kobayashi <tkobayas at indiana.edu>
> To: r-help at stat.math.ethz.ch
> Sent: Monday, December 24, 2007 10:52:11 AM
> Subject: Re: [R] R Compilation error on Ubuntu
> 
> 
> Hi
> 
> I bought a new laptop HP dv9500 just a week ago and installed a Ubuntu
> gutsy ribbon on this laptop. I wanted to install Fedora but there are
> more threads on Ubuntu, so I decided to install Ubuntu. After hours of
> struggle in configuring x server/graphic card stuff, I installed R for
> Gutsy ribbon using "sudo apt-get install g77 r-core'. 
> 
> Now when I tried to install 'sp' package I got this error message: 
> 
> Warning in install.packages("sp") : argument 'lib' is missing: using
> '/usr/local/lib/R/site-library'
> --- Please select a CRAN mirror for use in this session ---
> Loading Tcl/Tk interface ... done
> trying URL 'http://cran.mtu.edu/src/contrib/sp_0.9-17.tar.gz'
> Content type 'application/x-gzip' length 359194 bytes
> opened URL
> ==================================================
> downloaded 350Kb
> 
> * Installing *source* package 'sp' ...
> ** libs
> gcc-4.2 -std=gnu99 -I/usr/share/R/include -I/usr/share/R/include
> -fpic  -g -O2 -c gcdist.c -o gcdist.o
> /bin/bash: gcc-4.2: command not found
> make: *** [gcdist.o] Error 127
> ERROR: compilation failed for package 'sp'
> ** Removing '/usr/local/lib/R/site-library/sp'
> 
> The downloaded packages are in
>         /tmp/RtmpfOk3H7/downloaded_packages
> Warning message:
> installation of package 'sp' had non-zero exit status in:
> install.packages("sp") 
> 
> Then, I removed this gutsy ribbon version of R and attempted to install
> fresh R-2.6.1.tar.gz. When I tried to build it, I got this error
> message:
> 
> configure: error: --with-readline=yes (default) and headers/libs are
>  not
> available
> 
> I installed xserver-xorg-dev ok. 
> 
> I am using Fedora 6 on my desktop and have never seen these above error
> messages. Should I install a Fedora and R on it? Or should I just
>  create
> a link using 'ln -s'? 
> 
> I appreciate your help.
> 
> [[replacing trailing spam]]
> 
> taka
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
>  http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> 
> 
>       ____________________________________________________________________________________
> Never miss a thing.  Make Yahoo your home page.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Three out of two people have difficulties with fractions.


From lbacon at lba.com  Thu Dec 27 14:41:57 2007
From: lbacon at lba.com (Lynd Bacon)
Date: Thu, 27 Dec 2007 05:41:57 -0800
Subject: [R] how to remove paths from where library() looks for packages
In-Reply-To: <da79af330712270223n5feac4c7x43fbf515495c168f@mail.gmail.com>
References: <477065E0.9090709@lba.com>
	<da79af330712270223n5feac4c7x43fbf515495c168f@mail.gmail.com>
Message-ID: <4773ABA5.7020308@lba.com>

Thanks, Henrique!  I'm not sure this will help since the paths are 
already unique.

Phil Spector wrote separately(Phil, I hope it's ok that I quote you here 
for the list):
> Lynd -
>    If the message bothers you, look at /etc/R/Renviron and modify
> the line that defines R_LIBS .  However, note that these are set
> up for you so that if you install packages through either Ubuntu's
> repositories or the install.package() function, everything should
> just work, so you might want to leave things as they are.
This line in this particular installation of R 2.6.1 on my Ubuntu 7.10 
workstation reads:

R_LIBS=${R_LIBS-'/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library'}

One question (for me, at least), is why did I start to get this warning 
message, to begin with?  I wasn't getting it with  an earlier version of R.

The warning I'm getting started to appear after I updated from 2.5.1 to 
2.6.1. I did this using the procedure described on:

http://cran.cnr.berkeley.edu/bin/linux/ubuntu/

I used Ubuntu's Synaptic Package Manager to install R initially on this 
particular machine, from what I can recall.  After updating to 2.6.1, 
the packages I had previously installed with 2.5.1 were in the directory 
/usr/local/lib/R/site-library.   When I now install additional packages 
in 2.6.1, they are installed to this directory, and not to 
/usr/lib/R/site-library, the directory library() gives warnings about.   
My hypothesis is that the default library configurations for the two 
versions were different for some reason.

Thanks, Henrique and Phil, for your help!

Cheers,

Lynd


Henrique Dallazuanna wrote:
> Perhaps this:
>
> .libPaths(unique(installed.packages()[,2]))
>
> On 25/12/2007, Lynd Bacon <lbacon at lba.com> wrote:
>   
>> Greetings.  My current installation of R 2.6.1 on Unbuntu 7.10 ("Gutsy")
>> is looking for packages in a directory that doesn't contain any:
>>
>>  > library()
>> Warning message:
>> In library() : library "/usr/lib/R/site-library" contains no packages
>>
>>  > .libPaths()
>> [1] "/usr/local/lib/R/site-library" "/usr/lib/R/site-library"
>> [3] "/usr/lib/R/library"
>>
>> I'd like to eliminate the path to the empty directory so I don't get
>> this vexsome warning message.  Any and all suggestions will be greatly
>> appreciated!
>>
>> Cheers,
>>
>> Lynd
>>
>> --
>> _________________________________________
>> Lynd Bacon
>> Woodside CA USA GMT-08:00
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
>
>   

-- 
_________________________________________
Lynd Bacon
Woodside CA USA GMT-08:00
+1 650.593.2198 (home office)
+1 650.462.8015 (YouGov/Polimetrix Inc.)
+1 650.863.1959 (mobile)
lbacon at LBA.com


From Richard.Cotton at hsl.gov.uk  Thu Dec 27 14:58:53 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Thu, 27 Dec 2007 13:58:53 +0000
Subject: [R] probability from different values
In-Reply-To: <429134.34206.qm@hsl.gov.uk>
Message-ID: <OFC71BC7BB.270AF5D5-ON802573BE.004C6955-802573BE.004CC5E6@hsl.gov.uk>

> i have two vectors (A and B) and i need create another vector (C) 
> from the subtraction of A's values with the B's values. How can i 
> estimate the probability of C's values if i have differents values 
> combinations of A and B that can result in the same value? like 90 
> -20 = 70 and 91 - 21 = 70.
> 
> example:
> B = {18      18    18     19      20      21      22      23      24
> 25      26      27   28....} 
> A = {82     83     84     85      85     86       87     88       89
> 90      91     91    92 ....}

Try this:
A = c(18,18,18,19,20,21,22,23,24,25,26,27,28)
B = c(82,83,84,85,85,86,87,88,89,90,91,91,92)
lenA = length(A)
lenB = length(B)
AA = rep(A, lenB)
BB = rep(B, each=lenA)
table(AA-BB)/(lenA*lenB)

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From Matthias.Kohl at stamats.de  Thu Dec 27 15:16:39 2007
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Thu, 27 Dec 2007 15:16:39 +0100
Subject: [R] probability from different values
In-Reply-To: <OFC71BC7BB.270AF5D5-ON802573BE.004C6955-802573BE.004CC5E6@hsl.gov.uk>
References: <OFC71BC7BB.270AF5D5-ON802573BE.004C6955-802573BE.004CC5E6@hsl.gov.uk>
Message-ID: <4773B3C7.2090004@stamats.de>

Hello Ricardo,

another solution could be using package distr:

library(distr)
A <- c(18,18,18,19,20,21,22,23,24,25,26,27,28)
DA <- DiscreteDistribution(A)
# maybe
# support(DA)
# plot (DA)

B <- c(82,83,84,85,85,86,87,88,89,90,91,91,92)
DB <- DiscreteDistribution(B)
# support(DB)
# plot(DB)

DC <- DB - DA# convolution with of DB with (-DA)
# support(DC)
# plot(DC)

hth,
Matthias

Richard.Cotton at hsl.gov.uk wrote:
>> i have two vectors (A and B) and i need create another vector (C) 
>> from the subtraction of A's values with the B's values. How can i 
>> estimate the probability of C's values if i have differents values 
>> combinations of A and B that can result in the same value? like 90 
>> -20 = 70 and 91 - 21 = 70.
>>
>> example:
>> B = {18      18    18     19      20      21      22      23      24
>> 25      26      27   28....} 
>> A = {82     83     84     85      85     86       87     88       89
>> 90      91     91    92 ....}
>>     
>
> Try this:
> A = c(18,18,18,19,20,21,22,23,24,25,26,27,28)
> B = c(82,83,84,85,85,86,87,88,89,90,91,91,92)
> lenA = length(A)
> lenB = length(B)
> AA = rep(A, lenB)
> BB = rep(B, each=lenA)
> table(AA-BB)/(lenA*lenB)
>
> Regards,
> Richie.
>
> Mathematical Sciences Unit
> HSL
>
>
> ------------------------------------------------------------------------
> ATTENTION:
>
> This message contains privileged and confidential inform...{{dropped:20}}
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Matthias.Kohl at stamats.de  Thu Dec 27 15:19:29 2007
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Thu, 27 Dec 2007 15:19:29 +0100
Subject: [R] probability from different values
In-Reply-To: <4773B3C7.2090004@stamats.de>
References: <OFC71BC7BB.270AF5D5-ON802573BE.004C6955-802573BE.004CC5E6@hsl.gov.uk>
	<4773B3C7.2090004@stamats.de>
Message-ID: <4773B471.6040701@stamats.de>

one addition ...
> Hello Ricardo,
>
> another solution could be using package distr:
>
> library(distr)
> A <- c(18,18,18,19,20,21,22,23,24,25,26,27,28)
> DA <- DiscreteDistribution(A)
> # maybe
> # support(DA)
> # plot (DA)
>
> B <- c(82,83,84,85,85,86,87,88,89,90,91,91,92)
> DB <- DiscreteDistribution(B)
> # support(DB)
> # plot(DB)
>
> DC <- DB - DA# convolution with of DB with (-DA)
> # support(DC)
> # plot(DC)
>   
to get the probabilities for the C-values:
p(DC)(support(DC))

> hth,
> Matthias
>
> Richard.Cotton at hsl.gov.uk wrote:
>   
>>> i have two vectors (A and B) and i need create another vector (C) 
>>> from the subtraction of A's values with the B's values. How can i 
>>> estimate the probability of C's values if i have differents values 
>>> combinations of A and B that can result in the same value? like 90 
>>> -20 = 70 and 91 - 21 = 70.
>>>
>>> example:
>>> B = {18      18    18     19      20      21      22      23      24
>>> 25      26      27   28....} 
>>> A = {82     83     84     85      85     86       87     88       89
>>> 90      91     91    92 ....}
>>>     
>>>       
>> Try this:
>> A = c(18,18,18,19,20,21,22,23,24,25,26,27,28)
>> B = c(82,83,84,85,85,86,87,88,89,90,91,91,92)
>> lenA = length(A)
>> lenB = length(B)
>> AA = rep(A, lenB)
>> BB = rep(B, each=lenA)
>> table(AA-BB)/(lenA*lenB)
>>
>> Regards,
>> Richie.
>>
>> Mathematical Sciences Unit
>> HSL
>>
>>
>> ------------------------------------------------------------------------
>> ATTENTION:
>>
>> This message contains privileged and confidential inform...{{dropped:20}}
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Matthias.Kohl at stamats.de  Thu Dec 27 15:22:11 2007
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Thu, 27 Dec 2007 15:22:11 +0100
Subject: [R] probability from different values
In-Reply-To: <4773B471.6040701@stamats.de>
References: <OFC71BC7BB.270AF5D5-ON802573BE.004C6955-802573BE.004CC5E6@hsl.gov.uk>	<4773B3C7.2090004@stamats.de>
	<4773B471.6040701@stamats.de>
Message-ID: <4773B513.6040700@stamats.de>

Matthias Kohl wrote:
> one addition ...
>   
>> Hello Ricardo,
>>
>> another solution could be using package distr:
>>
>> library(distr)
>> A <- c(18,18,18,19,20,21,22,23,24,25,26,27,28)
>> DA <- DiscreteDistribution(A)
>> # maybe
>> # support(DA)
>> # plot (DA)
>>
>> B <- c(82,83,84,85,85,86,87,88,89,90,91,91,92)
>> DB <- DiscreteDistribution(B)
>> # support(DB)
>> # plot(DB)
>>
>> DC <- DB - DA# convolution with of DB with (-DA)
>> # support(DC)
>> # plot(DC)
>>   
>>     
> to get the probabilities for the C-values:
> p(DC)(support(DC))
>   
sorry, I meant
d(DC)(support(DC))
>   
>> hth,
>> Matthias
>>
>> Richard.Cotton at hsl.gov.uk wrote:
>>   
>>     
>>>> i have two vectors (A and B) and i need create another vector (C) 
>>>> from the subtraction of A's values with the B's values. How can i 
>>>> estimate the probability of C's values if i have differents values 
>>>> combinations of A and B that can result in the same value? like 90 
>>>> -20 = 70 and 91 - 21 = 70.
>>>>
>>>> example:
>>>> B = {18      18    18     19      20      21      22      23      24
>>>> 25      26      27   28....} 
>>>> A = {82     83     84     85      85     86       87     88       89
>>>> 90      91     91    92 ....}
>>>>     
>>>>       
>>>>         
>>> Try this:
>>> A = c(18,18,18,19,20,21,22,23,24,25,26,27,28)
>>> B = c(82,83,84,85,85,86,87,88,89,90,91,91,92)
>>> lenA = length(A)
>>> lenB = length(B)
>>> AA = rep(A, lenB)
>>> BB = rep(B, each=lenA)
>>> table(AA-BB)/(lenA*lenB)
>>>
>>> Regards,
>>> Richie.
>>>
>>> Mathematical Sciences Unit
>>> HSL
>>>
>>>
>>> ------------------------------------------------------------------------
>>> ATTENTION:
>>>
>>> This message contains privileged and confidential inform...{{dropped:20}}
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>     
>>>       
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From markus at insightfromdata.com  Thu Dec 27 16:56:32 2007
From: markus at insightfromdata.com (Markus Loecher)
Date: Thu, 27 Dec 2007 10:56:32 -0500
Subject: [R] Too many open files
Message-ID: <C3993560.443%markus@insightfromdata.com>

Dear all,
Did this problem that was posted in 2006 (see below) ever got fully resolved
?
I am encountering the exact same issue
; I have executed get.hist.quote() in a loop and now R not only refuses to
establish any further connections to yahoo, but, worse, it will not open any
files either. For example, I cannot even save my current workspace for that
reason.
I tried 
  closeAllConnections()
As well as 
  showConnections()
But I do not see any open connections.
Also, does get.hist.quote() not close its connection internally once it is
done ?

Any help would be immensely useful as I am quite stuck.
Thanks!

Markus



Re: [R] Too many open files

    * This message: [ Message body ] [ More options ]
    * Related messages: [ Next message ] [ Previous message ] [ In reply to
] [ [R] error reports ] [ Next in thread ]

From: Seth Falcon <sfalcon_at_fhcrc.org>
Date: Sat 27 May 2006 - 09:21:36 EST

"Omar Lakkis" <uofiowa at gmail.com> writes:

> This may be more of an OS question ...
> I have this call
>
> r = get.hist.quote(symbol, start= format(start, "%Y-%m-%d"), end=
> format(end, "%Y-%m-%d"))
> which does a url request
>
> in a loop and my program runs out of file handlers after few hundred
> rotations. The error message is: 'Too many open files'. Other than
> increasing the file handlers assigned to my process, is there a way to
> cleanly release and reuse these connections?

Inside your loop you need to close the connection object created by url().

for (i in 1:500) {

    con <- url(urls[i])
    ## ... stuff here ...
    close(con)
}

R only allows you to have a fixed number of open connections at one time,
and they do not get closed automatically when they go out of scope. These
commands may help make clear how things work...

> showConnections()

     description class mode text isopen can read can write
> f = url("http://www.r-project.org", open="r")
> showConnections()

From: Gabor Grothendieck <ggrothendieck_at_gmail.com>
Date: Sat 27 May 2006 - 09:47:20 EST

Try closeAllConnections()


From kyong.ho.park at us.army.mil  Thu Dec 27 17:04:31 2007
From: kyong.ho.park at us.army.mil (Park, Kyong H Mr ECBC)
Date: Thu, 27 Dec 2007 11:04:31 -0500
Subject: [R] A function for random test based on longest run (UNCLASSIFIED)
Message-ID: <E6B628D143F7AD4989E83E488B529352051C740D@apgrb1rdg-mail4.nae.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/ff61afc4/attachment.pl 

From kyong.ho.park at us.army.mil  Thu Dec 27 17:07:09 2007
From: kyong.ho.park at us.army.mil (Park, Kyong H Mr ECBC)
Date: Thu, 27 Dec 2007 11:07:09 -0500
Subject: [R] A function for random test based on longest run of same events
	(U	NCLASSIFIED)
Message-ID: <E6B628D143F7AD4989E83E488B529352051C740E@apgrb1rdg-mail4.nae.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/f2a70f6c/attachment.pl 

From br44114 at gmail.com  Thu Dec 27 18:08:13 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 27 Dec 2007 12:08:13 -0500
Subject: [R] A function for random test based on longest run
	(UNCLASSIFIED)
Message-ID: <8d5a36350712270908h574d6ff4y83b648e158e5da6e@mail.gmail.com>

  > require(tseries)
  > ?runs.test
Also, take a look at dieharder, it implements a large number of
randomness tests:
http://www.phy.duke.edu/~rgb/General/dieharder.php



> -----Original Message-----
> From: r-help-bounces at r-project.org
> [mailto:r-help-bounces at r-project.org] On Behalf Of Park,
> Kyong H Mr ECBC
> Sent: Thursday, December 27, 2007 11:05 AM
> To: 'r-help at r-project.org'
> Subject: [R] A function for random test based on longest run
> (UNCLASSIFIED)
>
> Classification:  UNCLASSIFIED
> Caveats: NONE
>
> Hello, R users,
>
> Has anybody written a function for random test based on the length of
> longest run of same events. I really appreciate your help.
>
> Kyong Park
> Classification:  UNCLASSIFIED
> Caveats: NONE
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gunter.berton at gene.com  Thu Dec 27 18:15:00 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 27 Dec 2007 09:15:00 -0800
Subject: [R] Principal  Components Analysis
In-Reply-To: <14507365.post@talk.nabble.com>
References: <14507365.post@talk.nabble.com>
Message-ID: <000901c848ac$03af8b30$6701a8c0@gne.windows.gene.com>

Are there lots of missing values in the data? If so, my guess would be that
na.omit(h1) leaves you with no data. Have you checked this?

Bert Gunter
Genentech

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of SNN
Sent: Wednesday, December 26, 2007 2:49 PM
To: r-help at r-project.org
Subject: [R] Principal Components Analysis


Hi,

I do have a file that has 500000 columns and 40 rows. I want to apply PCA on
that data and this is what I did

h1<-read.table("Ccode.txt", sep='\t', header=F) # reads the data from the
file Ccode.txt
 h2<-prcomp(na.omit(h1),center=T)
 
but I am getting the following error
 
"Error in svd(x, nu = 0) : 0 extent dimensions"


I appreciate if someone can help

Thanks

-- 
View this message in context:
http://www.nabble.com/Principal--Components-Analysis-tp14507365p14507365.htm
l
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mnevill at exitcheck.net  Thu Dec 27 18:18:45 2007
From: mnevill at exitcheck.net (Max)
Date: Thu, 27 Dec 2007 09:18:45 -0800
Subject: [R] ARIMA problem
References: <6cb8ca860712241453u3d2dad40yd6607d1ddc7f339e@mail.gmail.com>
	<1115a2b00712241531m7a75084cj71a112e7fde54b17@mail.gmail.com>
Message-ID: <mn.da2e7d7c0323b137.83239@exitcheck.net>

I agree with Wensui, if you have a low order ARIMA model it is possible 
to get forecasts with the same values.

Wensui Liu expressed precisely :
> Hi, Sandeep,
> what is your specification of p, d, q? it is not surprising to have
> all forecasted with same value.
>
>
> On 12/24/07, Sandeep Nikam <spndeep at googlemail.com> wrote:
>> Hi,
>> 
>> This is regarding the ARIMA model.
>> 
>> I am having time series data of stock of  2000 values. Using the ARIMA model
>> in R, I want the forcasted values for next 36 time points.
>> 
>> However when I run this model in R, I am getting same value for all 36 time
>> points.
>> I have tried to fit the data with ARIMA model by changing the parameters
>> p,d,q after looking at the errors and other criteria for selecting the p,d,q
>> value.
>> 
>> So could you please help me in this regard, and if require I can send the
>> data file too.
>> 
>> With rgds,
>> 
>> Sandeep
>> 
>>         [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Thu Dec 27 18:25:18 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 27 Dec 2007 17:25:18 +0000 (GMT)
Subject: [R] Principal  Components Analysis
In-Reply-To: <14507365.post@talk.nabble.com>
References: <14507365.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0712271719510.8345@gannet.stats.ox.ac.uk>

On Wed, 26 Dec 2007, SNN wrote:

>
> Hi,
>
> I do have a file that has 500000 columns and 40 rows. I want to apply PCA on
> that data and this is what I did
>
> h1<-read.table("Ccode.txt", sep='\t', header=F) # reads the data from the
> file Ccode.txt
> h2<-prcomp(na.omit(h1),center=T)
>
> but I am getting the following error
>
> "Error in svd(x, nu = 0) : 0 extent dimensions"
>
>
> I appreciate if someone can help

You probably have a missing value in every row.  But does PCA with 500000 
columns make sense: 4999961 of the PCs are constant and arbitrary?
It is possible you meant PC on the transpose (sometimes called Q-mode)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Dec 27 18:28:41 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 27 Dec 2007 17:28:41 +0000 (GMT)
Subject: [R] Problem of lmer under FreeBSD
In-Reply-To: <38b9f0350712270019h59649764n8091f410b0d82476@mail.gmail.com>
References: <38b9f0350712270019h59649764n8091f410b0d82476@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0712271725400.8345@gannet.stats.ox.ac.uk>

On Thu, 27 Dec 2007, ronggui wrote:

> I encounter such problem with lmer under FreeBSD, but not under
> Windows. Anyone knows why? Thanks.

Most likely you have a mixed-up system, with some packages installed under 
an earlier version of R.  Please use update.packages(checkBuilt=TRUE) to 
fix this.  (The culprit is probably Matrix.)

Searching the archives just after the releases of R 2.6.0 will find
quite a few postings from people who had made the same mistake.

>
>> example(lmer)
>
> lmer> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> Error in UseMethod("as.logical") : no applicable method for "as.logical"
>> traceback()
> 9: as.logical(EMverbose)
> 8: as.logical(EMverbose)
> 7: lmerControl()
> 6: do.call("lmerControl", control)
> 5: lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
> 4: eval.with.vis(expr, envir, enclos)
> 3: eval.with.vis(ei, envir)
> 2: source(zfile, local, echo = echo, prompt.echo = paste(prompt.prefix,
>       getOption("prompt"), sep = ""), continue.echo = paste(prompt.prefix,
>       getOption("continue"), sep = ""), verbose = verbose,
> max.deparse.length = Inf,
>       encoding = encoding, skip.echo = skips)
> 1: example(lmer)
>>
>
>> sessionInfo()
> R version 2.6.0 (2007-10-03)
> i386-portbld-freebsd6.3
>
> locale:
> C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.99875-9    Matrix_0.999375-2 lattice_0.16-5
>
> loaded via a namespace (and not attached):
> [1] grid_2.6.0      rcompgen_0.1-15 tools_2.6.0
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From geral at evangelismcenter.org  Thu Dec 27 18:43:18 2007
From: geral at evangelismcenter.org (Evangelismcenter)
Date: Thu, 27 Dec 2007 17:43:18 +0000
Subject: [R] =?iso-8859-1?q?Apresenta=E7=E3o_Evangelismcenter?=
Message-ID: <48d098196474fd1554edbd9759f13c31@www.evangelismcenter.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/8fe613de/attachment.pl 

From geral at evangelismcenter.org  Thu Dec 27 18:43:18 2007
From: geral at evangelismcenter.org (Evangelismcenter)
Date: Thu, 27 Dec 2007 17:43:18 +0000
Subject: [R] =?iso-8859-1?q?Apresenta=E7=E3o_Evangelismcenter?=
Message-ID: <a4f8407eab9e515357ed36deb5a685bd@www.evangelismcenter.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/a56a4e65/attachment.pl 

From kyong.ho.park at us.army.mil  Thu Dec 27 19:47:26 2007
From: kyong.ho.park at us.army.mil (Park, Kyong H Mr ECBC)
Date: Thu, 27 Dec 2007 13:47:26 -0500
Subject: [R] A function for random test based on longest run
	(UNCLASSI	FIED)
Message-ID: <E6B628D143F7AD4989E83E488B529352051C7411@apgrb1rdg-mail4.nae.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/1758b8ee/attachment.pl 

From edd at debian.org  Thu Dec 27 18:44:06 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 27 Dec 2007 11:44:06 -0600
Subject: [R] A function for random test based on longest
	run	(UNCLASSIFIED)
In-Reply-To: <8d5a36350712270908h574d6ff4y83b648e158e5da6e@mail.gmail.com>
References: <8d5a36350712270908h574d6ff4y83b648e158e5da6e@mail.gmail.com>
Message-ID: <18291.58470.122385.233652@ron.nulle.part>


On 27 December 2007 at 12:08, bogdan romocea wrote:
|   > require(tseries)
|   > ?runs.test
| Also, take a look at dieharder, it implements a large number of
| randomness tests:
| http://www.phy.duke.edu/~rgb/General/dieharder.php

Also note that CRAN has an RDieHarder package that provides access to
DieHarder tests from R.  RDieHarder is currently happier on Linux than on
Windows. If anybody wants to contribute build instructions for Windows ...

Dirk

-- 
Three out of two people have difficulties with fractions.


From eggsbrown at gmail.com  Thu Dec 27 19:51:57 2007
From: eggsbrown at gmail.com (eggsbrown)
Date: Thu, 27 Dec 2007 10:51:57 -0800 (PST)
Subject: [R]  getting a string output from str
Message-ID: <14517059.post@talk.nabble.com>


I would like to use the output from str() in a homemade function but str does
not return a value! 

eg: 
list(x=str(1:20),y=str(1:15))
returns odd output...

Thanks - Kevin.
-- 
View this message in context: http://www.nabble.com/getting-a-string-output-from-str-tp14517059p14517059.html
Sent from the R help mailing list archive at Nabble.com.


From Greg.Snow at imail.org  Thu Dec 27 20:12:33 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Thu, 27 Dec 2007 12:12:33 -0700
Subject: [R] getting a string output from str
References: <14517059.post@talk.nabble.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A1D1@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/994108cc/attachment.pl 

From Greg.Snow at imail.org  Thu Dec 27 20:20:47 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Thu, 27 Dec 2007 12:20:47 -0700
Subject: [R] A function for random test based on longest run(UNCLASSI
 FIED)
References: <E6B628D143F7AD4989E83E488B529352051C7411@apgrb1rdg-mail4.nae.ds.army.mil>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A1D2@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/f19622ca/attachment.pl 

From docpat2511 at yahoo.com  Thu Dec 27 20:31:36 2007
From: docpat2511 at yahoo.com (Pat Carroll)
Date: Thu, 27 Dec 2007 11:31:36 -0800 (PST)
Subject: [R] Odd time conversion glitch
Message-ID: <185766.72287.qm@web60821.mail.yahoo.com>

Hello, all.

I ran across an odd problem while working in R 2.6.0. The command line text follows. Basically, I attempted to convert a character vector of length 13 (in a data frame with 13 rows) from a character representation of dates to a POSIX representation using strptime. strptime returned a vector of length 9, which appears to contain 13 values (!) in the appropriate format.

I can't find any way to convince the strptime output that it has 13 values . . . even though (for example) posixvector[1:13] returns 13 values, anothervectorof length13<-posixvector[1:13] produces a length mismatch error.

Can anyone help diagnose this, or work around it?

Many thanks,
~Pat Carroll.


> Dataset$DateFilled
 [1] "10/20/2005" "11/4/2005"  "11/18/2005" "12/2/2005"  "4/3/2006"   "6/5/2006"   "7/14/2006"  "4/27/2007"  "5/7/2007"   "7/17/2007"  "2/14/2005" 
[12] "2/14/2005"  "2/21/2005" 
> datefilledpos<-strptime(Dataset$DateFilled,format="%m/%d/%Y")
> datefilledpos
 [1] "2005-10-20" "2005-11-04" "2005-11-18" "2005-12-02" "2006-04-03" "2006-06-05" "2006-07-14" "2007-04-27" "2007-05-07" "2007-07-17" "2005-02-14"
[12] "2005-02-14" "2005-02-21"
> length(datefilledpos)
[1] 9
> length(Dataset$DateFilled)
[1] 13
> datefilledpos[1:13]
 [1] "2005-10-20" "2005-11-04" "2005-11-18" "2005-12-02" "2006-04-03" "2006-06-05" "2006-07-14" "2007-04-27" "2007-05-07" "2007-07-17" "2005-02-14"
[12] "2005-02-14" "2005-02-21"
> Dataset$DateFilled<-datefilledpos
Error in `$<-.data.frame`(`*tmp*`, "DateFilled", value = list(sec = c(0,  : 
  replacement has 9 rows, data has 13
> Dataset$DateFilled<-datefilledpos[1:13]
Error in `$<-.data.frame`(`*tmp*`, "DateFilled", value = list(sec = c(0,  : 
  replacement has 9 rows, data has 13
> 

 
Pat Carroll. 
what matters most is how well you walk through the fire. 
bukowski.





      ____________________________________________________________________________________
Be a better friend, newshound, and


From jholtman at gmail.com  Thu Dec 27 21:00:19 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 27 Dec 2007 15:00:19 -0500
Subject: [R] Odd time conversion glitch
In-Reply-To: <185766.72287.qm@web60821.mail.yahoo.com>
References: <185766.72287.qm@web60821.mail.yahoo.com>
Message-ID: <644e1f320712271200v34cad61ehdba7bf52024e0789@mail.gmail.com>

You have converted to POSIXlt.  You want POSIXct:

as.POSIXct(strptime(...))

On Dec 27, 2007 2:31 PM, Pat Carroll <docpat2511 at yahoo.com> wrote:
> Hello, all.
>
> I ran across an odd problem while working in R 2.6.0. The command line text follows. Basically, I attempted to convert a character vector of length 13 (in a data frame with 13 rows) from a character representation of dates to a POSIX representation using strptime. strptime returned a vector of length 9, which appears to contain 13 values (!) in the appropriate format.
>
> I can't find any way to convince the strptime output that it has 13 values . . . even though (for example) posixvector[1:13] returns 13 values, anothervectorof length13<-posixvector[1:13] produces a length mismatch error.
>
> Can anyone help diagnose this, or work around it?
>
> Many thanks,
> ~Pat Carroll.
>
>
> > Dataset$DateFilled
>  [1] "10/20/2005" "11/4/2005"  "11/18/2005" "12/2/2005"  "4/3/2006"   "6/5/2006"   "7/14/2006"  "4/27/2007"  "5/7/2007"   "7/17/2007"  "2/14/2005"
> [12] "2/14/2005"  "2/21/2005"
> > datefilledpos<-strptime(Dataset$DateFilled,format="%m/%d/%Y")
> > datefilledpos
>  [1] "2005-10-20" "2005-11-04" "2005-11-18" "2005-12-02" "2006-04-03" "2006-06-05" "2006-07-14" "2007-04-27" "2007-05-07" "2007-07-17" "2005-02-14"
> [12] "2005-02-14" "2005-02-21"
> > length(datefilledpos)
> [1] 9
> > length(Dataset$DateFilled)
> [1] 13
> > datefilledpos[1:13]
>  [1] "2005-10-20" "2005-11-04" "2005-11-18" "2005-12-02" "2006-04-03" "2006-06-05" "2006-07-14" "2007-04-27" "2007-05-07" "2007-07-17" "2005-02-14"
> [12] "2005-02-14" "2005-02-21"
> > Dataset$DateFilled<-datefilledpos
> Error in `$<-.data.frame`(`*tmp*`, "DateFilled", value = list(sec = c(0,  :
>  replacement has 9 rows, data has 13
> > Dataset$DateFilled<-datefilledpos[1:13]
> Error in `$<-.data.frame`(`*tmp*`, "DateFilled", value = list(sec = c(0,  :
>  replacement has 9 rows, data has 13
> >
>
>
> Pat Carroll.
> what matters most is how well you walk through the fire.
> bukowski.
>
>
>
>
>
>      ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From Charles.Annis at StatisticalEngineering.com  Thu Dec 27 21:25:17 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 27 Dec 2007 15:25:17 -0500
Subject: [R] Running R from a CD on Windows?
Message-ID: <000c01c848c6$988a8b00$6400a8c0@DD4XFW31>

Greetings, R-ians:

Yes it's easy to run R from a CD: I copied my windows installation folder to
the CD, then copied my R working directory to the CD.

To run from the CD, I copy the working directory from the CD to the desktop,
and then create an R desktop icon that points to Rgui.exe on the CD, and
starts in the working directory folder I just copied to the desktop.

The problem is that to create the desktop R icon you need to know that
Rgui.exe is on the CD in the bin subfolder of the R-2.6.1 folder, which I
know but the person being introduced to R for the first time does not.  And
creating a desktop icon is busywork that some find annoying.  And then the
user needs to copy the basic working directory folder to the desktop, since
R needs to be able to write to it and writing to the CD can't work with a
write-once CD.  Alas, my uses just want to put the CD in the drive and have
the autorun facility take care all this:

1) If there isn't a folder named "R-project" on the desktop, copy the one on
the CD to the desktop.
2) Create a desktop icon for R that points to Rgui.exe on the CD.  The
installation must determine what drive is currently being accessed, since
"D:\R-2.6.1\bin\Rgui.exe" would only work if the CD was running from drive
D.
3) Have the icon Start in "desktop\R-project" which is legal if you create
the icon yourself, but apparently not legal to put on the CD since the
windows are erased and cannot be filled in if the icon is copied back from
the CD to the desktop.

Since the basic R installation on Windows can create a desktop icon, I know
this must be possible.  I would be grateful for any help.

Thanks

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?


From ggrothendieck at gmail.com  Thu Dec 27 21:36:54 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Dec 2007 15:36:54 -0500
Subject: [R] Running R from a CD on Windows?
In-Reply-To: <000c01c848c6$988a8b00$6400a8c0@DD4XFW31>
References: <000c01c848c6$988a8b00$6400a8c0@DD4XFW31>
Message-ID: <971536df0712271236v3fbb0f31u848f54ac9ca674d0@mail.gmail.com>

There is a batch script, rgui.bat, in batchfiles, see home page at
batchfiles.googlecode.com, that will search the registry and a
few other places for R and start up R if it can find it.  It does
not search  CD's automatically but that would be simple to add.

On Dec 27, 2007 3:25 PM, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> Greetings, R-ians:
>
> Yes it's easy to run R from a CD: I copied my windows installation folder to
> the CD, then copied my R working directory to the CD.
>
> To run from the CD, I copy the working directory from the CD to the desktop,
> and then create an R desktop icon that points to Rgui.exe on the CD, and
> starts in the working directory folder I just copied to the desktop.
>
> The problem is that to create the desktop R icon you need to know that
> Rgui.exe is on the CD in the bin subfolder of the R-2.6.1 folder, which I
> know but the person being introduced to R for the first time does not.  And
> creating a desktop icon is busywork that some find annoying.  And then the
> user needs to copy the basic working directory folder to the desktop, since
> R needs to be able to write to it and writing to the CD can't work with a
> write-once CD.  Alas, my uses just want to put the CD in the drive and have
> the autorun facility take care all this:
>
> 1) If there isn't a folder named "R-project" on the desktop, copy the one on
> the CD to the desktop.
> 2) Create a desktop icon for R that points to Rgui.exe on the CD.  The
> installation must determine what drive is currently being accessed, since
> "D:\R-2.6.1\bin\Rgui.exe" would only work if the CD was running from drive
> D.
> 3) Have the icon Start in "desktop\R-project" which is legal if you create
> the icon yourself, but apparently not legal to put on the CD since the
> windows are erased and cannot be filled in if the icon is copied back from
> the CD to the desktop.
>
> Since the basic R installation on Windows can create a desktop icon, I know
> this must be possible.  I would be grateful for any help.
>
> Thanks
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax: 614-455-3265
> http://www.StatisticalEngineering.com
>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Dec 27 21:55:31 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Dec 2007 15:55:31 -0500
Subject: [R] Odd time conversion glitch
In-Reply-To: <185766.72287.qm@web60821.mail.yahoo.com>
References: <185766.72287.qm@web60821.mail.yahoo.com>
Message-ID: <971536df0712271255g27b61371k159de83014eac20f@mail.gmail.com>

Read RNews 4/1 and follow the advice there.

On Dec 27, 2007 2:31 PM, Pat Carroll <docpat2511 at yahoo.com> wrote:
> Hello, all.
>
> I ran across an odd problem while working in R 2.6.0. The command line text follows. Basically, I attempted to convert a character vector of length 13 (in a data frame with 13 rows) from a character representation of dates to a POSIX representation using strptime. strptime returned a vector of length 9, which appears to contain 13 values (!) in the appropriate format.
>
> I can't find any way to convince the strptime output that it has 13 values . . . even though (for example) posixvector[1:13] returns 13 values, anothervectorof length13<-posixvector[1:13] produces a length mismatch error.
>
> Can anyone help diagnose this, or work around it?
>
> Many thanks,
> ~Pat Carroll.
>
>
> > Dataset$DateFilled
>  [1] "10/20/2005" "11/4/2005"  "11/18/2005" "12/2/2005"  "4/3/2006"   "6/5/2006"   "7/14/2006"  "4/27/2007"  "5/7/2007"   "7/17/2007"  "2/14/2005"
> [12] "2/14/2005"  "2/21/2005"
> > datefilledpos<-strptime(Dataset$DateFilled,format="%m/%d/%Y")
> > datefilledpos
>  [1] "2005-10-20" "2005-11-04" "2005-11-18" "2005-12-02" "2006-04-03" "2006-06-05" "2006-07-14" "2007-04-27" "2007-05-07" "2007-07-17" "2005-02-14"
> [12] "2005-02-14" "2005-02-21"
> > length(datefilledpos)
> [1] 9
> > length(Dataset$DateFilled)
> [1] 13
> > datefilledpos[1:13]
>  [1] "2005-10-20" "2005-11-04" "2005-11-18" "2005-12-02" "2006-04-03" "2006-06-05" "2006-07-14" "2007-04-27" "2007-05-07" "2007-07-17" "2005-02-14"
> [12] "2005-02-14" "2005-02-21"
> > Dataset$DateFilled<-datefilledpos
> Error in `$<-.data.frame`(`*tmp*`, "DateFilled", value = list(sec = c(0,  :
>  replacement has 9 rows, data has 13
> > Dataset$DateFilled<-datefilledpos[1:13]
> Error in `$<-.data.frame`(`*tmp*`, "DateFilled", value = list(sec = c(0,  :
>  replacement has 9 rows, data has 13
> >
>
>
> Pat Carroll.
> what matters most is how well you walk through the fire.
> bukowski.
>
>
>
>
>
>      ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Thu Dec 27 22:31:46 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 27 Dec 2007 22:31:46 +0100
Subject: [R] Running R from a CD on Windows?
In-Reply-To: <000c01c848c6$988a8b00$6400a8c0@DD4XFW31>
References: <000c01c848c6$988a8b00$6400a8c0@DD4XFW31>
Message-ID: <477419C2.1080601@statistik.uni-dortmund.de>

Sounds like you want to write some VisualBasic script that sets up the 
link etc., started by the autorun.inf facility.
In any case, many virus and/or behavior checkers will shout when you 
implement things like that ... - and I have disabled auto-anything for 
CD/USB devices not only form security reasons.

Uwe Ligges




Charles Annis, P.E. wrote:
> Greetings, R-ians:
> 
> Yes it's easy to run R from a CD: I copied my windows installation folder to
> the CD, then copied my R working directory to the CD.
> 
> To run from the CD, I copy the working directory from the CD to the desktop,
> and then create an R desktop icon that points to Rgui.exe on the CD, and
> starts in the working directory folder I just copied to the desktop.
> 
> The problem is that to create the desktop R icon you need to know that
> Rgui.exe is on the CD in the bin subfolder of the R-2.6.1 folder, which I
> know but the person being introduced to R for the first time does not.  And
> creating a desktop icon is busywork that some find annoying.  And then the
> user needs to copy the basic working directory folder to the desktop, since
> R needs to be able to write to it and writing to the CD can't work with a
> write-once CD.  Alas, my uses just want to put the CD in the drive and have
> the autorun facility take care all this:
> 
> 1) If there isn't a folder named "R-project" on the desktop, copy the one on
> the CD to the desktop.
> 2) Create a desktop icon for R that points to Rgui.exe on the CD.  The
> installation must determine what drive is currently being accessed, since
> "D:\R-2.6.1\bin\Rgui.exe" would only work if the CD was running from drive
> D.
> 3) Have the icon Start in "desktop\R-project" which is legal if you create
> the icon yourself, but apparently not legal to put on the CD since the
> windows are erased and cannot be filled in if the icon is copied back from
> the CD to the desktop.
> 
> Since the basic R installation on Windows can create a desktop icon, I know
> this must be possible.  I would be grateful for any help.
> 
> Thanks
> 
> Charles Annis, P.E.
> 
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>  
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From MikeJones at westat.com  Thu Dec 27 22:35:19 2007
From: MikeJones at westat.com (Mike Jones)
Date: Thu, 27 Dec 2007 16:35:19 -0500
Subject: [R] Conditionally incrementing a loop counter
Message-ID: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1A@MAILBE2.westat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/4469fa92/attachment.pl 

From gunter.berton at gene.com  Thu Dec 27 23:04:50 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 27 Dec 2007 14:04:50 -0800
Subject: [R] Conditionally incrementing a loop counter
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1A@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1A@MAILBE2.westat.com>
Message-ID: <000901c848d4$80c52710$6701a8c0@gne.windows.gene.com>

Please read the posting guide and provide a simple reproducible example as
it asks you to. Very likely a loop is not even needed.

-- Bert Gunter
Genentech, Inc. 

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Mike Jones
Sent: Thursday, December 27, 2007 1:35 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Conditionally incrementing a loop counter

Hi,
I am trying a for loop from 1 to 100 by 1. However, if a condition does
not get met, I want to "throw away" that iteration. So if my loop is
for (i in 1:100)
and i is say, 25 and the condition is not met then I don't want i to go
up to 26.  Is there a way to do that? I can't seem to manually adjust i
because from what I understand, R creates 100 long vector and uses that
to "loops thru" and I'm not sure how to get at the index of that vector.
Any suggestions? Thanks in advance.










Mike Jones
Westat
1650 Research Blvd. RE401
Rockville, MD 20850
Ph: 240.314.2312


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From aa2007r at gmail.com  Thu Dec 27 23:12:34 2007
From: aa2007r at gmail.com (AA)
Date: Thu, 27 Dec 2007 17:12:34 -0500
Subject: [R] warning on gamma option in par(args)  or calling par(= new)?
Message-ID: <00af01c848d5$9649c4f0$3301a8c0@MainDomain.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/0a63c416/attachment.pl 

From john.maindonald at anu.edu.au  Thu Dec 27 23:12:55 2007
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Fri, 28 Dec 2007 09:12:55 +1100
Subject: [R]  Reminiscing on 20 years using S
In-Reply-To: <mailman.20.1198753204.4706.r-help@r-project.org>
References: <mailman.20.1198753204.4706.r-help@r-project.org>
Message-ID: <39767564-D6E6-4AC2-B1A9-AFD0241B14C9@anu.edu.au>

My first exposure to S was on an AT&T 3B2 (a 3B2/100,
I think), at the Auckland (Mt Albert) Applied Mathematics
Division Station of the NZ Dept of Scientific and Industrial
Research.  The AMD Head Office in Wellington had one
also.  There may have been one or more others; I cannot
remember. This would have been in 1983, maybe.

It was a superbly engineered machine, but the sofware
(System V, version 3.2) had its problems.  If you back
deleted too far along the command line, something
unpleasant (losing the line? or worse?) happened.
On typing 1+1 at the S command line, it took a second
to get an answer.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 27 Dec 2007, at 10:00 PM, r-help-request at r-project.org wrote:

> From: roger koenker <roger at ysidro.econ.uiuc.edu>
> Date: 27 December 2007 9:56:45 AM
> To: Greg Snow <Greg.Snow at imail.org>
> Cc: R-help list <R-help at stat.math.ethz.ch>
> Subject: Re: [R] Reminiscing on 20 years using S
>
> On Dec 26, 2007, at 2:05 PM, Greg Snow wrote:
>
>> I realized earlier this year (2007) that it was in 1987 that I first
>> started using an early version of S (it was ported to VMS and was  
>> called
>> success).  That means that I have been using some variant of S (to
>> various degrees) for over 20 years now (I don't feel that old).
>
> Boxing day somehow seems appropriate for this thread.  R.I.P. to all  
> those old boxes
> of yesteryore and the software that ran on them -- and yet there is  
> always a residual  archaeological curiosity.
>
> I discovered recently that the MIT athena network contains a circa  
> 1989 version
> of S:  http://stuff.mit.edu/afs/athena/astaff/project/Sdev/S/  which  
> made me wonder
> whether there was any likelihood that one could recreate "S Thu Dec   
> 7 16:49:47 EST 1989".
> Curiosity is one thing, time to dig through the layers of ancient  
> civilizations is quite another.
> But if anyone would like to offer a (preferably educated) guess  
> about the feasibility of  such a project, like I said, I would be  
> curious.
>
>
> url:    www.econ.uiuc.edu/~roger                Roger Koenker
> email   rkoenker at uiuc.edu                       Department of  
> Economics
> vox:    217-333-4558                            University of Illinois
> fax:    217-244-6678                            Champaign, IL 61820


From MikeJones at westat.com  Thu Dec 27 23:20:23 2007
From: MikeJones at westat.com (Mike Jones)
Date: Thu, 27 Dec 2007 17:20:23 -0500
Subject: [R] Conditionally incrementing a loop counter: Take 2
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1A@MAILBE2.westat.com>
Message-ID: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1C@MAILBE2.westat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/7ecd74aa/attachment.pl 

From p.dalgaard at biostat.ku.dk  Thu Dec 27 23:22:41 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 27 Dec 2007 23:22:41 +0100
Subject: [R] Conditionally incrementing a loop counter
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1A@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1A@MAILBE2.westat.com>
Message-ID: <477425B1.9070006@biostat.ku.dk>

Mike Jones wrote:
> Hi,
> I am trying a for loop from 1 to 100 by 1. However, if a condition does
> not get met, I want to "throw away" that iteration. So if my loop is
> for (i in 1:100)
> and i is say, 25 and the condition is not met then I don't want i to go
> up to 26.  Is there a way to do that? I can't seem to manually adjust i
> because from what I understand, R creates 100 long vector and uses that
> to "loops thru" and I'm not sure how to get at the index of that vector.
> Any suggestions? Thanks in advance.
>   
You're not being entirely clear. If you don't increment the loop 
counter, you generally get stuck in an infinite loop. That is, unless 
you rely on external input somehow, which you never told us about. You 
can easily do a while()-type loop in R, but whether that solves your 
problem is hard to tell.

Often the solution lies in whole-object thinking, and can end up quite 
different from index-fiddling, so it might help if you said what the 
actual problem is.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p.dalgaard at biostat.ku.dk  Thu Dec 27 23:35:58 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 27 Dec 2007 23:35:58 +0100
Subject: [R] Conditionally incrementing a loop counter: Take 2
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1C@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1C@MAILBE2.westat.com>
Message-ID: <477428CE.4000801@biostat.ku.dk>

Mike Jones wrote:
> My apologies for not including a working example.
>
> Here it is:
>
> for (i in 1:10){
>    cat("initial i = ",i,"\n")
>    x <- runif(1)
>    if (x > 0.7){
>       i <- i-1
>    }   
>    cat("second i = ",i,"\n")
> }
>
> When I ran this i got what follows, so there were four cases where I
> wanted the i not to increment.
>
> initial i =  1 
> second i =  1 
> initial i =  2 
> second i =  1 
> initial i =  3 
> second i =  3 
> initial i =  4 
> second i =  3 
> initial i =  5 
> second i =  4 
> initial i =  6 
> second i =  6 
> initial i =  7 
> second i =  7 
> initial i =  8 
> second i =  7 
> initial i =  9 
> second i =  9 
> initial i =  10 
> second i =  10 
>
>   
Is this the kind of effect you want?

 > x <- runif(10)
 > cbind(x, 1:10, cumsum(x < .7))
                x    
 [1,] 0.384165631  1 1
 [2,] 0.392715845  2 2
 [3,] 0.895936431  3 2
 [4,] 0.910242185  4 2
 [5,] 0.689987301  5 3
 [6,] 0.237071326  6 4
 [7,] 0.225032680  7 5
 [8,] 0.001856286  8 6
 [9,] 0.392034868  9 7
[10,] 0.655076045 10 8

If you insist on using a loop, you need to separate the loop control 
from the manipulation of i, as in (e.g.)

i <- 0
for (j in 1:10){
   i <- i + 1
   cat("initial i = ",i,"\n")
   x <- runif(1)
   if (x > 0.7){
      i <- i-1
   }   
   cat("second i = ",i,"\n")
}


>>  -----Original Message-----
>> From: 	Mike Jones  
>> Sent:	Thursday, December 27, 2007 4:35 PM
>> To:	'r-help at lists.R-project.org'
>> Subject:	Conditionally incrementing a loop counter
>>
>> Hi,
>> I am trying a for loop from 1 to 10 by 1. However, if a condition does
>> not get met, I want to "throw away" that iteration. So if my loop is
>> for (i in 1:10)
>> and i is say, 4 and the condition is not met then I don't want i to go
>> up to 5.  Is there a way to do that? I can't seem to manually adjust i
>> because from what I understand, R creates 10 long vector and uses that
>> to "loops thru" and I'm not sure how to get at the index of that
>> vector. Any suggestions? Thanks in advance.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> Mike Jones
>> Westat
>> 1650 Research Blvd. RE401
>> Rockville, MD 20850
>> Ph: 240.314.2312
>>
>>     
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From aprevitali at hotmail.com  Thu Dec 27 23:42:27 2007
From: aprevitali at hotmail.com (andrea previtali)
Date: Thu, 27 Dec 2007 22:42:27 +0000
Subject: [R] groupedData function not found
Message-ID: <BAY139-W10E5EF414B783C90B6749BC6540@phx.gbl>


Hello,
I'm trying to use the groupedData function and R is giving me the message: Error: can not find function "groupedData"
The code is coming from a textbook so I think it should be correct:

pigs<-data.frame(cbind(pig.time,pig.id,pig.wt))
pig.growth<-groupedData(pig.wt~pig.time|pig.id,data=pigs)

I have added the package "nlme" and to confirm that it was installed correctly I requested the list of functions included in the package (library(help=nlme)) and I do see groupData in the list.
I am using R 2.6.1 in Windows XP.

I'll appreciate your help.
Thanks,
Andrea Previtali
Post-doc fellow
Dept. of Biology,
Univ. of Utah, SLC, UT.


From MikeJones at westat.com  Fri Dec 28 00:08:26 2007
From: MikeJones at westat.com (Mike Jones)
Date: Thu, 27 Dec 2007 18:08:26 -0500
Subject: [R] Conditionally incrementing a loop counter: Take 2
In-Reply-To: <477428CE.4000801@biostat.ku.dk>
Message-ID: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE20@MAILBE2.westat.com>

Since I didn't want the i to increment in the loop when the condition is not met, then in my example I wanted the loop to actually run 14 times instead of the 10 since I wanted 4 of the iterations to be thrown away, or ignored.  I still haven't been able to figure this out.  Going the "while" route doesn't seem to work for me either.


nums <- numeric(10)
i <- 1
garbage <- 0

while (i <= 10){
	x <- runif(1)
	cat("x = ",x,"\n")
	if (x < 0.1){
		nums[i] <- x
		i <- i + 1
	}
	else{
	        garbage <- garbage+1
	}    
cat("i = ",i,"garbage = ",garbage,"\n")
}

-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: Thursday, December 27, 2007 5:36 PM
To: Mike Jones
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Conditionally incrementing a loop counter: Take 2


Mike Jones wrote:
> My apologies for not including a working example.
>
> Here it is:
>
> for (i in 1:10){
>    cat("initial i = ",i,"\n")
>    x <- runif(1)
>    if (x > 0.7){
>       i <- i-1
>    }   
>    cat("second i = ",i,"\n")
> }
>
> When I ran this i got what follows, so there were four cases where I 
> wanted the i not to increment.
>
> initial i =  1
> second i =  1 
> initial i =  2 
> second i =  1 
> initial i =  3 
> second i =  3 
> initial i =  4 
> second i =  3 
> initial i =  5 
> second i =  4 
> initial i =  6 
> second i =  6 
> initial i =  7 
> second i =  7 
> initial i =  8 
> second i =  7 
> initial i =  9 
> second i =  9 
> initial i =  10 
> second i =  10 
>
>   
Is this the kind of effect you want?

 > x <- runif(10)
 > cbind(x, 1:10, cumsum(x < .7))
                x    
 [1,] 0.384165631  1 1
 [2,] 0.392715845  2 2
 [3,] 0.895936431  3 2
 [4,] 0.910242185  4 2
 [5,] 0.689987301  5 3
 [6,] 0.237071326  6 4
 [7,] 0.225032680  7 5
 [8,] 0.001856286  8 6
 [9,] 0.392034868  9 7
[10,] 0.655076045 10 8

If you insist on using a loop, you need to separate the loop control 
from the manipulation of i, as in (e.g.)

i <- 0
for (j in 1:10){
   i <- i + 1
   cat("initial i = ",i,"\n")
   x <- runif(1)
   if (x > 0.7){
      i <- i-1
   }   
   cat("second i = ",i,"\n")
}


>>  -----Original Message-----
>> From: 	Mike Jones  
>> Sent:	Thursday, December 27, 2007 4:35 PM
>> To:	'r-help at lists.R-project.org'
>> Subject:	Conditionally incrementing a loop counter
>>
>> Hi,
>> I am trying a for loop from 1 to 10 by 1. However, if a condition 
>> does not get met, I want to "throw away" that iteration. So if my 
>> loop is for (i in 1:10) and i is say, 4 and the condition is not met 
>> then I don't want i to go up to 5.  Is there a way to do that? I 
>> can't seem to manually adjust i because from what I understand, R 
>> creates 10 long vector and uses that to "loops thru" and I'm not sure 
>> how to get at the index of that vector. Any suggestions? Thanks in 
>> advance.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> Mike Jones
>> Westat
>> 1650 Research Blvd. RE401
>> Rockville, MD 20850
>> Ph: 240.314.2312
>>
>>     
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From pedrosmarques at portugalmail.pt  Fri Dec 28 00:14:26 2007
From: pedrosmarques at portugalmail.pt (pedrosmarques at portugalmail.pt)
Date: Thu, 27 Dec 2007 23:14:26 +0000
Subject: [R] Lda and Qda
Message-ID: <1198797266.477431d26f609@gold5.portugalmail.pt>



Hi all,

I'm working with some data: 54 variables and a column of classes, each observation as one of a possible seven different classes:

> var.can3<-lda(x=dados[,c(1:28,30:54)],grouping=dados[,55],CV=TRUE)
Warning message:
In lda.default(x, grouping, ...) : variables are collinear
> summary(var.can3)
          Length Class  Mode   
class      30000 factor numeric   ### why?? I don't understand it
posterior 210000 -none- numeric
call           4 -none- call    ## what's this?


> var.can<-lda(dados[,c(1:28,30:54)],dados[,55])#porque a variavel 29 ? constante
Warning message:
In lda.default(x, grouping, ...) : variables are collinear
> summary(var.can)
        Length Class  Mode     
prior     7    -none- numeric  
counts    7    -none- numeric  
means   371    -none- numeric  
scaling 318    -none- numeric  
lev       7    -none- character
svd       6    -none- numeric  
N         1    -none- numeric  
call      3    -none- call     
> (normalizar<-function(matriz){ n<-dim(matriz)[1]; m<-dim(matriz)[2]; normas<-sqrt(colSums(matriz*matriz)); matriz.normalizada<-matriz/t(matrix(rep(normas,n),m,n));return(matriz.normalizada)})
function(matriz){ n<-dim(matriz)[1]; m<-dim(matriz)[2]; normas<-sqrt(colSums(matriz*matriz)); matriz.normalizada<-matriz/t(matrix(rep(normas,n),m,n));return(matriz.normalizada)}
> var.canonicas<-as.matrix(dados[,c(1:28,30:54)])%*%(normalizar(var.can$scaling))
> summary(var.canonicas)
      LD1               LD2              LD3               LD4        
 Min.   :-21.942   Min.   :-6.820   Min.   :-10.138   Min.   :-6.584  
 1st Qu.:-20.014   1st Qu.:-5.480   1st Qu.: -8.280   1st Qu.: 0.872  
 Median :-19.495   Median :-5.007   Median : -7.800   Median : 1.083  
 Mean   :-18.827   Mean   :-4.760   Mean   : -7.803   Mean   : 1.134  
 3rd Qu.:-18.975   3rd Qu.:-4.456   3rd Qu.: -7.278   3rd Qu.: 1.311  
 Max.   : -7.886   Max.   : 3.116   Max.   : -1.619   Max.   : 5.556  
      LD5               LD6         
 Min.   :-11.083   Min.   :-4.4972  
 1st Qu.: -1.237   1st Qu.:-1.6497  
 Median : -1.100   Median :-1.0909  
 Mean   : -1.100   Mean   :-0.9808  
 3rd Qu.: -0.957   3rd Qu.:-0.4598  
 Max.   :  4.712   Max.   : 7.5356  
> 


I don't know wether I need to specify a training set and a testing set, I also don't know the error nor the classifier; shouldn't the lenght of class of var.can3 be 7 since  I only have 7 different classes?

Best regards,

Pedro Marques


From aaron_barzilai at yahoo.com  Fri Dec 28 00:18:31 2007
From: aaron_barzilai at yahoo.com (Aaron Barzilai)
Date: Thu, 27 Dec 2007 15:18:31 -0800 (PST)
Subject: [R] Help with lm and multiple linear regression?
Message-ID: <807378.10675.qm@web35610.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/e518894c/attachment.pl 

From aaron_barzilai at yahoo.com  Fri Dec 28 00:22:12 2007
From: aaron_barzilai at yahoo.com (Aaron Barzilai)
Date: Thu, 27 Dec 2007 15:22:12 -0800 (PST)
Subject: [R] Help with lm and multiple linear regression? (Plain Text
	version)
Message-ID: <439173.59898.qm@web35604.mail.mud.yahoo.com>

(Apologies the previous version was sent as rich text)

Hello,
I'm new to R, but I've read the intro to R and successfully connected it to an instance of mysql.  I'm trying to perform multiple linear regression, but I'm having trouble using the lm function.  To start, I have read in a simply y matrix of values(dependent variable) and x matrix of independent variables.  It says both are data frames, but lm is giving me an error that my y variable is a list.

Any suggestions on how to do this?  It's not clear to me what the problem is as they're both data frames.  My actual problem will use a much wider matrix of coefficients, I've only included two for illustration.  

Additionally, I'd actually like to weight the observations.  How would I go about doing that?  I also have that as a separate column vector.

Thanks,
Aaron

Here's my session:
> margin
    margin
1    66.67
2   -58.33
3   100.00
4   -33.33
5   200.00
6   -83.33
7  -100.00
8     0.00
9   100.00
10  -18.18
11  -55.36
12 -125.00
13  -33.33
14 -200.00
15    0.00
16 -100.00
17   75.00
18    0.00
19 -200.00
20   35.71
21  100.00
22   50.00
23  -86.67
24  165.00
> personcoeff
   Person1 Person2
1       -1       1
2       -1       1
3       -1       1
4       -1       1
5       -1       1
6       -1       1
7        0       0
8        0       0
9        0       1
10      -1       1
11      -1       1
12      -1       1
13      -1       1
14      -1       0
15       0       0
16       0       0
17       0       1
18      -1       1
19      -1       1
20      -1       1
21      -1       1
22      -1       1
23      -1       1
24      -1       1
> class(margin)
[1] "data.frame"
> class(personcoeff)
[1] "data.frame"
> lm(margin~personcoeff)
Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  : 
        invalid type (list) for variable 'margin'


      ____________________________________________________________________________________
Be a better friend, newshound, and


From gygulyas at yahoo.ca  Fri Dec 28 00:33:27 2007
From: gygulyas at yahoo.ca (Gyula Gulyas)
Date: Thu, 27 Dec 2007 15:33:27 -0800 (PST)
Subject: [R] SAS to R - if you don't have a SAS license
Message-ID: <776471.29074.qm@web36802.mail.mud.yahoo.com>

Hi all,

if you do not have a SAS license but want to convert
native SAS data files, the solution below will work.

# read SAS data without SAS

# 1. Download free SAS System Viewer from either of
the sites below:
#      
http://www.sas.com/apps/demosdownloads/setupcat.jsp?cat=SAS+System+Viewer
(requires registration)
#      
http://www.umass.edu/statdata/software/downloads/SASViewer/index.html
# 2. Open SAS data in the SAS System Viewer
# 3. View-Formatted sets the data in formatted view
# 4. Save As File...csv file - this is your SAS data
file
# 5. View-Variables (now showing the variable names
and formats)
# 6. Save As File...csv file - this is your SAS
variable definition file

# run code below

wrkdir<-getwd() # save working directory to reset
later

# Select the SAS data file...
sas.data<-read.table(file.choose(),header=T, sep=",",
na.strings=".")

# Select SAS variable definition file...
sas.def<-read.csv(file.choose())

# str(sas.def)
# sas.def$SASFORMAT[sas.def$Type=="Char"]<-"character"
# sas.def$SASFORMAT[sas.def$Type=="Num"]<-"numeric"

sas.def$SASFORMAT[substr(sas.def$Format,1,4)=="DATE"]<-"date"

sas.def<-sas.def[,length(names(sas.def))] # pick last
column

tmp<-which(sas.def=="date")

sas.data[,tmp] <-
as.data.frame(strptime(sas.data[,tmp],
"%d%b%Y:%H:%M:%S"))

str(sas.data)
print(head(sas.data))

setwd(wrkdir) # reset working directory

rm(wrkdir,tmp,sas.def)

# the end




      ____________________________________________________________________________________
Be a better friend, newshound, and


From wangtong at usc.edu  Fri Dec 28 00:44:40 2007
From: wangtong at usc.edu (Tong Wang)
Date: Thu, 27 Dec 2007 15:44:40 -0800
Subject: [R] Efficiency of for-loop in R
Message-ID: <dd66928314c6e.4773c868@usc.edu>

Hi,
   I just realized that in Matlab, as long as memory is pre-allocated, doing for-loop doesn't cost more time than doing things in vector form. 
   But it seems in R, it still cost a lot to do for-loop.  Is there any improvement in R that I missed. Thanks a lot. 

Merry Xmas Everyone !


From liuwensui at gmail.com  Fri Dec 28 00:46:48 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 27 Dec 2007 18:46:48 -0500
Subject: [R] SAS to R - if you don't have a SAS license
In-Reply-To: <776471.29074.qm@web36802.mail.mud.yahoo.com>
References: <776471.29074.qm@web36802.mail.mud.yahoo.com>
Message-ID: <1115a2b00712271546w743ff42bn41a249436901705c@mail.gmail.com>

while I move data between SAS and R all the time, personally I don't
think your recommendation is very practical. Instead, I feel SAS
transport file is much better than csv.

Plus, the sas dataset created on unix can't be opened by sas viewer on
windows. It is even undoable if the dataset is large.

Just my $0.02.


On Dec 27, 2007 6:33 PM, Gyula Gulyas <gygulyas at yahoo.ca> wrote:
> Hi all,
>
> if you do not have a SAS license but want to convert
> native SAS data files, the solution below will work.
>
> # read SAS data without SAS
>
> # 1. Download free SAS System Viewer from either of
> the sites below:
> #
> http://www.sas.com/apps/demosdownloads/setupcat.jsp?cat=SAS+System+Viewer
> (requires registration)
> #
> http://www.umass.edu/statdata/software/downloads/SASViewer/index.html
> # 2. Open SAS data in the SAS System Viewer
> # 3. View-Formatted sets the data in formatted view
> # 4. Save As File...csv file - this is your SAS data
> file
> # 5. View-Variables (now showing the variable names
> and formats)
> # 6. Save As File...csv file - this is your SAS
> variable definition file
>
> # run code below
>
> wrkdir<-getwd() # save working directory to reset
> later
>
> # Select the SAS data file...
> sas.data<-read.table(file.choose(),header=T, sep=",",
> na.strings=".")
>
> # Select SAS variable definition file...
> sas.def<-read.csv(file.choose())
>
> # str(sas.def)
> # sas.def$SASFORMAT[sas.def$Type=="Char"]<-"character"
> # sas.def$SASFORMAT[sas.def$Type=="Num"]<-"numeric"
>
> sas.def$SASFORMAT[substr(sas.def$Format,1,4)=="DATE"]<-"date"
>
> sas.def<-sas.def[,length(names(sas.def))] # pick last
> column
>
> tmp<-which(sas.def=="date")
>
> sas.data[,tmp] <-
> as.data.frame(strptime(sas.data[,tmp],
> "%d%b%Y:%H:%M:%S"))
>
> str(sas.data)
> print(head(sas.data))
>
> setwd(wrkdir) # reset working directory
>
> rm(wrkdir,tmp,sas.def)
>
> # the end
>
>
>
>
>       ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
===============================
WenSui Liu
Statistical Project Manager
ChoicePoint Precision Marketing
(http://spaces.msn.com/statcompute/blog)


From tim.calkins at gmail.com  Fri Dec 28 00:55:57 2007
From: tim.calkins at gmail.com (Tim Calkins)
Date: Fri, 28 Dec 2007 10:55:57 +1100
Subject: [R] Help with lm and multiple linear regression? (Plain Text
	version)
In-Reply-To: <439173.59898.qm@web35604.mail.mud.yahoo.com>
References: <439173.59898.qm@web35604.mail.mud.yahoo.com>
Message-ID: <ca5a2f2e0712271555r3f08783cq6ca66608ae57b5e4@mail.gmail.com>

consider merging everything into a singe dataframe.  i haven't tried
it, but something like the following could work:

> reg.data <- cbind(margin, personcoeff)
> names(reg.data) <- c('margin', 'p1', 'p2')
> lm(margin~p1+p2, data = reg.data)

the idea here is that by specifying the data frame with the data
argument in lm, R looks for the columns of the names specified in the
formula.

for weights, see ?lm and look for the weights argument.

cheers,
tc

On Dec 28, 2007 10:22 AM, Aaron Barzilai <aaron_barzilai at yahoo.com> wrote:
> (Apologies the previous version was sent as rich text)
>
> Hello,
> I'm new to R, but I've read the intro to R and successfully connected it to an instance of mysql.  I'm trying to perform multiple linear regression, but I'm having trouble using the lm function.  To start, I have read in a simply y matrix of values(dependent variable) and x matrix of independent variables.  It says both are data frames, but lm is giving me an error that my y variable is a list.
>
> Any suggestions on how to do this?  It's not clear to me what the problem is as they're both data frames.  My actual problem will use a much wider matrix of coefficients, I've only included two for illustration.
>
> Additionally, I'd actually like to weight the observations.  How would I go about doing that?  I also have that as a separate column vector.
>
> Thanks,
> Aaron
>
> Here's my session:
> > margin
>     margin
> 1    66.67
> 2   -58.33
> 3   100.00
> 4   -33.33
> 5   200.00
> 6   -83.33
> 7  -100.00
> 8     0.00
> 9   100.00
> 10  -18.18
> 11  -55.36
> 12 -125.00
> 13  -33.33
> 14 -200.00
> 15    0.00
> 16 -100.00
> 17   75.00
> 18    0.00
> 19 -200.00
> 20   35.71
> 21  100.00
> 22   50.00
> 23  -86.67
> 24  165.00
> > personcoeff
>    Person1 Person2
> 1       -1       1
> 2       -1       1
> 3       -1       1
> 4       -1       1
> 5       -1       1
> 6       -1       1
> 7        0       0
> 8        0       0
> 9        0       1
> 10      -1       1
> 11      -1       1
> 12      -1       1
> 13      -1       1
> 14      -1       0
> 15       0       0
> 16       0       0
> 17       0       1
> 18      -1       1
> 19      -1       1
> 20      -1       1
> 21      -1       1
> 22      -1       1
> 23      -1       1
> 24      -1       1
> > class(margin)
> [1] "data.frame"
> > class(personcoeff)
> [1] "data.frame"
> > lm(margin~personcoeff)
> Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  :
>         invalid type (list) for variable 'margin'
>
>
>       ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Tim Calkins
0406 753 997


From NordlDJ at dshs.wa.gov  Fri Dec 28 00:58:46 2007
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Thu, 27 Dec 2007 15:58:46 -0800
Subject: [R] Conditionally incrementing a loop counter
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1A@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE1A@MAILBE2.westat.com>
Message-ID: <941871A13165C2418EC144ACB212BDB04E1474@dshsmxoly1504g.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Mike Jones
> Sent: Thursday, December 27, 2007 1:35 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Conditionally incrementing a loop counter
> 
> Hi,
> I am trying a for loop from 1 to 100 by 1. However, if a 
> condition does
> not get met, I want to "throw away" that iteration. So if my loop is
> for (i in 1:100)
> and i is say, 25 and the condition is not met then I don't 
> want i to go
> up to 26.  Is there a way to do that? I can't seem to 
> manually adjust i
> because from what I understand, R creates 100 long vector and 
> uses that
> to "loops thru" and I'm not sure how to get at the index of 
> that vector.
> Any suggestions? Thanks in advance.
> 
> 
> 
> 
> Mike Jones
> Westat
> 1650 Research Blvd. RE401
> Rockville, MD 20850
> Ph: 240.314.2312
> 

Mike,

A question about R from SAS Mecca ??? :-)

I would use Peter Dalgaard's suggestion of a 'while' type loop along with the 'next' loop control, something like this

i <- 1
iter_number <- 0
while(i<=10){
   iter_number <- iter_number + 1
   x <- runif(1)
   if (x > 0.7) next
   cat(iter_number, i, x, "\n")
   i <- i+1
}

Where iter_number allows you to print out loop number each time the condition is met (for instructional purposes only).  When the if condition is met, the next  construct skips back to the top of the while loop without executing the rest of the statements.

Hope this is helpful,

Dan

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204
 
 


From jholtman at gmail.com  Fri Dec 28 01:39:32 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 27 Dec 2007 19:39:32 -0500
Subject: [R] Efficiency of for-loop in R
In-Reply-To: <dd66928314c6e.4773c868@usc.edu>
References: <dd66928314c6e.4773c868@usc.edu>
Message-ID: <644e1f320712271639g476d2a47r9160633698221394@mail.gmail.com>

Exactly "what is the problem you are trying to solve"?  Could you
"provide commented, minimal, self-contained, reproducible code"?

A lot depends on what you are trying to do,  There might be other
ways, in R, than a 'for' loop to solve your problems.

On Dec 27, 2007 6:44 PM, Tong Wang <wangtong at usc.edu> wrote:
> Hi,
>   I just realized that in Matlab, as long as memory is pre-allocated, doing for-loop doesn't cost more time than doing things in vector form.
>   But it seems in R, it still cost a lot to do for-loop.  Is there any improvement in R that I missed. Thanks a lot.
>
> Merry Xmas Everyone !
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Fri Dec 28 02:05:17 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 27 Dec 2007 20:05:17 -0500
Subject: [R] Reminiscing on 20 years using S
In-Reply-To: <39767564-D6E6-4AC2-B1A9-AFD0241B14C9@anu.edu.au>
References: <mailman.20.1198753204.4706.r-help@r-project.org>
	<39767564-D6E6-4AC2-B1A9-AFD0241B14C9@anu.edu.au>
Message-ID: <644e1f320712271705x28e5bcfesdfc3127f3ff1b797@mail.gmail.com>

My introduction to S was around 1984 on a 3B20 and VAX systems at Bell
Labs.  I still have a copy of the "brown" book written by Becker and
Chambers on the "S Interactive Language" (copyright 1984).  I remember
the "graphical" output on a daisy-wheel printer and using the HP
plotter that was connected in serial with the terminal you were using.
 When you wanted to plot, escape sequences were sent so the plotter
interpreted the output and plotted on paper that it moved back and
forth as the pens went horizontal.  It has changed a lot, but has also
stayed the same in a number of ways.

On Dec 27, 2007 5:12 PM, John Maindonald <john.maindonald at anu.edu.au> wrote:
> My first exposure to S was on an AT&T 3B2 (a 3B2/100,
> I think), at the Auckland (Mt Albert) Applied Mathematics
> Division Station of the NZ Dept of Scientific and Industrial
> Research.  The AMD Head Office in Wellington had one
> also.  There may have been one or more others; I cannot
> remember. This would have been in 1983, maybe.
>
> It was a superbly engineered machine, but the sofware
> (System V, version 3.2) had its problems.  If you back
> deleted too far along the command line, something
> unpleasant (losing the line? or worse?) happened.
> On typing 1+1 at the S command line, it took a second
> to get an answer.
>
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
>
>
> On 27 Dec 2007, at 10:00 PM, r-help-request at r-project.org wrote:
>
> > From: roger koenker <roger at ysidro.econ.uiuc.edu>
> > Date: 27 December 2007 9:56:45 AM
> > To: Greg Snow <Greg.Snow at imail.org>
> > Cc: R-help list <R-help at stat.math.ethz.ch>
> > Subject: Re: [R] Reminiscing on 20 years using S
> >
> > On Dec 26, 2007, at 2:05 PM, Greg Snow wrote:
> >
> >> I realized earlier this year (2007) that it was in 1987 that I first
> >> started using an early version of S (it was ported to VMS and was
> >> called
> >> success).  That means that I have been using some variant of S (to
> >> various degrees) for over 20 years now (I don't feel that old).
> >
> > Boxing day somehow seems appropriate for this thread.  R.I.P. to all
> > those old boxes
> > of yesteryore and the software that ran on them -- and yet there is
> > always a residual  archaeological curiosity.
> >
> > I discovered recently that the MIT athena network contains a circa
> > 1989 version
> > of S:  http://stuff.mit.edu/afs/athena/astaff/project/Sdev/S/  which
> > made me wonder
> > whether there was any likelihood that one could recreate "S Thu Dec
> > 7 16:49:47 EST 1989".
> > Curiosity is one thing, time to dig through the layers of ancient
> > civilizations is quite another.
> > But if anyone would like to offer a (preferably educated) guess
> > about the feasibility of  such a project, like I said, I would be
> > curious.
> >
> >
> > url:    www.econ.uiuc.edu/~roger                Roger Koenker
> > email   rkoenker at uiuc.edu                       Department of
> > Economics
> > vox:    217-333-4558                            University of Illinois
> > fax:    217-244-6678                            Champaign, IL 61820
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From p.dalgaard at biostat.ku.dk  Fri Dec 28 02:19:31 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 28 Dec 2007 02:19:31 +0100
Subject: [R] Conditionally incrementing a loop counter: Take 2
In-Reply-To: <000101c848e1$45722890$d05679b0$@ca>
References: <477428CE.4000801@biostat.ku.dk>
	<403593359CA56C4CAE1F8F4F00DCFE7D0CBECE20@MAILBE2.westat.com>
	<000101c848e1$45722890$d05679b0$@ca>
Message-ID: <47744F23.8070807@biostat.ku.dk>

John Fox wrote:
> Dear Mike,
>
> You could use a repeat loop and manage the index yourself:
>
> i <- 0
> repeat{
> 	x <- runif(1)
> 	if (x < .1){
> 	   i <- i + 1
> 	   cat("x = ", x, "\n")
> 	   }
>     if (i == 10) break
>     }
>
> But if your example problem reflects your actual application, why not just
> generate uniform random numbers on the interval (0, .1)?
>   

As I read it, it is the sequence of i's not the x's that are sought. The 
example suddenly skipped from something with x > .7 to something with x 
< .1, but it might be that Mike's while loop just needed to print i at 
the top of the loop rather than at the bottom. The result should 
(AFAICS) be equivalent to

 > rep(1:10, 1+rgeom(10, .7))
 [1]  1  1  2  3  4  5  6  7  8  8  9 10 10


> I hope this helps,
>  John
>
> --------------------------------
> John Fox, Professor
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
>
>
>   
>> -----Original Message-----
>> From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-
>> project.org] On Behalf Of Mike Jones
>> Sent: December-27-07 6:08 PM
>> To: Peter Dalgaard
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] Conditionally incrementing a loop counter: Take 2
>>
>> Since I didn't want the i to increment in the loop when the condition
>> is not met, then in my example I wanted the loop to actually run 14
>> times instead of the 10 since I wanted 4 of the iterations to be thrown
>> away, or ignored.  I still haven't been able to figure this out.  Going
>> the "while" route doesn't seem to work for me either.
>>
>>
>> nums <- numeric(10)
>> i <- 1
>> garbage <- 0
>>
>> while (i <= 10){
>> 	x <- runif(1)
>> 	cat("x = ",x,"\n")
>> 	if (x < 0.1){
>> 		nums[i] <- x
>> 		i <- i + 1
>> 	}
>> 	else{
>> 	        garbage <- garbage+1
>> 	}
>> cat("i = ",i,"garbage = ",garbage,"\n")
>> }
>>
>> -----Original Message-----
>> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
>> Sent: Thursday, December 27, 2007 5:36 PM
>> To: Mike Jones
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] Conditionally incrementing a loop counter: Take 2
>>
>>
>> Mike Jones wrote:
>>     
>>> My apologies for not including a working example.
>>>
>>> Here it is:
>>>
>>> for (i in 1:10){
>>>    cat("initial i = ",i,"\n")
>>>    x <- runif(1)
>>>    if (x > 0.7){
>>>       i <- i-1
>>>    }
>>>    cat("second i = ",i,"\n")
>>> }
>>>
>>> When I ran this i got what follows, so there were four cases where I
>>> wanted the i not to increment.
>>>
>>> initial i =  1
>>> second i =  1
>>> initial i =  2
>>> second i =  1
>>> initial i =  3
>>> second i =  3
>>> initial i =  4
>>> second i =  3
>>> initial i =  5
>>> second i =  4
>>> initial i =  6
>>> second i =  6
>>> initial i =  7
>>> second i =  7
>>> initial i =  8
>>> second i =  7
>>> initial i =  9
>>> second i =  9
>>> initial i =  10
>>> second i =  10
>>>
>>>
>>>       
>> Is this the kind of effect you want?
>>
>>  > x <- runif(10)
>>  > cbind(x, 1:10, cumsum(x < .7))
>>                 x
>>  [1,] 0.384165631  1 1
>>  [2,] 0.392715845  2 2
>>  [3,] 0.895936431  3 2
>>  [4,] 0.910242185  4 2
>>  [5,] 0.689987301  5 3
>>  [6,] 0.237071326  6 4
>>  [7,] 0.225032680  7 5
>>  [8,] 0.001856286  8 6
>>  [9,] 0.392034868  9 7
>> [10,] 0.655076045 10 8
>>
>> If you insist on using a loop, you need to separate the loop control
>> from the manipulation of i, as in (e.g.)
>>
>> i <- 0
>> for (j in 1:10){
>>    i <- i + 1
>>    cat("initial i = ",i,"\n")
>>    x <- runif(1)
>>    if (x > 0.7){
>>       i <- i-1
>>    }
>>    cat("second i = ",i,"\n")
>> }
>>
>>
>>     
>>>>  -----Original Message-----
>>>> From: 	Mike Jones
>>>> Sent:	Thursday, December 27, 2007 4:35 PM
>>>> To:	'r-help at lists.R-project.org'
>>>> Subject:	Conditionally incrementing a loop counter
>>>>
>>>> Hi,
>>>> I am trying a for loop from 1 to 10 by 1. However, if a condition
>>>> does not get met, I want to "throw away" that iteration. So if my
>>>> loop is for (i in 1:10) and i is say, 4 and the condition is not met
>>>> then I don't want i to go up to 5.  Is there a way to do that? I
>>>> can't seem to manually adjust i because from what I understand, R
>>>> creates 10 long vector and uses that to "loops thru" and I'm not
>>>>         
>> sure
>>     
>>>> how to get at the index of that vector. Any suggestions? Thanks in
>>>> advance.
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> Mike Jones
>>>> Westat
>>>> 1650 Research Blvd. RE401
>>>> Rockville, MD 20850
>>>> Ph: 240.314.2312
>>>>
>>>>
>>>>         
>>> 	[[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>       
>> --
>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
>> 35327918
>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
>> 35327907
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>     
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jholtman at gmail.com  Fri Dec 28 02:27:15 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 27 Dec 2007 20:27:15 -0500
Subject: [R] groupedData function not found
In-Reply-To: <BAY139-W10E5EF414B783C90B6749BC6540@phx.gbl>
References: <BAY139-W10E5EF414B783C90B6749BC6540@phx.gbl>
Message-ID: <644e1f320712271727k6b501eev744f9ae6038e7ba0@mail.gmail.com>

have you done:

library(nlme)

in your script?

On Dec 27, 2007 5:42 PM, andrea previtali <aprevitali at hotmail.com> wrote:
>
> Hello,
> I'm trying to use the groupedData function and R is giving me the message: Error: can not find function "groupedData"
> The code is coming from a textbook so I think it should be correct:
>
> pigs<-data.frame(cbind(pig.time,pig.id,pig.wt))
> pig.growth<-groupedData(pig.wt~pig.time|pig.id,data=pigs)
>
> I have added the package "nlme" and to confirm that it was installed correctly I requested the list of functions included in the package (library(help=nlme)) and I do see groupData in the list.
> I am using R 2.6.1 in Windows XP.
>
> I'll appreciate your help.
> Thanks,
> Andrea Previtali
> Post-doc fellow
> Dept. of Biology,
> Univ. of Utah, SLC, UT.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From wangtong at usc.edu  Fri Dec 28 02:39:56 2007
From: wangtong at usc.edu (Tong Wang)
Date: Thu, 27 Dec 2007 17:39:56 -0800
Subject: [R] Efficiency of for-loop in R
In-Reply-To: <644e1f320712271639g476d2a47r9160633698221394@mail.gmail.com>
References: <dd66928314c6e.4773c868@usc.edu>
	<644e1f320712271639g476d2a47r9160633698221394@mail.gmail.com>
Message-ID: <dd63f5d2170db.4773e36c@usc.edu>

HI,
   The question is meant to be a general one, I am trying to find out if there is new development in R that I might have missed. 

but here's a trivial example, 
       
To compute  y=sin(x) , x = 1,2,... 100000
 x=1:100000, 
1.  y =sin(x)
2.  for(i in 1:100000) y=sin(x[i])

1 is much faster than 2.  
Old Matlab also had this problem, but in new versions, 1 and 2 are mostly the same. 
I am just wondering if the same improvement has happened or will happen to R. 


Thanks . 

 

----- Original Message -----
From: jim holtman <jholtman at gmail.com>
Date: Thursday, December 27, 2007 4:39 pm
Subject: Re: [R] Efficiency of for-loop in R
To: Tong Wang <wangtong at usc.edu>
Cc: R help <r-help at stat.math.ethz.ch>

> Exactly "what is the problem you are trying to solve"?  Could you
> "provide commented, minimal, self-contained, reproducible code"?
> 
> A lot depends on what you are trying to do,  There might be other
> ways, in R, than a 'for' loop to solve your problems.
> 
> On Dec 27, 2007 6:44 PM, Tong Wang <wangtong at usc.edu> wrote:
> > Hi,
> >   I just realized that in Matlab, as long as memory is pre-
> allocated, doing for-loop doesn't cost more time than doing things 
> in vector form.
> >   But it seems in R, it still cost a lot to do for-loop.  Is 
> there any improvement in R that I missed. Thanks a lot.
> >
> > Merry Xmas Everyone !
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html> and provide commented, minimal, self-contained, 
> reproducible code.
> >
> 
> 
> 
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
> 
> What is the problem you are trying to solve?
>


From jholtman at gmail.com  Fri Dec 28 02:51:01 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 27 Dec 2007 20:51:01 -0500
Subject: [R] Efficiency of for-loop in R
In-Reply-To: <dd63f5d2170db.4773e36c@usc.edu>
References: <dd66928314c6e.4773c868@usc.edu>
	<644e1f320712271639g476d2a47r9160633698221394@mail.gmail.com>
	<dd63f5d2170db.4773e36c@usc.edu>
Message-ID: <644e1f320712271751v651fb8e2yb1220111b7957aa8@mail.gmail.com>

I will venture a guess in that in the case of the 'for' loop, you are
calling 'sin' 100,000 times incurring the cost of individual function
calls at the interpreter level plus evaluating 'for' loop.  In the
vectorized case, you are only interpreting a single 'sin' call and
then internally evaluating the 'sin' function, which is a lot faster.
Also the results are different in the two cases.  In case 1) you get
100000 values back in 'y' and in 2) you only get the value of the last
loop through the 'for' loop.

> system.time(for (i in 1:100000) y <- sin(i))
   user  system elapsed
   0.17    0.00    0.28
> str(y)
 num 0.0357
> system.time(y <- sin(1:100000))
   user  system elapsed
   0.01    0.00    0.02
> str(y)
 num [1:100000]  0.841  0.909  0.141 -0.757 -0.959 ...
>

R is typically optimized for vector type operations.

On Dec 27, 2007 8:39 PM, Tong Wang <wangtong at usc.edu> wrote:
> HI,
>   The question is meant to be a general one, I am trying to find out if there is new development in R that I might have missed.
>
> but here's a trivial example,
>
> To compute  y=sin(x) , x = 1,2,... 100000
>  x=1:100000,
> 1.  y =sin(x)
> 2.  for(i in 1:100000) y=sin(x[i])
>
> 1 is much faster than 2.
> Old Matlab also had this problem, but in new versions, 1 and 2 are mostly the same.
> I am just wondering if the same improvement has happened or will happen to R.
>
>
> Thanks .
>
>
>
>
> ----- Original Message -----
> From: jim holtman <jholtman at gmail.com>
> Date: Thursday, December 27, 2007 4:39 pm
> Subject: Re: [R] Efficiency of for-loop in R
> To: Tong Wang <wangtong at usc.edu>
> Cc: R help <r-help at stat.math.ethz.ch>
>
> > Exactly "what is the problem you are trying to solve"?  Could you
> > "provide commented, minimal, self-contained, reproducible code"?
> >
> > A lot depends on what you are trying to do,  There might be other
> > ways, in R, than a 'for' loop to solve your problems.
> >
> > On Dec 27, 2007 6:44 PM, Tong Wang <wangtong at usc.edu> wrote:
> > > Hi,
> > >   I just realized that in Matlab, as long as memory is pre-
> > allocated, doing for-loop doesn't cost more time than doing things
> > in vector form.
> > >   But it seems in R, it still cost a lot to do for-loop.  Is
> > there any improvement in R that I missed. Thanks a lot.
> > >
> > > Merry Xmas Everyone !
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html> and provide commented, minimal, self-contained,
> > reproducible code.
> > >
> >
> >
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 646 9390
> >
> > What is the problem you are trying to solve?
> >
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From aaron_barzilai at yahoo.com  Fri Dec 28 03:20:12 2007
From: aaron_barzilai at yahoo.com (Aaron Barzilai)
Date: Thu, 27 Dec 2007 18:20:12 -0800 (PST)
Subject: [R] Help with lm and multiple linear regression? (Plain Text
	version)
Message-ID: <490494.90334.qm@web35614.mail.mud.yahoo.com>

Tim (and others who responded privately),

Thanks for the help, this approach did work.  I have also reread ?lm a little more closely, I do see the weights functionality.

I have one last question: Now that I understand how to call this function and review the results, I want to extend it to my much larger real problem, with 100s of columns.  Is there a way to call the function in more of a matrix algebra syntax, where I would list the matrix(e.g. personcoeff) rather than the individual column names?  It seems like I might need to use lm.wfit, but per the help I'd rather use lm.

Thanks,
Aaron




----- Original Message ----
From: Tim Calkins <tim.calkins at gmail.com>
To: Aaron Barzilai <aaron_barzilai at yahoo.com>
Cc: r-help at r-project.org
Sent: Thursday, December 27, 2007 6:55:57 PM
Subject: Re: [R] Help with lm and multiple linear regression? (Plain Text version)

consider merging everything into a singe dataframe.  i haven't tried
it, but something like the following could work:

> reg.data <- cbind(margin, personcoeff)
> names(reg.data) <- c('margin', 'p1', 'p2')
> lm(margin~p1+p2, data = reg.data)

the idea here is that by specifying the data frame with the data
argument in lm, R looks for the columns of the names specified in the
formula.

for weights, see ?lm and look for the weights argument.

cheers,
tc

On Dec 28, 2007 10:22 AM, Aaron Barzilai <aaron_barzilai at yahoo.com> wrote:
> (Apologies the previous version was sent as rich text)
>
> Hello,
> I'm new to R, but I've read the intro to R and successfully connected it to an instance of mysql.  I'm trying to perform multiple linear regression, but I'm having trouble using the lm function.  To start, I have read in a simply y matrix of values(dependent variable) and x matrix of independent variables.  It says both are data frames, but lm is giving me an error that my y variable is a list.
>
> Any suggestions on how to do this?  It's not clear to me what the problem is as they're both data frames.  My actual problem will use a much wider matrix of coefficients, I've only included two for illustration.
>
> Additionally, I'd actually like to weight the observations.  How would I go about doing that?  I also have that as a separate column vector.
>
> Thanks,
> Aaron
>
> Here's my session:
> > margin
>    margin
> 1    66.67
> 2  -58.33
> 3  100.00
> 4  -33.33
> 5  200.00
> 6  -83.33
> 7  -100.00
> 8    0.00
> 9  100.00
> 10  -18.18
> 11  -55.36
> 12 -125.00
> 13  -33.33
> 14 -200.00
> 15    0.00
> 16 -100.00
> 17  75.00
> 18    0.00
> 19 -200.00
> 20  35.71
> 21  100.00
> 22  50.00
> 23  -86.67
> 24  165.00
> > personcoeff
>    Person1 Person2
> 1      -1      1
> 2      -1      1
> 3      -1      1
> 4      -1      1
> 5      -1      1
> 6      -1      1
> 7        0      0
> 8        0      0
> 9        0      1
> 10      -1      1
> 11      -1      1
> 12      -1      1
> 13      -1      1
> 14      -1      0
> 15      0      0
> 16      0      0
> 17      0      1
> 18      -1      1
> 19      -1      1
> 20      -1      1
> 21      -1      1
> 22      -1      1
> 23      -1      1
> 24      -1      1
> > class(margin)
> [1] "data.frame"
> > class(personcoeff)
> [1] "data.frame"
> > lm(margin~personcoeff)
> Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  :
>        invalid type (list) for variable 'margin'
>
>
>      ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Tim Calkins
0406 753 997


      ____________________________________________________________________________________
Be a better friend, newshound, and


From leeznar at yahoo.com.tw  Fri Dec 28 04:51:51 2007
From: leeznar at yahoo.com.tw (leeznar)
Date: Fri, 28 Dec 2007 11:51:51 +0800 (CST)
Subject: [R] How to catch data from the different dataframes and lm problem?
Message-ID: <692075.69409.qm@web73608.mail.tp2.yahoo.com>



Dear all: 


I am a new R-user and I have 2 questions
about it.  


1) I have a dataframe.  Based on ?formulation? and ?subject?, a
dataframe is split into 4 dataframes.  The
example is as follows.  Moreover, I want
to calculate ?test? value for these 4 dataframes.  My question is that the ?test? values not
correct and I do not know where the problem is.




2) There are 12 ?test? (y) values from 1).  Then, I want to model the relationship
between ?concentration? (X) and ?test? (Y) by fitting the linear regression,
such lm(Y~X) and this is my target.  I
think that if I can catch ?test? (Y) values and ?concentration? (X) values into
a dataframe, then I can carry out regression.  So, how to catch all ?test? values from
different dataframes?  Or does anyone
have better way to get this target?


 


Example: 


w<-(c(1,1,1,2,2,2,1,1,1,2,2,2))


y<-(c(1,1,1,1,1,1,2,2,2,2,2,2))


z<-(c(1,2,3,1,2,3,1,2,3,1,2,3))


c<-(c(0.1,10,20,0.3,2.5,6,20,25,60,35,40,45))


df<-data.frame(formulation=y, subject=w,
time=z, concentration=c)


A.split<-split(df, list(df$formulation,
df$subject) )


 


for (j in 1:length(A.split)){


test <- 0


for(i in
2:length(A.split[[j]][["time"]])){


   
test[i] <- (A.split[[1]][["time"]][i] -
A.split[[1]][["time"]][i-1]) *
(A.split[[1]][["concentration"]][i] -
A.split[[1]][["concentration"]][i-1])* 0.5


   
test[i]<-test[i]+test[i-1]


 }


output<-data.frame(A.split[[j]][["subject"]],A.split[[j]][["formulation"]],A.split[[j]][["time"]],A.split[[j]][["concentration"]],test)


colnames(output)<-list("subject","formulation","time","concentration","test")


show(output) 


}


Best regards,
Hsin-Ya Lee 







      _____________________________________________________________________________________
??????Yahoo!??????2.0? http://tw.mg0.mail.yahoo.com/dc/landing


From cskiadas at gmail.com  Fri Dec 28 08:08:56 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Fri, 28 Dec 2007 02:08:56 -0500
Subject: [R] Help with lm and multiple linear regression? (Plain Text
	version)
In-Reply-To: <490494.90334.qm@web35614.mail.mud.yahoo.com>
References: <490494.90334.qm@web35614.mail.mud.yahoo.com>
Message-ID: <D0D0136E-FFF3-4DDD-85C9-FB7F9848F04D@gmail.com>

Hi Aaron,

if I understand your question correctly, you can use the "." in the  
formula, like so:

dat <- data.frame(x=1:10,y=rnorm(10),z=10:1)
lm(x~., data=dat)


The dot there stands for everything not already specified, so in this  
case that would be y and z (since x is already on the lhs). You can  
even omit things from . with the minus operator:

lm(x~.-y, data=dat)

This will do a regression of x on z only (and a constant of course).

You can see another approach in the examples section of the help  
entry on ?formula (I was led to that entry from reading the Details  
section of ?lm).

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


On Dec 27, 2007, at 9:20 PM, Aaron Barzilai wrote:

> Tim (and others who responded privately),
>
> Thanks for the help, this approach did work.  I have also reread ? 
> lm a little more closely, I do see the weights functionality.
>
> I have one last question: Now that I understand how to call this  
> function and review the results, I want to extend it to my much  
> larger real problem, with 100s of columns.  Is there a way to call  
> the function in more of a matrix algebra syntax, where I would list  
> the matrix(e.g. personcoeff) rather than the individual column  
> names?  It seems like I might need to use lm.wfit, but per the help  
> I'd rather use lm.
>
> Thanks,
> Aaron
>
>
>
>
> ----- Original Message ----
> From: Tim Calkins <tim.calkins at gmail.com>
> To: Aaron Barzilai <aaron_barzilai at yahoo.com>
> Cc: r-help at r-project.org
> Sent: Thursday, December 27, 2007 6:55:57 PM
> Subject: Re: [R] Help with lm and multiple linear regression?  
> (Plain Text version)
>
> consider merging everything into a singe dataframe.  i haven't tried
> it, but something like the following could work:
>
>> reg.data <- cbind(margin, personcoeff)
>> names(reg.data) <- c('margin', 'p1', 'p2')
>> lm(margin~p1+p2, data = reg.data)
>
> the idea here is that by specifying the data frame with the data
> argument in lm, R looks for the columns of the names specified in the
> formula.
>
> for weights, see ?lm and look for the weights argument.
>
> cheers,
> tc
>
> On Dec 28, 2007 10:22 AM, Aaron Barzilai <aaron_barzilai at yahoo.com>  
> wrote:
>> (Apologies the previous version was sent as rich text)
>>
>> Hello,
>> I'm new to R, but I've read the intro to R and successfully  
>> connected it to an instance of mysql.  I'm trying to perform  
>> multiple linear regression, but I'm having trouble using the lm  
>> function.  To start, I have read in a simply y matrix of values 
>> (dependent variable) and x matrix of independent variables.  It  
>> says both are data frames, but lm is giving me an error that my y  
>> variable is a list.
>>
>> Any suggestions on how to do this?  It's not clear to me what the  
>> problem is as they're both data frames.  My actual problem will  
>> use a much wider matrix of coefficients, I've only included two  
>> for illustration.
>>
>> Additionally, I'd actually like to weight the observations.  How  
>> would I go about doing that?  I also have that as a separate  
>> column vector.
>>
>> Thanks,
>> Aaron
>>
>> Here's my session:
>>> margin
>>    margin
>> 1    66.67
>> 2  -58.33
>> 3  100.00
>> 4  -33.33
>> 5  200.00
>> 6  -83.33
>> 7  -100.00
>> 8    0.00
>> 9  100.00
>> 10  -18.18
>> 11  -55.36
>> 12 -125.00
>> 13  -33.33
>> 14 -200.00
>> 15    0.00
>> 16 -100.00
>> 17  75.00
>> 18    0.00
>> 19 -200.00
>> 20  35.71
>> 21  100.00
>> 22  50.00
>> 23  -86.67
>> 24  165.00
>>> personcoeff
>>    Person1 Person2
>> 1      -1      1
>> 2      -1      1
>> 3      -1      1
>> 4      -1      1
>> 5      -1      1
>> 6      -1      1
>> 7        0      0
>> 8        0      0
>> 9        0      1
>> 10      -1      1
>> 11      -1      1
>> 12      -1      1
>> 13      -1      1
>> 14      -1      0
>> 15      0      0
>> 16      0      0
>> 17      0      1
>> 18      -1      1
>> 19      -1      1
>> 20      -1      1
>> 21      -1      1
>> 22      -1      1
>> 23      -1      1
>> 24      -1      1
>>> class(margin)
>> [1] "data.frame"
>>> class(personcoeff)
>> [1] "data.frame"
>>> lm(margin~personcoeff)
>> Error in model.frame(formula, rownames, variables, varnames,  
>> extras, extranames,  :
>>        invalid type (list) for variable 'margin'


From ripley at stats.ox.ac.uk  Fri Dec 28 08:59:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 28 Dec 2007 07:59:13 +0000 (GMT)
Subject: [R] Lda and Qda
In-Reply-To: <1198797266.477431d26f609@gold5.portugalmail.pt>
References: <1198797266.477431d26f609@gold5.portugalmail.pt>
Message-ID: <Pine.LNX.4.64.0712280751520.29436@gannet.stats.ox.ac.uk>

?lda explains the object produced: please do study it.

Hint: you asked for leave-one-out cross-validation, and what is the output 
from cross-validation of a classifer?  The predicted class for each 
observation.  How many observations do you have?

You are using software from a contributed package without credit, and that 
software is support for a book (see library(help=MASS) and the help page). 
Please consult the book for the background.

On Thu, 27 Dec 2007, pedrosmarques at portugalmail.pt wrote:

>
>
> Hi all,
>
> I'm working with some data: 54 variables and a column of classes, each 
> observation as one of a possible seven different classes:
>
>> var.can3<-lda(x=dados[,c(1:28,30:54)],grouping=dados[,55],CV=TRUE)
> Warning message:
> In lda.default(x, grouping, ...) : variables are collinear
>> summary(var.can3)
>          Length Class  Mode
> class      30000 factor numeric   ### why?? I don't understand it
> posterior 210000 -none- numeric
> call           4 -none- call    ## what's this?
>
>
>> var.can<-lda(dados[,c(1:28,30:54)],dados[,55])#porque a variavel 29 ? constante
> Warning message:
> In lda.default(x, grouping, ...) : variables are collinear
>> summary(var.can)
>        Length Class  Mode
> prior     7    -none- numeric
> counts    7    -none- numeric
> means   371    -none- numeric
> scaling 318    -none- numeric
> lev       7    -none- character
> svd       6    -none- numeric
> N         1    -none- numeric
> call      3    -none- call
>> (normalizar<-function(matriz){ n<-dim(matriz)[1]; m<-dim(matriz)[2]; normas<-sqrt(colSums(matriz*matriz)); matriz.normalizada<-matriz/t(matrix(rep(normas,n),m,n));return(matriz.normalizada)})
> function(matriz){ n<-dim(matriz)[1]; m<-dim(matriz)[2]; normas<-sqrt(colSums(matriz*matriz)); matriz.normalizada<-matriz/t(matrix(rep(normas,n),m,n));return(matriz.normalizada)}
>> var.canonicas<-as.matrix(dados[,c(1:28,30:54)])%*%(normalizar(var.can$scaling))
>> summary(var.canonicas)
>      LD1               LD2              LD3               LD4
> Min.   :-21.942   Min.   :-6.820   Min.   :-10.138   Min.   :-6.584
> 1st Qu.:-20.014   1st Qu.:-5.480   1st Qu.: -8.280   1st Qu.: 0.872
> Median :-19.495   Median :-5.007   Median : -7.800   Median : 1.083
> Mean   :-18.827   Mean   :-4.760   Mean   : -7.803   Mean   : 1.134
> 3rd Qu.:-18.975   3rd Qu.:-4.456   3rd Qu.: -7.278   3rd Qu.: 1.311
> Max.   : -7.886   Max.   : 3.116   Max.   : -1.619   Max.   : 5.556
>      LD5               LD6
> Min.   :-11.083   Min.   :-4.4972
> 1st Qu.: -1.237   1st Qu.:-1.6497
> Median : -1.100   Median :-1.0909
> Mean   : -1.100   Mean   :-0.9808
> 3rd Qu.: -0.957   3rd Qu.:-0.4598
> Max.   :  4.712   Max.   : 7.5356
>>
>
>
> I don't know wether I need to specify a training set and a testing set, 
> I also don't know the error nor the classifier; shouldn't the lenght of 
> class of var.can3 be 7 since I only have 7 different classes?
>
> Best regards,
>
> Pedro Marques
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Fri Dec 28 09:09:37 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 28 Dec 2007 08:09:37 +0000 (GMT)
Subject: [R] warning on gamma option in par(args) or calling par(= new)?
In-Reply-To: <00af01c848d5$9649c4f0$3301a8c0@MainDomain.local>
References: <00af01c848d5$9649c4f0$3301a8c0@MainDomain.local>
Message-ID: <Pine.LNX.4.64.0712280804420.29436@gannet.stats.ox.ac.uk>

You are calling par() *before* opening  and (in on.exit) *after* closing 
the pdf() device.  par() applies on a per-device basis, and if no device 
is open the default device will be opened.

As for why you get the warnings about 'gamma' and 'new', see ?par and read 
their entries.  Since you don't change any pars, I do not see why you are 
attempting to reset them.


On Thu, 27 Dec 2007, AA wrote:

> Dear All,
>
> I have the following function
>
> tstpar <-
>  function(n = 200, want.pdf = FALSE, pdfFileName = NULL){
>    oldpar <- par(no.readonly = TRUE)
>    on.exit(par(oldpar))
>    steps  <- seq(from = 1, to = 8, by = 1)
>    h <- 10; w <- 6
>    if(want.pdf){pdf(file = pdfFileName, onefile = TRUE,
>    paper = "letter", width = w, height = h)}
>    par(mfrow = c(4,2))
>    for(i in steps){
>      txt <- paste("i = ", i)
>      hist(rnorm(n), main = txt)
>    }
>    if(want.pdf){dev.off()}
>  }
>
> when called with default values tstpar() every thing works fine.
> However I get 2 sorts of warnings
> 1- if I call the function to generate a pdf as
>   tstpar(want.pdf = TRUE, pdfFilename = "jj.pdf")
>   I get the following warning msg:
>   calling par(new=) with no plot in: par(oldpar)
>   with an empty device which I don not understand since I close
>   the device in the function.
> 2- If I comment the line
>   if (want.pdf){dev.off()}
>   I get the following warning
>   'gamma' cannot be modified on this device in: par(args)
>   with 2 empty devices.
>
> I do not understand why I get those warnings and why I get those empty devices.
> I looked up in RSiteSearch and I found the 2 following posts
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/38553.html
> In this post M. Schwarz explains that there is not a plot created that's why R generates the
> warning. In the case of tstpar, I generate the hist plots and should not be getting the warning.
>
> And in the following post Prof Ripley refers to an old R warning for pdf() devices
> Which is not the case here.
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/23215.html
>
> I do not see what I am missing. Thanks for any hint.
> I use R 2.5.1 under Win XP.
> I apologize if the question is related to my older R version which I will upgrade.
>
> AA.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Richard.Cotton at hsl.gov.uk  Fri Dec 28 09:55:43 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Fri, 28 Dec 2007 08:55:43 +0000
Subject: [R] How to catch data from the different dataframes and lm
	problem?
In-Reply-To: <692075.69409.qm@hsl.gov.uk>
Message-ID: <OF7C8183DE.A73E87C1-ON802573BF.0030013E-802573BF.0031060C@hsl.gov.uk>

> 1) I have a dataframe.  Based on ?formulation? and ?subject?, a
> dataframe is split into 4 dataframes.  The
> example is as follows.  Moreover, I want
> to calculate ?test? value for these 4 dataframes.  My question is 
> that the ?test? values not
> correct and I do not know where the problem is.
What does the variable "test" represent?  It isn't clear from your code 
what it means.  (Currently it looks like a cumulative rate of change of 
concentration with respect to time; are you really sure you want this?)

> 2) There are 12 ?test? (y) values from 1).  Then, I want to model 
> the relationship
> between ?concentration? (X) and ?test? (Y) by fitting the linear 
regression,
> such lm(Y~X) and this is my target.  I
> think that if I can catch ?test? (Y) values and ?concentration? (X) 
> values into
> a dataframe, then I can carry out regression.  So, how to catch all 
> ?test? values from
> different dataframes?  Or does anyone
> have better way to get this target?
Once you have all your "test" values, you probably want to perform a 
regression on all the data, in a single data frame, with "formulation" and 
"subject" as effects, e.g.
lm(test~concentration+formulation+subject, data=df)

If the subjects are drawn from a large population, it is better to 
represent them as random effects in a mixed effects model.
library(nlme)
lme(test~concentration+formulaion, data=df, random=~1|subject)

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential information intended
for the addressee(s) only. If this message was sent to you in error,
you must not disseminate, copy or take any action in reliance on it and
we request that you notify the sender immediately by return email.

Opinions expressed in this message and any attachments are not
necessarily those held by the Health and Safety Laboratory or any person
connected with the organisation, save those by whom the opinions were
expressed.

Please note that any messages sent or received by the Health and Safety
Laboratory email system may be monitored and stored in an information
retrieval system.
------------------------------------------------------------------------

------------------------------------------------------------------------
This e-mail message has been scanned for Viruses and Content and cleared 
by NetIQ MailMarshal
------------------------------------------------------------------------

From jim at bitwrit.com.au  Fri Dec 28 11:01:41 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Fri, 28 Dec 2007 21:01:41 +1100
Subject: [R] Conditionally incrementing a loop counter: Take 2
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE20@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE20@MAILBE2.westat.com>
Message-ID: <4774C985.8040807@bitwrit.com.au>

Mike Jones wrote:
>
>Hi,
>I am trying a for loop from 1 to 10 by 1. However, if a condition 
>does not get met, I want to "throw away" that iteration. So if my 
>loop is for (i in 1:10) and i is say, 4 and the condition is not met 
>then I don't want i to go up to 5.  Is there a way to do that? I 
>can't seem to manually adjust i because from what I understand, R 
>creates 10 long vector and uses that to "loops thru" and I'm not sure 
>how to get at the index of that vector. Any suggestions? Thanks in 
>advance.
>
Hi Mike,
Is this what you want?

i<-1
while(i < 11) {
  if(runif(1) < 0.5) i<-i+1
  print(i)
}

This increments if the condition is met, doesn't if it is not met.

Jim


From r.mueller at oeko-sorpe.de  Fri Dec 28 12:24:52 2007
From: r.mueller at oeko-sorpe.de (Richard =?iso-8859-1?q?M=FCller?=)
Date: Fri, 28 Dec 2007 12:24:52 +0100
Subject: [R] Return Value of TCl/Tk window in R
Message-ID: <200712281224.53027.r.mueller@oeko-sorpe.de>

Hello,
I have the TCl/Tk command 
"tkmessageBox(titel="",message="x",icon="question",type="okcancel")" in my R 
script. Now I want to perform some operation in relation to the user's 
choice, something like
"if (okpressed) xxx else yyy"
What values does this command give and how are they used?
Thank you, Richard
-- 
Richard M?ller - Am Spring 9 - D-58802 Balve-Eisborn
www.oeko-sorpe.de


From ripley at stats.ox.ac.uk  Fri Dec 28 12:52:18 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 28 Dec 2007 11:52:18 +0000 (GMT)
Subject: [R] Return Value of TCl/Tk window in R
In-Reply-To: <200712281224.53027.r.mueller@oeko-sorpe.de>
References: <200712281224.53027.r.mueller@oeko-sorpe.de>
Message-ID: <Pine.LNX.4.64.0712281145450.3365@gannet.stats.ox.ac.uk>

Is it so hard to find out?

Your tcl documentation will tell you what tk_messageBox returns, and as it 
is tcl string, you need to call tclvalue() on the value of tkmessageBox()
to get an R character vector.

On Fri, 28 Dec 2007, Richard M?ller wrote:

> Hello,
> I have the TCl/Tk command
> "tkmessageBox(titel="",message="x",icon="question",type="okcancel")" in my R
> script. Now I want to perform some operation in relation to the user's
> choice, something like
> "if (okpressed) xxx else yyy"
> What values does this command give and how are they used?
> Thank you, Richard
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From albmont at centroin.com.br  Fri Dec 28 12:56:01 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 28 Dec 2007 09:56:01 -0200
Subject: [R] Return Value of TCl/Tk window in R
In-Reply-To: <200712281224.53027.r.mueller@oeko-sorpe.de>
References: <200712281224.53027.r.mueller@oeko-sorpe.de>
Message-ID: <20071228115339.M37266@centroin.com.br>

Richard M?ller wrote:
>
> I have the TCl/Tk command 
> "tkmessageBox(titel="",message="x",icon="question",type="okcancel")" 
> in my R script. Now I want to perform some operation in relation to 
> the user's choice, something like "if (okpressed) xxx else yyy" What 
> values does this command give and how are they used? 
>
Why don't you test it yourself?

library(tcltk)
x <-  tkmessageBox(title="",message="x",icon="question",type="okcancel")
# press x or cancel
x
# <Tcl> ok or <Tcl> cancel

To get back from this Tcl-thing to an R-string, use

y <- tclvalue(x)

Alberto Monteiro


From samu.mantyniemi at helsinki.fi  Fri Dec 28 13:16:28 2007
From: samu.mantyniemi at helsinki.fi (=?ISO-8859-1?Q?Samu_M=E4ntyniemi?=)
Date: Fri, 28 Dec 2007 14:16:28 +0200
Subject: [R] Return Value of TCl/Tk window in R
In-Reply-To: <200712281224.53027.r.mueller@oeko-sorpe.de>
References: <200712281224.53027.r.mueller@oeko-sorpe.de>
Message-ID: <4774E91C.7000307@helsinki.fi>

This webpage has been very helpful for me:

http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/


-Samu

Richard M?ller kirjoitti:
> Hello,
> I have the TCl/Tk command 
> "tkmessageBox(titel="",message="x",icon="question",type="okcancel")" in my R 
> script. Now I want to perform some operation in relation to the user's 
> choice, something like
> "if (okpressed) xxx else yyy"
> What values does this command give and how are they used?
> Thank you, Richard


-- 
------------------------------------------
Samu M?ntyniemi
Researcher
Fisheries and Environmental Management Group (FEM)
Department of Biological and Environmental Sciences
Biocenter 3, room 4414
Viikinkaari 1
P.O. Box 65
FIN-00014 University of Helsinki

Phone: +358 9 191 58710
Fax: +358 9 191 58257

email: samu.mantyniemi at helsinki.fi
personal webpage: http://www.helsinki.fi/people/samu.mantyniemi/
FEM webpage: http://www.helsinki.fi/science/fem/


From bgreen at dyson.brisnet.org.au  Fri Dec 28 13:24:28 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Fri, 28 Dec 2007 22:24:28 +1000
Subject: [R] index question
In-Reply-To: <mailman.17.1198494003.12824.r-help@r-project.org>
References: <mailman.17.1198494003.12824.r-help@r-project.org>
Message-ID: <20071228122036.91A5E594F1E@borg.st.net.au>

I was hoping for some advice regarding indexing,

 From a dataframe there are 27 variables of interest, with the prefix of "pre".

  [7] "Decision"  "MHCDate"   "pre01"     "pre01111"  "pre012"    "pre013"
[13] "pre02"     "pre02111"  "pre02114"  "pre0211"   "pre0212"   "pre029"
[19] "pre03a"    "pre0311"   "pre0312"   "pre03"     "pre04"     "pre05"
[25] "pre06"     "pre07"     "pre08"     "pre09"     "pre10"     "pre11"
[31] "pre12"     "pre13"     "pre14"     "pre15"     "pre16"

I want to combine these variables into new variables, using the 
following criteria :

(1) create a single variable PRE, when any of the 27 'pre' variables 
have a value >= '1'
(2) create a variable HOM, when any of the pre01, pre01111, pre012, 
pre013 variables have a value >= '1'
(3) create a variable ASS, when any of the pre02, pre02111, pre02114, 
pre0211, pre0212, pre029  variables have a value   >= '1'
(4) create a variable SEX, when any of the pre03a, pre0311, pre0312, 
pre03 variables have a value   >= '1'
(5) create a variable VIO, when any of the pre01 to pre06 variables 
have a value   >= '1'
(6) create a variable SERASS. If pre02111 or pre2114 >= '1', assign a 
value of 1, if there is a value of 1 or greater for pre0211 assign a 
value of 2; &  if there is a value of
1 or greater for pre0212: assign a value of 3;  if there is a value 
of 1 or greater for pre2029 assign a value of 4; everything else = 0. 
If a case has multiple values, 02111 prevails over 2114, 2114 
prevails over 0211, 0211 prevails over 0212; 0212 prevails over 2029.


I believe I can generate new variables (1) - (5) using code such 
as:  ASS <- (reoffend$pre02 | reoffend$pre02111 | reoffend$pre02114 | 
reoffend$pre0211 | reoffend$pre0212 | reoffend$pre029 >= '1')


I have three questions:

1. If this is correct, what is the most efficient way to generate (1) 
without having to type all the variable names. The following does not 
work: PRE <- reoffend [,9:35], >= '1'

2. I am unsure as to how to generate Example 6.

3. I wanted to exclude cases with a reoffend$Decision of value of 3, 
using the code below. However, I received a message saying there were 
NAs produced, however, the raw variable did not have NAs.

 > MHT.decision <- reoffend[reoffend$Decision >= '2',]
 > table(MHT.decision)
Error in vector("integer", length) : vector size cannot be NA
In addition: Warning messages:
1: NAs produced by integer overflow in: pd * (as.integer(cat) - 1L)
2: NAs produced by integer overflow in: pd * nl

 > table(reoffend$Decision)
    1    2    3
1136  445   66


Any assistance is much appreciated,

Bob Green


From Richard.Cotton at hsl.gov.uk  Fri Dec 28 14:23:25 2007
From: Richard.Cotton at hsl.gov.uk (Richard.Cotton at hsl.gov.uk)
Date: Fri, 28 Dec 2007 13:23:25 +0000
Subject: [R] index question
In-Reply-To: <20071228122036.91A5E594F1E@hsl.gov.uk>
Message-ID: <OF59D107D2.E9649332-ON802573BF.00474572-802573BF.00498824@hsl.gov.uk>

>  From a dataframe there are 27 variables of interest, with the 
> prefix of "pre".
> 
>   [7] "Decision"  "MHCDate"   "pre01"     "pre01111"  "pre012" "pre013"
> [13] "pre02"     "pre02111"  "pre02114"  "pre0211"   "pre0212" "pre029"
> [19] "pre03a"    "pre0311"   "pre0312"   "pre03"     "pre04"     "pre05"
> [25] "pre06"     "pre07"     "pre08"     "pre09"     "pre10"     "pre11"
> [31] "pre12"     "pre13"     "pre14"     "pre15"     "pre16"
> 
> I want to combine these variables into new variables, using the 
> following criteria :
> 
> (1) create a single variable PRE, when any of the 27 'pre' variables 
> have a value >= '1'
> (2) create a variable HOM, when any of the pre01, pre01111, pre012, 
> pre013 variables have a value >= '1'
> (3) create a variable ASS, when any of the pre02, pre02111, pre02114, 
> pre0211, pre0212, pre029  variables have a value   >= '1'
> (4) create a variable SEX, when any of the pre03a, pre0311, pre0312, 
> pre03 variables have a value   >= '1'
> (5) create a variable VIO, when any of the pre01 to pre06 variables 
> have a value   >= '1'
> (6) create a variable SERASS. If pre02111 or pre2114 >= '1', assign a 
> value of 1, if there is a value of 1 or greater for pre0211 assign a 
> value of 2; &  if there is a value of
> 1 or greater for pre0212: assign a value of 3;  if there is a value 
> of 1 or greater for pre2029 assign a value of 4; everything else = 0. 
> If a case has multiple values, 02111 prevails over 2114, 2114 
> prevails over 0211, 0211 prevails over 0212; 0212 prevails over 2029.
> 
> 
> I believe I can generate new variables (1) - (5) using code such 
> as:  ASS <- (reoffend$pre02 | reoffend$pre02111 | reoffend$pre02114 | 
> reoffend$pre0211 | reoffend$pre0212 | reoffend$pre029 >= '1')
> 
> 
> I have three questions:
> 
> 1. If this is correct, what is the most efficient way to generate (1) 
> without having to type all the variable names. The following does not 
> work: PRE <- reoffend [,9:35], >= '1'

Try something like this (data frame simplified):
df <- data.frame(pre1=c(0,1,1,2),
                 pre2=c(0,0,1,0),
                 foo=c(0,0,1,3))
precols <- grep("pre", names(df))
gt1 <- function(x) x>=1
PRE <- apply(apply(df[,precols], 2, gt1), 1, any)


> 2. I am unsure as to how to generate Example 6.
SERASS <- rep(0, nrow(df))
SERASS[df$pre2029>=1] <- 4
SERASS[df$pre0212>=1] <- 3
SERASS[df$pre0211>=1] <- 2
SERASS[df$pre02111>=1 | df$pre2114>=1] <- 1


> 3. I wanted to exclude cases with a reoffend$Decision of value of 3, 
> using the code below. However, I received a message saying there were 
> NAs produced, however, the raw variable did not have NAs.
> 
>  > MHT.decision <- reoffend[reoffend$Decision >= '2',]
>  > table(MHT.decision)
> Error in vector("integer", length) : vector size cannot be NA
> In addition: Warning messages:
> 1: NAs produced by integer overflow in: pd * (as.integer(cat) - 1L)
> 2: NAs produced by integer overflow in: pd * nl
> 
>  > table(reoffend$Decision)
>     1    2    3
> 1136  445   66

I doubt that you want quotes around the '2' when defining MHT.decision.

Regards,
Richie.

Mathematical Sciences Unit
HSL


------------------------------------------------------------------------
ATTENTION:

This message contains privileged and confidential inform...{{dropped:20}}


From office at matthiaswendel.de  Fri Dec 28 14:40:29 2007
From: office at matthiaswendel.de (Matthias Wendel)
Date: Fri, 28 Dec 2007 14:40:29 +0100
Subject: [R] encoding question again
Message-ID: <005d01c84957$36654cd0$15b2a8c0@lifebook2>

 
Hi, 
I'm running the actual R version in JGR (version 1.5-8 ). 
Sys.getlocale(category = "LC_ALL") yields 
	[1] "LC_COLLATE=German_Germany.1252;LC_CTYPE=German_Germany.1252;LC_MONETARY=German_Germany.1252;LC_NUMERIC=C;LC_TIME=German_Germany.1252"

I want to write some HTML-Code enhanced by statistical results and labels encoded in Latin-1, which I pass to a function. Some label shall generate the filename. Although the labels are correctly handled in JGR they are somehow converted when they are written to the file. Also the filename is not constructed as wanted. The function definition is correctly sourced into R. The function is defined like this:

Itemtabelle.head <- function (abt ){
   # n?r z?m T?ST
   abt = iconv(abt,"UTF-8","LATIN1")   
   zz = file( paste("Itemtabelle/Itemtabelle", abt, ".html"), "wt", encoding = "LATIN1")
   cat(as.character("<html xmlns:o=\"urn:schemas-microsoft-com:office:office\" xmlns:x=\"urn:schemas-microsoft-com:office:excel\" xmlns=\"http://www.w3.org/TR/REC-html40\">  \n"),
       as.character("   <head>                                                                                                                                                \n"),
		.
		.
		.
       as.character("        <td colspan=5 class=xl28 width=727 style=\'width:545pt\'>Gesundheitsindikatoren:  "), abt, as.character("</td>                                   \n"),
       as.character("       </tr>                                                                                                                                               "), file = zz)
       close(zz)
       unlink(zz)
}
Setting abt as " ?rzte Innere, Gyn?kologie" and calling the function with this argument, yields a filename "Itemtabelle  ??rzte Innere, Gyn??kologie .html" and in the file a line 
         <td colspan=5 class=xl28 width=727 style='width:545pt'>Gesundheitsindikatoren:    ????rzte Innere, Gyn????kologie </td>  
is generated.                                 .
The problem remains the same in the rgui and rterm - except in rterm the resulting filename is "Itemtabelle ?rzte Innere, Gyn?kologie  .html".

Cheers,
Matthias
 


From shubhak at ambaresearch.com  Fri Dec 28 15:38:20 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Fri, 28 Dec 2007 20:08:20 +0530
Subject: [R] Accesing the value
Message-ID: <A36876D3F8A5734FA84A4338135E7CC302EB421F@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071228/e7b77f0b/attachment.pl 

From sundar.dorai-raj at pdf.com  Fri Dec 28 15:59:21 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 28 Dec 2007 08:59:21 -0600
Subject: [R] Accesing the value
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC302EB421F@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC302EB421F@BAN-MAILSRV03.Amba.com>
Message-ID: <47750F49.1060004@pdf.com>

get(x)

This is a FAQ:

http://cran.cnr.berkeley.edu/doc/FAQ/R-FAQ.html#How-can-I-turn-a-string-into-a-variable_003f

--sundar

Shubha Vishwanath Karanth said the following on 12/28/2007 8:38 AM:
> Hi R,
> 
>  
> 
> x="A"
> 
> A=5
> 
>  
> 
> I need to get the value of A using x only. How do I do this?
> 
>  
> 
> Thanks in advance, Shubha
> 
> This e-mail may contain confidential and/or privileged i...{{dropped:13}}
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sgwater at stanford.edu  Fri Dec 28 16:35:37 2007
From: sgwater at stanford.edu (Sharon Goldwater)
Date: Fri, 28 Dec 2007 07:35:37 -0800
Subject: [R] logistic mixed effects models with lmer
Message-ID: <7b8a41740712280735u6e6a57f0tb640f37cbe987648@mail.gmail.com>

I have a question about some strange results I get when using lmer to
build a logistic mixed effects model.  I have a data set of about 30k
points, and I'm trying to do backwards selection to reduce the number
of fixed effects in my model.  I've got 3 crossed random effects and
about 20 or so fixed effects.  At a certain point, I get a model (m17)
where the fixed effects are like this (full output is at end of
message):

> print(m17, corr=F)
...
Fixed effects:
                       Estimate Std. Error z value Pr(>|z|)
(Intercept)            -1.97887    0.19699 -10.045  < 2e-16 ***
sexM                    0.45553    0.14387   3.166 0.001544 **
...
is_discTRUE             0.24676    0.15204   1.623 0.104576
poly(wfreq, 2)1      -119.72397   11.00516 -10.879  < 2e-16 ***
poly(wfreq, 2)2        17.35646    5.44456   3.188 0.001433 **
poly(wlen_p, 2)1      -13.60798    7.26926  -1.872 0.061208 .
poly(wlen_p, 2)2       -6.43167    5.24119  -1.227 0.219770
...

where poly(wlen_p,2)2 is the least significant factor left in the
model.  So I then build a model (m18) with exactly the same random and
fixed effects except removing poly(wlen_p,2)2.  Then I do an anova,
and I get:

> anova(m17,m18)
Data:
Models:
m18: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
m17:     after_part + first_rep + is_open + is_disc + poly(wfreq,
m18:     2) + wlen_p + poly(utt_rate, 2) + poly(dur, 2) + pmean +
m17:     poly(log_prange, 2) + poly(imean, 2) + poly(irange, 2) +
m18:     (1 | speaker) + (1 | corpus) + (1 | ref)
m17: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
m18:     after_part + first_rep + is_open + is_disc + poly(wfreq,
m17:     2) + poly(wlen_p, 2) + poly(utt_rate, 2) + poly(dur, 2) +
m18:     pmean + poly(log_prange, 2) + poly(imean, 2) + poly(irange,
m17:     2) + (1 | speaker) + (1 | corpus) + (1 | ref)
    Df    AIC    BIC logLik  Chisq Chi Df Pr(>Chisq)
m18 27  25928  26153 -12937
m17 28  25925  26159 -12934 5.2136      1    0.02241 *

So my first question is: Should I be concerned that the significance
level shown in the original m17 is so different from the one shown by
the anova?  It's hard for me to see how this could happen.  I noticed
that there is a post on the FAQ about significance levels in linear
mixed models, but I'm not sure whether it applies to the logistic
case and if so how.

Now, my second question is a result of removing one more factor
(is_disc) from the model, creating m19.  I do another anova:

> anova(m19,m18)
Data:
Models:
m19: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
m18:     after_part + first_rep + is_open + poly(wfreq, 2) + wlen_p +
m19:     poly(utt_rate, 2) + poly(dur, 2) + pmean + poly(log_prange,
m18:     2) + poly(imean, 2) + poly(irange, 2) + (1 | speaker) + (1 |
m19:     corpus) + (1 | ref)
m18: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
m19:     after_part + first_rep + is_open + is_disc + poly(wfreq,
m18:     2) + wlen_p + poly(utt_rate, 2) + poly(dur, 2) + pmean +
m19:     poly(log_prange, 2) + poly(imean, 2) + poly(irange, 2) +
m18:     (1 | speaker) + (1 | corpus) + (1 | ref)
    Df    AIC    BIC logLik Chisq Chi Df Pr(>Chisq)
m19 26  25925  26142 -12936
m18 27  25928  26153 -12937     0      1          1

and now it seems that m19 (which contains fewer parameters than m18)
is a better fit.  I don't see how it's possible to remove parameters
from a model and get a better likelihood, but I will confess that I
don't entirely understand how these kinds of models are estimated.
Does this have something to do with approximations that R is making to
fit the models, or numerical rounding errors?  Could either problem be
due to correlations among variables?  All my fixed effects are either
binary or numeric, and there are some fairly high correlations between
a few pairs of them (maybe as high as .6 or .65 using Kendall tau) but I
figured that this would be ok given the large number of data points.
I think each value of each binary feature is observed at least
70-100 times.

In case it matters, I'm running R 2.5.1 on Linux, with lme4 0.99875-8, and
below is a full printout of all my output:

> m17 <- lmer(is_err ~ sex + starts_turn + before_hes + after_hes + before_part + after_part + first_rep + is_open + is_disc + poly(wfreq,2) + poly(wlen_p,2) + poly(utt_rate,2) + poly(dur,2) + pmean + poly(log_prange,2) + poly(imean,2) + poly(irange,2)+(1|speaker)+(1|corpus)+(1|ref), x=T,y=T,family="binomial")

> print(m17,corr=F)
Generalized linear mixed model fit using Laplace
Formula: is_err ~ sex + starts_turn + before_hes + after_hes +
before_part +      after_part + first_rep + is_open + is_disc +
poly(wfreq,      2) + poly(wlen_p, 2) + poly(utt_rate, 2) + poly(dur,
2) +      pmean + poly(log_prange, 2) + poly(imean, 2) + poly(irange,
    2) + (1 | speaker) + (1 | corpus) + (1 | ref)
 Family: binomial(logit link)
   AIC   BIC logLik deviance
 25925 26159 -12934    25869
Random effects:
 Groups  Name        Variance Std.Dev.
 ref     (Intercept) 0.434599 0.65924
 speaker (Intercept) 0.327813 0.57255
 corpus  (Intercept) 0.039722 0.19930
number of obs: 31017, groups: ref, 2601; speaker, 72; corpus, 2

Estimated scale (compare to  1 )  0.9810366

Fixed effects:
                       Estimate Std. Error z value Pr(>|z|)
(Intercept)            -1.97887    0.19699 -10.045  < 2e-16 ***
sexM                    0.45553    0.14387   3.166 0.001544 **
starts_turnTRUE         0.25426    0.08087   3.144 0.001666 **
before_hesTRUE          0.50943    0.12364   4.120 3.79e-05 ***
after_hesTRUE           0.26439    0.12162   2.174 0.029712 *
before_partTRUE         1.07972    0.10748  10.046  < 2e-16 ***
after_partTRUE          0.47089    0.12363   3.809 0.000140 ***
first_repTRUE           1.00673    0.13073   7.701 1.35e-14 ***
is_openTRUE            -0.42213    0.09894  -4.267 1.98e-05 ***
is_discTRUE             0.24676    0.15204   1.623 0.104576
poly(wfreq, 2)1      -119.72397   11.00516 -10.879  < 2e-16 ***
poly(wfreq, 2)2        17.35646    5.44456   3.188 0.001433 **
poly(wlen_p, 2)1      -13.60798    7.26926  -1.872 0.061208 .
poly(wlen_p, 2)2       -6.43167    5.24119  -1.227 0.219770
poly(utt_rate, 2)1      5.21605    3.56227   1.464 0.143126
poly(utt_rate, 2)2     22.75593    3.04234   7.480 7.45e-14 ***
poly(dur, 2)1        -110.73779    6.15641 -17.987  < 2e-16 ***
poly(dur, 2)2          38.71283    3.52495  10.983  < 2e-16 ***
pmean                   1.00184    0.17912   5.593 2.23e-08 ***
poly(log_prange, 2)1    3.71200    3.76913   0.985 0.324702
poly(log_prange, 2)2   17.07179    2.79954   6.098 1.07e-09 ***
poly(imean, 2)1       -21.73475    4.75449  -4.571 4.84e-06 ***
poly(imean, 2)2        28.22247    3.32294   8.493  < 2e-16 ***
poly(irange, 2)1       -6.47276    3.95778  -1.635 0.101955
poly(irange, 2)2        6.99340    3.11937   2.242 0.024966 *

#remove wlen^2
> m18 <- lmer(is_err ~ sex + starts_turn + before_hes + after_hes + before_part + after_part + first_rep + is_open + is_disc + poly(wfreq,2) + wlen_p + poly(utt_rate,2) + poly(dur,2) + pmean + poly(log_prange,2) + poly(imean,2) + poly(irange,2)+(1|speaker)+(1|corpus)+(1|ref), x=T,y=T,family="binomial")

> anova(m17,m18)
Data:
Models:
m18: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
m17:     after_part + first_rep + is_open + is_disc + poly(wfreq,
m18:     2) + wlen_p + poly(utt_rate, 2) + poly(dur, 2) + pmean +
m17:     poly(log_prange, 2) + poly(imean, 2) + poly(irange, 2) +
m18:     (1 | speaker) + (1 | corpus) + (1 | ref)
m17: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
m18:     after_part + first_rep + is_open + is_disc + poly(wfreq,
m17:     2) + poly(wlen_p, 2) + poly(utt_rate, 2) + poly(dur, 2) +
m18:     pmean + poly(log_prange, 2) + poly(imean, 2) + poly(irange,
m17:     2) + (1 | speaker) + (1 | corpus) + (1 | ref)
    Df    AIC    BIC logLik  Chisq Chi Df Pr(>Chisq)
m18 27  25928  26153 -12937
m17 28  25925  26159 -12934 5.2136      1    0.02241 *

#remove is_disc
> m19 <- lmer(is_err ~ sex + starts_turn + before_hes + after_hes + before_part + after_part + first_rep + is_open + poly(wfreq,2) + wlen_p + poly(utt_rate,2) + poly(dur,2) + pmean + poly(log_prange,2) + poly(imean,2) + poly(irange,2)+(1|speaker)+(1|corpus)+(1|ref), x=T,y=T,family="binomial")

> anova(m19,m18)
Data:
Models:
m19: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
m18:     after_part + first_rep + is_open + poly(wfreq, 2) + wlen_p +
m19:     poly(utt_rate, 2) + poly(dur, 2) + pmean + poly(log_prange,
m18:     2) + poly(imean, 2) + poly(irange, 2) + (1 | speaker) + (1 |
m19:     corpus) + (1 | ref)
m18: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
m19:     after_part + first_rep + is_open + is_disc + poly(wfreq,
m18:     2) + wlen_p + poly(utt_rate, 2) + poly(dur, 2) + pmean +
m19:     poly(log_prange, 2) + poly(imean, 2) + poly(irange, 2) +
m18:     (1 | speaker) + (1 | corpus) + (1 | ref)
    Df    AIC    BIC logLik Chisq Chi Df Pr(>Chisq)
m19 26  25925  26142 -12936
m18 27  25928  26153 -12937     0      1          1

--AuH2O

Sharon Goldwater
Postdoctoral Scholar
Department of Linguistics
Stanford Universtiy


From f.harrell at vanderbilt.edu  Fri Dec 28 18:21:55 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 28 Dec 2007 11:21:55 -0600
Subject: [R] SAS to R - if you don't have a SAS license
In-Reply-To: <1115a2b00712271546w743ff42bn41a249436901705c@mail.gmail.com>
References: <776471.29074.qm@web36802.mail.mud.yahoo.com>
	<1115a2b00712271546w743ff42bn41a249436901705c@mail.gmail.com>
Message-ID: <477530B3.2040204@vanderbilt.edu>

Wensui Liu wrote:
> while I move data between SAS and R all the time, personally I don't
> think your recommendation is very practical. Instead, I feel SAS
> transport file is much better than csv.
> 
> Plus, the sas dataset created on unix can't be opened by sas viewer on
> windows. It is even undoable if the dataset is large.

That's surprising.  I hoped that a "SAS Viewer" would read all formats 
of SAS binary files.

There is another definite limitation to SAS Viewer: SAS invested so 
little of their billions of $ into it that it only has 2 delimiters (tab 
and comma) and doesn't even check if character strings contain the 
delimiter so as to escape those occurrences.  So the Viewer often 
produces invalid csv files.

Frank

> 
> Just my $0.02.
> 
> 
> On Dec 27, 2007 6:33 PM, Gyula Gulyas <gygulyas at yahoo.ca> wrote:
>> Hi all,
>>
>> if you do not have a SAS license but want to convert
>> native SAS data files, the solution below will work.
>>
>> # read SAS data without SAS
>>
>> # 1. Download free SAS System Viewer from either of
>> the sites below:
>> #
>> http://www.sas.com/apps/demosdownloads/setupcat.jsp?cat=SAS+System+Viewer
>> (requires registration)
>> #
>> http://www.umass.edu/statdata/software/downloads/SASViewer/index.html
>> # 2. Open SAS data in the SAS System Viewer
>> # 3. View-Formatted sets the data in formatted view
>> # 4. Save As File...csv file - this is your SAS data
>> file
>> # 5. View-Variables (now showing the variable names
>> and formats)
>> # 6. Save As File...csv file - this is your SAS
>> variable definition file
>>
>> # run code below
>>
>> wrkdir<-getwd() # save working directory to reset
>> later
>>
>> # Select the SAS data file...
>> sas.data<-read.table(file.choose(),header=T, sep=",",
>> na.strings=".")
>>
>> # Select SAS variable definition file...
>> sas.def<-read.csv(file.choose())
>>
>> # str(sas.def)
>> # sas.def$SASFORMAT[sas.def$Type=="Char"]<-"character"
>> # sas.def$SASFORMAT[sas.def$Type=="Num"]<-"numeric"
>>
>> sas.def$SASFORMAT[substr(sas.def$Format,1,4)=="DATE"]<-"date"
>>
>> sas.def<-sas.def[,length(names(sas.def))] # pick last
>> column
>>
>> tmp<-which(sas.def=="date")
>>
>> sas.data[,tmp] <-
>> as.data.frame(strptime(sas.data[,tmp],
>> "%d%b%Y:%H:%M:%S"))
>>
>> str(sas.data)
>> print(head(sas.data))
>>
>> setwd(wrkdir) # reset working directory
>>
>> rm(wrkdir,tmp,sas.def)
>>
>> # the end
>>


From marc_schwartz at comcast.net  Fri Dec 28 18:43:57 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 28 Dec 2007 11:43:57 -0600
Subject: [R] SAS to R - if you don't have a SAS license
In-Reply-To: <477530B3.2040204@vanderbilt.edu>
References: <776471.29074.qm@web36802.mail.mud.yahoo.com>
	<1115a2b00712271546w743ff42bn41a249436901705c@mail.gmail.com>
	<477530B3.2040204@vanderbilt.edu>
Message-ID: <1198863837.3182.19.camel@Bellerophon.localdomain>


On Fri, 2007-12-28 at 11:21 -0600, Frank E Harrell Jr wrote:
> Wensui Liu wrote:
> > while I move data between SAS and R all the time, personally I don't
> > think your recommendation is very practical. Instead, I feel SAS
> > transport file is much better than csv.
> > 
> > Plus, the sas dataset created on unix can't be opened by sas viewer on
> > windows. It is even undoable if the dataset is large.
> 
> That's surprising.  I hoped that a "SAS Viewer" would read all formats 
> of SAS binary files.

<snip>

We have had that problem here, having received both 32 and 64 bit SAS
files from Linux and Solaris based SAS installations. The SAS System
Viewer was largely useless for the non-Windows SAS files that we have
had accessible to us, even though it will run quite nicely under Wine.

We have a 32 bit RHEL based SAS install here now, as I have noted in
prior posts. That has helped with certain aspects of SAS dataset
generation and transfer to and from clients.

DBMS/Copy, which also runs nicely under Wine, seems to be the only tool
that we have so far, that can provide for something of a universal SAS
"can opener". It will at least successfully _open_ SAS datasets
generated on a variety of platforms, when other approaches have failed.

However, as I have also noted in prior posts, the SAS datasets that
DBMS/Copy generates, are not guaranteed to be able to be opened by SAS
itself. Hence our need to install SAS here.

Theoretically, the .sas7bdat files are supposed to be cross-platform
compatible and I think Peter had commented on that in a prior thread on
this subject. However, hands on experience has suggested that this
expectation is not absolute.

HTH,

Marc Schwartz


From f.harrell at vanderbilt.edu  Fri Dec 28 19:06:04 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 28 Dec 2007 12:06:04 -0600
Subject: [R] SAS to R - if you don't have a SAS license
In-Reply-To: <1198863837.3182.19.camel@Bellerophon.localdomain>
References: <776471.29074.qm@web36802.mail.mud.yahoo.com>	
	<1115a2b00712271546w743ff42bn41a249436901705c@mail.gmail.com>	
	<477530B3.2040204@vanderbilt.edu>
	<1198863837.3182.19.camel@Bellerophon.localdomain>
Message-ID: <47753B0C.4000708@vanderbilt.edu>

Marc Schwartz wrote:
> On Fri, 2007-12-28 at 11:21 -0600, Frank E Harrell Jr wrote:
>> Wensui Liu wrote:
>>> while I move data between SAS and R all the time, personally I don't
>>> think your recommendation is very practical. Instead, I feel SAS
>>> transport file is much better than csv.
>>>
>>> Plus, the sas dataset created on unix can't be opened by sas viewer on
>>> windows. It is even undoable if the dataset is large.
>> That's surprising.  I hoped that a "SAS Viewer" would read all formats 
>> of SAS binary files.
> 
> <snip>
> 
> We have had that problem here, having received both 32 and 64 bit SAS
> files from Linux and Solaris based SAS installations. The SAS System
> Viewer was largely useless for the non-Windows SAS files that we have
> had accessible to us, even though it will run quite nicely under Wine.
> 
> We have a 32 bit RHEL based SAS install here now, as I have noted in
> prior posts. That has helped with certain aspects of SAS dataset
> generation and transfer to and from clients.
> 
> DBMS/Copy, which also runs nicely under Wine, seems to be the only tool
> that we have so far, that can provide for something of a universal SAS
> "can opener". It will at least successfully _open_ SAS datasets
> generated on a variety of platforms, when other approaches have failed.
> 
> However, as I have also noted in prior posts, the SAS datasets that
> DBMS/Copy generates, are not guaranteed to be able to be opened by SAS
> itself. Hence our need to install SAS here.
> 
> Theoretically, the .sas7bdat files are supposed to be cross-platform
> compatible and I think Peter had commented on that in a prior thread on
> this subject. However, hands on experience has suggested that this
> expectation is not absolute.
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> 

I have had good success with Stat/Transfer (and its Windows version runs 
perfectly under Linux using wine) but haven't tested it as extensively 
as you've tested DBMS/Copy.

Frank


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From agoralczyk at gmail.com  Fri Dec 28 19:07:55 2007
From: agoralczyk at gmail.com (Armin Goralczyk)
Date: Fri, 28 Dec 2007 19:07:55 +0100
Subject: [R] Pause loop
Message-ID: <a695fbee0712281007s3d3de474xc55e91fcecbcf0fb@mail.gmail.com>

Hi list

How can I pause a loop (or any other function) for a defined time? E.g.

for(i in 1:5) {print(1:i); 'function to pause for 10 seconds'}

Thanks for help.
-- 
Armin Goralczyk, M.D.
--
Universit?tsmedizin G?ttingen
Abteilung Allgemein- und Viszeralchirurgie
Rudolf-Koch-Str. 40
39099 G?ttingen
--
Dept. of General Surgery
University of G?ttingen
G?ttingen, Germany
--
http://www.gwdg.de/~agoralc

From marc_schwartz at comcast.net  Fri Dec 28 19:24:34 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 28 Dec 2007 12:24:34 -0600
Subject: [R] Pause loop
In-Reply-To: <a695fbee0712281007s3d3de474xc55e91fcecbcf0fb@mail.gmail.com>
References: <a695fbee0712281007s3d3de474xc55e91fcecbcf0fb@mail.gmail.com>
Message-ID: <1198866274.3182.25.camel@Bellerophon.localdomain>

On Fri, 2007-12-28 at 19:07 +0100, Armin Goralczyk wrote:
> Hi list
> 
> How can I pause a loop (or any other function) for a defined time? E.g.
> 
> for(i in 1:5) {print(1:i); 'function to pause for 10 seconds'}
> 
> Thanks for help.

At least on Linux, see:

  ?Sys.sleep

Note the caveats regarding time resolution...

HTH,

Marc Schwartz


From ba208 at exeter.ac.uk  Fri Dec 28 20:33:02 2007
From: ba208 at exeter.ac.uk (=?ISO-8859-1?Q?baptiste_Augui=E9?=)
Date: Fri, 28 Dec 2007 19:33:02 +0000
Subject: [R] unit attribute to list elements
Message-ID: <59FE4C0D-6FE2-4538-B801-F12D66C6C309@ex.ac.uk>

Hi,

I've started my own (first) package, part of which consists in  
listing common physical constants (Planck's constant, the speed of  
light in vacuum, etc). I'm wondering what would be a good way of  
dealing with pairs of value/unit.


> constants <- list( cel = 2.99792458e8 , #m/s
> Z0 = 376.730313461, #ohm
> eps0 = 8.854187817e-12,#F/m
> mu0 = 4*pi*1e-7,#N/A^2
> G = 6.67428e-11 # m^3 kg-1 s-2
> )


I thought I could include the unit in the names attribute of each  
element, as in :

> names(constants$cel)<- " speed of light in vacuum [m.s^-1]"


Writing this for every element is very redundant... Is there any way  
to access and set the name of each first level element of the list?

> namesFirstLevelElements(constants)<- c(" speed of light in vacuum  
> [m.s^-1]",
> "impedance of vacuum [some unit]",
> ...)


Quite possibly, I'm completely on the wring track;

- maybe such a package already exists

- a custom class or structure would be more appropriate? I don't  
really know how to deal with classes, though, and I'd like to keep  
the access to the constants' values as direct and generic as possible.

Many thanks in advance,

baptiste

_____________________________

Baptiste Augui?

Physics Department
University of Exeter
Stocker Road,
Exeter, Devon,
EX4 4QL, UK

Phone: +44 1392 264187

http://newton.ex.ac.uk/research/emag
http://projects.ex.ac.uk/atto


From bates at stat.wisc.edu  Fri Dec 28 20:55:33 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 28 Dec 2007 13:55:33 -0600
Subject: [R] logistic mixed effects models with lmer
In-Reply-To: <7b8a41740712280735u6e6a57f0tb640f37cbe987648@mail.gmail.com>
References: <7b8a41740712280735u6e6a57f0tb640f37cbe987648@mail.gmail.com>
Message-ID: <40e66e0b0712281155o6f48b5bewd92b7befb44f83c1@mail.gmail.com>

On Dec 28, 2007 9:35 AM, Sharon Goldwater <sgwater at stanford.edu> wrote:
> I have a question about some strange results I get when using lmer to
> build a logistic mixed effects model.  I have a data set of about 30k
> points, and I'm trying to do backwards selection to reduce the number
> of fixed effects in my model.  I've got 3 crossed random effects and
> about 20 or so fixed effects.  At a certain point, I get a model (m17)
> where the fixed effects are like this (full output is at end of
> message):
>
> > print(m17, corr=F)
> ...
> Fixed effects:
>                        Estimate Std. Error z value Pr(>|z|)
> (Intercept)            -1.97887    0.19699 -10.045  < 2e-16 ***
> sexM                    0.45553    0.14387   3.166 0.001544 **
> ...
> is_discTRUE             0.24676    0.15204   1.623 0.104576
> poly(wfreq, 2)1      -119.72397   11.00516 -10.879  < 2e-16 ***
> poly(wfreq, 2)2        17.35646    5.44456   3.188 0.001433 **
> poly(wlen_p, 2)1      -13.60798    7.26926  -1.872 0.061208 .
> poly(wlen_p, 2)2       -6.43167    5.24119  -1.227 0.219770
> ...
>
> where poly(wlen_p,2)2 is the least significant factor left in the
> model.  So I then build a model (m18) with exactly the same random and
> fixed effects except removing poly(wlen_p,2)2.  Then I do an anova,
> and I get:
>
> > anova(m17,m18)
> Data:
> Models:
> m18: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
> m17:     after_part + first_rep + is_open + is_disc + poly(wfreq,
> m18:     2) + wlen_p + poly(utt_rate, 2) + poly(dur, 2) + pmean +
> m17:     poly(log_prange, 2) + poly(imean, 2) + poly(irange, 2) +
> m18:     (1 | speaker) + (1 | corpus) + (1 | ref)
> m17: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
> m18:     after_part + first_rep + is_open + is_disc + poly(wfreq,
> m17:     2) + poly(wlen_p, 2) + poly(utt_rate, 2) + poly(dur, 2) +
> m18:     pmean + poly(log_prange, 2) + poly(imean, 2) + poly(irange,
> m17:     2) + (1 | speaker) + (1 | corpus) + (1 | ref)
>     Df    AIC    BIC logLik  Chisq Chi Df Pr(>Chisq)
> m18 27  25928  26153 -12937
> m17 28  25925  26159 -12934 5.2136      1    0.02241 *

> So my first question is: Should I be concerned that the significance
> level shown in the original m17 is so different from the one shown by
> the anova?  It's hard for me to see how this could happen.  I noticed
> that there is a post on the FAQ about significance levels in linear
> mixed models, but I'm not sure whether it applies to the logistic
> case and if so how.

It can happen that the significance levels reported by the different
tests will be different.  The test in the summary is based on a local
approximation to the conditional mean and assumes that the variances
of the random effects will not change substantially when fitting the
model with and without that term.  Did they?

One thing I noticed is your output (and thank you for including that)
is that it reports that corpus has only two levels.  Is that correct?
If so, I would not advise using a random effect for corpus.  It is
very difficult to estimate a variance from only two levels of a
factor.  I suggest using a fixed effect for corpus instead.

> Now, my second question is a result of removing one more factor
> (is_disc) from the model, creating m19.  I do another anova:
>
> > anova(m19,m18)
> Data:
> Models:
> m19: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
> m18:     after_part + first_rep + is_open + poly(wfreq, 2) + wlen_p +
> m19:     poly(utt_rate, 2) + poly(dur, 2) + pmean + poly(log_prange,
> m18:     2) + poly(imean, 2) + poly(irange, 2) + (1 | speaker) + (1 |
> m19:     corpus) + (1 | ref)
> m18: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
> m19:     after_part + first_rep + is_open + is_disc + poly(wfreq,
> m18:     2) + wlen_p + poly(utt_rate, 2) + poly(dur, 2) + pmean +
> m19:     poly(log_prange, 2) + poly(imean, 2) + poly(irange, 2) +
> m18:     (1 | speaker) + (1 | corpus) + (1 | ref)
>     Df    AIC    BIC logLik Chisq Chi Df Pr(>Chisq)
> m19 26  25925  26142 -12936
> m18 27  25928  26153 -12937     0      1          1
>
> and now it seems that m19 (which contains fewer parameters than m18)
> is a better fit.  I don't see how it's possible to remove parameters
> from a model and get a better likelihood, but I will confess that I
> don't entirely understand how these kinds of models are estimated.
> Does this have something to do with approximations that R is making to
> fit the models, or numerical rounding errors?  Could either problem be
> due to correlations among variables?  All my fixed effects are either
> binary or numeric, and there are some fairly high correlations between
> a few pairs of them (maybe as high as .6 or .65 using Kendall tau) but I
> figured that this would be ok given the large number of data points.
> I think each value of each binary feature is observed at least
> 70-100 times.

The difference could be due to approximations or due to poor
convergence.  For some models the Laplace approximation can be
generalized to a higher-order approximation, called adaptive
Gauss-Hermite quadrature (AGQ), but that isn't feasible when you have
crossed random effects (and, besides, I  still haven't written the
code for AGQ even in the case where you don't have crossed random
effects).

First I suggest that you check the value stored as the log-likelihood

dput(logLik(m17))
dput(logLik(m18))

to see how different they really are.

You could, if you are feeling brave, try the development version of
the lme4 package from http://r-forge.r-project.org.  To do that,
however, you will need to install a new version of R (R-2.6.1 is
preferred) and a new version of the Matrix package.  Then you can
install the new lme4 with

install.packages("lme4", repos = "http://r-forge.r-project.org")

If you prefer, you can contact me off-list and I can try to fit your
models for you using the development version.

> In case it matters, I'm running R 2.5.1 on Linux, with lme4 0.99875-8, and
> below is a full printout of all my output:
>
> > m17 <- lmer(is_err ~ sex + starts_turn + before_hes + after_hes + before_part + after_part + first_rep + is_open + is_disc + poly(wfreq,2) + poly(wlen_p,2) + poly(utt_rate,2) + poly(dur,2) + pmean + poly(log_prange,2) + poly(imean,2) + poly(irange,2)+(1|speaker)+(1|corpus)+(1|ref), x=T,y=T,family="binomial")
>
> > print(m17,corr=F)
> Generalized linear mixed model fit using Laplace
> Formula: is_err ~ sex + starts_turn + before_hes + after_hes +
> before_part +      after_part + first_rep + is_open + is_disc +
> poly(wfreq,      2) + poly(wlen_p, 2) + poly(utt_rate, 2) + poly(dur,
> 2) +      pmean + poly(log_prange, 2) + poly(imean, 2) + poly(irange,
>     2) + (1 | speaker) + (1 | corpus) + (1 | ref)
>  Family: binomial(logit link)
>    AIC   BIC logLik deviance
>  25925 26159 -12934    25869
> Random effects:
>  Groups  Name        Variance Std.Dev.
>  ref     (Intercept) 0.434599 0.65924
>  speaker (Intercept) 0.327813 0.57255
>  corpus  (Intercept) 0.039722 0.19930
> number of obs: 31017, groups: ref, 2601; speaker, 72; corpus, 2
>
> Estimated scale (compare to  1 )  0.9810366
>
> Fixed effects:
>                        Estimate Std. Error z value Pr(>|z|)
> (Intercept)            -1.97887    0.19699 -10.045  < 2e-16 ***
> sexM                    0.45553    0.14387   3.166 0.001544 **
> starts_turnTRUE         0.25426    0.08087   3.144 0.001666 **
> before_hesTRUE          0.50943    0.12364   4.120 3.79e-05 ***
> after_hesTRUE           0.26439    0.12162   2.174 0.029712 *
> before_partTRUE         1.07972    0.10748  10.046  < 2e-16 ***
> after_partTRUE          0.47089    0.12363   3.809 0.000140 ***
> first_repTRUE           1.00673    0.13073   7.701 1.35e-14 ***
> is_openTRUE            -0.42213    0.09894  -4.267 1.98e-05 ***
> is_discTRUE             0.24676    0.15204   1.623 0.104576
> poly(wfreq, 2)1      -119.72397   11.00516 -10.879  < 2e-16 ***
> poly(wfreq, 2)2        17.35646    5.44456   3.188 0.001433 **
> poly(wlen_p, 2)1      -13.60798    7.26926  -1.872 0.061208 .
> poly(wlen_p, 2)2       -6.43167    5.24119  -1.227 0.219770
> poly(utt_rate, 2)1      5.21605    3.56227   1.464 0.143126
> poly(utt_rate, 2)2     22.75593    3.04234   7.480 7.45e-14 ***
> poly(dur, 2)1        -110.73779    6.15641 -17.987  < 2e-16 ***
> poly(dur, 2)2          38.71283    3.52495  10.983  < 2e-16 ***
> pmean                   1.00184    0.17912   5.593 2.23e-08 ***
> poly(log_prange, 2)1    3.71200    3.76913   0.985 0.324702
> poly(log_prange, 2)2   17.07179    2.79954   6.098 1.07e-09 ***
> poly(imean, 2)1       -21.73475    4.75449  -4.571 4.84e-06 ***
> poly(imean, 2)2        28.22247    3.32294   8.493  < 2e-16 ***
> poly(irange, 2)1       -6.47276    3.95778  -1.635 0.101955
> poly(irange, 2)2        6.99340    3.11937   2.242 0.024966 *
>
> #remove wlen^2
> > m18 <- lmer(is_err ~ sex + starts_turn + before_hes + after_hes + before_part + after_part + first_rep + is_open + is_disc + poly(wfreq,2) + wlen_p + poly(utt_rate,2) + poly(dur,2) + pmean + poly(log_prange,2) + poly(imean,2) + poly(irange,2)+(1|speaker)+(1|corpus)+(1|ref), x=T,y=T,family="binomial")
>
> > anova(m17,m18)
> Data:
> Models:
> m18: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
> m17:     after_part + first_rep + is_open + is_disc + poly(wfreq,
> m18:     2) + wlen_p + poly(utt_rate, 2) + poly(dur, 2) + pmean +
> m17:     poly(log_prange, 2) + poly(imean, 2) + poly(irange, 2) +
> m18:     (1 | speaker) + (1 | corpus) + (1 | ref)
> m17: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
> m18:     after_part + first_rep + is_open + is_disc + poly(wfreq,
> m17:     2) + poly(wlen_p, 2) + poly(utt_rate, 2) + poly(dur, 2) +
> m18:     pmean + poly(log_prange, 2) + poly(imean, 2) + poly(irange,
> m17:     2) + (1 | speaker) + (1 | corpus) + (1 | ref)
>     Df    AIC    BIC logLik  Chisq Chi Df Pr(>Chisq)
> m18 27  25928  26153 -12937
> m17 28  25925  26159 -12934 5.2136      1    0.02241 *
>
> #remove is_disc
> > m19 <- lmer(is_err ~ sex + starts_turn + before_hes + after_hes + before_part + after_part + first_rep + is_open + poly(wfreq,2) + wlen_p + poly(utt_rate,2) + poly(dur,2) + pmean + poly(log_prange,2) + poly(imean,2) + poly(irange,2)+(1|speaker)+(1|corpus)+(1|ref), x=T,y=T,family="binomial")
>
> > anova(m19,m18)
> Data:
> Models:
> m19: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
> m18:     after_part + first_rep + is_open + poly(wfreq, 2) + wlen_p +
> m19:     poly(utt_rate, 2) + poly(dur, 2) + pmean + poly(log_prange,
> m18:     2) + poly(imean, 2) + poly(irange, 2) + (1 | speaker) + (1 |
> m19:     corpus) + (1 | ref)
> m18: is_err ~ sex + starts_turn + before_hes + after_hes + before_part +
> m19:     after_part + first_rep + is_open + is_disc + poly(wfreq,
> m18:     2) + wlen_p + poly(utt_rate, 2) + poly(dur, 2) + pmean +
> m19:     poly(log_prange, 2) + poly(imean, 2) + poly(irange, 2) +
> m18:     (1 | speaker) + (1 | corpus) + (1 | ref)
>     Df    AIC    BIC logLik Chisq Chi Df Pr(>Chisq)
> m19 26  25925  26142 -12936
> m18 27  25928  26153 -12937     0      1          1


From A.Robinson at ms.unimelb.edu.au  Fri Dec 28 00:59:32 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Fri, 28 Dec 2007 10:59:32 +1100
Subject: [R] groupedData function not found
In-Reply-To: <BAY139-W10E5EF414B783C90B6749BC6540@phx.gbl>
References: <BAY139-W10E5EF414B783C90B6749BC6540@phx.gbl>
Message-ID: <20071227235932.GB1065@ms.unimelb.edu.au>

Hi Andrea,

did you try

require(nlme)

?  Adding a package is not necessarily the same as loading it.

Cheers

Andrew


On Thu, Dec 27, 2007 at 10:42:27PM +0000, andrea previtali wrote:
> 
> Hello,
> I'm trying to use the groupedData function and R is giving me the message: Error: can not find function "groupedData"
> The code is coming from a textbook so I think it should be correct:
> 
> pigs<-data.frame(cbind(pig.time,pig.id,pig.wt))
> pig.growth<-groupedData(pig.wt~pig.time|pig.id,data=pigs)
> 
> I have added the package "nlme" and to confirm that it was installed correctly I requested the list of functions included in the package (library(help=nlme)) and I do see groupData in the list.
> I am using R 2.6.1 in Windows XP.
> 
> I'll appreciate your help.
> Thanks,
> Andrea Previtali
> Post-doc fellow
> Dept. of Biology,
> Univ. of Utah, SLC, UT.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> This message has been scanned for viruses and
> dangerous content by MailScanner, and is
> believed to be clean.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From gygulyas at yahoo.ca  Fri Dec 28 00:25:09 2007
From: gygulyas at yahoo.ca (Gyula Gulyas)
Date: Thu, 27 Dec 2007 15:25:09 -0800 (PST)
Subject: [R] SAS to R - if you have SAS 8.2+
Message-ID: <381640.5627.qm@web36812.mail.mud.yahoo.com>

Hi there,

the attached R function uses the SAS Integrated Object
Model (IOM) and it can deal with SAS dates and long
variable names. All you need to provide is the folder
where the SAS data file is and the data file name
without the extension. The function requires the rcom
package.

This is meant to be first cut...but improvements and
suggestions are more than welcome!

Gyula

 import.sas.data <-
function(inPath,inSAS,as.is=FALSE){
 # Function to read in a SAS data file
 # Packages needed: rcom
 # inPath: path to SAS file
 # inSAS: SAS data file name (no extension)
 # as.is: as per read.csv (FALSE - force all character
variables to be factors)
 # outputs a data.frame object
 # Created:  December 1, 2007
 # Modified: December 4, 2007
 # Author: Gyula Gulyas (gyula.gulyas at timberline.ca)
 # Use as is, no guarantees, liability due to data
loss is limited to the price you paid for this
function...
 
 require(rcom)

 # check if user has closing slash in path and fix it
if needed
 inPath <- sub("/$","",inPath)

 obWSM <-
comCreateObject("SASWorkspaceManager.WorkspaceManager")
 obWSM.Workspaces <- obWSM[["Workspaces"]]
 obSAS <- comCreateObject("SAS.Workspace")
 obSAS.DataService <- obSAS[["DataService"]]
 obSAS.LanguageService <- obSAS[["LanguageService"]]

 # hard-coded temporary files
 # sas temporary csv file
 csvdata <- paste(inPath,"/t__sd__t.csv", sep="")
 # sas temporary column definition file
 coldef <- paste(inPath,"/t__sc__t.csv", sep="")
 
 libRef <-
obSAS.DataService$AssignLibref("sasds","",inPath,"")

 # create the content csv file
 cont1 <- paste("proc contents data=sasds.",inSAS,"
out=_tmp1(KEEP=NAME TYPE LENGTH VARNUM FORMAT)
noprint; run;",sep="")
 cont2 <- "proc sort data=_tmp1; by varnum; run;"
 cont3 <- "data _tmp2; set _tmp1; if type=2 then
dummy='character'; "
 cont3 <- paste(cont3,"if type=1 then do; if format in
('DATE','DATETIME') then dummy='date'; ",sep="")
 cont3 <- paste(cont3,"else dummy='numeric'; end;
run;",sep="")
 cont4 <- "proc transpose data=_tmp2
out=_tmp3(DROP=_NAME_); id name; var dummy; run;"
 cont5 <- paste("proc export data=_tmp3
outfile='",coldef,"' dbms=csv; run;",sep="")

 obSAS.LanguageService$Submit(cont1)
 obSAS.LanguageService$Submit(cont2)
 obSAS.LanguageService$Submit(cont3)
 obSAS.LanguageService$Submit(cont4)
 obSAS.LanguageService$Submit(cont5)

 # column definitions, all as characters
 scf <- read.csv(coldef,as.is=T)

 sasline1 <- paste("data _tmp4; set sasds.", inSAS, ";
run;",sep="")

 obSAS.LanguageService$Submit(sasline1)

 sasline2 <- paste("proc export data=_tmp4
outfile='",csvdata,"' dbms=csv; run;",sep="")

 obSAS.LanguageService$Submit(sasline2)

 sdf <- read.csv(csvdata,as.is=as.is)

 # delete old csvdata file
 file.remove(csvdata)
 # delete old coldef file
 file.remove(coldef)
 
 # convert SAS DATETIME format into native R date
(POSIXct)
 x <- which(scf=="date")
 sdf[,x] <- as.data.frame(strptime(sdf[,x],
"%d%b%Y:%H:%M:%S"))

 # cleanup SAS objects
 obSAS.UniqueIdentifier <- obSAS[["UniqueIdentifier"]]

obWSM.Workspaces$RemoveWorkspaceByUUID(obSAS.UniqueIdentifier)
 obSAS$Close()

 return(sdf)

}

# example call
# not run
# test<- import.sas.data("path/to/data","sasdatafile")




      ____________________________________________________________________________________
Looking for last minute shopping deals?


From zhangchu at umkc.edu  Thu Dec 27 20:58:27 2007
From: zhangchu at umkc.edu (Chuanjun Zhang)
Date: Thu, 27 Dec 2007 14:58:27 -0500
Subject: [R] AIC and BIC in mixed effects model
Message-ID: <da973ba50712271158x29287f66y9d7a04d02396e368@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071227/623e78d0/attachment.pl 

From jfox at mcmaster.ca  Fri Dec 28 00:36:14 2007
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 27 Dec 2007 18:36:14 -0500
Subject: [R] Conditionally incrementing a loop counter: Take 2
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D0CBECE20@MAILBE2.westat.com>
References: <477428CE.4000801@biostat.ku.dk>
	<403593359CA56C4CAE1F8F4F00DCFE7D0CBECE20@MAILBE2.westat.com>
Message-ID: <000101c848e1$45722890$d05679b0$@ca>

Dear Mike,

You could use a repeat loop and manage the index yourself:

i <- 0
repeat{
	x <- runif(1)
	if (x < .1){
	   i <- i + 1
	   cat("x = ", x, "\n")
	   }
    if (i == 10) break
    }

But if your example problem reflects your actual application, why not just
generate uniform random numbers on the interval (0, .1)?

I hope this helps,
 John

--------------------------------
John Fox, Professor
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox


> -----Original Message-----
> From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-
> project.org] On Behalf Of Mike Jones
> Sent: December-27-07 6:08 PM
> To: Peter Dalgaard
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Conditionally incrementing a loop counter: Take 2
> 
> Since I didn't want the i to increment in the loop when the condition
> is not met, then in my example I wanted the loop to actually run 14
> times instead of the 10 since I wanted 4 of the iterations to be thrown
> away, or ignored.  I still haven't been able to figure this out.  Going
> the "while" route doesn't seem to work for me either.
> 
> 
> nums <- numeric(10)
> i <- 1
> garbage <- 0
> 
> while (i <= 10){
> 	x <- runif(1)
> 	cat("x = ",x,"\n")
> 	if (x < 0.1){
> 		nums[i] <- x
> 		i <- i + 1
> 	}
> 	else{
> 	        garbage <- garbage+1
> 	}
> cat("i = ",i,"garbage = ",garbage,"\n")
> }
> 
> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
> Sent: Thursday, December 27, 2007 5:36 PM
> To: Mike Jones
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Conditionally incrementing a loop counter: Take 2
> 
> 
> Mike Jones wrote:
> > My apologies for not including a working example.
> >
> > Here it is:
> >
> > for (i in 1:10){
> >    cat("initial i = ",i,"\n")
> >    x <- runif(1)
> >    if (x > 0.7){
> >       i <- i-1
> >    }
> >    cat("second i = ",i,"\n")
> > }
> >
> > When I ran this i got what follows, so there were four cases where I
> > wanted the i not to increment.
> >
> > initial i =  1
> > second i =  1
> > initial i =  2
> > second i =  1
> > initial i =  3
> > second i =  3
> > initial i =  4
> > second i =  3
> > initial i =  5
> > second i =  4
> > initial i =  6
> > second i =  6
> > initial i =  7
> > second i =  7
> > initial i =  8
> > second i =  7
> > initial i =  9
> > second i =  9
> > initial i =  10
> > second i =  10
> >
> >
> Is this the kind of effect you want?
> 
>  > x <- runif(10)
>  > cbind(x, 1:10, cumsum(x < .7))
>                 x
>  [1,] 0.384165631  1 1
>  [2,] 0.392715845  2 2
>  [3,] 0.895936431  3 2
>  [4,] 0.910242185  4 2
>  [5,] 0.689987301  5 3
>  [6,] 0.237071326  6 4
>  [7,] 0.225032680  7 5
>  [8,] 0.001856286  8 6
>  [9,] 0.392034868  9 7
> [10,] 0.655076045 10 8
> 
> If you insist on using a loop, you need to separate the loop control
> from the manipulation of i, as in (e.g.)
> 
> i <- 0
> for (j in 1:10){
>    i <- i + 1
>    cat("initial i = ",i,"\n")
>    x <- runif(1)
>    if (x > 0.7){
>       i <- i-1
>    }
>    cat("second i = ",i,"\n")
> }
> 
> 
> >>  -----Original Message-----
> >> From: 	Mike Jones
> >> Sent:	Thursday, December 27, 2007 4:35 PM
> >> To:	'r-help at lists.R-project.org'
> >> Subject:	Conditionally incrementing a loop counter
> >>
> >> Hi,
> >> I am trying a for loop from 1 to 10 by 1. However, if a condition
> >> does not get met, I want to "throw away" that iteration. So if my
> >> loop is for (i in 1:10) and i is say, 4 and the condition is not met
> >> then I don't want i to go up to 5.  Is there a way to do that? I
> >> can't seem to manually adjust i because from what I understand, R
> >> creates 10 long vector and uses that to "loops thru" and I'm not
> sure
> >> how to get at the index of that vector. Any suggestions? Thanks in
> >> advance.
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >> Mike Jones
> >> Westat
> >> 1650 Research Blvd. RE401
> >> Rockville, MD 20850
> >> Ph: 240.314.2312
> >>
> >>
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> --
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> 35327907
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From phgrosjean at sciviews.org  Fri Dec 28 12:40:28 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 28 Dec 2007 12:40:28 +0100
Subject: [R] Return Value of TCl/Tk window in R
In-Reply-To: <200712281224.53027.r.mueller@oeko-sorpe.de>
References: <200712281224.53027.r.mueller@oeko-sorpe.de>
Message-ID: <4774E0AC.3020206@sciviews.org>

 > res <- tkmessageBox(title = "test",message = "Continue?",
+        icon  ="question", type = "okcancel")
 > if (tclvalue(res) == "ok") 1 else 2

Happy new year!

Philippe Grosjean

Richard M?ller wrote:
> Hello,
> I have the TCl/Tk command 
> "tkmessageBox(titel="",message="x",icon="question",type="okcancel")" in my R 
> script. Now I want to perform some operation in relation to the user's 
> choice, something like
> "if (okpressed) xxx else yyy"
> What values does this command give and how are they used?
> Thank you, Richard


From perrone at rocketmail.com  Fri Dec 28 15:26:04 2007
From: perrone at rocketmail.com (Ricardo Perrone)
Date: Fri, 28 Dec 2007 06:26:04 -0800 (PST)
Subject: [R] gplot function - error
Message-ID: <455747.37438.qm@web34306.mail.mud.yahoo.com>

Hi all,

i installed the ggplot2 package via install.packages(), but the gplot function was not recognized in R console command. Is there any paths to configure? The error message reports that the function was not found.

Thanks
Ricardo




      ____________________________________________________________________________________
Be a better friend, newshound, and


From ggrothendieck at gmail.com  Fri Dec 28 21:06:58 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 28 Dec 2007 15:06:58 -0500
Subject: [R] unit attribute to list elements
In-Reply-To: <59FE4C0D-6FE2-4538-B801-F12D66C6C309@ex.ac.uk>
References: <59FE4C0D-6FE2-4538-B801-F12D66C6C309@ex.ac.uk>
Message-ID: <971536df0712281206x724d70e7t9b2edade58a2044e@mail.gmail.com>

Is this what you want?

> Lines <- "cel 3.0 m/s
+ Z0 367 ohm
+ eps0 8.9e-12 F/m
+ "
> Constants.DF <- read.table(textConnection(Lines), as.is = TRUE)
> Constants <- as.list(Constants.DF[[2]])
> names(Constants) <- Constants.DF[[1]]
> for(i in seq_along(Constants)) comment(Constants[[i]]) <- Constants.DF[i, 3]
>
> # get value
> Constants$Z0
[1] 367
> # get comment
> comment(Constants$Z0)
[1] "ohm"
> # add another Constant
> Constants$e <- 2.7
> comment(Constants$e) <- "exp"


On Dec 28, 2007 2:33 PM, baptiste Augui? <ba208 at exeter.ac.uk> wrote:
> Hi,
>
> I've started my own (first) package, part of which consists in
> listing common physical constants (Planck's constant, the speed of
> light in vacuum, etc). I'm wondering what would be a good way of
> dealing with pairs of value/unit.
>
>
> > constants <- list( cel = 2.99792458e8 , #m/s
> > Z0 = 376.730313461, #ohm
> > eps0 = 8.854187817e-12,#F/m
> > mu0 = 4*pi*1e-7,#N/A^2
> > G = 6.67428e-11 # m^3 kg-1 s-2
> > )
>
>
> I thought I could include the unit in the names attribute of each
> element, as in :
>
> > names(constants$cel)<- " speed of light in vacuum [m.s^-1]"
>
>
> Writing this for every element is very redundant... Is there any way
> to access and set the name of each first level element of the list?
>
> > namesFirstLevelElements(constants)<- c(" speed of light in vacuum
> > [m.s^-1]",
> > "impedance of vacuum [some unit]",
> > ...)
>
>
> Quite possibly, I'm completely on the wring track;
>
> - maybe such a package already exists
>
> - a custom class or structure would be more appropriate? I don't
> really know how to deal with classes, though, and I'd like to keep
> the access to the constants' values as direct and generic as possible.
>
> Many thanks in advance,
>
> baptiste
>
> _____________________________
>
> Baptiste Augui?
>
> Physics Department
> University of Exeter
> Stocker Road,
> Exeter, Devon,
> EX4 4QL, UK
>
> Phone: +44 1392 264187
>
> http://newton.ex.ac.uk/research/emag
> http://projects.ex.ac.uk/atto
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ba208 at exeter.ac.uk  Fri Dec 28 22:40:32 2007
From: ba208 at exeter.ac.uk (=?ISO-8859-1?Q?baptiste_Augui=E9?=)
Date: Fri, 28 Dec 2007 21:40:32 +0000
Subject: [R] unit attribute to list elements
In-Reply-To: <971536df0712281206x724d70e7t9b2edade58a2044e@mail.gmail.com>
References: <59FE4C0D-6FE2-4538-B801-F12D66C6C309@ex.ac.uk>
	<971536df0712281206x724d70e7t9b2edade58a2044e@mail.gmail.com>
Message-ID: <2BD05D59-38FE-4FA6-AB73-AC9713D52489@ex.ac.uk>


On 28 Dec 2007, at 20:06, Gabor Grothendieck wrote:

> Is this what you want?
>

Perfect! Thanks a lot!

>> Lines <- "cel 3.0 m/s
> + Z0 367 ohm
> + eps0 8.9e-12 F/m
> + "
>> Constants.DF <- read.table(textConnection(Lines), as.is = TRUE)
>> Constants <- as.list(Constants.DF[[2]])
>> names(Constants) <- Constants.DF[[1]]
>> for(i in seq_along(Constants)) comment(Constants[[i]]) <-  
>> Constants.DF[i, 3]
>>
>> # get value
>> Constants$Z0
> [1] 367
>> # get comment
>> comment(Constants$Z0)
> [1] "ohm"
>> # add another Constant
>> Constants$e <- 2.7
>> comment(Constants$e) <- "exp"
>
>
> On Dec 28, 2007 2:33 PM, baptiste Augui? <ba208 at exeter.ac.uk> wrote:
>> Hi,
>>
>> I've started my own (first) package, part of which consists in
>> listing common physical constants (Planck's constant, the speed of
>> light in vacuum, etc). I'm wondering what would be a good way of
>> dealing with pairs of value/unit.
>>
>>
>>> constants <- list( cel = 2.99792458e8 , #m/s
>>> Z0 = 376.730313461, #ohm
>>> eps0 = 8.854187817e-12,#F/m
>>> mu0 = 4*pi*1e-7,#N/A^2
>>> G = 6.67428e-11 # m^3 kg-1 s-2
>>> )
>>
>>
>> I thought I could include the unit in the names attribute of each
>> element, as in :
>>
>>> names(constants$cel)<- " speed of light in vacuum [m.s^-1]"
>>
>>
>> Writing this for every element is very redundant... Is there any way
>> to access and set the name of each first level element of the list?
>>
>>> namesFirstLevelElements(constants)<- c(" speed of light in vacuum
>>> [m.s^-1]",
>>> "impedance of vacuum [some unit]",
>>> ...)
>>
>>
>> Quite possibly, I'm completely on the wring track;
>>
>> - maybe such a package already exists
>>
>> - a custom class or structure would be more appropriate? I don't
>> really know how to deal with classes, though, and I'd like to keep
>> the access to the constants' values as direct and generic as  
>> possible.
>>
>> Many thanks in advance,
>>
>> baptiste
>>
>> _____________________________
>>
>> Baptiste Augui?
>>
>> Physics Department
>> University of Exeter
>> Stocker Road,
>> Exeter, Devon,
>> EX4 4QL, UK
>>
>> Phone: +44 1392 264187
>>
>> http://newton.ex.ac.uk/research/emag
>> http://projects.ex.ac.uk/atto
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From vistocco at unicas.it  Fri Dec 28 22:48:21 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Fri, 28 Dec 2007 22:48:21 +0100
Subject: [R] gplot function - error
In-Reply-To: <455747.37438.qm@web34306.mail.mud.yahoo.com>
References: <455747.37438.qm@web34306.mail.mud.yahoo.com>
Message-ID: <47756F25.4080303@unicas.it>

You changed the intial letter: the function is qplot and not gplot.

domenico

Ricardo Perrone wrote:
> Hi all,
>
> i installed the ggplot2 package via install.packages(), but the gplot function was not recognized in R console command. Is there any paths to configure? The error message reports that the function was not found.
>
> Thanks
> Ricardo
>
>
>
>
>       ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From thomas_schwander at web.de  Fri Dec 28 23:13:15 2007
From: thomas_schwander at web.de (Thomas Schwander)
Date: Fri, 28 Dec 2007 23:13:15 +0100
Subject: [R] Forcing "virtual" digits
Message-ID: <E1J8NS9-0002Jm-00@smtp07.web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071228/47230072/attachment.pl 

From maura.monville at gmail.com  Fri Dec 28 23:18:51 2007
From: maura.monville at gmail.com (Maura E Monville)
Date: Fri, 28 Dec 2007 16:18:51 -0600
Subject: [R] two plots on the same page
Message-ID: <36d691950712281418i420dd6few24b766e6fc826b1a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071228/21614b1a/attachment.pl 

From mazatlanmexico at yahoo.com  Fri Dec 28 23:32:39 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Fri, 28 Dec 2007 14:32:39 -0800 (PST)
Subject: [R] print ggplot2 summary on graphic
Message-ID: <101986.61022.qm@web56602.mail.re3.yahoo.com>

Hi all: The code below works well to print annotations
on base graphics but I can't make it work using the
ggplot2 package. I tried to use the capture.output
function but only prints the summary on the R console
and no on the graphic. Any ideas on how to print
output on ggplot2 graphs? For a powerpoint
presentation I basically just want to show viewers the
mean and Median on the graph since they won't be
seeing the data or the R console.
aa <- 1:10
plot(aa)
text(c(2,4),10, labels=c( "mean", "sd"),col=2)
text(c(2,4),9.5, labels=c(mean(aa), sd(aa)),col=4)
legend("topright",capture.output(sd(aa[1:2])))

m <- ggplot(movies, aes(x=rating))
m + geom_density()
capture.output(mean(movies[3]))


Felipe D. Carrillo
  Fishery Biologist
  US Fish & Wildlife Service
  California, USA



      ____________________________________________________________________________________
Looking for last minute shopping deals?


From jessen at econinfo.de  Fri Dec 28 23:59:33 2007
From: jessen at econinfo.de (Owe Jessen)
Date: Fri, 28 Dec 2007 23:59:33 +0100
Subject: [R] FYI: Package installation problem with windows
Message-ID: <47757FD5.4090808@econinfo.de>

Hello,

maybe this info is helpful for someone else, at least it is a reminder 
for me if the problem should reoccur:

The last hours R drove me nuts because it wouldn't install new packages, 
or more irritating still: It would chose randomly which packages to 
install when given a list ("unable to move temporary installation" ). 
The solution was to turn of the indexation of folders.

HTH

Owe

-- 
Owe Jessen
Diplom-Volkswirt
Hanssenstra?e 17
24106 Kiel

jessen at econinfo.de
http://www.econinfo.de


From bolker at ufl.edu  Fri Dec 28 23:02:41 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 28 Dec 2007 22:02:41 +0000 (UTC)
Subject: [R] AIC and BIC in mixed effects model
References: <da973ba50712271158x29287f66y9d7a04d02396e368@mail.gmail.com>
Message-ID: <loom.20071228T214932-525@post.gmane.org>

Chuanjun Zhang <zhangchu <at> umkc.edu> writes:

> 
> Dear R Users:
> 
> I am trying to compare several structures of the within-patient covariance
> such as unstructured, Autoregressive, and spatial by using the MIXED effects
> model. Can AIC, BIC be negative ? If yes, then in what situations they may
> be negative.
> 

  This almost deserves to be a FAQ, although it's a statistical rather than an R
issue:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46734.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/108660.html

  The one-sentence answer is that since probability *densities* can be >1,
log-likelihood *densities* can be >0 and hence negative log-likelihoods can be
<0 (so AIC/BIC can also be <0).  However, there's a potentially larger issue
with using AICs for mixed models, which is that it's not always entirely clear
what the right number of degrees of freedom is for a random effect ...
 
  cheers
    Ben Bolker


From francogrex at mail.com  Sat Dec 29 00:14:26 2007
From: francogrex at mail.com (francogrex)
Date: Fri, 28 Dec 2007 15:14:26 -0800 (PST)
Subject: [R]  How can I change the language back to english?
Message-ID: <14532778.post@talk.nabble.com>


I have reset my windows and re-installed R (I explicitly asked for english)
and although the windows XP version is in english I am having R display in
french. like:

"R est un logiciel libre livr? sans AUCUNE GARANTIE.
Vous pouvez le redistribuer sous certaines conditions.
Tapez 'license()' ou 'licence()' pour plus de d?tails.
R est un projet collaboratif avec de nombreux contributeurs.
Tapez 'contributors()' pour plus d'information et
'citation()' pour la fa?on de le citer dans les publications.
Tapez 'demo()' pour des d?monstrations, 'help()' pour l'aide
en ligne ou 'help.start()' pour obtenir l'aide au format HTML.
Tapez 'q()' pour quitter R."

I know I have an azerty french keyboard but even before the reset I had the
same azerty french
keyborad and the same XP settings and R was completely in english, now it's
annoyinly in french and reinstalling R again doesn't change it back to
english like I want it. how can I set it in english, I hate having my R
message in french. thanks.
-- 
View this message in context: http://www.nabble.com/How-can-I-change-the-language-back-to-english--tp14532778p14532778.html
Sent from the R help mailing list archive at Nabble.com.


From smckinney at bccrc.ca  Sat Dec 29 02:09:45 2007
From: smckinney at bccrc.ca (Steven McKinney)
Date: Fri, 28 Dec 2007 17:09:45 -0800
Subject: [R] two plots on the same page
References: <36d691950712281418i420dd6few24b766e6fc826b1a@mail.gmail.com>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0328A102@crcmail1.BCCRC.CA>


> -----Original Message-----
> From: r-help-bounces at r-project.org on behalf of Maura E Monville
> Sent: Fri 12/28/2007 2:18 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] two plots on the same page
>  
> I'd like to know why I cannot get a plot and the QQnorm in the same sheet.
> The commands are simple but:
> 
>  library(nlme)
>  glmod1 <- gls(upfmla,correlation=corAR1(),method="ML")
>  summary(glmod1)
>  par(mfrow = c(2,1))
>  plot(glmod1, main="GLS Residuals vs. GLS Fitted")
>  qqnorm(glmod1)
> 
> No matter what (I tried different permutations of the plotting commands) the
> second drawing is overlapped to the first one instead of being placed
> underneath it on the same page.
> How come?
> 

Because glmod1 is of class "gls"

> class(glmod1)
[1] "gls"

so when you plot it you are invoking
plot.gls()

See the help for function plot.gls() in the nlme library.

> ?plot.gls

reports that the value of a call is

   Value

   a diagnostic Trellis plot.

so you are dealing with Trellis plots handled by the 
lattice package.  Trellis plots do not use
the par() settings used by regular plots.


Here's one way to do it.  (I substituted
an example from nlme since I don't have your
data and "upfmla" object.


library(nlme)

glmod1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
           correlation = corAR1(form = ~ 1 | Mare))
summary(glmod1)

plot1 <- plot(glmod1, main="GLS Residuals vs. GLS Fitted")
plot2 <- qqnorm(glmod1)
print(plot1, split = c(1, 1, 1, 2), more = TRUE)
print(plot2, split = c(1, 2, 1, 2))



Try something like this.  The 'split' mechanism
works for me and I get both plots on one page.

Remember also that you can save the output of
trellis plot commands and print them later.  Trellis
plots do not always print right away - in many situations
you must explicitly wrap your trellis plot
command with print() before it will render on a
graphics device.


You can read more about putting multiple trellis plots on a
page in the help page for 
print.trellis 
in the lattice package.

Hope this helps.

After you master the lattice package
you can go to that New Years party
and celebrate!


Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney +at+ bccrc +dot+ ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada


> Thank you very much.
> Happy New Year,
> Maura
> 
> 
> 
> 
> 
> -- 
> Maura E.M
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 


From jholtman at gmail.com  Sat Dec 29 03:49:43 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 28 Dec 2007 21:49:43 -0500
Subject: [R] Forcing "virtual" digits
In-Reply-To: <E1J8NS9-0002Jm-00@smtp07.web.de>
References: <E1J8NS9-0002Jm-00@smtp07.web.de>
Message-ID: <644e1f320712281849p2faba3c7s1c98a26c476ba9c6@mail.gmail.com>

?sprintf

x <- 33
plot(0, main=sprintf("Value = %.1f", x))

On Dec 28, 2007 5:13 PM, Thomas Schwander <thomas_schwander at web.de> wrote:
> Hi there,
>
>
>
> I'm using XP with R 2.6.1
>
>
>
> I've got the following question: Is there a way to force "virtual" digits?
>
>
>
> I mean: In the external csv-file, I read in some numbers, e.g. "33" which is
> written into a variable.
>
>
>
> I want to add this variable into a plot with an additional digit: "33.0".
>
>
>
> I know, that this is a kind of cheating, because the number was not measured
> as precisely, but it looks better in the graph.
>
>
>
> Any ideas?
>
>
>
> Thanks for your help,
>
> Thomas
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ripley at stats.ox.ac.uk  Sat Dec 29 04:46:57 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 29 Dec 2007 03:46:57 +0000 (GMT)
Subject: [R] FYI: Package installation problem with windows
In-Reply-To: <47757FD5.4090808@econinfo.de>
References: <47757FD5.4090808@econinfo.de>
Message-ID: <Pine.LNX.4.64.0712290344300.23878@gannet.stats.ox.ac.uk>

On Fri, 28 Dec 2007, Owe Jessen wrote:

> Hello,
>
> maybe this info is helpful for someone else, at least it is a reminder
> for me if the problem should reoccur:
>
> The last hours R drove me nuts because it wouldn't install new packages,
> or more irritating still: It would chose randomly which packages to
> install when given a list ("unable to move temporary installation" ).
> The solution was to turn of the indexation of folders.

See the rw-FAQ Q4.8.
It is not normal for the Windows' Indexing Service to cause any problems 
here, but enhanced versions can.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sat Dec 29 04:56:20 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 29 Dec 2007 03:56:20 +0000 (GMT)
Subject: [R] How can I change the language back to english?
In-Reply-To: <14532778.post@talk.nabble.com>
References: <14532778.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0712290351000.23878@gannet.stats.ox.ac.uk>

This is rw-FAQ Q3.2, Q3.4.

But the underlying problem is that Windows is telling R it is in French, 
and perhaps you need to sort that out.

On Fri, 28 Dec 2007, francogrex wrote:

>
> I have reset my windows and re-installed R (I explicitly asked for english)
> and although the windows XP version is in english I am having R display in
> french. like:
>
> "R est un logiciel libre livr? sans AUCUNE GARANTIE.
> Vous pouvez le redistribuer sous certaines conditions.
> Tapez 'license()' ou 'licence()' pour plus de d?tails.
> R est un projet collaboratif avec de nombreux contributeurs.
> Tapez 'contributors()' pour plus d'information et
> 'citation()' pour la fa?on de le citer dans les publications.
> Tapez 'demo()' pour des d?monstrations, 'help()' pour l'aide
> en ligne ou 'help.start()' pour obtenir l'aide au format HTML.
> Tapez 'q()' pour quitter R."
>
> I know I have an azerty french keyboard but even before the reset I had the
> same azerty french
> keyborad and the same XP settings and R was completely in english, now it's
> annoyinly in french and reinstalling R again doesn't change it back to
> english like I want it. how can I set it in english, I hate having my R
> message in french. thanks.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From jim at bitwrit.com.au  Sat Dec 29 11:47:19 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 29 Dec 2007 21:47:19 +1100
Subject: [R] ASUS Eee R cookbook
Message-ID: <477625B7.8020201@bitwrit.com.au>

Hi all,
Having seen a few messages regarding the ASUS Eee PC (p701) and R, and 
then buying one and trying to get R installed on it, I thought there 
might be a few people on the list who would appreciate a cookbook on how 
to do it. It's a nice little piece of machinery, running Xandros 4 Linux 
(seems to be compatible with Debian Etch) and set up for the person who 
wants to do all those Webby things plus maybe a bit of office type work. 
All that works right out of the box. However, getting R going took me a 
few tries.

First get your Eee an internet connection. This was embarrassingly easy 
for me, I just plugged it into the D-Link router, clicked the LAN option 
and in a few seconds I was online. It also has wireless and even yer old 
bleep-buzz modem options.

You may be able to run R on the Easy Desktop, but I went straight to the 
Full Desktop (KDE). There is an excellent description of how to download 
the necessary files and enable KDE at:

http://wiki.eeeuser.com/howto:getkde

This gives you what most people who use Linux are used to (you can 
apparently enable GNOME, too). Now for the fun. I wasn't able to compile 
R from source as gcc wouldn't go. I'll let you know if I get it working.

As in setting up KDE, you have to add a repository.

1a. Applications|System|Synaptic Package Manager

OR

1b. Start a console, su to root and enter "synaptic"

2. Alt-S (Settings) | R (Repositories)

3. Alt-N (New)

4. Enter the URI of the repository, the distribution and the section(s)

This is where I ran afoul of the Debian voodoo. You don't actually enter 
the URI, just the bit before "dists" (I think). Then you enter the 
distribution (etch) and then "main". After many tries, my entry looks 
like this:

URI: ftp://ftp.au.debian.org/debian/
Distribution: etch
Section(s): main

while the actual URI is:

ftp://ftp.au.debian.org/debian/dists/etch/Contents-i386.gz

Maybe there is a better way. I'm happy to be corrected. The package 
listing will then download. When complete:

5. Scroll down to the "r-base" package, right click and select "Mark for 
Installation".

6. Click "Apply" near the top of the window.

7. Confirm in the dialog that appears and it should all happen.

At this point, I went off for lunch, as it looked like taking half an 
hour or so. The power pack on the Eee has those annoying half-shielded 
prongs and it had come loose at some point. When I returned, the Eee had 
shut down. Every time I powered it up, it would shut down again. I 
reverted to the factory settings using F9 on bootup (thereby losing all 
my upgrades) and it still kept shutting down, faster each time. I 
finally worked out that the battery had gone critically flat, recharged 
it, went through the whole process again and it works fine.

One thing that helps a lot is to resize the graphics window as the 
screen is kind of small. I tried:

x11(width=5,height=5)

and it didn't quite work, but then something I did (hit the wrong key? 
unintentionally clicked on something?) worked and it seemed to remember 
whatever I had done. Now my plots fit on the screen.

I have installed a few source packages, but until I get gcc to go, I 
will have to use synaptic to install anything that requires compilation. 
If I have made any gross mistakes or any of this doesn't appear to work, 
please let me know. I think it's absolutely fantastic to have R running 
on something that I can stick in a large pocket.

Jim


From samu.mantyniemi at helsinki.fi  Sat Dec 29 12:03:50 2007
From: samu.mantyniemi at helsinki.fi (=?ISO-8859-1?Q?Samu_M=E4ntyniemi?=)
Date: Sat, 29 Dec 2007 13:03:50 +0200
Subject: [R] Multicore computation in Windows network: How to set up Rmpi
In-Reply-To: <476B7453.1010406@helsinki.fi>
References: <476A7427.8060702@helsinki.fi> <476B7453.1010406@helsinki.fi>
Message-ID: <47762996.8090909@helsinki.fi>

Hello!

I finally got MPICH 1.06 + R 2.6.1 + Rmpi 0.5-5 working with multiple 
computers. The key was to realize that the number of processes should be 
one when launching Rgui using mpiexec and not the number of 
master+slaves, as I had first wrongly understood.

However, I seem to have a new problem which I have not been able to 
figure out:

After loading Rmpi, the first attempt to mpi.spawn.Rslaves() always 
spawns the slaves on the local machine instead of on both machines. If I 
close the slaves and spawn again, then one slave gets spawned on remote 
machine. Each time I close and then spawn againg, the order of machines 
is different, and eventually I get back to the situation where all 
slaves are on the local machine. Continuing to do spawning and closing 
seems to reveal a pattern. I can see similar behavior if I have more 
than two machines, and it takes more spawn-close cycles to get all my 
slave machines spawned on.

Below is an example session with two machines. This pattern shows 
everytime I start R and run this script. How to control the spawning so 
that I get everything right at the first call of mpi.spawn.Rslaves()?

Regards,

Samu

<R>

 >
 > library(Rmpi)
 > sessionInfo()
R version 2.6.1 (2007-11-26)
i386-pc-mingw32

locale:
LC_COLLATE=Finnish_Finland.1252;LC_CTYPE=Finnish_Finland.1252;LC_MONETARY=Finnish_Finland.1252;LC_NUMERIC=C;LC_TIME=Finnish_Finland.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] Rmpi_0.5-5
 > mpi.universe.size()
[1] 2
 > mpichhosts()
          master          slave1          slave2
"clustermaster" "clustermaster" "clusterslave1"
 > mpi.spawn.Rslaves()
         2 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 3 is running on: ClusterMaster
slave1 (rank 1, comm 1) of size 3 is running on: ClusterMaster
slave2 (rank 2, comm 1) of size 3 is running on: ClusterMaster
 > mpi.close.Rslaves()
[1] 1
 > mpi.spawn.Rslaves()
         2 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 3 is running on: ClusterMaster
slave1 (rank 1, comm 1) of size 3 is running on: ClusterSlave1
slave2 (rank 2, comm 1) of size 3 is running on: ClusterMaster
 > mpi.close.Rslaves()
[1] 1
 > mpi.spawn.Rslaves()
         2 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 3 is running on: ClusterMaster
slave1 (rank 1, comm 1) of size 3 is running on: ClusterMaster
slave2 (rank 2, comm 1) of size 3 is running on: ClusterSlave1
 > mpi.close.Rslaves()
[1] 1
 > mpi.spawn.Rslaves()
         2 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 3 is running on: ClusterMaster
slave1 (rank 1, comm 1) of size 3 is running on: ClusterMaster
slave2 (rank 2, comm 1) of size 3 is running on: ClusterMaster
 > mpi.close.Rslaves()
[1] 1
 > mpi.spawn.Rslaves()
         2 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 3 is running on: ClusterMaster
slave1 (rank 1, comm 1) of size 3 is running on: ClusterSlave1
slave2 (rank 2, comm 1) of size 3 is running on: ClusterMaster
 > mpi.close.Rslaves()
[1] 1
 > mpi.spawn.Rslaves()
         2 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 3 is running on: ClusterMaster
slave1 (rank 1, comm 1) of size 3 is running on: ClusterMaster
slave2 (rank 2, comm 1) of size 3 is running on: ClusterSlave1
 > mpi.close.Rslaves()
[1] 1
 >
 >
 > mpi.spawn.Rslaves()
         2 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 3 is running on: ClusterMaster
slave1 (rank 1, comm 1) of size 3 is running on: ClusterMaster
slave2 (rank 2, comm 1) of size 3 is running on: ClusterMaster
 > mpi.close.Rslaves()
[1] 1
 >

</R>


Samu M?ntyniemi kirjoitti:
> Some progress in my problem:
> 
> Samu M?ntyniemi kirjoitti:
> 
>> With MPICH2 I managed to connect my computers so that I was able to
>> remotely launch Rgui on both machines but R hanged when calling
>> "library(Rmpi)". If only one Rgui was launched on the localhost,
>> "library(Rmpi)" worked without errors, but trying to use
>> "mpi.spawn.Rslaves()" resulted in an error message, and so did
>> "mpi.universe.size()". (In my current setup I can not reproduce this 
>> error message, but I can go back to this setup if this seems to be an 
>> important piece of information)
> 
> I vent back to MPICH2 installation to see what the error was:
> "ERROR in names(HOSTNAMES)<-base: attempt to set an attribute on NULL"
> 
> Trying to rethink what the problem was I realized that unlike in 
> DeinoMPI, I need to write the host names manually on the "configurable 
> settings" -window, and in order to have one cpu available on the local 
> machine, I need to write "myhostname:2".
> 
> After these changes MPICH2 1.06 +R-2.6.0+Rmpi 0.5-5 work on the single 
> machine in the same way as my DeinoMPI installation: Correct number of 
> cpu:s is detected and I can "mpi.spawn.Rslaves()"
> 
> I will try to do this with two hosts next and see if there is more luck 
> with MPICH2 than DeinoMPI.
> 
> Samu
> 
> 
> 
> 
> ------------------------------------------
> Samu M?ntyniemi
> Researcher
> Fisheries and Environmental Management Group (FEM)
> Department of Biological and Environmental Sciences
> Biocenter 3, room 4414
> Viikinkaari 1
> P.O. Box 65
> FIN-00014 University of Helsinki
> 
> Phone: +358 9 191 58710
> Fax: +358 9 191 58257
> 
> email: samu.mantyniemi at helsinki.fi
> personal webpage: http://www.helsinki.fi/people/samu.mantyniemi/
> FEM webpage: http://www.helsinki.fi/science/fem/
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 


-- 
------------------------------------------
Samu M?ntyniemi
Researcher
Fisheries and Environmental Management Group (FEM)
Department of Biological and Environmental Sciences
Biocenter 3, room 4414
Viikinkaari 1
P.O. Box 65
FIN-00014 University of Helsinki

Phone: +358 9 191 58710
Fax: +358 9 191 58257

email: samu.mantyniemi at helsinki.fi
personal webpage: http://www.helsinki.fi/people/samu.mantyniemi/
FEM webpage: http://www.helsinki.fi/science/fem/


From bgreen at dyson.brisnet.org.au  Sat Dec 29 13:38:54 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Sat, 29 Dec 2007 22:38:54 +1000
Subject: [R] another index question
Message-ID: <20071229123500.71F44595578@borg.st.net.au>


I am hoping for some advice regarding another index problem. The code 
below is intended to assign a value of 'V' if values on certain 
variables are >= 1, assign a value of 'N' if values on certain 
variables are >= 1  on other variables and assign a 'O' if values on 
any variable which have a value <= 0.

The outcome of this code is to assign a single 'O'. Clearly I have 
made a mistake somewhere.

pre_  <- new <- rep(0, nrow(reoffend))
 > pre_  <- new[reoffend$pre01111 | reoffend$pre012 | 
reoffend$pre013>= 1] <-'V'
 > pre_  <- new[reoffend$pre02111>=1 | reoffend$pre02114>=1 | 
reoffend$pre2029>=1 | reoffend$pre0212>=1 | reoffend$pre0211>=1]<- 'V'
 > pre_  <- new[reoffend$pre03a >=1 | reoffend$pre0311 >=1 | 
reoffend$pre0312 >=1| reoffend$pre03 >= 1] <- 'V'
 > pre_  <- new[reoffend$pre04>=1 | reoffend$pre05 >=1 | 
reoffend$pre06 >=1] <- 'V'
 > pre_  <- new[reoffend$pre07 >=1 | reoffend$pre08 >=1 | 
reoffend$pre09 >=1| reoffend$pre10 >= 1 |reoffend$pre11 >=1 | 
reoffend$pre12 >=1 | reoffend$pre13 >=1| reoffend$pre14 >= 1 
|reoffend$pre15 >=1| reoffend$pre16 >= 1 ] <- 'N'
 > pre_  <- new[reoffend$pre01111 | reoffend$pre012 | reoffend$pre013<=0] <-'O'
 > pre_  <- new[reoffend$pre02111<=0 | reoffend$pre02114<=0 | 
reoffend$pre2029<=0 | reoffend$pre0212<=0 | reoffend$pre0211 <=0] <-'O'
 > pre_  <- new[reoffend$pre03a | reoffend$pre0311 | reoffend$pre0312 
| reoffend$pre03<=0] <- 'O'
 > pre_  <- new[reoffend$pre04 | reoffend$pre05 | reoffend$pre06<=0] <- 'O'
 > pre_ <- new[reoffend$pre07| reoffend$pre08 | reoffend$pre09 | 
reoffend$pre10 |reoffend$pre11 | reoffend$pre12 | reoffend$pre13 | 
reoffend$pre14 |reoffend$pre15 | reoffend$pre16<= 0 ] <- 'O'
 >

Secondly, once this is resolved I was hoping to change the prefix 
"pre_)" to "mhc_", repeat this code and join the pre_ and mhc_ 
variables - so the new variable would have values such as 'VO" or 
'NV' etc. I was hoping something like :  total <- c(pre_ , mhc_) 
would do this or am I misguided.

Any assistance is much appreciated,

Bob Green


From jholtman at gmail.com  Sat Dec 29 14:23:02 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 29 Dec 2007 08:23:02 -0500
Subject: [R] another index question
In-Reply-To: <20071229123500.71F44595578@borg.st.net.au>
References: <20071229123500.71F44595578@borg.st.net.au>
Message-ID: <644e1f320712290523x1042d8beld77fb8dd028a6237@mail.gmail.com>

First of all, your last statement:

 > pre_ <- new[reoffend$pre07| reoffend$pre08 | reoffend$pre09 |
reoffend$pre10 |reoffend$pre11 | reoffend$pre12 | reoffend$pre13 |
reoffend$pre14 |reoffend$pre15 | reoffend$pre16<= 0 ] <- 'O'

says that if any of the variables except pre16 are non-zero, the
result will be "O".  It is not saying is any of the variables are less
than or equal to zero; only the last one.  You probably want:

pre_ [reoffend$pre07 <= 0| reoffend$pre08 <= 0 | reoffend$pre09 <= 0 |
reoffend$pre10 <= 0 |reoffend$pre11  <= 0| reoffend$pre12 <= 0 |
reoffend$pre13 <= 0 |
reoffend$pre14 <= 0 |reoffend$pre15 <= 0 | reoffend$pre16<= 0] <- "O"

You really don't need 'new'.

I think you also have the same problem in:

> pre_  <- new[reoffend$pre01111 | reoffend$pre012 |
reoffend$pre013>= 1] <-'V'

pre01111 and pre012 only have to be non-zero to make the statement
TRUE. you probably want:

pre_[reoffend$pre01111 >=1 | reoffend$pre012 >=1|
reoffend$pre013>= 1] <-'V'

On Dec 29, 2007 7:38 AM, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
>
> I am hoping for some advice regarding another index problem. The code
> below is intended to assign a value of 'V' if values on certain
> variables are >= 1, assign a value of 'N' if values on certain
> variables are >= 1  on other variables and assign a 'O' if values on
> any variable which have a value <= 0.
>
> The outcome of this code is to assign a single 'O'. Clearly I have
> made a mistake somewhere.
>
> pre_  <- new <- rep(0, nrow(reoffend))
>  > pre_  <- new[reoffend$pre01111 | reoffend$pre012 |
> reoffend$pre013>= 1] <-'V'
>  > pre_  <- new[reoffend$pre02111>=1 | reoffend$pre02114>=1 |
> reoffend$pre2029>=1 | reoffend$pre0212>=1 | reoffend$pre0211>=1]<- 'V'
>  > pre_  <- new[reoffend$pre03a >=1 | reoffend$pre0311 >=1 |
> reoffend$pre0312 >=1| reoffend$pre03 >= 1] <- 'V'
>  > pre_  <- new[reoffend$pre04>=1 | reoffend$pre05 >=1 |
> reoffend$pre06 >=1] <- 'V'
>  > pre_  <- new[reoffend$pre07 >=1 | reoffend$pre08 >=1 |
> reoffend$pre09 >=1| reoffend$pre10 >= 1 |reoffend$pre11 >=1 |
> reoffend$pre12 >=1 | reoffend$pre13 >=1| reoffend$pre14 >= 1
> |reoffend$pre15 >=1| reoffend$pre16 >= 1 ] <- 'N'
>  > pre_  <- new[reoffend$pre01111 | reoffend$pre012 | reoffend$pre013<=0] <-'O'
>  > pre_  <- new[reoffend$pre02111<=0 | reoffend$pre02114<=0 |
> reoffend$pre2029<=0 | reoffend$pre0212<=0 | reoffend$pre0211 <=0] <-'O'
>  > pre_  <- new[reoffend$pre03a | reoffend$pre0311 | reoffend$pre0312
> | reoffend$pre03<=0] <- 'O'
>  > pre_  <- new[reoffend$pre04 | reoffend$pre05 | reoffend$pre06<=0] <- 'O'
>  > pre_ <- new[reoffend$pre07| reoffend$pre08 | reoffend$pre09 |
> reoffend$pre10 |reoffend$pre11 | reoffend$pre12 | reoffend$pre13 |
> reoffend$pre14 |reoffend$pre15 | reoffend$pre16<= 0 ] <- 'O'
>  >
>
> Secondly, once this is resolved I was hoping to change the prefix
> "pre_)" to "mhc_", repeat this code and join the pre_ and mhc_
> variables - so the new variable would have values such as 'VO" or
> 'NV' etc. I was hoping something like :  total <- c(pre_ , mhc_)
> would do this or am I misguided.
>
> Any assistance is much appreciated,
>
> Bob Green
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From frainj at gmail.com  Sat Dec 29 15:17:04 2007
From: frainj at gmail.com (John C Frain)
Date: Sat, 29 Dec 2007 14:17:04 +0000
Subject: [R] ASUS Eee R cookbook
In-Reply-To: <477625B7.8020201@bitwrit.com.au>
References: <477625B7.8020201@bitwrit.com.au>
Message-ID: <fad888a10712290617m5d8272f7i8b9e9a8d9e589b11@mail.gmail.com>

About a year ago I looked at running R (and the Rmetrics packages) on
an early UMPC Samsung Q2  with 512 GB of memory.  I ran R from a USB
flash memory drive and execution times were about twice that on my
current desktop PC with 2 GB memory and an Intel Core 2 Duo E6600
(2.4GHz).  I know that the relative results are probably specific to
the particular program that I was testing but they speak volumes for
the CPU in the Samsung Q2 which is the same as that in the Asus EEE
PC.  My desktop is running Windows XP Professional and the Samsung was
running the tablet version of Windows XP.

Given the small internal flash memory on the EEE PC I wonder if anyone
has tried to run R from an external flash drive in Linux.  Could one
expect as good a performance from the EEE PC as I got from the
Samsung.  (I also tried some TeX compilations from flash memory on the
Samsung with satisfactory results.  The Samsung  is about the same
size as the EEE PC but is a multiple of the cost.  I suspect that the
current price of the EEE PC will fall as soon as supply increases and
some other manufacturers enter the market. It would be great to be
able to have so portable a device that was capable of running R,
LaTeX, Octave, Mathematica etc, as well as open office and firefox.


On 29/12/2007, Jim Lemon <jim at bitwrit.com.au> wrote:
> Hi all,
> Having seen a few messages regarding the ASUS Eee PC (p701) and R, and
> then buying one and trying to get R installed on it, I thought there
> might be a few people on the list who would appreciate a cookbook on how
> to do it. It's a nice little piece of machinery, running Xandros 4 Linux
> (seems to be compatible with Debian Etch) and set up for the person who
> wants to do all those Webby things plus maybe a bit of office type work.
> All that works right out of the box. However, getting R going took me a
> few tries.
>
> First get your Eee an internet connection. This was embarrassingly easy
> for me, I just plugged it into the D-Link router, clicked the LAN option
> and in a few seconds I was online. It also has wireless and even yer old
> bleep-buzz modem options.
>
> You may be able to run R on the Easy Desktop, but I went straight to the
> Full Desktop (KDE). There is an excellent description of how to download
> the necessary files and enable KDE at:
>
> http://wiki.eeeuser.com/howto:getkde
>
> This gives you what most people who use Linux are used to (you can
> apparently enable GNOME, too). Now for the fun. I wasn't able to compile
> R from source as gcc wouldn't go. I'll let you know if I get it working.
>
> As in setting up KDE, you have to add a repository.
>
> 1a. Applications|System|Synaptic Package Manager
>
> OR
>
> 1b. Start a console, su to root and enter "synaptic"
>
> 2. Alt-S (Settings) | R (Repositories)
>
> 3. Alt-N (New)
>
> 4. Enter the URI of the repository, the distribution and the section(s)
>
> This is where I ran afoul of the Debian voodoo. You don't actually enter
> the URI, just the bit before "dists" (I think). Then you enter the
> distribution (etch) and then "main". After many tries, my entry looks
> like this:
>
> URI: ftp://ftp.au.debian.org/debian/
> Distribution: etch
> Section(s): main
>
> while the actual URI is:
>
> ftp://ftp.au.debian.org/debian/dists/etch/Contents-i386.gz
>
> Maybe there is a better way. I'm happy to be corrected. The package
> listing will then download. When complete:
>
> 5. Scroll down to the "r-base" package, right click and select "Mark for
> Installation".
>
> 6. Click "Apply" near the top of the window.
>
> 7. Confirm in the dialog that appears and it should all happen.
>
> At this point, I went off for lunch, as it looked like taking half an
> hour or so. The power pack on the Eee has those annoying half-shielded
> prongs and it had come loose at some point. When I returned, the Eee had
> shut down. Every time I powered it up, it would shut down again. I
> reverted to the factory settings using F9 on bootup (thereby losing all
> my upgrades) and it still kept shutting down, faster each time. I
> finally worked out that the battery had gone critically flat, recharged
> it, went through the whole process again and it works fine.
>
> One thing that helps a lot is to resize the graphics window as the
> screen is kind of small. I tried:
>
> x11(width=5,height=5)
>
> and it didn't quite work, but then something I did (hit the wrong key?
> unintentionally clicked on something?) worked and it seemed to remember
> whatever I had done. Now my plots fit on the screen.
>
> I have installed a few source packages, but until I get gcc to go, I
> will have to use synaptic to install anything that requires compilation.
> If I have made any gross mistakes or any of this doesn't appear to work,
> please let me know. I think it's absolutely fantastic to have R running
> on something that I can stick in a large pocket.
>
> Jim
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From perrone at rocketmail.com  Sat Dec 29 16:19:55 2007
From: perrone at rocketmail.com (Ricardo Perrone)
Date: Sat, 29 Dec 2007 07:19:55 -0800 (PST)
Subject: [R] gplot function - error
Message-ID: <671535.99825.qm@web34311.mail.mud.yahoo.com>

Sorry... the function is qplot from ggplot2 package.

i checked the R's documentation and the problem was solved with a simple command to load the package and its dependencies: library(ggplot2)

Thanks
Ricardo

----- Original Message ----
From: hadley wickham <h.wickham at gmail.com>
To: Ricardo Perrone <perrone at rocketmail.com>
Sent: Friday, December 28, 2007 5:20:42 PM
Subject: Re: [R] gplot function - error


Do you mean qplot or ggplot?

Hadley

On Dec 28, 2007 2:26 PM, Ricardo Perrone <perrone at rocketmail.com>
 wrote:
> Hi all,
>
> i installed the ggplot2 package via install.packages(), but the gplot
 function was not recognized in R console command. Is there any paths
 to configure? The error message reports that the function was not found.
>
> Thanks
> Ricardo
>
>
>
>
>      
 ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
 http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
http://had.co.nz/





      ____________________________________________________________________________________
Never miss a thing.  Make Yahoo your home page.


From ripley at stats.ox.ac.uk  Sat Dec 29 18:03:10 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 29 Dec 2007 17:03:10 +0000 (GMT)
Subject: [R] A function for random test based on longest run
	(UNCLASSIFIED)
In-Reply-To: <18291.58470.122385.233652@ron.nulle.part>
References: <8d5a36350712270908h574d6ff4y83b648e158e5da6e@mail.gmail.com>
	<18291.58470.122385.233652@ron.nulle.part>
Message-ID: <Pine.LNX.4.64.0712291653420.345@gannet.stats.ox.ac.uk>

On Thu, 27 Dec 2007, Dirk Eddelbuettel wrote:

>
> On 27 December 2007 at 12:08, bogdan romocea wrote:
> |   > require(tseries)
> |   > ?runs.test
> | Also, take a look at dieharder, it implements a large number of
> | randomness tests:
> | http://www.phy.duke.edu/~rgb/General/dieharder.php
>
> Also note that CRAN has an RDieHarder package that provides access to
> DieHarder tests from R.  RDieHarder is currently happier on Linux than on
> Windows. If anybody wants to contribute build instructions for Windows ...

It's not easy on Linux!  There is a src/Makefile that ignores most of the 
info from how R was built, including CC, CFLAGS andlibrary pats such as 
/usr/local/lib64.  And that's if you can get dieharder built: it gave me 
lots of errors before succeeding.  I managed to build from the SRPM, but 
that installed with the include files without read permissions.

But I've succeeded on Windows, and RDieHarder is now in the CRANextras 
collection (and so can be installed from the menus etc). I'll send Dirk 
separately notes on what I had to do to make it work.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From r.mueller at oeko-sorpe.de  Sat Dec 29 18:17:42 2007
From: r.mueller at oeko-sorpe.de (Richard =?iso-8859-1?q?M=FCller?=)
Date: Sat, 29 Dec 2007 18:17:42 +0100
Subject: [R] tcltk again
Message-ID: <200712291817.42974.r.mueller@oeko-sorpe.de>

Hello,
the admonition of Prof. Ripley to search the documentation to solve my problem 
helped, today I read a lot more on Tcl/Tk than before ;-)
But now I'm stuck again. With the help of my script some functions are plotted 
on the display, then I ask if the user wants to save it as pdf. In windows I 
use winDialog and it works. But I can't succeed in Linux. In short:
----------------------------------------------------
X11()
...
res <-    tkmessageBox(title="Beenden?", 
      message="Vor dem Beenden als PDF speichern?", 
      icon="question", type="okcancel")
if (tclvalue(res) == "ok")
...
-----------------------------------------------------
So far every thing is OK. Now I want a box the user should fill in the choosen 
filename (with a default)
and then
--------------------
pdf(filename)
...
--------------------
I experimented with tkgetSaveFile(). But in the moment the user gives the 
filename the file is not yet generated! And I can't generate it in advance, 
because the filename is not yet specified.
So I looked for a dialog box. [Some boxes are called "dialog boxes" though 
they only talk _to_ the user (besides the user's "yes" or "no")]. Just a 
simple box with a text field the user can type in and the value of this field 
given back.
One option was the "modal dialog", but I was not able to pick the essentials 
out of it. I did not want to press several buttons until the necessary window 
appears. Also the "editable text window" was not really a solution.
Perhaps a small hint again?
Richard

-- 
Richard M?ller - Am Spring 9 - D-58802 Balve-Eisborn
www.oeko-sorpe.de


From aa2007r at gmail.com  Sat Dec 29 19:44:34 2007
From: aa2007r at gmail.com (AA)
Date: Sat, 29 Dec 2007 13:44:34 -0500
Subject: [R] warning on gamma option in par(args) or calling par(= new)?
References: <00af01c848d5$9649c4f0$3301a8c0@MainDomain.local>
	<Pine.LNX.4.64.0712280804420.29436@gannet.stats.ox.ac.uk>
Message-ID: <010b01c84a4a$dca69880$3301a8c0@MainDomain.local>

Thank you for your quick reply Prof. Ripley.
I thought I am modifying par() in par(mfrow = c(4,2)). This is not clear for 
me.
But I do understand now that par() applies to each device.
Thanks again.
AA.
----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "AA" <aa2007r at gmail.com>
Cc: "'r-help'" <r-help at stat.math.ethz.ch>
Sent: Friday, December 28, 2007 3:09 AM
Subject: Re: [R] warning on gamma option in par(args) or calling par(= new)?


> You are calling par() *before* opening  and (in on.exit) *after* closing 
> the pdf() device.  par() applies on a per-device basis, and if no device 
> is open the default device will be opened.
>
> As for why you get the warnings about 'gamma' and 'new', see ?par and read 
> their entries.  Since you don't change any pars, I do not see why you are 
> attempting to reset them.
>
>
> On Thu, 27 Dec 2007, AA wrote:
>
>> Dear All,
>>
>> I have the following function
>>
>> tstpar <-
>>  function(n = 200, want.pdf = FALSE, pdfFileName = NULL){
>>    oldpar <- par(no.readonly = TRUE)
>>    on.exit(par(oldpar))
>>    steps  <- seq(from = 1, to = 8, by = 1)
>>    h <- 10; w <- 6
>>    if(want.pdf){pdf(file = pdfFileName, onefile = TRUE,
>>    paper = "letter", width = w, height = h)}
>>    par(mfrow = c(4,2))
>>    for(i in steps){
>>      txt <- paste("i = ", i)
>>      hist(rnorm(n), main = txt)
>>    }
>>    if(want.pdf){dev.off()}
>>  }
>>
>> when called with default values tstpar() every thing works fine.
>> However I get 2 sorts of warnings
>> 1- if I call the function to generate a pdf as
>>   tstpar(want.pdf = TRUE, pdfFilename = "jj.pdf")
>>   I get the following warning msg:
>>   calling par(new=) with no plot in: par(oldpar)
>>   with an empty device which I don not understand since I close
>>   the device in the function.
>> 2- If I comment the line
>>   if (want.pdf){dev.off()}
>>   I get the following warning
>>   'gamma' cannot be modified on this device in: par(args)
>>   with 2 empty devices.
>>
>> I do not understand why I get those warnings and why I get those empty 
>> devices.
>> I looked up in RSiteSearch and I found the 2 following posts
>> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/38553.html
>> In this post M. Schwarz explains that there is not a plot created that's 
>> why R generates the
>> warning. In the case of tstpar, I generate the hist plots and should not 
>> be getting the warning.
>>
>> And in the following post Prof Ripley refers to an old R warning for 
>> pdf() devices
>> Which is not the case here.
>> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/23215.html
>>
>> I do not see what I am missing. Thanks for any hint.
>> I use R 2.5.1 under Win XP.
>> I apologize if the question is related to my older R version which I will 
>> upgrade.
>>
>> AA.
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pedrosmarques at portugalmail.pt  Sat Dec 29 21:20:27 2007
From: pedrosmarques at portugalmail.pt (pedrosmarques at portugalmail.pt)
Date: Sat, 29 Dec 2007 20:20:27 +0000
Subject: [R] How to choose the best Kernel in SVM classification
Message-ID: <1198959627.4776ac0bb59d1@gold3.portugalmail.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071229/2dca9bd5/attachment.pl 

From p.dalgaard at biostat.ku.dk  Sat Dec 29 21:31:34 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sat, 29 Dec 2007 21:31:34 +0100
Subject: [R] tcltk again
In-Reply-To: <200712291817.42974.r.mueller@oeko-sorpe.de>
References: <200712291817.42974.r.mueller@oeko-sorpe.de>
Message-ID: <4776AEA6.8090803@biostat.ku.dk>

Richard M?ller wrote:
> Hello,
> the admonition of Prof. Ripley to search the documentation to solve my problem 
> helped, today I read a lot more on Tcl/Tk than before ;-)
> But now I'm stuck again. With the help of my script some functions are plotted 
> on the display, then I ask if the user wants to save it as pdf. In windows I 
> use winDialog and it works. But I can't succeed in Linux. In short:
> ----------------------------------------------------
> X11()
> ...
> res <-    tkmessageBox(title="Beenden?", 
>       message="Vor dem Beenden als PDF speichern?", 
>       icon="question", type="okcancel")
> if (tclvalue(res) == "ok")
> ...
> -----------------------------------------------------
> So far every thing is OK. Now I want a box the user should fill in the choosen 
> filename (with a default)
> and then
> --------------------
> pdf(filename)
> ...
> --------------------
> I experimented with tkgetSaveFile(). But in the moment the user gives the 
> filename the file is not yet generated! And I can't generate it in advance, 
> because the filename is not yet specified.
>   
I don't see what the problem is here.

fn <- tkgetSaveFile() allows you to select an existing file OR navigate to a directory and type in the name of a new file. In either case, you can feed tclvalue(fn) to pdf(). 

If you cancel the operation, then tclvalue(fn)=="", so test for that first.


> So I looked for a dialog box. [Some boxes are called "dialog boxes" though 
> they only talk _to_ the user (besides the user's "yes" or "no")]. Just a 
> simple box with a text field the user can type in and the value of this field 
> given back.
> One option was the "modal dialog", but I was not able to pick the essentials 
> out of it. I did not want to press several buttons until the necessary window 
> appears. Also the "editable text window" was not really a solution.
> Perhaps a small hint again?
> Richard
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From pedrosmarques at portugalmail.pt  Sat Dec 29 22:52:35 2007
From: pedrosmarques at portugalmail.pt (pedrosmarques at portugalmail.pt)
Date: Sat, 29 Dec 2007 21:52:35 +0000
Subject: [R] SVM simple error
Message-ID: <1198965155.4776c1a3de33e@gold3.portugalmail.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071229/5c730e60/attachment.pl 

From willis.charlie at gmail.com  Sun Dec 30 00:39:54 2007
From: willis.charlie at gmail.com (Charles Willis)
Date: Sat, 29 Dec 2007 18:39:54 -0500
Subject: [R] COMPAR.GEE error with logistic model
Message-ID: <f93d9f10712291539v5f65bc11raca5c85740274a89@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071229/9434f7fa/attachment.pl 

From nilsson.henric at gmail.com  Sun Dec 30 02:32:36 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Sun, 30 Dec 2007 02:32:36 +0100
Subject: [R] How can I extract the AIC score from a mixed model object
 produced using lmer?
In-Reply-To: <40e66e0b0712200631i684de05ardd564c71e6b9ecb1@mail.gmail.com>
References: <OFEA565530.971E9869-ON882573B5.00784A2E-882573B5.00797442@fs.fed.us>	<f21f775b0712181421w36e5fe6ep5d1229ed550cc58f@mail.gmail.com>	<14419438.post@talk.nabble.com>
	<40e66e0b0712200631i684de05ardd564c71e6b9ecb1@mail.gmail.com>
Message-ID: <4776F534.2030602@gmail.com>

Douglas Bates wrote:
> On Dec 19, 2007 9:42 AM, David Hewitt <dhewitt at vims.edu> wrote:
>>
>> David Barron-3 wrote:
>>> You can calculate the AIC as follows:
>>>
>>> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
>>> aic1 <- AIC(logLik(fm1))
>>>
>>>
> 
>> Is AIC() [extractAIC()] "valid" for models with random effects? I noticed
>> that the help page for extractAIC() does not list models with random
>> effects. I think this boils down to the difference between the likelihoods
>> for models with and without random effects, and I don't know. Just
>> curious...
> 
> The log-likelihood for a linear mixed model is well-defined.  Whether
> this makes AIC valid or not depends on how comfortable you are with
> the idea of AIC in the first place.  My impression is that the
> justification for AIC is not entirely rigorous but I must admit that I
> haven't gone back to look at the original literature on it.
> 
> To the best of my knowledge and ability the log-likelihood from a
> model fit by lmer with method = "ML" is properly defined and
> accurately evaluated.  (The default estimation criterion in lmer is
> REML and models fit by REML provide a close approximation to the
> log-likelihood but not an exact result.  If you really want a
> log-likelihood and AIC value you should refit with method = "ML".)
> What is later done to the log-likelihood to obtain the AIC value is
> more problematic.  In particular, one needs to provide a value for the
> number of parameters in the model and that can be tricky.  Recently I
> was working with models for data on test scores by students over time.
>  There were over 200,000 students.  Under one way of counting
> parameters, if I incorporate a random effect for the student this
> costs me only 1 parameter, corresponding to the variance component for
> that random effect.  However, I am incorporating over 200,000 random
> effects to help model the observed responses.  So is the number of
> parameters 1 or over 200,000?  I don't know.

I can't remember the exact details, but I do remember that the issue is 
discussed in

@ARTICLE{LMM:Vaida+Blanchard:2005,
   author = {Florin Vaida and Suzette Blanchard},
   title = {Conditional Akaike information for mixed-effects models},
   journal = {Biometrika},
   year = {2005},
   volume = {92},
   pages = {351?370},
   abstract = {This paper focuses on the Akaike information criterion,
               AIC, for linear mixed-effects models in the analysis of
               clustered data. We make the distinction between questions
               regarding the population and questions regarding the
               particular clusters in the data. We show that the AIC in
               current use is not appropriate for the focus on clusters,
               and we propose instead the conditional Akaike information
               and its corresponding criterion, the conditional AIC,
               cAIC. The penalty term in cAIC is related to the effective
               degrees of freedom p for a linear mixed model proposed by
               Hodges & Sargent (2001); p reflects an intermediate level
               of complexity between a fixed-effects model with no
               cluster effect and a corresponding model with fixed
               cluster effects. The cAIC is defined for both maximum
               likelihood and residual maximum likelihood estimation. A
               pharmacokinetics data application is used to illuminate
               the distinction between the two inference settings, and to
               illustrate the use of the conditional AIC in model
               selection.},
   keywords = {Akaike information; AIC; effective degrees of freedom;
               linear mixed model}
}


HTH,
Henric



> 
> Regarding the fact the the extractAIC help page doesn't mention models
> with random effects, it can't list all the possible methods because
> any package can add methods to a generic function.
> 
>>
>>> On 12/18/07, Peter H Singleton <psingleton at fs.fed.us> wrote:
>>>> I am running a series of candidate mixed models using lmer (package lme4)
>>>> and I'd like to be able to compile a list of the AIC scores for those
>>>> models so that I can quickly summarize and rank the models by AIC. When I
>>>> do logistic regression, I can easily generate this kind of list by
>>>> creating
>>>> the model objects using glm, and doing:
>>>>
>>>>> md <- c("md1.lr", "md2.lr", "md3.lr")
>>>>> aic <- c(md1.lr$aic, md2.lr$aic, md3.lr$aic)
>>>>> aic2 <- cbind(md, aic)
>>>> but when I try to extract the AIC score from the model object produced by
>>>> lmer I get:
>>>>
>>>>> md1.lme$aic
>>>> NULL
>>>> Warning message:
>>>> In md1.lme$aic : $ operator not defined for this S4 class, returning NULL
>>>>
>>>> So... How do I query the AIC value out of a mixed model object created by
>>>> lmer?
>>>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> -----
>> David Hewitt
>> Virginia Institute of Marine Science
>> http://www.vims.edu/fish/students/dhewitt/
>> --
>> View this message in context: http://www.nabble.com/How-can-I-extract-the-AIC-score-from-a-mixed-model-object-produced-using-lmer--tp14406832p14419438.html
>> Sent from the R help mailing list archive at Nabble.com.
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Charles.Annis at StatisticalEngineering.com  Sun Dec 30 04:19:20 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sat, 29 Dec 2007 22:19:20 -0500
Subject: [R] non-ascii characters in TclTk
Message-ID: <00e201c84a92$c706a040$6400a8c0@DD4XFW31>

Greetings, R-Helpers:

A year ago or so I was able to use the non-ascii character ? in my TclTk.
(You can type it on Windows as <alt>0226)  I can use something like
expression(hat(a)) elsewhere in R, but I seem to be doing something wrong
when I try that syntax in TclTk.  The widget will run but it displays
something frightening like R-call_lang 019C8480 019A7724 where I expected to
see ?.

Can anyone suggest how to use the a with caret in a TclTk widget?

Thanks.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?


From mi2kelgrum at yahoo.com  Sun Dec 30 07:45:38 2007
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sat, 29 Dec 2007 22:45:38 -0800 (PST)
Subject: [R] Date formats
Message-ID: <25485.18153.qm@web60219.mail.yahoo.com>

Is the following expected behaviour for a date used in
an ifelse function?

> date <- Sys.Date()
> date
[1] "2007-12-30"
> ifelse(TRUE, date-1, date)
[1] 13876
> ifelse(FALSE, date-1, date)
[1] 13877
> ifelse(TRUE, as.character(date-1), date)
[1] "2007-12-29"
> if (TRUE) {date}
[1] "2007-12-30"

It would seem more natural to me if a date produced
the same format in an if and an ifelse function.
Moreover, as far as I can see the ifelse function
consists of hardly anything but two if functions.

Mikkel
 
> sessionInfo()
R version 2.6.1 (2007-11-26) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_Ireland.1252;LC_CTYPE=English_Ireland.1252;LC_MONETARY=English_Ireland.1252;LC_NUMERIC=C;LC_TIME=English_Ireland.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets 
methods   base     



      ____________________________________________________________________________________
Be a better friend, newshound, and


From ggrothendieck at gmail.com  Sun Dec 30 08:13:09 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 30 Dec 2007 02:13:09 -0500
Subject: [R] Date formats
In-Reply-To: <25485.18153.qm@web60219.mail.yahoo.com>
References: <25485.18153.qm@web60219.mail.yahoo.com>
Message-ID: <971536df0712292313q484790e0n8cd92d975994a857@mail.gmail.com>

Read the warning in ?ifelse

On Dec 30, 2007 1:45 AM, Mikkel Grum <mi2kelgrum at yahoo.com> wrote:
> Is the following expected behaviour for a date used in
> an ifelse function?
>
> > date <- Sys.Date()
> > date
> [1] "2007-12-30"
> > ifelse(TRUE, date-1, date)
> [1] 13876
> > ifelse(FALSE, date-1, date)
> [1] 13877
> > ifelse(TRUE, as.character(date-1), date)
> [1] "2007-12-29"
> > if (TRUE) {date}
> [1] "2007-12-30"
>
> It would seem more natural to me if a date produced
> the same format in an if and an ifelse function.
> Moreover, as far as I can see the ifelse function
> consists of hardly anything but two if functions.
>
> Mikkel
>
> > sessionInfo()
> R version 2.6.1 (2007-11-26)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_Ireland.1252;LC_CTYPE=English_Ireland.1252;LC_MONETARY=English_Ireland.1252;LC_NUMERIC=C;LC_TIME=English_Ireland.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets
> methods   base
>
>
>
>      ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From helprhelp at gmail.com  Sun Dec 30 08:17:46 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Sun, 30 Dec 2007 02:17:46 -0500
Subject: [R] Date formats
In-Reply-To: <25485.18153.qm@web60219.mail.yahoo.com>
References: <25485.18153.qm@web60219.mail.yahoo.com>
Message-ID: <cdf817830712292317j502e1bfcw5bf1ffd1867818c3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071230/ba8e6873/attachment.pl 

From p.dalgaard at biostat.ku.dk  Sun Dec 30 11:09:46 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sun, 30 Dec 2007 11:09:46 +0100
Subject: [R] non-ascii characters in TclTk
In-Reply-To: <00e201c84a92$c706a040$6400a8c0@DD4XFW31>
References: <00e201c84a92$c706a040$6400a8c0@DD4XFW31>
Message-ID: <47776E6A.8020209@biostat.ku.dk>

Charles Annis, P.E. wrote:
> Greetings, R-Helpers:
>
> A year ago or so I was able to use the non-ascii character ? in my TclTk.
> (You can type it on Windows as <alt>0226)  I can use something like
> expression(hat(a)) elsewhere in R, but I seem to be doing something wrong
> when I try that syntax in TclTk.  The widget will run but it displays
> something frightening like R-call_lang 019C8480 019A7724 where I expected to
> see ?.
>
>   
Nothing frightening or unexpectable about that.... If you pass an 
expression, Tcl will think it is a callback. You shouldn't expect 
plotmath features to work outside of plots.

> Can anyone suggest how to use the a with caret in a TclTk widget?
>   
Can't you just type it in inside a string? Otherwise, Unicode escapes 
like "\u00e2" should do it.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p.dalgaard at biostat.ku.dk  Sun Dec 30 11:47:26 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sun, 30 Dec 2007 11:47:26 +0100
Subject: [R] Date formats
In-Reply-To: <971536df0712292313q484790e0n8cd92d975994a857@mail.gmail.com>
References: <25485.18153.qm@web60219.mail.yahoo.com>
	<971536df0712292313q484790e0n8cd92d975994a857@mail.gmail.com>
Message-ID: <4777773E.8040401@biostat.ku.dk>

Gabor Grothendieck wrote:
> Read the warning in ?ifelse
Yep.

And, yes, it is annoying that ifelse() strips attributes, including 
class, but it is one of those things that have been in the S languages 
"forever", and nobody really wants to  mess with. The fundamental issue 
is that you need the result to be able to hold values from both of the 
"yes" and the "no" arguments and there is no guarantee that that is 
possible outside of the R base types.

You'd like to have things like these "work"

d <- as.Date(c("1994-3-4", "1996-3-1"))

ifelse(d > "1996-1-1", "1996-1-1", d)

ifelse(d <= "1996-1-1", d, "1996-1-1")

in the sense that the result is a Date object, but once you start 
thinking about the details of how it _might_ work, you find that things 
aren't all that simple. If there was a general mechanism for coercion 
between classes, then maybe it could be done, but there isn't any.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Charles.Annis at StatisticalEngineering.com  Sun Dec 30 12:53:15 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sun, 30 Dec 2007 06:53:15 -0500
Subject: [R] non-ascii characters in TclTk
In-Reply-To: <47776E6A.8020209@biostat.ku.dk>
References: <00e201c84a92$c706a040$6400a8c0@DD4XFW31>
	<47776E6A.8020209@biostat.ku.dk>
Message-ID: <00ef01c84ada$9056e940$6400a8c0@DD4XFW31>

Thank you, Peter!

The Unicode sequence "\u00e2" for "?" is the ticket.  

While it is true that I can just use ? in my TclTk code, when I use
package.skeleton(name="unicode test"), the R code changes my unacceptable ?
to <e2> to indicate an error.  My code will still run, of course, but
doesn't display as expected (or does display as expected, depending on your
expectations).  ~:-)

Your Unicode suggestion fixes all that.  The package.skeleton() likes the
escape sequence and the resulting TclTk widget displays the ? as desired.

Thank you!

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: Sunday, December 30, 2007 5:10 AM
To: Charles.Annis at statisticalengineering.com
Cc: 'RHelp'
Subject: Re: [R] non-ascii characters in TclTk

Charles Annis, P.E. wrote:
> Greetings, R-Helpers:
>
> A year ago or so I was able to use the non-ascii character ? in my TclTk.
> (You can type it on Windows as <alt>0226)  I can use something like
> expression(hat(a)) elsewhere in R, but I seem to be doing something wrong
> when I try that syntax in TclTk.  The widget will run but it displays
> something frightening like R-call_lang 019C8480 019A7724 where I expected
to
> see ?.
>
>   
Nothing frightening or unexpectable about that.... If you pass an 
expression, Tcl will think it is a callback. You shouldn't expect 
plotmath features to work outside of plots.

> Can anyone suggest how to use the a with caret in a TclTk widget?
>   
Can't you just type it in inside a string? Otherwise, Unicode escapes 
like "\u00e2" should do it.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From h.wickham at gmail.com  Sun Dec 30 14:04:51 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 30 Dec 2007 13:04:51 +0000
Subject: [R] Date formats
In-Reply-To: <4777773E.8040401@biostat.ku.dk>
References: <25485.18153.qm@web60219.mail.yahoo.com>
	<971536df0712292313q484790e0n8cd92d975994a857@mail.gmail.com>
	<4777773E.8040401@biostat.ku.dk>
Message-ID: <f8e6ff050712300504t1f264811i2b6cc34342afe5b1@mail.gmail.com>

On Dec 30, 2007 10:47 AM, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Gabor Grothendieck wrote:
> > Read the warning in ?ifelse
> Yep.
>
> And, yes, it is annoying that ifelse() strips attributes, including
> class, but it is one of those things that have been in the S languages
> "forever", and nobody really wants to  mess with. The fundamental issue
> is that you need the result to be able to hold values from both of the
> "yes" and the "no" arguments and there is no guarantee that that is
> possible outside of the R base types.
>
> You'd like to have things like these "work"
>
> d <- as.Date(c("1994-3-4", "1996-3-1"))
>
> ifelse(d > "1996-1-1", "1996-1-1", d)
>
> ifelse(d <= "1996-1-1", d, "1996-1-1")
>
> in the sense that the result is a Date object, but once you start
> thinking about the details of how it _might_ work, you find that things
> aren't all that simple. If there was a general mechanism for coercion
> between classes, then maybe it could be done, but there isn't any.

But wouldn't it be better/simpler to say that the yes and no arguments
should be of the same "type"? (I realise that this would be difficult
to enforce in R).  In what cases is it actually sensible/useful to
have different types in the yes and no arguments?

Wrt the dates, I have always wondered if it would be nice to have a
date primitive - e.g. in microsoft access sql, #1996-1-1# is a date
(vs "1996-1-1" the string).  (Although access uses your system dates
settings to determine whether #1/4/07# is 4 Jan or 1 Mar, which makes
portability a real pain)

Hadley

-- 
http://had.co.nz/


From herrdittmann at yahoo.co.uk  Sun Dec 30 15:58:41 2007
From: herrdittmann at yahoo.co.uk (Bernd Dittmann)
Date: Sun, 30 Dec 2007 14:58:41 +0000
Subject: [R] apply in zoo
Message-ID: <4777B221.7050706@yahoo.co.uk>

Hi R users,

could anyone guide me in the right direction reg. the column-wise 
application of a function on a zoo dataframe.

The tseries function sharpe() can only be applied to a univariate time 
series. Suppose you have merged the returns of two assets with 
get.hist.quote(), i.e.

set <- merge(log(ibm), log(ge))

is it possible to apply the function sharpe(), in this case with the 
argument

scale=sqrt(52)

since I am using weekly close prices

to the dataset "set" column per column?

I tried apply() with no result:

 > apply(set, 2, sharpe(set, scale=sqrt(52)))
Fehler in sharpe(zz) : x is not a vector or univariate time series

In the case of two securities, I could easily do it by hand, but for say 
100 or even 500, I am looking for an automated solution.


Many thanks in advance!

Bernd


From ggrothendieck at gmail.com  Sun Dec 30 16:21:19 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 30 Dec 2007 10:21:19 -0500
Subject: [R] apply in zoo
In-Reply-To: <4777B221.7050706@yahoo.co.uk>
References: <4777B221.7050706@yahoo.co.uk>
Message-ID: <971536df0712300721w4691dd78h59546602eb878b51@mail.gmail.com>

1. Please read the last line on every message to r-help and in particular
the part about providing reproducible code when you post.

2. Your third argument to apply is wrong.  sharpe is a function but
sharpe(...whatever...) is not.  Suggest you carefully review the
examples on the ?apply page if the difference is not clear.

On Dec 30, 2007 9:58 AM, Bernd Dittmann <herrdittmann at yahoo.co.uk> wrote:
> Hi R users,
>
> could anyone guide me in the right direction reg. the column-wise
> application of a function on a zoo dataframe.
>
> The tseries function sharpe() can only be applied to a univariate time
> series. Suppose you have merged the returns of two assets with
> get.hist.quote(), i.e.
>
> set <- merge(log(ibm), log(ge))
>
> is it possible to apply the function sharpe(), in this case with the
> argument
>
> scale=sqrt(52)
>
> since I am using weekly close prices
>
> to the dataset "set" column per column?
>
> I tried apply() with no result:
>
>  > apply(set, 2, sharpe(set, scale=sqrt(52)))
> Fehler in sharpe(zz) : x is not a vector or univariate time series
>
> In the case of two securities, I could easily do it by hand, but for say
> 100 or even 500, I am looking for an automated solution.
>
>
> Many thanks in advance!
>
> Bernd
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Lindsay.Stirton at manchester.ac.uk  Sun Dec 30 17:27:11 2007
From: Lindsay.Stirton at manchester.ac.uk (Lindsay Stirton)
Date: Sun, 30 Dec 2007 16:27:11 +0000
Subject: [R] Trying to install rjags on Mac OS X 10.5
Message-ID: <20071230162711.s4q4h4pkg04o8c84@webmail.manchester.ac.uk>

Greetings,

I wonder if anyone can offer any help or advice--even direction to an
appropriate source of advice. I am trying to install rjags 1.0.1 on Mac OS X
10.5 (see http://www-fis.iarc.fr/~martyn/software/jags/). I have R.app 2.6.1
installed.

JAGS 1.0.1 is apparently successfully installed. I (think I) know this because
when I type 'jags' into Terminal, I get the following:

Macintosh:Desktop ljs$ jags
Welcome to JAGS 1.0.1 on Thu Dec 27 20:56:09 2007
JAGS is free software and comes with ABSOLUTELY NO WARRANTY
Loading module: basemod
Loading module: bugs
.

However, when I try to install rjags, I get the following problem.

Macintosh:Desktop ljs$ sudo R CMD INSTALL rjags_1.0.1-1.tar.gz
Password:
* Installing to library '/Library/Frameworks/R.framework/Resources/library'
* Installing *source* package 'rjags' ...
checking for pkg-config... /opt/local/bin/pkg-config
checking pkg-config is at least version 0.9.0... yes
checking for JAGS... configure: error: Package requirements (jags = 1.0.1) were
not met:

No package 'jags' found

Consider adjusting the PKG_CONFIG_PATH environment variable if you
installed software in a non-standard prefix.

ERROR: configuration failed for package 'rjags'
** Removing '/Library/Frameworks/R.framework/Resources/library/rjags'

I am really not sure what the problem is. I have (I think) set the proper
environment variables correctly. See below:

Macintosh:Desktop ljs$ which jags
/usr/local/bin/jags
Macintosh:Desktop ljs$ echo $PKG_CONFIG_PATH
/usr/local/bin

I should say that I had many other problems
along the way--gfortran not originally installed properly, R CMD INSTALL
couldn't find pkg-config. However, the above description shows the problems
remaining even after I have done a little homework.

Best wishes,

Lindsay Stirton


From retzerjj at gmail.com  Sun Dec 30 17:31:50 2007
From: retzerjj at gmail.com (Retzer Joe)
Date: Sun, 30 Dec 2007 10:31:50 -0600
Subject: [R] randomForest: Graph a Single Tree extracted from a random forest
Message-ID: <E1BBCBAC-EAF0-4941-8F68-2156955F73B9@gmail.com>

I was wondering if anyone had created a graph of a single tree created  
by the randomForest call. Perhaps obtaining the tree information using  
"getTree" in the randomForest package.

Many thanks for any advice,
Joe Retzer


From r.mueller at oeko-sorpe.de  Sun Dec 30 17:51:33 2007
From: r.mueller at oeko-sorpe.de (Richard =?iso-8859-1?q?M=FCller?=)
Date: Sun, 30 Dec 2007 17:51:33 +0100
Subject: [R] tcltk again
In-Reply-To: <5896D18E-3F3D-4CB5-8D11-29BB5DB00D1C@gmail.com>
References: <200712291817.42974.r.mueller@oeko-sorpe.de>
	<5896D18E-3F3D-4CB5-8D11-29BB5DB00D1C@gmail.com>
Message-ID: <200712301751.33237.r.mueller@oeko-sorpe.de>

> > ...
> > I experimented with tkgetSaveFile(). But in the moment the user
> > gives the
> > filename the file is not yet generated! And I can't generate it in
> > advance,
> > because the filename is not yet specified.
>
> I don't see why this is a problem. In tkgetSaveFile(), the user can
> just navigate to the correct directory and then type a filename in.
> Then that would be returned. The following worked over here:
>
> x<-tkgetSaveFile()
> #.. go to right directory and type in filename
> tclvalue(x)
Thank you, Charilaos, for your posting.
I wrote the following:
--------------------------------------------------
require(tcltk)
#Tcl/Tk - Texteingabe
Datei <- tkgetSaveFile(initialdir="temp/",defaultextension=".pdf",
initialfile="Haupt_Chl_ Phaeo.pdf")
fileName <- tclvalue(Datei)
pdf(fileName) # graphical device driver with filename
...(code for the graphics)...
---------------------------------------------------
As expected, I get an empty pdf-file. Specifying the pdf-device-driver results 
in automatically saving the generated pdf. The tkGetSave-function isn't 
helpful in this case. I must have the possibility to input a filename and 
then give this filename as an argument to the device-driver.
Richard

-- 
Richard M?ller - Am Spring 9 - D-58802 Balve-Eisborn
www.oeko-sorpe.de


From ripley at stats.ox.ac.uk  Sun Dec 30 17:54:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 30 Dec 2007 16:54:23 +0000 (GMT)
Subject: [R] Trying to install rjags on Mac OS X 10.5
In-Reply-To: <20071230162711.s4q4h4pkg04o8c84@webmail.manchester.ac.uk>
References: <20071230162711.s4q4h4pkg04o8c84@webmail.manchester.ac.uk>
Message-ID: <Pine.LNX.4.64.0712301649420.30788@gannet.stats.ox.ac.uk>

> Macintosh:Desktop ljs$ echo $PKG_CONFIG_PATH
> /usr/local/bin

is unlikely to be correct.  A typical Linux setting is

PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig:/usr/lib64/pkgconfig

and on my Mac the pkg-config files are in /usr/local/lib/pkgconfig and 
/usr/lib/pkgconfig.  Most likely it would work if PKG_CONFIG_PATH is not 
set, so try that first.


On Sun, 30 Dec 2007, Lindsay Stirton wrote:

> Greetings,
>
> I wonder if anyone can offer any help or advice--even direction to an
> appropriate source of advice. I am trying to install rjags 1.0.1 on Mac OS X
> 10.5 (see http://www-fis.iarc.fr/~martyn/software/jags/). I have R.app 2.6.1
> installed.
>
> JAGS 1.0.1 is apparently successfully installed. I (think I) know this because
> when I type 'jags' into Terminal, I get the following:
>
> Macintosh:Desktop ljs$ jags
> Welcome to JAGS 1.0.1 on Thu Dec 27 20:56:09 2007
> JAGS is free software and comes with ABSOLUTELY NO WARRANTY
> Loading module: basemod
> Loading module: bugs
> .
>
> However, when I try to install rjags, I get the following problem.
>
> Macintosh:Desktop ljs$ sudo R CMD INSTALL rjags_1.0.1-1.tar.gz
> Password:
> * Installing to library '/Library/Frameworks/R.framework/Resources/library'
> * Installing *source* package 'rjags' ...
> checking for pkg-config... /opt/local/bin/pkg-config
> checking pkg-config is at least version 0.9.0... yes
> checking for JAGS... configure: error: Package requirements (jags = 1.0.1) were
> not met:
>
> No package 'jags' found
>
> Consider adjusting the PKG_CONFIG_PATH environment variable if you
> installed software in a non-standard prefix.
>
> ERROR: configuration failed for package 'rjags'
> ** Removing '/Library/Frameworks/R.framework/Resources/library/rjags'
>
> I am really not sure what the problem is. I have (I think) set the proper
> environment variables correctly. See below:
>
> Macintosh:Desktop ljs$ which jags
> /usr/local/bin/jags
> Macintosh:Desktop ljs$ echo $PKG_CONFIG_PATH
> /usr/local/bin
>
> I should say that I had many other problems
> along the way--gfortran not originally installed properly, R CMD INSTALL
> couldn't find pkg-config. However, the above description shows the problems
> remaining even after I have done a little homework.
>
> Best wishes,
>
> Lindsay Stirton
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From r.mueller at oeko-sorpe.de  Sun Dec 30 17:55:33 2007
From: r.mueller at oeko-sorpe.de (Richard =?iso-8859-1?q?M=FCller?=)
Date: Sun, 30 Dec 2007 17:55:33 +0100
Subject: [R] tcltk again
In-Reply-To: <B3F75955-3AF3-4899-95AE-88E74FC3B8E1@gmail.com>
References: <200712291817.42974.r.mueller@oeko-sorpe.de>
	<200712292124.05887.r.mueller@oeko-sorpe.de>
	<B3F75955-3AF3-4899-95AE-88E74FC3B8E1@gmail.com>
Message-ID: <200712301755.33849.r.mueller@oeko-sorpe.de>

Oops, I just sent the wrong mail. It should be the following one. Please 
delete my mail from 30.Dez. 17:51


Sorry, but I don't really understand the recommended method using the tk-Box 
"tkGetSaveFile".
I wrote the following code:
X11()
# some code to generate a plot on the screen omitted

res <-    tkmessageBox(title="Finish?", 
      message="save as PDF?", 
      icon="question", type="okcancel")

if (tclvalue(res) == "ok") 
    Datei <- tkgetSaveFile(initialdir="temp/",defaultextension=".pdf",
    initialfile="Haupt_Chl_Phaeo.pdf")
    dev.copy(pdf, tclvalue(Datei))
else graphics.off()
graphics.off()

In the moment I'm saving I dont have the pdf generated yet. If I put it the 
other way round:
dev.copy(pdf, tclvalue(Datei))
if (tclvalue(res) == "ok") 
    Datei <- tkgetSaveFile(initialdir="temp/",defaultextension=".pdf",
    initialfile="Haupt_Chl_Phaeo.pdf")
I don't have the filename specified when it is needed.
(But today I could finish the Windows versions at last, using WinDialog ;-) 

Greetings
Richard
   
-- 
Richard M?ller - Am Spring 9 - D-58802 Balve-Eisborn
www.oeko-sorpe.de


From cskiadas at gmail.com  Sun Dec 30 18:19:01 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Sun, 30 Dec 2007 12:19:01 -0500
Subject: [R] tcltk again
In-Reply-To: <200712301755.33849.r.mueller@oeko-sorpe.de>
References: <200712291817.42974.r.mueller@oeko-sorpe.de>
	<200712292124.05887.r.mueller@oeko-sorpe.de>
	<B3F75955-3AF3-4899-95AE-88E74FC3B8E1@gmail.com>
	<200712301755.33849.r.mueller@oeko-sorpe.de>
Message-ID: <5D482020-E0A7-4A12-B9E3-785B0029444D@gmail.com>

On Dec 30, 2007, at 11:55 AM, Richard M?ller wrote:

> Oops, I just sent the wrong mail. It should be the following one.  
> Please
> delete my mail from 30.Dez. 17:51
>
>
> Sorry, but I don't really understand the recommended method using  
> the tk-Box
> "tkGetSaveFile".
> I wrote the following code:
> X11()
> # some code to generate a plot on the screen omitted
>
> res <-    tkmessageBox(title="Finish?",
>       message="save as PDF?",
>       icon="question", type="okcancel")
>
> if (tclvalue(res) == "ok")
>     Datei <- tkgetSaveFile(initialdir="temp/",defaultextension=".pdf",
>     initialfile="Haupt_Chl_Phaeo.pdf")
>     dev.copy(pdf, tclvalue(Datei))

You need to specify the arguments using their explicit names, namely:
	dev.copy(pdf,file=tclvaue(Datei))
And that should be followed by closing the pdf device (since dev.copy  
leaves the device open). So I would follow that by
	dev.off()

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College

> else graphics.off()
> graphics.off()
>
> In the moment I'm saving I dont have the pdf generated yet. If I  
> put it the
> other way round:
> dev.copy(pdf, tclvalue(Datei))
> if (tclvalue(res) == "ok")
>     Datei <- tkgetSaveFile(initialdir="temp/",defaultextension=".pdf",
>     initialfile="Haupt_Chl_Phaeo.pdf")
> I don't have the filename specified when it is needed.
> (But today I could finish the Windows versions at last, using  
> WinDialog ;-)
>
> Greetings
> Richard
>
> -- 
> Richard M?ller - Am Spring 9 - D-58802 Balve-Eisborn
> www.oeko-sorpe.de


From francogrex at mail.com  Sun Dec 30 18:25:10 2007
From: francogrex at mail.com (francogrex)
Date: Sun, 30 Dec 2007 09:25:10 -0800 (PST)
Subject: [R]  Bayesian Integration (stat question)
Message-ID: <14549095.post@talk.nabble.com>


This may be far-fetched:
In Bayesian analysis to find the marginal posterior distribution of a
parameter it requires integration out of the so-called nuisance parameters.
Is there in R (like an equivalent to the deriv function) but to find the
expression of the integration (or an anti-derivative)? Like for example I
want to integrate out mu in the normal distribution, I would introduce
(note:punctuation marks are made up):
integral(prod[i to n] 1/sqrt(2*pi*s^2)*exp(-(xi-mu)/2*s^2), dmu)
and it would give me:
1/(sqrt(n)*(2*pi*s^2)^((n-1)/2))*exp(-sum[i to n]((xi-xbar)^2)/2*s^2)
?
-- 
View this message in context: http://www.nabble.com/Bayesian-Integration-%28stat-question%29-tp14549095p14549095.html
Sent from the R help mailing list archive at Nabble.com.


From dan.oshea at dnr.state.mn.us  Sun Dec 30 18:49:26 2007
From: dan.oshea at dnr.state.mn.us (Daniel O'Shea)
Date: Sun, 30 Dec 2007 11:49:26 -0600
Subject: [R] refering to variable names in lm where the variable name is
	in	another variable
Message-ID: <477785C60200005A00009600@co5.dnr.state.mn.us>

I am trying to refer to a variable name in a lm regression where the variable name is in another variable, but it does seem to work.  Here is an example:

y<-rnorm(10)
dat<-data.frame(x1=rnorm(10),x2=rnorm(10),x3=rnorm(10))
nam<-c('x1','x2','x3')
library(gtools)
com<-combinations(3,2,1:3)
mod<-lm(y~nam[com[1,1]],data=dat)

#error in model frame....:variable lengths differ().

I also get the error if i just refer to variable x1 as nam[1] in the lm. any suggestions.  I am trying to set up a for loop that will perform an all subsets regression and calculate the AIC for each.

Dan


From rv15i at yahoo.se  Sun Dec 30 18:50:37 2007
From: rv15i at yahoo.se (Ravi Vishnu)
Date: Sun, 30 Dec 2007 18:50:37 +0100 (CET)
Subject: [R] Installing Rgraphviz package on a windows vista pc
Message-ID: <735686.71425.qm@web26614.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071230/3b917c80/attachment.pl 

From arnholtat at appstate.edu  Sun Dec 30 18:57:38 2007
From: arnholtat at appstate.edu (arnholtat at appstate.edu)
Date: Sun, 30 Dec 2007 17:57:38 GMT
Subject: [R] Histogram with different colors for different portions
Message-ID: <fc8cc0912fb6.4777dc12@appstate.edu>

Dear Rusers,
I would like to color different sections of a histogram different colors.
I have an example that was done by "brute force" given below.  Has anyone
implemented something like this in general?  If not, any suggestions/pointers
on how to write a general function to do so would be most appreciated.
Alan-
################################################################################
set.seed(13)
nnum <- rnorm(1000, 100, 10)
xbar <- mean(nnum)
SD <- sd(nnum)
BR <- seq(xbar - 4*SD, xbar + 4*SD, by = .25*SD)
# Histogram showing xbar +- sd in different colors
hist(nnum, breaks=BR, col=c(rep("red", 8), rep("blue", 4),
     rep("pink", 8), rep("blue", 4), rep("red", 8)))
################################################################################
# Histogram depicting Hinge Spread with boxplot
layout(matrix(c(1, 2)), heights=c(2, 1))
fn <- fivenum(nnum)
LH <- fn[2]
UH <- fn[4]
HS <- UH - LH
SHS <- HS/10
BR <- seq(LH - SHS*30, UH + SHS*30, SHS)
par(mar=c(0,2,3,2))
hist(nnum, breaks=BR, col=c(rep("red", 30), rep("blue", 10),
     rep("red", 30)), xlim=c(60,140), axes=FALSE, xlab="",
     ylab="", main="")
#
par(mar=c(3,2,0,2))
boxplot(nnum, col="blue", horizontal=TRUE, ylim=c(60, 140), axes=FALSE)
axis(side=1)

Alan T. Arnholt 
Professor and Assistant Chair 
Department of Mathematical Sciences 
Appalachian State University 
T:(828)262-2863 
F:(828)265-8617 
http://www1.appstate.edu/~arnholta/ 

From Lindsay.Stirton at manchester.ac.uk  Sun Dec 30 18:56:11 2007
From: Lindsay.Stirton at manchester.ac.uk (Lindsay Stirton)
Date: Sun, 30 Dec 2007 17:56:11 +0000
Subject: [R] Trying to install rjags on Mac OS X 10.5
In-Reply-To: <Pine.LNX.4.64.0712301649420.30788@gannet.stats.ox.ac.uk>
References: <20071230162711.s4q4h4pkg04o8c84@webmail.manchester.ac.uk>
	<Pine.LNX.4.64.0712301649420.30788@gannet.stats.ox.ac.uk>
Message-ID: <20071230175611.yhhn5stjk8gs0c8k@webmail.manchester.ac.uk>

Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:

> and on my Mac the pkg-config files are in /usr/local/lib/pkgconfig 
> and /usr/lib/pkgconfig.  Most likely it would work if PKG_CONFIG_PATH 
> is not set, so try that first.

Thanks--but this still doesn't seem to get me anywhere (see below).

Macintosh:Desktop ljs$ which jags
/usr/local/bin/jags
Macintosh:Desktop ljs$ whereis jags
Macintosh:Desktop ljs$


Last login: Sun Dec 30 17:40:41 on ttys001
Macintosh:~ ljs$ echo $PKG_CONFIG_PATH

Macintosh:~ ljs$ cd Desktop/
Macintosh:Desktop ljs$ sudo R CMD INSTALL rjags_1.0.1-1.tar.gz
* Installing to library '/Library/Frameworks/R.framework/Resources/library'
* Installing *source* package 'rjags' ...
checking for pkg-config... /opt/local/bin/pkg-config
checking pkg-config is at least version 0.9.0... yes
checking for JAGS... configure: error: Package requirements (jags = 
1.0.1) were
not met:

No package 'jags' found

Consider adjusting the PKG_CONFIG_PATH environment variable if you
installed software in a non-standard prefix.

ERROR: configuration failed for package 'rjags'
** Removing '/Library/Frameworks/R.framework/Resources/library/rjags'
Macintosh:Desktop ljs$

Would I be right in thinking that the problem is not in fiding the path to
pkg-config but in finding JAGS? I have

Macintosh:Desktop ljs$ which jags
/usr/local/bin/jags


Best wishes,

Lindsay Stirton


From ba208 at exeter.ac.uk  Sun Dec 30 19:19:40 2007
From: ba208 at exeter.ac.uk (=?ISO-8859-1?Q?baptiste_Augui=E9?=)
Date: Sun, 30 Dec 2007 18:19:40 +0000
Subject: [R] adding a function after package.skeleton()
Message-ID: <878BE177-8F3F-409F-B32B-6B4A7BDFA339@ex.ac.uk>

Dear R helpers,

I've successfully created a package 'constants' using package.skeleton 
() with one dataframe and a few functions. However, now that I want  
to add some functions and data to the package, I run into a problem.

I ran prompt(...) and moved + edited the resulting .Rd files as  
appropriate (I believe). The log file from RCMD check constants does  
indicate a few problems (full log below).

As far as I understand, the real problem would be on the lines:

> * checking R code for possible problems ... NOTE
> L2eV: no visible binding for global variable 'Constants'
> eV2L: no visible binding for global variable 'Constants'


These two functions use the dataframe "Constants", part of this package:

> `L2eV` <- function(lambda)
> {
> 	data("Constants")
> 	Constants$h*Constants$cel/Constants$ee/lambda ->eV
> 	eV
> 	}


and

> `eV2L` <- function(eV)
> {
> 	data("Constants")
> 	Constants$h*Constants$cel/Constants$ee/eV ->Lambda
> 	Lambda
> 	}


After searching the R archives about "no visible binding for global  
variable ", I added the quotes around "Constants" but it doesn't seem  
to help. What would be the correct way to use this data inside the  
package?

Best regards,

baptiste

----------------
Log of R CMD check constants :


>
> baptiste-auguies-ibook-g4:~ baptiste$ R CMD check constants
> * checking for working latex ... OK
> * using log directory '/Users/baptiste/constants.Rcheck'
> * using R version 2.6.1 (2007-11-26)
> * checking for file 'constants/DESCRIPTION' ... OK
> * checking extension type ... Package
> * this is package 'constants' version '1.0'
> * checking package dependencies ... OK
> * checking if this is a source package ... OK
> * checking whether package 'constants' can be installed ... OK
> * checking package directory ... OK
> * checking for portable file names ... OK
> * checking for sufficient/correct file permissions ... OK
> * checking DESCRIPTION meta-information ... OK
> * checking top-level files ... OK
> * checking index information ... OK
> * checking package subdirectories ... OK
> * checking R files for non-ASCII characters ... OK
> * checking R files for syntax errors ... OK
> * checking whether the package can be loaded ... OK
> * checking whether the package can be loaded with stated  
> dependencies ... OK
> * checking for unstated dependencies in R code ... OK
> * checking S3 generic/method consistency ... OK
> * checking replacement functions ... OK
> * checking foreign function calls ... OK
> * checking R code for possible problems ... NOTE
> L2eV: no visible binding for global variable 'Constants'
> eV2L: no visible binding for global variable 'Constants'
> * checking Rd files ... WARNING
> Rd files with non-standard keywords:
>   L2eV.Rd: kwd1 kwd2
>   constants-package.Rd: physical constants optical
>   delete.all.Rd: kwd1 kwd2
>   eV2L.Rd: kwd1 kwd2
>   epsilon2nk.Rd: kwd1 kwd2
>   fano.Rd: kwd1 kwd2
>   lorentz.Rd: kwd1 kwd2
>   nk2epsilon.Rd: kwd1 kwd2
> Each '\keyword' entry should specify one of the standard keywords (as
> listed in file 'KEYWORDS' in the R documentation directory).
>
> See the chapter 'Writing R documentation files' in manual 'Writing R
> Extensions'.
> * checking Rd cross-references ... OK
> * checking for missing documentation entries ... OK
> * checking for code/documentation mismatches ... OK
> * checking Rd \usage sections ... OK
> * checking data for non-ASCII characters ... OK
> * creating constants-Ex.R ... OK
> * checking examples ... OK
> * creating constants-manual.tex ... OK
> * checking constants-manual.tex ... OK
>
> WARNING: There was 1 warning, see
>   /Users/baptiste/constants.Rcheck/00check.log
> for details



_____________________________

Baptiste Augui?

Physics Department
University of Exeter
Stocker Road,
Exeter, Devon,
EX4 4QL, UK

Phone: +44 1392 264187

http://newton.ex.ac.uk/research/emag
http://projects.ex.ac.uk/atto


From cskiadas at gmail.com  Sun Dec 30 19:22:52 2007
From: cskiadas at gmail.com (Charilaos Skiadas)
Date: Sun, 30 Dec 2007 13:22:52 -0500
Subject: [R] refering to variable names in lm where the variable name is
	in	another variable
In-Reply-To: <477785C60200005A00009600@co5.dnr.state.mn.us>
References: <477785C60200005A00009600@co5.dnr.state.mn.us>
Message-ID: <1323C474-2C6E-4C0B-A75D-84A83433FF82@gmail.com>

On Dec 30, 2007, at 12:49 PM, Daniel O'Shea wrote:

> I am trying to refer to a variable name in a lm regression where  
> the variable name is in another variable, but it does seem to  
> work.  Here is an example:
>
> y<-rnorm(10)
> dat<-data.frame(x1=rnorm(10),x2=rnorm(10),x3=rnorm(10))
> nam<-c('x1','x2','x3')
> library(gtools)
> com<-combinations(3,2,1:3)
> mod<-lm(y~nam[com[1,1]],data=dat)
>
> #error in model frame....:variable lengths differ().
>
> I also get the error if i just refer to variable x1 as nam[1] in  
> the lm. any suggestions.  I am trying to set up a for loop that  
> will perform an all subsets regression and calculate the AIC for each.

There's probably a number of ways to go about it. The problem is that  
nam[1] is a string vector, while you want the underlying "symbol". In  
your case, I think the simplest solution would be:

frm <- formula(paste("y~",nam[1]))
mod<-lm(frm,data=dat)

This would also work:

frm <- bquote(y~.(var1), list(var1=as.name(nam[1])))
mod<-lm(frm,data=dat)

This however does have a possible sideeffect, as in the following:
frm <- bquote(y~.(var1), list(var1=as.name(nam[2])))
mod2 <- update(mod)

mod2 is now the model for x2, even though we didn't give it an  
explicit new formula (The old formula had kept the difference in frm.

You can probably avoid this by:
mod<-lm(bquote(y~.(var1), list(var1=as.name(nam[1]))),data=dat)

Though again the name of the formula is not very pretty. The best  
one, from the point of view of getting the correct call in lm,  
probably is:

eval(bquote(lm(y~.(var1), data=dat), list(var1=as.name(nam[1]))))

> Dan

Hope this helps,
Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From ggrothendieck at gmail.com  Sun Dec 30 19:27:43 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 30 Dec 2007 13:27:43 -0500
Subject: [R] adding a function after package.skeleton()
In-Reply-To: <878BE177-8F3F-409F-B32B-6B4A7BDFA339@ex.ac.uk>
References: <878BE177-8F3F-409F-B32B-6B4A7BDFA339@ex.ac.uk>
Message-ID: <971536df0712301027q27b8970ew60ac648c0cbc38cb@mail.gmail.com>

On Dec 30, 2007 1:19 PM, baptiste Augui? <ba208 at exeter.ac.uk> wrote:

> These two functions use the dataframe "Constants", part of this package:
>
> > `L2eV` <- function(lambda)
> > {
> >       data("Constants")
> >       Constants$h*Constants$cel/Constants$ee/lambda ->eV
> >       eV
> >       }
>

This does not answer your question but note that you can do this:

   with(Constants, h * cel / ee / lambda)


From milton_ruser at yahoo.com.br  Sun Dec 30 19:45:05 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sun, 30 Dec 2007 10:45:05 -0800 (PST)
Subject: [R] updating R version and packages.
Message-ID: <486910.42845.qm@web56009.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071230/0d82c1cc/attachment.pl 

From mi2kelgrum at yahoo.com  Sun Dec 30 20:07:12 2007
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sun, 30 Dec 2007 11:07:12 -0800 (PST)
Subject: [R] Date formats
Message-ID: <47818.19020.qm@web60220.mail.yahoo.com>

Thanks to both of you. I tried to work on it, but the closest I could get was:

lifelse <- function (test, y, z) {
    iffy <- function(x) {
        if (x) {y} else {z}
    }
    lapply(test, iffy)
}

> lifelse(c(TRUE, FALSE), date -1, date)
[[1]]
[1] "2007-12-29"

[[2]]
[1] "2007-12-30"

> 
> d <- as.Date(c("1994-3-4", "1996-3-1"))
> lifelse(d > "1996-1-1", "1996-1-1", d)
[[1]]
[1] "1994-03-04" "1996-03-01"

[[2]]
[1] "1996-1-1"

> lifelse(d <= "1996-1-1", d, "1996-1-1")
[[1]]
[1] "1994-03-04" "1996-03-01"

[[2]]
[1] "1996-1-1"

Any attempts to unlist, paste, etc. to remove the list structure converted/removed the Date class.

Mikkel

----- Original Message ----
From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
To: Gabor Grothendieck <ggrothendieck at gmail.com>
Cc: Mikkel Grum <mi2kelgrum at yahoo.com>; r-help at stat.math.ethz.ch
Sent: Sunday, December 30, 2007 1:47:26 PM
Subject: Re: [R] Date formats

 Gabor Grothendieck wrote:
> Read the warning in ?ifelse
Yep.

And, yes, it is annoying that ifelse() strips attributes, including 
class, but it is one of those things that have been in the S languages 
"forever", and nobody really wants to  mess with. The fundamental issue  
is that you need the result to be able to hold values from both of the 
"yes" and the "no" arguments and there is no guarantee that that is 
possible outside of the R base types.

You'd like to have things like these "work"

d <- as.Date(c("1994-3-4", "1996-3-1"))

ifelse(d > "1996-1-1", "1996-1-1", d)

ifelse(d <= "1996-1-1", d, "1996-1-1")

in the sense that the result is a Date object, but once you start 
thinking about the details of how it _might_ work, you find that things  
aren't all that simple. If there was a general mechanism for coercion 
between classes, then maybe it could be done, but there isn't any.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)  35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)  35327907



      ____________________________________________________________________________________
Be a better friend, newshound, and 
know-it-all with Yahoo! Mobile.  Try it now.  http://mobile.yahoo.com/;_ylt=Ahu06i62sR8HDtDypao8Wcj9tAcJ 



From francogrex at mail.com  Sun Dec 30 20:16:14 2007
From: francogrex at mail.com (francogrex)
Date: Sun, 30 Dec 2007 11:16:14 -0800 (PST)
Subject: [R] Bayesian Integration (stat question)
In-Reply-To: <14549095.post@talk.nabble.com>
References: <14549095.post@talk.nabble.com>
Message-ID: <14550219.post@talk.nabble.com>


Yacas and Ryacas does exactly what I want. Actually I had asked previously a
question about algebra (not integration precisely) and guys here have
pointed out package Ryacas. Sorry to disturb.


francogrex wrote:
> 
> This may be far-fetched:
> In Bayesian analysis to find the marginal posterior distribution of a
> parameter it requires integration out of the so-called nuisance
> parameters. Is there in R (like an equivalent to the deriv function) but
> to find the expression of the integration (or an anti-derivative)? Like
> for example I want to integrate out mu in the normal distribution, I would
> introduce (note:punctuation marks are made up):
> integral(prod[i to n] 1/sqrt(2*pi*s^2)*exp(-(xi-mu)/2*s^2), dmu)
> and it would give me:
> 1/(sqrt(n)*(2*pi*s^2)^((n-1)/2))*exp(-sum[i to n]((xi-xbar)^2)/2*s^2)
> ?
> 

-- 
View this message in context: http://www.nabble.com/Bayesian-Integration-%28stat-question%29-tp14549095p14550219.html
Sent from the R help mailing list archive at Nabble.com.


From r_stat_solutions at hotmail.es  Sun Dec 30 20:45:48 2007
From: r_stat_solutions at hotmail.es (_Fede_)
Date: Sun, 30 Dec 2007 11:45:48 -0800 (PST)
Subject: [R]  Bootstrap Confidence Intervals
Message-ID: <14550471.post@talk.nabble.com>



Hi all.

This is my first post in this forum. Finally I find a forum in the web about
R, although is not in my language. 

Now I'm working with Bootstrap CI. I'd like to know how I can calculate a
Bootstrap CI for any statistic, in particular, for Kurtosis Coeficient. I
have done the following code lines:

> library(boot)
> x=rnorm(20)
> kurtosis=function(x) (mean((x-mean(x))^4))/(sd(x)^4)
> z <- numeric(10000)
> for(i in 1:10000)
> z[i]=kurtosis(sample(x, replace=TRUE))
> boot.ci(z, conf = 0.95,type = c("norm","basic","perc","bca"))

But the output shows the next error:

Error en if (ncol(boot.out$t) < max(index)) { : 
        argumento tiene longitud cero

I don't know what is wrong.

I hope that somebody can help me. Sorry for my english.

All have a nice new year.

_Fede_

-- 
View this message in context: http://www.nabble.com/Bootstrap-Confidence-Intervals-tp14550471p14550471.html
Sent from the R help mailing list archive at Nabble.com.


From miguel.tremblay at ptaff.ca  Sun Dec 30 20:57:35 2007
From: miguel.tremblay at ptaff.ca (Miguel Tremblay)
Date: Sun, 30 Dec 2007 14:57:35 -0500 (EST)
Subject: [R] Text over the plot and the margins
Message-ID: <alpine.LNX.1.00.0712301420290.13822@octet.ca>

Hi,

I am having problem to display text that goes start inside the 
plot but should go over the margins of the plot.

You can have an example of what I mean by looking at the graphs on this 
page:
http://ptaff.ca/soleil/?l1pays=Canada&l1etat=Qu%C3%A9bec&l1ville=Montr%C3%A9al&year=2007&month=12&day=30&l1cityname=Montr%C3%A9al%2C+Qu%C3%A9bec%2C+Canada&l1ltd=45&l1ltm=30&l1lts=0&l1ltx=N&l1lgd=73&l1lgm=34&l1lgs=47&l1lgx=W&l1tz=-5.0&l1dst=US&l2ltx=N&l2lgx=E&l2tz=0&lang=en_CA&go=Voir+le+graphe%21
or
http://tinyurl.com/3ykaha

The sunset and sunlight values near the bullets should be displayed but 
they are truncated in the middle.

For what I understand, the "text" method is used to write inside the plot 
area while the "mtext" is used to write in the margins, i.e. outsite the 
plot.

Is there a way to start writing in the plot area and to indicate that if 
the strings goes outside the area, it can continue to write there?

Thanks,

Miguel Tremblay
http://ptaff.ca/miguel/


From ripley at stats.ox.ac.uk  Sun Dec 30 21:04:59 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 30 Dec 2007 20:04:59 +0000 (GMT)
Subject: [R] Installing Rgraphviz package on a windows vista pc
In-Reply-To: <735686.71425.qm@web26614.mail.ukl.yahoo.com>
References: <735686.71425.qm@web26614.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0712302002500.2887@gannet.stats.ox.ac.uk>

On Sun, 30 Dec 2007, Ravi Vishnu wrote:

> Hi,

> I want to install Rgraphviz on a windows vista pc. The only source file 
> that I found in the bioconductor homepage is a tar file. I can't install 
> from this using the packages command in the R menu. I would like to get 
> some tips on how I can do the installation.
>
> Can I just place the extracted Rgraph folder (from the source tar file) 
> in the library folder of R2.6.1 along with the other packages? How do I 
> then alert R that this folder is there?

(No, it will not work.)

If you select the Bioconductor repository (from Packages -> Set 
Repositories) Rgraphviz will be found: it works for me.

If you need more help, please ask on the Bioconductor list (see the R 
posting guide).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mtmorgan at fhcrc.org  Sun Dec 30 21:07:24 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Sun, 30 Dec 2007 12:07:24 -0800
Subject: [R] Installing Rgraphviz package on a windows vista pc
In-Reply-To: <735686.71425.qm@web26614.mail.ukl.yahoo.com> (Ravi Vishnu's
	message of "Sun, 30 Dec 2007 18:50:37 +0100 (CET)")
References: <735686.71425.qm@web26614.mail.ukl.yahoo.com>
Message-ID: <6ph3atkrloj.fsf@gopher4.fhcrc.org>

Hi Ravi -- this is a Bioconductor package, so please ask on the
Bioconductor mailing list.

There is a windows binary available

http://bioconductor.org/packages/2.1/bioc/html/Rgraphviz.html

The best way to install this and all Bioconductor packages is

> source('http://bioconductor.org/biocLite.R')
> biocLite('Rgraphviz')

Martin

Ravi Vishnu <rv15i at yahoo.se> writes:

> Hi, I want to install Rgraphviz on a windows vista pc. The only source
> file that I found in the bioconductor homepage is a tar file. I can't
> install from this using the packages command in the R menu. I would
> like to get some tips on how I can do the installation.
>
> Can I just place the extracted Rgraph folder (from the source tar
> file) in the library folder of R2.6.1 along with the other packages?
> How do I then alert R that this folder is there?
>
> Thanking you, Ravi
>
>        
> --------------------------------- S?k efter k?rleken!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________ R-help at r-project.org
> mailing list https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
> read the posting guide http://www.R-project.org/posting-guide.html and
> provide commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Computational Biology / Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M2 B169
Phone: (206) 667-2793


From ripley at stats.ox.ac.uk  Sun Dec 30 21:09:38 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 30 Dec 2007 20:09:38 +0000 (GMT)
Subject: [R] Bootstrap Confidence Intervals
In-Reply-To: <14550471.post@talk.nabble.com>
References: <14550471.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0712302006270.2887@gannet.stats.ox.ac.uk>

You need to call boot() to create an object to pass to boot.ci().

There are lots of examples in the help pages and in the book that package 
'boot' supports. From the help:

Usage:

      boot.ci(boot.out, conf = 0.95, type = "all",
              index = 1:min(2,length(boot.out$t0)), var.t0 = NULL,
              var.t = NULL, t0 = NULL, t = NULL, L = NULL, h = function(t) t,
              hdot = function(t) rep(1,length(t)), hinv = function(t) t, ...)

Arguments:

boot.out: An object of class '"boot"' containing the output of a
           bootstrap calculation.

and try class(z) .


On Sun, 30 Dec 2007, _Fede_ wrote:

>
>
> Hi all.
>
> This is my first post in this forum. Finally I find a forum in the web about
> R, although is not in my language.
>
> Now I'm working with Bootstrap CI. I'd like to know how I can calculate a
> Bootstrap CI for any statistic, in particular, for Kurtosis Coeficient. I
> have done the following code lines:
>
>> library(boot)
>> x=rnorm(20)
>> kurtosis=function(x) (mean((x-mean(x))^4))/(sd(x)^4)
>> z <- numeric(10000)
>> for(i in 1:10000)
>> z[i]=kurtosis(sample(x, replace=TRUE))
>> boot.ci(z, conf = 0.95,type = c("norm","basic","perc","bca"))
>
> But the output shows the next error:
>
> Error en if (ncol(boot.out$t) < max(index)) { :
>        argumento tiene longitud cero
>
> I don't know what is wrong.
>
> I hope that somebody can help me. Sorry for my english.
>
> All have a nice new year.
>
> _Fede_
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ba208 at exeter.ac.uk  Sun Dec 30 21:28:09 2007
From: ba208 at exeter.ac.uk (=?ISO-8859-1?Q?baptiste_Augui=E9?=)
Date: Sun, 30 Dec 2007 20:28:09 +0000
Subject: [R] adding a function after package.skeleton()
In-Reply-To: <971536df0712301027q27b8970ew60ac648c0cbc38cb@mail.gmail.com>
References: <878BE177-8F3F-409F-B32B-6B4A7BDFA339@ex.ac.uk>
	<971536df0712301027q27b8970ew60ac648c0cbc38cb@mail.gmail.com>
Message-ID: <9F2055AC-0B47-4009-90F5-6E1907877EDD@ex.ac.uk>

Hi,

Thanks for this tip, I'm always amazed at the number of clever  
functions built-in in R ?? just wish i could think of their name  
rather than reinventing the wheel.

However, I'm still stupidly stuck with this basic question: how  
should a function access data in its own package?


On 30 Dec 2007, at 18:27, Gabor Grothendieck wrote:

> On Dec 30, 2007 1:19 PM, baptiste Augui? <ba208 at exeter.ac.uk> wrote:
>
>> These two functions use the dataframe "Constants", part of this  
>> package:
>>
>>> `L2eV` <- function(lambda)
>>> {
>>>       data("Constants")
>>>       Constants$h*Constants$cel/Constants$ee/lambda ->eV
>>>       eV
>>>       }
>>
>
> This does not answer your question but note that you can do this:
>
>    with(Constants, h * cel / ee / lambda)



Initial post:

>> 	From: 	  ba208 at exeter.ac.uk
>> 	Subject: 	adding a function after package.skeleton()
>> 	Date: 	30 December 2007 18:19:40 GMT
>> 	To: 	  r-help at r-project.org
>>
>> Dear R helpers,
>>
>> I've successfully created a package 'constants' using  
>> package.skeleton() with one dataframe and a few functions.  
>> However, now that I want to add some functions and data to the  
>> package, I run into a problem.
>>
>> I ran prompt(...) and moved + edited the resulting .Rd files as  
>> appropriate (I believe). The log file from RCMD check constants  
>> does indicate a few problems (full log below).
>>
>> As far as I understand, the real problem would be on the lines:
>>
>>
>>> * checking R code for possible problems ... NOTE
>>> L2eV: no visible binding for global variable 'Constants'
>>> eV2L: no visible binding for global variable 'Constants'
>>>
>>
>>
>> These two functions use the dataframe "Constants", part of this  
>> package:
>>
>>
>>> `L2eV` <- function(lambda)
>>> {
>>> 	data("Constants")
>>> 	Constants$h*Constants$cel/Constants$ee/lambda ->eV
>>> 	eV
>>> 	}
>>>
>>
>>
>> and
>>
>>
>>> `eV2L` <- function(eV)
>>> {
>>> 	data("Constants")
>>> 	Constants$h*Constants$cel/Constants$ee/eV ->Lambda
>>> 	Lambda
>>> 	}
>>>
>>
>>
>> After searching the R archives about "no visible binding for  
>> global variable ", I added the quotes around "Constants" but it  
>> doesn't seem to help. What would be the correct way to use this  
>> data inside the package?
>>
>> Best regards,
>>
>> baptiste
>>
>> ----------------
>> Log of R CMD check constants :
>>
>>
>>
>>>
>>> baptiste-auguies-ibook-g4:~ baptiste$ R CMD check constants
>>> * checking for working latex ... OK
>>> * using log directory '/Users/baptiste/constants.Rcheck'
>>> * using R version 2.6.1 (2007-11-26)
>>> * checking for file 'constants/DESCRIPTION' ... OK
>>> * checking extension type ... Package
>>> * this is package 'constants' version '1.0'
>>> * checking package dependencies ... OK
>>> * checking if this is a source package ... OK
>>> * checking whether package 'constants' can be installed ... OK
>>> * checking package directory ... OK
>>> * checking for portable file names ... OK
>>> * checking for sufficient/correct file permissions ... OK
>>> * checking DESCRIPTION meta-information ... OK
>>> * checking top-level files ... OK
>>> * checking index information ... OK
>>> * checking package subdirectories ... OK
>>> * checking R files for non-ASCII characters ... OK
>>> * checking R files for syntax errors ... OK
>>> * checking whether the package can be loaded ... OK
>>> * checking whether the package can be loaded with stated  
>>> dependencies ... OK
>>> * checking for unstated dependencies in R code ... OK
>>> * checking S3 generic/method consistency ... OK
>>> * checking replacement functions ... OK
>>> * checking foreign function calls ... OK
>>> * checking R code for possible problems ... NOTE
>>> L2eV: no visible binding for global variable 'Constants'
>>> eV2L: no visible binding for global variable 'Constants'
>>> * checking Rd files ... WARNING
>>> Rd files with non-standard keywords:
>>>   L2eV.Rd: kwd1 kwd2
>>>   constants-package.Rd: physical constants optical
>>>   delete.all.Rd: kwd1 kwd2
>>>   eV2L.Rd: kwd1 kwd2
>>>   epsilon2nk.Rd: kwd1 kwd2
>>>   fano.Rd: kwd1 kwd2
>>>   lorentz.Rd: kwd1 kwd2
>>>   nk2epsilon.Rd: kwd1 kwd2
>>> Each '\keyword' entry should specify one of the standard keywords  
>>> (as
>>> listed in file 'KEYWORDS' in the R documentation directory).
>>>
>>> See the chapter 'Writing R documentation files' in manual 'Writing R
>>> Extensions'.
>>> * checking Rd cross-references ... OK
>>> * checking for missing documentation entries ... OK
>>> * checking for code/documentation mismatches ... OK
>>> * checking Rd \usage sections ... OK
>>> * checking data for non-ASCII characters ... OK
>>> * creating constants-Ex.R ... OK
>>> * checking examples ... OK
>>> * creating constants-manual.tex ... OK
>>> * checking constants-manual.tex ... OK
>>>
>>> WARNING: There was 1 warning, see
>>>   /Users/baptiste/constants.Rcheck/00check.log
>>> for details
>>>




_____________________________

Baptiste Augui?

Physics Department
University of Exeter
Stocker Road,
Exeter, Devon,
EX4 4QL, UK

Phone: +44 1392 264187

http://newton.ex.ac.uk/research/emag
http://projects.ex.ac.uk/atto


From jrkrideau at yahoo.ca  Sun Dec 30 22:37:33 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 30 Dec 2007 16:37:33 -0500 (EST)
Subject: [R] Text over the plot and the margins
In-Reply-To: <alpine.LNX.1.00.0712301420290.13822@octet.ca>
Message-ID: <964858.85794.qm@web32811.mail.mud.yahoo.com>

Hi Miguel,

Some sample code would help but I think xpd in ?par
may be what you're looking for however it probably
would be easier/better to set  the xlim values to give
more space on the right hand side of the graph or just
modify the text( at=) value to keep the numbers inside
the plot area.


--- Miguel Tremblay <miguel.tremblay at ptaff.ca> wrote:

> Hi,
> 
> I am having problem to display text that goes start
> inside the 
> plot but should go over the margins of the plot.
> 
> You can have an example of what I mean by looking at
> the graphs on this 
> page:
>
http://ptaff.ca/soleil/?l1pays=Canada&l1etat=Qu%C3%A9bec&l1ville=Montr%C3%A9al&year=2007&month=12&day=30&l1cityname=Montr%C3%A9al%2C+Qu%C3%A9bec%2C+Canada&l1ltd=45&l1ltm=30&l1lts=0&l1ltx=N&l1lgd=73&l1lgm=34&l1lgs=47&l1lgx=W&l1tz=-5.0&l1dst=US&l2ltx=N&l2lgx=E&l2tz=0&lang=en_CA&go=Voir+le+graphe%21
> or
> http://tinyurl.com/3ykaha
> 
> The sunset and sunlight values near the bullets
> should be displayed but 
> they are truncated in the middle.
> 
> For what I understand, the "text" method is used to
> write inside the plot 
> area while the "mtext" is used to write in the
> margins, i.e. outsite the 
> plot.
> 
> Is there a way to start writing in the plot area and
> to indicate that if 
> the strings goes outside the area, it can continue
> to write there?
> 
> Thanks,
> 
> Miguel Tremblay
> http://ptaff.ca/miguel/
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From goedman at mac.com  Sun Dec 30 22:54:54 2007
From: goedman at mac.com (Rob Goedman)
Date: Sun, 30 Dec 2007 13:54:54 -0800
Subject: [R] Trying to install rjags on Mac OS X 10.5
In-Reply-To: <20071230162711.s4q4h4pkg04o8c84@webmail.manchester.ac.uk>
References: <20071230162711.s4q4h4pkg04o8c84@webmail.manchester.ac.uk>
Message-ID: <EA00A37F-B9F7-4981-9ACA-CA535B27BD3C@mac.com>

Hi Lindsay,

Did have some difficulties as well, but got it to work using Xcode3.0,  
Apple's gcc4.2preview release and gfortran 4.2.1 from Simon's web  
site. This was on R-devel though, on a Mac Intel.

Can it find the jags executable say from your home directory? Have you  
tried 'make check' in the classic-bugs/vol1 or vol2 example dirs (in  
the Terminal)? That would indeed confirm jags is running fine.

For rjags, I am a bit worried about the /opt/... . Did you at some  
point install software from Fink or Darwin ports? Until I installed  
the above newer toolset, I had never been able to get rjags to work.

Of course you don't really need rjags, only coda. I have also  
installed the package 'arm' and the packages arm depends on (Matrix,  
lattice, lme4, R2WinBugs). If it would help I can send you a  
'template' dir for the line example that does not use rjags.

A dependency I ran into was coda v 13.1, as of a couple of days ago  
not available as a binary package,
but available as a source package on CRAN.

Rob

On Dec 30, 2007, at 8:27 AM, Lindsay Stirton wrote:

> Greetings,
>
> I wonder if anyone can offer any help or advice--even direction to an
> appropriate source of advice. I am trying to install rjags 1.0.1 on  
> Mac OS X
> 10.5 (see http://www-fis.iarc.fr/~martyn/software/jags/). I have  
> R.app 2.6.1
> installed.
>
> JAGS 1.0.1 is apparently successfully installed. I (think I) know  
> this because
> when I type 'jags' into Terminal, I get the following:
>
> Macintosh:Desktop ljs$ jags
> Welcome to JAGS 1.0.1 on Thu Dec 27 20:56:09 2007
> JAGS is free software and comes with ABSOLUTELY NO WARRANTY
> Loading module: basemod
> Loading module: bugs
> .
>
> However, when I try to install rjags, I get the following problem.
>
> Macintosh:Desktop ljs$ sudo R CMD INSTALL rjags_1.0.1-1.tar.gz
> Password:
> * Installing to library '/Library/Frameworks/R.framework/Resources/ 
> library'
> * Installing *source* package 'rjags' ...
> checking for pkg-config... /opt/local/bin/pkg-config
> checking pkg-config is at least version 0.9.0... yes
> checking for JAGS... configure: error: Package requirements (jags =  
> 1.0.1) were
> not met:
>
> No package 'jags' found
>
> Consider adjusting the PKG_CONFIG_PATH environment variable if you
> installed software in a non-standard prefix.
>
> ERROR: configuration failed for package 'rjags'
> ** Removing '/Library/Frameworks/R.framework/Resources/library/rjags'
>
> I am really not sure what the problem is. I have (I think) set the  
> proper
> environment variables correctly. See below:
>
> Macintosh:Desktop ljs$ which jags
> /usr/local/bin/jags
> Macintosh:Desktop ljs$ echo $PKG_CONFIG_PATH
> /usr/local/bin
>
> I should say that I had many other problems
> along the way--gfortran not originally installed properly, R CMD  
> INSTALL
> couldn't find pkg-config. However, the above description shows the  
> problems
> remaining even after I have done a little homework.
>
> Best wishes,
>
> Lindsay Stirton
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From slamb at slamb.org  Sun Dec 30 23:11:46 2007
From: slamb at slamb.org (Scott Lamb)
Date: Sun, 30 Dec 2007 14:11:46 -0800
Subject: [R] plot multiple data sets on same axis
Message-ID: <477817A2.6080206@slamb.org>

I'm new to R and struggling to reproduce graphs I've made with gnuplot.
Example here:

    http://www.slamb.org/tmp/one-active.png

I have three different data sets plotted on the same axis. (I also have
a number of samples for each X value which I displayed with quartiles
rather than plotting every point; that will likely be the subject of my
next question.)

My attempts to do this in R: I've put the data into a frame with (x, y,
dataset) columns; dataset is a categorical variable. I can get a coplot
which sort of shows the information:

    attach(myframe)
    coplot(y ~ x | dataset)

but not on the same axis with a legend. I'd like to start by getting
that in a scatterplot form:

    # XXX: datasets hardcoded in here...
    # is split() supposed to do something similar to this?
    # or how do I get a list of datasets to feed into subset?
    myframe_a <- subset(myframe, dataset=='a')
    myframe_b <- subset(myframe, dataset=='b')
    ...

and then I can apparently plot one and add points from others to it:

    # XXX: more hardcoding...
    attach(myframe_a)
    plot(x, y, col='red')
    detach(myframe_a)
    attach(myframe_b)
    points(x, y, col='blue')
    detach(myframe_b)
    ...
    legend("topleft",
           legend=c("a", "b", ...),
           fill=c("red", "blue", ...))

but there are several things I don't like about this solution:

* there probably is an existing function which does this? I can't find it.

* obviously I don't want to duplicate code for each dataset. I'd rather
loop based on whatever datasets are in the frame, but I'm missing how to
do that here.

* points() appears to not alter xlim and ylim. Is there a convenient way
to autodetermine them based on all the points?

* I've hardcoded the colors. This is the sort of thing I'd rather leave
to an expert. (I.e. someone who has looked at colorblindness studies and
knows which colors are easiest to distinguish.)

Any ideas?

Cheers,
Scott


From jholtman at gmail.com  Sun Dec 30 23:54:59 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 30 Dec 2007 17:54:59 -0500
Subject: [R] plot multiple data sets on same axis
In-Reply-To: <477817A2.6080206@slamb.org>
References: <477817A2.6080206@slamb.org>
Message-ID: <644e1f320712301454q6a715a8chcd26922b50e1ba02@mail.gmail.com>

Here is one way of doing it by splitting the data and plotting each
set in a different color:

# generate some test data
mydf <- data.frame(x=runif(200), y=rnorm(200),
dataset=sample(LETTERS[1:4], 200, TRUE))
# setup the plot are for the maximum of the data
plot(0, xlim=range(mydf$x), ylim=range(mydf$y), type='n', ylab="Y", xlab="X")
# split by 'dataset' and then plot each series; sort by 'x' first
mydf[] <- mydf[order(mydf$x),]
split.df <- split(mydf, mydf$dataset)
# loop through each set and plot with a different color
for (i in seq_along(split.df)){
    lines(split.df[[i]]$x, split.df[[i]]$y, col=i)
}


On Dec 30, 2007 5:11 PM, Scott Lamb <slamb at slamb.org> wrote:
> I'm new to R and struggling to reproduce graphs I've made with gnuplot.
> Example here:
>
>    http://www.slamb.org/tmp/one-active.png
>
> I have three different data sets plotted on the same axis. (I also have
> a number of samples for each X value which I displayed with quartiles
> rather than plotting every point; that will likely be the subject of my
> next question.)
>
> My attempts to do this in R: I've put the data into a frame with (x, y,
> dataset) columns; dataset is a categorical variable. I can get a coplot
> which sort of shows the information:
>
>    attach(myframe)
>    coplot(y ~ x | dataset)
>
> but not on the same axis with a legend. I'd like to start by getting
> that in a scatterplot form:
>
>    # XXX: datasets hardcoded in here...
>    # is split() supposed to do something similar to this?
>    # or how do I get a list of datasets to feed into subset?
>    myframe_a <- subset(myframe, dataset=='a')
>    myframe_b <- subset(myframe, dataset=='b')
>    ...
>
> and then I can apparently plot one and add points from others to it:
>
>    # XXX: more hardcoding...
>    attach(myframe_a)
>    plot(x, y, col='red')
>    detach(myframe_a)
>    attach(myframe_b)
>    points(x, y, col='blue')
>    detach(myframe_b)
>    ...
>    legend("topleft",
>           legend=c("a", "b", ...),
>           fill=c("red", "blue", ...))
>
> but there are several things I don't like about this solution:
>
> * there probably is an existing function which does this? I can't find it.
>
> * obviously I don't want to duplicate code for each dataset. I'd rather
> loop based on whatever datasets are in the frame, but I'm missing how to
> do that here.
>
> * points() appears to not alter xlim and ylim. Is there a convenient way
> to autodetermine them based on all the points?
>
> * I've hardcoded the colors. This is the sort of thing I'd rather leave
> to an expert. (I.e. someone who has looked at colorblindness studies and
> knows which colors are easiest to distinguish.)
>
> Any ideas?
>
> Cheers,
> Scott
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From john.maindonald at anu.edu.au  Mon Dec 31 00:45:24 2007
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Mon, 31 Dec 2007 10:45:24 +1100
Subject: [R] Symbolic substitution in parallel; use infinity symbol?
Message-ID: <C741B895-56F0-4322-B5AC-79EA275ECF91@anu.edu.au>


I'd like to be able to modify axlab in (C) below so that 'Inf'
is replaced by the infinity symbol.

y <- rnorm(40)
breaks <- c(-Inf, -1, 1, Inf)
x <- cut(y, breaks=breaks)
plot(unclass(x), y, xaxt="n", xlab="")

## A: The following gives the axis labels "(-Inf, 1]", etc.
axis(1, at=1:3, labels=expression("(-Inf,-1]", "(-1,1]", "(1, Inf]"))

## B: The following replaces Inf by the infinity symbol
axis(1, at=1:3, labels=expression("(" * -infinity * ", " * -1 * "]",
                                 "(" * -1 * ", 1]", "(1, " * infinity  
* "]"),
    line=1.25, lty=0)

## C: The following gives the axis labels "(-Inf, 1]", etc.,
## in a more automated manner.
axlab <- lapply(levels(x), function(x)substitute(a, list(a=x)))
# Can alternatively use bquote()
axis(1, at=1:3, labels=as.expression(axlab), line=2.25, lty=0)

However I have been unable to modify axlab so that the infinity
symbol appears in place of 'Inf'.  Is there is some relatively
straightforward way to do this?  The issue is of course more
general than this specific example.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


From slamb at slamb.org  Mon Dec 31 02:26:28 2007
From: slamb at slamb.org (Scott Lamb)
Date: Sun, 30 Dec 2007 17:26:28 -0800
Subject: [R] plot multiple data sets on same axis
In-Reply-To: <644e1f320712301454q6a715a8chcd26922b50e1ba02@mail.gmail.com>
References: <477817A2.6080206@slamb.org>
	<644e1f320712301454q6a715a8chcd26922b50e1ba02@mail.gmail.com>
Message-ID: <47784544.90709@slamb.org>

Thank you - that was exactly what I needed. I just discovered read.csv
understands URLs, so here it is with my actual data and formatting:

    df <- read.csv("http://www.slamb.org/tmp/one-active.csv")
    split.df <- split(df, df$method)
    plot(0, xlim=range(df$inactive), ylim=range(df$elapsed), type="n",
         ylab="time (?s)", xlab="inactive file descriptors", log="y",
         main="1 active descriptor, 1 write", bty="l")
    grid()
    for (i in seq_along(split.df)) {
        lines(split.df[[i]]$inactive, split.df[[i]]$elapsed, col=i)
    }
    legend("topleft", legend=labels(split.df),
           fill=seq_along(split.df), bty="n")

Next question. Rather than plot each point, I'd like to use boxplots to
show the interquartile range for each x.

I've tried replacing the for loop body with this:

    this_method <- split.df[[i]]
    boxplot(elapsed~inactive, data=this_method,
            add=TRUE, border=i, boxfill=i, outline=FALSE)

but it has two problems:

* it doesn't plot at the correct x values. It looks like I need to
supply a list of the x values as the "at" parameter, and I don't know
how to get unique values from this_method$inactive. (I tried the clunky
labels(split(this_method, this_method$inactive)), but it returns them as
strings.)

* it redraws the graph's frame, and it ignores bty="l" when doing so.

Not sure how to correct either.

Cheers,
Scott

-- 
Scott Lamb <http://www.slamb.org/>


From p.dalgaard at biostat.ku.dk  Mon Dec 31 02:35:37 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 31 Dec 2007 02:35:37 +0100
Subject: [R] Symbolic substitution in parallel; use infinity symbol?
In-Reply-To: <C741B895-56F0-4322-B5AC-79EA275ECF91@anu.edu.au>
References: <C741B895-56F0-4322-B5AC-79EA275ECF91@anu.edu.au>
Message-ID: <47784769.9090509@biostat.ku.dk>

John Maindonald wrote:
> I'd like to be able to modify axlab in (C) below so that 'Inf'
> is replaced by the infinity symbol.
>
> y <- rnorm(40)
> breaks <- c(-Inf, -1, 1, Inf)
> x <- cut(y, breaks=breaks)
> plot(unclass(x), y, xaxt="n", xlab="")
>
> ## A: The following gives the axis labels "(-Inf, 1]", etc.
> axis(1, at=1:3, labels=expression("(-Inf,-1]", "(-1,1]", "(1, Inf]"))
>
> ## B: The following replaces Inf by the infinity symbol
> axis(1, at=1:3, labels=expression("(" * -infinity * ", " * -1 * "]",
>                                  "(" * -1 * ", 1]", "(1, " * infinity  
> * "]"),
>     line=1.25, lty=0)
>
> ## C: The following gives the axis labels "(-Inf, 1]", etc.,
> ## in a more automated manner.
> axlab <- lapply(levels(x), function(x)substitute(a, list(a=x)))
> # Can alternatively use bquote()
> axis(1, at=1:3, labels=as.expression(axlab), line=2.25, lty=0)
>
> However I have been unable to modify axlab so that the infinity
> symbol appears in place of 'Inf'.  Is there is some relatively
> straightforward way to do this?  The issue is of course more
> general than this specific example.
>   
Here's an idea, leaving some tinkering for you:

breaks <- c(-Inf, -1, 1, Inf)

zz <- lapply(breaks, function(x) 
      if(x==-Inf) quote(-infinity) else
      if (x==Inf) quote(infinity) else 
      format(x))

lbl <- mapply(function(x,y)
                 bquote("(" * .(x) * "," * .(y) * "]"),
              zz[-4], zz[-1], SIMPLIFY=FALSE)


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From slamb at slamb.org  Mon Dec 31 02:49:17 2007
From: slamb at slamb.org (Scott Lamb)
Date: Sun, 30 Dec 2007 17:49:17 -0800
Subject: [R] plot multiple data sets on same axis
In-Reply-To: <47784544.90709@slamb.org>
References: <477817A2.6080206@slamb.org>
	<644e1f320712301454q6a715a8chcd26922b50e1ba02@mail.gmail.com>
	<47784544.90709@slamb.org>
Message-ID: <47784A9D.70101@slamb.org>

Scott Lamb wrote:
> I've tried replacing the for loop body with this:
> 
>     this_method <- split.df[[i]]
>     boxplot(elapsed~inactive, data=this_method,
>             add=TRUE, border=i, boxfill=i, outline=FALSE)
> 
> but it has two problems:
> 
> * it doesn't plot at the correct x values. It looks like I need to
> supply a list of the x values as the "at" parameter, and I don't know
> how to get unique values from this_method$inactive. (I tried the clunky
> labels(split(this_method, this_method$inactive)), but it returns them as
> strings.)

Ahh. I missed the obvious answer - there's a function called unique.
at=unique(this_method$inactive) works.

> 
> * it redraws the graph's frame, and it ignores bty="l" when doing so.

also the x axis tics and labels...they're totally unreadable now.

Cheers,
Scott

-- 
Scott Lamb <http://www.slamb.org/>


From JohnField at ozemail.com.au  Mon Dec 31 03:15:42 2007
From: JohnField at ozemail.com.au (John Field)
Date: Mon, 31 Dec 2007 12:45:42 +1030
Subject: [R] Survival analysis with no events in one treatment group
Message-ID: <6.2.3.4.2.20071231104836.03ae0590@pop.ozemail.com.au>

I'm trying to fit a Cox proportional hazards model to some hospital 
admission data.  About 25% of the patients have had at least one 
admission, and of these, 40% have had two admissions within the 12 
month period of the study.  Each patients has had one of 4 
treatments, and one of the treatment groups has had no admissions for 
the period.  I used:

surv.obj<-Surv(time=time1,time2=time2,event=event,type="counting")
model<-coxph(surv.obj~Treatment+cluster(Subject))

and, as explained in the coxph help page, I get a warning message 
about convergence because the MLE of one of the coefficients is 
infinite since there are no admissions in one group.

I'm looking for suggestions about how to proceed with an analysis of 
these data.  I'd prefer not to ignore the fact that there are 
multiple admissions, but any alternative ideas I have at the moment do this.

Many thanks,
John Field
=================================
Faculty of Health Sciences Statistical Support Service
The University of Adelaide, Australia 5005


From daniel at umd.edu  Mon Dec 31 03:29:23 2007
From: daniel at umd.edu (Daniel Malter)
Date: Sun, 30 Dec 2007 21:29:23 -0500
Subject: [R] Survival analysis with no events in one treatment group
In-Reply-To: <6.2.3.4.2.20071231104836.03ae0590@pop.ozemail.com.au>
Message-ID: <200712310228.CJA67356@md2.mail.umd.edu>

Hi John,

I am on the slow side - can you provide sample code. How can one treatment
group have no admissions? 

Let's say there are treatments W, X, Y, Z. Do you mean that NONE of the
patients who got admitted the first time and, say, received treatment X
during the first admission, have ever had a second admission (in your data).
And for the other treatments W, Y, and Z some of those who got admitted the
first time came in a second time?

Cheers and a happy new year's eve,
Daniel

-------------------------
cuncta stricte discussurus
-------------------------

-----Urspr?ngliche Nachricht-----
Von: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Im
Auftrag von John Field
Gesendet: Sunday, December 30, 2007 9:16 PM
An: R-help at r-project.org
Betreff: [R] Survival analysis with no events in one treatment group

I'm trying to fit a Cox proportional hazards model to some hospital
admission data.  About 25% of the patients have had at least one admission,
and of these, 40% have had two admissions within the 12 month period of the
study.  Each patients has had one of 4 treatments, and one of the treatment
groups has had no admissions for the period.  I used:

surv.obj<-Surv(time=time1,time2=time2,event=event,type="counting")
model<-coxph(surv.obj~Treatment+cluster(Subject))

and, as explained in the coxph help page, I get a warning message about
convergence because the MLE of one of the coefficients is infinite since
there are no admissions in one group.

I'm looking for suggestions about how to proceed with an analysis of these
data.  I'd prefer not to ignore the fact that there are multiple admissions,
but any alternative ideas I have at the moment do this.

Many thanks,
John Field
=================================
Faculty of Health Sciences Statistical Support Service The University of
Adelaide, Australia 5005

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From daniel at umd.edu  Mon Dec 31 03:35:57 2007
From: daniel at umd.edu (Daniel Malter)
Date: Sun, 30 Dec 2007 21:35:57 -0500
Subject: [R] Survival analysis with no events in one treatment group
In-Reply-To: <6.2.3.4.2.20071231104836.03ae0590@pop.ozemail.com.au>
Message-ID: <200712310235.CJA67455@md2.mail.umd.edu>

Hi again,

I meant some rows of sample data, rather than sample code. Sorry about that.

Daniel 


-------------------------
cuncta stricte discussurus
-------------------------

-----Urspr?ngliche Nachricht-----
Von: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Im
Auftrag von John Field
Gesendet: Sunday, December 30, 2007 9:16 PM
An: R-help at r-project.org
Betreff: [R] Survival analysis with no events in one treatment group

I'm trying to fit a Cox proportional hazards model to some hospital
admission data.  About 25% of the patients have had at least one admission,
and of these, 40% have had two admissions within the 12 month period of the
study.  Each patients has had one of 4 treatments, and one of the treatment
groups has had no admissions for the period.  I used:

surv.obj<-Surv(time=time1,time2=time2,event=event,type="counting")
model<-coxph(surv.obj~Treatment+cluster(Subject))

and, as explained in the coxph help page, I get a warning message about
convergence because the MLE of one of the coefficients is infinite since
there are no admissions in one group.

I'm looking for suggestions about how to proceed with an analysis of these
data.  I'd prefer not to ignore the fact that there are multiple admissions,
but any alternative ideas I have at the moment do this.

Many thanks,
John Field
=================================
Faculty of Health Sciences Statistical Support Service The University of
Adelaide, Australia 5005

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From JohnField at ozemail.com.au  Mon Dec 31 04:14:15 2007
From: JohnField at ozemail.com.au (John Field)
Date: Mon, 31 Dec 2007 13:44:15 +1030
Subject: [R] Survival analysis with no events in one treatment  group
In-Reply-To: <200712310228.CJA67356@md2.mail.umd.edu>
References: <6.2.3.4.2.20071231104836.03ae0590@pop.ozemail.com.au>
	<200712310228.CJA67356@md2.mail.umd.edu>
Message-ID: <6.2.3.4.2.20071231133536.02ebaba0@pop.ozemail.com.au>

Hi Daniel,

Sorry, it may have been clearer if I had used 
"subjects" instead of "patients".  The treatments 
were administered to all subjects, and then in 
the succeeding 12 months, some were hospitalised 
and some were not.  Hence only about 25% of the subjects were hospitalised.

The start of the data:
subj    time1   time2   event Treat
1       0       6.2     1       A
1       6.2     12      0       A
2       0       12      0       A

so subject 1 was hospitalised at 6.2 months, subject 2 not at all.

Hope this makes it clearer.
Regards,
John





.At 12:59 PM 31/12/2007, Daniel Malter wrote:
>Hi John,
>
>I am on the slow side - can you provide sample code. How can one treatment
>group have no admissions?
>
>Let's say there are treatments W, X, Y, Z. Do you mean that NONE of the
>patients who got admitted the first time and, say, received treatment X
>during the first admission, have ever had a second admission (in your data).
>And for the other treatments W, Y, and Z some of those who got admitted the
>first time came in a second time?
>
>Cheers and a happy new year's eve,
>Daniel
>
>-------------------------
>cuncta stricte discussurus
>-------------------------
>
>-----Urspr?ngliche Nachricht-----
>Von: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Im
>Auftrag von John Field
>Gesendet: Sunday, December 30, 2007 9:16 PM
>An: R-help at r-project.org
>Betreff: [R] Survival analysis with no events in one treatment group
>
>I'm trying to fit a Cox proportional hazards model to some hospital
>admission data.  About 25% of the patients have had at least one admission,
>and of these, 40% have had two admissions within the 12 month period of the
>study.  Each patients has had one of 4 treatments, and one of the treatment
>groups has had no admissions for the period.  I used:
>
>surv.obj<-Surv(time=time1,time2=time2,event=event,type="counting")
>model<-coxph(surv.obj~Treatment+cluster(Subject))
>
>and, as explained in the coxph help page, I get a warning message about
>convergence because the MLE of one of the coefficients is infinite since
>there are no admissions in one group.
>
>I'm looking for suggestions about how to proceed with an analysis of these
>data.  I'd prefer not to ignore the fact that there are multiple admissions,
>but any alternative ideas I have at the moment do this.
>
>Many thanks,
>John Field
>=================================
>Faculty of Health Sciences Statistical Support Service The University of
>Adelaide, Australia 5005
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From riddle_chen94 at yahoo.com  Mon Dec 31 04:52:52 2007
From: riddle_chen94 at yahoo.com (Yingchen Wang)
Date: Sun, 30 Dec 2007 19:52:52 -0800 (PST)
Subject: [R] help with matrix
Message-ID: <728642.19637.qm@web45710.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071230/2826370c/attachment.pl 

From jholtman at gmail.com  Mon Dec 31 05:21:14 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 30 Dec 2007 23:21:14 -0500
Subject: [R] help with matrix
In-Reply-To: <728642.19637.qm@web45710.mail.sp1.yahoo.com>
References: <728642.19637.qm@web45710.mail.sp1.yahoo.com>
Message-ID: <644e1f320712302021m795c78a4h4047d04ebb47f6df@mail.gmail.com>

Does this give you want you want?  I assume that you have to also
ignore the first column in matrixC or else you don't have a reasonable
comparison.  Also your data for matrixA and matrixB did not have any
spaces between the numbers, so I just took a guess at what they should
be.

> a.m <- scan(textConnection("     1.851 1.4 0.083 1.001
+ 0.877 1.3 0.116 1.33
+ 1.902 1.2 1.102 0.302
+ 0.864 0.126 1.11 0.252
+ 1.823 0.216 1.002 0.307"))
Read 20 items
> a.m <- matrix(a.m, ncol=4, byrow=TRUE)[, -1]  # ignore 1st column
>
> b.m <- scan(textConnection("    0.876 1.77 0.193 0.328
+ 0.891 1.009 0.238 1.004
+ 0.864 1.115 0.276 0.22
+ 0.887 1.306 0.166 0.239
+ 0.852 1.001 1.008 0.251"))
Read 20 items
> b.m <- matrix(b.m, ncol=4, byrow=TRUE)[, -1]
>
> c.m <- scan(textConnection(" 0.5  1.0   0.5  1.0
+ 1.0   1.0  1.0   0.5
+ 1.0   1.0  1.0   0.5
+ 1.0   1.0  1.0   0.5
+ 0.5   0.5  1.0   0.5"))
Read 20 items
> c.m <- matrix(c.m, ncol=4, byrow=TRUE)[, -1]
> closeAllConnections()
>
> result <- a.m * b.m
> # set to NA entries where c.m != 1
> is.na(result) <- c.m != 1
> result
         [,1]     [,2]     [,3]
[1,] 2.478000       NA 0.328328
[2,] 1.311700 0.027608       NA
[3,] 1.338000 0.304152       NA
[4,] 0.164556 0.184260       NA
[5,]       NA 1.010016       NA
>


On Dec 30, 2007 10:52 PM, Yingchen Wang <riddle_chen94 at yahoo.com> wrote:
> Hi, dear all:
>   I am a beginner. I appreciate any help or hint from you.
>   I am trying to do calculation with matrices. I have 3 matrices. One is matrixA, 2nd is matrixB, and last is matrixC.
>    Here is matrixA:
>       1.8511.40.0831.001
> 0.8771.30.1161.33
> 1.9021.21.1020.302
> 0.8640.1261.110.252
> 1.8230.2161.0020.307
>
> Next is matrixB:
>     0.8761.770.1930.328
> 0.8911.0090.2381.004
> 0.8641.1150.2760.22
> 0.8871.3060.1660.239
> 0.8521.0011.0080.251
>
> Last is matrixC:
>  0.5  1.0   0.5  1.0
> 1.0   1.0  1.0   0.5
> 1.0   1.0  1.0   0.5
> 1.0   1.0  1.0   0.5
> 0.5   0.5  1.0   0.5
>
> What I want is to match both matrixA and matrixB with matrixC. Ignore the 1st column in matrixA and matrixB. When the element im matrixC is 1.0, multilply the corresponding element in matrixA by the corresponding element in matrixB. Otherwise, ignore it. I wrote the following code with some error in the logic:
>
>
> A<-read.table('matrixA.txt', header=F)
> B<-read.table('matrixB.txt', header=F)
> C<-read.table('matrixC.txt', header=F)
> a1<-as.matrix(A[,1])
> arest<-as.matrix(A[,2:5])
> b1<-as.matrix(B[,1])
> brest<-as.matrix(B[,2:5])
> C<-as.matrix(C)
> if(C= =1){
> ra<-(arest*brest)
> }else {
> throw<-(arest*brest)
> }
> ra
>
> I got a warning message:
> Warning message:the condition has length > 1 and only the first element will be used in: if.....
>
> When I changed the if part to:
>  ifelse (C= =1,ra<-(arest*brest),throw<-(arest*brest))
>
> The result is the multiplication of each element in matrixA and matrixB.
>
> Anyone has an idea about what is wrong. Thanks
>
> Riddle
>
>
>      ____________________________________________________________________________________
> Be a better friend, newshound, and
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From slamb at slamb.org  Mon Dec 31 05:32:40 2007
From: slamb at slamb.org (Scott Lamb)
Date: Sun, 30 Dec 2007 20:32:40 -0800
Subject: [R] plot multiple data sets on same axis
In-Reply-To: <47784A9D.70101@slamb.org>
References: <477817A2.6080206@slamb.org>
	<644e1f320712301454q6a715a8chcd26922b50e1ba02@mail.gmail.com>
	<47784544.90709@slamb.org> <47784A9D.70101@slamb.org>
Message-ID: <477870E8.3000108@slamb.org>

Scott Lamb wrote:
> Scott Lamb wrote:
>> I've tried replacing the for loop body with this:
>>
>>     this_method <- split.df[[i]]
>>     boxplot(elapsed~inactive, data=this_method,
>>             add=TRUE, border=i, boxfill=i, outline=FALSE)
>>
>> but it has two problems:
>>
>> * it doesn't plot at the correct x values. It looks like I need to
>> supply a list of the x values as the "at" parameter, and I don't know
>> how to get unique values from this_method$inactive. (I tried the clunky
>> labels(split(this_method, this_method$inactive)), but it returns them as
>> strings.)
> 
> Ahh. I missed the obvious answer - there's a function called unique.
> at=unique(this_method$inactive) works.
> 
>> * it redraws the graph's frame, and it ignores bty="l" when doing so.
> 
> also the x axis tics and labels...they're totally unreadable now.

Ahh. There is an "axis=FALSE" parameter to bxp, which boxplot passes
along. Thanks again, and sorry for all the list noise.

For the record, here's exactly what I did to duplicate the original graph:

df <- read.csv("http://www.slamb.org/tmp/one-active.csv")
png(filename="one-active.png", width=800, height=600)
split.df <- split(df, df$method)
plot(0, xlim=c(0, max(df$inactive)), ylim=range(df$elapsed),
     ylab="time (?s)", xlab="inactive file descriptors", log="y",
     main="1 active descriptor, 1 write", bty="n", type="n")
grid()
for (i in seq_along(split.df)) {
    this_method <- split.df[[i]]
    unique_inactive <- unique(this_method$inactive)
    boxplot(elapsed~inactive, data=this_method,
            at=unique_inactive, axes=FALSE,
            add=TRUE, border=i, boxfill=i, outline=FALSE, bty="l",
            whisklty="solid", staplelty="blank", medlty="blank",
            boxwex=max(unique_inactive)/length(unique_inactive)/2)
}
legend("topleft", legend=labels(split.df),
       fill=seq_along(split.df), bty="n")

Cheers,
Scott

-- 
Scott Lamb <http://www.slamb.org/>


From daniel at umd.edu  Mon Dec 31 06:05:03 2007
From: daniel at umd.edu (Daniel Malter)
Date: Mon, 31 Dec 2007 00:05:03 -0500
Subject: [R] Survival analysis with no events in one treatment  group
In-Reply-To: <6.2.3.4.2.20071231133536.02ebaba0@pop.ozemail.com.au>
Message-ID: <200712310504.CJA69507@md2.mail.umd.edu>

Hi John,

to me it still seems that you have two "problems", given your first email.
The first one, if I am correct, is that you have NO admissions in one of the
groups. That is, you have Treat A, B, C, D and for one of these treatments
there is not a single admission. Then you cannot estimate a survival
function for this group because nobody died (got hospitalized). I think you
have to exclude this group because your survival rate is 100% and your
hazard rate 0%. But more proficient people may have better advice with that.

That subject 2 did not get hospitalized is not a problem though. The
observation is censored at 12 months. Since there are more patients in his
treatment group (subject 1) who had an event, the survival function can be
estimated. 

To the multiple admissions problem: To account for the fact that some have
more than one event you may create a "number of events" variable, say
numEvents, and include that in a strata() argument to your regression call:

surv.obj<-Surv(time=time1,time2=time2,event=event,type="counting")
model<-coxph(surv.obj~Treatment+strata(numEvents))

Use strata if you think that your baseline hazard is different in the
different strata (in your case: if you think that the baseline hazard of
having an event differs with having had prior event(s)). At the same time
you assume that your treatment effect  - the beta on your Treat variable -
is the same across all strata. If you have reason to assume that the effect
of your treatment varies (interacts) with the number of prior events, then
it is not the correct approach. In addition you may include the
cluster(Subject) or frailty(Subject) commands.

Instead of strata you might also consider using a dummy variable, coding the
number of prior events (if the maximum number admissions per patient is
reasonably small and the number of cell frequencies reasonably large).

Finally, you may want to consult Terry Thernau and Patricia Grambsch's book
"Modeling survival data". It shows how to apply the techniques in R/S-Plus.
I find it invaluable.

Does that help you?

Daniel


-------------------------
cuncta stricte discussurus
-------------------------

-----Urspr?ngliche Nachricht-----
Von: John Field [mailto:JohnField at ozemail.com.au] 
Gesendet: Sunday, December 30, 2007 10:14 PM
An: Daniel Malter; R-help at r-project.org
Betreff: Re: AW: [R] Survival analysis with no events in one treatment group

Hi Daniel,

Sorry, it may have been clearer if I had used "subjects" instead of
"patients".  The treatments were administered to all subjects, and then in
the succeeding 12 months, some were hospitalised and some were not.  Hence
only about 25% of the subjects were hospitalised.

The start of the data:
subj    time1   time2   event Treat
1       0       6.2     1       A
1       6.2     12      0       A
2       0       12      0       A

so subject 1 was hospitalised at 6.2 months, subject 2 not at all.

Hope this makes it clearer.
Regards,
John





.At 12:59 PM 31/12/2007, Daniel Malter wrote:
>Hi John,
>
>I am on the slow side - can you provide sample code. How can one 
>treatment group have no admissions?
>
>Let's say there are treatments W, X, Y, Z. Do you mean that NONE of the 
>patients who got admitted the first time and, say, received treatment X 
>during the first admission, have ever had a second admission (in your
data).
>And for the other treatments W, Y, and Z some of those who got admitted 
>the first time came in a second time?
>
>Cheers and a happy new year's eve,
>Daniel
>
>-------------------------
>cuncta stricte discussurus
>-------------------------
>
>-----Urspr?ngliche Nachricht-----
>Von: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] 
>Im Auftrag von John Field
>Gesendet: Sunday, December 30, 2007 9:16 PM
>An: R-help at r-project.org
>Betreff: [R] Survival analysis with no events in one treatment group
>
>I'm trying to fit a Cox proportional hazards model to some hospital 
>admission data.  About 25% of the patients have had at least one 
>admission, and of these, 40% have had two admissions within the 12 
>month period of the study.  Each patients has had one of 4 treatments, 
>and one of the treatment groups has had no admissions for the period.  I
used:
>
>surv.obj<-Surv(time=time1,time2=time2,event=event,type="counting")
>model<-coxph(surv.obj~Treatment+cluster(Subject))
>
>and, as explained in the coxph help page, I get a warning message about 
>convergence because the MLE of one of the coefficients is infinite 
>since there are no admissions in one group.
>
>I'm looking for suggestions about how to proceed with an analysis of 
>these data.  I'd prefer not to ignore the fact that there are multiple 
>admissions, but any alternative ideas I have at the moment do this.
>
>Many thanks,
>John Field
>=================================
>Faculty of Health Sciences Statistical Support Service The University 
>of Adelaide, Australia 5005
>
>______________________________________________
>R-help at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jgalkows at akamai.com  Mon Dec 31 06:14:45 2007
From: jgalkows at akamai.com (Galkowski, Jan)
Date: Mon, 31 Dec 2007 00:14:45 -0500
Subject: [R] the woes of NA
In-Reply-To: <mailman.12.1198926002.22678.r-help@r-project.org>
References: <mailman.12.1198926002.22678.r-help@r-project.org>
Message-ID: <76EB4827B2104D40AE7E43AA5D8582EA481433@MAVS1.kendall.corp.akamai.com>

Joyful.

I'm adapting a FORTRAN 77 package for use with R.  Pretty
straightforward.  

Except for a glitch it took me some time to figure out.  This existing
package has subroutines which have parameters called "NA". So, I called
subroutines like

bnodes <- function(n, lst, lptr, lend, nodes, nb, na, nt)
{
  ensure.all.numeric(list(n, lst, lptr, lend, nodes, nb, n.a, nt), 
                     "all arguments to -bnodes- must be numeric")
  out <- .Fortran("bnodes", N=as.integer(n), LIST=as.integer(lst),
                  LPTR=as.integer(lptr), LEND=as.integer(lend),
NODES=as.integer(nodes),
                  NB=as.integer(nb), NA=as.integer(na),
NT=as.integer(nt))
  return(out[5:8])
}

I had called routines successfully before, so I couldn't figure out what
was wrong.  By elimination, I discovered that the parameter pass

    NA=as.integer(na)

was to blame.  So, thinking the right-hand-side (R's world) was the
problem, even if "na" wasn't recognized as "not available", I changed
to:

bnodes <- function(n, lst, lptr, lend, nodes, nb, n.a, nt)
{
  ensure.all.numeric(list(n, lst, lptr, lend, nodes, nb, n.a, nt), 
                     "all arguments to -bnodes- must be numeric")
  out <- .Fortran("bnodes", N=as.integer(n), LIST=as.integer(lst),
                  LPTR=as.integer(lptr), LEND=as.integer(lend),
NODES=as.integer(nodes),
                  NB=as.integer(nb), NA=as.integer(n.a),
NT=as.integer(nt))
  return(out[5:8])
}

No win.  I would only be happy if I used

bnodes <- function(n, lst, lptr, lend, nodes, nb, n.a, nt)
{
  ensure.all.numeric(list(n, lst, lptr, lend, nodes, nb, n.a, nt), 
                     "all arguments to -bnodes- must be numeric")
  out <- .Fortran("bnodes", N=as.integer(n), LIST=as.integer(lst),
                  LPTR=as.integer(lptr), LEND=as.integer(lend),
NODES=as.integer(nodes),
                  NB=as.integer(nb), NAA=as.integer(n.a),
NT=as.integer(nt))
  return(out[5:8])
}

and had to actually change the FORTRAN code to comply.

Sounds to me like there's a little room for improvement here.  Should be
documented anyway.


From leeznar at yahoo.com.tw  Mon Dec 31 06:22:53 2007
From: leeznar at yahoo.com.tw (Hsin-Ya Lee)
Date: Sun, 30 Dec 2007 21:22:53 -0800 (PST)
Subject: [R] How to catch data from the different dataframes and lm
 problem?
In-Reply-To: <OF7C8183DE.A73E87C1-ON802573BF.0030013E-802573BF.0031060C@hsl.gov.uk>
References: <692075.69409.qm@web73608.mail.tp2.yahoo.com>
	<OF7C8183DE.A73E87C1-ON802573BF.0030013E-802573BF.0031060C@hsl.gov.uk>
Message-ID: <14554462.post@talk.nabble.com>


Dear Richie:

1) I have a mistake in the code that ?A.split[[1]][["time"]][i]? should
replace with ?A.split[[j]][["time"]][i]?.  The ?test? value is ?auc? which
is a cumulative rate of change of concentration with respect to time.  

for (j in 1:length(A.split)){
test <- 0
for(i in 2:length(A.split[[j]][["time"]])){
    test[i] <- (A.split[[j]][["time"]][i] - A.split[[j]][["time"]][i-1]) *
(A.split[[j]][["concentration"]][i] - A.split[[j]][["concentration"]][i-1])*
0.5
    test[i]<-test[i]+test[i-1]
 }
output<-data.frame(A.split[[j]][["subject"]],A.split[[j]][["formulation"]],A.split[[j]][["time"]],A.split[[j]][["concentration"]],test)
colnames(output)<-list("subject","formulation","time","concentration","test")
show(output) 
}

subject formulation time   concentration(X) test(Y)
1       1           1    1           0.1  0.00
2       1           1    2          10.0  4.95
3       1           1    3          20.0  9.95
  subject formulation time concentration(X) test(Y)
1       1           2    1            20  0.0
2       1           2    2            25  2.5
3       1           2    3            60  20.0
  subject formulation time concentration(X) test(Y)
1       2           1    1           0.3  0.00
2       2           1    2           2.5  1.10
3       2           1    3           6.0  2.85
  subject formulation time concentration(X) test(Y)
1       2           2    1            35  0.0
2       2           2    2            40  2.5
3       2           2    3            45  5.0

2) Then, I want to pool all "concentration" (X) and pool all "test" (Y) to
perform a 
regression.  For example, I used the ?regression? function of Microsoft
Excel 2003 and intercept is -0.01894 and X is 0.185758.  I think that if I
can catch ?test? (Y) values and ?concentration? (X) values into a dataframe,
then I can use ?lm? to fit linear models.  So, how to catch all ?test?
values from different dataframes?  Or what should I do? 

Best regards,

Hsin-Ya Lee

-- 
View this message in context: http://www.nabble.com/How-to-catch-data-from-the-different-dataframes-and-lm-problem--tp14521967p14554462.html
Sent from the R help mailing list archive at Nabble.com.


From leeznar at yahoo.com.tw  Mon Dec 31 07:49:09 2007
From: leeznar at yahoo.com.tw (Hsin-Ya Lee)
Date: Sun, 30 Dec 2007 22:49:09 -0800 (PST)
Subject: [R] How to catch data from the different dataframes and lm
 problem?
In-Reply-To: <14554462.post@talk.nabble.com>
References: <692075.69409.qm@web73608.mail.tp2.yahoo.com>
	<OF7C8183DE.A73E87C1-ON802573BF.0030013E-802573BF.0031060C@hsl.gov.uk>
	<14554462.post@talk.nabble.com>
Message-ID: <14554872.post@talk.nabble.com>


Dear all:

I used lm to perform a regression in R and and intercept is -0.01894 and X
is 0.185758.  This is my target.  
Y<-(c(0,4.95,9.95,0,2.5,20,0,1.1,2.85,0,2.5,5))
X<-(c(0.1,10,20,20,25,60,0.6,2.5,6,35,40,45))
lm(Y~X)
anova(wnlm<-lm( Y~X))
summary(wnlm<-lm( Y~X))

I catch test (Y) values and concentration (X) values by myself.  Then, here
is my question.  How to ?auto-catch? test (Y) values and concentration (X)
values from the split dataframes?  Because I think that if I can catch
?test? (Y) values and ?concentration? (X) values into a dataframe, then I
can use ?lm? to fit linear models.  So, how to catch all ?test? values from
different dataframes?  Or what should I do? 

Best regards,

Hsin-Ya Lee


-- 
View this message in context: http://www.nabble.com/How-to-catch-data-from-the-different-dataframes-and-lm-problem--tp14521967p14554872.html
Sent from the R help mailing list archive at Nabble.com.


From baganda at mac.com  Mon Dec 31 08:30:32 2007
From: baganda at mac.com (Danstan Bagenda)
Date: Mon, 31 Dec 2007 02:30:32 -0500
Subject: [R] read.dta error after OSX  change in time zone
Message-ID: <9A6D301A-E243-4211-A929-6A09345656D4@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/9685ad4e/attachment.pl 

From r.mueller at oeko-sorpe.de  Mon Dec 31 09:06:19 2007
From: r.mueller at oeko-sorpe.de (Richard =?iso-8859-1?q?M=FCller?=)
Date: Mon, 31 Dec 2007 09:06:19 +0100
Subject: [R] tcltk again (solved)
In-Reply-To: <200712301755.33849.r.mueller@oeko-sorpe.de>
References: <200712291817.42974.r.mueller@oeko-sorpe.de>
	<B3F75955-3AF3-4899-95AE-88E74FC3B8E1@gmail.com>
	<200712301755.33849.r.mueller@oeko-sorpe.de>
Message-ID: <200712310906.19578.r.mueller@oeko-sorpe.de>

> I wrote the following code:
> X11()
> # some code to generate a plot on the screen omitted
>
> res <-    tkmessageBox(title="Finish?",
>       message="save as PDF?",
>       icon="question", type="okcancel")
>
> if (tclvalue(res) == "ok")
>     Datei <- tkgetSaveFile(initialdir="temp/",defaultextension=".pdf",
>     initialfile="Haupt_Chl_Phaeo.pdf")
>     dev.copy(pdf, tclvalue(Datei))
> else graphics.off()
> graphics.off()
>
> In the moment I'm saving I dont have the pdf generated yet. 

That's wrong. It works perfectly. Thanks to all contributors. Happy New Year!
 Greetings
 Richard

-- 
Richard M?ller - Am Spring 9 - D-58802 Balve-Eisborn
www.oeko-sorpe.de


From vincent.nijs at gmail.com  Sun Dec 30 20:07:55 2007
From: vincent.nijs at gmail.com (Vincent)
Date: Sun, 30 Dec 2007 11:07:55 -0800 (PST)
Subject: [R] Trying to install rjags on Mac OS X 10.5
In-Reply-To: <20071230175611.yhhn5stjk8gs0c8k@webmail.manchester.ac.uk>
References: <20071230162711.s4q4h4pkg04o8c84@webmail.manchester.ac.uk> 
	<Pine.LNX.4.64.0712301649420.30788@gannet.stats.ox.ac.uk>
	<20071230175611.yhhn5stjk8gs0c8k@webmail.manchester.ac.uk>
Message-ID: <4df88146-c222-46fe-83ef-6e1115fec4e0@j20g2000hsi.googlegroups.com>

I had the same issue on my Mac. After install pkg-config from MacPorts
Martyn suggested to add the following 2 lines to my .bashrc file:

export LD_LIBRARY_PATH="/usr/local/lib"
export PKG_CONFIG_PATH="/usr/local/lib/pkgconfig"

Everything worked after that.

Best,

Vincent


On Dec 30, 11:56 am, Lindsay Stirton
<Lindsay.Stir... at manchester.ac.uk> wrote:
> Quoting Prof Brian Ripley <rip... at stats.ox.ac.uk>:
>
> > and on my Mac the pkg-config files are in /usr/local/lib/pkgconfig
> > and /usr/lib/pkgconfig.  Most likely it would work if PKG_CONFIG_PATH
> > is not set, so try that first.
>
> Thanks--but this still doesn't seem to get me anywhere (see below).
>
> Macintosh:Desktop ljs$ which jags
> /usr/local/bin/jags
> Macintosh:Desktop ljs$ whereis jags
> Macintosh:Desktop ljs$
>
> Last login: Sun Dec 30 17:40:41 on ttys001
> Macintosh:~ ljs$ echo $PKG_CONFIG_PATH
>
> Macintosh:~ ljs$ cd Desktop/
> Macintosh:Desktop ljs$ sudo R CMD INSTALL rjags_1.0.1-1.tar.gz
> * Installing to library '/Library/Frameworks/R.framework/Resources/library'
> * Installing *source* package 'rjags' ...
> checking for pkg-config... /opt/local/bin/pkg-config
> checking pkg-config is at least version 0.9.0... yes
> checking for JAGS... configure: error: Package requirements (jags =
> 1.0.1) were
> not met:
>
> No package 'jags' found
>
> Consider adjusting the PKG_CONFIG_PATH environment variable if you
> installed software in a non-standard prefix.
>
> ERROR: configuration failed for package 'rjags'
> ** Removing '/Library/Frameworks/R.framework/Resources/library/rjags'
> Macintosh:Desktop ljs$
>
> Would I be right in thinking that the problem is not in fiding the path to
> pkg-config but in finding JAGS? I have
>
> Macintosh:Desktop ljs$ which jags
> /usr/local/bin/jags
>
> Best wishes,
>
> Lindsay Stirton
>
> ______________________________________________
> R-h... at r-project.org mailing listhttps://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From edd at debian.org  Sat Dec 29 18:27:56 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 29 Dec 2007 11:27:56 -0600
Subject: [R] A function for random test based on longest run
	(UNCLASSIFIED)
In-Reply-To: <Pine.LNX.4.64.0712291653420.345@gannet.stats.ox.ac.uk>
References: <8d5a36350712270908h574d6ff4y83b648e158e5da6e@mail.gmail.com>
	<18291.58470.122385.233652@ron.nulle.part>
	<Pine.LNX.4.64.0712291653420.345@gannet.stats.ox.ac.uk>
Message-ID: <18294.33692.186531.184645@ron.nulle.part>


Brian,  

On 29 December 2007 at 17:03, Prof Brian Ripley wrote:
| On Thu, 27 Dec 2007, Dirk Eddelbuettel wrote:
| 
| >
| > On 27 December 2007 at 12:08, bogdan romocea wrote:
| > |   > require(tseries)
| > |   > ?runs.test
| > | Also, take a look at dieharder, it implements a large number of
| > | randomness tests:
| > | http://www.phy.duke.edu/~rgb/General/dieharder.php
| >
| > Also note that CRAN has an RDieHarder package that provides access to
| > DieHarder tests from R.  RDieHarder is currently happier on Linux than on
| > Windows. If anybody wants to contribute build instructions for Windows ...
| 
| It's not easy on Linux!  There is a src/Makefile that ignores most of the 
| info from how R was built, including CC, CFLAGS andlibrary pats such as 
| /usr/local/lib64.  

Ack, that needs fixing for amd64 etc. The rest worked for us...

| And that's if you can get dieharder built: it gave me 
| lots of errors before succeeding.  

FWIW, Robert, DieHarder's principal author (CC'ed), works on a Fedora system,
and we know of successful ports to (at least) BSD.  So unless you tell us how
it fails there is little we can do.

| I managed to build from the SRPM, but 
| that installed with the include files without read permissions.
|
| But I've succeeded on Windows,

Excellent!  I had one quick look a few month ago on my MinGW setup at work
and ... didn't have enough (and interest) to make it work.  But having a
working RDieHarder on Windows would be very useful, so thanks!

| and RDieHarder is now in the CRANextras 
| collection (and so can be installed from the menus etc). I'll send Dirk 
| separately notes on what I had to do to make it work.

I look forward to receiving those.  We also got patches from Anton
Korobeynikov (CC'ed) for building DieHarder on Windoze but I haven't had time
to look at those.

Thanks, Dirk

-- 
Three out of two people have difficulties with fractions.


From thomas_schwander at web.de  Sat Dec 29 19:25:09 2007
From: thomas_schwander at web.de (Thomas Schwander)
Date: Sat, 29 Dec 2007 19:25:09 +0100
Subject: [R] Avoiding "." when importing from csv
Message-ID: <E1J8gMz-0004zc-00@smtp07.web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071229/443c4bce/attachment.pl 

From camping9 at hotmail.com  Sat Dec 29 22:31:43 2007
From: camping9 at hotmail.com (LUZhiping)
Date: Sat, 29 Dec 2007 21:31:43 +0000
Subject: [R] how to run my R code in Mac OS X 10.5 Leopard
In-Reply-To: <mailman.0.1198962941.26350.r-help@r-project.org>
References: <mailman.0.1198962941.26350.r-help@r-project.org>
Message-ID: <BLU102-W564CBDAA4D6A53EF4175FE89560@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071229/e78af48a/attachment.pl 

From perrone at rocketmail.com  Sun Dec 30 02:20:30 2007
From: perrone at rocketmail.com (Ricardo Perrone)
Date: Sat, 29 Dec 2007 17:20:30 -0800 (PST)
Subject: [R] joint probability
Message-ID: <155901.20531.qm@web34311.mail.mud.yahoo.com>

Hi,

I need some instructoins about:

Given two larger vectors(A and B) with discrite time values, and considering that each vector was ordered previously.  how to compute the joint probability for each pair of values, considering the restriction that each element in A(Ai) is combined only with elements in B(Bi) that are lower than Ai? what is the more appropriate package/function to manipulate this kind of probability?

Thanks a lot 
Ricardo




      ____________________________________________________________________________________
Be a better friend, newshound, and


From danbebber at forestecology.co.uk  Sun Dec 30 11:14:27 2007
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Sun, 30 Dec 2007 10:14:27 -0000
Subject: [R] plot.augPred with grouping factors
Message-ID: <000001c84acc$c3e24970$0201a8c0@DPB9400>

I fitted the following model:
dgr.lme2 <- lme(Average ~ poly(Time, 2) * Treatment, random = ~1|Replicate,
data = dgr, correlation = corAR1(), method = "ML")

I plotted the data in separate panels for each Treatment:
xyplot(Average ~ Time|Treatment, groups = Replicate, data = dgr, type =
c("p","l"), ylab = expression(paste(Delta, G)))

I would now like to overlay the population-level predictions from dgr.lme2
onto each Treatment-level panel.

How can I tell plot.augPred to do this? Or do I need a bespoke panel
function?
(plot.augPred gives panels for each Replicate by default)

Many thanks,
Dan Bebber


Checked by AVG Free Edition. 

11:51


From office at matthiaswendel.de  Sun Dec 30 19:04:06 2007
From: office at matthiaswendel.de (Matthias Wendel)
Date: Sun, 30 Dec 2007 19:04:06 +0100
Subject: [R] Another problem with encoding
Message-ID: <004301c84b0e$5ee7e420$15b2a8c0@lifebook2>

Hi
    I've imported an spss-file using read.spss. One variable has value like '?rzte'. I thought this is UTF-8 encoded, but it is not
(as the results of iconv and utf8ToInt suggest). Is there any way to find out how these spss-values are encoded?
Regards,
Matthias


From p.dalgaard at biostat.ku.dk  Mon Dec 31 10:33:52 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 31 Dec 2007 10:33:52 +0100
Subject: [R] the woes of NA
In-Reply-To: <76EB4827B2104D40AE7E43AA5D8582EA481433@MAVS1.kendall.corp.akamai.com>
References: <mailman.12.1198926002.22678.r-help@r-project.org>
	<76EB4827B2104D40AE7E43AA5D8582EA481433@MAVS1.kendall.corp.akamai.com>
Message-ID: <4778B780.5090102@biostat.ku.dk>

Galkowski, Jan wrote:
> Joyful.
>
> I'm adapting a FORTRAN 77 package for use with R.  Pretty
> straightforward.  
>
> Except for a glitch it took me some time to figure out.  This existing
> package has subroutines which have parameters called "NA". So, I called
> subroutines like
>
> bnodes <- function(n, lst, lptr, lend, nodes, nb, na, nt)
> {
>   ensure.all.numeric(list(n, lst, lptr, lend, nodes, nb, n.a, nt), 
>                      "all arguments to -bnodes- must be numeric")
>   out <- .Fortran("bnodes", N=as.integer(n), LIST=as.integer(lst),
>                   LPTR=as.integer(lptr), LEND=as.integer(lend),
> NODES=as.integer(nodes),
>                   NB=as.integer(nb), NA=as.integer(na),
> NT=as.integer(nt))
>   return(out[5:8])
> }
>
> I had called routines successfully before, so I couldn't figure out what
> was wrong.  By elimination, I discovered that the parameter pass
>
>     NA=as.integer(na)
>
> was to blame.  So, thinking the right-hand-side (R's world) was the
> problem, even if "na" wasn't recognized as "not available", I changed
> to:
>
> bnodes <- function(n, lst, lptr, lend, nodes, nb, n.a, nt)
> {
>   ensure.all.numeric(list(n, lst, lptr, lend, nodes, nb, n.a, nt), 
>                      "all arguments to -bnodes- must be numeric")
>   out <- .Fortran("bnodes", N=as.integer(n), LIST=as.integer(lst),
>                   LPTR=as.integer(lptr), LEND=as.integer(lend),
> NODES=as.integer(nodes),
>                   NB=as.integer(nb), NA=as.integer(n.a),
> NT=as.integer(nt))
>   return(out[5:8])
> }
>
> No win.  I would only be happy if I used
>
> bnodes <- function(n, lst, lptr, lend, nodes, nb, n.a, nt)
> {
>   ensure.all.numeric(list(n, lst, lptr, lend, nodes, nb, n.a, nt), 
>                      "all arguments to -bnodes- must be numeric")
>   out <- .Fortran("bnodes", N=as.integer(n), LIST=as.integer(lst),
>                   LPTR=as.integer(lptr), LEND=as.integer(lend),
> NODES=as.integer(nodes),
>                   NB=as.integer(nb), NAA=as.integer(n.a),
> NT=as.integer(nt))
>   return(out[5:8])
> }
>
> and had to actually change the FORTRAN code to comply.
>
> Sounds to me like there's a little room for improvement here.  Should be
> documented anyway.
>   
Do the argument names even get used on the Fortran side?? AFAIK, it only 
matters for labeling the result.

Anyways, this has nothing to do with "not available", a name like PACK 
would get you equally confused. You are being bitten by partial argument 
matching: NA matches NAOK. One workaround is to add NAOK=FALSE 
explicitly to the call. Or just use lowercase names.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p.dalgaard at biostat.ku.dk  Mon Dec 31 10:45:25 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 31 Dec 2007 10:45:25 +0100
Subject: [R] Another problem with encoding
In-Reply-To: <004301c84b0e$5ee7e420$15b2a8c0@lifebook2>
References: <004301c84b0e$5ee7e420$15b2a8c0@lifebook2>
Message-ID: <4778BA35.1030704@biostat.ku.dk>

Matthias Wendel wrote:
> Hi
>     I've imported an spss-file using read.spss. One variable has value like '?rzte'. I thought this is UTF-8 encoded, but it is not
> (as the results of iconv and utf8ToInt suggest). Is there any way to find out how these spss-values are encoded?
>   
You are assuming a bit much of your readers.

What exactly are you doing? Is it a value, a value label, or perhaps a 
variable name. How do the results of read.spss look on the R side? How 
did you apply iconv and utf8ToInt? What is your locale?

I mean, we could try and guess all those details, but you are the one 
with the hard info, and the motivation...

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Mon Dec 31 11:14:28 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 31 Dec 2007 10:14:28 +0000 (GMT)
Subject: [R] Avoiding "." when importing from csv
In-Reply-To: <E1J8gMz-0004zc-00@smtp07.web.de>
References: <E1J8gMz-0004zc-00@smtp07.web.de>
Message-ID: <Pine.LNX.4.64.0712311011530.30718@gannet.stats.ox.ac.uk>

See the help, especially argument 'check.names'.

On Sat, 29 Dec 2007, Thomas Schwander wrote:

> Hi guys, another question today:
>
>
>
> If I import an external csv-file with headers (with read.csv2), the blanks
> are interpreted as points. So "Test 1" becomes "Test.1".
>
>
>
> Is there a way to change this?
>
>
>
> Thanks,
>
> Thomas
>
>
>
> I'm using XP and R 2.6.1
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do (no HTML, for example).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wwwhsd at gmail.com  Mon Dec 31 11:20:43 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 31 Dec 2007 08:20:43 -0200
Subject: [R] updating R version and packages.
In-Reply-To: <486910.42845.qm@web56009.mail.re3.yahoo.com>
References: <486910.42845.qm@web56009.mail.re3.yahoo.com>
Message-ID: <da79af330712310220k33027b07o2b96776199c39073@mail.gmail.com>

See ?update.packages function.

On 30/12/2007, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
> Dear All,
>
> Is there a way of I update automatically the R version and the respective packages which I have installed on my computer? Case not, how can I know about what packages I installed by my self on the computer?
>
> All have a nice new year.
>
> from miltinho, Brazil.
>
>
>
>  para armazenamento!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


From ripley at stats.ox.ac.uk  Mon Dec 31 11:34:02 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 31 Dec 2007 10:34:02 +0000 (GMT)
Subject: [R] read.dta error after OSX  change in time zone
In-Reply-To: <9A6D301A-E243-4211-A929-6A09345656D4@mac.com>
References: <9A6D301A-E243-4211-A929-6A09345656D4@mac.com>
Message-ID: <Pine.LNX.4.64.0712311023420.31150@gannet.stats.ox.ac.uk>

This _is_ Mac-specific, so please use R-sig-mac.

It looks like changing the time zone is changing the behaviour of the 
system function strptime.  But we need a reproducible example (see the 
footer of this and every message).  You can probably create one by 
debugging the failing call and seeing what string it is that fromchar() is 
trying to interpret (and if this is called from as.Date.character or 
as.POSIXlt).  If that's beyond you, you will need to make your file 
available to us.

Please post full details as requested in the posting guide and the 
requested example on R-sig-mac.

On Mon, 31 Dec 2007, Danstan Bagenda wrote:

> Hello all,
>
> I recently moved from the US to Africa  &  on changing my time zone
> in the OSX system preferences from EST to GMT+3 ( East Africa)  on
> attempting to use read.dta  using the foreign package    began
> getting an error  of:
>
> "Error in fromchar(x) : character string is not in a standard
> unambiguous format"
>
> This error goes away & the data is correctly read in when I revert to
> the EST  time zone.
>
> I installed & am currently using R  Version 1.19-pre (devel, r4291)
> (4291)  while I was still in the US.

There is no such R version: that is the version of R.app.  Please see the 
posting guide for the information we do need.  I don't think this the 
devel version of R.

> I am running OS X 10.4.11 on an intel core duo Macbbook pro.
>
> I would appreciate any help .  I have tried to search the list & FAQ
> with no solution to this problem.
>
> Happy 2008.
>
> Danstan Bagenda
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pedrosmarques at portugalmail.pt  Mon Dec 31 12:55:21 2007
From: pedrosmarques at portugalmail.pt (pedrosmarques at portugalmail.pt)
Date: Mon, 31 Dec 2007 11:55:21 +0000
Subject: [R] SVM error
Message-ID: <1199102121.4778d8a9e2c26@gold5.portugalmail.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/e8973fa8/attachment.pl 

From Lindsay.Stirton at manchester.ac.uk  Mon Dec 31 15:34:47 2007
From: Lindsay.Stirton at manchester.ac.uk (Lindsay Stirton)
Date: Mon, 31 Dec 2007 14:34:47 +0000
Subject: [R] Trying to install rjags on Mac OS X 10.5
In-Reply-To: <EA00A37F-B9F7-4981-9ACA-CA535B27BD3C@mac.com>
References: <20071230162711.s4q4h4pkg04o8c84@webmail.manchester.ac.uk>
	<EA00A37F-B9F7-4981-9ACA-CA535B27BD3C@mac.com>
Message-ID: <20071231143447.cnmj67156gwkckw8@webmail.manchester.ac.uk>

Thanks Rob,

Since I already had Xcode 3.0, I re-installed gfortran (from the R 
website, not
from the .dmg) and gcc4.2.1 preview. I am afraid the outcome was the 
same, i.e.
jags 1.0.1 not found.

Yes I do use Macports (and formery used Fink, which is still installed). What
issues does this raise? On your suggestion, I made sure that both were up to
date--still no success.

Your offer to send me a template directory to figure things out from is very
much appreciated. Yes please!

Lindsay


Quoting Rob Goedman <goedman at mac.com>:

> Hi Lindsay,
>
> Did have some difficulties as well, but got it to work using 
> Xcode3.0,  Apple's gcc4.2preview release and gfortran 4.2.1 from 
> Simon's web  site. This was on R-devel though, on a Mac Intel.
>
> Can it find the jags executable say from your home directory? Have 
> you  tried 'make check' in the classic-bugs/vol1 or vol2 example dirs 
> (in  the Terminal)? That would indeed confirm jags is running fine.
>
> For rjags, I am a bit worried about the /opt/... . Did you at some  
> point install software from Fink or Darwin ports? Until I installed  
> the above newer toolset, I had never been able to get rjags to work.
>
> Of course you don't really need rjags, only coda. I have also  
> installed the package 'arm' and the packages arm depends on (Matrix,  
> lattice, lme4, R2WinBugs). If it would help I can send you a  
> 'template' dir for the line example that does not use rjags.
>
> A dependency I ran into was coda v 13.1, as of a couple of days ago  
> not available as a binary package,
> but available as a source package on CRAN.
>
> Rob
>
> On Dec 30, 2007, at 8:27 AM, Lindsay Stirton wrote:
>
>> Greetings,
>>
>> I wonder if anyone can offer any help or advice--even direction to an
>> appropriate source of advice. I am trying to install rjags 1.0.1 on  
>> Mac OS X
>> 10.5 (see http://www-fis.iarc.fr/~martyn/software/jags/). I have  
>> R.app 2.6.1
>> installed.
>>
>> JAGS 1.0.1 is apparently successfully installed. I (think I) know  
>> this because
>> when I type 'jags' into Terminal, I get the following:
>>
>> Macintosh:Desktop ljs$ jags
>> Welcome to JAGS 1.0.1 on Thu Dec 27 20:56:09 2007
>> JAGS is free software and comes with ABSOLUTELY NO WARRANTY
>> Loading module: basemod
>> Loading module: bugs
>> .
>>
>> However, when I try to install rjags, I get the following problem.
>>
>> Macintosh:Desktop ljs$ sudo R CMD INSTALL rjags_1.0.1-1.tar.gz
>> Password:
>> * Installing to library '/Library/Frameworks/R.framework/Resources/ library'
>> * Installing *source* package 'rjags' ...
>> checking for pkg-config... /opt/local/bin/pkg-config
>> checking pkg-config is at least version 0.9.0... yes
>> checking for JAGS... configure: error: Package requirements (jags =  
>> 1.0.1) were
>> not met:
>>
>> No package 'jags' found
>>
>> Consider adjusting the PKG_CONFIG_PATH environment variable if you
>> installed software in a non-standard prefix.
>>
>> ERROR: configuration failed for package 'rjags'
>> ** Removing '/Library/Frameworks/R.framework/Resources/library/rjags'
>>
>> I am really not sure what the problem is. I have (I think) set the  proper
>> environment variables correctly. See below:
>>
>> Macintosh:Desktop ljs$ which jags
>> /usr/local/bin/jags
>> Macintosh:Desktop ljs$ echo $PKG_CONFIG_PATH
>> /usr/local/bin
>>
>> I should say that I had many other problems
>> along the way--gfortran not originally installed properly, R CMD  INSTALL
>> couldn't find pkg-config. However, the above description shows the  problems
>> remaining even after I have done a little homework.
>>
>> Best wishes,
>>
>> Lindsay Stirton
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From f.harrell at vanderbilt.edu  Mon Dec 31 15:47:10 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 31 Dec 2007 08:47:10 -0600
Subject: [R] Avoiding "." when importing from csv
In-Reply-To: <Pine.LNX.4.64.0712311011530.30718@gannet.stats.ox.ac.uk>
References: <E1J8gMz-0004zc-00@smtp07.web.de>
	<Pine.LNX.4.64.0712311011530.30718@gannet.stats.ox.ac.uk>
Message-ID: <477900EE.4090609@vanderbilt.edu>

Prof Brian Ripley wrote:
> See the help, especially argument 'check.names'.
> 
> On Sat, 29 Dec 2007, Thomas Schwander wrote:
> 
>> Hi guys, another question today:
>>
>>
>>
>> If I import an external csv-file with headers (with read.csv2), the blanks
>> are interpreted as points. So "Test 1" becomes "Test.1".
>>
>>
>>
>> Is there a way to change this?
>>
>>
>>
>> Thanks,
>>
>> Thomas

Depending on the ultimate need, you can also use the csv.get function in 
the Hmisc package.  csv.get keeps the original column names as variable 
labels, then transforms the names to become legal R names as with 
read.csv.  In the new version there is also an option to read the 
variable labels from another row.

Frank Harrell


From gunter.berton at gene.com  Mon Dec 31 15:53:04 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 31 Dec 2007 06:53:04 -0800
Subject: [R] refering to variable names in lm where the variable name
	isin	another variable
In-Reply-To: <1323C474-2C6E-4C0B-A75D-84A83433FF82@gmail.com>
References: <477785C60200005A00009600@co5.dnr.state.mn.us>
	<1323C474-2C6E-4C0B-A75D-84A83433FF82@gmail.com>
Message-ID: <000901c84bbc$d9632f30$6701a8c0@gne.windows.gene.com>

The semantic gymnastics below aren't necessary (nor are necessarily they a
bad idea). This is basically FAQ 7.21. So, e.g.

z <- c("x1","x2")
mod <- lm(y ~ get(z[1]) + get(z[2]))

Bert Gunter
Genentech


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] On
Behalf Of Charilaos Skiadas
Sent: Sunday, December 30, 2007 10:23 AM
To: Daniel O'Shea
Cc: r-help at r-project.org
Subject: Re: [R] refering to variable names in lm where the variable name
isin another variable

On Dec 30, 2007, at 12:49 PM, Daniel O'Shea wrote:

> I am trying to refer to a variable name in a lm regression where  
> the variable name is in another variable, but it does seem to  
> work.  Here is an example:
>
> y<-rnorm(10)
> dat<-data.frame(x1=rnorm(10),x2=rnorm(10),x3=rnorm(10))
> nam<-c('x1','x2','x3')
> library(gtools)
> com<-combinations(3,2,1:3)
> mod<-lm(y~nam[com[1,1]],data=dat)
>
> #error in model frame....:variable lengths differ().
>
> I also get the error if i just refer to variable x1 as nam[1] in  
> the lm. any suggestions.  I am trying to set up a for loop that  
> will perform an all subsets regression and calculate the AIC for each.

There's probably a number of ways to go about it. The problem is that  
nam[1] is a string vector, while you want the underlying "symbol". In  
your case, I think the simplest solution would be:

frm <- formula(paste("y~",nam[1]))
mod<-lm(frm,data=dat)

This would also work:

frm <- bquote(y~.(var1), list(var1=as.name(nam[1])))
mod<-lm(frm,data=dat)

This however does have a possible sideeffect, as in the following:
frm <- bquote(y~.(var1), list(var1=as.name(nam[2])))
mod2 <- update(mod)

mod2 is now the model for x2, even though we didn't give it an  
explicit new formula (The old formula had kept the difference in frm.

You can probably avoid this by:
mod<-lm(bquote(y~.(var1), list(var1=as.name(nam[1]))),data=dat)

Though again the name of the formula is not very pretty. The best  
one, from the point of view of getting the correct call in lm,  
probably is:

eval(bquote(lm(y~.(var1), data=dat), list(var1=as.name(nam[1]))))

> Dan

Hope this helps,
Haris Skiadas
Department of Mathematics and Computer Science
Hanover College

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From diegol81 at gmail.com  Mon Dec 31 16:11:29 2007
From: diegol81 at gmail.com (diegol)
Date: Mon, 31 Dec 2007 07:11:29 -0800 (PST)
Subject: [R] updating R version and packages.
In-Reply-To: <486910.42845.qm@web56009.mail.re3.yahoo.com>
References: <486910.42845.qm@web56009.mail.re3.yahoo.com>
Message-ID: <14558490.post@talk.nabble.com>


A menu-driven alternative: in the R Console GUI, select Packages | Update
packages.



Milton Cezar Ribeiro wrote:
> 
> Dear All,
> 
> Is there a way of I update automatically the R version and the respective
> packages which I have installed on my computer? Case not, how can I know
> about what packages I installed by my self on the computer?
> 


-----
~~~~~~~~~~~~~~~~~~~~~~~~~~
Diego Mazzeo
Actuarial Science Student
Facultad de Ciencias Econ?micas
Universidad de Buenos Aires
Buenos Aires, Argentina
-- 
View this message in context: http://www.nabble.com/updating-R-version-and-packages.-tp14549954p14558490.html
Sent from the R help mailing list archive at Nabble.com.


From cgenolin at u-paris10.fr  Mon Dec 31 16:24:04 2007
From: cgenolin at u-paris10.fr (Christophe Genolini)
Date: Mon, 31 Dec 2007 16:24:04 +0100
Subject: [R] R to LaTeX Univariate Analysis
Message-ID: <47790994.2010006@u-paris10.fr>

Hi all

Well, first: happy new year...

Second: I write a function in R that might interest some other people. 
On the other hand, I am closer to beginners than experts; I don't know 
how valuable my code is. I don't know how long it will take to me to 
create a library and I don't know if it's worth to. So before starting 
this long process, I would like some advices, both on the interest it 
present, on the way of using it and on the inner R code...

My function is called "r2lUniv" for "R to LaTeX, Univariate analysis". 
Given a variable, it performs some basic analysis in R and generates 
LaTeX code to include in a document that print the analysis in a clean 
LaTeX way. The basic analysis depends of the variable type. 4 types are 
considered:
  - Nominal: modality, size, barplot
  - Ordinal: modality, size, quartile, barplot
  - Discrete: modality, size, mean, var, quartile, boxplot, barplot
  - Continuous: mean, var, quartile, boxplot, barplot

A generalization of r2lUniv deals with data.frame by runnig the basic 
analysis on every column. So to use it:
 > dataFrame <- read.csv("myData.csv")
 > r2lUniv(dataFrame,"fileOut.tex")

It performs the basic analysis and creates all graphs. Then I add 
\input{fileOut.tex} in my main.tex file.

My source file is available at: 
http://christophe.genolini.free.fr/r2lUniv/mySource/
It is definitely NOT a code ready for a library, there is probably bugs 
in it, all comments and all variables used for testing are in. But 
still, any advices will be welcome...

Christophe

PS: Does the  manual "Writing R Extensions" exists in French? Or 
anything equivalent?


From epistat at gmail.com  Mon Dec 31 16:27:45 2007
From: epistat at gmail.com (zhijie zhang)
Date: Mon, 31 Dec 2007 23:27:45 +0800
Subject: [R] help on ROC analysis
Message-ID: <2fc17e30712310727r18ce79b9g8f99b436901c726b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/bc6146e7/attachment.pl 

From therneau at mayo.edu  Mon Dec 31 16:29:18 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Mon, 31 Dec 2007 09:29:18 -0600 (CST)
Subject: [R] Survival analysis with no events in one treatment group
Message-ID: <200712311529.lBVFTHF27739@hsrnfs-101.mayo.edu>

John,
   The key issue with a data set that has no events in one group is that the 
usual Wald tests, i.e., beta/se(beta), do not work.  They is based on a Taylor 
series argument of the usual type: f(x) = f(x0) + a polynomial in (x-x0), and 
for infinite beta "x" and "x0" are so far apart that the approximation isn't 
even close.  The score and likelihood ratio statistics are just fine, however. 
So if you completely ignore any printout that involves the "infinite beta" 
columns of the estimated variance matrix all should be well.
   
   In your case, a second issue is that the likelihood ratio test is not valid 
for data sets with multiple events per subject. You have to account for the 
within subject correlation in some way, either by moving to a random effects 
model or a gee type variance.  You have done the latter by adding "cluster" to 
your coxph call.  Thus, only the "robust score test" line of your final output 
is reliable.  The LR test is tainted by multiple events, and the robust Wald by 
the infinite beta.  Use summary() to print all the test statistics.
   
  Assume you have variables x1 and x2 in a model, and want to test the 
importance of adding x3.  Stepwise regression programs often use score tests for 
this, but most users have never done it and don't know how. 
   	> fit1 <- coxph(Surv(time1, time2, event) ~ x1 + x2 + cluster(subject),
   		               subset= (!is.na(x3)))
   	> fit2 <- coxph(Surv(time1, time2, event) ~ x1 + x2 + x3 + 
   		              cluster(subject), init=c(coef(fit1), 0))
   		              
Now the "robust score test" in fit2 is a test of (coef1, coef2, 0) [no need for 
x3] vs the model with x3.  If you have no missing values you can skip the subset 
argument, but make sure that the two fits are actually on the same data set: 
always check that the "n=" part of the printout matches.

	Terry Therneau
	Mayo Clinic


From liuwensui at gmail.com  Mon Dec 31 16:32:03 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 31 Dec 2007 10:32:03 -0500
Subject: [R] help on ROC analysis
In-Reply-To: <2fc17e30712310727r18ce79b9g8f99b436901c726b@mail.gmail.com>
References: <2fc17e30712310727r18ce79b9g8f99b436901c726b@mail.gmail.com>
Message-ID: <1115a2b00712310732i6c6cf4b2r418f6b7f572558e7@mail.gmail.com>

Hi, ZJ,
In verification package, there is a function that takes observed
response and predicted probability and then calculate the ROC.
I am not sure if it is what you are after.

On Dec 31, 2007 10:27 AM, zhijie zhang <epistat at gmail.com> wrote:
> Dear all,
>   Some functions like 'ROC(Epi)' can be used to perform ROC analyssi, but it
> needs us to specify the fitting model in the argument. Now i have got the
> predicted p-values (0,1) for the 0/1 response variable using some other
> approach, see the following example dataset:
>
> id   mark  predict.pvalue
>
> 1      1       0.927
>
> 2      0       0.928
>
> 3      1       0.928
>
> ..................
>
> *mark* is the true classes, *predict.pvalue* is the predicted p-values,
> which was used to determine the predicted classes. So i need to specify some
> cut points for *predict.pvalue*, and then compare it with *mark*class,
> generate the 2*2 tables, and then calculate some sensitivity,
> specifity....statistcs, and ROC curve.
>  I have searched some functions, such as roc(analogue),'ROC(Epi),etc. They
> may need to specify the fitting model in the codes or group varibles,
> and may be not appropriate for my condition. I think that it should
> have been performed in some package for ROC analysis.
>   Anybody can tell me which function is for this case?
>   Thanks very much.
> --
> With Kind Regards,
>
> oooO:::::::::
> (..):::::::::
> :\.(:::Oooo::
> ::\_)::(..)::
> :::::::)./:::
> ::::::(_/::::
> :::::::::::::
> [***********************************************************************]
> Zhi Jie,Zhang ,PHD
> Tel:+86-21-54237149
> Dept. of Epidemiology,School of Public Health,Fudan University
> Address:No. 138 Yi Xue Yuan Road,Shanghai,China
> Postcode:200032
> Email:epistat at gmail.com
> Website: www.statABC.com
> [***********************************************************************]
> oooO:::::::::
> (..):::::::::
> :\.(:::Oooo::
> ::\_)::(..)::
> :::::::)./:::
> ::::::(_/::::
> :::::::::::::
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
===============================
WenSui Liu
Statistical Project Manager
ChoicePoint Precision Marketing
(http://spaces.msn.com/statcompute/blog)


From f.harrell at vanderbilt.edu  Mon Dec 31 16:38:33 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 31 Dec 2007 09:38:33 -0600
Subject: [R] help on ROC analysis
In-Reply-To: <2fc17e30712310727r18ce79b9g8f99b436901c726b@mail.gmail.com>
References: <2fc17e30712310727r18ce79b9g8f99b436901c726b@mail.gmail.com>
Message-ID: <47790CF9.9090306@vanderbilt.edu>

zhijie zhang wrote:
> Dear all,
>   Some functions like 'ROC(Epi)' can be used to perform ROC analyssi, but it
> needs us to specify the fitting model in the argument. Now i have got the
> predicted p-values (0,1) for the 0/1 response variable using some other
> approach, see the following example dataset:
> 
> id   mark  predict.pvalue
> 
> 1      1       0.927
> 
> 2      0       0.928
> 
> 3      1       0.928
> 
> ..................
> 
> *mark* is the true classes, *predict.pvalue* is the predicted p-values,
> which was used to determine the predicted classes. So i need to specify some
> cut points for *predict.pvalue*, and then compare it with *mark*class,
> generate the 2*2 tables, and then calculate some sensitivity,
> specifity....statistcs, and ROC curve.
>  I have searched some functions, such as roc(analogue),'ROC(Epi),etc. They
> may need to specify the fitting model in the codes or group varibles,
> and may be not appropriate for my condition. I think that it should
> have been performed in some package for ROC analysis.
>   Anybody can tell me which function is for this case?
>   Thanks very much.

Forming the ROC curve can lead to bad statistical practice, e.g., use of 
non-pre-specified cutpoints and use of cutpoints in general.  The area 
under the ROC curve is a valid measure of predictive discrimination 
though (even though it cannot be used to compare 2 models as it is not 
sensitive enough).  To get the ROC area you can use the simple somers2 
function in the Hmisc package.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From m_hofert at web.de  Mon Dec 31 16:41:08 2007
From: m_hofert at web.de (Hofert Marius)
Date: Mon, 31 Dec 2007 16:41:08 +0100
Subject: [R] Different number of labels in different panels
Message-ID: <BFCE4071-0AA1-4E15-94B8-F411908E713D@web.de>

Hi,

I would like to put a number to each of the plotted curves in each  
panel. The problem is that there are different numbers of curves in  
different panels, so as you can see from the code below, I could put  
the correct numbers to the curves in the first panel, but for the  
second panel, both location and number of labels are incorrect (I  
would like to have "(1)" at location x=3.5, y=211 and of course no  
label "(2)" in the second panel). What is the correct way to solve  
this? I have a vector where the i-th entry specifies the number of  
curves in panel i, so I somehow have to bring this vector into play...

Thanks very much in advance.

Marius

library(lattice)
column1=c(1,1,1,1,1,1,2,2)
column2=c(1,1,1,2,2,2,1,1)
column3=c(1,2,3,1,2,3,4,5)
column4=c(111,112,113,121,122,123,211,212)
dataframe=data.frame 
(panelnumber=column1,curvenumber=column2,x=column3,y=column4)
xyplot(dataframe[,4]~dataframe[,3]|dataframe[, 
1],type="l",lty=1,groups=dataframe[,2],layout=c(2,1),aspect=1,
   panel=function(...){
     panel.xyplot(...)
     panel.text(3.4,113,label="(1)")
     panel.text(3.4,123,label="(2)")
   }
)


From gunter.berton at gene.com  Mon Dec 31 16:42:41 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 31 Dec 2007 07:42:41 -0800
Subject: [R] Symbolic substitution in parallel; use infinity symbol?
References: <C741B895-56F0-4322-B5AC-79EA275ECF91@anu.edu.au> 
Message-ID: <001d01c84bc3$c73def50$6701a8c0@gne.windows.gene.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/80a118bc/attachment.pl 

From ggrothendieck at gmail.com  Mon Dec 31 17:40:13 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 31 Dec 2007 11:40:13 -0500
Subject: [R] Different number of labels in different panels
In-Reply-To: <BFCE4071-0AA1-4E15-94B8-F411908E713D@web.de>
References: <BFCE4071-0AA1-4E15-94B8-F411908E713D@web.de>
Message-ID: <971536df0712310840y5b5625aflc35e006b12f048ac@mail.gmail.com>

Use this panel function:

function(x,y,subscripts,...){
    panel.superpose(x, y, subscripts, ...)
    grps <- as.character(sort(unique(dataframe[subscripts,2])))
    draw.key(simpleKey(grps), TRUE, vp = viewport(0.15, 0.9))
}

The code below is the same as in your post except we have added
library(grid) and replaced the panel function with the above one:

library(lattice)
library(grid)
column1=c(1,1,1,1,1,1,2,2)
column2=c(1,1,1,2,2,2,1,1)
column3=c(1,2,3,1,2,3,4,5)
column4=c(111,112,113,121,122,123,211,212)
dataframe=data.frame(panelnumber=column1,curvenumber=column2,x=column3,y=column4)
xyplot(dataframe[,4]~dataframe[,3]|dataframe[,1],type="l",lty=1,groups=dataframe[,2],layout=c(2,1),aspect=1,
  panel=function(x,y,subscripts,...){
    panel.superpose(x, y, subscripts, ...)
    grps <- as.character(sort(unique(dataframe[subscripts,2])))
    draw.key(simpleKey(grps), TRUE, vp = viewport(0.15, 0.9))
  }
)


On Dec 31, 2007 10:41 AM, Hofert Marius <m_hofert at web.de> wrote:
> Hi,
>
> I would like to put a number to each of the plotted curves in each
> panel. The problem is that there are different numbers of curves in
> different panels, so as you can see from the code below, I could put
> the correct numbers to the curves in the first panel, but for the
> second panel, both location and number of labels are incorrect (I
> would like to have "(1)" at location x=3.5, y=211 and of course no
> label "(2)" in the second panel). What is the correct way to solve
> this? I have a vector where the i-th entry specifies the number of
> curves in panel i, so I somehow have to bring this vector into play...
>
> Thanks very much in advance.
>
> Marius
>
> library(lattice)
> column1=c(1,1,1,1,1,1,2,2)
> column2=c(1,1,1,2,2,2,1,1)
> column3=c(1,2,3,1,2,3,4,5)
> column4=c(111,112,113,121,122,123,211,212)
> dataframe=data.frame
> (panelnumber=column1,curvenumber=column2,x=column3,y=column4)
> xyplot(dataframe[,4]~dataframe[,3]|dataframe[,
> 1],type="l",lty=1,groups=dataframe[,2],layout=c(2,1),aspect=1,
>   panel=function(...){
>     panel.xyplot(...)
>     panel.text(3.4,113,label="(1)")
>     panel.text(3.4,123,label="(2)")
>   }
> )
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tom.soyer at gmail.com  Mon Dec 31 17:53:30 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Mon, 31 Dec 2007 10:53:30 -0600
Subject: [R] stack charts right on top of each other
Message-ID: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/9b663d53/attachment.pl 

From jholtman at gmail.com  Mon Dec 31 18:39:34 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 31 Dec 2007 12:39:34 -0500
Subject: [R] stack charts right on top of each other
In-Reply-To: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
References: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
Message-ID: <644e1f320712310939v6f1d1ce3r5f1a07d6119bd4da@mail.gmail.com>

try this to get them to butt up against each other:

layout(rbind(1,2))
par(mar=c(0,4,3,2))
plot(rnorm(1:3),xaxt="n",xlab="")
par(mar=c(4,4,0,2))
plot(rnorm(1:3))

On Dec 31, 2007 11:53 AM, tom soyer <tom.soyer at gmail.com> wrote:
> Hi,
>
> I tried to stack two charts on top of each other using the following
> R functions:
>
> par(mfrow=c(2,1))
> plot(rnorm(1:3),xaxt="n",xlab="")
> plot(rnorm(1:3))
>
> This created two charts, one on top of the other, but there is too much
> space between them. Does anyone know how to elimiate the space in between
> the charts?
>
> Thanks,
>
> --
> Tom
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ligges at statistik.uni-dortmund.de  Mon Dec 31 18:43:01 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 31 Dec 2007 18:43:01 +0100
Subject: [R] stack charts right on top of each other
In-Reply-To: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
References: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
Message-ID: <47792A25.7080307@statistik.uni-dortmund.de>

See ?par and its argument "mar"

Uwe Ligges



tom soyer wrote:
> Hi,
> 
> I tried to stack two charts on top of each other using the following
> R functions:
> 
> par(mfrow=c(2,1))
> plot(rnorm(1:3),xaxt="n",xlab="")
> plot(rnorm(1:3))
> 
> This created two charts, one on top of the other, but there is too much
> space between them. Does anyone know how to elimiate the space in between
> the charts?
> 
> Thanks,
>


From deepayan.sarkar at gmail.com  Mon Dec 31 18:49:12 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 31 Dec 2007 09:49:12 -0800
Subject: [R] Different number of labels in different panels
In-Reply-To: <BFCE4071-0AA1-4E15-94B8-F411908E713D@web.de>
References: <BFCE4071-0AA1-4E15-94B8-F411908E713D@web.de>
Message-ID: <eb555e660712310949w29921d3blac55022971d67670@mail.gmail.com>

On 12/31/07, Hofert Marius <m_hofert at web.de> wrote:
> Hi,
>
> I would like to put a number to each of the plotted curves in each
> panel. The problem is that there are different numbers of curves in
> different panels, so as you can see from the code below, I could put
> the correct numbers to the curves in the first panel, but for the
> second panel, both location and number of labels are incorrect (I
> would like to have "(1)" at location x=3.5, y=211 and of course no
> label "(2)" in the second panel). What is the correct way to solve
> this? I have a vector where the i-th entry specifies the number of
> curves in panel i, so I somehow have to bring this vector into play...

A common legend is the usual approach, but take a look at

?panel.number

-Deepayan


From ligges at statistik.uni-dortmund.de  Mon Dec 31 18:50:15 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 31 Dec 2007 18:50:15 +0100
Subject: [R] SVM error
In-Reply-To: <1199102121.4778d8a9e2c26@gold5.portugalmail.pt>
References: <1199102121.4778d8a9e2c26@gold5.portugalmail.pt>
Message-ID: <47792BD7.9000708@statistik.uni-dortmund.de>

Are we talking about package "e1071"?


pedrosmarques at portugalmail.pt wrote:
> Hi all,
> 
> I'm having this error, since I'm working with a data matrix I don't understand what's happening; I've tried several ways to solve this, even working with sparse matrix, but nothing seems to solve it, I've also tried svm (with a simple matrix 3*3 and still got the same error.
> 
>> dados<-read.table("b.txt",sep="",nrows=30000)


We cannot reproduce: We do not have "b.txt". See the posting guide.



>> dados1<-as.matrix(dados[,-1],nrows=30000,ncol=13,type=numeric)

Why do you specify nrows, ncol and type=numeric?


>> dados2<-as.vector(dados[,1])

Well, as.vector() strips attributes, particularly it removes the 
"factor" attributes!


>> model<-svm(scale=TRUE,type=C,dados[,-1],y=dados[,1],kernel=RBF)

type=C and kernel=RBF cannot make sense, I guess you mean

     model <- svm(x=dados[,-1], y=dados[,1], scale=TRUE,
                  type="C", kernel="RBF")


> Error in as.character(x) : cannot coerce to vector
>> model<-svm(scale=TRUE,type=C,dados1,y=dados2,kernel=RBF)

Same as before.

Please do read "An Introduction to R" - particularly on how to specify 
arguments in function calls before proceeding.

Uwe Ligges



> Error in as.character(x) : cannot coerce to vector
> 
> Best regards,
> 
> Pedro Marques
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mw-u2 at gmx.de  Mon Dec 31 19:00:13 2007
From: mw-u2 at gmx.de (mw-u2 at gmx.de)
Date: Mon, 31 Dec 2007 19:00:13 +0100
Subject: [R] Optimize jackknife code
Message-ID: <20071231180013.60620@gmx.net>

Hi, 

I have the following jackknife code which is much slower than my colleagues C code. Yet I like R very much and wonder how R experts would optimize this.

I think that the for (i in 1:N_B) part is bad because Rprof() said sum() is called very often but I have no idea how to optimize it.

 
#O <- read.table("foo.dat")$V1
O <- runif(100000);

k=100 # size of block to delete
      # the jacknife block has size N-k

total_sum=sum(O);

for (k in 1:2) {

    N_B = length(O) %/% k;
    N = N_B*k; # truncate data size to multiple of k
               # data beyond O[N] is not used

    #total_sum = sum(O[1:N]) # truncate data size N (which is a multiple of k)

    delete_block_sums = rep(0, N_B);
    for (i in 1:N_B)
    {
        # calculate indizes of the block boundaries
        a = 1+k*(i-1);
        b = k*i;

        # sum of block to delete
        delete_block_sums[i] = sum(O[a:b]);                      
    }

    v = (total_sum - delete_block_sums) / (N-k)

    # The Jackknife error is given by
    # eps^2 = (N_B-1)/N_B sum( (O_J - \bar{O})^2 )
    # I don't understand the prefactor 

    #c(k, (N_B-1)**2/N_B * var(v));
    print(c(k, total_sum/N, sqrt( (N_B-1)/N_B * sum( (v-mean(v))**2 )) ));
}

--


From tom.soyer at gmail.com  Mon Dec 31 19:16:50 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Mon, 31 Dec 2007 12:16:50 -0600
Subject: [R] stack charts right on top of each other
In-Reply-To: <644e1f320712310939v6f1d1ce3r5f1a07d6119bd4da@mail.gmail.com>
References: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
	<644e1f320712310939v6f1d1ce3r5f1a07d6119bd4da@mail.gmail.com>
Message-ID: <65cc7bdf0712311016i324d562au803c09cc8641dae5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/66b06114/attachment.pl 

From ggrothendieck at gmail.com  Mon Dec 31 19:29:17 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 31 Dec 2007 13:29:17 -0500
Subject: [R] stack charts right on top of each other
In-Reply-To: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
References: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
Message-ID: <971536df0712311029i20ff5c73g8de3a515c66eca6d@mail.gmail.com>

Check out:
http://research.stowers-institute.org/efg/R/Graphics/Basics/mar-oma/index.htm

On Dec 31, 2007 11:53 AM, tom soyer <tom.soyer at gmail.com> wrote:
> Hi,
>
> I tried to stack two charts on top of each other using the following
> R functions:
>
> par(mfrow=c(2,1))
> plot(rnorm(1:3),xaxt="n",xlab="")
> plot(rnorm(1:3))
>
> This created two charts, one on top of the other, but there is too much
> space between them. Does anyone know how to elimiate the space in between
> the charts?
>
> Thanks,
>
> --
> Tom
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at imail.org  Mon Dec 31 19:58:03 2007
From: Greg.Snow at imail.org (Greg Snow)
Date: Mon, 31 Dec 2007 11:58:03 -0700
Subject: [R] Histogram with different colors for different portions
References: <fc8cc0912fb6.4777dc12@appstate.edu>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A1DA@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/9d688aa2/attachment.pl 

From tom.soyer at gmail.com  Mon Dec 31 20:05:28 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Mon, 31 Dec 2007 13:05:28 -0600
Subject: [R] stack charts right on top of each other
In-Reply-To: <971536df0712311029i20ff5c73g8de3a515c66eca6d@mail.gmail.com>
References: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
	<971536df0712311029i20ff5c73g8de3a515c66eca6d@mail.gmail.com>
Message-ID: <65cc7bdf0712311105i65581d3ay4c3cc905fe1ee8f3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/0254aca0/attachment.pl 

From jean-v.cote at sympatico.ca  Mon Dec 31 20:37:28 2007
From: jean-v.cote at sympatico.ca (=?iso-8859-1?B?SmVhbi1WaWN0b3IgQ/R06Q==?=)
Date: Mon, 31 Dec 2007 14:37:28 -0500
Subject: [R] Program output to file using a batch command file
Message-ID: <BAY111-F21746ADC992400F783466BD9500@phx.gbl>


If I copy the following commands in my batch file to the R console, R 
creates a file named "basicStatsIBM.lis" with the program output in it:

Sortie <- file(description = "basicStatsIBM.lis", open = "wt", blocking = 
TRUE,
     encoding = "UTF-8")
sink(file = Sortie, append = FALSE, type = "output", split = FALSE)
basicStats(ibm)
close(Sortie)

However, the same commands in a text file (named batch.txt, for instance) 
referred to from the R console by the command source("batch.txt") yields an 
empty "basicStatsIBM.lis" file.

Why the discrepancy in processing?
How to make the batch commands work as if the commands were pasted directly 
in the R console?
A solution to this problem would save time and aggravation to many people.  
Thank you.


From jholtman at gmail.com  Mon Dec 31 20:46:37 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 31 Dec 2007 14:46:37 -0500
Subject: [R] stack charts right on top of each other
In-Reply-To: <65cc7bdf0712311016i324d562au803c09cc8641dae5@mail.gmail.com>
References: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
	<644e1f320712310939v6f1d1ce3r5f1a07d6119bd4da@mail.gmail.com>
	<65cc7bdf0712311016i324d562au803c09cc8641dae5@mail.gmail.com>
Message-ID: <644e1f320712311146w704fad6ci5fc9931737b9ce13@mail.gmail.com>

To make the charts the same size, try setting the top and bottom
margins to be the same; e.g.,

layout(rbind(1,2))
par(mar=c(0,4,4,2))
plot(rnorm(1:3),xaxt="n",xlab="")
par(mar=c(4,4,0,2))
plot(rnorm(1:3))

On Dec 31, 2007 1:16 PM, tom soyer <tom.soyer at gmail.com> wrote:
> Thanks Jim! It seems layout() is necessary in addition to mar=. I have a
> follow up question: is there a way to specify the height of each chart so
> that all the charts have the same height? I tried pin=, but it created more
> space (if the height is set to a small value) between the charts, although
> it fixed the height of each chart. Thanks!
>
>
> On 12/31/07, jim holtman <jholtman at gmail.com> wrote:
> > try this to get them to butt up against each other:
> >
> > layout(rbind(1,2))
> > par(mar=c(0,4,3,2))
> > plot(rnorm(1:3),xaxt="n",xlab="")
> > par(mar=c(4,4,0,2))
> > plot(rnorm(1:3))
> >
> > On Dec 31, 2007 11:53 AM, tom soyer <tom.soyer at gmail.com> wrote:
> > > Hi,
> > >
> > > I tried to stack two charts on top of each other using the following
> > > R functions:
> > >
> > > par(mfrow=c(2,1))
> > > plot(rnorm(1:3),xaxt="n",xlab="")
> > > plot(rnorm(1:3))
> > >
> > > This created two charts, one on top of the other, but there is too much
> > > space between them. Does anyone know how to elimiate the space in
> between
> > > the charts?
> > >
> > > Thanks,
> > >
> > > --
> > > Tom
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 646 9390
> >
> > What is the problem you are trying to solve?
> >
>
>
>
> --
> Tom



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From tom.soyer at gmail.com  Mon Dec 31 21:20:08 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Mon, 31 Dec 2007 14:20:08 -0600
Subject: [R] stack charts right on top of each other
In-Reply-To: <65cc7bdf0712311105i65581d3ay4c3cc905fe1ee8f3@mail.gmail.com>
References: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
	<971536df0712311029i20ff5c73g8de3a515c66eca6d@mail.gmail.com>
	<65cc7bdf0712311105i65581d3ay4c3cc905fe1ee8f3@mail.gmail.com>
Message-ID: <65cc7bdf0712311220l14349956le6482533a87aa8da@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/e4eae40e/attachment.pl 

From tom.soyer at gmail.com  Mon Dec 31 21:33:27 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Mon, 31 Dec 2007 14:33:27 -0600
Subject: [R] stack charts right on top of each other
In-Reply-To: <644e1f320712311146w704fad6ci5fc9931737b9ce13@mail.gmail.com>
References: <65cc7bdf0712310853k44f5ce8dna3cae80c2a5366d9@mail.gmail.com>
	<644e1f320712310939v6f1d1ce3r5f1a07d6119bd4da@mail.gmail.com>
	<65cc7bdf0712311016i324d562au803c09cc8641dae5@mail.gmail.com>
	<644e1f320712311146w704fad6ci5fc9931737b9ce13@mail.gmail.com>
Message-ID: <65cc7bdf0712311233v5639d877k801fd26e4e148d40@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/543377f0/attachment.pl 

From nuranisah_mohamed at yahoo.com  Mon Dec 31 21:51:14 2007
From: nuranisah_mohamed at yahoo.com (mohamed nur anisah)
Date: Mon, 31 Dec 2007 12:51:14 -0800 (PST)
Subject: [R] How to import ENSEMBL text data using R
Message-ID: <797458.21759.qm@web55702.mail.re3.yahoo.com>

Dear all,
  I have a data which is in text file and i would like to import the data to R. From the manual, i?ve found the read.table command function is the most appropriate but when i wrote the command an error had occur. It say ?Error in read.table"C:/Users/user/Documents/cfa-1.txt", header = T, sep = "\t",skip=10) :more columns than column names?. Please help me with this as i?m a first time user to R.
   
  Thanks in advance.
   
  Cheers,
  Anisah
   

       
---------------------------------
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: cfa-1.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/c50b082d/attachment.txt 

From anup_nandialath at yahoo.com  Mon Dec 31 22:16:07 2007
From: anup_nandialath at yahoo.com (Anup Nandialath)
Date: Mon, 31 Dec 2007 13:16:07 -0800 (PST)
Subject: [R] Bootstrapping
Message-ID: <165149.33373.qm@web53308.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/b4333778/attachment.pl 

From njoseph0 at gmail.com  Mon Dec 31 14:34:05 2007
From: njoseph0 at gmail.com (Joesph)
Date: Mon, 31 Dec 2007 21:34:05 +0800
Subject: [R] proximity on prediction in cforest
Message-ID: <42ebc9840712310534t799fb050rcd44d0a9c5451509@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/39e35401/attachment.pl 

From njoseph0 at gmail.com  Mon Dec 31 14:34:05 2007
From: njoseph0 at gmail.com (Joesph)
Date: Mon, 31 Dec 2007 21:34:05 +0800
Subject: [R] proximity on prediction in cforest
Message-ID: <42ebc9840712310534t799fb050rcd44d0a9c5451509@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20071231/39e35401/attachment.asc 

